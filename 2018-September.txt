From henrik@bengt@@on @ending from gm@il@com  Sat Sep  1 02:24:32 2018
From: henrik@bengt@@on @ending from gm@il@com (Henrik Bengtsson)
Date: Fri, 31 Aug 2018 17:24:32 -0700
Subject: [Rd] ROBUSTNESS: x || y and x && y to give warning/error if
 length(x) != 1 or length(y) != 1
In-Reply-To: <CF5BD448-B846-43EE-AB33-F3BFD68C5E34@dans.knaw.nl>
References: <CAFDcVCSEqHvfhD358GD3Fgz6FjuOiMiYHswz+pyTnAEoV0CyZw@mail.gmail.com>
 <CAO1zAVaCyKOQLFi2161PB3ycHbr72ue3EtTa189Lz-C+rO1_0w@mail.gmail.com>
 <c5c5721f-65fb-39f0-c7e9-77bb413fbb03@kogentum.hu>
 <CAO1zAVYgQyKP3AbyB_AdTPKogWGQs=cC+YaTtLuG2OSahzvrVA@mail.gmail.com>
 <23432.5145.624837.968060@stat.math.ethz.ch>
 <CABdHhvHvjHNdD3XkU=qA8CzxnL6VFritQ2tPy7JrY8VfbnR4rg@mail.gmail.com>
 <CF5BD448-B846-43EE-AB33-F3BFD68C5E34@dans.knaw.nl>
Message-ID: <CAFDcVCSkhGPSPBPXaqwh5gVB1MhbzJWUVz4iNzJPysrWoS8X=A@mail.gmail.com>

Thanks all for a great discussion.

I think we can introduce assertions for length(x) <= 1 (and produce a
warning/error if not) without changing the value of these &&/||
expressions.

In R 3.4.0, '_R_CHECK_LENGTH_1_CONDITION_=true' was introduced to turn
warnings on "the condition has length > 1 and only the first element
will be used" in cases like 'if (c(TRUE, TRUE)) 42'  into errors.  The
idea is to later make '_R_CHECK_LENGTH_1_CONDITION_=true' the new
default.  I guess, someday this will always produce an error.

Similarly, the test for this &&/|| issue could be controlled by
'_R_CHECK_LENGTH_1_LOGICAL_OPS_=warn' and
'_R_CHECK_LENGTH_1_LOGICAL_OPS_=err' and possibly have
'_R_CHECK_LENGTH_1_LOGICAL_OPS_=true' default to 'warn' and later
'err'.

Changing the behavior of cases where length(x) == 0 is more likely to
break *some* code out there, and might require a separate
discussion/set of validations.  It's not unlikely that someone
actually relied on this to resolve to NA.  BTW, since it hasn't been
explicitly said, it's "logical" that we have TRUE && logical(0)
resolving to NA, because it currently behaves as TRUE[1] &&
logical(0)[1], which resolves to TRUE && NA => NA.  If a decision on
the zero-length case would delay fixing the length(x) > 1 case, I
would postpone the decision on the former.

/Henrik

On Fri, Aug 31, 2018 at 2:48 AM Emil Bode <emil.bode at dans.knaw.nl> wrote:
>
>
> ?On 30/08/2018, 20:15, "R-devel on behalf of Hadley Wickham" <r-devel-bounces at r-project.org on behalf of h.wickham at gmail.com> wrote:
>
>     On Thu, Aug 30, 2018 at 10:58 AM Martin Maechler
>     <maechler at stat.math.ethz.ch> wrote:
>     >
>     > >>>>> Joris Meys
>     > >>>>>     on Thu, 30 Aug 2018 14:48:01 +0200 writes:
>     >
>     >     > On Thu, Aug 30, 2018 at 2:09 PM D?nes T?th
>     >     > <toth.denes at kogentum.hu> wrote:
>     >     >> Note that `||` and `&&` have never been symmetric:
>     >     >>
>     >     >> TRUE || stop() # returns TRUE stop() || TRUE # returns an
>     >     >> error
>     >     >>
>     >     >>
>     >     > Fair point. So the suggestion would be to check whether x
>     >     > is of length 1 and whether y is of length 1 only when
>     >     > needed. I.e.
>     >
>     >     > c(TRUE,FALSE) || TRUE
>     >
>     >     > would give an error and
>     >
>     >     > TRUE || c(TRUE, FALSE)
>     >
>     >     > would pass.
>     >
>     >     > Thought about it a bit more, and I can't come up with a
>     >     > use case where the first line must pass. So if the short
>     >     > circuiting remains and the extra check only gives a small
>     >     > performance penalty, adding the error could indeed make
>     >     > some bugs more obvious.
>     >
>     > I agree "in theory".
>     > Thank you, Henrik, for bringing it up!
>     >
>     > In practice I think we should start having a warning signalled.
>     > I have checked the source code in the mean time, and the check
>     > is really very cheap
>     > { because it can/should be done after checking isNumber(): so
>     >   then we know we have an atomic and can use XLENGTH() }
>     >
>     >
>     > The 0-length case I don't think we should change as I do find
>     > NA (is logical!) to be an appropriate logical answer.
>
>     Can you explain your reasoning a bit more here? I'd like to understand
>     the general principle, because from my perspective it's more
>     parsimonious to say that the inputs to || and && must be length 1,
>     rather than to say that inputs could be length 0 or length 1, and in
>     the length 0 case they are replaced with NA.
>
>     Hadley
>
> I would say the value NA would cause warnings later on, that are easy to track down, so a return of NA is far less likely to cause problems than an unintended TRUE or FALSE. And I guess there would be some code reliant on 'logical(0) || TRUE' returning TRUE, that wouldn't necessarily be a mistake.
>
> But I think it's hard to predict how exactly people are using functions. I personally can't imagine a situation where I'd use || or && outside an if-statement, so I'd rather have the current behaviour, because I'm not sure if I'm reliant on logical(0) || TRUE  somewhere in my code (even though that would be ugly code, it's not wrong per se)
> But I could always rewrite it, so I believe it's more a question of how much would have to be rewritten. Maybe implement it first in devel, to see how many people would complain?
>
> Emil Bode
>
>
>
>


From hugh@p@r@on@ge @ending from gm@il@com  Sat Sep  1 09:42:52 2018
From: hugh@p@r@on@ge @ending from gm@il@com (Hugh Parsonage)
Date: Sat, 1 Sep 2018 17:42:52 +1000
Subject: [Rd] ROBUSTNESS: x || y and x && y to give warning/error if
 length(x) != 1 or length(y) != 1
In-Reply-To: <0C3093D1-5FE0-4DCC-AFC0-CEA6FCBA4580@dans.knaw.nl>
References: <CAFDcVCSEqHvfhD358GD3Fgz6FjuOiMiYHswz+pyTnAEoV0CyZw@mail.gmail.com>
 <0C3093D1-5FE0-4DCC-AFC0-CEA6FCBA4580@dans.knaw.nl>
Message-ID: <CAJmOi+PAxdRgTevFHC8Fna-hbtgQLOMrEU+bGzwG5U_fwoB5Eg@mail.gmail.com>

I would add my support for the change.

>From a cursory survey of existing code, I'd say that usage like `class(x)
== 'y'` -- rather than inherits(x, 'y') or is.y -- is probably going to be
the major source of new warnings. So perhaps in the NEWS item it could be
noted as a clue for developers encountering nascent warnings.

Of course `if (class(x) == 'y')` already throws a warning, just not `if
(class(x) == 'y' && TRUE)`.



On Thu, 30 Aug 2018 at 21:09 Emil Bode <emil.bode at dans.knaw.nl> wrote:

> I have to disagree, I think one of the advantages of '||' (or &&) is the
> lazy evaluation, i.e. you can use the first condition to "not care" about
> the second (and stop errors from being thrown).
> So if I want to check if x is a length-one numeric with value a value
> between 0 and 1, I can do 'class(x)=='numeric' && length(x)==1 && x>0 &&
> x<1'.
> In your proposal, having x=c(1,2) would throw an error or multiple
> warnings.
> Also code that relies on the second argument not being evaluated would
> break, as we need to evaluate y in order to know length(y)
> There may be some benefit in checking for length(x) only, though that
> could also cause some false positives (e.g. 'x==-1 || length(x)==0' would
> be a bit ugly, but not necessarily wrong, same for someone too lazy to
> write x[1] instead of x).
>
> And I don?t really see the advantage. The casting to length one is (I
> think), a feature, not a bug. If I have/need a length one x, and a length
> one y, why not use '|' and '&'? I have to admit I only use them in
> if-statements, and if I need an error to be thrown when x and y are not
> length one, I can use the shorter versions and then the if throws a warning
> (or an error for a length-0 or NA result).
>
> I get it that for someone just starting in R, the differences between |
> and || can be confusing, but I guess that's just the price to pay for
> having a vectorized language.
>
> Best regards,
> Emil Bode
>
> Data-analyst
>
> +31 6 43 83 89 33
> emil.bode at dans.knaw.nl
>
> DANS: Netherlands Institute for Permanent Access to Digital Research
> Resources
> Anna van Saksenlaan 51 | 2593 HW Den Haag
> <https://maps.google.com/?q=Anna+van+Saksenlaan+51+%7C+2593+HW+Den+Haag&entry=gmail&source=g>
> | +31 70 349 44 50 | info at dans.knaw.nl <mailto:info at dans.kn> |
> dans.knaw.nl <applewebdata://71F677F0-6872-45F3-A6C4-4972BF87185B/
> www.dans.knaw.nl>
> DANS is an institute of the Dutch Academy KNAW <http://knaw.nl/nl> and
> funding organisation NWO <http://www.nwo.nl/>.
>
> ?On 29/08/2018, 05:03, "R-devel on behalf of Henrik Bengtsson" <
> r-devel-bounces at r-project.org on behalf of henrik.bengtsson at gmail.com>
> wrote:
>
>     # Issue
>
>     'x || y' performs 'x[1] || y' for length(x) > 1.  For instance (here
>     using R 3.5.1),
>
>     > c(TRUE, TRUE) || FALSE
>     [1] TRUE
>     > c(TRUE, FALSE) || FALSE
>     [1] TRUE
>     > c(TRUE, NA) || FALSE
>     [1] TRUE
>     > c(FALSE, TRUE) || FALSE
>     [1] FALSE
>
>     This property is symmetric in LHS and RHS (i.e. 'y || x' behaves the
>     same) and it also applies to 'x && y'.
>
>     Note also how the above truncation of 'x' is completely silent -
>     there's neither an error nor a warning being produced.
>
>
>     # Discussion/Suggestion
>
>     Using 'x || y' and 'x && y' with a non-scalar 'x' or 'y' is likely a
>     mistake.  Either the code is written assuming 'x' and 'y' are scalars,
>     or there is a coding error and vectorized versions 'x | y' and 'x & y'
>     were intended.  Should 'x || y' always be considered an mistake if
>     'length(x) != 1' or 'length(y) != 1'?  If so, should it be a warning
>     or an error?  For instance,
>     '''r
>     > x <- c(TRUE, TRUE)
>     > y <- FALSE
>     > x || y
>
>     Error in x || y : applying scalar operator || to non-scalar elements
>     Execution halted
>
>     What about the case where 'length(x) == 0' or 'length(y) == 0'?  Today
>     'x || y' returns 'NA' in such cases, e.g.
>
>     > logical(0) || c(FALSE, NA)
>     [1] NA
>     > logical(0) || logical(0)
>     [1] NA
>     > logical(0) && logical(0)
>     [1] NA
>
>     I don't know the background for this behavior, but I'm sure there is
>     an argument behind that one.  Maybe it's simply that '||' and '&&'
>     should always return a scalar logical and neither TRUE nor FALSE can
>     be returned.
>
>     /Henrik
>
>     PS. This is in the same vein as
>     https://mailman.stat.ethz.ch/pipermail/r-devel/2017-March/073817.html
>     - in R (>=3.4.0) we now get that if (1:2 == 1) ... is an error if
>     _R_CHECK_LENGTH_1_CONDITION_=true
>
>     ______________________________________________
>     R-devel at r-project.org mailing list
>     https://stat.ethz.ch/mailman/listinfo/r-devel
>
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From Kurt@Hornik @ending from wu@@c@@t  Sat Sep  1 14:42:08 2018
From: Kurt@Hornik @ending from wu@@c@@t (Kurt Hornik)
Date: Sat, 1 Sep 2018 14:42:08 +0200
Subject: [Rd] Argument 'dim' misspelled in error message
In-Reply-To: <777dc8b8-368a-dcdb-0aaf-e484475ed90f@fredhutch.org>
References: <777dc8b8-368a-dcdb-0aaf-e484475ed90f@fredhutch.org>
Message-ID: <23434.35104.846770.367530@hornik.net>

>>>>> Herv? Pag?s writes:

Thanks: fixed in the trunk with c75223.

Best
-k

> Hi,
> The following error message misspells the name of
> the 'dim' argument:

>> array(integer(0), dim=integer(0))
>    Error in array(integer(0), dim = integer(0)) :
>      'dims' cannot be of length 0

> The name of the argument is 'dim' not 'dims':

>> args(array)
>    function (data = NA, dim = length(data), dimnames = NULL)
>    NULL

> Cheers,
> H.

> -- 
> Herv? Pag?s

> Program in Computational Biology
> Division of Public Health Sciences
> Fred Hutchinson Cancer Research Center
> 1100 Fairview Ave. N, M1-B514
> P.O. Box 19024
> Seattle, WA 98109-1024

> E-mail: hpages at fredhutch.org
> Phone:  (206) 667-5791
> Fax:    (206) 667-1319

> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From Kurt@Hornik @ending from wu@@c@@t  Sat Sep  1 14:48:15 2018
From: Kurt@Hornik @ending from wu@@c@@t (Kurt Hornik)
Date: Sat, 1 Sep 2018 14:48:15 +0200
Subject: [Rd] Segfault when performing match on POSIXlt object
In-Reply-To: <CAA9Rc3PLkKxJbnb_gQ8JN=PKQaO4TEAqRKvyw-NYcLC0oX2MvA@mail.gmail.com>
References: <CAA9Rc3PFDr_Rn9toAcwpp5MOvcdEbGMgfDnc7n6PFK+cHxZYAw@mail.gmail.com>
 <23433.18919.859126.521094@stat.math.ethz.ch>
 <23433.19189.841070.766209@stat.math.ethz.ch>
 <CAA9Rc3N=Lji03ozt0qC=CeOa589ckQc9=iuq3hAmJfDLhmnkWw@mail.gmail.com>
 <23433.29002.152055.941837@stat.math.ethz.ch>
 <CAA9Rc3PLkKxJbnb_gQ8JN=PKQaO4TEAqRKvyw-NYcLC0oX2MvA@mail.gmail.com>
Message-ID: <23434.35471.446222.983272@hornik.net>

>>>>> Marco Giuliano writes:

Thanks.  Should be fixed in the trunk with c75224: will close the PR
after more testing.

Best
-k

> Bug report submitted :
> https://bugs.r-project.org/bugzilla/show_bug.cgi?id=17459
> Thanks!

> On Fri, Aug 31, 2018 at 6:48 PM Martin Maechler <maechler at stat.math.ethz.ch>
> wrote:

>> >>>>> Marco Giuliano
>> >>>>>     on Fri, 31 Aug 2018 16:50:56 +0200 writes:
>> 
>> > Hi Martin, should I file a formal bug report somewhere or
>> > you've already done it ?
>> 
>> No, I haven't,
>> and as I may not address this bug further myself (in the near
>> future), it may be best if you file a formal report.
>> 
>> I will create an account for you on R's bugzilla - you will be
>> notified and can update your initial pseudo-random password.
>> 
>> Best,
>> Martin
>> 
>> 

> 	[[alternative HTML version deleted]]

> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From mgiuli@no@m@il @ending from gm@il@com  Sat Sep  1 17:36:30 2018
From: mgiuli@no@m@il @ending from gm@il@com (Marco Giuliano)
Date: Sat, 1 Sep 2018 17:36:30 +0200
Subject: [Rd] Segfault when performing match on POSIXlt object
In-Reply-To: <23434.35471.446222.983272@hornik.net>
References: <CAA9Rc3PFDr_Rn9toAcwpp5MOvcdEbGMgfDnc7n6PFK+cHxZYAw@mail.gmail.com>
 <23433.18919.859126.521094@stat.math.ethz.ch>
 <23433.19189.841070.766209@stat.math.ethz.ch>
 <CAA9Rc3N=Lji03ozt0qC=CeOa589ckQc9=iuq3hAmJfDLhmnkWw@mail.gmail.com>
 <23433.29002.152055.941837@stat.math.ethz.ch>
 <CAA9Rc3PLkKxJbnb_gQ8JN=PKQaO4TEAqRKvyw-NYcLC0oX2MvA@mail.gmail.com>
 <23434.35471.446222.983272@hornik.net>
Message-ID: <CAA9Rc3O6MjdGoAEDO=VJEi9nJqE8DTJK4ey28An_4CdSyqyqgg@mail.gmail.com>

Thanks a lot !

Best,
Marco

On Sat, Sep 1, 2018 at 2:48 PM Kurt Hornik <Kurt.Hornik at wu.ac.at> wrote:

> >>>>> Marco Giuliano writes:
>
> Thanks.  Should be fixed in the trunk with c75224: will close the PR
> after more testing.
>
> Best
> -k
>
> > Bug report submitted :
> > https://bugs.r-project.org/bugzilla/show_bug.cgi?id=17459
> > Thanks!
>
> > On Fri, Aug 31, 2018 at 6:48 PM Martin Maechler <
> maechler at stat.math.ethz.ch>
> > wrote:
>
> >> >>>>> Marco Giuliano
> >> >>>>>     on Fri, 31 Aug 2018 16:50:56 +0200 writes:
> >>
> >> > Hi Martin, should I file a formal bug report somewhere or
> >> > you've already done it ?
> >>
> >> No, I haven't,
> >> and as I may not address this bug further myself (in the near
> >> future), it may be best if you file a formal report.
> >>
> >> I will create an account for you on R's bugzilla - you will be
> >> notified and can update your initial pseudo-random password.
> >>
> >> Best,
> >> Martin
> >>
> >>
>
> >       [[alternative HTML version deleted]]
>
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From toth@dene@ @ending from kogentum@hu  Sun Sep  2 01:19:55 2018
From: toth@dene@ @ending from kogentum@hu (=?UTF-8?B?RMOpbmVzIFTDs3Ro?=)
Date: Sun, 2 Sep 2018 01:19:55 +0200
Subject: [Rd] True length - length(unclass(x)) - without having to call
 unclass()?
In-Reply-To: <CAFDcVCQdmLYM4gPj_zoz4KeoHGGUzJux9hLK_3vMVaeWUsrHQQ@mail.gmail.com>
References: <CAFDcVCQdmLYM4gPj_zoz4KeoHGGUzJux9hLK_3vMVaeWUsrHQQ@mail.gmail.com>
Message-ID: <d2733c97-5bec-dc0e-cb05-812b0cd63ff5@kogentum.hu>

The solution below introduces a dependency on data.table, but otherwise 
it does what you need:

---

# special method for Foo objects
length.Foo <- function(x) {
   length(unlist(x, recursive = TRUE, use.names = FALSE))
}

# an instance of a Foo object
x <- structure(list(a = 1, b = list(b1 = 1, b2 = 2)), class = "Foo")

# its length
stopifnot(length(x) == 3L)

# get its length as if it were a standard list
.length <- function(x) {
   cls <- class(x)
   # setattr() does not make a copy, but modifies by reference
   data.table::setattr(x, "class", NULL)
   # get the length
   len <- base::length(x)
   # re-set original classes
   data.table::setattr(x, "class", cls)
   # return the unclassed length
   len
}

# to check that we do not make unwanted changes
orig_class <- class(x)

# check that the address in RAM does not change
a1 <- data.table::address(x)

# 'unclassed' length
stopifnot(.length(x) == 2L)

# check that address is the same
stopifnot(a1 == data.table::address(x))

# check against original class
stopifnot(identical(orig_class, class(x)))

---


On 08/24/2018 07:55 PM, Henrik Bengtsson wrote:
> Is there a low-level function that returns the length of an object 'x'
> - the length that for instance .subset(x) and .subset2(x) see? An
> obvious candidate would be to use:
> 
> .length <- function(x) length(unclass(x))
> 
> However, I'm concerned that calling unclass(x) may trigger an
> expensive copy internally in some cases.  Is that concern unfounded?
> 
> Thxs,
> 
> Henrik
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From henrik@bengt@@on @ending from gm@il@com  Sun Sep  2 06:54:38 2018
From: henrik@bengt@@on @ending from gm@il@com (Henrik Bengtsson)
Date: Sat, 1 Sep 2018 21:54:38 -0700
Subject: [Rd] ROBUSTNESS: a:b to give an error when length(a) > 1 or
 length(b) > 1 - not just a warning
Message-ID: <CAFDcVCRW2QLoqYGaKVM=PX3nza7wzfcBYrDiJzAWPKKzNzEgoA@mail.gmail.com>

For the construct a:b, we get an error if either 'a' or 'b' is empty, e.g.

> x <- integer(0)
> x:3
Error in x:3 : argument of length 0
> 3:x
Error in 3:x : argument of length 0


However, for length(a) > 1 or length(b) > 1, we only get a warning;

> x <- 1:2
> x:3
[1] 1 2 3
Warning in x:3 :
In x:3 : numerical expression has 2 elements: only the first used
> 3:x
[1] 3 2 1
Warning message:
In 3:x : numerical expression has 2 elements: only the first used


I'd like to suggest to update R to produce an *error* whenever
length(a) != 1 or length(b) != 1.  For example,

> x <- 1:3
> x:3
Error in x:3 : argument is not of length one
> 3:x
Error in 3:x : argument is not of length one

The code involved is in the native function do_colon() of src/main/seq.c.

/Henrik


From h@wickh@m @ending from gm@il@com  Sun Sep  2 15:08:35 2018
From: h@wickh@m @ending from gm@il@com (Hadley Wickham)
Date: Sun, 2 Sep 2018 08:08:35 -0500
Subject: [Rd] True length - length(unclass(x)) - without having to call
 unclass()?
In-Reply-To: <CAFDcVCQdmLYM4gPj_zoz4KeoHGGUzJux9hLK_3vMVaeWUsrHQQ@mail.gmail.com>
References: <CAFDcVCQdmLYM4gPj_zoz4KeoHGGUzJux9hLK_3vMVaeWUsrHQQ@mail.gmail.com>
Message-ID: <CABdHhvEQTOO=+qmwad+0aJb5WTzF2BFQhN5Lde9NgyiTNrsfWg@mail.gmail.com>

For the new vctrs::records class, I implemented length, names, [[, and
[[<- myself in https://github.com/r-lib/vctrs/blob/master/src/fields.c.
That lets me override the default S3 methods while still being able to
access the underlying data that I'm interested in.

Another option that avoids (that you should never discuss in public
?) is temporarily setting the object bit to FALSE.

In the long run, I think an ALTREP vector that exposes the underlying
data of an S3 object (i.e. sans attributes apart from names) is
probably the way forward.

Hadley
On Fri, Aug 24, 2018 at 1:03 PM Henrik Bengtsson
<henrik.bengtsson at gmail.com> wrote:
>
> Is there a low-level function that returns the length of an object 'x'
> - the length that for instance .subset(x) and .subset2(x) see? An
> obvious candidate would be to use:
>
> .length <- function(x) length(unclass(x))
>
> However, I'm concerned that calling unclass(x) may trigger an
> expensive copy internally in some cases.  Is that concern unfounded?
>
> Thxs,
>
> Henrik
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel



-- 
http://hadley.nz


From jtelleri@@rproject @ending from gm@il@com  Mon Sep  3 09:20:12 2018
From: jtelleri@@rproject @ending from gm@il@com (Juan Telleria Ruiz de Aguirre)
Date: Mon, 3 Sep 2018 09:20:12 +0200
Subject: [Rd] compairing doubles
In-Reply-To: <CALEXWq3hNdLZwf9v=YZNTJ1FjTJfN6xuuUeSZ-zyZq1Sp+8yKA@mail.gmail.com>
References: <AM0PR0502MB387404BC690F9AF421B6F5EAD6080@AM0PR0502MB3874.eurprd05.prod.outlook.com>
 <CALEXWq3wSS4uaYu33oA-KUu7gSBuqKYF0ESemwvsbmp6KktOJg@mail.gmail.com>
 <95F39593-22F4-46B6-A766-8FF60F608C0C@dans.knaw.nl>
 <CAOKDuOgykFYMA-ZT4kUxr_NKXBEBsxZV8s-3sAuQ3Motc5M_Fw@mail.gmail.com>
 <CALEXWq2ke081QLqbv89z=yfTuoocLw9dzQWx6-05gzSFqAD3vQ@mail.gmail.com>
 <CAOKDuOg5bA0Uf5bmQX_9ZP6qbnO=hewb7cVbjEEZEvu5KOGC5w@mail.gmail.com>
 <e275b09d-39c8-a7e2-fc11-30c00034d1e4@insa-toulouse.fr>
 <CALEXWq3hNdLZwf9v=YZNTJ1FjTJfN6xuuUeSZ-zyZq1Sp+8yKA@mail.gmail.com>
Message-ID: <CAJXDcw3CJsDoLN_=_jf7Ud554Cm6DikHOhwQLWw9aUOycuuy5A@mail.gmail.com>

Maybe a new Operator could be defined for a fast and easy double
Comparison: `~~`

`~~` <- function (e1, e2)  all.equal(e1, e2)

And document it properly.


From ruipb@rr@d@@ @ending from @@po@pt  Mon Sep  3 10:58:34 2018
From: ruipb@rr@d@@ @ending from @@po@pt (Rui Barradas)
Date: Mon, 3 Sep 2018 09:58:34 +0100
Subject: [Rd] compairing doubles
In-Reply-To: <CAJXDcw3CJsDoLN_=_jf7Ud554Cm6DikHOhwQLWw9aUOycuuy5A@mail.gmail.com>
References: <AM0PR0502MB387404BC690F9AF421B6F5EAD6080@AM0PR0502MB3874.eurprd05.prod.outlook.com>
 <CALEXWq3wSS4uaYu33oA-KUu7gSBuqKYF0ESemwvsbmp6KktOJg@mail.gmail.com>
 <95F39593-22F4-46B6-A766-8FF60F608C0C@dans.knaw.nl>
 <CAOKDuOgykFYMA-ZT4kUxr_NKXBEBsxZV8s-3sAuQ3Motc5M_Fw@mail.gmail.com>
 <CALEXWq2ke081QLqbv89z=yfTuoocLw9dzQWx6-05gzSFqAD3vQ@mail.gmail.com>
 <CAOKDuOg5bA0Uf5bmQX_9ZP6qbnO=hewb7cVbjEEZEvu5KOGC5w@mail.gmail.com>
 <e275b09d-39c8-a7e2-fc11-30c00034d1e4@insa-toulouse.fr>
 <CALEXWq3hNdLZwf9v=YZNTJ1FjTJfN6xuuUeSZ-zyZq1Sp+8yKA@mail.gmail.com>
 <CAJXDcw3CJsDoLN_=_jf7Ud554Cm6DikHOhwQLWw9aUOycuuy5A@mail.gmail.com>
Message-ID: <d89880dd-5cdd-6fc8-0716-6c036507eb9c@sapo.pt>

Hello,

Watch out for operator precedence.



all.equal(0.3, 0.1*3)
#[1] TRUE


`%~~%` <- function (e1, e2)  all.equal(e1, e2)

0.3 %~~% 0.1*3
#Error in 0.3 %~~% 0.1 * 3 : argumento n?o-num?rico para operador bin?rio


0.3 %~~% (0.1*3)
#[1] TRUE


Now with isTRUE. The problem changes a bit.


isTRUE(all.equal(0.3, 0.1*3))
#[1] TRUE


`%~~%` <- function (e1, e2)  isTRUE(all.equal(e1, e2))

0.3 %~~% 0.1*3
#[1] 0

0.3 %~~% (0.1*3)
#[1] TRUE


Hope this helps,

Rui Barradas

?s 08:20 de 03/09/2018, Juan Telleria Ruiz de Aguirre escreveu:
> Maybe a new Operator could be defined for a fast and easy double
> Comparison: `~~`
> 
> `~~` <- function (e1, e2)  all.equal(e1, e2)
> 
> And document it properly.
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From tom@@@k@liber@ @ending from gm@il@com  Mon Sep  3 11:49:16 2018
From: tom@@@k@liber@ @ending from gm@il@com (Tomas Kalibera)
Date: Mon, 3 Sep 2018 11:49:16 +0200
Subject: [Rd] True length - length(unclass(x)) - without having to call
 unclass()?
In-Reply-To: <d2733c97-5bec-dc0e-cb05-812b0cd63ff5@kogentum.hu>
References: <CAFDcVCQdmLYM4gPj_zoz4KeoHGGUzJux9hLK_3vMVaeWUsrHQQ@mail.gmail.com>
 <d2733c97-5bec-dc0e-cb05-812b0cd63ff5@kogentum.hu>
Message-ID: <d7b7ef40-d6f7-2d11-9fb1-b5b2085bb702@gmail.com>

Please don't do this to get the underlying vector length (or to achieve 
anything else). Setting/deleting attributes of an R object without 
checking the reference count violates R semantics, which in turn can 
have unpredictable results on R programs (essentially undebuggable 
segfaults now or more likely later when new optimizations or features 
are added to the language). Setting attributes on objects with reference 
count (currently NAMED value) greater than 0 (in some special cases 1 is 
ok) is cheating - please see Writing R Extensions - and getting speedups 
via cheating leads to fragile, unmaintainable and buggy code. Doing so 
in packages is particularly unhelpful to the whole community - packages 
should only use the public API as documented.

Similarly, getting a physical address of an object to hack around 
whether R has copied it or not should certainly not be done in packages 
and R code should never be working with or even obtaining physical 
address of an object. This is also why one cannot obtain such address 
using base R (apart in textual form from certain diagnostic messages 
where it can indeed be useful for low-level debugging).

Tomas

On 09/02/2018 01:19 AM, D?nes T?th wrote:
> The solution below introduces a dependency on data.table, but 
> otherwise it does what you need:
>
> ---
>
> # special method for Foo objects
> length.Foo <- function(x) {
> ? length(unlist(x, recursive = TRUE, use.names = FALSE))
> }
>
> # an instance of a Foo object
> x <- structure(list(a = 1, b = list(b1 = 1, b2 = 2)), class = "Foo")
>
> # its length
> stopifnot(length(x) == 3L)
>
> # get its length as if it were a standard list
> .length <- function(x) {
> ? cls <- class(x)
> ? # setattr() does not make a copy, but modifies by reference
> ? data.table::setattr(x, "class", NULL)
> ? # get the length
> ? len <- base::length(x)
> ? # re-set original classes
> ? data.table::setattr(x, "class", cls)
> ? # return the unclassed length
> ? len
> }
>
> # to check that we do not make unwanted changes
> orig_class <- class(x)
>
> # check that the address in RAM does not change
> a1 <- data.table::address(x)
>
> # 'unclassed' length
> stopifnot(.length(x) == 2L)
>
> # check that address is the same
> stopifnot(a1 == data.table::address(x))
>
> # check against original class
> stopifnot(identical(orig_class, class(x)))
>
> ---
>
>
> On 08/24/2018 07:55 PM, Henrik Bengtsson wrote:
>> Is there a low-level function that returns the length of an object 'x'
>> - the length that for instance .subset(x) and .subset2(x) see? An
>> obvious candidate would be to use:
>>
>> .length <- function(x) length(unclass(x))
>>
>> However, I'm concerned that calling unclass(x) may trigger an
>> expensive copy internally in some cases.? Is that concern unfounded?
>>
>> Thxs,
>>
>> Henrik
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From medved7 @ending from live@ru  Mon Sep  3 05:39:53 2018
From: medved7 @ending from live@ru (Yu Lee)
Date: Mon, 3 Sep 2018 03:39:53 +0000
Subject: [Rd] Bug report: problems with saving plots on a Windows PC with 4k
 monitor - wrong picture size
Message-ID: <DB7PR10MB23169C023DE9B802E2298DE5E90C0@DB7PR10MB2316.EURPRD10.PROD.OUTLOOK.COM>

Steps to reproduce the problem:

win.metafile("myplot.wmf",height=3,width=5)
plot(1:9)
dev.off()


Details:
When I try to save plots as WMF or EMF pictures specifying small picture size, e.g.., 3x5 inches, I get a wrong size of the WMF/EMF picture. The plot itself resides in the left upper corner of the picture.
I use a 4k monitor, there are no problems with 1920x1080 monitors.

One must turn off "display scaling for higher DPI settings" for RGui to make it work correctly:

"Right-click the R shortcut on your desktop, then select Properties from the menu.
Once the Properties window is up, go to the Compatibility tab.
You will see a 'Disable display scaling on high DPI' option.
"

That works for 32-bit RGui version only, since 64-bit RGui is considered as a native application.

	[[alternative HTML version deleted]]


From tom@@@k@liber@ @ending from gm@il@com  Mon Sep  3 14:07:23 2018
From: tom@@@k@liber@ @ending from gm@il@com (Tomas Kalibera)
Date: Mon, 3 Sep 2018 14:07:23 +0200
Subject: [Rd] 
 Get Logical processor count correctly whether NUMA is enabled
 or disabled
In-Reply-To: <928CB09B31774648949A4D61FE4622AB02338369@PWSTLCEXMBX001.AD.MLP.com>
References: <928CB09B31774648949A4D61FE4622AB023343B7@PWSTLCEXMBX001.AD.MLP.com>
 <1ce7682a-4656-0366-ba0f-8c751697db7e@gmail.com>
 <928CB09B31774648949A4D61FE4622AB02335A3A@PWSTLCEXMBX001.AD.MLP.com>
 <7ea84564-bc20-ba10-fb78-74c53f65f3b4@gmail.com>
 <928CB09B31774648949A4D61FE4622AB02338369@PWSTLCEXMBX001.AD.MLP.com>
Message-ID: <79fd7131-01e2-8eb6-5f44-4b67212d43e7@gmail.com>

A summary for reference: the new detectCores() for Windows in R-devel 
seems to be working both for logical and physical cores on systems with 
 >64 logical processors? (thanks to Arun for testing!). If the feature 
is important for anyone particularly using an older version of Windows 
and/or on a system with >64 logical processors, it would be nice if you 
could test and report any possible problem.

As I mentioned earlier, in older versions of R one can as a workaround 
use "wmic" to detect the number of processors on systems with >64 
logical processors (with appropriate error handling added as needed):

# detectCores()
out <- system("wmic cpu get numberoflogicalprocessors", intern=TRUE)
sum(as.numeric(gsub("([0-9]+).*", "\\1", grep("[0-9]+[ \t]*", out, 
value=TRUE))))

#detectCores(logical=FALSE)
out <- system("wmic cpu get numberofcores", intern=TRUE)
sum(as.numeric(gsub("([0-9]+).*", "\\1", grep("[0-9]+[ \t]*", out, 
value=TRUE))))

The remaining problem with running using >64 processors on Windows 
turned out to be due to a bug in sockets communication, debugged and 
fixed in R-devel by Luke Tierney.

Tomas

On 08/29/2018 12:42 PM, Srinivasan, Arunkumar wrote:
> Dear Tomas, thank you very much. I installed r-devel r75201 and tested.
>
> The machine with 88 cores has NUMA disabled. It therefore has 2 processor groups with 64 and 24 processors each.
>
> require(parallel)
> detectCores()
> # [1] 88
>
> This is great!
>
> Then I went on to test with a simple 'foreach()' loop. I started with 64 processors (max limit of 1 processor group). I ran with a simple function of 0.5s sleep.
>
> require(snow)
> require(doSNOW)
> require(foreach)
>
> cl <- makeCluster(64L, "SOCK")
> registerDoSNOW(cl)
> system.time(foreach(i=1:64) %dopar% Sys.sleep(0.5))
> # user  system elapsed
> # 0.06    0.00    0.64
> system.time(foreach(i=1:65) %dopar% Sys.sleep(0.5))
> #    user  system elapsed
> #    0.03    0.01    1.04
> stopCluster(cl)
>
> With a cluster of 64 processors and loop running with 64 iterations, it completed in ~.5s (0.64), and with 65 iterations, it took ~1s as expected.
>   
> cl <- makeCluster(65L, "SOCK")
> registerDoSNOW(cl)
> system.time(foreach(i=1:64) %dopar% Sys.sleep(0.5))
>     user  system elapsed
>     0.03    0.02    0.61
> system.time(foreach(i=1:65) %dopar% Sys.sleep(0.5))
> # Timing stopped at: 0.08 0 293
> stopCluster(cl)
>
> However, when I increased the cluster to have 65 processors, a loop with 64 iterations seem to complete as expected, but using all 65 processors to loop over 65 iterations didn't seem to complete. I stopped it after ~5mins. The same happens with the cluster started with any number between 65 and 88. It seems to me like we are still not being able to use >64 processors all at the same time even if detectCores() returns the right count now.
>
> I'd appreciate your thoughts on this.
>
> Best,
> Arun.
>
> -----Original Message-----
> From: Tomas Kalibera <tomas.kalibera at gmail.com>
> Sent: 27 August 2018 19:43
> To: Srinivasan, Arunkumar <Arunkumar.Srinivasan at uk.mlp.com>; r-devel at r-project.org
> Subject: Re: [Rd] Get Logical processor count correctly whether NUMA is enabled or disabled
>
> Dear Arun,
>
> thank you for checking the workaround scripts.
>
> I've modified detectCores() to use GetLogicalProcessorInformationEx. It is in revision 75198 of R-devel, could you please test it on your machines? For a binary, you can wait until the R-devel snapshot build gets to at least this svn revision.
>
> Thanks for the link to the processor groups documentation. I don't have a machine to test this on, but I would hope that snow clusters (e.g.
> PSOCK) should work fine on systems with >64 logical processors as they spawn new processes (not just threads). Note that FORK clusters are not supported on Windows.
>
> Thanks
> Tomas
>
> On 08/21/2018 02:53 PM, Srinivasan, Arunkumar wrote:
>> Dear Tomas, thank you for looking into this. Here's the output:
>>
>> # number of logical processors - what detectCores() should return out
>> <- system("wmic cpu get numberoflogicalprocessors", intern=TRUE)
>> [1] "NumberOfLogicalProcessors  \r" "22                         \r" "22                         \r"
>> [4] "20                         \r" "22                         \r" "\r"
>> sum(as.numeric(gsub("([0-9]+).*", "\\1", grep("[0-9]+[ \t]*", out,
>> value=TRUE)))) # [1] 86
>>
>> [I've asked the IT team to understand why one of the values is 20 instead of 22].
>>
>> # number of cores - what detectCores(FALSE) should return out <-
>> system("wmic cpu get numberofcores", intern=TRUE)
>> [1] "NumberOfCores  \r" "22             \r" "22             \r" "20             \r" "22             \r"
>> [6] "\r"
>> sum(as.numeric(gsub("([0-9]+).*", "\\1", grep("[0-9]+[ \t]*", out,
>> value=TRUE)))) # [1] 86
>>
>> [Currently hyperthreading is disabled. So this output being identical to the previous output makes sense].
>>
>> system("wmic computersystem get numberofprocessors")
>> NumberOfProcessors
>> 4
>>
>> In addition, I'd also bring to your attention this documentation: https://docs.microsoft.com/en-us/windows/desktop/ProcThread/processor-groups on processor groups which explain how one should go about running a process ro run on multiple groups (which seems to be different to NUMA). All this seems overly complicated to allow a process to use all cores by default TBH.
>>
>> Here's a project on Github 'fio' where the issue of running a process on more than 1 processor group has come up -  https://github.com/axboe/fio/issues/527 and is addressed - https://github.com/axboe/fio/blob/c479640d6208236744f0562b1e79535eec290e2b/os/os-windows-7.h . I am not sure though if this is entirely relevant since we would be forking new processes in R instead of allowing a single process to use all cores. Apologies if this is utterly irrelevant.
>>
>> Thank you,
>> Arun.
>>
>> From: Tomas Kalibera <tomas.kalibera at gmail.com>
>> Sent: 21 August 2018 11:50
>> To: Srinivasan, Arunkumar <Arunkumar.Srinivasan at uk.mlp.com>;
>> r-devel at r-project.org
>> Subject: Re: [Rd] Get Logical processor count correctly whether NUMA
>> is enabled or disabled
>>
>> Dear Arun,
>>
>> thank you for the report. I agree with the analysis, detectCores() will only report logical processors in the NUMA group in which R is running. I don't have a system to test on, could you please check these workarounds for me on your systems?
>>
>> # number of logical processors - what detectCores() should return out
>> <- system("wmic cpu get numberoflogicalprocessors", intern=TRUE)
>> sum(as.numeric(gsub("([0-9]+).*", "\\1", grep("[0-9]+[ \t]*", out,
>> value=TRUE))))
>>
>> # number of cores - what detectCores(FALSE) should return out <-
>> system("wmic cpu get numberofcores", intern=TRUE)
>> sum(as.numeric(gsub("([0-9]+).*", "\\1", grep("[0-9]+[ \t]*", out,
>> value=TRUE))))
>>
>> # number of physical processors - as a sanity check
>>
>> system("wmic computersystem get numberofprocessors")
>>
>> Thanks,
>> Tomas
>>
>> On 08/17/2018 05:11 PM, Srinivasan, Arunkumar wrote:
>> Dear R-devel list,
>>
>> R's detectCores() function internally calls "ncpus" function to get the total number of logical processors. However, this doesnot seem to take NUMA into account on Windows machines.
>>
>> On a machine having 48 processors (24 cores) in total and windows server 2012 installed, if NUMA is enabled and has 2 nodes (node 0 and node 1 each having 24 CPUs), then R's detectCores() only detects 24 instead of the total 48. If NUMA is disabled, detectCores() returns 48.
>>
>> Similarly, on a machine with 88 cores (176 processors) and windows server 2012, detectCores() with NUMA disabled only returns the maximum value of 64. If NUMA is enabled with 4 nodes (44 processors each), then detectCores() will only return 44. This is particularly limiting since we cannot get to use all processors by enabling/disabling NUMA in this case.
>>
>> We think this is because R's ncpus.c file uses "PSYSTEM_LOGICAL_PROCESSOR_INFORMATION" (https://msdn.microsoft.com/en-us/library/windows/desktop/ms683194(v=vs.85).aspx) instead of "PSYSTEM_LOGICAL_PROCESSOR_INFORMATION_EX" (https://msdn.microsoft.com/en-us/library/windows/desktop/dd405488(v=vs.85).aspx). Specifically, quoting from the first link:
>>
>> "On systems with more than 64 logical processors, the?GetLogicalProcessorInformation?function retrieves logical processor information about processors in the?https://msdn.microsoft.com/en-us/library/windows/desktop/dd405503(v=vs.85).aspx?to which the calling thread is currently assigned. Use the?https://msdn.microsoft.com/en-us/library/windows/desktop/dd405488(v=vs.85).aspx?function to retrieve information about processors in all processor groups on the system."
>>
>> Therefore, it might be possible to get the right count of total processors even with NUMA enabled by using "GetLogicalProcessorInformationEX".  It'd be nice to know what you think.
>>
>> Thank you very much,
>> Arun.
>>
>> --
>> Arun Srinivasan
>> Analyst, Millennium Management LLC
>> 50 Berkeley Street | London, W1J 8HD
>>

	[[alternative HTML version deleted]]


From r@dford @ending from c@@toronto@edu  Mon Sep  3 15:18:11 2018
From: r@dford @ending from c@@toronto@edu (Radford Neal)
Date: Mon, 3 Sep 2018 09:18:11 -0400
Subject: [Rd] True length - length(unclass(x)) - without having to call
 unclass()?
In-Reply-To: <mailman.47618.5.1535968801.62998.r-devel@r-project.org>
References: <mailman.47618.5.1535968801.62998.r-devel@r-project.org>
Message-ID: <20180903131811.GA31865@mail.cs.toronto.edu>

Regarding the discussion of getting length(unclass(x)) without an
unclassed version of x being created...

There are already no copies done for length(unclass(x)) in pqR
(current version of 2017-06-09 at pqR-project.org, as well as the
soon-to-be-release new version).  This is part of a more general
facility for avoiding copies from unclass in other circumstances as
well - eg, unclass(a)+unclass(b).

It's implemented using pqR's internal "variant result" mechanism.
Primitives such as "length" and "+" can ask for their arguments to be
evaluated in such a way that an "unclassed" result is possibly
returned with its class attribute still there, but with a flag set
(not in the object) to indicate that it should be ignored.

The variant result mechanism is also central to many other pqR
improvements, including deferred evaluation to enable automatic use of
multiple cores, and optimizations that allow fast evaluation of things
like any(x<0), any(is.na(x)), or all(is.na(x)) without creation of
intermediate results and with early termination when the result is
determined.

It is much better to use such a general mechanism that speeds up
existing code than to implement more and more special-case functions
like anyNA or some special function to allow length(unclass(x)) to be
done quickly.

The variant result mechanism has extremely low overhead, and is not
hard to implement.

   Radford Neal


From Arunkum@r@Sriniv@@@n @ending from uk@mlp@com  Mon Sep  3 15:24:11 2018
From: Arunkum@r@Sriniv@@@n @ending from uk@mlp@com (Srinivasan, Arunkumar)
Date: Mon, 3 Sep 2018 13:24:11 +0000
Subject: [Rd] 
 Get Logical processor count correctly whether NUMA is enabled
 or disabled
In-Reply-To: <79fd7131-01e2-8eb6-5f44-4b67212d43e7@gmail.com>
References: <928CB09B31774648949A4D61FE4622AB023343B7@PWSTLCEXMBX001.AD.MLP.com>
 <1ce7682a-4656-0366-ba0f-8c751697db7e@gmail.com>
 <928CB09B31774648949A4D61FE4622AB02335A3A@PWSTLCEXMBX001.AD.MLP.com>
 <7ea84564-bc20-ba10-fb78-74c53f65f3b4@gmail.com>
 <928CB09B31774648949A4D61FE4622AB02338369@PWSTLCEXMBX001.AD.MLP.com>
 <79fd7131-01e2-8eb6-5f44-4b67212d43e7@gmail.com>
Message-ID: <928CB09B31774648949A4D61FE4622AB02339C47@PWSTLCEXMBX001.AD.MLP.com>

Tomas, Luke, thank you very much once again for patching both issues swiftly. This?ll be incredibly valuable to us once we move to 3.6.0.

From: Tomas Kalibera <tomas.kalibera at gmail.com>
Sent: 03 September 2018 13:07
To: r-devel at r-project.org
Cc: Srinivasan, Arunkumar <Arunkumar.Srinivasan at uk.mlp.com>
Subject: Re: [Rd] Get Logical processor count correctly whether NUMA is enabled or disabled

A summary for reference: the new detectCores() for Windows in R-devel seems to be working both for logical and physical cores on systems with >64 logical processors  (thanks to Arun for testing!). If the feature is important for anyone particularly using an older version of Windows and/or on a system with >64 logical processors, it would be nice if you could test and report any possible problem.

As I mentioned earlier, in older versions of R one can as a workaround use "wmic" to detect the number of processors on systems with >64 logical processors (with appropriate error handling added as needed):

# detectCores()
out <- system("wmic cpu get numberoflogicalprocessors", intern=TRUE)
sum(as.numeric(gsub("([0-9]+).*", "\\1<file://1>", grep("[0-9]+[ \t]*", out, value=TRUE))))

#detectCores(logical=FALSE)
out <- system("wmic cpu get numberofcores", intern=TRUE)
sum(as.numeric(gsub("([0-9]+).*", "\\1<file://1>", grep("[0-9]+[ \t]*", out, value=TRUE))))

The remaining problem with running using >64 processors on Windows turned out to be due to a bug in sockets communication, debugged and fixed in R-devel by Luke Tierney.

Tomas

On 08/29/2018 12:42 PM, Srinivasan, Arunkumar wrote:

Dear Tomas, thank you very much. I installed r-devel r75201 and tested.



The machine with 88 cores has NUMA disabled. It therefore has 2 processor groups with 64 and 24 processors each.



require(parallel)

detectCores()

# [1] 88



This is great!



Then I went on to test with a simple 'foreach()' loop. I started with 64 processors (max limit of 1 processor group). I ran with a simple function of 0.5s sleep.



require(snow)

require(doSNOW)

require(foreach)



cl <- makeCluster(64L, "SOCK")

registerDoSNOW(cl)

system.time(foreach(i=1:64) %dopar% Sys.sleep(0.5))

# user  system elapsed

# 0.06    0.00    0.64

system.time(foreach(i=1:65) %dopar% Sys.sleep(0.5))

#    user  system elapsed

#    0.03    0.01    1.04

stopCluster(cl)



With a cluster of 64 processors and loop running with 64 iterations, it completed in ~.5s (0.64), and with 65 iterations, it took ~1s as expected.



cl <- makeCluster(65L, "SOCK")

registerDoSNOW(cl)

system.time(foreach(i=1:64) %dopar% Sys.sleep(0.5))

   user  system elapsed

   0.03    0.02    0.61

system.time(foreach(i=1:65) %dopar% Sys.sleep(0.5))

# Timing stopped at: 0.08 0 293

stopCluster(cl)



However, when I increased the cluster to have 65 processors, a loop with 64 iterations seem to complete as expected, but using all 65 processors to loop over 65 iterations didn't seem to complete. I stopped it after ~5mins. The same happens with the cluster started with any number between 65 and 88. It seems to me like we are still not being able to use >64 processors all at the same time even if detectCores() returns the right count now.



I'd appreciate your thoughts on this.



Best,

Arun.



-----Original Message-----

From: Tomas Kalibera <tomas.kalibera at gmail.com><mailto:tomas.kalibera at gmail.com>

Sent: 27 August 2018 19:43

To: Srinivasan, Arunkumar <Arunkumar.Srinivasan at uk.mlp.com><mailto:Arunkumar.Srinivasan at uk.mlp.com>; r-devel at r-project.org<mailto:r-devel at r-project.org>

Subject: Re: [Rd] Get Logical processor count correctly whether NUMA is enabled or disabled



Dear Arun,



thank you for checking the workaround scripts.



I've modified detectCores() to use GetLogicalProcessorInformationEx. It is in revision 75198 of R-devel, could you please test it on your machines? For a binary, you can wait until the R-devel snapshot build gets to at least this svn revision.



Thanks for the link to the processor groups documentation. I don't have a machine to test this on, but I would hope that snow clusters (e.g.

PSOCK) should work fine on systems with >64 logical processors as they spawn new processes (not just threads). Note that FORK clusters are not supported on Windows.



Thanks

Tomas



On 08/21/2018 02:53 PM, Srinivasan, Arunkumar wrote:

Dear Tomas, thank you for looking into this. Here's the output:



# number of logical processors - what detectCores() should return out

<- system("wmic cpu get numberoflogicalprocessors", intern=TRUE)

[1] "NumberOfLogicalProcessors  \r" "22                         \r" "22                         \r"

[4] "20                         \r" "22                         \r" "\r"

sum(as.numeric(gsub("([0-9]+).*", "\\1<file://1>", grep("[0-9]+[ \t]*", out,

value=TRUE)))) # [1] 86



[I've asked the IT team to understand why one of the values is 20 instead of 22].



# number of cores - what detectCores(FALSE) should return out <-

system("wmic cpu get numberofcores", intern=TRUE)

[1] "NumberOfCores  \r" "22             \r" "22             \r" "20             \r" "22             \r"

[6] "\r"

sum(as.numeric(gsub("([0-9]+).*", "\\1<file://1>", grep("[0-9]+[ \t]*", out,

value=TRUE)))) # [1] 86



[Currently hyperthreading is disabled. So this output being identical to the previous output makes sense].



system("wmic computersystem get numberofprocessors")

NumberOfProcessors

4



In addition, I'd also bring to your attention this documentation: https://docs.microsoft.com/en-us/windows/desktop/ProcThread/processor-groups on processor groups which explain how one should go about running a process ro run on multiple groups (which seems to be different to NUMA). All this seems overly complicated to allow a process to use all cores by default TBH.



Here's a project on Github 'fio' where the issue of running a process on more than 1 processor group has come up -  https://github.com/axboe/fio/issues/527 and is addressed - https://github.com/axboe/fio/blob/c479640d6208236744f0562b1e79535eec290e2b/os/os-windows-7.h . I am not sure though if this is entirely relevant since we would be forking new processes in R instead of allowing a single process to use all cores. Apologies if this is utterly irrelevant.



Thank you,

Arun.



From: Tomas Kalibera <tomas.kalibera at gmail.com><mailto:tomas.kalibera at gmail.com>

Sent: 21 August 2018 11:50

To: Srinivasan, Arunkumar <Arunkumar.Srinivasan at uk.mlp.com><mailto:Arunkumar.Srinivasan at uk.mlp.com>;

r-devel at r-project.org<mailto:r-devel at r-project.org>

Subject: Re: [Rd] Get Logical processor count correctly whether NUMA

is enabled or disabled



Dear Arun,



thank you for the report. I agree with the analysis, detectCores() will only report logical processors in the NUMA group in which R is running. I don't have a system to test on, could you please check these workarounds for me on your systems?



# number of logical processors - what detectCores() should return out

<- system("wmic cpu get numberoflogicalprocessors", intern=TRUE)

sum(as.numeric(gsub("([0-9]+).*", "\\1<file://1>", grep("[0-9]+[ \t]*", out,

value=TRUE))))



# number of cores - what detectCores(FALSE) should return out <-

system("wmic cpu get numberofcores", intern=TRUE)

sum(as.numeric(gsub("([0-9]+).*", "\\1<file://1>", grep("[0-9]+[ \t]*", out,

value=TRUE))))



# number of physical processors - as a sanity check



system("wmic computersystem get numberofprocessors")



Thanks,

Tomas



On 08/17/2018 05:11 PM, Srinivasan, Arunkumar wrote:

Dear R-devel list,



R's detectCores() function internally calls "ncpus" function to get the total number of logical processors. However, this doesnot seem to take NUMA into account on Windows machines.



On a machine having 48 processors (24 cores) in total and windows server 2012 installed, if NUMA is enabled and has 2 nodes (node 0 and node 1 each having 24 CPUs), then R's detectCores() only detects 24 instead of the total 48. If NUMA is disabled, detectCores() returns 48.



Similarly, on a machine with 88 cores (176 processors) and windows server 2012, detectCores() with NUMA disabled only returns the maximum value of 64. If NUMA is enabled with 4 nodes (44 processors each), then detectCores() will only return 44. This is particularly limiting since we cannot get to use all processors by enabling/disabling NUMA in this case.



We think this is because R's ncpus.c file uses "PSYSTEM_LOGICAL_PROCESSOR_INFORMATION" (https://msdn.microsoft.com/en-us/library/windows/desktop/ms683194(v=vs.85).aspx) instead of "PSYSTEM_LOGICAL_PROCESSOR_INFORMATION_EX" (https://msdn.microsoft.com/en-us/library/windows/desktop/dd405488(v=vs.85).aspx). Specifically, quoting from the first link:



"On systems with more than 64 logical processors, the GetLogicalProcessorInformation function retrieves logical processor information about processors in the https://msdn.microsoft.com/en-us/library/windows/desktop/dd405503(v=vs.85).aspx to which the calling thread is currently assigned. Use the https://msdn.microsoft.com/en-us/library/windows/desktop/dd405488(v=vs.85).aspx function to retrieve information about processors in all processor groups on the system."



Therefore, it might be possible to get the right count of total processors even with NUMA enabled by using "GetLogicalProcessorInformationEX".  It'd be nice to know what you think.



Thank you very much,

Arun.



--

Arun Srinivasan

Analyst, Millennium Management LLC

50 Berkeley Street | London, W1J 8HD




######################################################################

The information contained in this communication is confidential and

intended only for the individual(s) named above. If you are not a named

addressee, please notify the sender immediately and delete this email

from your system and do not disclose the email or any part of it to any

person. The views expressed in this email are the views of the author

and do not necessarily represent the views of Millennium Capital Partners

LLP (MCP LLP) or any of its affiliates. Outgoing and incoming electronic

communications of MCP LLP and its affiliates, including telephone

communications, may be electronically archived and subject to review

and/or disclosure to someone other than the recipient. MCP LLP is

authorized and regulated by the Financial Conduct Authority. Millennium

Capital Partners LLP is a limited liability partnership registered in

England & Wales with number OC312897 and with its registered office at

50 Berkeley Street, London, W1J 8HD.

######################################################################

	[[alternative HTML version deleted]]


From m@echler @ending from @t@t@m@th@ethz@ch  Mon Sep  3 15:35:45 2018
From: m@echler @ending from @t@t@m@th@ethz@ch (Martin Maechler)
Date: Mon, 3 Sep 2018 15:35:45 +0200
Subject: [Rd] compairing doubles
In-Reply-To: <d89880dd-5cdd-6fc8-0716-6c036507eb9c@sapo.pt>
References: <AM0PR0502MB387404BC690F9AF421B6F5EAD6080@AM0PR0502MB3874.eurprd05.prod.outlook.com>
 <CALEXWq3wSS4uaYu33oA-KUu7gSBuqKYF0ESemwvsbmp6KktOJg@mail.gmail.com>
 <95F39593-22F4-46B6-A766-8FF60F608C0C@dans.knaw.nl>
 <CAOKDuOgykFYMA-ZT4kUxr_NKXBEBsxZV8s-3sAuQ3Motc5M_Fw@mail.gmail.com>
 <CALEXWq2ke081QLqbv89z=yfTuoocLw9dzQWx6-05gzSFqAD3vQ@mail.gmail.com>
 <CAOKDuOg5bA0Uf5bmQX_9ZP6qbnO=hewb7cVbjEEZEvu5KOGC5w@mail.gmail.com>
 <e275b09d-39c8-a7e2-fc11-30c00034d1e4@insa-toulouse.fr>
 <CALEXWq3hNdLZwf9v=YZNTJ1FjTJfN6xuuUeSZ-zyZq1Sp+8yKA@mail.gmail.com>
 <CAJXDcw3CJsDoLN_=_jf7Ud554Cm6DikHOhwQLWw9aUOycuuy5A@mail.gmail.com>
 <d89880dd-5cdd-6fc8-0716-6c036507eb9c@sapo.pt>
Message-ID: <23437.14513.702181.359473@stat.math.ethz.ch>

>>>>> Rui Barradas 
>>>>>     on Mon, 3 Sep 2018 09:58:34 +0100 writes:

    > Hello, Watch out for operator precedence.

indeed!  (but not only)

> all.equal(0.3, 0.1*3)
> #[1] TRUE
> 
> 
> `%~~%` <- function (e1, e2)  all.equal(e1, e2)
> 
> 0.3 %~~% 0.1*3
> #Error in 0.3 %~~% 0.1 * 3 : argumento n?o-num?rico para operador bin?rio
> 
> 
> 0.3 %~~% (0.1*3)
> #[1] TRUE
> 
> 
> Now with isTRUE. The problem changes a bit.
> 
> 
> isTRUE(all.equal(0.3, 0.1*3))
> #[1] TRUE
> 
> 
> `%~~%` <- function (e1, e2)  isTRUE(all.equal(e1, e2))
> 
> 0.3 %~~% 0.1*3
> #[1] 0
> 
> 0.3 %~~% (0.1*3)
> #[1] TRUE
> 

> Hope this helps,
> Rui Barradas
> 
> ?s 08:20 de 03/09/2018, Juan Telleria Ruiz de Aguirre escreveu:
> > Maybe a new Operator could be defined for a fast and easy double
> > Comparison: `~~`
> > 
> > `~~` <- function (e1, e2)  all.equal(e1, e2)
> > 
> > And document it properly.
> > 

I would still quite strongly recommend against such a
definition:

If you ask for  help(all.equal)
you do see that it is a generic with a   all.equal.numeric()
method which has several extra arguments 
(new ones even in R-devel)  the most important one being the
numerical  'tolerance'  with a default of
sqrt(.Machine$double.eps)  { == 2^-26 == 1.490116e-08  on all current platforms}

Of course there is some arbitraryness in that choice
{{ but only *some*: the default is related to finding the minimum of
   smooth function which hence is locally quadratic at a "decent"
   minimum hence sqrt(.)
}}
but I find it important sometimes to increase the equality
strictness of that tolerance.

Hiding everything behind a new operator which does not allow to
take into account that there are quite a few versions of
near-equality --- only partly) mirrored by the existence of
extra arguments of all.equal() --- only encourages simplified
thinking about the underlying subtle issues  which already too
many people don't care to know about.

(( e.g. all those people only caring for speed, but not for
   accuracy and reliability ... ))

Martin Maechler
ETH Zurich and R Core


From toth@dene@ @ending from kogentum@hu  Mon Sep  3 15:59:39 2018
From: toth@dene@ @ending from kogentum@hu (=?UTF-8?B?RMOpbmVzIFTDs3Ro?=)
Date: Mon, 3 Sep 2018 15:59:39 +0200
Subject: [Rd] True length - length(unclass(x)) - without having to call
 unclass()?
In-Reply-To: <d7b7ef40-d6f7-2d11-9fb1-b5b2085bb702@gmail.com>
References: <CAFDcVCQdmLYM4gPj_zoz4KeoHGGUzJux9hLK_3vMVaeWUsrHQQ@mail.gmail.com>
 <d2733c97-5bec-dc0e-cb05-812b0cd63ff5@kogentum.hu>
 <d7b7ef40-d6f7-2d11-9fb1-b5b2085bb702@gmail.com>
Message-ID: <2a16eea3-54b0-62e6-aee3-e000f744b022@kogentum.hu>

Hi Tomas,

On 09/03/2018 11:49 AM, Tomas Kalibera wrote:
> Please don't do this to get the underlying vector length (or to achieve 
> anything else). Setting/deleting attributes of an R object without 
> checking the reference count violates R semantics, which in turn can 
> have unpredictable results on R programs (essentially undebuggable 
> segfaults now or more likely later when new optimizations or features 
> are added to the language). Setting attributes on objects with reference 
> count (currently NAMED value) greater than 0 (in some special cases 1 is 
> ok) is cheating - please see Writing R Extensions - and getting speedups 
> via cheating leads to fragile, unmaintainable and buggy code. 

Please note that data.table::setattr is an exported function of a widely 
used package (available from CRAN), which also has a description in 
?data.table::setattr why it might be useful.

Of course one has to use set* functions from data.table with extreme 
care, but if one does it in the right way, they can help a lot. For 
example there is no real danger of using them in internal functions 
where one can control what is get passed to the function or created 
within the function (so when one knows that the refcount==0 condition is 
true).

(Notwithstanding the above, but also supporting you argumentation, it 
took me hours to debug a particular problem in one of my internal 
packages, see https://github.com/Rdatatable/data.table/issues/1281)

In the present case, an important and unanswered question is (cited from 
Henrik):
 >>> However, I'm concerned that calling unclass(x) may trigger an
 >>> expensive copy internally in some cases.  Is that concern unfounded?

If no copy is made, length(unclass(x)) beats length(setattr(..)) in all 
scenarios.


> Doing so 
> in packages is particularly unhelpful to the whole community - packages 
> should only use the public API as documented.
> 
> Similarly, getting a physical address of an object to hack around 
> whether R has copied it or not should certainly not be done in packages 
> and R code should never be working with or even obtaining physical 
> address of an object. This is also why one cannot obtain such address 
> using base R (apart in textual form from certain diagnostic messages 
> where it can indeed be useful for low-level debugging).

Getting the physical address of the object was done exclusively for 
demonstration purposes. I totally agree that is should not be used for 
the purpose you described and I have never ever done so.

Regards,
Denes

> 
> Tomas
> 
> On 09/02/2018 01:19 AM, D?nes T?th wrote:
>> The solution below introduces a dependency on data.table, but 
>> otherwise it does what you need:
>>
>> ---
>>
>> # special method for Foo objects
>> length.Foo <- function(x) {
>>   length(unlist(x, recursive = TRUE, use.names = FALSE))
>> }
>>
>> # an instance of a Foo object
>> x <- structure(list(a = 1, b = list(b1 = 1, b2 = 2)), class = "Foo")
>>
>> # its length
>> stopifnot(length(x) == 3L)
>>
>> # get its length as if it were a standard list
>> .length <- function(x) {
>>   cls <- class(x)
>>   # setattr() does not make a copy, but modifies by reference
>>   data.table::setattr(x, "class", NULL)
>>   # get the length
>>   len <- base::length(x)
>>   # re-set original classes
>>   data.table::setattr(x, "class", cls)
>>   # return the unclassed length
>>   len
>> }
>>
>> # to check that we do not make unwanted changes
>> orig_class <- class(x)
>>
>> # check that the address in RAM does not change
>> a1 <- data.table::address(x)
>>
>> # 'unclassed' length
>> stopifnot(.length(x) == 2L)
>>
>> # check that address is the same
>> stopifnot(a1 == data.table::address(x))
>>
>> # check against original class
>> stopifnot(identical(orig_class, class(x)))
>>
>> ---
>>
>>
>> On 08/24/2018 07:55 PM, Henrik Bengtsson wrote:
>>> Is there a low-level function that returns the length of an object 'x'
>>> - the length that for instance .subset(x) and .subset2(x) see? An
>>> obvious candidate would be to use:
>>>
>>> .length <- function(x) length(unclass(x))
>>>
>>> However, I'm concerned that calling unclass(x) may trigger an
>>> expensive copy internally in some cases.  Is that concern unfounded?
>>>
>>> Thxs,
>>>
>>> Henrik
>>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
> 
> 
>


From tom@@@k@liber@ @ending from gm@il@com  Mon Sep  3 16:49:06 2018
From: tom@@@k@liber@ @ending from gm@il@com (Tomas Kalibera)
Date: Mon, 3 Sep 2018 16:49:06 +0200
Subject: [Rd] True length - length(unclass(x)) - without having to call
 unclass()?
In-Reply-To: <2a16eea3-54b0-62e6-aee3-e000f744b022@kogentum.hu>
References: <CAFDcVCQdmLYM4gPj_zoz4KeoHGGUzJux9hLK_3vMVaeWUsrHQQ@mail.gmail.com>
 <d2733c97-5bec-dc0e-cb05-812b0cd63ff5@kogentum.hu>
 <d7b7ef40-d6f7-2d11-9fb1-b5b2085bb702@gmail.com>
 <2a16eea3-54b0-62e6-aee3-e000f744b022@kogentum.hu>
Message-ID: <c06266c3-ba07-218f-5c71-46c980b15d98@gmail.com>

On 09/03/2018 03:59 PM, D?nes T?th wrote:
> Hi Tomas,
>
> On 09/03/2018 11:49 AM, Tomas Kalibera wrote:
>> Please don't do this to get the underlying vector length (or to 
>> achieve anything else). Setting/deleting attributes of an R object 
>> without checking the reference count violates R semantics, which in 
>> turn can have unpredictable results on R programs (essentially 
>> undebuggable segfaults now or more likely later when new 
>> optimizations or features are added to the language). Setting 
>> attributes on objects with reference count (currently NAMED value) 
>> greater than 0 (in some special cases 1 is ok) is cheating - please 
>> see Writing R Extensions - and getting speedups via cheating leads to 
>> fragile, unmaintainable and buggy code. 
>
Hi Denes,

> Please note that data.table::setattr is an exported function of a 
> widely used package (available from CRAN), which also has a 
> description in ?data.table::setattr why it might be useful.
indeed, and not your fault, but the function is cheating and that it is 
in a widely used package, even exported from it, does not make it any 
safer. The related optimization in base R (shallow copying) mentioned in 
the documentation of data.table::setattr is on the other hand sound, it 
does not break the semantics.
> Of course one has to use set* functions from data.table with extreme 
> care, but if one does it in the right way, they can help a lot. For 
> example there is no real danger of using them in internal functions 
> where one can control what is get passed to the function or created 
> within the function (so when one knows that the refcount==0 condition 
> is true).
Extreme care is not enough as the internals can and do change (and with 
the limits given by documentation, they are likely to change soon wrt to 
NAMED/reference counting), not mentioning that they are very 
complicated. The approach of "modify in place because we know the 
reference count is 0" is particularly error prone and unnecessary. It is 
unnecessary because there is documented C API for legitimate use in 
packages to find out whether an object may be referenced/shared 
(indirectly checks the reference count). If not, it can be modified in 
place without cheating, and some packages do it. It is error prone 
because the reference count can change due to many things package 
developers cannot be expected to know (and again, these things change): 
in set* functions for example, it will never be 0 (!), these functions 
with their current API can never be implemented in current R without 
breaking the semantics.

In principle one can do similar things legitimately by wrapping objects 
in an environment, passing such environment (environments can 
legitimately be modified in place), checking the contained objects have 
reference count of 1 (not shared), and if so, modifying them in place. 
But indeed, as soon as such objects become shared, there is no way out, 
one has to copy (in the current R).

Best
Tomas

> (Notwithstanding the above, but also supporting you argumentation, it 
> took me hours to debug a particular problem in one of my internal 
> packages, see https://github.com/Rdatatable/data.table/issues/1281)
>
> In the present case, an important and unanswered question is (cited 
> from Henrik):
> >>> However, I'm concerned that calling unclass(x) may trigger an
> >>> expensive copy internally in some cases.? Is that concern unfounded?
>
> If no copy is made, length(unclass(x)) beats length(setattr(..)) in 
> all scenarios.
>
>
>> Doing so in packages is particularly unhelpful to the whole community 
>> - packages should only use the public API as documented.
>>
>> Similarly, getting a physical address of an object to hack around 
>> whether R has copied it or not should certainly not be done in 
>> packages and R code should never be working with or even obtaining 
>> physical address of an object. This is also why one cannot obtain 
>> such address using base R (apart in textual form from certain 
>> diagnostic messages where it can indeed be useful for low-level 
>> debugging).
>
> Getting the physical address of the object was done exclusively for 
> demonstration purposes. I totally agree that is should not be used for 
> the purpose you described and I have never ever done so.
>
> Regards,
> Denes
>
>>
>> Tomas
>>
>> On 09/02/2018 01:19 AM, D?nes T?th wrote:
>>> The solution below introduces a dependency on data.table, but 
>>> otherwise it does what you need:
>>>
>>> ---
>>>
>>> # special method for Foo objects
>>> length.Foo <- function(x) {
>>> ? length(unlist(x, recursive = TRUE, use.names = FALSE))
>>> }
>>>
>>> # an instance of a Foo object
>>> x <- structure(list(a = 1, b = list(b1 = 1, b2 = 2)), class = "Foo")
>>>
>>> # its length
>>> stopifnot(length(x) == 3L)
>>>
>>> # get its length as if it were a standard list
>>> .length <- function(x) {
>>> ? cls <- class(x)
>>> ? # setattr() does not make a copy, but modifies by reference
>>> ? data.table::setattr(x, "class", NULL)
>>> ? # get the length
>>> ? len <- base::length(x)
>>> ? # re-set original classes
>>> ? data.table::setattr(x, "class", cls)
>>> ? # return the unclassed length
>>> ? len
>>> }
>>>
>>> # to check that we do not make unwanted changes
>>> orig_class <- class(x)
>>>
>>> # check that the address in RAM does not change
>>> a1 <- data.table::address(x)
>>>
>>> # 'unclassed' length
>>> stopifnot(.length(x) == 2L)
>>>
>>> # check that address is the same
>>> stopifnot(a1 == data.table::address(x))
>>>
>>> # check against original class
>>> stopifnot(identical(orig_class, class(x)))
>>>
>>> ---
>>>
>>>
>>> On 08/24/2018 07:55 PM, Henrik Bengtsson wrote:
>>>> Is there a low-level function that returns the length of an object 'x'
>>>> - the length that for instance .subset(x) and .subset2(x) see? An
>>>> obvious candidate would be to use:
>>>>
>>>> .length <- function(x) length(unclass(x))
>>>>
>>>> However, I'm concerned that calling unclass(x) may trigger an
>>>> expensive copy internally in some cases.? Is that concern unfounded?
>>>>
>>>> Thxs,
>>>>
>>>> Henrik
>>>>
>>>> ______________________________________________
>>>> R-devel at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>
>>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>>
>>


From d@vid@@her@ @ending from verizon@net  Mon Sep  3 16:29:12 2018
From: d@vid@@her@ @ending from verizon@net (David Shera)
Date: Mon, 3 Sep 2018 14:29:12 +0000 (UTC)
Subject: [Rd] ubuntu software updater clash with with cloud.r-project
References: <58748859.306172.1535984952524.ref@mail.yahoo.com>
Message-ID: <58748859.306172.1535984952524@mail.yahoo.com>

This seems the most appropriate place to report this.

I just updated my ubuntu to 18.04.? And installed R by adding the line to /etc/apt/source.list:?? deb ... cloud.r-project ... bionic-cran35/
R installed just fine.
However, my ubuntu software update would not finish correctly any more.? Failed to access ... check internet connection?? (internet connection was just fine.)

Commented out the line in sources.list and software update runs just fine now.

So if I want to use apt to get packages, I'll have to uncomment the line, get the packages, and then comment it out again?? Seems like a bug, but not part of any R package.


Thanks.
-David



	[[alternative HTML version deleted]]


From gor@n@bro@trom @ending from umu@@e  Tue Sep  4 09:01:49 2018
From: gor@n@bro@trom @ending from umu@@e (=?UTF-8?Q?G=c3=b6ran_Brostr=c3=b6m?=)
Date: Tue, 4 Sep 2018 09:01:49 +0200
Subject: [Rd] ubuntu software updater clash with with cloud.r-project
In-Reply-To: <58748859.306172.1535984952524@mail.yahoo.com>
References: <58748859.306172.1535984952524.ref@mail.yahoo.com>
 <58748859.306172.1535984952524@mail.yahoo.com>
Message-ID: <10fe8e00-1ed6-060d-0496-f35024b9e4b1@umu.se>



On 2018-09-03 16:29, David Shera wrote:
> This seems the most appropriate place to report this.

Not to me: You should try r-sig-debian.

G?ran

> 
> I just updated my ubuntu to 18.04.? And installed R by adding the line to /etc/apt/source.list:?? deb ... cloud.r-project ... bionic-cran35/
> R installed just fine.
> However, my ubuntu software update would not finish correctly any more.? Failed to access ... check internet connection?? (internet connection was just fine.)
> 
> Commented out the line in sources.list and software update runs just fine now.
> 
> So if I want to use apt to get packages, I'll have to uncomment the line, get the packages, and then comment it out again?? Seems like a bug, but not part of any R package.
> 
> 
> Thanks.
> -David
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From edd @ending from debi@n@org  Tue Sep  4 17:11:23 2018
From: edd @ending from debi@n@org (Dirk Eddelbuettel)
Date: Tue, 4 Sep 2018 10:11:23 -0500
Subject: [Rd] How do I prevent macOS from attempting to build my package?
Message-ID: <23438.41115.736885.421246@rob.eddelbuettel.com>


[ Initially posted in r-package-devel, now reposting here by request from
  Uwe; my apologies for the cross-post. ]

Package status reports come in three different severities: NOTE, WARNING, and
ERROR. Motivated by Brodie's nice (dependency-free) accessor function for
per-maintainer status [1],  I have looked into reducing the number of ERRORs.

I hit a road block. Several of my packages depend on external libraries that
must be present. I test for these in configure, but their absence is still an
ERROR.  This makes the situation on macOS a little delicate. Simon, who is
doing, and always done, a metric ton of work around R and OS X / maxOS is the
only one who could change this but I cannot realistically ask him to keep a
number of (in some cases more difficult or esoteric) libraries afloat. And
some of these have now been missing on his platform for several years.

And in one case (RcppAPT, requiring libapt-dev) the build is even
imppossible.  Now, the Fedora maintainer knows this and has the build
blacklisted.

Hence:

  R> source("checkCRAN.R")                            
                 Package ERROR WARN NOTE OK
  [...]
  23             RcppAPT     2            4
  [...]

No failures from Fedora.  But two from macOS which I can never ever get rid
off (unless I do silly code acrobatics by #ifdef'ing all real code away).

So here is my question:  Can we we please refine

  OS_type: unix

a little more, and/or maybe allow other blacklists in the package upload?

Thoughts or comments most welcome.

Thanks,  Dirk



[1] https://gist.github.com/brodieG/e60c94d4036f45018530ea504258bcf3#file-cran-check-r

-- 
http://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From hp@ge@ @ending from fredhutch@org  Tue Sep  4 20:45:50 2018
From: hp@ge@ @ending from fredhutch@org (=?UTF-8?B?SGVydsOpIFBhZ8Oocw==?=)
Date: Tue, 4 Sep 2018 11:45:50 -0700
Subject: [Rd] Argument 'dim' misspelled in error message
In-Reply-To: <23434.35104.846770.367530@hornik.net>
References: <777dc8b8-368a-dcdb-0aaf-e484475ed90f@fredhutch.org>
 <23434.35104.846770.367530@hornik.net>
Message-ID: <52ff598b-58e5-9ab3-8e51-8ada64b2614b@fredhutch.org>

Thanks!

On 09/01/2018 05:42 AM, Kurt Hornik wrote:
>>>>>> Herv? Pag?s writes:
> 
> Thanks: fixed in the trunk with c75223.
> 
> Best
> -k
> 
>> Hi,
>> The following error message misspells the name of
>> the 'dim' argument:
> 
>>> array(integer(0), dim=integer(0))
>>     Error in array(integer(0), dim = integer(0)) :
>>       'dims' cannot be of length 0
> 
>> The name of the argument is 'dim' not 'dims':
> 
>>> args(array)
>>     function (data = NA, dim = length(data), dimnames = NULL)
>>     NULL
> 
>> Cheers,
>> H.
> 
>> -- 
>> Herv? Pag?s
> 
>> Program in Computational Biology
>> Division of Public Health Sciences
>> Fred Hutchinson Cancer Research Center
>> 1100 Fairview Ave. N, M1-B514
>> P.O. Box 19024
>> Seattle, WA 98109-1024
> 
>> E-mail: hpages at fredhutch.org
>> Phone:  (206) 667-5791
>> Fax:    (206) 667-1319
> 
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Ddevel&d=DwIFAw&c=eRAMFD45gAfqt84VtBcfhQ&r=BK7q3XeAvimeWdGbWY_wJYbW0WYiZvSXAJJKaaPhzWA&m=SzMRc3M_TJEtaAqp-2nqiquGAjCH605Ocf2-jkPG_1E&s=1PeobGV2Ld7gOtIS5coLotgg3VLknDQyCXVjO08DbX4&e=

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fredhutch.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From p@ul @ending from @t@t@@uckl@nd@@c@nz  Tue Sep  4 23:55:16 2018
From: p@ul @ending from @t@t@@uckl@nd@@c@nz (Paul Murrell)
Date: Wed, 5 Sep 2018 09:55:16 +1200
Subject: [Rd] [FORGED] Re:  plotmath degree symbol
In-Reply-To: <CAGAA5beFhuiO4QDm0_NGagti1w7_R4SuyKKTQS2oa5ydiznaVg@mail.gmail.com>
References: <d83487b0-3b9b-43ab-bfd7-6526ee63012a@uni-muenster.de>
 <CAGAA5beFhuiO4QDm0_NGagti1w7_R4SuyKKTQS2oa5ydiznaVg@mail.gmail.com>
Message-ID: <feca79b4-79d8-41d2-f24d-146ceae155aa@stat.auckland.ac.nz>

Hi

Thanks for that, but I still cannot confirm on ...

sudo docker run -v $(pwd):/home/work/ -w /home/work --rm -ti 
rocker/r-ver:3.5.1

Could you please read the comments within the "Cairo Fonts" section of 
the ?X11 help page, in case that offers some explanation.

Paul


On 29/08/18 02:15, Martin M?ller Skarbiniks Pedersen wrote:
> On Fri, 24 Aug 2018 at 19:53, Edzer Pebesma
> <edzer.pebesma at uni-muenster.de> wrote:
>>
>> In plotmath expressions, R's degree symbol, e.g. shown by
>>
>> plot(1, main = parse(text = "1*degree*C"))
>>
>> has sunk to halfway the text line, instead of touching its top. In older
>> R versions this looked much better.
> 
> I can confirm this problem.
> 
> R version 3.5.1 (2018-07-02)
> Platform: x86_64-pc-linux-gnu (64-bit)
> Running under: Ubuntu 18.04.1 LTS
> 
> Matrix products: default
> BLAS: /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.7.1
> LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.7.1
> 
> locale:
>   [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
>   [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
>   [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8
>   [7] LC_PAPER=en_US.UTF-8       LC_NAME=C
>   [9] LC_ADDRESS=C               LC_TELEPHONE=C
> [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C
> 
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
> 
> loaded via a namespace (and not attached):
> [1] compiler_3.5.1
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 

-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/


From edzer@pebe@m@ @ending from uni-muen@ter@de  Wed Sep  5 09:46:25 2018
From: edzer@pebe@m@ @ending from uni-muen@ter@de (Edzer Pebesma)
Date: Wed, 5 Sep 2018 09:46:25 +0200
Subject: [Rd] [FORGED] Re:  plotmath degree symbol
In-Reply-To: <feca79b4-79d8-41d2-f24d-146ceae155aa@stat.auckland.ac.nz>
References: <d83487b0-3b9b-43ab-bfd7-6526ee63012a@uni-muenster.de>
 <CAGAA5beFhuiO4QDm0_NGagti1w7_R4SuyKKTQS2oa5ydiznaVg@mail.gmail.com>
 <feca79b4-79d8-41d2-f24d-146ceae155aa@stat.auckland.ac.nz>
Message-ID: <c32eaaa3-ad15-c81d-ce3e-5b54adbb30d9@uni-muenster.de>

Thanks, Paul -- setting the ~/.fonts.conf file the way ?X11 describes it
under the section you pointed to resolved the problem for me, on ubuntu.

On 09/04/2018 11:55 PM, Paul Murrell wrote:
> Hi
> 
> Thanks for that, but I still cannot confirm on ...
> 
> sudo docker run -v $(pwd):/home/work/ -w /home/work --rm -ti
> rocker/r-ver:3.5.1
> 
> Could you please read the comments within the "Cairo Fonts" section of
> the ?X11 help page, in case that offers some explanation.
> 
> Paul
> 
> 
> On 29/08/18 02:15, Martin M?ller Skarbiniks Pedersen wrote:
>> On Fri, 24 Aug 2018 at 19:53, Edzer Pebesma
>> <edzer.pebesma at uni-muenster.de> wrote:
>>>
>>> In plotmath expressions, R's degree symbol, e.g. shown by
>>>
>>> plot(1, main = parse(text = "1*degree*C"))
>>>
>>> has sunk to halfway the text line, instead of touching its top. In older
>>> R versions this looked much better.
>>
>> I can confirm this problem.
>>
>> R version 3.5.1 (2018-07-02)
>> Platform: x86_64-pc-linux-gnu (64-bit)
>> Running under: Ubuntu 18.04.1 LTS
>>
>> Matrix products: default
>> BLAS: /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.7.1
>> LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.7.1
>>
>> locale:
>> ? [1] LC_CTYPE=en_US.UTF-8?????? LC_NUMERIC=C
>> ? [3] LC_TIME=en_US.UTF-8??????? LC_COLLATE=en_US.UTF-8
>> ? [5] LC_MONETARY=en_US.UTF-8??? LC_MESSAGES=en_US.UTF-8
>> ? [7] LC_PAPER=en_US.UTF-8?????? LC_NAME=C
>> ? [9] LC_ADDRESS=C?????????????? LC_TELEPHONE=C
>> [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C
>>
>> attached base packages:
>> [1] stats???? graphics? grDevices utils???? datasets? methods?? base
>>
>> loaded via a namespace (and not attached):
>> [1] compiler_3.5.1
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
> 

-- 
Edzer Pebesma
Institute for Geoinformatics
Heisenbergstrasse 2, 48151 Muenster, Germany
Phone: +49 251 8333081

From tom@@@k@liber@ @ending from gm@il@com  Wed Sep  5 10:09:04 2018
From: tom@@@k@liber@ @ending from gm@il@com (Tomas Kalibera)
Date: Wed, 5 Sep 2018 10:09:04 +0200
Subject: [Rd] True length - length(unclass(x)) - without having to call
 unclass()?
In-Reply-To: <CAFDcVCQdmLYM4gPj_zoz4KeoHGGUzJux9hLK_3vMVaeWUsrHQQ@mail.gmail.com>
References: <CAFDcVCQdmLYM4gPj_zoz4KeoHGGUzJux9hLK_3vMVaeWUsrHQQ@mail.gmail.com>
Message-ID: <d16444c2-a703-d822-bb59-f94c92ecb260@gmail.com>

On 08/24/2018 07:55 PM, Henrik Bengtsson wrote:
> Is there a low-level function that returns the length of an object 'x'
> - the length that for instance .subset(x) and .subset2(x) see? An
> obvious candidate would be to use:
>
> .length <- function(x) length(unclass(x))
>
> However, I'm concerned that calling unclass(x) may trigger an
> expensive copy internally in some cases.  Is that concern unfounded?
Unclass() will always copy when "x" is really a variable, because the 
value in "x" will be referenced; whether it is prohibitively expensive 
or not depends only on the workload - if "x" is a very long list and 
this functions is called often then it could, but at least to me this 
sounds unlikely. Unless you have a strong reason to believe it is the 
case I would just use length(unclass(x)).

If the copying is really a problem, I would think about why the 
underlying vector length is needed at R level - whether you really need 
to know the length without actually having the unclassed vector anyway 
for something else, so whether you are not paying for the copy anyway. 
Or, from the other end, if you need to do more without copying, and it 
is possible without breaking the value semantics, then you might need to 
switch to C anyway and for a bigger piece of code.

If it were still just .length() you needed and it were performance 
critical, you could just switch to C and call Rf_length. That does not 
violate the semantics, just indeed it is not elegant as you are 
switching to C.

If you stick to R and can live with the overhead of length(unclass(x)) 
then there is a chance the overhead will decrease as R is optimized 
internally. This is possible in principle when the runtime knows that 
the unclassed vector is only needed to compute something that does not 
modify the vector. The current R cannot optimize this out, but it should 
be possible with ALTREP at some point (and as Radford mentioned pqR does 
it differently). Even with such internal optimizations indeed it is 
often necessary to make guesses about realistic workloads, so if you 
have a realistic workload where say length(unclass(x)) is critical, you 
are more than welcome to donate it as benchmark.

Obviously, if you use a C version calling Rf_length, after such R 
optimization your code would be unnecessarily non-elegant, but would 
still work and probably without overhead, because R can't do much less 
than Rf_length. In more complicated cases though hand-optimized C code 
to implement say 2 operations in sequence could be slower than what 
better optimizing runtime could do by joining the effect of possibly 
more operations, which is in principle another danger of switching from 
R to C. But as far as the semantics is followed, there is no other danger.

The temptation should be small anyway in this case when Rf_length() 
would be the simplest, but as I made it more than clear in the previous 
email, one should never violate the value semantics by temporarily 
modifying the object (temporarily removing the class attribute or 
temporarily remove the object bit). Violating semantics causes bugs, if 
not with the present then with future versions of R (where version may 
be an svn revision). A concrete recent example: modifying objects in 
place in violation of the semantics caused a lot of bugs with 
introduction of unification of constants in the byte-code compiler.

Best
Tomas

>
> Thxs,
>
> Henrik
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From iuc@r @ending from fedor@project@org  Wed Sep  5 11:18:42 2018
From: iuc@r @ending from fedor@project@org (=?UTF-8?Q?I=C3=B1aki_Ucar?=)
Date: Wed, 5 Sep 2018 11:18:42 +0200
Subject: [Rd] True length - length(unclass(x)) - without having to call
 unclass()?
In-Reply-To: <d16444c2-a703-d822-bb59-f94c92ecb260@gmail.com>
References: <CAFDcVCQdmLYM4gPj_zoz4KeoHGGUzJux9hLK_3vMVaeWUsrHQQ@mail.gmail.com>
 <d16444c2-a703-d822-bb59-f94c92ecb260@gmail.com>
Message-ID: <CALEXWq15ASNPhOOd0PiF0F2ky09j-QNk6=qWTfoK3ZEqTBAjYg@mail.gmail.com>

The bottomline here is that one can always call a base method,
inexpensively and without modifying the object, in, let's say,
*formal* OOP languages. In R, this is not possible in general. It
would be possible if there was always a foo.default, but primitives
use internal dispatch.

I was wondering whether it would be possible to provide a super(x, n)
function which simply causes the dispatching system to avoid "n"
classes in the hierarchy, so that:

> x <- structure(list(), class=c("foo", "bar"))
> length(super(x, 0)) # looks for a length.foo
> length(super(x, 1)) # looks for a length.bar
> length(super(x, 2)) # calls the default
> length(super(x, Inf)) # calls the default

I?aki

El mi?., 5 sept. 2018 a las 10:09, Tomas Kalibera
(<tomas.kalibera at gmail.com>) escribi?:
>
> On 08/24/2018 07:55 PM, Henrik Bengtsson wrote:
> > Is there a low-level function that returns the length of an object 'x'
> > - the length that for instance .subset(x) and .subset2(x) see? An
> > obvious candidate would be to use:
> >
> > .length <- function(x) length(unclass(x))
> >
> > However, I'm concerned that calling unclass(x) may trigger an
> > expensive copy internally in some cases.  Is that concern unfounded?
> Unclass() will always copy when "x" is really a variable, because the
> value in "x" will be referenced; whether it is prohibitively expensive
> or not depends only on the workload - if "x" is a very long list and
> this functions is called often then it could, but at least to me this
> sounds unlikely. Unless you have a strong reason to believe it is the
> case I would just use length(unclass(x)).
>
> If the copying is really a problem, I would think about why the
> underlying vector length is needed at R level - whether you really need
> to know the length without actually having the unclassed vector anyway
> for something else, so whether you are not paying for the copy anyway.
> Or, from the other end, if you need to do more without copying, and it
> is possible without breaking the value semantics, then you might need to
> switch to C anyway and for a bigger piece of code.
>
> If it were still just .length() you needed and it were performance
> critical, you could just switch to C and call Rf_length. That does not
> violate the semantics, just indeed it is not elegant as you are
> switching to C.
>
> If you stick to R and can live with the overhead of length(unclass(x))
> then there is a chance the overhead will decrease as R is optimized
> internally. This is possible in principle when the runtime knows that
> the unclassed vector is only needed to compute something that does not
> modify the vector. The current R cannot optimize this out, but it should
> be possible with ALTREP at some point (and as Radford mentioned pqR does
> it differently). Even with such internal optimizations indeed it is
> often necessary to make guesses about realistic workloads, so if you
> have a realistic workload where say length(unclass(x)) is critical, you
> are more than welcome to donate it as benchmark.
>
> Obviously, if you use a C version calling Rf_length, after such R
> optimization your code would be unnecessarily non-elegant, but would
> still work and probably without overhead, because R can't do much less
> than Rf_length. In more complicated cases though hand-optimized C code
> to implement say 2 operations in sequence could be slower than what
> better optimizing runtime could do by joining the effect of possibly
> more operations, which is in principle another danger of switching from
> R to C. But as far as the semantics is followed, there is no other danger.
>
> The temptation should be small anyway in this case when Rf_length()
> would be the simplest, but as I made it more than clear in the previous
> email, one should never violate the value semantics by temporarily
> modifying the object (temporarily removing the class attribute or
> temporarily remove the object bit). Violating semantics causes bugs, if
> not with the present then with future versions of R (where version may
> be an svn revision). A concrete recent example: modifying objects in
> place in violation of the semantics caused a lot of bugs with
> introduction of unification of constants in the byte-code compiler.
>
> Best
> Tomas
>
> >
> > Thxs,
> >
> > Henrik
> >
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel



-- 
I?aki Ucar


From kevinu@hey @ending from gm@il@com  Wed Sep  5 18:30:34 2018
From: kevinu@hey @ending from gm@il@com (Kevin Ushey)
Date: Wed, 5 Sep 2018 09:30:34 -0700
Subject: [Rd] True length - length(unclass(x)) - without having to call
 unclass()?
In-Reply-To: <CALEXWq15ASNPhOOd0PiF0F2ky09j-QNk6=qWTfoK3ZEqTBAjYg@mail.gmail.com>
References: <CAFDcVCQdmLYM4gPj_zoz4KeoHGGUzJux9hLK_3vMVaeWUsrHQQ@mail.gmail.com>
 <d16444c2-a703-d822-bb59-f94c92ecb260@gmail.com>
 <CALEXWq15ASNPhOOd0PiF0F2ky09j-QNk6=qWTfoK3ZEqTBAjYg@mail.gmail.com>
Message-ID: <CAJXgQP3QkqEEWoCsw7cyHB3-XNL3h-z7-v20tODjf9bpVium2g@mail.gmail.com>

More generally, I think one of the issues is that R is not yet able to
decrement a reference count (or mark a 'shared' data object as
'unshared' after it knows only one binding to it exists). This means
passing variables to R closures will mark that object as shared:

    x <- list()
    .Internal(inspect(x))  # NAM(1)
    identity(x)
    .Internal(inspect(x))  # NAM(3)

I think for this reason users often resort to 'hacks' that involve
directly setting attributes on the object, since they 'know' only one
reference to a particular object exists. I'm not sure if this really
is 'safe', though -- likely not given potential future optimizations
to R, as Tomas has alluded to.

I think true reference counting has been implemented in the R sources,
but the switch has not yet been flipped to enable that by default.
Hopefully having that will make cases like the above work as expected?

Thanks,
Kevin

On Wed, Sep 5, 2018 at 2:19 AM I?aki Ucar <iucar at fedoraproject.org> wrote:
>
> The bottomline here is that one can always call a base method,
> inexpensively and without modifying the object, in, let's say,
> *formal* OOP languages. In R, this is not possible in general. It
> would be possible if there was always a foo.default, but primitives
> use internal dispatch.
>
> I was wondering whether it would be possible to provide a super(x, n)
> function which simply causes the dispatching system to avoid "n"
> classes in the hierarchy, so that:
>
> > x <- structure(list(), class=c("foo", "bar"))
> > length(super(x, 0)) # looks for a length.foo
> > length(super(x, 1)) # looks for a length.bar
> > length(super(x, 2)) # calls the default
> > length(super(x, Inf)) # calls the default
>
> I?aki
>


From m@cqueen1 @ending from llnl@gov  Wed Sep  5 21:13:58 2018
From: m@cqueen1 @ending from llnl@gov (MacQueen, Don)
Date: Wed, 5 Sep 2018 19:13:58 +0000
Subject: [Rd] svg ignores cex.axis in R3.5.1 on macOS
In-Reply-To: <c506e0f9-bf78-cab1-3d38-ef8b0b1dd850@prodsyse.com>
References: <ac8f9908-f064-d723-61a8-7e8ca085dd43@prodsyse.com>
 <c506e0f9-bf78-cab1-3d38-ef8b0b1dd850@prodsyse.com>
Message-ID: <02422C33-8D62-41AA-9064-EE4FADD33F2D@llnl.gov>

Seems ok on my system. Axis label size changes when cex.axis does.

## tested in the middle of another long session, so many additional packages are attached, including some personal packages not available elsewhere

> sessionInfo()
R version 3.5.1 (2018-07-02)
Platform: x86_64-apple-darwin15.6.0 (64-bit)
Running under: macOS High Sierra 10.13.6

Matrix products: default
BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib

locale:
[1] C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
 [1] survival_2.42-3 ROracle_1.3-1   DBI_1.0.0       bookdown_0.7    knitr_1.20      rmarkdown_1.10  wdr_3.2         taurus_3.2-4    xlsx_0.6.1     
[10] rmacq_1.3-8    

loaded via a namespace (and not attached):
 [1] Rcpp_0.12.17    magrittr_1.5    splines_3.5.1   lattice_0.20-35 highr_0.7       stringr_1.3.1   tools_3.5.1     grid_3.5.1      xfun_0.3       
[10] tinytex_0.6     htmltools_0.3.6 yaml_2.1.19     rprojroot_1.3-2 digest_0.6.15   zip_1.0.0       Matrix_1.2-14   rJava_0.9-10    xlsxjars_0.6.1 
[19] evaluate_0.10.1 openxlsx_4.1.0  stringi_1.2.3   compiler_3.5.1  backports_1.1.2

--
Don MacQueen
Lawrence Livermore National Laboratory
7000 East Ave., L-627
Livermore, CA 94550
925-423-1062
Lab cell 925-724-7509
 
 

?On 8/31/18, 1:02 PM, "R-devel on behalf of Spencer Graves" <r-devel-bounces at r-project.org on behalf of spencer.graves at prodsyse.com> wrote:

    
    
    On 2018-08-31 14:21, Spencer Graves wrote:
    > Plots produced using svg in R 3.5.1 under macOS 10.13.6 ignores 
    > cex.axis=2.  Consider the following:
    >
    >
    > > plot(1:2, cex.axis=2)
    > > svg('svg_ignores_cex.axis.svg')
    > > plot(1:2, cex.axis=2)
    > > dev.off()
    > > sessionInfo()
    > R version 3.5.1 (2018-07-02)
    > Platform: x86_64-apple-darwin15.6.0 (64-bit)
    > Running under: macOS High Sierra 10.13.6
    >
    > Matrix products: default
    > BLAS: 
    > /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
    > LAPACK: 
    > /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
    >
    > locale:
    > [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
    >
    > attached base packages:
    > [1] stats     graphics  grDevices utils     datasets  methods base
    >
    > loaded via a namespace (and not attached):
    > [1] compiler_3.5.1
    >
    >
    >       ** The axis labels are appropriately expanded with the first 
    > "plot(1:2, cex.axis=2)".  However, when I wrote that to an svg file 
    > and opened it in other applications (GIMP and Safari), the cex.axis 
    > request was ignored.  This also occurred inside RStudio on my Mac. It 
    > worked properly using R 3.2.1 under Windows 7.
    
    
    I just confirmed that when I created a file like this under Windows 7 
    and brought it back to my Mac, it displayed fine.  I have not tried this 
    with the current version of R under Windows 7 nor an old version of R on 
    my Mac.  Thanks.  Spencer
    >
    >
    >       Thanks,
    >       Spencer Graves
    >
    > ______________________________________________
    > R-devel at r-project.org mailing list
    > https://stat.ethz.ch/mailman/listinfo/r-devel
    >
    
    ______________________________________________
    R-devel at r-project.org mailing list
    https://stat.ethz.ch/mailman/listinfo/r-devel
    


From p@ul @ending from @t@t@@uckl@nd@@c@nz  Wed Sep  5 22:34:21 2018
From: p@ul @ending from @t@t@@uckl@nd@@c@nz (Paul Murrell)
Date: Thu, 6 Sep 2018 08:34:21 +1200
Subject: [Rd] [FORGED] Re:  plotmath degree symbol
In-Reply-To: <c32eaaa3-ad15-c81d-ce3e-5b54adbb30d9@uni-muenster.de>
References: <d83487b0-3b9b-43ab-bfd7-6526ee63012a@uni-muenster.de>
 <CAGAA5beFhuiO4QDm0_NGagti1w7_R4SuyKKTQS2oa5ydiznaVg@mail.gmail.com>
 <feca79b4-79d8-41d2-f24d-146ceae155aa@stat.auckland.ac.nz>
 <c32eaaa3-ad15-c81d-ce3e-5b54adbb30d9@uni-muenster.de>
Message-ID: <d22cdf87-4232-8e88-1181-5537f4e76b1f@stat.auckland.ac.nz>


Awesome.  Thanks for confirming.

Paul

On 05/09/18 19:46, Edzer Pebesma wrote:
> Thanks, Paul -- setting the ~/.fonts.conf file the way ?X11 describes it
> under the section you pointed to resolved the problem for me, on ubuntu.
> 
> On 09/04/2018 11:55 PM, Paul Murrell wrote:
>> Hi
>>
>> Thanks for that, but I still cannot confirm on ...
>>
>> sudo docker run -v $(pwd):/home/work/ -w /home/work --rm -ti
>> rocker/r-ver:3.5.1
>>
>> Could you please read the comments within the "Cairo Fonts" section of
>> the ?X11 help page, in case that offers some explanation.
>>
>> Paul
>>
>>
>> On 29/08/18 02:15, Martin M?ller Skarbiniks Pedersen wrote:
>>> On Fri, 24 Aug 2018 at 19:53, Edzer Pebesma
>>> <edzer.pebesma at uni-muenster.de> wrote:
>>>>
>>>> In plotmath expressions, R's degree symbol, e.g. shown by
>>>>
>>>> plot(1, main = parse(text = "1*degree*C"))
>>>>
>>>> has sunk to halfway the text line, instead of touching its top. In older
>>>> R versions this looked much better.
>>>
>>> I can confirm this problem.
>>>
>>> R version 3.5.1 (2018-07-02)
>>> Platform: x86_64-pc-linux-gnu (64-bit)
>>> Running under: Ubuntu 18.04.1 LTS
>>>
>>> Matrix products: default
>>> BLAS: /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.7.1
>>> LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.7.1
>>>
>>> locale:
>>>  ? [1] LC_CTYPE=en_US.UTF-8?????? LC_NUMERIC=C
>>>  ? [3] LC_TIME=en_US.UTF-8??????? LC_COLLATE=en_US.UTF-8
>>>  ? [5] LC_MONETARY=en_US.UTF-8??? LC_MESSAGES=en_US.UTF-8
>>>  ? [7] LC_PAPER=en_US.UTF-8?????? LC_NAME=C
>>>  ? [9] LC_ADDRESS=C?????????????? LC_TELEPHONE=C
>>> [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C
>>>
>>> attached base packages:
>>> [1] stats???? graphics? grDevices utils???? datasets? methods?? base
>>>
>>> loaded via a namespace (and not attached):
>>> [1] compiler_3.5.1
>>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>
>>
> 

-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/


From luke-tier@ey m@ili@g off uiow@@edu  Wed Sep  5 23:38:38 2018
From: luke-tier@ey m@ili@g off uiow@@edu (luke-tier@ey m@ili@g off uiow@@edu)
Date: Wed, 5 Sep 2018 16:38:38 -0500 (CDT)
Subject: [Rd] True length - length(unclass(x)) - without having to call
 unclass()?
In-Reply-To: <CAJXgQP3QkqEEWoCsw7cyHB3-XNL3h-z7-v20tODjf9bpVium2g@mail.gmail.com>
References: <CAFDcVCQdmLYM4gPj_zoz4KeoHGGUzJux9hLK_3vMVaeWUsrHQQ@mail.gmail.com>
 <d16444c2-a703-d822-bb59-f94c92ecb260@gmail.com>
 <CALEXWq15ASNPhOOd0PiF0F2ky09j-QNk6=qWTfoK3ZEqTBAjYg@mail.gmail.com>
 <CAJXgQP3QkqEEWoCsw7cyHB3-XNL3h-z7-v20tODjf9bpVium2g@mail.gmail.com>
Message-ID: <alpine.DEB.2.21.1809051634260.2971@luke-Latitude-7480>

On Wed, 5 Sep 2018, Kevin Ushey wrote:

> More generally, I think one of the issues is that R is not yet able to
> decrement a reference count (or mark a 'shared' data object as
> 'unshared' after it knows only one binding to it exists). This means
> passing variables to R closures will mark that object as shared:
>
>    x <- list()
>    .Internal(inspect(x))  # NAM(1)
>    identity(x)
>    .Internal(inspect(x))  # NAM(3)
>
> I think for this reason users often resort to 'hacks' that involve
> directly setting attributes on the object, since they 'know' only one
> reference to a particular object exists. I'm not sure if this really
> is 'safe', though -- likely not given potential future optimizations
> to R, as Tomas has alluded to.
>
> I think true reference counting has been implemented in the R sources,
> but the switch has not yet been flipped to enable that by default.
> Hopefully having that will make cases like the above work as expected?

Current R-devel built with reference counting by setting

CFLAGS="-O3 -g -Wall -pedantic -DSWITCH_TO_REFCNT"

gives


x <- list()
.Internal(inspect(x))
## @55ad788e3b28 19 VECSXP g0c0 [REF(1)] (len=0, tl=0)
identity(x)
## list()
.Internal(inspect(x))
## @55ad788e3b28 19 VECSXP g0c0 [REF(1)] (len=0, tl=0)

I'm moderately hopeful we'll be able to switch to this for 3.6.0 but
depends on finding enough time to sort out some loose ends.

Best,

luke

>
> Thanks,
> Kevin
>
> On Wed, Sep 5, 2018 at 2:19 AM I?aki Ucar <iucar at fedoraproject.org> wrote:
>>
>> The bottomline here is that one can always call a base method,
>> inexpensively and without modifying the object, in, let's say,
>> *formal* OOP languages. In R, this is not possible in general. It
>> would be possible if there was always a foo.default, but primitives
>> use internal dispatch.
>>
>> I was wondering whether it would be possible to provide a super(x, n)
>> function which simply causes the dispatching system to avoid "n"
>> classes in the hierarchy, so that:
>>
>>> x <- structure(list(), class=c("foo", "bar"))
>>> length(super(x, 0)) # looks for a length.foo
>>> length(super(x, 1)) # looks for a length.bar
>>> length(super(x, 2)) # calls the default
>>> length(super(x, Inf)) # calls the default
>>
>> I?aki
>>
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

-- 
Luke Tierney
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu

From m@ttjdowle @ending from gm@il@com  Thu Sep  6 04:25:33 2018
From: m@ttjdowle @ending from gm@il@com (Matt Dowle)
Date: Wed, 5 Sep 2018 19:25:33 -0700
Subject: [Rd] config.status: error: cannot find input file: `po/Makefile.in'
Message-ID: <CAOuOy3egenDW_NLspBPQrkNQTtAg5PjoXHBsNPEBk9vQdCqwow@mail.gmail.com>

Hi,

Does anyone else see the following or is it just me?  It usually works
fine. I checked latest R-devel commits and couldn't see anything very
recently changed or fixed w.r.t. po/ or Makefile.

wget https://stat.ethz.ch/R/daily/R-devel.tar.gz
tar xvf R-devel.tar.gz
cd R-devel
./configure --without-recommended-packages
...
config.status: error: cannot find input file: `po/Makefile.in'

$ lsb_release -a
Description:    Ubuntu 18.04.1 LTS
Release:        18.04
Codename:       bionic

Best, Matt

	[[alternative HTML version deleted]]


From m@echler @ending from @t@t@m@th@ethz@ch  Thu Sep  6 09:29:43 2018
From: m@echler @ending from @t@t@m@th@ethz@ch (Martin Maechler)
Date: Thu, 6 Sep 2018 09:29:43 +0200
Subject: [Rd] 
 config.status: error: cannot find input file: `po/Makefile.in'
In-Reply-To: <CAOuOy3egenDW_NLspBPQrkNQTtAg5PjoXHBsNPEBk9vQdCqwow@mail.gmail.com>
References: <CAOuOy3egenDW_NLspBPQrkNQTtAg5PjoXHBsNPEBk9vQdCqwow@mail.gmail.com>
Message-ID: <23440.55143.190728.696388@stat.math.ethz.ch>

>>>>> Matt Dowle 
>>>>>     on Wed, 5 Sep 2018 19:25:33 -0700 writes:

    > Hi,
    > Does anyone else see the following or is it just me?  It usually works
    > fine. I checked latest R-devel commits and couldn't see anything very
    > recently changed or fixed w.r.t. po/ or Makefile.

    > wget https://stat.ethz.ch/R/daily/R-devel.tar.gz
    > tar xvf R-devel.tar.gz
    > cd R-devel
    > ./configure --without-recommended-packages
    > ...
    > config.status: error: cannot find input file: `po/Makefile.in'

Hmm, I can easily reproduce also with last night's tar ball.

The tar ball is missing 90% of the source files.
If you use replace
     R-devel.tar.gz
by   R-devel_2018-09-03.tar.gz

it works... but of course you want the daily latest version, and
I'll have a look on what causes the problem, and will fix it.

Best,
Martin


From pd@lgd @ending from gm@il@com  Thu Sep  6 11:47:13 2018
From: pd@lgd @ending from gm@il@com (peter dalgaard)
Date: Thu, 6 Sep 2018 11:47:13 +0200
Subject: [Rd] svg ignores cex.axis in R3.5.1 on macOS
In-Reply-To: <02422C33-8D62-41AA-9064-EE4FADD33F2D@llnl.gov>
References: <ac8f9908-f064-d723-61a8-7e8ca085dd43@prodsyse.com>
 <c506e0f9-bf78-cab1-3d38-ef8b0b1dd850@prodsyse.com>
 <02422C33-8D62-41AA-9064-EE4FADD33F2D@llnl.gov>
Message-ID: <E0DF64F8-672E-4634-AB2D-7C2EE2EF83A1@gmail.com>

I think this needs to be taken off the bug repository and continued here. By now it seems pretty clear that this is not an R bug, but a local problem on Spencer's machine, likely connected to font configurations.

I poked around a bit on the three Macs that I can access, and found that fc-match does different things, including throwing warnings, hanging and even crashing my old MB Air...

One possible reason is that it can apparently be installed in multiple locations, for reasons lost in the mists of time:

Peters-iMac:BUILD-dist pd$ ls -l /opt/local/bin/fc-*
-rwxr-xr-x  1 root  wheel  44072 Apr  5  2014 /opt/local/bin/fc-cache
-rwxr-xr-x  1 root  wheel  43444 Apr  5  2014 /opt/local/bin/fc-cat
-rwxr-xr-x  1 root  wheel  34480 Apr  5  2014 /opt/local/bin/fc-list
-rwxr-xr-x  1 root  wheel  34928 Apr  5  2014 /opt/local/bin/fc-match
-rwxr-xr-x  1 root  wheel  34480 Apr  5  2014 /opt/local/bin/fc-pattern
-rwxr-xr-x  1 root  wheel  34008 Apr  5  2014 /opt/local/bin/fc-query
-rwxr-xr-x  1 root  wheel  34448 Apr  5  2014 /opt/local/bin/fc-scan
-rwxr-xr-x  1 root  wheel  38780 Apr  5  2014 /opt/local/bin/fc-validate
Peters-iMac:BUILD-dist pd$ ls -l /opt/X11/bin/fc-*
-rwxr-xr-x  1 root  wheel  58128 Oct 26  2016 /opt/X11/bin/fc-cache
-rwxr-xr-x  1 root  wheel  57600 Oct 26  2016 /opt/X11/bin/fc-cat
-rwxr-xr-x  1 root  wheel  48384 Oct 26  2016 /opt/X11/bin/fc-list
-rwxr-xr-x  1 root  wheel  48992 Oct 26  2016 /opt/X11/bin/fc-match
-rwxr-xr-x  1 root  wheel  44256 Oct 26  2016 /opt/X11/bin/fc-pattern
-rwxr-xr-x  1 root  wheel  44000 Oct 26  2016 /opt/X11/bin/fc-query
-rwxr-xr-x  1 root  wheel  44288 Oct 26  2016 /opt/X11/bin/fc-scan
-rwxr-xr-x  1 root  wheel  48608 Oct 26  2016 /opt/X11/bin/fc-validate
Peters-iMac:BUILD-dist pd$ ls -l /usr/local/bin/fc-*
-rwxr-xr-x@ 1 root  wheel  1463900 Oct 21  2008 /usr/local/bin/fc-cache
-rwxr-xr-x@ 1 root  wheel  1459780 Oct 21  2008 /usr/local/bin/fc-cat
-rwxr-xr-x@ 1 root  wheel  1455628 Oct 21  2008 /usr/local/bin/fc-list
-rwxr-xr-x@ 1 root  wheel  1476560 Oct 21  2008 /usr/local/bin/fc-match

Notice that these are all different, no links. I guess that the ones you want are in /opt/X11, presumably installed by XQuartz.

So, going out on a limb, I have two ideas:

(A) Rebuild the font cache with

/opt/X11/bin/fc-cache -vf

(B) Check that XQuartz is up to date (possibly reinstall it, even if it is) 

-pd

> On 5 Sep 2018, at 21:13 , MacQueen, Don via R-devel <r-devel at r-project.org> wrote:
> 
> Seems ok on my system. Axis label size changes when cex.axis does.
> 
> ## tested in the middle of another long session, so many additional packages are attached, including some personal packages not available elsewhere
> 
>> sessionInfo()
> R version 3.5.1 (2018-07-02)
> Platform: x86_64-apple-darwin15.6.0 (64-bit)
> Running under: macOS High Sierra 10.13.6
> 
> Matrix products: default
> BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
> LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
> 
> locale:
> [1] C
> 
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base     
> 
> other attached packages:
> [1] survival_2.42-3 ROracle_1.3-1   DBI_1.0.0       bookdown_0.7    knitr_1.20      rmarkdown_1.10  wdr_3.2         taurus_3.2-4    xlsx_0.6.1     
> [10] rmacq_1.3-8    
> 
> loaded via a namespace (and not attached):
> [1] Rcpp_0.12.17    magrittr_1.5    splines_3.5.1   lattice_0.20-35 highr_0.7       stringr_1.3.1   tools_3.5.1     grid_3.5.1      xfun_0.3       
> [10] tinytex_0.6     htmltools_0.3.6 yaml_2.1.19     rprojroot_1.3-2 digest_0.6.15   zip_1.0.0       Matrix_1.2-14   rJava_0.9-10    xlsxjars_0.6.1 
> [19] evaluate_0.10.1 openxlsx_4.1.0  stringi_1.2.3   compiler_3.5.1  backports_1.1.2
> 
> --
> Don MacQueen
> Lawrence Livermore National Laboratory
> 7000 East Ave., L-627
> Livermore, CA 94550
> 925-423-1062
> Lab cell 925-724-7509
> 
> 
> 
> ?On 8/31/18, 1:02 PM, "R-devel on behalf of Spencer Graves" <r-devel-bounces at r-project.org on behalf of spencer.graves at prodsyse.com> wrote:
> 
> 
> 
>    On 2018-08-31 14:21, Spencer Graves wrote:
>> Plots produced using svg in R 3.5.1 under macOS 10.13.6 ignores 
>> cex.axis=2.  Consider the following:
>> 
>> 
>>> plot(1:2, cex.axis=2)
>>> svg('svg_ignores_cex.axis.svg')
>>> plot(1:2, cex.axis=2)
>>> dev.off()
>>> sessionInfo()
>> R version 3.5.1 (2018-07-02)
>> Platform: x86_64-apple-darwin15.6.0 (64-bit)
>> Running under: macOS High Sierra 10.13.6
>> 
>> Matrix products: default
>> BLAS: 
>> /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
>> LAPACK: 
>> /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
>> 
>> locale:
>> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
>> 
>> attached base packages:
>> [1] stats     graphics  grDevices utils     datasets  methods base
>> 
>> loaded via a namespace (and not attached):
>> [1] compiler_3.5.1
>> 
>> 
>>      ** The axis labels are appropriately expanded with the first 
>> "plot(1:2, cex.axis=2)".  However, when I wrote that to an svg file 
>> and opened it in other applications (GIMP and Safari), the cex.axis 
>> request was ignored.  This also occurred inside RStudio on my Mac. It 
>> worked properly using R 3.2.1 under Windows 7.
> 
> 
>    I just confirmed that when I created a file like this under Windows 7 
>    and brought it back to my Mac, it displayed fine.  I have not tried this 
>    with the current version of R under Windows 7 nor an old version of R on 
>    my Mac.  Thanks.  Spencer
>> 
>> 
>>      Thanks,
>>      Spencer Graves
>> 
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>> 
> 
>    ______________________________________________
>    R-devel at r-project.org mailing list
>    https://stat.ethz.ch/mailman/listinfo/r-devel
> 
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From ripley @ending from @t@t@@ox@@c@uk  Thu Sep  6 12:17:18 2018
From: ripley @ending from @t@t@@ox@@c@uk (Prof Brian Ripley)
Date: Thu, 6 Sep 2018 11:17:18 +0100
Subject: [Rd] svg ignores cex.axis in R3.5.1 on macOS
In-Reply-To: <E0DF64F8-672E-4634-AB2D-7C2EE2EF83A1@gmail.com>
References: <ac8f9908-f064-d723-61a8-7e8ca085dd43@prodsyse.com>
 <c506e0f9-bf78-cab1-3d38-ef8b0b1dd850@prodsyse.com>
 <02422C33-8D62-41AA-9064-EE4FADD33F2D@llnl.gov>
 <E0DF64F8-672E-4634-AB2D-7C2EE2EF83A1@gmail.com>
Message-ID: <b3b2fc46-f072-bf1e-73f3-1d0f1fa1b9b9@stats.ox.ac.uk>

On 06/09/2018 10:47, peter dalgaard wrote:
> I think this needs to be taken off the bug repository and continued here. By now it seems pretty clear that this is not an R bug, but a local problem on Spencer's machine, likely connected to font configurations.

Or even on R-sig-Mac.

> I poked around a bit on the three Macs that I can access, and found that fc-match does different things, including throwing warnings, hanging and even crashing my old MB Air...
> 
> One possible reason is that it can apparently be installed in multiple locations, for reasons lost in the mists of time:
> 
> Peters-iMac:BUILD-dist pd$ ls -l /opt/local/bin/fc-*
> -rwxr-xr-x  1 root  wheel  44072 Apr  5  2014 /opt/local/bin/fc-cache
> -rwxr-xr-x  1 root  wheel  43444 Apr  5  2014 /opt/local/bin/fc-cat
> -rwxr-xr-x  1 root  wheel  34480 Apr  5  2014 /opt/local/bin/fc-list
> -rwxr-xr-x  1 root  wheel  34928 Apr  5  2014 /opt/local/bin/fc-match
> -rwxr-xr-x  1 root  wheel  34480 Apr  5  2014 /opt/local/bin/fc-pattern
> -rwxr-xr-x  1 root  wheel  34008 Apr  5  2014 /opt/local/bin/fc-query
> -rwxr-xr-x  1 root  wheel  34448 Apr  5  2014 /opt/local/bin/fc-scan
> -rwxr-xr-x  1 root  wheel  38780 Apr  5  2014 /opt/local/bin/fc-validate
> Peters-iMac:BUILD-dist pd$ ls -l /opt/X11/bin/fc-*
> -rwxr-xr-x  1 root  wheel  58128 Oct 26  2016 /opt/X11/bin/fc-cache
> -rwxr-xr-x  1 root  wheel  57600 Oct 26  2016 /opt/X11/bin/fc-cat
> -rwxr-xr-x  1 root  wheel  48384 Oct 26  2016 /opt/X11/bin/fc-list
> -rwxr-xr-x  1 root  wheel  48992 Oct 26  2016 /opt/X11/bin/fc-match
> -rwxr-xr-x  1 root  wheel  44256 Oct 26  2016 /opt/X11/bin/fc-pattern
> -rwxr-xr-x  1 root  wheel  44000 Oct 26  2016 /opt/X11/bin/fc-query
> -rwxr-xr-x  1 root  wheel  44288 Oct 26  2016 /opt/X11/bin/fc-scan
> -rwxr-xr-x  1 root  wheel  48608 Oct 26  2016 /opt/X11/bin/fc-validate
> Peters-iMac:BUILD-dist pd$ ls -l /usr/local/bin/fc-*
> -rwxr-xr-x@ 1 root  wheel  1463900 Oct 21  2008 /usr/local/bin/fc-cache
> -rwxr-xr-x@ 1 root  wheel  1459780 Oct 21  2008 /usr/local/bin/fc-cat
> -rwxr-xr-x@ 1 root  wheel  1455628 Oct 21  2008 /usr/local/bin/fc-list
> -rwxr-xr-x@ 1 root  wheel  1476560 Oct 21  2008 /usr/local/bin/fc-match
> 
> Notice that these are all different, no links. I guess that the ones you want are in /opt/X11, presumably installed by XQuartz.

Yes, for the device compiled into the CRAN binary R package.  (Other 
builds may differ.)  On that, the cairo-based devices such as svg() are 
linked to (current versions on my machine)

	/usr/lib/libz.1.dylib (compatibility version 1.0.0, current version 1.2.5)
	/opt/X11/lib/libcairo.2.dylib (compatibility version 11403.0.0, current 
version 11403.6.0)
	/opt/X11/lib/libpixman-1.0.dylib (compatibility version 35.0.0, current 
version 35.0.0)
	/opt/X11/lib/libfontconfig.1.dylib (compatibility version 11.0.0, 
current version 11.2.0)
...


> So, going out on a limb, I have two ideas:
> 
> (A) Rebuild the font cache with
> 
> /opt/X11/bin/fc-cache -vf
> 
> (B) Check that XQuartz is up to date (possibly reinstall it, even if it is)

(B) is expected to do (A).  My advice was going to be to reinstall 
xquartz: macOS updates can partially break it.

> 
> -pd
> 
>> On 5 Sep 2018, at 21:13 , MacQueen, Don via R-devel <r-devel at r-project.org> wrote:
>>
>> Seems ok on my system. Axis label size changes when cex.axis does.
>>
>> ## tested in the middle of another long session, so many additional packages are attached, including some personal packages not available elsewhere
>>
>>> sessionInfo()
>> R version 3.5.1 (2018-07-02)
>> Platform: x86_64-apple-darwin15.6.0 (64-bit)
>> Running under: macOS High Sierra 10.13.6
>>
>> Matrix products: default
>> BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
>> LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
>>
>> locale:
>> [1] C
>>
>> attached base packages:
>> [1] stats     graphics  grDevices utils     datasets  methods   base
>>
>> other attached packages:
>> [1] survival_2.42-3 ROracle_1.3-1   DBI_1.0.0       bookdown_0.7    knitr_1.20      rmarkdown_1.10  wdr_3.2         taurus_3.2-4    xlsx_0.6.1
>> [10] rmacq_1.3-8
>>
>> loaded via a namespace (and not attached):
>> [1] Rcpp_0.12.17    magrittr_1.5    splines_3.5.1   lattice_0.20-35 highr_0.7       stringr_1.3.1   tools_3.5.1     grid_3.5.1      xfun_0.3
>> [10] tinytex_0.6     htmltools_0.3.6 yaml_2.1.19     rprojroot_1.3-2 digest_0.6.15   zip_1.0.0       Matrix_1.2-14   rJava_0.9-10    xlsxjars_0.6.1
>> [19] evaluate_0.10.1 openxlsx_4.1.0  stringi_1.2.3   compiler_3.5.1  backports_1.1.2
>>
>> --
>> Don MacQueen
>> Lawrence Livermore National Laboratory
>> 7000 East Ave., L-627
>> Livermore, CA 94550
>> 925-423-1062
>> Lab cell 925-724-7509
>>
>>
>>
>> ?On 8/31/18, 1:02 PM, "R-devel on behalf of Spencer Graves" <r-devel-bounces at r-project.org on behalf of spencer.graves at prodsyse.com> wrote:
>>
>>
>>
>>     On 2018-08-31 14:21, Spencer Graves wrote:
>>> Plots produced using svg in R 3.5.1 under macOS 10.13.6 ignores
>>> cex.axis=2.  Consider the following:
>>>
>>>
>>>> plot(1:2, cex.axis=2)
>>>> svg('svg_ignores_cex.axis.svg')
>>>> plot(1:2, cex.axis=2)
>>>> dev.off()
>>>> sessionInfo()
>>> R version 3.5.1 (2018-07-02)
>>> Platform: x86_64-apple-darwin15.6.0 (64-bit)
>>> Running under: macOS High Sierra 10.13.6
>>>
>>> Matrix products: default
>>> BLAS:
>>> /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
>>> LAPACK:
>>> /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
>>>
>>> locale:
>>> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
>>>
>>> attached base packages:
>>> [1] stats     graphics  grDevices utils     datasets  methods base
>>>
>>> loaded via a namespace (and not attached):
>>> [1] compiler_3.5.1
>>>
>>>
>>>       ** The axis labels are appropriately expanded with the first
>>> "plot(1:2, cex.axis=2)".  However, when I wrote that to an svg file
>>> and opened it in other applications (GIMP and Safari), the cex.axis
>>> request was ignored.  This also occurred inside RStudio on my Mac. It
>>> worked properly using R 3.2.1 under Windows 7.
>>
>>
>>     I just confirmed that when I created a file like this under Windows 7
>>     and brought it back to my Mac, it displayed fine.  I have not tried this
>>     with the current version of R under Windows 7 nor an old version of R on
>>     my Mac.  Thanks.  Spencer
>>>
>>>
>>>       Thanks,
>>>       Spencer Graves

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Emeritus Professor of Applied Statistics, University of Oxford


From @pencer@gr@ve@ @ending from prod@y@e@com  Thu Sep  6 15:29:51 2018
From: @pencer@gr@ve@ @ending from prod@y@e@com (Spencer Graves)
Date: Thu, 6 Sep 2018 08:29:51 -0500
Subject: [Rd] svg ignores cex.axis in R3.5.1 on macOS
In-Reply-To: <b3b2fc46-f072-bf1e-73f3-1d0f1fa1b9b9@stats.ox.ac.uk>
References: <ac8f9908-f064-d723-61a8-7e8ca085dd43@prodsyse.com>
 <c506e0f9-bf78-cab1-3d38-ef8b0b1dd850@prodsyse.com>
 <02422C33-8D62-41AA-9064-EE4FADD33F2D@llnl.gov>
 <E0DF64F8-672E-4634-AB2D-7C2EE2EF83A1@gmail.com>
 <b3b2fc46-f072-bf1e-73f3-1d0f1fa1b9b9@stats.ox.ac.uk>
Message-ID: <7ddf8356-811b-da56-342a-9afc44cbf0ea@prodsyse.com>



On 2018-09-06 05:17, Prof Brian Ripley wrote:
> On 06/09/2018 10:47, peter dalgaard wrote:
>> I think this needs to be taken off the bug repository and continued 
>> here. By now it seems pretty clear that this is not an R bug, but a 
>> local problem on Spencer's machine, likely connected to font 
>> configurations.
>
> Or even on R-sig-Mac.
>
>> I poked around a bit on the three Macs that I can access, and found 
>> that fc-match does different things, including throwing warnings, 
>> hanging and even crashing my old MB Air...
>>
>> One possible reason is that it can apparently be installed in 
>> multiple locations, for reasons lost in the mists of time:
>>
>> Peters-iMac:BUILD-dist pd$ ls -l /opt/local/bin/fc-*
>> -rwxr-xr-x? 1 root? wheel? 44072 Apr? 5? 2014 /opt/local/bin/fc-cache
>> -rwxr-xr-x? 1 root? wheel? 43444 Apr? 5? 2014 /opt/local/bin/fc-cat
>> -rwxr-xr-x? 1 root? wheel? 34480 Apr? 5? 2014 /opt/local/bin/fc-list
>> -rwxr-xr-x? 1 root? wheel? 34928 Apr? 5? 2014 /opt/local/bin/fc-match
>> -rwxr-xr-x? 1 root? wheel? 34480 Apr? 5? 2014 /opt/local/bin/fc-pattern
>> -rwxr-xr-x? 1 root? wheel? 34008 Apr? 5? 2014 /opt/local/bin/fc-query
>> -rwxr-xr-x? 1 root? wheel? 34448 Apr? 5? 2014 /opt/local/bin/fc-scan
>> -rwxr-xr-x? 1 root? wheel? 38780 Apr? 5? 2014 /opt/local/bin/fc-validate
>> Peters-iMac:BUILD-dist pd$ ls -l /opt/X11/bin/fc-*
>> -rwxr-xr-x? 1 root? wheel? 58128 Oct 26? 2016 /opt/X11/bin/fc-cache
>> -rwxr-xr-x? 1 root? wheel? 57600 Oct 26? 2016 /opt/X11/bin/fc-cat
>> -rwxr-xr-x? 1 root? wheel? 48384 Oct 26? 2016 /opt/X11/bin/fc-list
>> -rwxr-xr-x? 1 root? wheel? 48992 Oct 26? 2016 /opt/X11/bin/fc-match
>> -rwxr-xr-x? 1 root? wheel? 44256 Oct 26? 2016 /opt/X11/bin/fc-pattern
>> -rwxr-xr-x? 1 root? wheel? 44000 Oct 26? 2016 /opt/X11/bin/fc-query
>> -rwxr-xr-x? 1 root? wheel? 44288 Oct 26? 2016 /opt/X11/bin/fc-scan
>> -rwxr-xr-x? 1 root? wheel? 48608 Oct 26? 2016 /opt/X11/bin/fc-validate
>> Peters-iMac:BUILD-dist pd$ ls -l /usr/local/bin/fc-*
>> -rwxr-xr-x@ 1 root? wheel? 1463900 Oct 21? 2008 /usr/local/bin/fc-cache
>> -rwxr-xr-x@ 1 root? wheel? 1459780 Oct 21? 2008 /usr/local/bin/fc-cat
>> -rwxr-xr-x@ 1 root? wheel? 1455628 Oct 21? 2008 /usr/local/bin/fc-list
>> -rwxr-xr-x@ 1 root? wheel? 1476560 Oct 21? 2008 /usr/local/bin/fc-match
>>
>> Notice that these are all different, no links. I guess that the ones 
>> you want are in /opt/X11, presumably installed by XQuartz.
>
> Yes, for the device compiled into the CRAN binary R package. (Other 
> builds may differ.)? On that, the cairo-based devices such as svg() 
> are linked to (current versions on my machine)
>
> ????/usr/lib/libz.1.dylib (compatibility version 1.0.0, current 
> version 1.2.5)
> ????/opt/X11/lib/libcairo.2.dylib (compatibility version 11403.0.0, 
> current version 11403.6.0)
> ????/opt/X11/lib/libpixman-1.0.dylib (compatibility version 35.0.0, 
> current version 35.0.0)
> ????/opt/X11/lib/libfontconfig.1.dylib (compatibility version 11.0.0, 
> current version 11.2.0)
> ...
>
>
>> So, going out on a limb, I have two ideas:
>>
>> (A) Rebuild the font cache with
>>
>> /opt/X11/bin/fc-cache -vf
>>
>> (B) Check that XQuartz is up to date (possibly reinstall it, even if 
>> it is)
>
> (B) is expected to do (A).? My advice was going to be to reinstall 
> xquartz: macOS updates can partially break it.


 ????? I was going to try that, but I rebooted (again), and now it's 
working.


 ????? I rebooted before I first reported the problem, and I've rebooted 
a couple of times since without success.? This time was different, I 
don't know why.? Before I rebooted this time, I saw "XQuartz" on my 
taskbar / "Dock", switched to it, then clicked on the XQuartz icon in 
upper left and selected "About X11".? This said "XQuartz 2.7.11 
(xorg-server 1.18.4)."? Then I rebooted and restarted RStudio then tried 
svg again with cex.axis=2, and it worked.? Moreover, a web search took 
me to "https://xquartz.en.softonic.com/mac", which says that the current 
XQuartz for Mac is 2.6.1.? Since I now have 2.7.11 and it's working, I 
think I should leave it alone.


 ????? If anyone wants me to try something further to add to this 
record, I will.? Otherwise, I'll wait:? If the problem recurs, I'll try 
reinstalling XQuartz again, as Professors Dalgaard and Ripley 
suggested.? And if I have another problem with svg and need further 
help, I will consider R-sig-Mac.


 ????? Thanks also to Paul Murrell, who provided several responses to my 
(non)-bug report.


 ????? Spencer Graves
>
>>
>> -pd
>>
>>> On 5 Sep 2018, at 21:13 , MacQueen, Don via R-devel 
>>> <r-devel at r-project.org> wrote:
>>>
>>> Seems ok on my system. Axis label size changes when cex.axis does.
>>>
>>> ## tested in the middle of another long session, so many additional 
>>> packages are attached, including some personal packages not 
>>> available elsewhere
>>>
>>>> sessionInfo()
>>> R version 3.5.1 (2018-07-02)
>>> Platform: x86_64-apple-darwin15.6.0 (64-bit)
>>> Running under: macOS High Sierra 10.13.6
>>>
>>> Matrix products: default
>>> BLAS: 
>>> /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
>>> LAPACK: 
>>> /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
>>>
>>> locale:
>>> [1] C
>>>
>>> attached base packages:
>>> [1] stats???? graphics? grDevices utils???? datasets methods?? base
>>>
>>> other attached packages:
>>> [1] survival_2.42-3 ROracle_1.3-1?? DBI_1.0.0 bookdown_0.7??? 
>>> knitr_1.20????? rmarkdown_1.10 wdr_3.2???????? taurus_3.2-4??? 
>>> xlsx_0.6.1
>>> [10] rmacq_1.3-8
>>>
>>> loaded via a namespace (and not attached):
>>> [1] Rcpp_0.12.17??? magrittr_1.5??? splines_3.5.1 lattice_0.20-35 
>>> highr_0.7?????? stringr_1.3.1 tools_3.5.1???? grid_3.5.1????? xfun_0.3
>>> [10] tinytex_0.6???? htmltools_0.3.6 yaml_2.1.19 rprojroot_1.3-2 
>>> digest_0.6.15?? zip_1.0.0 Matrix_1.2-14?? rJava_0.9-10??? 
>>> xlsxjars_0.6.1
>>> [19] evaluate_0.10.1 openxlsx_4.1.0? stringi_1.2.3 compiler_3.5.1? 
>>> backports_1.1.2
>>>
>>> -- 
>>> Don MacQueen
>>> Lawrence Livermore National Laboratory
>>> 7000 East Ave., L-627
>>> Livermore, CA 94550
>>> 925-423-1062
>>> Lab cell 925-724-7509
>>>
>>>
>>>
>>> ?On 8/31/18, 1:02 PM, "R-devel on behalf of Spencer Graves" 
>>> <r-devel-bounces at r-project.org on behalf of 
>>> spencer.graves at prodsyse.com> wrote:
>>>
>>>
>>>
>>> ??? On 2018-08-31 14:21, Spencer Graves wrote:
>>>> Plots produced using svg in R 3.5.1 under macOS 10.13.6 ignores
>>>> cex.axis=2.? Consider the following:
>>>>
>>>>
>>>>> plot(1:2, cex.axis=2)
>>>>> svg('svg_ignores_cex.axis.svg')
>>>>> plot(1:2, cex.axis=2)
>>>>> dev.off()
>>>>> sessionInfo()
>>>> R version 3.5.1 (2018-07-02)
>>>> Platform: x86_64-apple-darwin15.6.0 (64-bit)
>>>> Running under: macOS High Sierra 10.13.6
>>>>
>>>> Matrix products: default
>>>> BLAS:
>>>> /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib 
>>>>
>>>> LAPACK:
>>>> /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib 
>>>>
>>>>
>>>> locale:
>>>> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
>>>>
>>>> attached base packages:
>>>> [1] stats???? graphics? grDevices utils???? datasets methods base
>>>>
>>>> loaded via a namespace (and not attached):
>>>> [1] compiler_3.5.1
>>>>
>>>>
>>>> ????? ** The axis labels are appropriately expanded with the first
>>>> "plot(1:2, cex.axis=2)".? However, when I wrote that to an svg file
>>>> and opened it in other applications (GIMP and Safari), the cex.axis
>>>> request was ignored.? This also occurred inside RStudio on my Mac. It
>>>> worked properly using R 3.2.1 under Windows 7.
>>>
>>>
>>> ??? I just confirmed that when I created a file like this under 
>>> Windows 7
>>> ??? and brought it back to my Mac, it displayed fine.? I have not 
>>> tried this
>>> ??? with the current version of R under Windows 7 nor an old version 
>>> of R on
>>> ??? my Mac.? Thanks.? Spencer
>>>>
>>>>
>>>> ????? Thanks,
>>>> ????? Spencer Graves
>


	[[alternative HTML version deleted]]


From m@ttjdowle @ending from gm@il@com  Fri Sep  7 02:39:48 2018
From: m@ttjdowle @ending from gm@il@com (Matt Dowle)
Date: Thu, 6 Sep 2018 17:39:48 -0700
Subject: [Rd] 
 config.status: error: cannot find input file: `po/Makefile.in'
In-Reply-To: <23440.55143.190728.696388@stat.math.ethz.ch>
References: <CAOuOy3egenDW_NLspBPQrkNQTtAg5PjoXHBsNPEBk9vQdCqwow@mail.gmail.com>
 <23440.55143.190728.696388@stat.math.ethz.ch>
Message-ID: <CAOuOy3cOSOD7+uw9eYY6Cs+Jsoe2-aVO-YJ7BgnR9sb-9e55iA@mail.gmail.com>

Martin - many thanks, it's working now.

	[[alternative HTML version deleted]]


From @uh@rto_@nggono @ending from y@hoo@com  Fri Sep  7 23:34:42 2018
From: @uh@rto_@nggono @ending from y@hoo@com (Suharto Anggono Suharto Anggono)
Date: Fri, 7 Sep 2018 21:34:42 +0000 (UTC)
Subject: [Rd] Proposal: more accurate seq(from, to, length=n)
References: <1837118162.1355212.1536356082390.ref@mail.yahoo.com>
Message-ID: <1837118162.1355212.1536356082390@mail.yahoo.com>

In R,
seq(0, 1, 0.1)
gives the same result as
(0:10)*0.1.
It is not the same as
c(0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1) ,
as 0.1 is not represented exactly. I am fine with it.

In R,
seq(0, 1, length=11)
gives the same result as
seq(0, 1, 0.1).
However, for 
seq(0, 1, length=11),
it is more accurate to return
c(0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1) .
It can be obtained by
(0:10)/10.

When 'from', 'to', and 'length.out' are specified and length.out > 2, I propose for function 'seq.default' in R to use something like
from + ((0:(length.out - 1))/(length.out - 1)) * (to - from)
instead of something like
from + (0:(length.out - 1)) * ((to - from)/(length.out - 1)) .


From becker@g@be @ending from gene@com  Sat Sep  8 00:38:21 2018
From: becker@g@be @ending from gene@com (Gabe Becker)
Date: Fri, 7 Sep 2018 15:38:21 -0700
Subject: [Rd] Proposal: more accurate seq(from, to, length=n)
In-Reply-To: <1837118162.1355212.1536356082390@mail.yahoo.com>
References: <1837118162.1355212.1536356082390.ref@mail.yahoo.com>
 <1837118162.1355212.1536356082390@mail.yahoo.com>
Message-ID: <CAMFmJs=Hzow94X_j59bjDfn=x8GAVakroZqZq2vERFe9up+sng@mail.gmail.com>

Suharto,

My 2c inline.

On Fri, Sep 7, 2018 at 2:34 PM, Suharto Anggono Suharto Anggono via R-devel
<r-devel at r-project.org> wrote:

> In R,
> seq(0, 1, 0.1)
> gives the same result as
> (0:10)*0.1.
> It is not the same as
> c(0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1) ,
> as 0.1 is not represented exactly. I am fine with it.
>
> In R,
> seq(0, 1, length=11)
> gives the same result as
> seq(0, 1, 0.1).
> However, for
> seq(0, 1, length=11),
> it is more accurate to return
> c(0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1) .

It can be obtained by
> (0:10)/10.
>
> When 'from', 'to', and 'length.out' are specified and length.out > 2, I
> propose for function 'seq.default' in R to use something like
> from + ((0:(length.out - 1))/(length.out - 1)) * (to - from)
> instead of something like
> from + (0:(length.out - 1)) * ((to - from)/(length.out - 1)) .
>

In your example case under 3.50 on my system these two expressions give
results which return TRUE from all.equal, which is the accepted way of
comparing non-integer numerics in R for "sameness".

> from = 0

> to = 1

> length.out = 11

> all.equal(from + ((0:(length.out - 1))/(length.out - 1)) * (to - from),
from + (0:(length.out - 1)) * ((to - from)/(length.out - 1)))

[1] TRUE

Given that I'm wondering what the benefit you're looking for here is that
would outweigh the very large set of existing code whose behavior would
technically change  under this change. Then again, it wouldn't change with
respect to the accepted all.equal test, so I guess you could argue that
either there's "no change" or the change is ok?

I'd still like to know what practical problem you're trying to solve
though. if you're looking for the ability to use == to compare non integer
sequences generated different ways, as far as I understand the answer is
that you shouldn't be expecting to be able to do that.

Best,
~G


> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>



-- 
Gabriel Becker, Ph.D
Scientist
Bioinformatics and Computational Biology
Genentech Research

	[[alternative HTML version deleted]]


From @uh@rto_@nggono @ending from y@hoo@com  Sat Sep  8 13:00:17 2018
From: @uh@rto_@nggono @ending from y@hoo@com (Suharto Anggono Suharto Anggono)
Date: Sat, 8 Sep 2018 11:00:17 +0000 (UTC)
Subject: [Rd] Proposal: more accurate seq(from, to, length=n)
References: <30376138.564978.1536404417740.ref@mail.yahoo.com>
Message-ID: <30376138.564978.1536404417740@mail.yahoo.com>

I just thought that returning a more accurate result was better and that
(1:10)/10
was an obvious way to calculate. It turned out that it was not that easy.

I found that calculation that I proposed previously was not accurate for
seq(-5, 5, length=101).
I then thought of
from + (0:(length.out - 1))/((length.out - 1)/(to - from)) ,
that is dividing by (1/by) instead of multiplying by 'by'. But I then found that 1/(1/49) didn't give 49.

So, now I am proposing dividing by (1/by) selectively, like
from + if (abs(to - from) < length.out - 1 &&
abs(to - from) >= 2^(-22)  # exact with 16 significant digits
) (0:(length.out - 1))/((length.out - 1)/(to - from)) else
(0:(length.out - 1))*((to - from)/(length.out - 1))

Not changing 'seq.default' is fine, too.

--------------------------------------------
On Sat, 8/9/18, Gabe Becker <becker.gabe at gene.com> wrote:

 Subject: Re: [Rd] Proposal: more accurate seq(from, to, length=n)

 Cc: "r-devel" <r-devel at r-project.org>
 Date: Saturday, 8 September, 2018, 5:38 AM

 Suharto,
 My 2c
 inline.
 On Fri,
 Sep 7, 2018 at 2:34 PM, Suharto Anggono Suharto Anggono via
 R-devel <r-devel at r-project.org>
 wrote:
 In R,

 seq(0, 1, 0.1)

 gives the same result as

 (0:10)*0.1.

 It is not the same as

 c(0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1) ,

 as 0.1 is not represented exactly. I am fine with it.



 In R,

 seq(0, 1, length=11)

 gives the same result as

 seq(0, 1, 0.1).

 However, for 

 seq(0, 1, length=11),

 it is more accurate to return

 c(0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1)
 .
 It can be obtained by

 (0:10)/10.



 When 'from', 'to', and 'length.out'
 are specified and length.out > 2, I propose for function
 'seq.default' in R to use something like

 from + ((0:(length.out - 1))/(length.out - 1)) * (to -
 from)

 instead of something like

 from + (0:(length.out - 1)) * ((to - from)/(length.out - 1))
 .

 In your example case under
 3.50 on my system?these two expressions give results
 which return TRUE from all.equal, which is the accepted way
 of comparing non-integer numerics in R for
 "sameness".











 > from =
 0
 > to =
 1
 > length.out =
 11
 > all.equal(from +
 ((0:(length.out - 1))/(length.out - 1)) * (to - from), from
 + (0:(length.out - 1)) * ((to - from)/(length.out -
 1)))
 [1] TRUE



 Given that I'm
 wondering what the benefit you're looking for here is
 that would outweigh the very large set of existing code
 whose behavior would technically change? under this change.
 Then again, it wouldn't change with respect to the
 accepted all.equal test, so I guess you could argue that
 either there's "no change" or the change is
 ok??
 I'd still
 like to know what practical problem you're trying to
 solve though. if you're looking for the ability to use
 == to compare non integer sequences generated different
 ways, as far as I understand the answer is that you
 shouldn't be expecting to be able to do
 that.
 Best,~G



 ______________________________ ________________

 R-devel at r-project.org
 mailing list

 https://stat.ethz.ch/mailman/
 listinfo/r-devel




 -- 
 Gabriel Becker, Ph.DScientistBioinformatics and
 Computational BiologyGenentech Research


From tom@@@k@liber@ @ending from gm@il@com  Mon Sep 10 14:18:07 2018
From: tom@@@k@liber@ @ending from gm@il@com (Tomas Kalibera)
Date: Mon, 10 Sep 2018 14:18:07 +0200
Subject: [Rd] True length - length(unclass(x)) - without having to call
 unclass()?
In-Reply-To: <CALEXWq15ASNPhOOd0PiF0F2ky09j-QNk6=qWTfoK3ZEqTBAjYg@mail.gmail.com>
References: <CAFDcVCQdmLYM4gPj_zoz4KeoHGGUzJux9hLK_3vMVaeWUsrHQQ@mail.gmail.com>
 <d16444c2-a703-d822-bb59-f94c92ecb260@gmail.com>
 <CALEXWq15ASNPhOOd0PiF0F2ky09j-QNk6=qWTfoK3ZEqTBAjYg@mail.gmail.com>
Message-ID: <2e3650d9-7de7-3243-1f76-2c0039036222@gmail.com>

On 09/05/2018 11:18 AM, I?aki Ucar wrote:
> The bottomline here is that one can always call a base method,
> inexpensively and without modifying the object, in, let's say,
> *formal* OOP languages. In R, this is not possible in general. It
> would be possible if there was always a foo.default, but primitives
> use internal dispatch.
>
> I was wondering whether it would be possible to provide a super(x, n)
> function which simply causes the dispatching system to avoid "n"
> classes in the hierarchy, so that:
>
>> x <- structure(list(), class=c("foo", "bar"))
>> length(super(x, 0)) # looks for a length.foo
>> length(super(x, 1)) # looks for a length.bar
>> length(super(x, 2)) # calls the default
>> length(super(x, Inf)) # calls the default
I think that a cast should always to be for a specific class, defined by 
the name of the class. Identifying classes by their inheritance index 
might be unnecessarily brittle - it would break if someone introduced a 
new ancestor class. Apart from the syntax - supporting fast casts for S3 
dispatch in the current implementation would be quite a bit of work, 
probably not worth it, also it would probably slow down the internal 
dispatch in primitives. But a partial solution could be implemented at 
some point with ALTREP wrappers when one could without copying create a 
wrapper object with a modified class attribute.

Tomas
> I?aki
>
> El mi?., 5 sept. 2018 a las 10:09, Tomas Kalibera
> (<tomas.kalibera at gmail.com>) escribi?:
>> On 08/24/2018 07:55 PM, Henrik Bengtsson wrote:
>>> Is there a low-level function that returns the length of an object 'x'
>>> - the length that for instance .subset(x) and .subset2(x) see? An
>>> obvious candidate would be to use:
>>>
>>> .length <- function(x) length(unclass(x))
>>>
>>> However, I'm concerned that calling unclass(x) may trigger an
>>> expensive copy internally in some cases.  Is that concern unfounded?
>> Unclass() will always copy when "x" is really a variable, because the
>> value in "x" will be referenced; whether it is prohibitively expensive
>> or not depends only on the workload - if "x" is a very long list and
>> this functions is called often then it could, but at least to me this
>> sounds unlikely. Unless you have a strong reason to believe it is the
>> case I would just use length(unclass(x)).
>>
>> If the copying is really a problem, I would think about why the
>> underlying vector length is needed at R level - whether you really need
>> to know the length without actually having the unclassed vector anyway
>> for something else, so whether you are not paying for the copy anyway.
>> Or, from the other end, if you need to do more without copying, and it
>> is possible without breaking the value semantics, then you might need to
>> switch to C anyway and for a bigger piece of code.
>>
>> If it were still just .length() you needed and it were performance
>> critical, you could just switch to C and call Rf_length. That does not
>> violate the semantics, just indeed it is not elegant as you are
>> switching to C.
>>
>> If you stick to R and can live with the overhead of length(unclass(x))
>> then there is a chance the overhead will decrease as R is optimized
>> internally. This is possible in principle when the runtime knows that
>> the unclassed vector is only needed to compute something that does not
>> modify the vector. The current R cannot optimize this out, but it should
>> be possible with ALTREP at some point (and as Radford mentioned pqR does
>> it differently). Even with such internal optimizations indeed it is
>> often necessary to make guesses about realistic workloads, so if you
>> have a realistic workload where say length(unclass(x)) is critical, you
>> are more than welcome to donate it as benchmark.
>>
>> Obviously, if you use a C version calling Rf_length, after such R
>> optimization your code would be unnecessarily non-elegant, but would
>> still work and probably without overhead, because R can't do much less
>> than Rf_length. In more complicated cases though hand-optimized C code
>> to implement say 2 operations in sequence could be slower than what
>> better optimizing runtime could do by joining the effect of possibly
>> more operations, which is in principle another danger of switching from
>> R to C. But as far as the semantics is followed, there is no other danger.
>>
>> The temptation should be small anyway in this case when Rf_length()
>> would be the simplest, but as I made it more than clear in the previous
>> email, one should never violate the value semantics by temporarily
>> modifying the object (temporarily removing the class attribute or
>> temporarily remove the object bit). Violating semantics causes bugs, if
>> not with the present then with future versions of R (where version may
>> be an svn revision). A concrete recent example: modifying objects in
>> place in violation of the semantics caused a lot of bugs with
>> introduction of unification of constants in the byte-code compiler.
>>
>> Best
>> Tomas
>>
>>> Thxs,
>>>
>>> Henrik
>>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>
>


From iuc@r @ending from fedor@project@org  Mon Sep 10 14:30:47 2018
From: iuc@r @ending from fedor@project@org (=?UTF-8?Q?I=C3=B1aki_Ucar?=)
Date: Mon, 10 Sep 2018 14:30:47 +0200
Subject: [Rd] True length - length(unclass(x)) - without having to call
 unclass()?
In-Reply-To: <2e3650d9-7de7-3243-1f76-2c0039036222@gmail.com>
References: <CAFDcVCQdmLYM4gPj_zoz4KeoHGGUzJux9hLK_3vMVaeWUsrHQQ@mail.gmail.com>
 <d16444c2-a703-d822-bb59-f94c92ecb260@gmail.com>
 <CALEXWq15ASNPhOOd0PiF0F2ky09j-QNk6=qWTfoK3ZEqTBAjYg@mail.gmail.com>
 <2e3650d9-7de7-3243-1f76-2c0039036222@gmail.com>
Message-ID: <CALEXWq0kjjjGfZU+J=Yyhx6CJFef0DsG=eE+CzZne58CuySM4A@mail.gmail.com>

El lun., 10 sept. 2018 a las 14:18, Tomas Kalibera
(<tomas.kalibera at gmail.com>) escribi?:
>
> On 09/05/2018 11:18 AM, I?aki Ucar wrote:
> > The bottomline here is that one can always call a base method,
> > inexpensively and without modifying the object, in, let's say,
> > *formal* OOP languages. In R, this is not possible in general. It
> > would be possible if there was always a foo.default, but primitives
> > use internal dispatch.
> >
> > I was wondering whether it would be possible to provide a super(x, n)
> > function which simply causes the dispatching system to avoid "n"
> > classes in the hierarchy, so that:
> >
> >> x <- structure(list(), class=c("foo", "bar"))
> >> length(super(x, 0)) # looks for a length.foo
> >> length(super(x, 1)) # looks for a length.bar
> >> length(super(x, 2)) # calls the default
> >> length(super(x, Inf)) # calls the default
> I think that a cast should always to be for a specific class, defined by
> the name of the class. Identifying classes by their inheritance index
> might be unnecessarily brittle - it would break if someone introduced a
> new ancestor class.

Agree. But just wanted to point out that, then, something like
super(x, "default") should always work to point to default methods,
even if a method is internal and there's no foo.default defined.
Otherwise, we would have the same problem.

I?aki

> Apart from the syntax - supporting fast casts for S3
> dispatch in the current implementation would be quite a bit of work,
> probably not worth it, also it would probably slow down the internal
> dispatch in primitives. But a partial solution could be implemented at
> some point with ALTREP wrappers when one could without copying create a
> wrapper object with a modified class attribute.
>
> Tomas
> > I?aki
> >
> > El mi?., 5 sept. 2018 a las 10:09, Tomas Kalibera
> > (<tomas.kalibera at gmail.com>) escribi?:
> >> On 08/24/2018 07:55 PM, Henrik Bengtsson wrote:
> >>> Is there a low-level function that returns the length of an object 'x'
> >>> - the length that for instance .subset(x) and .subset2(x) see? An
> >>> obvious candidate would be to use:
> >>>
> >>> .length <- function(x) length(unclass(x))
> >>>
> >>> However, I'm concerned that calling unclass(x) may trigger an
> >>> expensive copy internally in some cases.  Is that concern unfounded?
> >> Unclass() will always copy when "x" is really a variable, because the
> >> value in "x" will be referenced; whether it is prohibitively expensive
> >> or not depends only on the workload - if "x" is a very long list and
> >> this functions is called often then it could, but at least to me this
> >> sounds unlikely. Unless you have a strong reason to believe it is the
> >> case I would just use length(unclass(x)).
> >>
> >> If the copying is really a problem, I would think about why the
> >> underlying vector length is needed at R level - whether you really need
> >> to know the length without actually having the unclassed vector anyway
> >> for something else, so whether you are not paying for the copy anyway.
> >> Or, from the other end, if you need to do more without copying, and it
> >> is possible without breaking the value semantics, then you might need to
> >> switch to C anyway and for a bigger piece of code.
> >>
> >> If it were still just .length() you needed and it were performance
> >> critical, you could just switch to C and call Rf_length. That does not
> >> violate the semantics, just indeed it is not elegant as you are
> >> switching to C.
> >>
> >> If you stick to R and can live with the overhead of length(unclass(x))
> >> then there is a chance the overhead will decrease as R is optimized
> >> internally. This is possible in principle when the runtime knows that
> >> the unclassed vector is only needed to compute something that does not
> >> modify the vector. The current R cannot optimize this out, but it should
> >> be possible with ALTREP at some point (and as Radford mentioned pqR does
> >> it differently). Even with such internal optimizations indeed it is
> >> often necessary to make guesses about realistic workloads, so if you
> >> have a realistic workload where say length(unclass(x)) is critical, you
> >> are more than welcome to donate it as benchmark.
> >>
> >> Obviously, if you use a C version calling Rf_length, after such R
> >> optimization your code would be unnecessarily non-elegant, but would
> >> still work and probably without overhead, because R can't do much less
> >> than Rf_length. In more complicated cases though hand-optimized C code
> >> to implement say 2 operations in sequence could be slower than what
> >> better optimizing runtime could do by joining the effect of possibly
> >> more operations, which is in principle another danger of switching from
> >> R to C. But as far as the semantics is followed, there is no other danger.
> >>
> >> The temptation should be small anyway in this case when Rf_length()
> >> would be the simplest, but as I made it more than clear in the previous
> >> email, one should never violate the value semantics by temporarily
> >> modifying the object (temporarily removing the class attribute or
> >> temporarily remove the object bit). Violating semantics causes bugs, if
> >> not with the present then with future versions of R (where version may
> >> be an svn revision). A concrete recent example: modifying objects in
> >> place in violation of the semantics caused a lot of bugs with
> >> introduction of unification of constants in the byte-code compiler.
> >>
> >> Best
> >> Tomas
> >>
> >>> Thxs,
> >>>
> >>> Henrik
> >>>


From emil@bode @ending from d@n@@kn@w@nl  Tue Sep 11 17:23:23 2018
From: emil@bode @ending from d@n@@kn@w@nl (Emil Bode)
Date: Tue, 11 Sep 2018 15:23:23 +0000
Subject: [Rd] Modification-proposal for %% (modulo) when supplied with double
Message-ID: <1BC486D5-5B53-4C23-B422-877B00EF91B5@dans.knaw.nl>

Hi all,



Could we modify the "%%" (modulo)-operator to include some tolerance for rounding-errors when supplied with doubles?

It's not much work (patch supplied on the bottom), and I don't think it would break anything, only if you were really interested in analysing rounding differences.

Any ideas about implementing this and overwriting base::`%%`, or would we want another method (as I've done for the moment)?



Background

I was writing some code where something has to happen at a certain interval, with progress indicated, something like this:

interval <- .001

progress <- .1

for(i in 1:1000*interval) {myFun(i); Sys.sleep(interval); if(i %% progress, 0))) cat(i, '\n')}

without interval and progress being known in advance. I could work around it and make i integer, or do something like

isTRUE(all.equal(i %% progress,0)) || isTRUE(all.equal(i %% progress, progress),

but I think my code is clearer as it is. And I like the idea behind all.equal: we want double to approximately identical.



So my patch (with roxygen2-markup):

#' Modulo-operator with near-equality

#'

#' The \code{\link[base:Arithmetic]{`\%\%`}} operator calculates the modulo, but sometimes has rounding errors, e.g. "\code{(9.1/.1) \%\% 1}" gives ~ 1, instead of 0.\cr

#' Comparable to what all.equal does, this operator has some tolerance for small rounding errors.\cr

#' If the answer would be equal to the divisor within a small tolerance, 0 is returned instead.

#'

#' For integer x and y, the normal \%\%-operator is used

#'

#' @usage `\%mod\%`(x, y, tolerance = sqrt(.Machine$double.eps))

#' x \%mod\% y

#' @param x,y numeric vectors, similar to those passed on to \%\%

#' @param tolerance numeric, maximum difference, see \code{\link[base]{all.equal}}. The default is ~ \code{1.5e-8}

#' @return identical to the result for \%\%, unless the answer would be really close to y, in which case 0 is returned

#' @note To specify tolerance, use the call \code{`\%mod\%`(x,y,tolerance)}

#' @note The precedence for \code{\%mod\%} is the same as that for \code{\%\%}

#'

#' @name mod

#' @rdname mod

#'

#' @export

`%mod%` <- function(x,y, tolerance = sqrt(.Machine$double.eps)) {

  stopifnot(is.numeric(x), is.numeric(y), is.numeric(tolerance),

            !is.na(tolerance), length(tolerance)==1, tolerance>=0)

  if(is.integer(x) && is.integer(y)) {

    return(x %% y)

  } else {

    ans <- x %% y

    return(ifelse(abs(ans-y)<tolerance | abs(ans)<tolerance, 0, ans))

  }

}



Best regards,

Emil Bode

	[[alternative HTML version deleted]]


From murdoch@dunc@n @ending from gm@il@com  Tue Sep 11 18:11:29 2018
From: murdoch@dunc@n @ending from gm@il@com (Duncan Murdoch)
Date: Tue, 11 Sep 2018 12:11:29 -0400
Subject: [Rd] Modification-proposal for %% (modulo) when supplied with
 double
In-Reply-To: <1BC486D5-5B53-4C23-B422-877B00EF91B5@dans.knaw.nl>
References: <1BC486D5-5B53-4C23-B422-877B00EF91B5@dans.knaw.nl>
Message-ID: <15346010-f2c5-8aa9-7b93-5e24c0692c32@gmail.com>

On 11/09/2018 11:23 AM, Emil Bode wrote:
> Hi all,
> 
> 
> 
> Could we modify the "%%" (modulo)-operator to include some tolerance for rounding-errors when supplied with doubles?
> 
> It's not much work (patch supplied on the bottom), and I don't think it would break anything, only if you were really interested in analysing rounding differences.
> 
> Any ideas about implementing this and overwriting base::`%%`, or would we want another method (as I've done for the moment)?

I think this is a bad idea.  Your comments say "The 
\code{\link[base:Arithmetic]{`\%\%`}} operator calculates the modulo, 
but sometimes has rounding errors, e.g. "\code{(9.1/.1) \%\% 1}" gives ~ 
1, instead of 0."

This is false.  The %% calculation is exactly correct.  The rounding 
error happened in your input:  9.1/0.1 is not equal to 91, it is a 
little bit less:

 > options(digits=20)
 > 9.1/.1
[1] 90.999999999999985789

And %% did not return 1, it returned the correct value:

 > (9.1/.1) %% 1
[1] 0.99999999999998578915

So it makes no sense to change %%.

You might argue that the division 9.1/.1 is giving the wrong answer, but 
in fact that answer is correct too.  The real problem is that in double 
precision floating point the numbers 9.1 and .1 can't be represented 
exactly.  This is well known, it's in the FAQ (question 7.31).

Duncan Murdoch

> 
> 
> 
> Background
> 
> I was writing some code where something has to happen at a certain interval, with progress indicated, something like this:
> 
> interval <- .001
> 
> progress <- .1
> 
> for(i in 1:1000*interval) {myFun(i); Sys.sleep(interval); if(i %% progress, 0))) cat(i, '\n')}
> 
> without interval and progress being known in advance. I could work around it and make i integer, or do something like
> 
> isTRUE(all.equal(i %% progress,0)) || isTRUE(all.equal(i %% progress, progress),
> 
> but I think my code is clearer as it is. And I like the idea behind all.equal: we want double to approximately identical.
> 
> 
> 
> So my patch (with roxygen2-markup):
> 
> #' Modulo-operator with near-equality
> 
> #'
> 
> #' The \code{\link[base:Arithmetic]{`\%\%`}} operator calculates the modulo, but sometimes has rounding errors, e.g. "\code{(9.1/.1) \%\% 1}" gives ~ 1, instead of 0.\cr
> 
> #' Comparable to what all.equal does, this operator has some tolerance for small rounding errors.\cr
> 
> #' If the answer would be equal to the divisor within a small tolerance, 0 is returned instead.
> 
> #'
> 
> #' For integer x and y, the normal \%\%-operator is used
> 
> #'
> 
> #' @usage `\%mod\%`(x, y, tolerance = sqrt(.Machine$double.eps))
> 
> #' x \%mod\% y
> 
> #' @param x,y numeric vectors, similar to those passed on to \%\%
> 
> #' @param tolerance numeric, maximum difference, see \code{\link[base]{all.equal}}. The default is ~ \code{1.5e-8}
> 
> #' @return identical to the result for \%\%, unless the answer would be really close to y, in which case 0 is returned
> 
> #' @note To specify tolerance, use the call \code{`\%mod\%`(x,y,tolerance)}
> 
> #' @note The precedence for \code{\%mod\%} is the same as that for \code{\%\%}
> 
> #'
> 
> #' @name mod
> 
> #' @rdname mod
> 
> #'
> 
> #' @export
> 
> `%mod%` <- function(x,y, tolerance = sqrt(.Machine$double.eps)) {
> 
>    stopifnot(is.numeric(x), is.numeric(y), is.numeric(tolerance),
> 
>              !is.na(tolerance), length(tolerance)==1, tolerance>=0)
> 
>    if(is.integer(x) && is.integer(y)) {
> 
>      return(x %% y)
> 
>    } else {
> 
>      ans <- x %% y
> 
>      return(ifelse(abs(ans-y)<tolerance | abs(ans)<tolerance, 0, ans))
> 
>    }
> 
> }
> 
> 
> 
> Best regards,
> 
> Emil Bode
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From rich@rd_r@ubert@@ @ending from merck@com  Tue Sep 11 20:52:55 2018
From: rich@rd_r@ubert@@ @ending from merck@com (Raubertas, Richard)
Date: Tue, 11 Sep 2018 18:52:55 +0000
Subject: [Rd] var() with 0-length vector -- docs inconsistent with result
Message-ID: <DM5P10602MB0089AD308AD4A2159BD015EA8A040@DM5P10602MB0089.NAMP106.PROD.OUTLOOK.COM>

R 3.5.1 on Windows 7

The documentation for 'var' says:  "These functions return 'NA' when there is only one observation (whereas S-PLUS has been  returning 'NaN'), and fail if 'x' has length zero."  The function 'sd' (based on 'var') has similar documentation.

However, I get:
var(numeric(0))
[1] NA

rather than an error.

Personally I prefer that basic summary functions like 'var' not throw errors even in corner cases.   But either way, the result and the docs are inconsistent.

Richard Raubertas

> sessionInfo()
R version 3.5.1 (2018-07-02)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 7 x64 (build 7601) Service Pack 1

Matrix products: default

locale:
[1] LC_COLLATE=English_United States.1252 
[2] LC_CTYPE=English_United States.1252   
[3] LC_MONETARY=English_United States.1252
[4] LC_NUMERIC=C                          
[5] LC_TIME=English_United States.1252    

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

loaded via a namespace (and not attached):
[1] compiler_3.5.1 tools_3.5.1   
>



Notice:  This e-mail message, together with any attachme...{{dropped:13}}


From frederik m@ili@g off ofb@@et  Tue Sep 11 21:10:09 2018
From: frederik m@ili@g off ofb@@et (frederik m@ili@g off ofb@@et)
Date: Tue, 11 Sep 2018 12:10:09 -0700
Subject: [Rd] Modification-proposal for %% (modulo) when supplied with
 double
In-Reply-To: <15346010-f2c5-8aa9-7b93-5e24c0692c32@gmail.com>
References: <1BC486D5-5B53-4C23-B422-877B00EF91B5@dans.knaw.nl>
 <15346010-f2c5-8aa9-7b93-5e24c0692c32@gmail.com>
Message-ID: <20180911191009.GB18382@ofb.net>

Duncan, I think Emil realizes that the floating point format isn't
able to represent certain numbers, that's why he is suggesting this
change rather than complaining about our arithmetic being broken.

However, I agree with you that we should not adopt his proposal. It
would not make things more "user friendly" for people. Everyone has a
different application and a different use of %% and they just need to
keep in mind that they are talking to a computer and not a blackboard.
Here is an example of a feature that was meant to help users get more
intuitive results with floating point numbers, but which actually
caused headaches instead:
https://github.com/Rdatatable/data.table/issues/1642 It is a slightly
different scenario to this one, but I think it is still a good example
of how we can end up creating unforeseen problems for people if we
change core functionality to do unsolicited rounding behind the
scenes.

Best wishes,

Frederick

On Tue, Sep 11, 2018 at 12:11:29PM -0400, Duncan Murdoch wrote:
> On 11/09/2018 11:23 AM, Emil Bode wrote:
> > Hi all,
> > 
> > 
> > 
> > Could we modify the "%%" (modulo)-operator to include some tolerance for rounding-errors when supplied with doubles?
> > 
> > It's not much work (patch supplied on the bottom), and I don't think it would break anything, only if you were really interested in analysing rounding differences.
> > 
> > Any ideas about implementing this and overwriting base::`%%`, or would we want another method (as I've done for the moment)?
> 
> I think this is a bad idea.  Your comments say "The
> \code{\link[base:Arithmetic]{`\%\%`}} operator calculates the modulo, but
> sometimes has rounding errors, e.g. "\code{(9.1/.1) \%\% 1}" gives ~ 1,
> instead of 0."
> 
> This is false.  The %% calculation is exactly correct.  The rounding error
> happened in your input:  9.1/0.1 is not equal to 91, it is a little bit
> less:
> 
> > options(digits=20)
> > 9.1/.1
> [1] 90.999999999999985789
> 
> And %% did not return 1, it returned the correct value:
> 
> > (9.1/.1) %% 1
> [1] 0.99999999999998578915
> 
> So it makes no sense to change %%.
> 
> You might argue that the division 9.1/.1 is giving the wrong answer, but in
> fact that answer is correct too.  The real problem is that in double
> precision floating point the numbers 9.1 and .1 can't be represented
> exactly.  This is well known, it's in the FAQ (question 7.31).
> 
> Duncan Murdoch
> 
> > 
> > 
> > 
> > Background
> > 
> > I was writing some code where something has to happen at a certain interval, with progress indicated, something like this:
> > 
> > interval <- .001
> > 
> > progress <- .1
> > 
> > for(i in 1:1000*interval) {myFun(i); Sys.sleep(interval); if(i %% progress, 0))) cat(i, '\n')}
> > 
> > without interval and progress being known in advance. I could work around it and make i integer, or do something like
> > 
> > isTRUE(all.equal(i %% progress,0)) || isTRUE(all.equal(i %% progress, progress),
> > 
> > but I think my code is clearer as it is. And I like the idea behind all.equal: we want double to approximately identical.
> > 
> > 
> > 
> > So my patch (with roxygen2-markup):
> > 
> > #' Modulo-operator with near-equality
> > 
> > #'
> > 
> > #' The \code{\link[base:Arithmetic]{`\%\%`}} operator calculates the modulo, but sometimes has rounding errors, e.g. "\code{(9.1/.1) \%\% 1}" gives ~ 1, instead of 0.\cr
> > 
> > #' Comparable to what all.equal does, this operator has some tolerance for small rounding errors.\cr
> > 
> > #' If the answer would be equal to the divisor within a small tolerance, 0 is returned instead.
> > 
> > #'
> > 
> > #' For integer x and y, the normal \%\%-operator is used
> > 
> > #'
> > 
> > #' @usage `\%mod\%`(x, y, tolerance = sqrt(.Machine$double.eps))
> > 
> > #' x \%mod\% y
> > 
> > #' @param x,y numeric vectors, similar to those passed on to \%\%
> > 
> > #' @param tolerance numeric, maximum difference, see \code{\link[base]{all.equal}}. The default is ~ \code{1.5e-8}
> > 
> > #' @return identical to the result for \%\%, unless the answer would be really close to y, in which case 0 is returned
> > 
> > #' @note To specify tolerance, use the call \code{`\%mod\%`(x,y,tolerance)}
> > 
> > #' @note The precedence for \code{\%mod\%} is the same as that for \code{\%\%}
> > 
> > #'
> > 
> > #' @name mod
> > 
> > #' @rdname mod
> > 
> > #'
> > 
> > #' @export
> > 
> > `%mod%` <- function(x,y, tolerance = sqrt(.Machine$double.eps)) {
> > 
> >    stopifnot(is.numeric(x), is.numeric(y), is.numeric(tolerance),
> > 
> >              !is.na(tolerance), length(tolerance)==1, tolerance>=0)
> > 
> >    if(is.integer(x) && is.integer(y)) {
> > 
> >      return(x %% y)
> > 
> >    } else {
> > 
> >      ans <- x %% y
> > 
> >      return(ifelse(abs(ans-y)<tolerance | abs(ans)<tolerance, 0, ans))
> > 
> >    }
> > 
> > }
> > 
> > 
> > 
> > Best regards,
> > 
> > Emil Bode
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
> > 
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From m@echler @ending from @t@t@m@th@ethz@ch  Wed Sep 12 11:50:58 2018
From: m@echler @ending from @t@t@m@th@ethz@ch (Martin Maechler)
Date: Wed, 12 Sep 2018 11:50:58 +0200
Subject: [Rd] 
 var() with 0-length vector -- docs inconsistent with result
In-Reply-To: <DM5P10602MB0089AD308AD4A2159BD015EA8A040@DM5P10602MB0089.NAMP106.PROD.OUTLOOK.COM>
References: <DM5P10602MB0089AD308AD4A2159BD015EA8A040@DM5P10602MB0089.NAMP106.PROD.OUTLOOK.COM>
Message-ID: <23448.57730.644941.84382@stat.math.ethz.ch>

>>>>> Raubertas, Richard via R-devel 
>>>>>     on Tue, 11 Sep 2018 18:52:55 +0000 writes:

    > R 3.5.1 on Windows 7 The documentation for 'var' says:

    > "These functions return 'NA' when there is only one
    > observation (whereas S-PLUS has been returning 'NaN'), and
    > fail if 'x' has length zero."  


Well, that help says much more, notably the paragraph
immediately before the sentence you cite ends saying

     Note that (the equivalent of) ?var(double(0), use = *)? gives ?NA?
     for ?use = "everything"? and ?"na.or.complete"?, and gives an
     error in the other cases.

which is true.

Thank you, Richard, for the report.
The current docs are indeed easily misleading here.
I think that just erasing the ending half-sentence

 " , and fail if 'x' has length zero. "  

should do.

    > The function 'sd' (based on 'var') has similar documentation.

indeed... and "much worse", it says

  The standard deviation of a zero-length vector (after removal of
  ?NA?s if ?na.rm = TRUE?) is not defined and gives an error.  

I propose also just amend the docu there, and do not change
the code (as you Richard also seem favor).
After all,  `NA` is also pretty close to  "not defined", and in that sense valid.

Martin

    > However, I get:
    >  > var(numeric(0))
    >  [1] NA

    > rather than an error.

    > Personally I prefer that basic summary functions like
    > 'var' not throw errors even in corner cases.  But either
    > way, the result and the docs are inconsistent.

    > Richard Raubertas


From chri@topher@culn@ne @ending from unimelb@edu@@u  Wed Sep 12 09:31:30 2018
From: chri@topher@culn@ne @ending from unimelb@edu@@u (Chris Culnane)
Date: Wed, 12 Sep 2018 07:31:30 +0000
Subject: [Rd] Bug 17432 in readLines with R >= 3.5.0 still a problem
Message-ID: <MEXPR01MB1527ADCFA3441A6E943C9D96C91B0@MEXPR01MB1527.ausprd01.prod.outlook.com>

Bug 17432 (https://bugs.r-project.org/bugzilla3/show_bug.cgi?id=17432) is still a problem when using pipes for IPC. 

The bug is evident when calling R from another process and trying to communicate via StdIn. R will buffer the input and not read lines until the buffer is exceeded or StdIn is closed by the sending process. This prevents interactive communication between a calling process and a child R process. 

>From a quick look at the source code, it looks like the bug is caused by only disabling buffering when isatty() returns true for a file descriptor (connections.c). This fixes the original bug when the script is run in a terminal, but doesn't help for pipes, which will return false for isatty().

An example R script and python script are provided to demonstrate the problem:

R script (example.r):
================
f <- file("stdin")
open(f)
while(length(line <- readLines(f,n=1)) > 0) {
  write(line, stderr())
}

Python3 script:
============
import sys, os, subprocess
process = subprocess.Popen(['Rscript', 'example.r'], stdin=subprocess.PIPE, stdout=subprocess.PIPE)
for line in sys.stdin:
    process.stdin.write((line + '\n').encode('utf-8'))
    process.stdin.flush()


Expected Behaviour:
Run python script, each line entered is echoed back immediately by the R script - which is what happens on 3.4.4

Observed Behaviiour on >=3.5.0 (include devel):
The R script does not process lines as they are sent, it only receives them when StdIn is closed.


Best Regards

Chris 


From niek@boum@n @ending from keygene@com  Wed Sep 12 13:07:16 2018
From: niek@boum@n @ending from keygene@com (Niek Bouman)
Date: Wed, 12 Sep 2018 11:07:16 +0000
Subject: [Rd] Environments and parallel processing
Message-ID: <173ff2cb39cd47bfb5cc82158e9df0bc@keygene.com>

While using parallelization R seems to clone all environments (that are normally passed by reference) that are returned from a child process. In particular, consider the following example:
library(parallel)
env1 <- new.env()
envs2 <- lapply(1:4, function(x) env1)

cl<-makeCluster(2, type="FORK")
envs3 <- parLapply(cl, 1:4, function(x) env1)
envs4 <- parLapply(cl, 1:4, function(x) capture.output(str(env1)))
stopCluster(cl)

First I make an environment (env1). Then using the non-parallel lapply I get a list (envs2) where all entries contain a pointer to env1. Now when using the parallel parLapply the entries in the list I get (envs3) contain pointers to different environments, which are supposedly clones of env1 (also note that the first two entries contain the same pointer as the last two; supposedly because I use 2 child nodes for a loop of length 4). This cloning seems to happen when the child node returns their results to the master. To see this I save the pointer of env1 in the child nodes to the list envs4.
Why are environments cloned at the moment they are returned?, and is there a way to pass environments by reference when using parallel processing in R?

Keygene N.V. | P.O. Box 216 | 6700 AE Wageningen | The Netherlands
T (+31) 317 46 68 66 | F (+31) 317 42 49 39 | CoC. 09066631 | http://www.keygene.com<http://www.keygene.com/>



p/28de203a344b/keygene-invites-you-to-keygene-nodigt-u-uit>

Stay up-to-date! Subscribe to our bimonthly newsletter here<http://www.keygene.com/newsletter>


company/KeyGene>   [http://www.keygene.com/images/twitter-grey.png] <https://twitter.com/KeyGeneInfo>     [http://www.keygene.com/images/facebook-grey.png] <https://www.facebook.com/KeyGeneNV>

The information contained in this message, and attachments if any, may be privileged and/or confidential and is intended to be received only by persons
entitled to receive such information. Use of any part of this message and/or its attachments if any, in any other way than as explicitly stated by the sender is strictly prohibited. Should you receive this
message unintentionally please notify the sender immediately, and delete it together with all attachments, if any. Thank you. The transmission of messages and/or information via the Internet is not
secured and may be intercepted by third parties. KeyGene assumes no liability for any damage caused by any unintentional disclosure and/or use of the content of this message and attachments if any.


	[[alternative HTML version deleted]]


From c@@rdi@g@bor @ending from gm@il@com  Wed Sep 12 20:20:23 2018
From: c@@rdi@g@bor @ending from gm@il@com (=?UTF-8?B?R8OhYm9yIENzw6FyZGk=?=)
Date: Wed, 12 Sep 2018 19:20:23 +0100
Subject: [Rd] Environments and parallel processing
In-Reply-To: <173ff2cb39cd47bfb5cc82158e9df0bc@keygene.com>
References: <173ff2cb39cd47bfb5cc82158e9df0bc@keygene.com>
Message-ID: <CABtg=KkZwe8avLGrn84GEA13Z-h_rVWG6ehLFnKe3xD+Z=Ct6g@mail.gmail.com>

This is all normal, a fork cluster works with processes, that do not
share memory. When you create a fork cluster, you create a new
process, that has the same memory layout as the parent. But from this
moment its memory is independent of the parent process. When parLapply
is done, the results are serialized and copied back to the parent
process. The serialized environment is independent of the original
environment in the parent process, when parLapply unserializes the
results it creates new objects.

Environments have reference semantics, but not across processes.

Gabor
On Wed, Sep 12, 2018 at 7:09 PM Niek Bouman <niek.bouman at keygene.com> wrote:
>
> While using parallelization R seems to clone all environments (that are normally passed by reference) that are returned from a child process. In particular, consider the following example:
> library(parallel)
> env1 <- new.env()
> envs2 <- lapply(1:4, function(x) env1)
>
> cl<-makeCluster(2, type="FORK")
> envs3 <- parLapply(cl, 1:4, function(x) env1)
> envs4 <- parLapply(cl, 1:4, function(x) capture.output(str(env1)))
> stopCluster(cl)
>
> First I make an environment (env1). Then using the non-parallel lapply I get a list (envs2) where all entries contain a pointer to env1. Now when using the parallel parLapply the entries in the list I get (envs3) contain pointers to different environments, which are supposedly clones of env1 (also note that the first two entries contain the same pointer as the last two; supposedly because I use 2 child nodes for a loop of length 4). This cloning seems to happen when the child node returns their results to the master. To see this I save the pointer of env1 in the child nodes to the list envs4.
> Why are environments cloned at the moment they are returned?, and is there a way to pass environments by reference when using parallel processing in R?
>
> Keygene N.V. | P.O. Box 216 | 6700 AE Wageningen | The Netherlands
> T (+31) 317 46 68 66 | F (+31) 317 42 49 39 | CoC. 09066631 | http://www.keygene.com<http://www.keygene.com/>
>
>
>
> p/28de203a344b/keygene-invites-you-to-keygene-nodigt-u-uit>
>
> Stay up-to-date! Subscribe to our bimonthly newsletter here<http://www.keygene.com/newsletter>
>
>
> company/KeyGene>   [http://www.keygene.com/images/twitter-grey.png] <https://twitter.com/KeyGeneInfo>     [http://www.keygene.com/images/facebook-grey.png] <https://www.facebook.com/KeyGeneNV>
>
> The information contained in this message, and attachments if any, may be privileged and/or confidential and is intended to be received only by persons
> entitled to receive such information. Use of any part of this message and/or its attachments if any, in any other way than as explicitly stated by the sender is strictly prohibited. Should you receive this
> message unintentionally please notify the sender immediately, and delete it together with all attachments, if any. Thank you. The transmission of messages and/or information via the Internet is not
> secured and may be intercepted by third parties. KeyGene assumes no liability for any damage caused by any unintentional disclosure and/or use of the content of this message and attachments if any.
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From r@lf@@tubner @ending from d@q@n@@com  Wed Sep 12 20:31:06 2018
From: r@lf@@tubner @ending from d@q@n@@com (Ralf Stubner)
Date: Wed, 12 Sep 2018 20:31:06 +0200
Subject: [Rd] Environments and parallel processing
In-Reply-To: <CABtg=KkZwe8avLGrn84GEA13Z-h_rVWG6ehLFnKe3xD+Z=Ct6g@mail.gmail.com>
References: <173ff2cb39cd47bfb5cc82158e9df0bc@keygene.com>
 <CABtg=KkZwe8avLGrn84GEA13Z-h_rVWG6ehLFnKe3xD+Z=Ct6g@mail.gmail.com>
Message-ID: <c0004f24-fed9-f9ff-9d2f-e6b7ee91163b@daqana.com>

On 12.09.2018 20:20, G?bor Cs?rdi wrote:
> This is all normal, a fork cluster works with processes, that do not
> share memory.

And if you are after shared-memory parallelism, you can try the 'Rdsm'
package: https://cran.r-project.org/package=Rdsm

Greetings
Ralf

-- 
Ralf Stubner
Senior Software Engineer / Trainer

daqana GmbH
Dortustra?e 48
14467 Potsdam

T: +49 331 23 61 93 11
F: +49 331 23 61 93 90
M: +49 162 20 91 196
Mail: ralf.stubner at daqana.com

Sitz: Potsdam
Register: AG Potsdam HRB 27966 P
Ust.-IdNr.: DE300072622
Gesch?ftsf?hrer: Prof. Dr. Dr. Karl-Kuno Kunze


-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 833 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20180912/c40caf0f/attachment.sig>

From emil@bode @ending from d@n@@kn@w@nl  Thu Sep 13 11:03:47 2018
From: emil@bode @ending from d@n@@kn@w@nl (Emil Bode)
Date: Thu, 13 Sep 2018 09:03:47 +0000
Subject: [Rd] Modification-proposal for %% (modulo) when supplied with
 double
In-Reply-To: <20180911191009.GB18382@ofb.net>
References: <1BC486D5-5B53-4C23-B422-877B00EF91B5@dans.knaw.nl>
 <15346010-f2c5-8aa9-7b93-5e24c0692c32@gmail.com>
 <20180911191009.GB18382@ofb.net>
Message-ID: <FF9A2857-AC33-4DD4-8769-57CF89EBBC4E@dans.knaw.nl>

Okay, thanks for your reactions.
I realized it's not something being "broken", just that it's a situation where my intuition messes things up, and that in this situation it's not as easy as using all.equal, which is my usual approach when working with floats.
But maybe I underestimated the impact a change would have, thanks for the example Frederick.

Best,
Emil

?On 11/09/2018, 21:10, "frederik at ofb.net" <frederik at ofb.net> wrote:

    Duncan, I think Emil realizes that the floating point format isn't
    able to represent certain numbers, that's why he is suggesting this
    change rather than complaining about our arithmetic being broken.
    
    However, I agree with you that we should not adopt his proposal. It
    would not make things more "user friendly" for people. Everyone has a
    different application and a different use of %% and they just need to
    keep in mind that they are talking to a computer and not a blackboard.
    Here is an example of a feature that was meant to help users get more
    intuitive results with floating point numbers, but which actually
    caused headaches instead:
    https://github.com/Rdatatable/data.table/issues/1642 It is a slightly
    different scenario to this one, but I think it is still a good example
    of how we can end up creating unforeseen problems for people if we
    change core functionality to do unsolicited rounding behind the
    scenes.
    
    Best wishes,
    
    Frederick
    
    On Tue, Sep 11, 2018 at 12:11:29PM -0400, Duncan Murdoch wrote:
    > On 11/09/2018 11:23 AM, Emil Bode wrote:
    > > Hi all,
    > > 
    > > 
    > > 
    > > Could we modify the "%%" (modulo)-operator to include some tolerance for rounding-errors when supplied with doubles?
    > > 
    > > It's not much work (patch supplied on the bottom), and I don't think it would break anything, only if you were really interested in analysing rounding differences.
    > > 
    > > Any ideas about implementing this and overwriting base::`%%`, or would we want another method (as I've done for the moment)?
    > 
    > I think this is a bad idea.  Your comments say "The
    > \code{\link[base:Arithmetic]{`\%\%`}} operator calculates the modulo, but
    > sometimes has rounding errors, e.g. "\code{(9.1/.1) \%\% 1}" gives ~ 1,
    > instead of 0."
    > 
    > This is false.  The %% calculation is exactly correct.  The rounding error
    > happened in your input:  9.1/0.1 is not equal to 91, it is a little bit
    > less:
    > 
    > > options(digits=20)
    > > 9.1/.1
    > [1] 90.999999999999985789
    > 
    > And %% did not return 1, it returned the correct value:
    > 
    > > (9.1/.1) %% 1
    > [1] 0.99999999999998578915
    > 
    > So it makes no sense to change %%.
    > 
    > You might argue that the division 9.1/.1 is giving the wrong answer, but in
    > fact that answer is correct too.  The real problem is that in double
    > precision floating point the numbers 9.1 and .1 can't be represented
    > exactly.  This is well known, it's in the FAQ (question 7.31).
    > 
    > Duncan Murdoch
    > 
    > > 
    > > 
    > > 
    > > Background
    > > 
    > > I was writing some code where something has to happen at a certain interval, with progress indicated, something like this:
    > > 
    > > interval <- .001
    > > 
    > > progress <- .1
    > > 
    > > for(i in 1:1000*interval) {myFun(i); Sys.sleep(interval); if(i %% progress, 0))) cat(i, '\n')}
    > > 
    > > without interval and progress being known in advance. I could work around it and make i integer, or do something like
    > > 
    > > isTRUE(all.equal(i %% progress,0)) || isTRUE(all.equal(i %% progress, progress),
    > > 
    > > but I think my code is clearer as it is. And I like the idea behind all.equal: we want double to approximately identical.
    > > 
    > > 
    > > 
    > > So my patch (with roxygen2-markup):
    > > 
    > > #' Modulo-operator with near-equality
    > > 
    > > #'
    > > 
    > > #' The \code{\link[base:Arithmetic]{`\%\%`}} operator calculates the modulo, but sometimes has rounding errors, e.g. "\code{(9.1/.1) \%\% 1}" gives ~ 1, instead of 0.\cr
    > > 
    > > #' Comparable to what all.equal does, this operator has some tolerance for small rounding errors.\cr
    > > 
    > > #' If the answer would be equal to the divisor within a small tolerance, 0 is returned instead.
    > > 
    > > #'
    > > 
    > > #' For integer x and y, the normal \%\%-operator is used
    > > 
    > > #'
    > > 
    > > #' @usage `\%mod\%`(x, y, tolerance = sqrt(.Machine$double.eps))
    > > 
    > > #' x \%mod\% y
    > > 
    > > #' @param x,y numeric vectors, similar to those passed on to \%\%
    > > 
    > > #' @param tolerance numeric, maximum difference, see \code{\link[base]{all.equal}}. The default is ~ \code{1.5e-8}
    > > 
    > > #' @return identical to the result for \%\%, unless the answer would be really close to y, in which case 0 is returned
    > > 
    > > #' @note To specify tolerance, use the call \code{`\%mod\%`(x,y,tolerance)}
    > > 
    > > #' @note The precedence for \code{\%mod\%} is the same as that for \code{\%\%}
    > > 
    > > #'
    > > 
    > > #' @name mod
    > > 
    > > #' @rdname mod
    > > 
    > > #'
    > > 
    > > #' @export
    > > 
    > > `%mod%` <- function(x,y, tolerance = sqrt(.Machine$double.eps)) {
    > > 
    > >    stopifnot(is.numeric(x), is.numeric(y), is.numeric(tolerance),
    > > 
    > >              !is.na(tolerance), length(tolerance)==1, tolerance>=0)
    > > 
    > >    if(is.integer(x) && is.integer(y)) {
    > > 
    > >      return(x %% y)
    > > 
    > >    } else {
    > > 
    > >      ans <- x %% y
    > > 
    > >      return(ifelse(abs(ans-y)<tolerance | abs(ans)<tolerance, 0, ans))
    > > 
    > >    }
    > > 
    > > }
    > > 
    > > 
    > > 
    > > Best regards,
    > > 
    > > Emil Bode
    > > 
    > > 	[[alternative HTML version deleted]]
    > > 
    > > ______________________________________________
    > > R-devel at r-project.org mailing list
    > > https://stat.ethz.ch/mailman/listinfo/r-devel
    > > 
    > 
    > ______________________________________________
    > R-devel at r-project.org mailing list
    > https://stat.ethz.ch/mailman/listinfo/r-devel
    > 
    


From l@wrence@mich@el @ending from gene@com  Thu Sep 13 14:14:46 2018
From: l@wrence@mich@el @ending from gene@com (Michael Lawrence)
Date: Thu, 13 Sep 2018 05:14:46 -0700
Subject: [Rd] Bug 17432 in readLines with R >= 3.5.0 still a problem
In-Reply-To: <MEXPR01MB1527ADCFA3441A6E943C9D96C91B0@MEXPR01MB1527.ausprd01.prod.outlook.com>
References: <MEXPR01MB1527ADCFA3441A6E943C9D96C91B0@MEXPR01MB1527.ausprd01.prod.outlook.com>
Message-ID: <CAOQ5Nycb=wCiwrFOZz1hQSi0eDA=T-N1B+Ep+1=w++X=8Wxj+w@mail.gmail.com>

Thanks, I responded to this on bugzilla.
On Wed, Sep 12, 2018 at 9:04 AM Chris Culnane
<christopher.culnane at unimelb.edu.au> wrote:
>
> Bug 17432 (https://bugs.r-project.org/bugzilla3/show_bug.cgi?id=17432) is still a problem when using pipes for IPC.
>
> The bug is evident when calling R from another process and trying to communicate via StdIn. R will buffer the input and not read lines until the buffer is exceeded or StdIn is closed by the sending process. This prevents interactive communication between a calling process and a child R process.
>
> From a quick look at the source code, it looks like the bug is caused by only disabling buffering when isatty() returns true for a file descriptor (connections.c). This fixes the original bug when the script is run in a terminal, but doesn't help for pipes, which will return false for isatty().
>
> An example R script and python script are provided to demonstrate the problem:
>
> R script (example.r):
> ================
> f <- file("stdin")
> open(f)
> while(length(line <- readLines(f,n=1)) > 0) {
>   write(line, stderr())
> }
>
> Python3 script:
> ============
> import sys, os, subprocess
> process = subprocess.Popen(['Rscript', 'example.r'], stdin=subprocess.PIPE, stdout=subprocess.PIPE)
> for line in sys.stdin:
>     process.stdin.write((line + '\n').encode('utf-8'))
>     process.stdin.flush()
>
>
> Expected Behaviour:
> Run python script, each line entered is echoed back immediately by the R script - which is what happens on 3.4.4
>
> Observed Behaviiour on >=3.5.0 (include devel):
> The R script does not process lines as they are sent, it only receives them when StdIn is closed.
>
>
> Best Regards
>
> Chris
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From cl@rkfitzg @ending from gm@il@com  Thu Sep 13 19:31:36 2018
From: cl@rkfitzg @ending from gm@il@com (Clark Fitzgerald)
Date: Thu, 13 Sep 2018 10:31:36 -0700
Subject: [Rd] Environments and parallel processing
In-Reply-To: <c0004f24-fed9-f9ff-9d2f-e6b7ee91163b@daqana.com>
References: <173ff2cb39cd47bfb5cc82158e9df0bc@keygene.com>
 <CABtg=KkZwe8avLGrn84GEA13Z-h_rVWG6ehLFnKe3xD+Z=Ct6g@mail.gmail.com>
 <c0004f24-fed9-f9ff-9d2f-e6b7ee91163b@daqana.com>
Message-ID: <CAE_rN=31Sdrqrb=w2HS+5RkSN3MWg7vahmm65j0s7dGHpv3M_Q@mail.gmail.com>

+1 to what Gabor and Ralf said.

In this case the memory address can be misleading. My understanding is that
the environments in all the processes, 1 parent and 2 child, have the
*same* memory address, but once you write to them the operating system
makes copies and maps the address to a *different* physical address. This
is the copy-on-write UNIX fork model.

The example below shows 3 processes that all use the same address for the
environment env1, yet there must be 3 different environments, because
env1$x has 3 different values simultaneously.

-Clark


library(parallel)
env1 <- new.env()
env1$x = 0
cl <- makeCluster(2, type="FORK")

# Now we write to the environment, and env1$x has two distinct values
clusterEvalQ(cl, env1$x <- rnorm(1))
# [[1]]
# [1] -1.296702
#
# [[2]]
# [1] -0.4001104

# The environments in the 2 child processes still have the same address,
# which is the same as the original address
parLapply(cl, 1:4, function(x) capture.output(str(env1)))
env1

# The original x is unchanged
env1$x

stopCluster(cl)


On Wed, Sep 12, 2018 at 11:31 AM Ralf Stubner <ralf.stubner at daqana.com>
wrote:

> On 12.09.2018 20:20, G?bor Cs?rdi wrote:
> > This is all normal, a fork cluster works with processes, that do not
> > share memory.
>
> And if you are after shared-memory parallelism, you can try the 'Rdsm'
> package: https://cran.r-project.org/package=Rdsm
>
> Greetings
> Ralf
>
> --
> Ralf Stubner
> Senior Software Engineer / Trainer
>
> daqana GmbH
> Dortustra?e 48
> 14467 Potsdam
>
> T: +49 331 23 61 93 11
> F: +49 331 23 61 93 90
> M: +49 162 20 91 196
> Mail: ralf.stubner at daqana.com
>
> Sitz: Potsdam
> Register: AG Potsdam HRB 27966 P
> Ust.-IdNr.: DE300072622
> Gesch?ftsf?hrer: Prof. Dr. Dr. Karl-Kuno Kunze
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From brodie@g@@l@m @ending from y@hoo@com  Fri Sep 14 04:03:35 2018
From: brodie@g@@l@m @ending from y@hoo@com (brodie gaslam)
Date: Fri, 14 Sep 2018 02:03:35 +0000 (UTC)
Subject: [Rd] Possible bug with chromatic adaptation in
 grDevices::convertColor
References: <754180083.2444264.1536890615931.ref@mail.yahoo.com>
Message-ID: <754180083.2444264.1536890615931@mail.yahoo.com>

It appears that the chromatic adaptation feature of `grDevices::convertColor`is broken, and likely has been for many years.? While a little surprising, it is an obscure enough feature that there is some possibility this is actually broken, as opposed to user error on my part.? If it turns out to the latter, I apologize in advance for spamming this list.
Consider:
??? rgb.in <- c("#CCCCCC", "#EEEEEE")??? clr <- t(col2rgb(rgb.in)) / 255??? clr.out <- convertColor(clr, "sRGB", "sRGB")??? rgb(clr.out)??? ## [1] "#CCCCCC" "#EEEEEE"
??? convertColor(clr, "sRGB", "sRGB", "D65", "D50")??? ## Error in match.arg(from, nWhite) :??? ##?? 'arg' must be NULL or a character vector
This appears to be because `grDevices:::chromaticAdaptation` expects the whitepoints to be provided in the character format (e.g. "D65"), but they are already converted by `convertColor` into the tristimulus values.? After applying the patch at the end of this e-mail, we get:
??? clr.out <- convertColor(clr, "sRGB", "sRGB", "D65", "D50")??? rgb(clr.out)??? ## [1] "#DACAB0" "#FEECCE"
I do not have a great way of confirming that the conversion is correct with my changes, but I did verify that the `M` matrix computed within`grDevics:::chromaticAdaptation` for the "D65"->"D50" conversion (approximately) matches the corresponding matrix from brucelindbloom.com chromatic adaptation page:
http://www.brucelindbloom.com/Eqn_ChromAdapt.html
Additionally visual inspection via
 ??? scales::show_col(c(rgb.in, rgb(clr.out)))
is consistent with a shift from bluer indirect daylight ("D65") to yellower direct daylight ("D50") illuminant.
It is worth noting that the adaption method currently in`grDevices:::chromaticAdaptation` appears to be the "Von Kries" method, not the "Bradford" method as documented in `?convertColor` and in the comments of thesources.? I base this on comparing the cone response domain matrices on the aforementioned brucelindbloom.com page to the `Ma` matrix defined in`grDevics:::chromaticAdaptation`.
Given that brucelindbloom.com appears to recommend "Bradford", that the sources suggest that was the intent, that `chromaticAdaptation` is only used by`convertColor` in the R sources, that `chromaticAdapation` is not exported, and that that feature appears currently inaccessible via `convertColor`, it may be worth using this opportunity to change the adaptation method to "Bradford".
A suggested patch follows.? It is intended to minimize the required changes, although doing so requires a double transposition.? The transpositions could be easily avoided, but it would require reformulating the calculations in`chromaticAdaption`.
Best,
Brodie.

Index: src/library/grDevices/R/convertColor.R
===================================================================
--- src/library/grDevices/R/convertColor.R?? ?(revision 75298)
+++ src/library/grDevices/R/convertColor.R?? ?(working copy)
@@ -81,7 +81,7 @@
?}
?
?chromaticAdaptation <- function(xyz, from, to) {
-??? ## bradford scaling algorithm
+??? ## Von Kries scaling algorithm
???? Ma <- matrix(c( 0.40024, -0.22630, 0.,
???????????????????? 0.70760,? 1.16532, 0.,
??????????????????? -0.08081,? 0.04570, 0.91822), nrow = 3L, byrow = TRUE)
@@ -242,8 +242,8 @@
?? if (is.null(from.ref.white))
?????? from.ref.white <- to.ref.white
?
-? from.ref.white <- c2to3(white.points[, from.ref.white])
-? to.ref.white?? <- c2to3(white.points[, to.ref.white])
+? from.ref.white.3 <- c2to3(white.points[, from.ref.white])
+? to.ref.white.3?? <- c2to3(white.points[, to.ref.white])
?
?? if (is.null(nrow(color)))
???? color <- matrix(color, nrow = 1L)
@@ -262,19 +262,19 @@
?????? rgb
?? }
?
-? xyz <- apply(color, 1L, from$toXYZ, from.ref.white)
+? xyz <- apply(color, 1L, from$toXYZ, from.ref.white.3)
?
?? if (is.null(nrow(xyz)))
???? xyz <- matrix(xyz, nrow = 1L)
?
-? if (!isTRUE(all.equal(from.ref.white, to.ref.white))) {
+? if (!isTRUE(all.equal(from.ref.white.3, to.ref.white.3))) {
?????? mc <- match.call()
?????? if (is.null(mc$from.ref.white) || is.null(mc$to.ref.white))
?????????? warning("color spaces use different reference whites")
-????? xyz <- chromaticAdaptation(xyz, from.ref.white, to.ref.white)
+????? xyz <- t(chromaticAdaptation(t(xyz), from.ref.white, to.ref.white))
?? }
?
-? rval <- apply(xyz, 2L, to$fromXYZ, to.ref.white)
+? rval <- apply(xyz, 2L, to$fromXYZ, to.ref.white.3)
?
?? if (inherits(to,"RGBcolorConverter"))
?????? rval <- trim(rval)




	[[alternative HTML version deleted]]


From frederik m@ili@g off ofb@@et  Fri Sep 14 08:33:41 2018
From: frederik m@ili@g off ofb@@et (frederik m@ili@g off ofb@@et)
Date: Thu, 13 Sep 2018 23:33:41 -0700
Subject: [Rd] Poor documentation for "adj" and text()
In-Reply-To: <20170411190105.GV3858@ofb.net>
References: <58EC8486020000A1000258ED@gwsmtp1.uni-regensburg.de>
 <58EC8486020000A1000258ED@gwsmtp1.uni-regensburg.de>
 <20170411190105.GV3858@ofb.net>
Message-ID: <20180914063341.GC28532@ofb.net>

Hello Core Team,

I sent this patch over a year ago. It looks like it was sent in
response to another user's complaint which echoed some of my own
observations about problems in the documentation for 'text'. Did
anyone have a chance to look it over? I'd like to get it out of my
queue.

Thanks,

Frederick

On Tue, Apr 11, 2017 at 12:01:05PM -0700, frederik at ofb.net wrote:
> Thanks Ulrich for sharing your experience.
> 
> I'm attaching a patch which tries to address the issues you raised.
> 
> I agree with you in principle, but I think it makes sense to leave
> some details under "Details". However, the descriptions in "Arguments"
> should give enough information that a user can get the function to do
> something predictable in at least one situation, and I feel this is
> not the case at present.
> 
> I tried to fix the wording so that 'adj' and 'offset' are no longer
> confusing to new users (or to me, every time I forget what they mean).
> 
> I also fixed the paragraph on rotated text; it is more correct now, at
> least for X11-cairo.
> 
> I hope that someone in the Core Team can look this over and apply it.
> 
> Thank you,
> 
> Frederick
> 
> On Tue, Apr 11, 2017 at 09:23:50AM +0200, Ulrich Windl wrote:
> > Hi!
> > 
> > (I'd like to be able to access your bugzilla, BTW)
> > The documentation for parameter "adj" of text() in R 3.3.3 is hard to understand (unless you know what it does already):
> > 
> > "adj 
> > one or two values in [0, 1] which specify the x (and optionally y) adjustment of the labels. On most devices values outside that interval will also work."
> > 
> > What is the meaning of the values? I think the description ("adj allows adjustment of the text with respect to (x, y). Values of 0, 0.5, and 1 specify left/bottom, middle and right/top alignment, respectively. The default is for centered text, i.e., adj = c(0.5, NA). Accurate vertical centering needs character metric information on individual characters which is only available on some devices. Vertical alignment is done slightly differently for character strings and for expressions: adj = c(0,0) means to left-justify and to align on the baseline for strings but on the bottom of the bounding box for expressions. This also affects vertical centering: for strings the centering excludes any descenders whereas for expressions it includes them. Using NA for strings centers them, including descenders.") should be moved to the parameter.
> > 
> > In general I'd suggest to describe the range, meaning and default of every parameter where the parameter is listed. "Details" should only give an overview of the functions.
> > 
> > Likewise "offset": Will the direction be influenced by "pos"? The description is quite silent on that.
> > 
> > Documentation should be structured to help the user to find the facts easily without having to read the whole page.
> > 
> > Regards,
> > Ulrich Windl
> > 
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
> > 

> --- text.Rd	2016-11-27 18:33:26.541516325 -0800
> +++ new-text.Rd	2017-04-11 11:48:32.668926075 -0700
> @@ -26,16 +26,18 @@
>      If \code{labels} is longer than \code{x} and
>      \code{y}, the coordinates are recycled to the length of \code{labels}.}
>    \item{adj}{one or two values in \eqn{[0, 1]} which specify the x
> -    (and optionally y) adjustment of the labels.  On most devices values
> -    outside that interval will also work.}
> +    (and optionally y) justification of the labels, with 0 for
> +    left/bottom, 1 for right/top, and 0.5 for centered.
> +    On most devices values
> +    outside \eqn{[0, 1]} will also work. See below.}
>    \item{pos}{a position specifier for the text.  If specified this
>      overrides any \code{adj} value given.  Values of \code{1},
>      \code{2}, \code{3} and \code{4}, respectively indicate
>      positions below, to the left of, above and to the right of
> -    the specified coordinates.}
> -  \item{offset}{when \code{pos} is specified, this value gives the
> -    offset of the label from the specified coordinate in fractions
> -    of a character width.}
> +    \code{(x, y)}.}
> +  \item{offset}{when \code{pos} is specified, this value controls the
> +    distance of the text label from \code{(x, y)}, in fractions of a
> +    character width.}
>    \item{vfont}{\code{NULL} for the current font family, or a character
>      vector of length 2 for Hershey vector fonts.  The first element of
>      the vector selects a typeface and the second element selects a
> @@ -62,10 +64,11 @@
>    mathematical notation is available such as sub- and superscripts,
>    greek letters, fractions, etc.
>  
> -  \code{adj} allows \emph{adj}ustment of the text with respect to
> +  \code{adj} allows \emph{adj}ustment of the text position with respect to
>    \code{(x, y)}.
> -  Values of 0, 0.5, and 1 specify left/bottom, middle and
> -  right/top alignment, respectively.  The default is for centered text, i.e.,
> +  Values of 0, 0.5, and 1 specify that \code{(x, y)} should align with
> +  the left/bottom, middle and
> +  right/top of the text, respectively.  The default is for centered text, i.e.,
>    \code{adj = c(0.5, NA)}.  Accurate vertical centering needs
>    character metric information on individual characters which is
>    only available on some devices.  Vertical alignment is done slightly
> @@ -81,8 +84,17 @@
>    labelled plot.
>  
>    Text can be rotated by using \link{graphical parameters} \code{srt}
> -  (see \code{\link{par}}); this rotates about the centre set by
> -  \code{adj}.
> +  (see \code{\link{par}}). When \code{adj} is specified, a non-zero
> +  \code{srt} rotates the label about \code{(x, y)}. If \code{pos} is
> +  specified, \code{srt} rotates the text about the point on its bounding
> +  box which is closest to \code{(x, y)}: top center for \code{pos = 1},
> +  right center for \code{pos = 2}, bottom center for \code{pos = 3}, and
> +  left center for \code{pos = 4}. The \code{pos} interface is not as
> +  useful for rotated text because the result is no longer centered
> +  vertically or horizontally with respect to \code{(x, y)}. At present
> +  there is no interface in the base libraries to directly rotate text
> +  about its center, but you can achieve this by fiddling with \code{adj}
> +  each time you change \code{srt}.
>  
>    Graphical parameters \code{col}, \code{cex} and \code{font} can be
>    vectors and will then be applied cyclically to the \code{labels} (and

> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From wewol@ki @ending from gm@il@com  Fri Sep 14 09:22:20 2018
From: wewol@ki @ending from gm@il@com (Witold E Wolski)
Date: Fri, 14 Sep 2018 09:22:20 +0200
Subject: [Rd] Problem building rmarkdown vignettes with child
Message-ID: <CAAjnpdjfMg+07bh+o0n1FTEZTp7dYm=6BTCTFD3MBRtfA0MvZw@mail.gmail.com>

Dear Community,

I have an Rmarkdown vignette with optional child documents.
Since all markdown files in the vingette folder are build when
executing R CMD build
I did place the child documents in
/inst/ParametrizedReportChid/
so they are excluded from the build.

I do reference them from the markdown file in the vignette folder by:

```{r referencingChildDocument}
child_docs <- "Grp2Analysis_MissingInOneCondtion.Rmd"
if(!sum(NAinfo$nrProteins > 0) > 0){
  child_docs <- "Grp2Analysis_Empty.Rmd"
}
child_docs <- system.file("ParametrizedReportsChild",child_docs,package
= "SRMService")
```

```{r includeMissingInOne, child = child_docs}
```

When running
devtools::clean_vignettes()
devtools::build_vingettes()
devtools::build() # or R CMD build PACKAGE

all works fine. However,

devtools::clean_vignettes()
devtools::build() # or R CMD build PACKAGE

Fails with the diagnostic:

** building package indices
** installing vignettes
   ?Grp2Analysis.Rmd? using ?UTF-8?
Error in eval(x, envir = envir) : object 'child_docs' not found
Warning in readLines(if (is.character(input2)) { :
  cannot open file './child_docs': No such file or directory
Quitting from lines 387-388 (./child_docs)
Error in readLines(if (is.character(input2)) { :
  cannot open the connection
ERROR: installing vignettes failed

I did try to use the base::system.file and devtools::system.file
function but both generate the same error.

The package and vignette can be found here:
https://github.com/protviz/SRMService


Have a great day

Best regards
Witek



-- 
Witold Eryk Wolski


From m@echler @ending from @t@t@m@th@ethz@ch  Fri Sep 14 09:38:42 2018
From: m@echler @ending from @t@t@m@th@ethz@ch (Martin Maechler)
Date: Fri, 14 Sep 2018 09:38:42 +0200
Subject: [Rd] Poor documentation for "adj" and text()
In-Reply-To: <20180914063341.GC28532@ofb.net>
References: <58EC8486020000A1000258ED@gwsmtp1.uni-regensburg.de>
 <20170411190105.GV3858@ofb.net> <20180914063341.GC28532@ofb.net>
Message-ID: <23451.25986.467626.955824@stat.math.ethz.ch>

>>>>>   
>>>>>     on Thu, 13 Sep 2018 23:33:41 -0700 writes:

    > Hello Core Team, I sent this patch over a year ago. It
    > looks like it was sent in response to another user's
    > complaint which echoed some of my own observations about
    > problems in the documentation for 'text'. Did anyone have
    > a chance to look it over? 

I see it marked in my box as some (of too many !) thing I had
wanted to look at.

OTOH,   R core  may not be terribly motivated by e-mail threads
starting with "Poor <something>" notably when that thing has
been in R (and S before that) for decades.
OTOH^2,  improving documentation is often a good and helpful
thing.... and I will look at it now.

Thank you, Frederick, for trying to help making R better!
Martin


    > I'd like to get it out of my queue.

    > Thanks,

    > Frederick

    > On Tue, Apr 11, 2017 at 12:01:05PM -0700, frederik at ofb.net
    > wrote:
    >> Thanks Ulrich for sharing your experience.
    >> 
    >> I'm attaching a patch which tries to address the issues
    >> you raised.
    >> 
    >> I agree with you in principle, but I think it makes sense
    >> to leave some details under "Details". However, the
    >> descriptions in "Arguments" should give enough
    >> information that a user can get the function to do
    >> something predictable in at least one situation, and I
    >> feel this is not the case at present.
    >> 
    >> I tried to fix the wording so that 'adj' and 'offset' are
    >> no longer confusing to new users (or to me, every time I
    >> forget what they mean).
    >> 
    >> I also fixed the paragraph on rotated text; it is more
    >> correct now, at least for X11-cairo.
    >> 
    >> I hope that someone in the Core Team can look this over
    >> and apply it.
    >> 
    >> Thank you,
    >> 
    >> Frederick
    >> 
    >> On Tue, Apr 11, 2017 at 09:23:50AM +0200, Ulrich Windl
    >> wrote: > Hi!
    >> > 
    >> > (I'd like to be able to access your bugzilla, BTW) >
    >> The documentation for parameter "adj" of text() in R
    >> 3.3.3 is hard to understand (unless you know what it does
    >> already):
    >> > 
    >> > "adj > one or two values in [0, 1] which specify the x
    >> (and optionally y) adjustment of the labels. On most
    >> devices values outside that interval will also work."
    >> > 
    >> > What is the meaning of the values? I think the
    >> description ("adj allows adjustment of the text with
    >> respect to (x, y). Values of 0, 0.5, and 1 specify
    >> left/bottom, middle and right/top alignment,
    >> respectively. The default is for centered text, i.e., adj
    >> = c(0.5, NA). Accurate vertical centering needs character
    >> metric information on individual characters which is only
    >> available on some devices. Vertical alignment is done
    >> slightly differently for character strings and for
    >> expressions: adj = c(0,0) means to left-justify and to
    >> align on the baseline for strings but on the bottom of
    >> the bounding box for expressions. This also affects
    >> vertical centering: for strings the centering excludes
    >> any descenders whereas for expressions it includes
    >> them. Using NA for strings centers them, including
    >> descenders.") should be moved to the parameter.
    >> > 
    >> > In general I'd suggest to describe the range, meaning
    >> and default of every parameter where the parameter is
    >> listed. "Details" should only give an overview of the
    >> functions.
    >> > 
    >> > Likewise "offset": Will the direction be influenced by
    >> "pos"? The description is quite silent on that.
    >> > 
    >> > Documentation should be structured to help the user to
    >> find the facts easily without having to read the whole
    >> page.
    >> > 
    >> > Regards, > Ulrich Windl
    >> > 
    >> > ______________________________________________ >
    >> R-devel at r-project.org mailing list >
    >> https://stat.ethz.ch/mailman/listinfo/r-devel
    >> > 

    >> --- text.Rd 2016-11-27 18:33:26.541516325 -0800 +++
    >> new-text.Rd 2017-04-11 11:48:32.668926075 -0700 @@ -26,16
    >> +26,18 @@ If \code{labels} is longer than \code{x} and
    >> \code{y}, the coordinates are recycled to the length of
    >> \code{labels}.}  \item{adj}{one or two values in \eqn{[0,
    >> 1]} which specify the x - (and optionally y) adjustment
    >> of the labels.  On most devices values - outside that
    >> interval will also work.}  + (and optionally y)
    >> justification of the labels, with 0 for + left/bottom, 1
    >> for right/top, and 0.5 for centered.  + On most devices
    >> values + outside \eqn{[0, 1]} will also work. See below.}
    >> \item{pos}{a position specifier for the text.  If
    >> specified this overrides any \code{adj} value given.
    >> Values of \code{1}, \code{2}, \code{3} and \code{4},
    >> respectively indicate positions below, to the left of,
    >> above and to the right of - the specified coordinates.}
    >> - \item{offset}{when \code{pos} is specified, this value
    >> gives the - offset of the label from the specified
    >> coordinate in fractions - of a character width.}  +
    >> \code{(x, y)}.}  + \item{offset}{when \code{pos} is
    >> specified, this value controls the + distance of the text
    >> label from \code{(x, y)}, in fractions of a + character
    >> width.}  \item{vfont}{\code{NULL} for the current font
    >> family, or a character vector of length 2 for Hershey
    >> vector fonts.  The first element of the vector selects a
    >> typeface and the second element selects a @@ -62,10
    >> +64,11 @@ mathematical notation is available such as sub-
    >> and superscripts, greek letters, fractions, etc.
    >> 
    >> - \code{adj} allows \emph{adj}ustment of the text with
    >> respect to + \code{adj} allows \emph{adj}ustment of the
    >> text position with respect to \code{(x, y)}.  - Values of
    >> 0, 0.5, and 1 specify left/bottom, middle and - right/top
    >> alignment, respectively.  The default is for centered
    >> text, i.e., + Values of 0, 0.5, and 1 specify that
    >> \code{(x, y)} should align with + the left/bottom, middle
    >> and + right/top of the text, respectively.  The default
    >> is for centered text, i.e., \code{adj = c(0.5, NA)}.
    >> Accurate vertical centering needs character metric
    >> information on individual characters which is only
    >> available on some devices.  Vertical alignment is done
    >> slightly @@ -81,8 +84,17 @@ labelled plot.
    >> 
    >> Text can be rotated by using \link{graphical parameters}
    >> \code{srt} - (see \code{\link{par}}); this rotates about
    >> the centre set by - \code{adj}.  + (see
    >> \code{\link{par}}). When \code{adj} is specified, a
    >> non-zero + \code{srt} rotates the label about \code{(x,
    >> y)}. If \code{pos} is + specified, \code{srt} rotates the
    >> text about the point on its bounding + box which is
    >> closest to \code{(x, y)}: top center for \code{pos = 1},
    >> + right center for \code{pos = 2}, bottom center for
    >> \code{pos = 3}, and + left center for \code{pos = 4}. The
    >> \code{pos} interface is not as + useful for rotated text
    >> because the result is no longer centered + vertically or
    >> horizontally with respect to \code{(x, y)}. At present +
    >> there is no interface in the base libraries to directly
    >> rotate text + about its center, but you can achieve this
    >> by fiddling with \code{adj} + each time you change
    >> \code{srt}.
    >> 
    >> Graphical parameters \code{col}, \code{cex} and
    >> \code{font} can be vectors and will then be applied
    >> cyclically to the \code{labels} (and

    >> ______________________________________________
    >> R-devel at r-project.org mailing list
    >> https://stat.ethz.ch/mailman/listinfo/r-devel

    > ______________________________________________
    > R-devel at r-project.org mailing list
    > https://stat.ethz.ch/mailman/listinfo/r-devel


From murdoch@dunc@n @ending from gm@il@com  Fri Sep 14 11:19:35 2018
From: murdoch@dunc@n @ending from gm@il@com (Duncan Murdoch)
Date: Fri, 14 Sep 2018 05:19:35 -0400
Subject: [Rd] Problem building rmarkdown vignettes with child
In-Reply-To: <CAAjnpdjfMg+07bh+o0n1FTEZTp7dYm=6BTCTFD3MBRtfA0MvZw@mail.gmail.com>
References: <CAAjnpdjfMg+07bh+o0n1FTEZTp7dYm=6BTCTFD3MBRtfA0MvZw@mail.gmail.com>
Message-ID: <db1b67b0-d142-a5a8-30d3-52606229d0e5@gmail.com>

On 14/09/2018 3:22 AM, Witold E Wolski wrote:
> Dear Community,
> 
> I have an Rmarkdown vignette with optional child documents.
> Since all markdown files in the vingette folder are build when
> executing R CMD build
> I did place the child documents in
> /inst/ParametrizedReportChid/
> so they are excluded from the build.
> 
> I do reference them from the markdown file in the vignette folder by:
> 
> ```{r referencingChildDocument}
> child_docs <- "Grp2Analysis_MissingInOneCondtion.Rmd"
> if(!sum(NAinfo$nrProteins > 0) > 0){
>    child_docs <- "Grp2Analysis_Empty.Rmd"
> }
> child_docs <- system.file("ParametrizedReportsChild",child_docs,package
> = "SRMService")
> ```
> 
> ```{r includeMissingInOne, child = child_docs}
> ```
> 
> When running
> devtools::clean_vignettes()
> devtools::build_vingettes()
> devtools::build() # or R CMD build PACKAGE
> 
> all works fine. However,
> 
> devtools::clean_vignettes()
> devtools::build() # or R CMD build PACKAGE
> 
> Fails with the diagnostic:
> 
> ** building package indices
> ** installing vignettes
>     ?Grp2Analysis.Rmd? using ?UTF-8?
> Error in eval(x, envir = envir) : object 'child_docs' not found

This message makes it look as though it is looking for an R object named 
child_docs and not finding it.

> Warning in readLines(if (is.character(input2)) { :
>    cannot open file './child_docs': No such file or directory

This message makes it look as though it thinks "child_docs" is the 
filename, not a variable holding the filename.

> Quitting from lines 387-388 (./child_docs)

This one looks like it's a filename again.


It's a long vignette, and not easy to spot where things are going wrong. 
  I'd recommend backing up the original, and just cutting things out 
until the only thing left is the error.  Hopefully then it will be clear 
what's wrong.

Duncan Murdoch

> Error in readLines(if (is.character(input2)) { :
>    cannot open the connection
> ERROR: installing vignettes failed
> 
> I did try to use the base::system.file and devtools::system.file
> function but both generate the same error.
> 
> The package and vignette can be found here:
> https://github.com/protviz/SRMService


From irene@@teve@ @ending from gm@il@com  Fri Sep 14 11:16:43 2018
From: irene@@teve@ @ending from gm@il@com (Irene Steves)
Date: Fri, 14 Sep 2018 12:16:43 +0300
Subject: [Rd] Suggestion: use mustWork = TRUE as the default for system.file
Message-ID: <CAAky9CK17icTdLaVPKmgEPkb_RSEU9UO1YSCW1fKbfVSqL=3eA@mail.gmail.com>

Hello all,

Currently, the default behavior for system.file() is to return "" for
faulty paths.

I've found this behavior to be difficult when debugging, since it passes
the empty path onto other functions.  I initially wrote in errors myself
(with code like `if(path == "") stop(?Path not found?)`), but I now use
mustWork=TRUE or fs::path_package(), which errors by default.

What are the general thoughts on this issue?  I would love to
see mustWork=TRUE become the default in future versions of R, but
I would be happy to hear any counterarguments (or support!) for this
suggestion.

Cheers,
Irene Steves

	[[alternative HTML version deleted]]


From emil@bode @ending from d@n@@kn@w@nl  Fri Sep 14 15:04:29 2018
From: emil@bode @ending from d@n@@kn@w@nl (Emil Bode)
Date: Fri, 14 Sep 2018 13:04:29 +0000
Subject: [Rd] Bug when calling system/system2 (and request for Bugzilla
 account)
Message-ID: <1A6E24FE-022E-44F1-9AB0-58B2DC8A2DB9@dans.knaw.nl>

Hi all,

I found some strange behaviour, which I think is a bug. Could someone make an account for me on Bugzilla or pass on my report?

The problem:
When pressing Ctrl-C when a file is sourced in R, run from Terminal (macOS), sometimes the entire session is ended right away, while I just want to stop the script. This is the case when I press Ctrl-C while some functions are running that don?t catch the interrupt. However, the behaviour is different whether I?m in a clean session (in which case some time is allowed to pass, so that when the function returns the script can be interrupted), or whether I have called base::system() or system2() with timeout other than 0.

Reproducible example:
cat('Start non-interruptable functions\n')
sample_a <- sample(1:1e7)
sample_b <- sample(1:2e7)
matching <- match(sample_a, sample_b)
cat('Finished\n')
Sys.sleep(10)

Observed behaviour:
In a clean session, when I hit Ctrl-C during the execution of match, there is a delay, and as soon as Sys.sleep() is invoked, the script is interrupted, I get back my R ?>?-prompt (unless options(error=?) is set)
But If I add the line system2("ls", timeout = 5), or something similar, when I try to break during the first part of the script, my Rsession ends, I get thrown back to my terminal-prompt.

Desired behaviour:
The best setup would probably be if Ctrl-C would always try to break from the sourced file, and only if that doesn?t success in n seconds, break the rsession altogether, ideally with a customizable option. But maybe that?s too hard, so maybe the most pragmatic would be to have 2 hotkeys: one to break from a hanging/broken rsession, and one to gently try to break from a script. But at least I think it should be:

Expected behaviour:
Consistent behaviour for Ctrl-C: either trying to break the script, or end the session altogether.

Some observations:

  *   I can still break cleanly during the Sys.sleep(). But for larger scripts, it is largely a matter of luck if I hit Ctrl-C during the right moment.
  *   I don?t notice any difference between using system or system2, or any of the arguments other than timeout provided
  *   I don?t notice any difference whether the timeout is actually exhausted or not.
  *   Later calls to system/system2 don?t change anything (i.e. later calling system(?, timeout=0) does not revert back to the old situation)

My setup:
R 3.5.1 (Feather Spray), run with ?vanilla option
GNU bash, version 3.2.57(1)-release (x86_64-apple-darwin17)
macOS High Sierra 10.13.6

Best regards,
Emil Bode

Data-analyst

+31 6 43 83 89 33
emil.bode at dans.knaw.nl<mailto:emil.bode at dans.knaw.nl>

DANS: Netherlands Institute for Permanent Access to Digital Research Resources
Anna van Saksenlaan 51 | 2593 HW Den Haag | +31 70 349 44 50 | info at dans.knaw.nl<mailto:info at dans.kn> | dans.knaw.nl<applewebdata://71F677F0-6872-45F3-A6C4-4972BF87185B/www.dans.knaw.nl>
DANS is an institute of the Dutch Academy KNAW<http://knaw.nl/nl> and funding organisation NWO<http://www.nwo.nl/>.

	[[alternative HTML version deleted]]


From luke-tier@ey m@ili@g off uiow@@edu  Fri Sep 14 17:39:26 2018
From: luke-tier@ey m@ili@g off uiow@@edu (luke-tier@ey m@ili@g off uiow@@edu)
Date: Fri, 14 Sep 2018 10:39:26 -0500 (CDT)
Subject: [Rd] Bug when calling system/system2 (and request for Bugzilla
 account)
In-Reply-To: <1A6E24FE-022E-44F1-9AB0-58B2DC8A2DB9@dans.knaw.nl>
References: <1A6E24FE-022E-44F1-9AB0-58B2DC8A2DB9@dans.knaw.nl>
Message-ID: <alpine.OSX.2.21.1809141037560.55702@lukes-macbook-air.local>

I can't reproduce this. Can you be more precise: exactly where are you
putting the system2 call and exactly where are you sending the
interrupt signal with ^C?

Best,

luke

On Fri, 14 Sep 2018, Emil Bode wrote:

> Hi all,
>
> I found some strange behaviour, which I think is a bug. Could someone make an account for me on Bugzilla or pass on my report?
>
> The problem:
> When pressing Ctrl-C when a file is sourced in R, run from Terminal (macOS), sometimes the entire session is ended right away, while I just want to stop the script. This is the case when I press Ctrl-C while some functions are running that don?t catch the interrupt. However, the behaviour is different whether I?m in a clean session (in which case some time is allowed to pass, so that when the function returns the script can be interrupted), or whether I have called base::system() or system2() with timeout other than 0.
>
> Reproducible example:
> cat('Start non-interruptable functions\n')
> sample_a <- sample(1:1e7)
> sample_b <- sample(1:2e7)
> matching <- match(sample_a, sample_b)
> cat('Finished\n')
> Sys.sleep(10)
>
> Observed behaviour:
> In a clean session, when I hit Ctrl-C during the execution of match, there is a delay, and as soon as Sys.sleep() is invoked, the script is interrupted, I get back my R ?>?-prompt (unless options(error=?) is set)
> But If I add the line system2("ls", timeout = 5), or something similar, when I try to break during the first part of the script, my Rsession ends, I get thrown back to my terminal-prompt.
>
> Desired behaviour:
> The best setup would probably be if Ctrl-C would always try to break from the sourced file, and only if that doesn?t success in n seconds, break the rsession altogether, ideally with a customizable option. But maybe that?s too hard, so maybe the most pragmatic would be to have 2 hotkeys: one to break from a hanging/broken rsession, and one to gently try to break from a script. But at least I think it should be:
>
> Expected behaviour:
> Consistent behaviour for Ctrl-C: either trying to break the script, or end the session altogether.
>
> Some observations:
>
>  *   I can still break cleanly during the Sys.sleep(). But for larger scripts, it is largely a matter of luck if I hit Ctrl-C during the right moment.
>  *   I don?t notice any difference between using system or system2, or any of the arguments other than timeout provided
>  *   I don?t notice any difference whether the timeout is actually exhausted or not.
>  *   Later calls to system/system2 don?t change anything (i.e. later calling system(?, timeout=0) does not revert back to the old situation)
>
> My setup:
> R 3.5.1 (Feather Spray), run with ?vanilla option
> GNU bash, version 3.2.57(1)-release (x86_64-apple-darwin17)
> macOS High Sierra 10.13.6
>
> Best regards,
> Emil Bode
>
> Data-analyst
>
> +31 6 43 83 89 33
> emil.bode at dans.knaw.nl<mailto:emil.bode at dans.knaw.nl>
>
> DANS: Netherlands Institute for Permanent Access to Digital Research Resources
> Anna van Saksenlaan 51 | 2593 HW Den Haag | +31 70 349 44 50 | info at dans.knaw.nl<mailto:info at dans.kn> | dans.knaw.nl<applewebdata://71F677F0-6872-45F3-A6C4-4972BF87185B/www.dans.knaw.nl>
> DANS is an institute of the Dutch Academy KNAW<http://knaw.nl/nl> and funding organisation NWO<http://www.nwo.nl/>.
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
Luke Tierney
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu

From jennifer@@@lyon @ending from gm@il@com  Fri Sep 14 18:22:19 2018
From: jennifer@@@lyon @ending from gm@il@com (Jennifer Lyon)
Date: Fri, 14 Sep 2018 10:22:19 -0600
Subject: [Rd] Bug 17432 in readLines with R >= 3.5.0 still a problem
Message-ID: <CAKstpn7YODuBZUoyTCN1e0oi5pqd_h=7SM3Mj4QU=mpTmquWLQ@mail.gmail.com>

Michael:

I don't see any comments on Bug 17432 (
https://bugs.r-project.org/bugzilla3/show_bug.cgi?id=17432) later than June
1, 2018. Would you please supply a link pointing to the followup to this
discussion on bugzilla?

Thanks.

Jen.

> On Thu Sep 13 14:14:46 CEST 2018 Michael Lawrence wrote:
>
> Thanks, I responded to this on bugzilla.
> On Wed, Sep 12, 2018 at 9:04 AM Chris Culnane
> <christopher.culnane using unimelb.edu.au> wrote:
> >
> > Bug 17432 (https://bugs.r-project.org/bugzilla3/show_bug.cgi?id=17432)
is still a problem when using pipes for IPC.
> >
> > The bug is evident when calling R from another process and trying to
communicate via StdIn. R will buffer the input and not read lines until the
buffer is exceeded or StdIn is closed by the sending process. This prevents
interactive communication between a calling process and a child R process.
> >
> > From a quick look at the source code, it looks like the bug is caused
by only disabling buffering when isatty() returns true for a file
descriptor (connections.c). This fixes the original bug when the script is
run in a terminal, but doesn't help for pipes, which will return false for
isatty().
> >
> > An example R script and python script are provided to demonstrate the
problem:
> >
> > R script (example.r):
> > ================
> > f <- file("stdin")
> > open(f)
> > while(length(line <- readLines(f,n=1)) > 0) {
> >   write(line, stderr())
> > }
> >
> > Python3 script:
> > ============
> > import sys, os, subprocess
> > process = subprocess.Popen(['Rscript', 'example.r'],
stdin=subprocess.PIPE, stdout=subprocess.PIPE)
> > for line in sys.stdin:
> >     process.stdin.write((line + '\n').encode('utf-8'))
> >     process.stdin.flush()
> >
> >
> > Expected Behaviour:
> > Run python script, each line entered is echoed back immediately by the
R script - which is what happens on 3.4.4
> >
> > Observed Behaviiour on >=3.5.0 (include devel):
> > The R script does not process lines as they are sent, it only receives
them when StdIn is closed.
> >
> >
> > Best Regards
> >
> > Chris

	[[alternative HTML version deleted]]


From l@wrence@mich@el @ending from gene@com  Fri Sep 14 18:52:15 2018
From: l@wrence@mich@el @ending from gene@com (Michael Lawrence)
Date: Fri, 14 Sep 2018 09:52:15 -0700
Subject: [Rd] Bug 17432 in readLines with R >= 3.5.0 still a problem
In-Reply-To: <CAKstpn7YODuBZUoyTCN1e0oi5pqd_h=7SM3Mj4QU=mpTmquWLQ@mail.gmail.com>
References: <CAKstpn7YODuBZUoyTCN1e0oi5pqd_h=7SM3Mj4QU=mpTmquWLQ@mail.gmail.com>
Message-ID: <CAOQ5NycqN5RDZqe5Qz3eJA58buTGQy0xndRzao5_gtAB+kqPVw@mail.gmail.com>

The actual bug corresponding to this thread is:
https://bugs.r-project.org/bugzilla/show_bug.cgi?id=17470
On Fri, Sep 14, 2018 at 9:22 AM Jennifer Lyon <jennifer.s.lyon at gmail.com> wrote:
>
> Michael:
>
> I don't see any comments on Bug 17432 (https://bugs.r-project.org/bugzilla3/show_bug.cgi?id=17432) later than June 1, 2018. Would you please supply a link pointing to the followup to this discussion on bugzilla?
>
> Thanks.
>
> Jen.
>
> > On Thu Sep 13 14:14:46 CEST 2018 Michael Lawrence wrote:
> >
> > Thanks, I responded to this on bugzilla.
> > On Wed, Sep 12, 2018 at 9:04 AM Chris Culnane
> > <christopher.culnane using unimelb.edu.au> wrote:
> > >
> > > Bug 17432 (https://bugs.r-project.org/bugzilla3/show_bug.cgi?id=17432) is still a problem when using pipes for IPC.
> > >
> > > The bug is evident when calling R from another process and trying to communicate via StdIn. R will buffer the input and not read lines until the buffer is exceeded or StdIn is closed by the sending process. This prevents interactive communication between a calling process and a child R process.
> > >
> > > From a quick look at the source code, it looks like the bug is caused by only disabling buffering when isatty() returns true for a file descriptor (connections.c). This fixes the original bug when the script is run in a terminal, but doesn't help for pipes, which will return false for isatty().
> > >
> > > An example R script and python script are provided to demonstrate the problem:
> > >
> > > R script (example.r):
> > > ================
> > > f <- file("stdin")
> > > open(f)
> > > while(length(line <- readLines(f,n=1)) > 0) {
> > >   write(line, stderr())
> > > }
> > >
> > > Python3 script:
> > > ============
> > > import sys, os, subprocess
> > > process = subprocess.Popen(['Rscript', 'example.r'], stdin=subprocess.PIPE, stdout=subprocess.PIPE)
> > > for line in sys.stdin:
> > >     process.stdin.write((line + '\n').encode('utf-8'))
> > >     process.stdin.flush()
> > >
> > >
> > > Expected Behaviour:
> > > Run python script, each line entered is echoed back immediately by the R script - which is what happens on 3.4.4
> > >
> > > Observed Behaviiour on >=3.5.0 (include devel):
> > > The R script does not process lines as they are sent, it only receives them when StdIn is closed.
> > >
> > >
> > > Best Regards
> > >
> > > Chris
>
>


From emil@bode @ending from d@n@@kn@w@nl  Fri Sep 14 19:52:52 2018
From: emil@bode @ending from d@n@@kn@w@nl (Emil Bode)
Date: Fri, 14 Sep 2018 17:52:52 +0000
Subject: [Rd] Bug when calling system/system2 (and request for Bugzilla
 account)
In-Reply-To: <alpine.OSX.2.21.1809141037560.55702@lukes-macbook-air.local>
References: <1A6E24FE-022E-44F1-9AB0-58B2DC8A2DB9@dans.knaw.nl>
 <alpine.OSX.2.21.1809141037560.55702@lukes-macbook-air.local>
Message-ID: <8F2C26BE-4F8B-4C17-9EE8-6FD35A3B0E04@dans.knaw.nl>

I hope it's not too specific in my setup...
I've tried with system2 added on the first line, so:

Example.R:
system2('ls', timeout=5)
cat('Start non-interruptable functions\n')
sample_a <- sample(1:1e7)
sample_b <- sample(1:2e7)
matching <- match(sample_a, sample_b)
cat('Finished\n')
Sys.sleep(10)

And in terminal/bash:
R --vanilla
source('Example.R')
Send ^C between the messages (Start...  until Finished)

Or if you have a more powerful CPU you can increase the samples, the exact code doesn't matter very much.
As soon as you restart and source again with the system2 call outcommented, the behaviour is different, there is a pause, and your return to the R-prompt.

Best, Emil



?On 14/09/2018, 17:39, "luke-tierney at uiowa.edu" <luke-tierney at uiowa.edu> wrote:

    I can't reproduce this. Can you be more precise: exactly where are you
    putting the system2 call and exactly where are you sending the
    interrupt signal with ^C?
    
    Best,
    
    luke
    
    On Fri, 14 Sep 2018, Emil Bode wrote:
    
    > Hi all,
    >
    > I found some strange behaviour, which I think is a bug. Could someone make an account for me on Bugzilla or pass on my report?
    >
    > The problem:
    > When pressing Ctrl-C when a file is sourced in R, run from Terminal (macOS), sometimes the entire session is ended right away, while I just want to stop the script. This is the case when I press Ctrl-C while some functions are running that don?t catch the interrupt. However, the behaviour is different whether I?m in a clean session (in which case some time is allowed to pass, so that when the function returns the script can be interrupted), or whether I have called base::system() or system2() with timeout other than 0.
    >
    > Reproducible example:
    > cat('Start non-interruptable functions\n')
    > sample_a <- sample(1:1e7)
    > sample_b <- sample(1:2e7)
    > matching <- match(sample_a, sample_b)
    > cat('Finished\n')
    > Sys.sleep(10)
    >
    > Observed behaviour:
    > In a clean session, when I hit Ctrl-C during the execution of match, there is a delay, and as soon as Sys.sleep() is invoked, the script is interrupted, I get back my R ?>?-prompt (unless options(error=?) is set)
    > But If I add the line system2("ls", timeout = 5), or something similar, when I try to break during the first part of the script, my Rsession ends, I get thrown back to my terminal-prompt.
    >
    > Desired behaviour:
    > The best setup would probably be if Ctrl-C would always try to break from the sourced file, and only if that doesn?t success in n seconds, break the rsession altogether, ideally with a customizable option. But maybe that?s too hard, so maybe the most pragmatic would be to have 2 hotkeys: one to break from a hanging/broken rsession, and one to gently try to break from a script. But at least I think it should be:
    >
    > Expected behaviour:
    > Consistent behaviour for Ctrl-C: either trying to break the script, or end the session altogether.
    >
    > Some observations:
    >
    >  *   I can still break cleanly during the Sys.sleep(). But for larger scripts, it is largely a matter of luck if I hit Ctrl-C during the right moment.
    >  *   I don?t notice any difference between using system or system2, or any of the arguments other than timeout provided
    >  *   I don?t notice any difference whether the timeout is actually exhausted or not.
    >  *   Later calls to system/system2 don?t change anything (i.e. later calling system(?, timeout=0) does not revert back to the old situation)
    >
    > My setup:
    > R 3.5.1 (Feather Spray), run with ?vanilla option
    > GNU bash, version 3.2.57(1)-release (x86_64-apple-darwin17)
    > macOS High Sierra 10.13.6
    >
    > Best regards,
    > Emil Bode
    >
    > Data-analyst
    >
    > +31 6 43 83 89 33
    > emil.bode at dans.knaw.nl<mailto:emil.bode at dans.knaw.nl>
    >
    > DANS: Netherlands Institute for Permanent Access to Digital Research Resources
    > Anna van Saksenlaan 51 | 2593 HW Den Haag | +31 70 349 44 50 | info at dans.knaw.nl<mailto:info at dans.kn> | dans.knaw.nl<applewebdata://71F677F0-6872-45F3-A6C4-4972BF87185B/www.dans.knaw.nl>
    > DANS is an institute of the Dutch Academy KNAW<http://knaw.nl/nl> and funding organisation NWO<http://www.nwo.nl/>.
    >
    > 	[[alternative HTML version deleted]]
    >
    > ______________________________________________
    > R-devel at r-project.org mailing list
    > https://stat.ethz.ch/mailman/listinfo/r-devel
    
    -- 
    Luke Tierney
    Ralph E. Wareham Professor of Mathematical Sciences
    University of Iowa                  Phone:             319-335-3386
    Department of Statistics and        Fax:               319-335-3017
        Actuarial Science
    241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
    Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu


From kevinu@hey @ending from gm@il@com  Fri Sep 14 19:58:48 2018
From: kevinu@hey @ending from gm@il@com (Kevin Ushey)
Date: Fri, 14 Sep 2018 10:58:48 -0700
Subject: [Rd] Bug when calling system/system2 (and request for Bugzilla
 account)
In-Reply-To: <8F2C26BE-4F8B-4C17-9EE8-6FD35A3B0E04@dans.knaw.nl>
References: <1A6E24FE-022E-44F1-9AB0-58B2DC8A2DB9@dans.knaw.nl>
 <alpine.OSX.2.21.1809141037560.55702@lukes-macbook-air.local>
 <8F2C26BE-4F8B-4C17-9EE8-6FD35A3B0E04@dans.knaw.nl>
Message-ID: <CAJXgQP2zXr_udq1nt7CtxBdDvr3rfjmZG95TBPok0vUW5iC9Hg@mail.gmail.com>

FWIW I can reproduce on macOS with R 3.5.1. A smaller example:

    system2("ls", timeout = 5); x <- sample(1:1E8)

If I try to interrupt R while that sample call is running, R itself is closed.

Best,
Kevin

On Fri, Sep 14, 2018 at 10:53 AM Emil Bode <emil.bode at dans.knaw.nl> wrote:
>
> I hope it's not too specific in my setup...
> I've tried with system2 added on the first line, so:
>
> Example.R:
> system2('ls', timeout=5)
> cat('Start non-interruptable functions\n')
> sample_a <- sample(1:1e7)
> sample_b <- sample(1:2e7)
> matching <- match(sample_a, sample_b)
> cat('Finished\n')
> Sys.sleep(10)
>
> And in terminal/bash:
> R --vanilla
> source('Example.R')
> Send ^C between the messages (Start...  until Finished)
>
> Or if you have a more powerful CPU you can increase the samples, the exact code doesn't matter very much.
> As soon as you restart and source again with the system2 call outcommented, the behaviour is different, there is a pause, and your return to the R-prompt.
>
> Best, Emil
>
>
>
> ?On 14/09/2018, 17:39, "luke-tierney at uiowa.edu" <luke-tierney at uiowa.edu> wrote:
>
>     I can't reproduce this. Can you be more precise: exactly where are you
>     putting the system2 call and exactly where are you sending the
>     interrupt signal with ^C?
>
>     Best,
>
>     luke
>
>     On Fri, 14 Sep 2018, Emil Bode wrote:
>
>     > Hi all,
>     >
>     > I found some strange behaviour, which I think is a bug. Could someone make an account for me on Bugzilla or pass on my report?
>     >
>     > The problem:
>     > When pressing Ctrl-C when a file is sourced in R, run from Terminal (macOS), sometimes the entire session is ended right away, while I just want to stop the script. This is the case when I press Ctrl-C while some functions are running that don?t catch the interrupt. However, the behaviour is different whether I?m in a clean session (in which case some time is allowed to pass, so that when the function returns the script can be interrupted), or whether I have called base::system() or system2() with timeout other than 0.
>     >
>     > Reproducible example:
>     > cat('Start non-interruptable functions\n')
>     > sample_a <- sample(1:1e7)
>     > sample_b <- sample(1:2e7)
>     > matching <- match(sample_a, sample_b)
>     > cat('Finished\n')
>     > Sys.sleep(10)
>     >
>     > Observed behaviour:
>     > In a clean session, when I hit Ctrl-C during the execution of match, there is a delay, and as soon as Sys.sleep() is invoked, the script is interrupted, I get back my R ?>?-prompt (unless options(error=?) is set)
>     > But If I add the line system2("ls", timeout = 5), or something similar, when I try to break during the first part of the script, my Rsession ends, I get thrown back to my terminal-prompt.
>     >
>     > Desired behaviour:
>     > The best setup would probably be if Ctrl-C would always try to break from the sourced file, and only if that doesn?t success in n seconds, break the rsession altogether, ideally with a customizable option. But maybe that?s too hard, so maybe the most pragmatic would be to have 2 hotkeys: one to break from a hanging/broken rsession, and one to gently try to break from a script. But at least I think it should be:
>     >
>     > Expected behaviour:
>     > Consistent behaviour for Ctrl-C: either trying to break the script, or end the session altogether.
>     >
>     > Some observations:
>     >
>     >  *   I can still break cleanly during the Sys.sleep(). But for larger scripts, it is largely a matter of luck if I hit Ctrl-C during the right moment.
>     >  *   I don?t notice any difference between using system or system2, or any of the arguments other than timeout provided
>     >  *   I don?t notice any difference whether the timeout is actually exhausted or not.
>     >  *   Later calls to system/system2 don?t change anything (i.e. later calling system(?, timeout=0) does not revert back to the old situation)
>     >
>     > My setup:
>     > R 3.5.1 (Feather Spray), run with ?vanilla option
>     > GNU bash, version 3.2.57(1)-release (x86_64-apple-darwin17)
>     > macOS High Sierra 10.13.6
>     >
>     > Best regards,
>     > Emil Bode
>     >
>     > Data-analyst
>     >
>     > +31 6 43 83 89 33
>     > emil.bode at dans.knaw.nl<mailto:emil.bode at dans.knaw.nl>
>     >
>     > DANS: Netherlands Institute for Permanent Access to Digital Research Resources
>     > Anna van Saksenlaan 51 | 2593 HW Den Haag | +31 70 349 44 50 | info at dans.knaw.nl<mailto:info at dans.kn> | dans.knaw.nl<applewebdata://71F677F0-6872-45F3-A6C4-4972BF87185B/www.dans.knaw.nl>
>     > DANS is an institute of the Dutch Academy KNAW<http://knaw.nl/nl> and funding organisation NWO<http://www.nwo.nl/>.
>     >
>     >   [[alternative HTML version deleted]]
>     >
>     > ______________________________________________
>     > R-devel at r-project.org mailing list
>     > https://stat.ethz.ch/mailman/listinfo/r-devel
>
>     --
>     Luke Tierney
>     Ralph E. Wareham Professor of Mathematical Sciences
>     University of Iowa                  Phone:             319-335-3386
>     Department of Statistics and        Fax:               319-335-3017
>         Actuarial Science
>     241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
>     Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From luke-tier@ey m@ili@g off uiow@@edu  Fri Sep 14 21:18:22 2018
From: luke-tier@ey m@ili@g off uiow@@edu (luke-tier@ey m@ili@g off uiow@@edu)
Date: Fri, 14 Sep 2018 14:18:22 -0500 (CDT)
Subject: [Rd] Bug when calling system/system2 (and request for Bugzilla
 account)
In-Reply-To: <CAJXgQP2zXr_udq1nt7CtxBdDvr3rfjmZG95TBPok0vUW5iC9Hg@mail.gmail.com>
References: <1A6E24FE-022E-44F1-9AB0-58B2DC8A2DB9@dans.knaw.nl>
 <alpine.OSX.2.21.1809141037560.55702@lukes-macbook-air.local>
 <8F2C26BE-4F8B-4C17-9EE8-6FD35A3B0E04@dans.knaw.nl>
 <CAJXgQP2zXr_udq1nt7CtxBdDvr3rfjmZG95TBPok0vUW5iC9Hg@mail.gmail.com>
Message-ID: <alpine.DEB.2.21.1809141409450.16079@luke-Latitude-7480>

Thanks to you both -- I see it now on Ubuntu. A typo in the code for
restoring the SIGINT handler in the timeout cleanup was installing the
wrong handler. Fixed in R-devel and R-patched.

Best,

luke

On Fri, 14 Sep 2018, Kevin Ushey wrote:

> FWIW I can reproduce on macOS with R 3.5.1. A smaller example:
>
>    system2("ls", timeout = 5); x <- sample(1:1E8)
>
> If I try to interrupt R while that sample call is running, R itself is closed.
>
> Best,
> Kevin
>
> On Fri, Sep 14, 2018 at 10:53 AM Emil Bode <emil.bode at dans.knaw.nl> wrote:
>>
>> I hope it's not too specific in my setup...
>> I've tried with system2 added on the first line, so:
>>
>> Example.R:
>> system2('ls', timeout=5)
>> cat('Start non-interruptable functions\n')
>> sample_a <- sample(1:1e7)
>> sample_b <- sample(1:2e7)
>> matching <- match(sample_a, sample_b)
>> cat('Finished\n')
>> Sys.sleep(10)
>>
>> And in terminal/bash:
>> R --vanilla
>> source('Example.R')
>> Send ^C between the messages (Start...  until Finished)
>>
>> Or if you have a more powerful CPU you can increase the samples, the exact code doesn't matter very much.
>> As soon as you restart and source again with the system2 call outcommented, the behaviour is different, there is a pause, and your return to the R-prompt.
>>
>> Best, Emil
>>
>>
>>
>> ?On 14/09/2018, 17:39, "luke-tierney at uiowa.edu" <luke-tierney at uiowa.edu> wrote:
>>
>>     I can't reproduce this. Can you be more precise: exactly where are you
>>     putting the system2 call and exactly where are you sending the
>>     interrupt signal with ^C?
>>
>>     Best,
>>
>>     luke
>>
>>     On Fri, 14 Sep 2018, Emil Bode wrote:
>>
>>    > Hi all,
>>    >
>>    > I found some strange behaviour, which I think is a bug. Could someone make an account for me on Bugzilla or pass on my report?
>>    >
>>    > The problem:
>>    > When pressing Ctrl-C when a file is sourced in R, run from Terminal (macOS), sometimes the entire session is ended right away, while I just want to stop the script. This is the case when I press Ctrl-C while some functions are running that don?t catch the interrupt. However, the behaviour is different whether I?m in a clean session (in which case some time is allowed to pass, so that when the function returns the script can be interrupted), or whether I have called base::system() or system2() with timeout other than 0.
>>    >
>>    > Reproducible example:
>>    > cat('Start non-interruptable functions\n')
>>    > sample_a <- sample(1:1e7)
>>    > sample_b <- sample(1:2e7)
>>    > matching <- match(sample_a, sample_b)
>>    > cat('Finished\n')
>>    > Sys.sleep(10)
>>    >
>>    > Observed behaviour:
>>    > In a clean session, when I hit Ctrl-C during the execution of match, there is a delay, and as soon as Sys.sleep() is invoked, the script is interrupted, I get back my R ?>?-prompt (unless options(error=?) is set)
>>    > But If I add the line system2("ls", timeout = 5), or something similar, when I try to break during the first part of the script, my Rsession ends, I get thrown back to my terminal-prompt.
>>    >
>>    > Desired behaviour:
>>    > The best setup would probably be if Ctrl-C would always try to break from the sourced file, and only if that doesn?t success in n seconds, break the rsession altogether, ideally with a customizable option. But maybe that?s too hard, so maybe the most pragmatic would be to have 2 hotkeys: one to break from a hanging/broken rsession, and one to gently try to break from a script. But at least I think it should be:
>>    >
>>    > Expected behaviour:
>>    > Consistent behaviour for Ctrl-C: either trying to break the script, or end the session altogether.
>>    >
>>    > Some observations:
>>    >
>>    >  *   I can still break cleanly during the Sys.sleep(). But for larger scripts, it is largely a matter of luck if I hit Ctrl-C during the right moment.
>>    >  *   I don?t notice any difference between using system or system2, or any of the arguments other than timeout provided
>>    >  *   I don?t notice any difference whether the timeout is actually exhausted or not.
>>    >  *   Later calls to system/system2 don?t change anything (i.e. later calling system(?, timeout=0) does not revert back to the old situation)
>>    >
>>    > My setup:
>>    > R 3.5.1 (Feather Spray), run with ?vanilla option
>>    > GNU bash, version 3.2.57(1)-release (x86_64-apple-darwin17)
>>    > macOS High Sierra 10.13.6
>>    >
>>    > Best regards,
>>    > Emil Bode
>>    >
>>    > Data-analyst
>>    >
>>    > +31 6 43 83 89 33
>>    > emil.bode at dans.knaw.nl<mailto:emil.bode at dans.knaw.nl>
>>    >
>>    > DANS: Netherlands Institute for Permanent Access to Digital Research Resources
>>    > Anna van Saksenlaan 51 | 2593 HW Den Haag | +31 70 349 44 50 | info at dans.knaw.nl<mailto:info at dans.kn> | dans.knaw.nl<applewebdata://71F677F0-6872-45F3-A6C4-4972BF87185B/www.dans.knaw.nl>
>>    > DANS is an institute of the Dutch Academy KNAW<http://knaw.nl/nl> and funding organisation NWO<http://www.nwo.nl/>.
>>    >
>>    >   [[alternative HTML version deleted]]
>>    >
>>    > ______________________________________________
>>    > R-devel at r-project.org mailing list
>>    > https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>>     --
>>     Luke Tierney
>>     Ralph E. Wareham Professor of Mathematical Sciences
>>     University of Iowa                  Phone:             319-335-3386
>>     Department of Statistics and        Fax:               319-335-3017
>>         Actuarial Science
>>     241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
>>     Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>

-- 
Luke Tierney
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu

From tom@@@k@liber@ @ending from gm@il@com  Mon Sep 17 07:02:35 2018
From: tom@@@k@liber@ @ending from gm@il@com (Tomas Kalibera)
Date: Sun, 16 Sep 2018 22:02:35 -0700
Subject: [Rd] 
 Suggestion: use mustWork = TRUE as the default for system.file
In-Reply-To: <CAAky9CK17icTdLaVPKmgEPkb_RSEU9UO1YSCW1fKbfVSqL=3eA@mail.gmail.com>
References: <CAAky9CK17icTdLaVPKmgEPkb_RSEU9UO1YSCW1fKbfVSqL=3eA@mail.gmail.com>
Message-ID: <ad579c6c-cc7c-95af-ca51-86bee3174792@gmail.com>

Hello Irene,

we can only change the documented behavior when there is a very strong 
reason to do so, because it indeed can break existing code. A lot of 
existing code would depend on the current behavior, using e.g. nzchar() 
to check the output of system.file(). Changing the behavior that is used 
on error paths would be particularly tricky, because the error paths are 
typically not covered by tests (normally when considering a change I 
would run tests for all CRAN+BIOC packages to test if that change would 
be too disruptive, but it would not help in this case). I am afraid such 
a change is impossible.

Best
Tomas

On 09/14/2018 02:16 AM, Irene Steves wrote:
> Hello all,
>
> Currently, the default behavior for system.file() is to return "" for
> faulty paths.
>
> I've found this behavior to be difficult when debugging, since it passes
> the empty path onto other functions.  I initially wrote in errors myself
> (with code like `if(path == "") stop(?Path not found?)`), but I now use
> mustWork=TRUE or fs::path_package(), which errors by default.
>
> What are the general thoughts on this issue?  I would love to
> see mustWork=TRUE become the default in future versions of R, but
> I would be happy to hear any counterarguments (or support!) for this
> suggestion.
>
> Cheers,
> Irene Steves
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From c@c@voeten @ending from hum@leidenuniv@nl  Sun Sep 16 10:53:03 2018
From: c@c@voeten @ending from hum@leidenuniv@nl (Voeten, C.C.)
Date: Sun, 16 Sep 2018 08:53:03 +0000
Subject: [Rd] Rscript -e does not accept newlines under Linux?
Message-ID: <D14049CE02C4F54D95360EEC06CE45C50F937902@SPMXM08.VUW.leidenuniv.nl>

Hello,

I have found what I believe to be a bug in the Linux version of the Rscript binary.
Under Windows (official 64-bit 3.5.1 R distribution running on an up-to-date Win10), I can do the following (e.g. under powershell):

PS H:\Users\Cesko> Rscript -e 'ls()
>> ls()'
character(0)
character(0)

which works as I expect: I am running Rscript with two arguments, namely (1) '-e', and (2) two lines of code to be run, and it indeed executes those two lines of code.

This fails when attempted on a Linux build (amd64, compiled from the official 3.5.1 sources, but also reproducible with today's r-devel snapshot):

$ Rscript -e 'ls()
ls()'
ARGUMENT 'ls()' __ignored__

character(0)

This behavior is not what I expected. Have I found a bug, or am I simply using it wrong?

Regards,
Cesko

From R@iner @ending from krug@@de  Mon Sep 17 12:41:57 2018
From: R@iner @ending from krug@@de (Rainer Krug)
Date: Mon, 17 Sep 2018 12:41:57 +0200
Subject: [Rd] Rscript -e does not accept newlines under Linux?
In-Reply-To: <D14049CE02C4F54D95360EEC06CE45C50F937902@SPMXM08.VUW.leidenuniv.nl>
References: <D14049CE02C4F54D95360EEC06CE45C50F937902@SPMXM08.VUW.leidenuniv.nl>
Message-ID: <0231ED85-92E9-49DA-90F0-796608FCA123@krugs.de>

Same on Mac:

 $ Rscript -e 'ls()
> ls()'
ARGUMENT 'ls()' __ignored__

character(0)


as well as using ?\n? as a line separator:


 $ Rscript -e 'ls()\nls()'
ARGUMENT 'ls()' __ignored__

character(0)




> On 16 Sep 2018, at 10:53, Voeten, C.C. <c.c.voeten at hum.leidenuniv.nl> wrote:
> 
> Rscript -e 'ls()

--
Rainer M. Krug, PhD (Conservation Ecology, SUN), MSc (Conservation Biology, UCT), Dipl. Phys. (Germany)

University of Z?rich

Cell:       +41 (0)78 630 66 57
email:      Rainer at krugs.de
Skype:      RMkrug

PGP: 0x0F52F982




-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 488 bytes
Desc: Message signed with OpenPGP
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20180917/3247a469/attachment.sig>

From murdoch@dunc@n @ending from gm@il@com  Mon Sep 17 13:09:32 2018
From: murdoch@dunc@n @ending from gm@il@com (Duncan Murdoch)
Date: Mon, 17 Sep 2018 07:09:32 -0400
Subject: [Rd] Rscript -e does not accept newlines under Linux?
In-Reply-To: <D14049CE02C4F54D95360EEC06CE45C50F937902@SPMXM08.VUW.leidenuniv.nl>
References: <D14049CE02C4F54D95360EEC06CE45C50F937902@SPMXM08.VUW.leidenuniv.nl>
Message-ID: <c49909a1-68a9-020d-34a3-881100690326@gmail.com>

On 16/09/2018 4:53 AM, Voeten, C.C. wrote:
> Hello,
> 
> I have found what I believe to be a bug in the Linux version of the Rscript binary.
> Under Windows (official 64-bit 3.5.1 R distribution running on an up-to-date Win10), I can do the following (e.g. under powershell):
> 
> PS H:\Users\Cesko> Rscript -e 'ls()
>>> ls()'
> character(0)
> character(0)
> 
> which works as I expect: I am running Rscript with two arguments, namely (1) '-e', and (2) two lines of code to be run, and it indeed executes those two lines of code.
> 
> This fails when attempted on a Linux build (amd64, compiled from the official 3.5.1 sources, but also reproducible with today's r-devel snapshot):
> 
> $ Rscript -e 'ls()
> ls()'
> ARGUMENT 'ls()' __ignored__
> 
> character(0)
> 
> This behavior is not what I expected. Have I found a bug, or am I simply using it wrong?

I would not assume that shell behaviour in Windows and Unix would always 
be the same.  A better comparison would be to list some other command on 
the same system that behaves differently.  For example, on MacOS I see

$ echo 'ls()
 > ls()'
ls()
ls()


which suggests that what you wrote should be legal, but the form of that 
command is different: there's no equivalent of "-e".  Maybe someone else 
who knows Unix shell behaviour better can comment on whether they'd 
expect your Rscript command to work.

By the way, if you just want multiple commands to execute, you can 
separate them by semi-colons, and that does work:

$ Rscript -e 'ls(); ls()'
character(0)
character(0)

And I see this, which may explain the original problem:

$ Rscript -e 'commandArgs(); ls()'
[1] "/Library/Frameworks/R.framework/Resources/bin/exec/R"
[2] "--slave"
[3] "--no-restore"
[4] "-e"
[5] "commandArgs();~+~ls()"
character(0)

Notice that argument 5 includes both commands, whereas with the newline 
they are separated:

$ Rscript -e 'commandArgs()
 > ls()'
ARGUMENT 'ls()' __ignored__

[1] "/Library/Frameworks/R.framework/Resources/bin/exec/R"
[2] "--slave"
[3] "--no-restore"
[4] "-e"
[5] "commandArgs()"
[6] "ls()"

And finally, this also works:

Rscript -e 'ls()
-e
ls()'


Duncan Murdoch


From edd @ending from debi@n@org  Mon Sep 17 13:37:46 2018
From: edd @ending from debi@n@org (Dirk Eddelbuettel)
Date: Mon, 17 Sep 2018 06:37:46 -0500
Subject: [Rd] Rscript -e does not accept newlines under Linux?
In-Reply-To: <c49909a1-68a9-020d-34a3-881100690326@gmail.com>
References: <D14049CE02C4F54D95360EEC06CE45C50F937902@SPMXM08.VUW.leidenuniv.nl>
 <c49909a1-68a9-020d-34a3-881100690326@gmail.com>
Message-ID: <23455.37386.987213.487828@rob.eddelbuettel.com>


On 17 September 2018 at 07:09, Duncan Murdoch wrote:
| I would not assume that shell behaviour in Windows and Unix would always 
| be the same.  A better comparison would be to list some other command on 
| the same system that behaves differently.  For example, on MacOS I see
| 
| $ echo 'ls()
|  > ls()'
| ls()
| ls()
| 
| 
| which suggests that what you wrote should be legal, but the form of that 
| command is different: there's no equivalent of "-e".  Maybe someone else 
| who knows Unix shell behaviour better can comment on whether they'd 
| expect your Rscript command to work.

When we wrote littler, ie 'r', just before Rscript was added to R itself, the
ability to work from standard input just like any other Unix tool does was in
fact a design feature.  So with littler it works (and you need -p to print as
we are silent by default by another design choice)

  edd at rob:~$ (echo "ls()"; echo "ls()")
  ls()
  ls()
  edd at rob:~$ (echo "ls()"; echo "ls()") | r -p
  [1] "argv"
  [1] "argv"
  edd at rob:~$

argv is a global variable to hold the arguments (as in C).

Hence standard things work the way you expect them to:

  edd at rob:~$ echo "set.seed(123); print(summary(rnorm(1e6)))" | r 
      Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
  -4.79919 -0.67439 -0.00026 -0.00052  0.67333  4.85077 
  edd at rob:~$ 

just like r -e ... would, or R -e do now.  It is harder to do this for R and
Rscript due to the way they are invoked, setting shell variables and all that
before calling into $R_HOME/exec/bin/R.

Dirk


-- 
http://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From li@t@ @ending from revelle@net  Mon Sep 17 18:14:41 2018
From: li@t@ @ending from revelle@net (William Revelle)
Date: Mon, 17 Sep 2018 11:14:41 -0500
Subject: [Rd] diag(-1) produces weird result
Message-ID: <C9E6B4F9-DA68-457F-A99A-1D371E4BDE93@revelle.net>

Dear list

A strange bug in the psych package is due to the behavior of the diag function:

It gives the expected values for 1, a vector (-1,1), but not for -1

Is this a known feature?


> diag(1)
     [,1]
[1,]    1
> diag(c(-1,1))
     [,1] [,2]
[1,]   -1    0
[2,]    0    1
> diag(-1)
Error in diag(-1) : invalid 'nrow' value (< 0)


Bill


William Revelle		   personality-project.org/revelle.html
Professor			          personality-project.org
Department of Psychology www.wcas.northwestern.edu/psych/
Northwestern University	   www.northwestern.edu/
Use R for psychology         personality-project.org/r
It is 2   minutes to midnight   www.thebulletin.org


From murdoch@dunc@n @ending from gm@il@com  Mon Sep 17 18:22:13 2018
From: murdoch@dunc@n @ending from gm@il@com (Duncan Murdoch)
Date: Mon, 17 Sep 2018 12:22:13 -0400
Subject: [Rd] diag(-1) produces weird result
In-Reply-To: <C9E6B4F9-DA68-457F-A99A-1D371E4BDE93@revelle.net>
References: <C9E6B4F9-DA68-457F-A99A-1D371E4BDE93@revelle.net>
Message-ID: <c78c0e66-ce2a-f036-690f-a9145be1c68b@gmail.com>

On 17/09/2018 12:14 PM, William Revelle wrote:
> Dear list
> 
> A strange bug in the psych package is due to the behavior of the diag function:
> 
> It gives the expected values for 1, a vector (-1,1), but not for -1
> 
> Is this a known feature?

It is pretty clearly documented:

"diag has four distinct usages:

...

3.  x is a scalar (length-one vector) and the only argument, it returns 
a square identity matrix of size given by the scalar."

Duncan Murdoch

> 
> 
>> diag(1)
>       [,1]
> [1,]    1
>> diag(c(-1,1))
>       [,1] [,2]
> [1,]   -1    0
> [2,]    0    1
>> diag(-1)
> Error in diag(-1) : invalid 'nrow' value (< 0)
> 
> 
> Bill
> 
> 
> William Revelle		   personality-project.org/revelle.html
> Professor			          personality-project.org
> Department of Psychology www.wcas.northwestern.edu/psych/
> Northwestern University	   www.northwestern.edu/
> Use R for psychology         personality-project.org/r
> It is 2   minutes to midnight   www.thebulletin.org
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From c@@rdi@g@bor @ending from gm@il@com  Mon Sep 17 18:22:31 2018
From: c@@rdi@g@bor @ending from gm@il@com (=?UTF-8?B?R8OhYm9yIENzw6FyZGk=?=)
Date: Mon, 17 Sep 2018 17:22:31 +0100
Subject: [Rd] diag(-1) produces weird result
In-Reply-To: <C9E6B4F9-DA68-457F-A99A-1D371E4BDE93@revelle.net>
References: <C9E6B4F9-DA68-457F-A99A-1D371E4BDE93@revelle.net>
Message-ID: <CABtg=K=XPahEinrnGSRjSJDWgBSEr-ayYx69rNLxsLm2dLJDeg@mail.gmail.com>

I would say it is a mis-feature. If the 'x' argument of diag() is a
vector of length 1, then it creates an identity matrix of that size,
instead of creating a 1x1 matrix with the given value:

? diag(3)
     [,1] [,2] [,3]
[1,]    1    0    0
[2,]    0    1    0
[3,]    0    0    1

Of course this makes it cumbersome to use diag() in a package, when
you are not sure if the input vector is longer than 1. This seems to
be a good workaround:

? diag(-1, nrow = 1)
     [,1]
[1,]   -1

Or, in general if you have vector v:

? v <- -1
? diag(v, nrow = length(v))
     [,1]
[1,]   -1

Gabor
On Mon, Sep 17, 2018 at 5:14 PM William Revelle <lists at revelle.net> wrote:
>
> Dear list
>
> A strange bug in the psych package is due to the behavior of the diag function:
>
> It gives the expected values for 1, a vector (-1,1), but not for -1
>
> Is this a known feature?
>
>
> > diag(1)
>      [,1]
> [1,]    1
> > diag(c(-1,1))
>      [,1] [,2]
> [1,]   -1    0
> [2,]    0    1
> > diag(-1)
> Error in diag(-1) : invalid 'nrow' value (< 0)
>
>
> Bill
>
>
> William Revelle            personality-project.org/revelle.html
> Professor                                 personality-project.org
> Department of Psychology www.wcas.northwestern.edu/psych/
> Northwestern University    www.northwestern.edu/
> Use R for psychology         personality-project.org/r
> It is 2   minutes to midnight   www.thebulletin.org
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From b@rowling@on @ending from l@nc@@ter@@c@uk  Mon Sep 17 20:08:44 2018
From: b@rowling@on @ending from l@nc@@ter@@c@uk (Barry Rowlingson)
Date: Mon, 17 Sep 2018 19:08:44 +0100
Subject: [Rd] diag(-1) produces weird result
In-Reply-To: <8afc0f794d614405957feb241d04765f@LNXP265MB1290.GBRP265.PROD.OUTLOOK.COM>
References: <C9E6B4F9-DA68-457F-A99A-1D371E4BDE93@revelle.net>
 <8afc0f794d614405957feb241d04765f@LNXP265MB1290.GBRP265.PROD.OUTLOOK.COM>
Message-ID: <CANVKczMh3VTThqh_oaO_RysaW+yoq4d+EUNX4EN4qFSOAHd13w@mail.gmail.com>

On Mon, Sep 17, 2018 at 5:22 PM, G?bor Cs?rdi <csardi.gabor at gmail.com>
wrote:

> I would say it is a mis-feature. If the 'x' argument of diag() is a
> vector of length 1, then it creates an identity matrix of that size,
> instead of creating a 1x1 matrix with the given value:
>
> ? diag(3)
>      [,1] [,2] [,3]
> [1,]    1    0    0
> [2,]    0    1    0
> [3,]    0    0    1
>
> Of course this makes it cumbersome to use diag() in a package, when
> you are not sure if the input vector is longer than 1. This seems to
> be a good workaround:
>
> ? diag(-1, nrow = 1)
>      [,1]
> [1,]   -1
>
> Or, in general if you have vector v:
>
> ? v <- -1
> ? diag(v, nrow = length(v))
>      [,1]
> [1,]   -1
> >
>

Anyone else getting deja-vu with the `sample` function?

 > sample(5:3)
 [1] 3 5 4

ok...

 > sample(5:4)
 [1] 4 5

fine...

 > sample(5:5)
 [1] 3 1 5 2 4

uh oh. Documented, of course.

B

	[[alternative HTML version deleted]]


From r@vi@v@r@dh@n @ending from jhu@edu  Mon Sep 17 22:22:53 2018
From: r@vi@v@r@dh@n @ending from jhu@edu (Ravi Varadhan)
Date: Mon, 17 Sep 2018 20:22:53 +0000
Subject: [Rd] diag(-1) produces weird result
In-Reply-To: <CABtg=K=XPahEinrnGSRjSJDWgBSEr-ayYx69rNLxsLm2dLJDeg@mail.gmail.com>
References: <C9E6B4F9-DA68-457F-A99A-1D371E4BDE93@revelle.net>,
 <CABtg=K=XPahEinrnGSRjSJDWgBSEr-ayYx69rNLxsLm2dLJDeg@mail.gmail.com>
Message-ID: <406c611762e94f5f984cd180688fe4af@jhu.edu>

It behaves as per documentation.


" Using diag(x) can have unexpected effects if x is a vector that could be of length one. Use diag(x, nrow = length(x)) for consistent behavior."


Ravi


________________________________
From: R-devel <r-devel-bounces at r-project.org> on behalf of G?bor Cs?rdi <csardi.gabor at gmail.com>
Sent: Monday, September 17, 2018 12:22:31 PM
To: lists at revelle.net
Cc: r-devel
Subject: Re: [Rd] diag(-1) produces weird result


I would say it is a mis-feature. If the 'x' argument of diag() is a
vector of length 1, then it creates an identity matrix of that size,
instead of creating a 1x1 matrix with the given value:

? diag(3)
     [,1] [,2] [,3]
[1,]    1    0    0
[2,]    0    1    0
[3,]    0    0    1

Of course this makes it cumbersome to use diag() in a package, when
you are not sure if the input vector is longer than 1. This seems to
be a good workaround:

? diag(-1, nrow = 1)
     [,1]
[1,]   -1

Or, in general if you have vector v:

? v <- -1
? diag(v, nrow = length(v))
     [,1]
[1,]   -1

Gabor
On Mon, Sep 17, 2018 at 5:14 PM William Revelle <lists at revelle.net> wrote:
>
> Dear list
>
> A strange bug in the psych package is due to the behavior of the diag function:
>
> It gives the expected values for 1, a vector (-1,1), but not for -1
>
> Is this a known feature?
>
>
> > diag(1)
>      [,1]
> [1,]    1
> > diag(c(-1,1))
>      [,1] [,2]
> [1,]   -1    0
> [2,]    0    1
> > diag(-1)
> Error in diag(-1) : invalid 'nrow' value (< 0)
>
>
> Bill
>
>
> William Revelle            personality-project.org/revelle.html
> Professor                                 personality-project.org
> Department of Psychology www.wcas.northwestern.edu/psych/<http://www.wcas.northwestern.edu/psych/>
> Northwestern University    www.northwestern.edu/<http://www.northwestern.edu/>
> Use R for psychology         personality-project.org/r
> It is 2   minutes to midnight   www.thebulletin.org<http://www.thebulletin.org>
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

______________________________________________
R-devel at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-devel


	[[alternative HTML version deleted]]


From m@echler @ending from @t@t@m@th@ethz@ch  Tue Sep 18 11:06:17 2018
From: m@echler @ending from @t@t@m@th@ethz@ch (Martin Maechler)
Date: Tue, 18 Sep 2018 11:06:17 +0200
Subject: [Rd] Bug when calling system/system2 (and request for Bugzilla
 account)
In-Reply-To: <1A6E24FE-022E-44F1-9AB0-58B2DC8A2DB9@dans.knaw.nl>
References: <1A6E24FE-022E-44F1-9AB0-58B2DC8A2DB9@dans.knaw.nl>
Message-ID: <23456.49161.407056.397750@stat.math.ethz.ch>

>>>>> Emil Bode    on Fri, 14 Sep 2018 13:04:29 +0000 writes:

    > I found some strange behaviour, which I think is a bug. 
    > Could someone make an account for me on Bugzilla or
    > pass on my report?

I did create a bugzilla account for you (you should've got the
automatic e-mail in the mean time)...
but *DO* (all readers!) still keep in mind what we wrote carefully in

    https://www.r-project.org/bugs.html

In particular:

   Not everything that surprises you in R is a bug.
   Rather carefully read the official documentation (the help
   page at a minimum) of a function before claiming a bug. 

I've been a bit appalled about the amount of recent e-mails on
R-devel of people not having looked at the reference
documentation and claiming things...

  [......omitted..........]
  [......omitted..........]

and you did note and report a real bug indeed,
thank you !!

Martin


From pd@lgd @ending from gm@il@com  Tue Sep 18 15:00:29 2018
From: pd@lgd @ending from gm@il@com (peter dalgaard)
Date: Tue, 18 Sep 2018 15:00:29 +0200
Subject: [Rd] diag(-1) produces weird result
In-Reply-To: <CANVKczMh3VTThqh_oaO_RysaW+yoq4d+EUNX4EN4qFSOAHd13w@mail.gmail.com>
References: <C9E6B4F9-DA68-457F-A99A-1D371E4BDE93@revelle.net>
 <8afc0f794d614405957feb241d04765f@LNXP265MB1290.GBRP265.PROD.OUTLOOK.COM>
 <CANVKczMh3VTThqh_oaO_RysaW+yoq4d+EUNX4EN4qFSOAHd13w@mail.gmail.com>
Message-ID: <CD2E0BDE-66DD-4374-B4A2-028B47B7ED07@gmail.com>

Yes, both are rooted in age-old design infelicities (in which, basically, interactive expedience has taken precedence over consistency and generality). 

Unfortunately, they are quite difficult to rectify, because there are bound to be countless uses of, say, diag(5) as a 5x5 identity matrix which would break if it suddenly meant the 1x1 matrix(5) instead.

We'd need a very carefully orchestrated warn-deprecate-defunct-newBehaviour sequence, with a time scale of years, most likely. It is, in principle, doable (I think), but we don't really have the mechanisms to follow through on it.  Almost all developers have main job responsibilities, and it is very easy to get sidetracked at the wrong moment, so most changes get done on a now-or-never basis. 

I have toyed with the idea of setting up for version-dependent code where code sections could be coded up front and then activated when the relevant version is reached. Then I got swamped again...

-pd

> On 17 Sep 2018, at 20:08 , Barry Rowlingson <b.rowlingson at lancaster.ac.uk> wrote:
> 
> On Mon, Sep 17, 2018 at 5:22 PM, G?bor Cs?rdi <csardi.gabor at gmail.com>
> wrote:
> 
>> I would say it is a mis-feature. If the 'x' argument of diag() is a
>> vector of length 1, then it creates an identity matrix of that size,
>> instead of creating a 1x1 matrix with the given value:
>> 
>> ? diag(3)
>>     [,1] [,2] [,3]
>> [1,]    1    0    0
>> [2,]    0    1    0
>> [3,]    0    0    1
>> 
>> Of course this makes it cumbersome to use diag() in a package, when
>> you are not sure if the input vector is longer than 1. This seems to
>> be a good workaround:
>> 
>> ? diag(-1, nrow = 1)
>>     [,1]
>> [1,]   -1
>> 
>> Or, in general if you have vector v:
>> 
>> ? v <- -1
>> ? diag(v, nrow = length(v))
>>     [,1]
>> [1,]   -1
>>> 
>> 
> 
> Anyone else getting deja-vu with the `sample` function?
> 
>> sample(5:3)
> [1] 3 5 4
> 
> ok...
> 
>> sample(5:4)
> [1] 4 5
> 
> fine...
> 
>> sample(5:5)
> [1] 3 1 5 2 4
> 
> uh oh. Documented, of course.
> 
> B
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From c@@rdi@g@bor @ending from gm@il@com  Tue Sep 18 15:07:31 2018
From: c@@rdi@g@bor @ending from gm@il@com (=?UTF-8?B?R8OhYm9yIENzw6FyZGk=?=)
Date: Tue, 18 Sep 2018 14:07:31 +0100
Subject: [Rd] diag(-1) produces weird result
In-Reply-To: <CD2E0BDE-66DD-4374-B4A2-028B47B7ED07@gmail.com>
References: <C9E6B4F9-DA68-457F-A99A-1D371E4BDE93@revelle.net>
 <8afc0f794d614405957feb241d04765f@LNXP265MB1290.GBRP265.PROD.OUTLOOK.COM>
 <CANVKczMh3VTThqh_oaO_RysaW+yoq4d+EUNX4EN4qFSOAHd13w@mail.gmail.com>
 <CD2E0BDE-66DD-4374-B4A2-028B47B7ED07@gmail.com>
Message-ID: <CABtg=Kn_kPTTwsmg1iRcrvDFm94FB32yXmTHAL38J1trj1_24Q@mail.gmail.com>

Some languages that recognized early design mistakes introduced a
strict mode, which applies to a local context, and enforces a safer,
more consistent API. This is a pragmatic solution, as it does not
require any changes to existing code, but still allows writing better
code in the future.

Implementing a context restricted strict mode is probably not trivial,
considering the flexibility of the language, but maybe it is worth
some thinking.

G?bor
On Tue, Sep 18, 2018 at 2:00 PM peter dalgaard <pdalgd at gmail.com> wrote:
>
> Yes, both are rooted in age-old design infelicities (in which, basically, interactive expedience has taken precedence over consistency and generality).
>
> Unfortunately, they are quite difficult to rectify, because there are bound to be countless uses of, say, diag(5) as a 5x5 identity matrix which would break if it suddenly meant the 1x1 matrix(5) instead.
>
> We'd need a very carefully orchestrated warn-deprecate-defunct-newBehaviour sequence, with a time scale of years, most likely. It is, in principle, doable (I think), but we don't really have the mechanisms to follow through on it.  Almost all developers have main job responsibilities, and it is very easy to get sidetracked at the wrong moment, so most changes get done on a now-or-never basis.
>
> I have toyed with the idea of setting up for version-dependent code where code sections could be coded up front and then activated when the relevant version is reached. Then I got swamped again...
>
> -pd
>
> > On 17 Sep 2018, at 20:08 , Barry Rowlingson <b.rowlingson at lancaster.ac.uk> wrote:
> >
> > On Mon, Sep 17, 2018 at 5:22 PM, G?bor Cs?rdi <csardi.gabor at gmail.com>
> > wrote:
> >
> >> I would say it is a mis-feature. If the 'x' argument of diag() is a
> >> vector of length 1, then it creates an identity matrix of that size,
> >> instead of creating a 1x1 matrix with the given value:
> >>
> >> ? diag(3)
> >>     [,1] [,2] [,3]
> >> [1,]    1    0    0
> >> [2,]    0    1    0
> >> [3,]    0    0    1
> >>
> >> Of course this makes it cumbersome to use diag() in a package, when
> >> you are not sure if the input vector is longer than 1. This seems to
> >> be a good workaround:
> >>
> >> ? diag(-1, nrow = 1)
> >>     [,1]
> >> [1,]   -1
> >>
> >> Or, in general if you have vector v:
> >>
> >> ? v <- -1
> >> ? diag(v, nrow = length(v))
> >>     [,1]
> >> [1,]   -1
> >>>
> >>
> >
> > Anyone else getting deja-vu with the `sample` function?
> >
> >> sample(5:3)
> > [1] 3 5 4
> >
> > ok...
> >
> >> sample(5:4)
> > [1] 4 5
> >
> > fine...
> >
> >> sample(5:5)
> > [1] 3 1 5 2 4
> >
> > uh oh. Documented, of course.
> >
> > B
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
>
>
>
>
>
>
>
>


From jori@mey@ @ending from gm@il@com  Tue Sep 18 17:28:11 2018
From: jori@mey@ @ending from gm@il@com (Joris Meys)
Date: Tue, 18 Sep 2018 17:28:11 +0200
Subject: [Rd] memory footprint of readRDS()
Message-ID: <CAO1zAVYNvp6ykmEAqdXx9rGZyniu=3ih8MfgWD1VZ5ty+OB6zw@mail.gmail.com>

Dear all,

I tried to read in a 3.8Gb RDS file on a computer with 16Gb available
memory. To my astonishment, the memory footprint of R rises quickly to over
13Gb and the attempt ends with an error that says "cannot allocate vector
of size 5.8Gb".

I would expect that 3 times the memory would be enough to read in that
file, but apparently I was wrong. I checked the memory.limit() and that one
gave me a value of more than 13Gb. So I wondered if this was to be
expected, or if there could be an underlying reason why this file doesn't
want to open.

Thank you in advance
Joris

-- 
Joris Meys
Statistical consultant

Department of Data Analysis and Mathematical Modelling
Ghent University
Coupure Links 653, B-9000 Gent (Belgium)
<https://maps.google.com/?q=Coupure+links+653,%C2%A0B-9000+Gent,%C2%A0Belgium&entry=gmail&source=g>

-----------
Biowiskundedagen 2017-2018
http://www.biowiskundedagen.ugent.be/

-------------------------------
Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php

	[[alternative HTML version deleted]]


From bri@n @ending from br@verock@com  Tue Sep 18 17:30:55 2018
From: bri@n @ending from br@verock@com (Brian G. Peterson)
Date: Tue, 18 Sep 2018 10:30:55 -0500
Subject: [Rd] memory footprint of readRDS()
In-Reply-To: <CAO1zAVYNvp6ykmEAqdXx9rGZyniu=3ih8MfgWD1VZ5ty+OB6zw@mail.gmail.com>
References: <CAO1zAVYNvp6ykmEAqdXx9rGZyniu=3ih8MfgWD1VZ5ty+OB6zw@mail.gmail.com>
Message-ID: <1537284655.21577.22.camel@braverock.com>

Your RDS file is likely compressed, and could have compression of 10x
or more depending on the composition of the data that is in it and the
compression method used. 'gzip' compression is used by default.

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock

On Tue, 2018-09-18 at 17:28 +0200, Joris Meys wrote:
> Dear all,
> 
> I tried to read in a 3.8Gb RDS file on a computer with 16Gb available
> memory. To my astonishment, the memory footprint of R rises quickly
> to over 13Gb and the attempt ends with an error that says "cannot
> allocate vector of size 5.8Gb".
> 
> I would expect that 3 times the memory would be enough to read in
> that file, but apparently I was wrong. I checked the memory.limit()
> and that one gave me a value of more than 13Gb. So I wondered if this
> was to be expected, or if there could be an underlying reason why
> this file doesn't want to open.
> 
> Thank you in advance
> Joris
>


From wdunl@p @ending from tibco@com  Tue Sep 18 18:16:13 2018
From: wdunl@p @ending from tibco@com (William Dunlap)
Date: Tue, 18 Sep 2018 09:16:13 -0700
Subject: [Rd] memory footprint of readRDS()
In-Reply-To: <CAO1zAVYNvp6ykmEAqdXx9rGZyniu=3ih8MfgWD1VZ5ty+OB6zw@mail.gmail.com>
References: <CAO1zAVYNvp6ykmEAqdXx9rGZyniu=3ih8MfgWD1VZ5ty+OB6zw@mail.gmail.com>
Message-ID: <CAF8bMcZ9uBogRjiHSjD5-KMa7Eo+eJdU3jpuo9uJkXrfuR36fg@mail.gmail.com>

The ratio of object size to rds file size depends on the object.  Some
variation is due to how header information is stored in memory and in the
file but I suspect most is due to how compression works (e.g., a vector of
repeated values can be compressed into a smaller file than a bunch of
random bytes).

f <- function (data, ...)  {
    force(data)
    tf <- tempfile()
    on.exit(unlink(tf))
    save(data, file = tf)
    c(`obj/file size` = as.numeric(object.size(data)/file.size(tf)))
}

> f(rep(0,1e6))
obj/file size
     1021.456
> f(rep(0,1e6), compress=FALSE)
obj/file size
    0.9999986
> f(rep(89.7,1e6))
obj/file size
     682.6555
> f(log(1:1e6))
obj/file size
     1.309126
> f(vector("list",1e6))
obj/file size
     2021.744
> f(as.list(log(1:1e6)))
obj/file size
     8.907579
> f(sample(as.raw(0:255),size=8e6,replace=TRUE))
obj/file size
    0.9998433
> f(rep(as.raw(0:255),length=8e6))
obj/file size
     254.5595
> f(as.character(1:1e6))
obj/file size
      23.5567



Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Tue, Sep 18, 2018 at 8:28 AM, Joris Meys <jorismeys at gmail.com> wrote:

> Dear all,
>
> I tried to read in a 3.8Gb RDS file on a computer with 16Gb available
> memory. To my astonishment, the memory footprint of R rises quickly to over
> 13Gb and the attempt ends with an error that says "cannot allocate vector
> of size 5.8Gb".
>
> I would expect that 3 times the memory would be enough to read in that
> file, but apparently I was wrong. I checked the memory.limit() and that one
> gave me a value of more than 13Gb. So I wondered if this was to be
> expected, or if there could be an underlying reason why this file doesn't
> want to open.
>
> Thank you in advance
> Joris
>
> --
> Joris Meys
> Statistical consultant
>
> Department of Data Analysis and Mathematical Modelling
> Ghent University
> Coupure Links 653, B-9000 Gent (Belgium)
> <https://maps.google.com/?q=Coupure+links+653,%C2%A0B-
> 9000+Gent,%C2%A0Belgium&entry=gmail&source=g>
>
> -----------
> Biowiskundedagen 2017-2018
> http://www.biowiskundedagen.ugent.be/
>
> -------------------------------
> Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From jori@mey@ @ending from gm@il@com  Tue Sep 18 18:23:34 2018
From: jori@mey@ @ending from gm@il@com (Joris Meys)
Date: Tue, 18 Sep 2018 18:23:34 +0200
Subject: [Rd] memory footprint of readRDS()
In-Reply-To: <CAF8bMcZ9uBogRjiHSjD5-KMa7Eo+eJdU3jpuo9uJkXrfuR36fg@mail.gmail.com>
References: <CAO1zAVYNvp6ykmEAqdXx9rGZyniu=3ih8MfgWD1VZ5ty+OB6zw@mail.gmail.com>
 <CAF8bMcZ9uBogRjiHSjD5-KMa7Eo+eJdU3jpuo9uJkXrfuR36fg@mail.gmail.com>
Message-ID: <CAO1zAVYKOwG36Sqj9F3as3pk_6x3xcmCTDdCr1WSwZxyqEin-A@mail.gmail.com>

Thx William and Brian for your swift responses, very insightful. I'll have
to hunt for more memory.

Cheers
Joris

On Tue, Sep 18, 2018 at 6:16 PM William Dunlap <wdunlap at tibco.com> wrote:

> The ratio of object size to rds file size depends on the object.  Some
> variation is due to how header information is stored in memory and in the
> file but I suspect most is due to how compression works (e.g., a vector of
> repeated values can be compressed into a smaller file than a bunch of
> random bytes).
>
> f <- function (data, ...)  {
>     force(data)
>     tf <- tempfile()
>     on.exit(unlink(tf))
>     save(data, file = tf)
>     c(`obj/file size` = as.numeric(object.size(data)/file.size(tf)))
> }
>
> > f(rep(0,1e6))
> obj/file size
>      1021.456
> > f(rep(0,1e6), compress=FALSE)
> obj/file size
>     0.9999986
> > f(rep(89.7,1e6))
> obj/file size
>      682.6555
> > f(log(1:1e6))
> obj/file size
>      1.309126
> > f(vector("list",1e6))
> obj/file size
>      2021.744
> > f(as.list(log(1:1e6)))
> obj/file size
>      8.907579
> > f(sample(as.raw(0:255),size=8e6,replace=TRUE))
> obj/file size
>     0.9998433
> > f(rep(as.raw(0:255),length=8e6))
> obj/file size
>      254.5595
> > f(as.character(1:1e6))
> obj/file size
>       23.5567
>
>
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
> On Tue, Sep 18, 2018 at 8:28 AM, Joris Meys <jorismeys at gmail.com> wrote:
>
>> Dear all,
>>
>> I tried to read in a 3.8Gb RDS file on a computer with 16Gb available
>> memory. To my astonishment, the memory footprint of R rises quickly to
>> over
>> 13Gb and the attempt ends with an error that says "cannot allocate vector
>> of size 5.8Gb".
>>
>> I would expect that 3 times the memory would be enough to read in that
>> file, but apparently I was wrong. I checked the memory.limit() and that
>> one
>> gave me a value of more than 13Gb. So I wondered if this was to be
>> expected, or if there could be an underlying reason why this file doesn't
>> want to open.
>>
>> Thank you in advance
>> Joris
>>
>> --
>> Joris Meys
>> Statistical consultant
>>
>> Department of Data Analysis and Mathematical Modelling
>> Ghent University
>> Coupure Links 653, B-9000 Gent (Belgium)
>> <
>> https://maps.google.com/?q=Coupure+links+653,%C2%A0B-9000+Gent,%C2%A0Belgium&entry=gmail&source=g
>> >
>>
>> -----------
>> Biowiskundedagen 2017-2018
>> http://www.biowiskundedagen.ugent.be/
>>
>> -------------------------------
>> Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>
>

-- 
Joris Meys
Statistical consultant

Department of Data Analysis and Mathematical Modelling
Ghent University
Coupure Links 653, B-9000 Gent (Belgium)
<https://maps.google.com/?q=Coupure+links+653,%C2%A0B-9000+Gent,%C2%A0Belgium&entry=gmail&source=g>

-----------
Biowiskundedagen 2017-2018
http://www.biowiskundedagen.ugent.be/

-------------------------------
Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php

	[[alternative HTML version deleted]]


From M@rcel@R@mo@ @ending from ro@wellp@rk@org  Tue Sep 18 20:16:27 2018
From: M@rcel@R@mo@ @ending from ro@wellp@rk@org (Marcel Ramos)
Date: Tue, 18 Sep 2018 18:16:27 +0000
Subject: [Rd] Suggested Patch: Adding commas to list of packages after R CMD
 check
Message-ID: <7e1e0dfa-714f-aab8-1011-4d32fcb1c5e6@roswellpark.org>

Dear R-devs,


Scenario:

When checking a package via `R CMD check package_tar.ball`, required / suggested packages may be missing. R subsequently returns a list of packages that are missing (delimited by spaces).

Example:

```
R CMD check glmSparseNet_0.99.13.tar.gz
* using log directory '/home/ubuntu/Bioconductor/glmSparseNet.Rcheck'
* using R Under development (unstable) (2018-06-06 r74855)
* using platform: x86_64-pc-linux-gnu (64-bit)
* using session charset: UTF-8
* checking for file 'glmSparseNet/DESCRIPTION' ... OK
* checking extension type ... Package
* this is package 'glmSparseNet' version '0.99.13'
* package encoding: UTF-8
* checking package namespace information ... OK
* checking package dependencies ... ERROR
Packages required but not available:
  'MultiAssayExperiment' 'glmnet' 'SummarizedExperiment' 'STRINGdb'
  'biomaRt' 'futile.logger' 'sparsebn' 'sparsebnUtils' 'forcats'
  'dplyr' 'readr' 'ggplot2' 'ggfortify' 'reshape2' 'rlang' 'loose.rock'

Packages suggested but not available:
  'testthat' 'knitr' 'rmarkdown' 'survcomp' 'pROC' 'devtools'
  'VennDiagram' 'BiocStyle' 'curatedTCGAData'

VignetteBuilder package required for checking but not installed: 'knitr'

The suggested packages are required for a complete check.
Checking can be attempted without them by setting the environment
variable _R_CHECK_FORCE_SUGGESTS_ to a false value.

See section 'The DESCRIPTION file' in the 'Writing R Extensions'
manual.
* DONE

Status: 1 ERROR
See
  '/home/ubuntu/Bioconductor/glmSparseNet.Rcheck/00check.log'
for details.
```


Suggested Patch:

To return a list of missing dependencies delimited by a comma and a space (", ") so to make it easier for the user to copy and paste this list.
This would be especially helpful when the list of missing dependencies is extensive.


Example output:

```
R CMD check glmSparseNet_0.99.13.tar.gz
* using log directory '/home/ubuntu/Bioconductor/glmSparseNet.Rcheck'
* using R Under development (unstable) (2018-09-18 r75322)
* using platform: x86_64-pc-linux-gnu (64-bit)
* using session charset: UTF-8
* checking for file 'glmSparseNet/DESCRIPTION' ... OK
* checking extension type ... Package
* this is package 'glmSparseNet' version '0.99.13'
* package encoding: UTF-8
* checking package namespace information ... OK
* checking package dependencies ... ERROR
Packages required but not available:
  'MultiAssayExperiment', 'glmnet', 'SummarizedExperiment', 'STRINGdb',
  'biomaRt', 'futile.logger', 'sparsebn', 'sparsebnUtils', 'forcats',
  'dplyr', 'readr', 'ggplot2', 'ggfortify', 'reshape2', 'stringr',
  'rlang', 'loose.rock'

Packages suggested but not available:
  'testthat', 'knitr', 'rmarkdown', 'survcomp', 'pROC', 'devtools',
  'roxygen2', 'VennDiagram', 'BiocStyle', 'curatedTCGAData'

VignetteBuilder package required for checking but not installed: 'knitr'

The suggested packages are required for a complete check.
Checking can be attempted without them by setting the environment
variable _R_CHECK_FORCE_SUGGESTS_ to a false value.

See section 'The DESCRIPTION file' in the 'Writing R Extensions'
manual.
* DONE

Status: 1 ERROR
See
  '/home/ubuntu/Bioconductor/glmSparseNet.Rcheck/00check.log'
for details.
```


svn diff:


Index: src/library/tools/R/QC.R
===================================================================
--- src/library/tools/R/QC.R    (revision 75322)
+++ src/library/tools/R/QC.R    (working copy)
@@ -8536,13 +8536,13 @@
 .pretty_format <-
 function(x)
 {
-    strwrap(paste(sQuote(x), collapse = " "),
+    strwrap(paste(sQuote(x), collapse = ", "),
             indent = 2L, exdent = 2L)
 }
 .pretty_format2 <-
 function(msg, x)
 {
-    xx <- strwrap(paste(sQuote(x), collapse = " "), exdent = 2L)
+    xx <- strwrap(paste(sQuote(x), collapse = ", "), exdent = 2L)
     if (length(xx) > 1L || nchar(msg) + nchar(xx) + 1L > 75L)
         c(msg, .pretty_format(x))
     else paste(msg, xx)



PS. I would also advocate for setting `useFancyQuotes` to `FALSE` by default but it would be better to get more
input from the community.

Thanks!


Best regards,

Marcel

--
Marcel Ramos
Bioconductor Core Team
Roswell Park Comprehensive Care Center
Dept. of Biostatistics & Bioinformatics
Elm & Carlton Streets
Buffalo, New York 14263


This email message may contain legally privileged and/or confidential information.  If you are not the intended recipient(s), or the employee or agent responsible for the delivery of this message to the intended recipient(s), you are hereby notified that any disclosure, copying, distribution, or use of this email message is prohibited.  If you have received this message in error, please notify the sender immediately by e-mail and delete this email message from your computer. Thank you.
	[[alternative HTML version deleted]]


From murdoch@dunc@n @ending from gm@il@com  Tue Sep 18 22:23:47 2018
From: murdoch@dunc@n @ending from gm@il@com (Duncan Murdoch)
Date: Tue, 18 Sep 2018 16:23:47 -0400
Subject: [Rd] 
 Suggested Patch: Adding commas to list of packages after R CMD check
In-Reply-To: <7e1e0dfa-714f-aab8-1011-4d32fcb1c5e6@roswellpark.org>
References: <7e1e0dfa-714f-aab8-1011-4d32fcb1c5e6@roswellpark.org>
Message-ID: <5d8e2a34-4f71-6002-8e50-09fed8a74c97@gmail.com>

On 18/09/2018 2:16 PM, Marcel Ramos wrote:
> Dear R-devs,
> 
> 
> Scenario:
> 
> When checking a package via `R CMD check package_tar.ball`, required / suggested packages may be missing. R subsequently returns a list of packages that are missing (delimited by spaces).
> 
> Example:
> 
> ```
> R CMD check glmSparseNet_0.99.13.tar.gz
> * using log directory '/home/ubuntu/Bioconductor/glmSparseNet.Rcheck'
> * using R Under development (unstable) (2018-06-06 r74855)
> * using platform: x86_64-pc-linux-gnu (64-bit)
> * using session charset: UTF-8
> * checking for file 'glmSparseNet/DESCRIPTION' ... OK
> * checking extension type ... Package
> * this is package 'glmSparseNet' version '0.99.13'
> * package encoding: UTF-8
> * checking package namespace information ... OK
> * checking package dependencies ... ERROR
> Packages required but not available:
>    'MultiAssayExperiment' 'glmnet' 'SummarizedExperiment' 'STRINGdb'
>    'biomaRt' 'futile.logger' 'sparsebn' 'sparsebnUtils' 'forcats'
>    'dplyr' 'readr' 'ggplot2' 'ggfortify' 'reshape2' 'rlang' 'loose.rock'
> 
> Packages suggested but not available:
>    'testthat' 'knitr' 'rmarkdown' 'survcomp' 'pROC' 'devtools'
>    'VennDiagram' 'BiocStyle' 'curatedTCGAData'
> 
> VignetteBuilder package required for checking but not installed: 'knitr'
> 
> The suggested packages are required for a complete check.
> Checking can be attempted without them by setting the environment
> variable _R_CHECK_FORCE_SUGGESTS_ to a false value.
> 
> See section 'The DESCRIPTION file' in the 'Writing R Extensions'
> manual.
> * DONE
> 
> Status: 1 ERROR
> See
>    '/home/ubuntu/Bioconductor/glmSparseNet.Rcheck/00check.log'
> for details.
> ```
> 
> 
> Suggested Patch:
> 
> To return a list of missing dependencies delimited by a comma and a space (", ") so to make it easier for the user to copy and paste this list.
> This would be especially helpful when the list of missing dependencies is extensive.

This seems like a reasonable suggestion, considering how easy it is to do.

Another suggestion would be to (optionally) automatically install 
missing dependencies.  I think the devtools::install_deps function will 
do that.  (I don't use it, I have a homebrewed function for that purpose.)

Duncan Murdoch


> 
> 
> Example output:
> 
> ```
> R CMD check glmSparseNet_0.99.13.tar.gz
> * using log directory '/home/ubuntu/Bioconductor/glmSparseNet.Rcheck'
> * using R Under development (unstable) (2018-09-18 r75322)
> * using platform: x86_64-pc-linux-gnu (64-bit)
> * using session charset: UTF-8
> * checking for file 'glmSparseNet/DESCRIPTION' ... OK
> * checking extension type ... Package
> * this is package 'glmSparseNet' version '0.99.13'
> * package encoding: UTF-8
> * checking package namespace information ... OK
> * checking package dependencies ... ERROR
> Packages required but not available:
>    'MultiAssayExperiment', 'glmnet', 'SummarizedExperiment', 'STRINGdb',
>    'biomaRt', 'futile.logger', 'sparsebn', 'sparsebnUtils', 'forcats',
>    'dplyr', 'readr', 'ggplot2', 'ggfortify', 'reshape2', 'stringr',
>    'rlang', 'loose.rock'
> 
> Packages suggested but not available:
>    'testthat', 'knitr', 'rmarkdown', 'survcomp', 'pROC', 'devtools',
>    'roxygen2', 'VennDiagram', 'BiocStyle', 'curatedTCGAData'
> 
> VignetteBuilder package required for checking but not installed: 'knitr'
> 
> The suggested packages are required for a complete check.
> Checking can be attempted without them by setting the environment
> variable _R_CHECK_FORCE_SUGGESTS_ to a false value.
> 
> See section 'The DESCRIPTION file' in the 'Writing R Extensions'
> manual.
> * DONE
> 
> Status: 1 ERROR
> See
>    '/home/ubuntu/Bioconductor/glmSparseNet.Rcheck/00check.log'
> for details.
> ```
> 
> 
> svn diff:
> 
> 
> Index: src/library/tools/R/QC.R
> ===================================================================
> --- src/library/tools/R/QC.R    (revision 75322)
> +++ src/library/tools/R/QC.R    (working copy)
> @@ -8536,13 +8536,13 @@
>   .pretty_format <-
>   function(x)
>   {
> -    strwrap(paste(sQuote(x), collapse = " "),
> +    strwrap(paste(sQuote(x), collapse = ", "),
>               indent = 2L, exdent = 2L)
>   }
>   .pretty_format2 <-
>   function(msg, x)
>   {
> -    xx <- strwrap(paste(sQuote(x), collapse = " "), exdent = 2L)
> +    xx <- strwrap(paste(sQuote(x), collapse = ", "), exdent = 2L)
>       if (length(xx) > 1L || nchar(msg) + nchar(xx) + 1L > 75L)
>           c(msg, .pretty_format(x))
>       else paste(msg, xx)
> 
> 
> 
> PS. I would also advocate for setting `useFancyQuotes` to `FALSE` by default but it would be better to get more
> input from the community.
> 
> Thanks!
> 
> 
> Best regards,
> 
> Marcel
> 
> --
> Marcel Ramos
> Bioconductor Core Team
> Roswell Park Comprehensive Care Center
> Dept. of Biostatistics & Bioinformatics
> Elm & Carlton Streets
> Buffalo, New York 14263
> 
> 
> This email message may contain legally privileged and/or confidential information.  If you are not the intended recipient(s), or the employee or agent responsible for the delivery of this message to the intended recipient(s), you are hereby notified that any disclosure, copying, distribution, or use of this email message is prohibited.  If you have received this message in error, please notify the sender immediately by e-mail and delete this email message from your computer. Thank you.
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From p@ul @ending from @t@t@@uckl@nd@@c@nz  Tue Sep 18 23:02:13 2018
From: p@ul @ending from @t@t@@uckl@nd@@c@nz (Paul Murrell)
Date: Wed, 19 Sep 2018 09:02:13 +1200
Subject: [Rd] [FORGED] Possible bug with chromatic adaptation in
 grDevices::convertColor
In-Reply-To: <754180083.2444264.1536890615931@mail.yahoo.com>
References: <754180083.2444264.1536890615931.ref@mail.yahoo.com>
 <754180083.2444264.1536890615931@mail.yahoo.com>
Message-ID: <9d2fcee5-6c8c-5faf-fe38-346cf3da4f10@stat.auckland.ac.nz>


Thanks for the report (and the bug report).
I need to make time to have a closer look at this.

Paul

On 14/09/18 14:03, brodie gaslam via R-devel wrote:
> It appears that the chromatic adaptation feature of `grDevices::convertColor`is broken, and likely has been for many years.? While a little surprising, it is an obscure enough feature that there is some possibility this is actually broken, as opposed to user error on my part.? If it turns out to the latter, I apologize in advance for spamming this list.
> Consider:
>  ??? rgb.in <- c("#CCCCCC", "#EEEEEE")??? clr <- t(col2rgb(rgb.in)) / 255??? clr.out <- convertColor(clr, "sRGB", "sRGB")??? rgb(clr.out)??? ## [1] "#CCCCCC" "#EEEEEE"
>  ??? convertColor(clr, "sRGB", "sRGB", "D65", "D50")??? ## Error in match.arg(from, nWhite) :??? ##?? 'arg' must be NULL or a character vector
> This appears to be because `grDevices:::chromaticAdaptation` expects the whitepoints to be provided in the character format (e.g. "D65"), but they are already converted by `convertColor` into the tristimulus values.? After applying the patch at the end of this e-mail, we get:
>  ??? clr.out <- convertColor(clr, "sRGB", "sRGB", "D65", "D50")??? rgb(clr.out)??? ## [1] "#DACAB0" "#FEECCE"
> I do not have a great way of confirming that the conversion is correct with my changes, but I did verify that the `M` matrix computed within`grDevics:::chromaticAdaptation` for the "D65"->"D50" conversion (approximately) matches the corresponding matrix from brucelindbloom.com chromatic adaptation page:
> http://www.brucelindbloom.com/Eqn_ChromAdapt.html
> Additionally visual inspection via
>   ??? scales::show_col(c(rgb.in, rgb(clr.out)))
> is consistent with a shift from bluer indirect daylight ("D65") to yellower direct daylight ("D50") illuminant.
> It is worth noting that the adaption method currently in`grDevices:::chromaticAdaptation` appears to be the "Von Kries" method, not the "Bradford" method as documented in `?convertColor` and in the comments of thesources.? I base this on comparing the cone response domain matrices on the aforementioned brucelindbloom.com page to the `Ma` matrix defined in`grDevics:::chromaticAdaptation`.
> Given that brucelindbloom.com appears to recommend "Bradford", that the sources suggest that was the intent, that `chromaticAdaptation` is only used by`convertColor` in the R sources, that `chromaticAdapation` is not exported, and that that feature appears currently inaccessible via `convertColor`, it may be worth using this opportunity to change the adaptation method to "Bradford".
> A suggested patch follows.? It is intended to minimize the required changes, although doing so requires a double transposition.? The transpositions could be easily avoided, but it would require reformulating the calculations in`chromaticAdaption`.
> Best,
> Brodie.
> 
> Index: src/library/grDevices/R/convertColor.R
> ===================================================================
> --- src/library/grDevices/R/convertColor.R?? ?(revision 75298)
> +++ src/library/grDevices/R/convertColor.R?? ?(working copy)
> @@ -81,7 +81,7 @@
>  ?}
>   
>  ?chromaticAdaptation <- function(xyz, from, to) {
> -??? ## bradford scaling algorithm
> +??? ## Von Kries scaling algorithm
>  ???? Ma <- matrix(c( 0.40024, -0.22630, 0.,
>  ???????????????????? 0.70760,? 1.16532, 0.,
>  ??????????????????? -0.08081,? 0.04570, 0.91822), nrow = 3L, byrow = TRUE)
> @@ -242,8 +242,8 @@
>  ?? if (is.null(from.ref.white))
>  ?????? from.ref.white <- to.ref.white
>   
> -? from.ref.white <- c2to3(white.points[, from.ref.white])
> -? to.ref.white?? <- c2to3(white.points[, to.ref.white])
> +? from.ref.white.3 <- c2to3(white.points[, from.ref.white])
> +? to.ref.white.3?? <- c2to3(white.points[, to.ref.white])
>   
>  ?? if (is.null(nrow(color)))
>  ???? color <- matrix(color, nrow = 1L)
> @@ -262,19 +262,19 @@
>  ?????? rgb
>  ?? }
>   
> -? xyz <- apply(color, 1L, from$toXYZ, from.ref.white)
> +? xyz <- apply(color, 1L, from$toXYZ, from.ref.white.3)
>   
>  ?? if (is.null(nrow(xyz)))
>  ???? xyz <- matrix(xyz, nrow = 1L)
>   
> -? if (!isTRUE(all.equal(from.ref.white, to.ref.white))) {
> +? if (!isTRUE(all.equal(from.ref.white.3, to.ref.white.3))) {
>  ?????? mc <- match.call()
>  ?????? if (is.null(mc$from.ref.white) || is.null(mc$to.ref.white))
>  ?????????? warning("color spaces use different reference whites")
> -????? xyz <- chromaticAdaptation(xyz, from.ref.white, to.ref.white)
> +????? xyz <- t(chromaticAdaptation(t(xyz), from.ref.white, to.ref.white))
>  ?? }
>   
> -? rval <- apply(xyz, 2L, to$fromXYZ, to.ref.white)
> +? rval <- apply(xyz, 2L, to$fromXYZ, to.ref.white.3)
>   
>  ?? if (inherits(to,"RGBcolorConverter"))
>  ?????? rval <- trim(rval)
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 

-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/


From cboettig @ending from berkeley@edu  Tue Sep 18 23:46:42 2018
From: cboettig @ending from berkeley@edu (Carl Boettiger)
Date: Tue, 18 Sep 2018 14:46:42 -0700
Subject: [Rd] Bias in R's random integers?
Message-ID: <CAN_1p9wVX=AR8aYtC2LJo0bgN9cRW3XcbYJqnVHdCzA-QA1ceg@mail.gmail.com>

Dear list,

It looks to me that R samples random integers using an intuitive but biased
algorithm by going from a random number on [0,1) from the PRNG to a random
integer, e.g.
https://github.com/wch/r-source/blob/tags/R-3-5-1/src/main/RNG.c#L808

Many other languages use various rejection sampling approaches which
provide an unbiased method for sampling, such as in Go, python, and others
described here:  https://arxiv.org/abs/1805.10941 (I believe the biased
algorithm currently used in R is also described there).  I'm not an expert
in this area, but does it make sense for the R to adopt one of the unbiased
random sample algorithms outlined there and used in other languages?  Would
a patch providing such an algorithm be welcome? What concerns would need to
be addressed first?

I believe this issue was also raised by Killie & Philip in
http://r.789695.n4.nabble.com/Bug-in-sample-td4729483.html, and more
recently in
https://www.stat.berkeley.edu/~stark/Preprints/r-random-issues.pdf,
pointing to the python implementation for comparison:
https://github.com/statlab/cryptorandom/blob/master/cryptorandom/cryptorandom.py#L265

Thanks!

Carl
-- 

http://carlboettiger.info

	[[alternative HTML version deleted]]


From murdoch@dunc@n @ending from gm@il@com  Wed Sep 19 14:43:26 2018
From: murdoch@dunc@n @ending from gm@il@com (Duncan Murdoch)
Date: Wed, 19 Sep 2018 08:43:26 -0400
Subject: [Rd] Bias in R's random integers?
In-Reply-To: <CAN_1p9wVX=AR8aYtC2LJo0bgN9cRW3XcbYJqnVHdCzA-QA1ceg@mail.gmail.com>
References: <CAN_1p9wVX=AR8aYtC2LJo0bgN9cRW3XcbYJqnVHdCzA-QA1ceg@mail.gmail.com>
Message-ID: <ffb74021-8d95-a820-8013-f0e51dc1a9b3@gmail.com>

On 18/09/2018 5:46 PM, Carl Boettiger wrote:
> Dear list,
> 
> It looks to me that R samples random integers using an intuitive but biased
> algorithm by going from a random number on [0,1) from the PRNG to a random
> integer, e.g.
> https://github.com/wch/r-source/blob/tags/R-3-5-1/src/main/RNG.c#L808
> 
> Many other languages use various rejection sampling approaches which
> provide an unbiased method for sampling, such as in Go, python, and others
> described here:  https://arxiv.org/abs/1805.10941 (I believe the biased
> algorithm currently used in R is also described there).  I'm not an expert
> in this area, but does it make sense for the R to adopt one of the unbiased
> random sample algorithms outlined there and used in other languages?  Would
> a patch providing such an algorithm be welcome? What concerns would need to
> be addressed first?
> 
> I believe this issue was also raised by Killie & Philip in
> http://r.789695.n4.nabble.com/Bug-in-sample-td4729483.html, and more
> recently in
> https://www.stat.berkeley.edu/~stark/Preprints/r-random-issues.pdf,
> pointing to the python implementation for comparison:
> https://github.com/statlab/cryptorandom/blob/master/cryptorandom/cryptorandom.py#L265

I think the analyses are correct, but I doubt if a change to the default 
is likely to be accepted as it would make it more difficult to reproduce 
older results.

On the other hand, a contribution of a new function like sample() but 
not suffering from the bias would be good.  The normal way to make such 
a contribution is in a user contributed package.

By the way, R code illustrating the bias is probably not very hard to 
put together.  I believe the bias manifests itself in sample() producing 
values with two different probabilities (instead of all equal 
probabilities).  Those may differ by as much as one part in 2^32.  It's 
very difficult to detect a probability difference that small, but if you 
define the partition of values into the high probability values vs the 
low probability values, you can probably detect the difference in a 
feasible simulation.

Duncan Murdoch


From iuc@r @ending from fedor@project@org  Wed Sep 19 15:09:47 2018
From: iuc@r @ending from fedor@project@org (=?UTF-8?Q?I=C3=B1aki_Ucar?=)
Date: Wed, 19 Sep 2018 15:09:47 +0200
Subject: [Rd] Bias in R's random integers?
In-Reply-To: <ffb74021-8d95-a820-8013-f0e51dc1a9b3@gmail.com>
References: <CAN_1p9wVX=AR8aYtC2LJo0bgN9cRW3XcbYJqnVHdCzA-QA1ceg@mail.gmail.com>
 <ffb74021-8d95-a820-8013-f0e51dc1a9b3@gmail.com>
Message-ID: <CALEXWq3booRRTSYvYqcjV7qPZGOC1oh5o45b5o=J2ypQi5s-ag@mail.gmail.com>

El mi?., 19 sept. 2018 a las 14:43, Duncan Murdoch
(<murdoch.duncan at gmail.com>) escribi?:
>
> On 18/09/2018 5:46 PM, Carl Boettiger wrote:
> > Dear list,
> >
> > It looks to me that R samples random integers using an intuitive but biased
> > algorithm by going from a random number on [0,1) from the PRNG to a random
> > integer, e.g.
> > https://github.com/wch/r-source/blob/tags/R-3-5-1/src/main/RNG.c#L808
> >
> > Many other languages use various rejection sampling approaches which
> > provide an unbiased method for sampling, such as in Go, python, and others
> > described here:  https://arxiv.org/abs/1805.10941 (I believe the biased
> > algorithm currently used in R is also described there).  I'm not an expert
> > in this area, but does it make sense for the R to adopt one of the unbiased
> > random sample algorithms outlined there and used in other languages?  Would
> > a patch providing such an algorithm be welcome? What concerns would need to
> > be addressed first?
> >
> > I believe this issue was also raised by Killie & Philip in
> > http://r.789695.n4.nabble.com/Bug-in-sample-td4729483.html, and more
> > recently in
> > https://www.stat.berkeley.edu/~stark/Preprints/r-random-issues.pdf,
> > pointing to the python implementation for comparison:
> > https://github.com/statlab/cryptorandom/blob/master/cryptorandom/cryptorandom.py#L265
>
> I think the analyses are correct, but I doubt if a change to the default
> is likely to be accepted as it would make it more difficult to reproduce
> older results.
>
> On the other hand, a contribution of a new function like sample() but
> not suffering from the bias would be good.  The normal way to make such
> a contribution is in a user contributed package.
>
> By the way, R code illustrating the bias is probably not very hard to
> put together.  I believe the bias manifests itself in sample() producing
> values with two different probabilities (instead of all equal
> probabilities).  Those may differ by as much as one part in 2^32.  It's

According to Kellie and Philip, in the attachment of the thread
referenced by Carl, "The maximum ratio of selection probabilities can
get as large as 1.5 if n is just below 2^31".

I?aki

> very difficult to detect a probability difference that small, but if you
> define the partition of values into the high probability values vs the
> low probability values, you can probably detect the difference in a
> feasible simulation.
>
> Duncan Murdoch
>


From c@gille@pie @ending from gm@il@com  Wed Sep 19 15:15:40 2018
From: c@gille@pie @ending from gm@il@com (Colin Gillespie)
Date: Wed, 19 Sep 2018 14:15:40 +0100
Subject: [Rd] R-admin typo
Message-ID: <CADbDLZn+HzuN_8+VYq8vd0M2X+G3EvdSSihRO3+FZUcWp9rMNg@mail.gmail.com>

Hi,

Section 3.2 of the R-admin manual

https://cran.ma.imperial.ac.uk/doc/manuals/r-release/R-admin.html#Testing-a-Windows-Installation

could be improved. The particular sentence is

The Rtools are not needed to run these tests. but more comprehensive
analysis of errors will be given if diff is in the path (and errorsAreFatal
= FALSE is then not needed below).

Thanks

Colin

	[[alternative HTML version deleted]]


From d@vidhughjone@ @ending from gm@il@com  Wed Sep 19 15:40:28 2018
From: d@vidhughjone@ @ending from gm@il@com (David Hugh-Jones)
Date: Wed, 19 Sep 2018 14:40:28 +0100
Subject: [Rd] Bias in R's random integers?
In-Reply-To: <ffb74021-8d95-a820-8013-f0e51dc1a9b3@gmail.com>
References: <CAN_1p9wVX=AR8aYtC2LJo0bgN9cRW3XcbYJqnVHdCzA-QA1ceg@mail.gmail.com>
 <ffb74021-8d95-a820-8013-f0e51dc1a9b3@gmail.com>
Message-ID: <CAARY7khVi==mZs5o_8fZkTUDDuY-ABwE=gA45COX7qbRgBLhsg@mail.gmail.com>

On Wed, 19 Sep 2018 at 13:43, Duncan Murdoch <murdoch.duncan at gmail.com>
wrote:

>
> I think the analyses are correct, but I doubt if a change to the default
> is likely to be accepted as it would make it more difficult to reproduce
> older results.


I'm a bit alarmed by the logic here. Unbiased sampling seems basic for a
statistical language. As a consumer of R I'd like to think that e.g. my
bootstrapped p values are correct.
Surely if the old results depend on the biased algorithm, then they are
false results?
-- 
Sent from Gmail Mobile

	[[alternative HTML version deleted]]


From bbolker @ending from gm@il@com  Wed Sep 19 16:03:35 2018
From: bbolker @ending from gm@il@com (Ben Bolker)
Date: Wed, 19 Sep 2018 10:03:35 -0400
Subject: [Rd] Bias in R's random integers?
In-Reply-To: <CAARY7khVi==mZs5o_8fZkTUDDuY-ABwE=gA45COX7qbRgBLhsg@mail.gmail.com>
References: <CAN_1p9wVX=AR8aYtC2LJo0bgN9cRW3XcbYJqnVHdCzA-QA1ceg@mail.gmail.com>
 <ffb74021-8d95-a820-8013-f0e51dc1a9b3@gmail.com>
 <CAARY7khVi==mZs5o_8fZkTUDDuY-ABwE=gA45COX7qbRgBLhsg@mail.gmail.com>
Message-ID: <3d7901f0-6b06-89a9-d841-738d2a8307ec@gmail.com>



On 2018-09-19 09:40 AM, David Hugh-Jones wrote:
> On Wed, 19 Sep 2018 at 13:43, Duncan Murdoch <murdoch.duncan at gmail.com>
> wrote:
> 
>>
>> I think the analyses are correct, but I doubt if a change to the default
>> is likely to be accepted as it would make it more difficult to reproduce
>> older results.
> 
> 
> I'm a bit alarmed by the logic here. Unbiased sampling seems basic for a
> statistical language. As a consumer of R I'd like to think that e.g. my
> bootstrapped p values are correct.
> Surely if the old results depend on the biased algorithm, then they are
> false results?
> 

   Balancing backward compatibility and correctness is a tough problem
here.  If this goes into base R, what's the best way to do it?  What was
the protocol for migrating away from the "buggy Kinderman-Ramage"
generator, back in the day?   (Version 1.7 was sometime between 2001 and
2004).

  I couldn't find the exact commit in the GitHub mirror: this is related ...

https://github.com/wch/r-source/commit/7ad3044639fd1fe093c655e573fd1a67aa7f55f6#diff-dbcad570d4fb9b7005550ff630543b37



===
?normal.kind? can be ?"Kinderman-Ramage"?, ?"Buggy
     Kinderman-Ramage"? (not for ?set.seed?), ?"Ahrens-Dieter"?,
     ?"Box-Muller"?, ?"Inversion"? (the default), or ?"user-supplied"?.
     (For inversion, see the reference in ?qnorm?.)  The
     Kinderman-Ramage generator used in versions prior to 1.7.0 (now
     called ?"Buggy"?) had several approximation errors and should only
     be used for reproduction of old results.


From tom@@@k@liber@ @ending from gm@il@com  Wed Sep 19 16:22:59 2018
From: tom@@@k@liber@ @ending from gm@il@com (Tomas Kalibera)
Date: Wed, 19 Sep 2018 07:22:59 -0700
Subject: [Rd] R-admin typo
In-Reply-To: <CADbDLZn+HzuN_8+VYq8vd0M2X+G3EvdSSihRO3+FZUcWp9rMNg@mail.gmail.com>
References: <CADbDLZn+HzuN_8+VYq8vd0M2X+G3EvdSSihRO3+FZUcWp9rMNg@mail.gmail.com>
Message-ID: <ae43825a-fd45-1b37-c732-8a5b1b331425@gmail.com>

Thanks, Tomas

On 09/19/2018 06:15 AM, Colin Gillespie wrote:
> Hi,
>
> Section 3.2 of the R-admin manual
>
> https://cran.ma.imperial.ac.uk/doc/manuals/r-release/R-admin.html#Testing-a-Windows-Installation
>
> could be improved. The particular sentence is
>
> The Rtools are not needed to run these tests. but more comprehensive
> analysis of errors will be given if diff is in the path (and errorsAreFatal
> = FALSE is then not needed below).
>
> Thanks
>
> Colin
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From murdoch@dunc@n @ending from gm@il@com  Wed Sep 19 18:05:33 2018
From: murdoch@dunc@n @ending from gm@il@com (Duncan Murdoch)
Date: Wed, 19 Sep 2018 12:05:33 -0400
Subject: [Rd] Bias in R's random integers?
In-Reply-To: <CALEXWq3booRRTSYvYqcjV7qPZGOC1oh5o45b5o=J2ypQi5s-ag@mail.gmail.com>
References: <CAN_1p9wVX=AR8aYtC2LJo0bgN9cRW3XcbYJqnVHdCzA-QA1ceg@mail.gmail.com>
 <ffb74021-8d95-a820-8013-f0e51dc1a9b3@gmail.com>
 <CALEXWq3booRRTSYvYqcjV7qPZGOC1oh5o45b5o=J2ypQi5s-ag@mail.gmail.com>
Message-ID: <ece52410-ddb2-2198-9291-201475d3f46e@gmail.com>

On 19/09/2018 9:09 AM, I?aki Ucar wrote:
> El mi?., 19 sept. 2018 a las 14:43, Duncan Murdoch
> (<murdoch.duncan at gmail.com>) escribi?:
>>
>> On 18/09/2018 5:46 PM, Carl Boettiger wrote:
>>> Dear list,
>>>
>>> It looks to me that R samples random integers using an intuitive but biased
>>> algorithm by going from a random number on [0,1) from the PRNG to a random
>>> integer, e.g.
>>> https://github.com/wch/r-source/blob/tags/R-3-5-1/src/main/RNG.c#L808
>>>
>>> Many other languages use various rejection sampling approaches which
>>> provide an unbiased method for sampling, such as in Go, python, and others
>>> described here:  https://arxiv.org/abs/1805.10941 (I believe the biased
>>> algorithm currently used in R is also described there).  I'm not an expert
>>> in this area, but does it make sense for the R to adopt one of the unbiased
>>> random sample algorithms outlined there and used in other languages?  Would
>>> a patch providing such an algorithm be welcome? What concerns would need to
>>> be addressed first?
>>>
>>> I believe this issue was also raised by Killie & Philip in
>>> http://r.789695.n4.nabble.com/Bug-in-sample-td4729483.html, and more
>>> recently in
>>> https://www.stat.berkeley.edu/~stark/Preprints/r-random-issues.pdf,
>>> pointing to the python implementation for comparison:
>>> https://github.com/statlab/cryptorandom/blob/master/cryptorandom/cryptorandom.py#L265
>>
>> I think the analyses are correct, but I doubt if a change to the default
>> is likely to be accepted as it would make it more difficult to reproduce
>> older results.
>>
>> On the other hand, a contribution of a new function like sample() but
>> not suffering from the bias would be good.  The normal way to make such
>> a contribution is in a user contributed package.
>>
>> By the way, R code illustrating the bias is probably not very hard to
>> put together.  I believe the bias manifests itself in sample() producing
>> values with two different probabilities (instead of all equal
>> probabilities).  Those may differ by as much as one part in 2^32.  It's
> 
> According to Kellie and Philip, in the attachment of the thread
> referenced by Carl, "The maximum ratio of selection probabilities can
> get as large as 1.5 if n is just below 2^31".

Sorry, I didn't write very well.  I meant to say that the difference in 
probabilities would be 2^-32, not that the ratio of probabilities would 
be 1 + 2^-32.

By the way, I don't see the statement giving the ratio as 1.5, but maybe 
I was looking in the wrong place.  In Theorem 1 of the paper I was 
looking in the ratio was "1 + m 2^{-w + 1}".  In that formula m is your 
n.  If it is near 2^31, R uses w = 57 random bits, so the ratio would be 
very, very small (one part in 2^25).

The worst case for R would happen when m  is just below  2^25, where w 
is at least 31 for the default generators.  In that case the ratio could 
be about 1.03.

Duncan Murdoch


From murdoch@dunc@n @ending from gm@il@com  Wed Sep 19 18:15:39 2018
From: murdoch@dunc@n @ending from gm@il@com (Duncan Murdoch)
Date: Wed, 19 Sep 2018 12:15:39 -0400
Subject: [Rd] Bias in R's random integers?
In-Reply-To: <CAARY7khVi==mZs5o_8fZkTUDDuY-ABwE=gA45COX7qbRgBLhsg@mail.gmail.com>
References: <CAN_1p9wVX=AR8aYtC2LJo0bgN9cRW3XcbYJqnVHdCzA-QA1ceg@mail.gmail.com>
 <ffb74021-8d95-a820-8013-f0e51dc1a9b3@gmail.com>
 <CAARY7khVi==mZs5o_8fZkTUDDuY-ABwE=gA45COX7qbRgBLhsg@mail.gmail.com>
Message-ID: <997228e9-7b3f-0ab9-6541-b347de00b40c@gmail.com>

On 19/09/2018 9:40 AM, David Hugh-Jones wrote:
> 
> 
> On Wed, 19 Sep 2018 at 13:43, Duncan Murdoch <murdoch.duncan at gmail.com 
> <mailto:murdoch.duncan at gmail.com>> wrote:
> 
> 
>     I think the analyses are correct, but I doubt if a change to the
>     default
>     is likely to be accepted as it would make it more difficult to
>     reproduce
>     older results.
> 
> 
> I'm a bit alarmed by the logic here. Unbiased sampling seems basic for a 
> statistical language. As a consumer of R I'd like to think that e.g. my 
> bootstrapped p values are correct.
> Surely if the old results depend on the biased algorithm, then they are 
> false results?

All Monte Carlo results contain Monte Carlo error.  Using the biased 
function will have some additional error, but for almost all 
simulations, it will be negligible compared to the Monte Carlo error.  I 
suspect the only simulations where the bias was anywhere near the same 
order of magnitude as the Monte Carlo error would be ones designed with 
this specific code in mind.

Duncan Murdoch


From murdoch@dunc@n @ending from gm@il@com  Wed Sep 19 18:20:38 2018
From: murdoch@dunc@n @ending from gm@il@com (Duncan Murdoch)
Date: Wed, 19 Sep 2018 12:20:38 -0400
Subject: [Rd] Bias in R's random integers?
In-Reply-To: <CAA7KnHHEejYVn8NtKrY1KuKcJUq8n4LoJSDx9BZv-LJyC=r9yQ@mail.gmail.com>
References: <CAN_1p9wVX=AR8aYtC2LJo0bgN9cRW3XcbYJqnVHdCzA-QA1ceg@mail.gmail.com>
 <ffb74021-8d95-a820-8013-f0e51dc1a9b3@gmail.com>
 <CALEXWq3booRRTSYvYqcjV7qPZGOC1oh5o45b5o=J2ypQi5s-ag@mail.gmail.com>
 <ece52410-ddb2-2198-9291-201475d3f46e@gmail.com>
 <CAA7KnHHEejYVn8NtKrY1KuKcJUq8n4LoJSDx9BZv-LJyC=r9yQ@mail.gmail.com>
Message-ID: <73c4667d-38ad-f5ef-468c-4c368e0fdc12@gmail.com>

On 19/09/2018 12:09 PM, Philip B. Stark wrote:
> The 53 bits only encode at most 2^{32} possible values, because the 
> source of the float is the output of a 32-bit PRNG (the obsolete version 
> of MT). 53 bits isn't the relevant number here.

No, two calls to unif_rand() are used.  There are two 32 bit values, but 
some of the bits are thrown away.

Duncan Murdoch

> 
> The selection ratios can get close to 2. Computer scientists don't do it 
> the way R does, for a reason.
> 
> Regards,
> Philip
> 
> On Wed, Sep 19, 2018 at 9:05 AM Duncan Murdoch <murdoch.duncan at gmail.com 
> <mailto:murdoch.duncan at gmail.com>> wrote:
> 
>     On 19/09/2018 9:09 AM, I?aki Ucar wrote:
>      > El mi?., 19 sept. 2018 a las 14:43, Duncan Murdoch
>      > (<murdoch.duncan at gmail.com <mailto:murdoch.duncan at gmail.com>>)
>     escribi?:
>      >>
>      >> On 18/09/2018 5:46 PM, Carl Boettiger wrote:
>      >>> Dear list,
>      >>>
>      >>> It looks to me that R samples random integers using an
>     intuitive but biased
>      >>> algorithm by going from a random number on [0,1) from the PRNG
>     to a random
>      >>> integer, e.g.
>      >>>
>     https://github.com/wch/r-source/blob/tags/R-3-5-1/src/main/RNG.c#L808
>      >>>
>      >>> Many other languages use various rejection sampling approaches
>     which
>      >>> provide an unbiased method for sampling, such as in Go, python,
>     and others
>      >>> described here: https://arxiv.org/abs/1805.10941 (I believe the
>     biased
>      >>> algorithm currently used in R is also described there).? I'm
>     not an expert
>      >>> in this area, but does it make sense for the R to adopt one of
>     the unbiased
>      >>> random sample algorithms outlined there and used in other
>     languages?? Would
>      >>> a patch providing such an algorithm be welcome? What concerns
>     would need to
>      >>> be addressed first?
>      >>>
>      >>> I believe this issue was also raised by Killie & Philip in
>      >>> http://r.789695.n4.nabble.com/Bug-in-sample-td4729483.html, and
>     more
>      >>> recently in
>      >>>
>     https://www.stat.berkeley.edu/~stark/Preprints/r-random-issues.pdf
>     <https://www.stat.berkeley.edu/%7Estark/Preprints/r-random-issues.pdf>,
>      >>> pointing to the python implementation for comparison:
>      >>>
>     https://github.com/statlab/cryptorandom/blob/master/cryptorandom/cryptorandom.py#L265
>      >>
>      >> I think the analyses are correct, but I doubt if a change to the
>     default
>      >> is likely to be accepted as it would make it more difficult to
>     reproduce
>      >> older results.
>      >>
>      >> On the other hand, a contribution of a new function like
>     sample() but
>      >> not suffering from the bias would be good.? The normal way to
>     make such
>      >> a contribution is in a user contributed package.
>      >>
>      >> By the way, R code illustrating the bias is probably not very
>     hard to
>      >> put together.? I believe the bias manifests itself in sample()
>     producing
>      >> values with two different probabilities (instead of all equal
>      >> probabilities).? Those may differ by as much as one part in
>     2^32.? It's
>      >
>      > According to Kellie and Philip, in the attachment of the thread
>      > referenced by Carl, "The maximum ratio of selection probabilities can
>      > get as large as 1.5 if n is just below 2^31".
> 
>     Sorry, I didn't write very well.? I meant to say that the difference in
>     probabilities would be 2^-32, not that the ratio of probabilities would
>     be 1 + 2^-32.
> 
>     By the way, I don't see the statement giving the ratio as 1.5, but
>     maybe
>     I was looking in the wrong place.? In Theorem 1 of the paper I was
>     looking in the ratio was "1 + m 2^{-w + 1}".? In that formula m is your
>     n.? If it is near 2^31, R uses w = 57 random bits, so the ratio
>     would be
>     very, very small (one part in 2^25).
> 
>     The worst case for R would happen when m? is just below? 2^25, where w
>     is at least 31 for the default generators.? In that case the ratio
>     could
>     be about 1.03.
> 
>     Duncan Murdoch
> 
> 
> 
> -- 
> Philip B. Stark | Associate Dean, Mathematical and Physical Sciences | 
> Professor, ?Department of Statistics |
> University of California
> Berkeley, CA 94720-3860 | 510-394-5077 | statistics.berkeley.edu/~stark 
> <http://statistics.berkeley.edu/%7Estark> |
> @philipbstark
>


From murdoch@dunc@n @ending from gm@il@com  Wed Sep 19 18:50:40 2018
From: murdoch@dunc@n @ending from gm@il@com (Duncan Murdoch)
Date: Wed, 19 Sep 2018 12:50:40 -0400
Subject: [Rd] Bias in R's random integers?
In-Reply-To: <CAA7KnHFcY5YJ2kTAfPJJ0PHwcWx7CumE49A4NgJWC2NK5CV=-A@mail.gmail.com>
References: <CAN_1p9wVX=AR8aYtC2LJo0bgN9cRW3XcbYJqnVHdCzA-QA1ceg@mail.gmail.com>
 <ffb74021-8d95-a820-8013-f0e51dc1a9b3@gmail.com>
 <CALEXWq3booRRTSYvYqcjV7qPZGOC1oh5o45b5o=J2ypQi5s-ag@mail.gmail.com>
 <ece52410-ddb2-2198-9291-201475d3f46e@gmail.com>
 <CAA7KnHHEejYVn8NtKrY1KuKcJUq8n4LoJSDx9BZv-LJyC=r9yQ@mail.gmail.com>
 <73c4667d-38ad-f5ef-468c-4c368e0fdc12@gmail.com>
 <CAA7KnHFcY5YJ2kTAfPJJ0PHwcWx7CumE49A4NgJWC2NK5CV=-A@mail.gmail.com>
Message-ID: <c3590da8-2e92-4e46-3a81-0d0dbfc81845@gmail.com>

On 19/09/2018 12:23 PM, Philip B. Stark wrote:
> No, the 2nd call only happens when m > 2**31. Here's the code:

Yes, you're right. Sorry!

So the ratio really does come close to 2.  However, the difference in 
probabilities between outcomes is still at most 2^-32 when m is less 
than that cutoff.  That's not feasible to detect; the only detectable 
difference would happen if some event was constructed to hold an 
abundance of outcomes with especially low (or especially high) probability.

As I said in my original post, it's probably not hard to construct such 
a thing, but as I've said more recently, it probably wouldn't happen by 
chance.  Here's one attempt to do it:

Call the values from unif_rand() "the unif_rand() outcomes".  Call the 
values from sample() the sample outcomes.

It would be easiest to see the error if half of the sample() outcomes 
used two unif_rand() outcomes, and half used just one.  That would mean 
m should be (2/3) * 2^32, but that's too big and would trigger the other 
version.

So how about half use 2 unif_rands(), and half use 3?  That means m = 
(2/5) * 2^32 = 1717986918.  A good guess is that sample() outcomes would 
alternate between the two possibilities, so our event could be even 
versus odd outcomes.

Let's try it:

 > m <- (2/5)*2^32
 > m > 2^31
[1] FALSE
 > x <- sample(m, 1000000, replace = TRUE)
 > table(x %% 2)

      0      1
399850 600150

Since m is an even number, the true proportions of evens and odds should 
be exactly 0.5.  That's some pretty strong evidence of the bug in the 
generator.  (Note that the ratio of the observed probabilities is about 
1.5, so I may not be the first person to have done this.)

I'm still not convinced that there has ever been a simulation run with 
detectable bias compared to Monte Carlo error unless it (like this one) 
was designed specifically to show the problem.

Duncan Murdoch

> 
> (RNG.c, lines 793ff)
> 
> double R_unif_index(double dn)
> {
>  ? ? double cut = INT_MAX;
> 
>  ? ? switch(RNG_kind) {
>  ? ? case KNUTH_TAOCP:
>  ? ? case USER_UNIF:
>  ? ? case KNUTH_TAOCP2:
> cut = 33554431.0; /* 2^25 - 1 */
> break;
>  ? ? default:
> break;
>  ? ?}
> 
>  ? ? double u = dn > cut ? ru() : unif_rand();
>  ? ? return floor(dn * u);
> }
> 
> On Wed, Sep 19, 2018 at 9:20 AM Duncan Murdoch <murdoch.duncan at gmail.com 
> <mailto:murdoch.duncan at gmail.com>> wrote:
> 
>     On 19/09/2018 12:09 PM, Philip B. Stark wrote:
>      > The 53 bits only encode at most 2^{32} possible values, because the
>      > source of the float is the output of a 32-bit PRNG (the obsolete
>     version
>      > of MT). 53 bits isn't the relevant number here.
> 
>     No, two calls to unif_rand() are used.? There are two 32 bit values,
>     but
>     some of the bits are thrown away.
> 
>     Duncan Murdoch
> 
>      >
>      > The selection ratios can get close to 2. Computer scientists
>     don't do it
>      > the way R does, for a reason.
>      >
>      > Regards,
>      > Philip
>      >
>      > On Wed, Sep 19, 2018 at 9:05 AM Duncan Murdoch
>     <murdoch.duncan at gmail.com <mailto:murdoch.duncan at gmail.com>
>      > <mailto:murdoch.duncan at gmail.com
>     <mailto:murdoch.duncan at gmail.com>>> wrote:
>      >
>      >? ? ?On 19/09/2018 9:09 AM, I?aki Ucar wrote:
>      >? ? ? > El mi?., 19 sept. 2018 a las 14:43, Duncan Murdoch
>      >? ? ? > (<murdoch.duncan at gmail.com
>     <mailto:murdoch.duncan at gmail.com> <mailto:murdoch.duncan at gmail.com
>     <mailto:murdoch.duncan at gmail.com>>>)
>      >? ? ?escribi?:
>      >? ? ? >>
>      >? ? ? >> On 18/09/2018 5:46 PM, Carl Boettiger wrote:
>      >? ? ? >>> Dear list,
>      >? ? ? >>>
>      >? ? ? >>> It looks to me that R samples random integers using an
>      >? ? ?intuitive but biased
>      >? ? ? >>> algorithm by going from a random number on [0,1) from
>     the PRNG
>      >? ? ?to a random
>      >? ? ? >>> integer, e.g.
>      >? ? ? >>>
>      > https://github.com/wch/r-source/blob/tags/R-3-5-1/src/main/RNG.c#L808
>      >? ? ? >>>
>      >? ? ? >>> Many other languages use various rejection sampling
>     approaches
>      >? ? ?which
>      >? ? ? >>> provide an unbiased method for sampling, such as in Go,
>     python,
>      >? ? ?and others
>      >? ? ? >>> described here: https://arxiv.org/abs/1805.10941 (I
>     believe the
>      >? ? ?biased
>      >? ? ? >>> algorithm currently used in R is also described there).? I'm
>      >? ? ?not an expert
>      >? ? ? >>> in this area, but does it make sense for the R to adopt
>     one of
>      >? ? ?the unbiased
>      >? ? ? >>> random sample algorithms outlined there and used in other
>      >? ? ?languages?? Would
>      >? ? ? >>> a patch providing such an algorithm be welcome? What
>     concerns
>      >? ? ?would need to
>      >? ? ? >>> be addressed first?
>      >? ? ? >>>
>      >? ? ? >>> I believe this issue was also raised by Killie & Philip in
>      >? ? ? >>>
>     http://r.789695.n4.nabble.com/Bug-in-sample-td4729483.html, and
>      >? ? ?more
>      >? ? ? >>> recently in
>      >? ? ? >>>
>      >
>     https://www.stat.berkeley.edu/~stark/Preprints/r-random-issues.pdf
>     <https://www.stat.berkeley.edu/%7Estark/Preprints/r-random-issues.pdf>
>      >   
>      ?<https://www.stat.berkeley.edu/%7Estark/Preprints/r-random-issues.pdf>,
>      >? ? ? >>> pointing to the python implementation for comparison:
>      >? ? ? >>>
>      >
>     https://github.com/statlab/cryptorandom/blob/master/cryptorandom/cryptorandom.py#L265
>      >? ? ? >>
>      >? ? ? >> I think the analyses are correct, but I doubt if a change
>     to the
>      >? ? ?default
>      >? ? ? >> is likely to be accepted as it would make it more
>     difficult to
>      >? ? ?reproduce
>      >? ? ? >> older results.
>      >? ? ? >>
>      >? ? ? >> On the other hand, a contribution of a new function like
>      >? ? ?sample() but
>      >? ? ? >> not suffering from the bias would be good.? The normal way to
>      >? ? ?make such
>      >? ? ? >> a contribution is in a user contributed package.
>      >? ? ? >>
>      >? ? ? >> By the way, R code illustrating the bias is probably not very
>      >? ? ?hard to
>      >? ? ? >> put together.? I believe the bias manifests itself in
>     sample()
>      >? ? ?producing
>      >? ? ? >> values with two different probabilities (instead of all equal
>      >? ? ? >> probabilities).? Those may differ by as much as one part in
>      >? ? ?2^32.? It's
>      >? ? ? >
>      >? ? ? > According to Kellie and Philip, in the attachment of the
>     thread
>      >? ? ? > referenced by Carl, "The maximum ratio of selection
>     probabilities can
>      >? ? ? > get as large as 1.5 if n is just below 2^31".
>      >
>      >? ? ?Sorry, I didn't write very well.? I meant to say that the
>     difference in
>      >? ? ?probabilities would be 2^-32, not that the ratio of
>     probabilities would
>      >? ? ?be 1 + 2^-32.
>      >
>      >? ? ?By the way, I don't see the statement giving the ratio as
>     1.5, but
>      >? ? ?maybe
>      >? ? ?I was looking in the wrong place.? In Theorem 1 of the paper
>     I was
>      >? ? ?looking in the ratio was "1 + m 2^{-w + 1}".? In that formula
>     m is your
>      >? ? ?n.? If it is near 2^31, R uses w = 57 random bits, so the ratio
>      >? ? ?would be
>      >? ? ?very, very small (one part in 2^25).
>      >
>      >? ? ?The worst case for R would happen when m? is just below 
>     2^25, where w
>      >? ? ?is at least 31 for the default generators.? In that case the
>     ratio
>      >? ? ?could
>      >? ? ?be about 1.03.
>      >
>      >? ? ?Duncan Murdoch
>      >
>      >
>      >
>      > --
>      > Philip B. Stark | Associate Dean, Mathematical and Physical
>     Sciences |
>      > Professor, ?Department of Statistics |
>      > University of California
>      > Berkeley, CA 94720-3860 | 510-394-5077 |
>     statistics.berkeley.edu/~stark
>     <http://statistics.berkeley.edu/%7Estark>
>      > <http://statistics.berkeley.edu/%7Estark> |
>      > @philipbstark
>      >
> 
> 
> 
> -- 
> Philip B. Stark | Associate Dean, Mathematical and Physical Sciences | 
> Professor, ?Department of Statistics |
> University of California
> Berkeley, CA 94720-3860 | 510-394-5077 | statistics.berkeley.edu/~stark 
> <http://statistics.berkeley.edu/%7Estark> |
> @philipbstark
>


From murdoch@dunc@n @ending from gm@il@com  Wed Sep 19 19:57:31 2018
From: murdoch@dunc@n @ending from gm@il@com (Duncan Murdoch)
Date: Wed, 19 Sep 2018 13:57:31 -0400
Subject: [Rd] A different error in sample()
Message-ID: <5b6950ce-ac7a-4f6a-da61-8e70d25bb08c@gmail.com>

This may be a doc error or a coding bug.

The help page for sample says:

"Non-integer positive numerical values of n or x will be truncated to 
the next smallest integer, which has to be no larger than 
.Machine$integer.max."

This is not true:

 > table(sample(2.5, 1000000, replace = TRUE))

      1      2      3
399933 399716 200351

We shouldn't have those 3's if truncation of x had occurred.

Duncan Murdoch

 > sessionInfo()
R version 3.5.1 (2018-07-02)
Platform: x86_64-apple-darwin15.6.0 (64-bit)
Running under: macOS High Sierra 10.13.6

Matrix products: default
BLAS: 
/Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
LAPACK: 
/Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib

locale:
[1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

loaded via a namespace (and not attached):
[1] compiler_3.5.1 tools_3.5.1


From jori@mey@ @ending from gm@il@com  Wed Sep 19 20:18:50 2018
From: jori@mey@ @ending from gm@il@com (Joris Meys)
Date: Wed, 19 Sep 2018 20:18:50 +0200
Subject: [Rd] A different error in sample()
In-Reply-To: <5b6950ce-ac7a-4f6a-da61-8e70d25bb08c@gmail.com>
References: <5b6950ce-ac7a-4f6a-da61-8e70d25bb08c@gmail.com>
Message-ID: <CAO1zAVbHXAz9o7uf98Q5pUm7R8pSHFigAAtQn90FrGdPfqUn1A@mail.gmail.com>

I believe the word "truncated" is causing the confusion. 3 is "the next
smallest integer" following 2.5. But it is not the truncation done by
trunc(). Rewording to "rounding the next smallest integer" would get rid of
that confusion imho.

Cheers
Joris

On Wed, Sep 19, 2018 at 7:57 PM Duncan Murdoch <murdoch.duncan at gmail.com>
wrote:

> This may be a doc error or a coding bug.
>
> The help page for sample says:
>
> "Non-integer positive numerical values of n or x will be truncated to
> the next smallest integer, which has to be no larger than
> .Machine$integer.max."
>
> This is not true:
>
>  > table(sample(2.5, 1000000, replace = TRUE))
>
>       1      2      3
> 399933 399716 200351
>
> We shouldn't have those 3's if truncation of x had occurred.
>
> Duncan Murdoch
>
>  > sessionInfo()
> R version 3.5.1 (2018-07-02)
> Platform: x86_64-apple-darwin15.6.0 (64-bit)
> Running under: macOS High Sierra 10.13.6
>
> Matrix products: default
> BLAS:
> /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
> LAPACK:
> /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
>
> locale:
> [1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> loaded via a namespace (and not attached):
> [1] compiler_3.5.1 tools_3.5.1
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


-- 
Joris Meys
Statistical consultant

Department of Data Analysis and Mathematical Modelling
Ghent University
Coupure Links 653, B-9000 Gent (Belgium)
<https://maps.google.com/?q=Coupure+links+653,%C2%A0B-9000+Gent,%C2%A0Belgium&entry=gmail&source=g>

-----------
Biowiskundedagen 2017-2018
http://www.biowiskundedagen.ugent.be/

-------------------------------
Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php

	[[alternative HTML version deleted]]


From @t@rk @ending from @t@t@berkeley@edu  Wed Sep 19 18:09:09 2018
From: @t@rk @ending from @t@t@berkeley@edu (Philip B. Stark)
Date: Wed, 19 Sep 2018 09:09:09 -0700
Subject: [Rd] Bias in R's random integers?
In-Reply-To: <ece52410-ddb2-2198-9291-201475d3f46e@gmail.com>
References: <CAN_1p9wVX=AR8aYtC2LJo0bgN9cRW3XcbYJqnVHdCzA-QA1ceg@mail.gmail.com>
 <ffb74021-8d95-a820-8013-f0e51dc1a9b3@gmail.com>
 <CALEXWq3booRRTSYvYqcjV7qPZGOC1oh5o45b5o=J2ypQi5s-ag@mail.gmail.com>
 <ece52410-ddb2-2198-9291-201475d3f46e@gmail.com>
Message-ID: <CAA7KnHHEejYVn8NtKrY1KuKcJUq8n4LoJSDx9BZv-LJyC=r9yQ@mail.gmail.com>

The 53 bits only encode at most 2^{32} possible values, because the source
of the float is the output of a 32-bit PRNG (the obsolete version of MT).
53 bits isn't the relevant number here.

The selection ratios can get close to 2. Computer scientists don't do it
the way R does, for a reason.

Regards,
Philip

On Wed, Sep 19, 2018 at 9:05 AM Duncan Murdoch <murdoch.duncan at gmail.com>
wrote:

> On 19/09/2018 9:09 AM, I?aki Ucar wrote:
> > El mi?., 19 sept. 2018 a las 14:43, Duncan Murdoch
> > (<murdoch.duncan at gmail.com>) escribi?:
> >>
> >> On 18/09/2018 5:46 PM, Carl Boettiger wrote:
> >>> Dear list,
> >>>
> >>> It looks to me that R samples random integers using an intuitive but
> biased
> >>> algorithm by going from a random number on [0,1) from the PRNG to a
> random
> >>> integer, e.g.
> >>> https://github.com/wch/r-source/blob/tags/R-3-5-1/src/main/RNG.c#L808
> >>>
> >>> Many other languages use various rejection sampling approaches which
> >>> provide an unbiased method for sampling, such as in Go, python, and
> others
> >>> described here:  https://arxiv.org/abs/1805.10941 (I believe the
> biased
> >>> algorithm currently used in R is also described there).  I'm not an
> expert
> >>> in this area, but does it make sense for the R to adopt one of the
> unbiased
> >>> random sample algorithms outlined there and used in other languages?
> Would
> >>> a patch providing such an algorithm be welcome? What concerns would
> need to
> >>> be addressed first?
> >>>
> >>> I believe this issue was also raised by Killie & Philip in
> >>> http://r.789695.n4.nabble.com/Bug-in-sample-td4729483.html, and more
> >>> recently in
> >>> https://www.stat.berkeley.edu/~stark/Preprints/r-random-issues.pdf,
> >>> pointing to the python implementation for comparison:
> >>>
> https://github.com/statlab/cryptorandom/blob/master/cryptorandom/cryptorandom.py#L265
> >>
> >> I think the analyses are correct, but I doubt if a change to the default
> >> is likely to be accepted as it would make it more difficult to reproduce
> >> older results.
> >>
> >> On the other hand, a contribution of a new function like sample() but
> >> not suffering from the bias would be good.  The normal way to make such
> >> a contribution is in a user contributed package.
> >>
> >> By the way, R code illustrating the bias is probably not very hard to
> >> put together.  I believe the bias manifests itself in sample() producing
> >> values with two different probabilities (instead of all equal
> >> probabilities).  Those may differ by as much as one part in 2^32.  It's
> >
> > According to Kellie and Philip, in the attachment of the thread
> > referenced by Carl, "The maximum ratio of selection probabilities can
> > get as large as 1.5 if n is just below 2^31".
>
> Sorry, I didn't write very well.  I meant to say that the difference in
> probabilities would be 2^-32, not that the ratio of probabilities would
> be 1 + 2^-32.
>
> By the way, I don't see the statement giving the ratio as 1.5, but maybe
> I was looking in the wrong place.  In Theorem 1 of the paper I was
> looking in the ratio was "1 + m 2^{-w + 1}".  In that formula m is your
> n.  If it is near 2^31, R uses w = 57 random bits, so the ratio would be
> very, very small (one part in 2^25).
>
> The worst case for R would happen when m  is just below  2^25, where w
> is at least 31 for the default generators.  In that case the ratio could
> be about 1.03.
>
> Duncan Murdoch
>


-- 
Philip B. Stark | Associate Dean, Mathematical and Physical Sciences |
Professor,  Department of Statistics |
University of California
Berkeley, CA 94720-3860 | 510-394-5077 | statistics.berkeley.edu/~stark |
@philipbstark

	[[alternative HTML version deleted]]


From @t@rk @ending from @t@t@berkeley@edu  Wed Sep 19 18:18:18 2018
From: @t@rk @ending from @t@t@berkeley@edu (Philip B. Stark)
Date: Wed, 19 Sep 2018 09:18:18 -0700
Subject: [Rd] Bias in R's random integers?
In-Reply-To: <997228e9-7b3f-0ab9-6541-b347de00b40c@gmail.com>
References: <CAN_1p9wVX=AR8aYtC2LJo0bgN9cRW3XcbYJqnVHdCzA-QA1ceg@mail.gmail.com>
 <ffb74021-8d95-a820-8013-f0e51dc1a9b3@gmail.com>
 <CAARY7khVi==mZs5o_8fZkTUDDuY-ABwE=gA45COX7qbRgBLhsg@mail.gmail.com>
 <997228e9-7b3f-0ab9-6541-b347de00b40c@gmail.com>
Message-ID: <CAA7KnHGSWG9N_zynoKpCtbsN9MES_D60ukbP4sUwcLo5Zhz+qw@mail.gmail.com>

That depends on the number of replications, among other things.

Moreover, because of the bias, the usual formulae for uncertainty in
estimates based on random samples, etc., are incorrect: sample() does not
give a simple random sample.

On Wed, Sep 19, 2018 at 9:15 AM Duncan Murdoch <murdoch.duncan at gmail.com>
wrote:

> On 19/09/2018 9:40 AM, David Hugh-Jones wrote:
> >
> >
> > On Wed, 19 Sep 2018 at 13:43, Duncan Murdoch <murdoch.duncan at gmail.com
> > <mailto:murdoch.duncan at gmail.com>> wrote:
> >
> >
> >     I think the analyses are correct, but I doubt if a change to the
> >     default
> >     is likely to be accepted as it would make it more difficult to
> >     reproduce
> >     older results.
> >
> >
> > I'm a bit alarmed by the logic here. Unbiased sampling seems basic for a
> > statistical language. As a consumer of R I'd like to think that e.g. my
> > bootstrapped p values are correct.
> > Surely if the old results depend on the biased algorithm, then they are
> > false results?
>
> All Monte Carlo results contain Monte Carlo error.  Using the biased
> function will have some additional error, but for almost all
> simulations, it will be negligible compared to the Monte Carlo error.  I
> suspect the only simulations where the bias was anywhere near the same
> order of magnitude as the Monte Carlo error would be ones designed with
> this specific code in mind.
>
> Duncan Murdoch
>
>

-- 
Philip B. Stark | Associate Dean, Mathematical and Physical Sciences |
Professor,  Department of Statistics |
University of California
Berkeley, CA 94720-3860 | 510-394-5077 | statistics.berkeley.edu/~stark |
@philipbstark

	[[alternative HTML version deleted]]


From @t@rk @ending from @t@t@berkeley@edu  Wed Sep 19 18:23:07 2018
From: @t@rk @ending from @t@t@berkeley@edu (Philip B. Stark)
Date: Wed, 19 Sep 2018 09:23:07 -0700
Subject: [Rd] Bias in R's random integers?
In-Reply-To: <73c4667d-38ad-f5ef-468c-4c368e0fdc12@gmail.com>
References: <CAN_1p9wVX=AR8aYtC2LJo0bgN9cRW3XcbYJqnVHdCzA-QA1ceg@mail.gmail.com>
 <ffb74021-8d95-a820-8013-f0e51dc1a9b3@gmail.com>
 <CALEXWq3booRRTSYvYqcjV7qPZGOC1oh5o45b5o=J2ypQi5s-ag@mail.gmail.com>
 <ece52410-ddb2-2198-9291-201475d3f46e@gmail.com>
 <CAA7KnHHEejYVn8NtKrY1KuKcJUq8n4LoJSDx9BZv-LJyC=r9yQ@mail.gmail.com>
 <73c4667d-38ad-f5ef-468c-4c368e0fdc12@gmail.com>
Message-ID: <CAA7KnHFcY5YJ2kTAfPJJ0PHwcWx7CumE49A4NgJWC2NK5CV=-A@mail.gmail.com>

No, the 2nd call only happens when m > 2**31. Here's the code:

(RNG.c, lines 793ff)

double R_unif_index(double dn)
{
    double cut = INT_MAX;

    switch(RNG_kind) {
    case KNUTH_TAOCP:
    case USER_UNIF:
    case KNUTH_TAOCP2:
cut = 33554431.0; /* 2^25 - 1 */
  break;
    default:
  break;
   }

    double u = dn > cut ? ru() : unif_rand();
    return floor(dn * u);
}

On Wed, Sep 19, 2018 at 9:20 AM Duncan Murdoch <murdoch.duncan at gmail.com>
wrote:

> On 19/09/2018 12:09 PM, Philip B. Stark wrote:
> > The 53 bits only encode at most 2^{32} possible values, because the
> > source of the float is the output of a 32-bit PRNG (the obsolete version
> > of MT). 53 bits isn't the relevant number here.
>
> No, two calls to unif_rand() are used.  There are two 32 bit values, but
> some of the bits are thrown away.
>
> Duncan Murdoch
>
> >
> > The selection ratios can get close to 2. Computer scientists don't do it
> > the way R does, for a reason.
> >
> > Regards,
> > Philip
> >
> > On Wed, Sep 19, 2018 at 9:05 AM Duncan Murdoch <murdoch.duncan at gmail.com
> > <mailto:murdoch.duncan at gmail.com>> wrote:
> >
> >     On 19/09/2018 9:09 AM, I?aki Ucar wrote:
> >      > El mi?., 19 sept. 2018 a las 14:43, Duncan Murdoch
> >      > (<murdoch.duncan at gmail.com <mailto:murdoch.duncan at gmail.com>>)
> >     escribi?:
> >      >>
> >      >> On 18/09/2018 5:46 PM, Carl Boettiger wrote:
> >      >>> Dear list,
> >      >>>
> >      >>> It looks to me that R samples random integers using an
> >     intuitive but biased
> >      >>> algorithm by going from a random number on [0,1) from the PRNG
> >     to a random
> >      >>> integer, e.g.
> >      >>>
> >
> https://github.com/wch/r-source/blob/tags/R-3-5-1/src/main/RNG.c#L808
> >      >>>
> >      >>> Many other languages use various rejection sampling approaches
> >     which
> >      >>> provide an unbiased method for sampling, such as in Go, python,
> >     and others
> >      >>> described here: https://arxiv.org/abs/1805.10941 (I believe the
> >     biased
> >      >>> algorithm currently used in R is also described there).  I'm
> >     not an expert
> >      >>> in this area, but does it make sense for the R to adopt one of
> >     the unbiased
> >      >>> random sample algorithms outlined there and used in other
> >     languages?  Would
> >      >>> a patch providing such an algorithm be welcome? What concerns
> >     would need to
> >      >>> be addressed first?
> >      >>>
> >      >>> I believe this issue was also raised by Killie & Philip in
> >      >>> http://r.789695.n4.nabble.com/Bug-in-sample-td4729483.html, and
> >     more
> >      >>> recently in
> >      >>>
> >     https://www.stat.berkeley.edu/~stark/Preprints/r-random-issues.pdf
> >     <
> https://www.stat.berkeley.edu/%7Estark/Preprints/r-random-issues.pdf>,
> >      >>> pointing to the python implementation for comparison:
> >      >>>
> >
> https://github.com/statlab/cryptorandom/blob/master/cryptorandom/cryptorandom.py#L265
> >      >>
> >      >> I think the analyses are correct, but I doubt if a change to the
> >     default
> >      >> is likely to be accepted as it would make it more difficult to
> >     reproduce
> >      >> older results.
> >      >>
> >      >> On the other hand, a contribution of a new function like
> >     sample() but
> >      >> not suffering from the bias would be good.  The normal way to
> >     make such
> >      >> a contribution is in a user contributed package.
> >      >>
> >      >> By the way, R code illustrating the bias is probably not very
> >     hard to
> >      >> put together.  I believe the bias manifests itself in sample()
> >     producing
> >      >> values with two different probabilities (instead of all equal
> >      >> probabilities).  Those may differ by as much as one part in
> >     2^32.  It's
> >      >
> >      > According to Kellie and Philip, in the attachment of the thread
> >      > referenced by Carl, "The maximum ratio of selection probabilities
> can
> >      > get as large as 1.5 if n is just below 2^31".
> >
> >     Sorry, I didn't write very well.  I meant to say that the difference
> in
> >     probabilities would be 2^-32, not that the ratio of probabilities
> would
> >     be 1 + 2^-32.
> >
> >     By the way, I don't see the statement giving the ratio as 1.5, but
> >     maybe
> >     I was looking in the wrong place.  In Theorem 1 of the paper I was
> >     looking in the ratio was "1 + m 2^{-w + 1}".  In that formula m is
> your
> >     n.  If it is near 2^31, R uses w = 57 random bits, so the ratio
> >     would be
> >     very, very small (one part in 2^25).
> >
> >     The worst case for R would happen when m  is just below  2^25, where
> w
> >     is at least 31 for the default generators.  In that case the ratio
> >     could
> >     be about 1.03.
> >
> >     Duncan Murdoch
> >
> >
> >
> > --
> > Philip B. Stark | Associate Dean, Mathematical and Physical Sciences |
> > Professor,  Department of Statistics |
> > University of California
> > Berkeley, CA 94720-3860 | 510-394-5077 | statistics.berkeley.edu/~stark
> > <http://statistics.berkeley.edu/%7Estark> |
> > @philipbstark
> >
>
>

-- 
Philip B. Stark | Associate Dean, Mathematical and Physical Sciences |
Professor,  Department of Statistics |
University of California
Berkeley, CA 94720-3860 | 510-394-5077 | statistics.berkeley.edu/~stark |
@philipbstark

	[[alternative HTML version deleted]]


From @t@rk @ending from @t@t@berkeley@edu  Wed Sep 19 21:52:31 2018
From: @t@rk @ending from @t@t@berkeley@edu (Philip B. Stark)
Date: Wed, 19 Sep 2018 12:52:31 -0700
Subject: [Rd] Bias in R's random integers?
In-Reply-To: <c3590da8-2e92-4e46-3a81-0d0dbfc81845@gmail.com>
References: <CAN_1p9wVX=AR8aYtC2LJo0bgN9cRW3XcbYJqnVHdCzA-QA1ceg@mail.gmail.com>
 <ffb74021-8d95-a820-8013-f0e51dc1a9b3@gmail.com>
 <CALEXWq3booRRTSYvYqcjV7qPZGOC1oh5o45b5o=J2ypQi5s-ag@mail.gmail.com>
 <ece52410-ddb2-2198-9291-201475d3f46e@gmail.com>
 <CAA7KnHHEejYVn8NtKrY1KuKcJUq8n4LoJSDx9BZv-LJyC=r9yQ@mail.gmail.com>
 <73c4667d-38ad-f5ef-468c-4c368e0fdc12@gmail.com>
 <CAA7KnHFcY5YJ2kTAfPJJ0PHwcWx7CumE49A4NgJWC2NK5CV=-A@mail.gmail.com>
 <c3590da8-2e92-4e46-3a81-0d0dbfc81845@gmail.com>
Message-ID: <CAA7KnHE9OSxML3Cp8Y-gd7CbBj-86UP9qAy6yzgvfcb+=Vg86Q@mail.gmail.com>

Hi Duncan--

Nice simulation!

The absolute difference in probabilities is small, but the maximum relative
difference grows from something negligible to almost 2 as m approaches
2**31.

Because the L_1 distance between the uniform distribution on {1, ..., m}
and what you actually get is large, there have to be test functions whose
expectations are quite different under the two distributions. Whether those
correspond to commonly used statistics or not, I have no idea.

Regarding backwards compatibility: as a user, I'd rather the default
sample() do the best possible thing, and take an extra step to use
something like sample(..., legacy=TRUE) if I want to reproduce old results.

Regards,
Philip

On Wed, Sep 19, 2018 at 9:50 AM Duncan Murdoch <murdoch.duncan at gmail.com>
wrote:

> On 19/09/2018 12:23 PM, Philip B. Stark wrote:
> > No, the 2nd call only happens when m > 2**31. Here's the code:
>
> Yes, you're right. Sorry!
>
> So the ratio really does come close to 2.  However, the difference in
> probabilities between outcomes is still at most 2^-32 when m is less
> than that cutoff.  That's not feasible to detect; the only detectable
> difference would happen if some event was constructed to hold an
> abundance of outcomes with especially low (or especially high) probability.
>
> As I said in my original post, it's probably not hard to construct such
> a thing, but as I've said more recently, it probably wouldn't happen by
> chance.  Here's one attempt to do it:
>
> Call the values from unif_rand() "the unif_rand() outcomes".  Call the
> values from sample() the sample outcomes.
>
> It would be easiest to see the error if half of the sample() outcomes
> used two unif_rand() outcomes, and half used just one.  That would mean
> m should be (2/3) * 2^32, but that's too big and would trigger the other
> version.
>
> So how about half use 2 unif_rands(), and half use 3?  That means m =
> (2/5) * 2^32 = 1717986918.  A good guess is that sample() outcomes would
> alternate between the two possibilities, so our event could be even
> versus odd outcomes.
>
> Let's try it:
>
>  > m <- (2/5)*2^32
>  > m > 2^31
> [1] FALSE
>  > x <- sample(m, 1000000, replace = TRUE)
>  > table(x %% 2)
>
>       0      1
> 399850 600150
>
> Since m is an even number, the true proportions of evens and odds should
> be exactly 0.5.  That's some pretty strong evidence of the bug in the
> generator.  (Note that the ratio of the observed probabilities is about
> 1.5, so I may not be the first person to have done this.)
>
> I'm still not convinced that there has ever been a simulation run with
> detectable bias compared to Monte Carlo error unless it (like this one)
> was designed specifically to show the problem.
>
> Duncan Murdoch
>
> >
> > (RNG.c, lines 793ff)
> >
> > double R_unif_index(double dn)
> > {
> >      double cut = INT_MAX;
> >
> >      switch(RNG_kind) {
> >      case KNUTH_TAOCP:
> >      case USER_UNIF:
> >      case KNUTH_TAOCP2:
> > cut = 33554431.0; /* 2^25 - 1 */
> > break;
> >      default:
> > break;
> >     }
> >
> >      double u = dn > cut ? ru() : unif_rand();
> >      return floor(dn * u);
> > }
> >
> > On Wed, Sep 19, 2018 at 9:20 AM Duncan Murdoch <murdoch.duncan at gmail.com
> > <mailto:murdoch.duncan at gmail.com>> wrote:
> >
> >     On 19/09/2018 12:09 PM, Philip B. Stark wrote:
> >      > The 53 bits only encode at most 2^{32} possible values, because
> the
> >      > source of the float is the output of a 32-bit PRNG (the obsolete
> >     version
> >      > of MT). 53 bits isn't the relevant number here.
> >
> >     No, two calls to unif_rand() are used.  There are two 32 bit values,
> >     but
> >     some of the bits are thrown away.
> >
> >     Duncan Murdoch
> >
> >      >
> >      > The selection ratios can get close to 2. Computer scientists
> >     don't do it
> >      > the way R does, for a reason.
> >      >
> >      > Regards,
> >      > Philip
> >      >
> >      > On Wed, Sep 19, 2018 at 9:05 AM Duncan Murdoch
> >     <murdoch.duncan at gmail.com <mailto:murdoch.duncan at gmail.com>
> >      > <mailto:murdoch.duncan at gmail.com
> >     <mailto:murdoch.duncan at gmail.com>>> wrote:
> >      >
> >      >     On 19/09/2018 9:09 AM, I?aki Ucar wrote:
> >      >      > El mi?., 19 sept. 2018 a las 14:43, Duncan Murdoch
> >      >      > (<murdoch.duncan at gmail.com
> >     <mailto:murdoch.duncan at gmail.com> <mailto:murdoch.duncan at gmail.com
> >     <mailto:murdoch.duncan at gmail.com>>>)
> >      >     escribi?:
> >      >      >>
> >      >      >> On 18/09/2018 5:46 PM, Carl Boettiger wrote:
> >      >      >>> Dear list,
> >      >      >>>
> >      >      >>> It looks to me that R samples random integers using an
> >      >     intuitive but biased
> >      >      >>> algorithm by going from a random number on [0,1) from
> >     the PRNG
> >      >     to a random
> >      >      >>> integer, e.g.
> >      >      >>>
> >      >
> https://github.com/wch/r-source/blob/tags/R-3-5-1/src/main/RNG.c#L808
> >      >      >>>
> >      >      >>> Many other languages use various rejection sampling
> >     approaches
> >      >     which
> >      >      >>> provide an unbiased method for sampling, such as in Go,
> >     python,
> >      >     and others
> >      >      >>> described here: https://arxiv.org/abs/1805.10941 (I
> >     believe the
> >      >     biased
> >      >      >>> algorithm currently used in R is also described there).
> I'm
> >      >     not an expert
> >      >      >>> in this area, but does it make sense for the R to adopt
> >     one of
> >      >     the unbiased
> >      >      >>> random sample algorithms outlined there and used in other
> >      >     languages?  Would
> >      >      >>> a patch providing such an algorithm be welcome? What
> >     concerns
> >      >     would need to
> >      >      >>> be addressed first?
> >      >      >>>
> >      >      >>> I believe this issue was also raised by Killie & Philip
> in
> >      >      >>>
> >     http://r.789695.n4.nabble.com/Bug-in-sample-td4729483.html, and
> >      >     more
> >      >      >>> recently in
> >      >      >>>
> >      >
> >     https://www.stat.berkeley.edu/~stark/Preprints/r-random-issues.pdf
> >     <
> https://www.stat.berkeley.edu/%7Estark/Preprints/r-random-issues.pdf>
> >      >
> >       <
> https://www.stat.berkeley.edu/%7Estark/Preprints/r-random-issues.pdf>,
> >      >      >>> pointing to the python implementation for comparison:
> >      >      >>>
> >      >
> >
> https://github.com/statlab/cryptorandom/blob/master/cryptorandom/cryptorandom.py#L265
> >      >      >>
> >      >      >> I think the analyses are correct, but I doubt if a change
> >     to the
> >      >     default
> >      >      >> is likely to be accepted as it would make it more
> >     difficult to
> >      >     reproduce
> >      >      >> older results.
> >      >      >>
> >      >      >> On the other hand, a contribution of a new function like
> >      >     sample() but
> >      >      >> not suffering from the bias would be good.  The normal
> way to
> >      >     make such
> >      >      >> a contribution is in a user contributed package.
> >      >      >>
> >      >      >> By the way, R code illustrating the bias is probably not
> very
> >      >     hard to
> >      >      >> put together.  I believe the bias manifests itself in
> >     sample()
> >      >     producing
> >      >      >> values with two different probabilities (instead of all
> equal
> >      >      >> probabilities).  Those may differ by as much as one part
> in
> >      >     2^32.  It's
> >      >      >
> >      >      > According to Kellie and Philip, in the attachment of the
> >     thread
> >      >      > referenced by Carl, "The maximum ratio of selection
> >     probabilities can
> >      >      > get as large as 1.5 if n is just below 2^31".
> >      >
> >      >     Sorry, I didn't write very well.  I meant to say that the
> >     difference in
> >      >     probabilities would be 2^-32, not that the ratio of
> >     probabilities would
> >      >     be 1 + 2^-32.
> >      >
> >      >     By the way, I don't see the statement giving the ratio as
> >     1.5, but
> >      >     maybe
> >      >     I was looking in the wrong place.  In Theorem 1 of the paper
> >     I was
> >      >     looking in the ratio was "1 + m 2^{-w + 1}".  In that formula
> >     m is your
> >      >     n.  If it is near 2^31, R uses w = 57 random bits, so the
> ratio
> >      >     would be
> >      >     very, very small (one part in 2^25).
> >      >
> >      >     The worst case for R would happen when m  is just below
> >     2^25, where w
> >      >     is at least 31 for the default generators.  In that case the
> >     ratio
> >      >     could
> >      >     be about 1.03.
> >      >
> >      >     Duncan Murdoch
> >      >
> >      >
> >      >
> >      > --
> >      > Philip B. Stark | Associate Dean, Mathematical and Physical
> >     Sciences |
> >      > Professor,  Department of Statistics |
> >      > University of California
> >      > Berkeley, CA 94720-3860 | 510-394-5077 |
> >     statistics.berkeley.edu/~stark
> >     <http://statistics.berkeley.edu/%7Estark>
> >      > <http://statistics.berkeley.edu/%7Estark> |
> >      > @philipbstark
> >      >
> >
> >
> >
> > --
> > Philip B. Stark | Associate Dean, Mathematical and Physical Sciences |
> > Professor,  Department of Statistics |
> > University of California
> > Berkeley, CA 94720-3860 | 510-394-5077 | statistics.berkeley.edu/~stark
> > <http://statistics.berkeley.edu/%7Estark> |
> > @philipbstark
> >
>
>

-- 
Philip B. Stark | Associate Dean, Mathematical and Physical Sciences |
Professor,  Department of Statistics |
University of California
Berkeley, CA 94720-3860 | 510-394-5077 | statistics.berkeley.edu/~stark |
@philipbstark

	[[alternative HTML version deleted]]


From @t@rk @ending from @t@t@berkeley@edu  Wed Sep 19 21:58:12 2018
From: @t@rk @ending from @t@t@berkeley@edu (Philip B. Stark)
Date: Wed, 19 Sep 2018 12:58:12 -0700
Subject: [Rd] Bias in R's random integers?
In-Reply-To: <CAA7KnHE9OSxML3Cp8Y-gd7CbBj-86UP9qAy6yzgvfcb+=Vg86Q@mail.gmail.com>
References: <CAN_1p9wVX=AR8aYtC2LJo0bgN9cRW3XcbYJqnVHdCzA-QA1ceg@mail.gmail.com>
 <ffb74021-8d95-a820-8013-f0e51dc1a9b3@gmail.com>
 <CALEXWq3booRRTSYvYqcjV7qPZGOC1oh5o45b5o=J2ypQi5s-ag@mail.gmail.com>
 <ece52410-ddb2-2198-9291-201475d3f46e@gmail.com>
 <CAA7KnHHEejYVn8NtKrY1KuKcJUq8n4LoJSDx9BZv-LJyC=r9yQ@mail.gmail.com>
 <73c4667d-38ad-f5ef-468c-4c368e0fdc12@gmail.com>
 <CAA7KnHFcY5YJ2kTAfPJJ0PHwcWx7CumE49A4NgJWC2NK5CV=-A@mail.gmail.com>
 <c3590da8-2e92-4e46-3a81-0d0dbfc81845@gmail.com>
 <CAA7KnHE9OSxML3Cp8Y-gd7CbBj-86UP9qAy6yzgvfcb+=Vg86Q@mail.gmail.com>
Message-ID: <CAA7KnHG_hL_CWm=dNYvgWDNwcFL+tKEcJWrfpvR5_GdLHNpxqQ@mail.gmail.com>

One more thing, apropos this:

I'm still not convinced that there has ever been a simulation run with
detectable bias compared to Monte Carlo error unless it (like this one) was
designed specifically to show the problem.


I often use random permutations to simulate p-values to calibrate
permutation tests. If I'm trying to simulate the probability of a
low-probability event, this could matter a lot.

Best wishes,
Philip

On Wed, Sep 19, 2018 at 12:52 PM Philip B. Stark <stark at stat.berkeley.edu>
wrote:

> Hi Duncan--
>
> Nice simulation!
>
> The absolute difference in probabilities is small, but the maximum
> relative difference grows from something negligible to almost 2 as m
> approaches 2**31.
>
> Because the L_1 distance between the uniform distribution on {1, ..., m}
> and what you actually get is large, there have to be test functions whose
> expectations are quite different under the two distributions. Whether those
> correspond to commonly used statistics or not, I have no idea.
>
> Regarding backwards compatibility: as a user, I'd rather the default
> sample() do the best possible thing, and take an extra step to use
> something like sample(..., legacy=TRUE) if I want to reproduce old results.
>
> Regards,
> Philip
>
> On Wed, Sep 19, 2018 at 9:50 AM Duncan Murdoch <murdoch.duncan at gmail.com>
> wrote:
>
>> On 19/09/2018 12:23 PM, Philip B. Stark wrote:
>> > No, the 2nd call only happens when m > 2**31. Here's the code:
>>
>> Yes, you're right. Sorry!
>>
>> So the ratio really does come close to 2.  However, the difference in
>> probabilities between outcomes is still at most 2^-32 when m is less
>> than that cutoff.  That's not feasible to detect; the only detectable
>> difference would happen if some event was constructed to hold an
>> abundance of outcomes with especially low (or especially high)
>> probability.
>>
>> As I said in my original post, it's probably not hard to construct such
>> a thing, but as I've said more recently, it probably wouldn't happen by
>> chance.  Here's one attempt to do it:
>>
>> Call the values from unif_rand() "the unif_rand() outcomes".  Call the
>> values from sample() the sample outcomes.
>>
>> It would be easiest to see the error if half of the sample() outcomes
>> used two unif_rand() outcomes, and half used just one.  That would mean
>> m should be (2/3) * 2^32, but that's too big and would trigger the other
>> version.
>>
>> So how about half use 2 unif_rands(), and half use 3?  That means m =
>> (2/5) * 2^32 = 1717986918.  A good guess is that sample() outcomes would
>> alternate between the two possibilities, so our event could be even
>> versus odd outcomes.
>>
>> Let's try it:
>>
>>  > m <- (2/5)*2^32
>>  > m > 2^31
>> [1] FALSE
>>  > x <- sample(m, 1000000, replace = TRUE)
>>  > table(x %% 2)
>>
>>       0      1
>> 399850 600150
>>
>> Since m is an even number, the true proportions of evens and odds should
>> be exactly 0.5.  That's some pretty strong evidence of the bug in the
>> generator.  (Note that the ratio of the observed probabilities is about
>> 1.5, so I may not be the first person to have done this.)
>>
>> I'm still not convinced that there has ever been a simulation run with
>> detectable bias compared to Monte Carlo error unless it (like this one)
>> was designed specifically to show the problem.
>>
>> Duncan Murdoch
>>
>> >
>> > (RNG.c, lines 793ff)
>> >
>> > double R_unif_index(double dn)
>> > {
>> >      double cut = INT_MAX;
>> >
>> >      switch(RNG_kind) {
>> >      case KNUTH_TAOCP:
>> >      case USER_UNIF:
>> >      case KNUTH_TAOCP2:
>> > cut = 33554431.0; /* 2^25 - 1 */
>> > break;
>> >      default:
>> > break;
>> >     }
>> >
>> >      double u = dn > cut ? ru() : unif_rand();
>> >      return floor(dn * u);
>> > }
>> >
>> > On Wed, Sep 19, 2018 at 9:20 AM Duncan Murdoch <
>> murdoch.duncan at gmail.com
>> > <mailto:murdoch.duncan at gmail.com>> wrote:
>> >
>> >     On 19/09/2018 12:09 PM, Philip B. Stark wrote:
>> >      > The 53 bits only encode at most 2^{32} possible values, because
>> the
>> >      > source of the float is the output of a 32-bit PRNG (the obsolete
>> >     version
>> >      > of MT). 53 bits isn't the relevant number here.
>> >
>> >     No, two calls to unif_rand() are used.  There are two 32 bit values,
>> >     but
>> >     some of the bits are thrown away.
>> >
>> >     Duncan Murdoch
>> >
>> >      >
>> >      > The selection ratios can get close to 2. Computer scientists
>> >     don't do it
>> >      > the way R does, for a reason.
>> >      >
>> >      > Regards,
>> >      > Philip
>> >      >
>> >      > On Wed, Sep 19, 2018 at 9:05 AM Duncan Murdoch
>> >     <murdoch.duncan at gmail.com <mailto:murdoch.duncan at gmail.com>
>> >      > <mailto:murdoch.duncan at gmail.com
>> >     <mailto:murdoch.duncan at gmail.com>>> wrote:
>> >      >
>> >      >     On 19/09/2018 9:09 AM, I?aki Ucar wrote:
>> >      >      > El mi?., 19 sept. 2018 a las 14:43, Duncan Murdoch
>> >      >      > (<murdoch.duncan at gmail.com
>> >     <mailto:murdoch.duncan at gmail.com> <mailto:murdoch.duncan at gmail.com
>> >     <mailto:murdoch.duncan at gmail.com>>>)
>> >      >     escribi?:
>> >      >      >>
>> >      >      >> On 18/09/2018 5:46 PM, Carl Boettiger wrote:
>> >      >      >>> Dear list,
>> >      >      >>>
>> >      >      >>> It looks to me that R samples random integers using an
>> >      >     intuitive but biased
>> >      >      >>> algorithm by going from a random number on [0,1) from
>> >     the PRNG
>> >      >     to a random
>> >      >      >>> integer, e.g.
>> >      >      >>>
>> >      >
>> https://github.com/wch/r-source/blob/tags/R-3-5-1/src/main/RNG.c#L808
>> >      >      >>>
>> >      >      >>> Many other languages use various rejection sampling
>> >     approaches
>> >      >     which
>> >      >      >>> provide an unbiased method for sampling, such as in Go,
>> >     python,
>> >      >     and others
>> >      >      >>> described here: https://arxiv.org/abs/1805.10941 (I
>> >     believe the
>> >      >     biased
>> >      >      >>> algorithm currently used in R is also described
>> there).  I'm
>> >      >     not an expert
>> >      >      >>> in this area, but does it make sense for the R to adopt
>> >     one of
>> >      >     the unbiased
>> >      >      >>> random sample algorithms outlined there and used in
>> other
>> >      >     languages?  Would
>> >      >      >>> a patch providing such an algorithm be welcome? What
>> >     concerns
>> >      >     would need to
>> >      >      >>> be addressed first?
>> >      >      >>>
>> >      >      >>> I believe this issue was also raised by Killie & Philip
>> in
>> >      >      >>>
>> >     http://r.789695.n4.nabble.com/Bug-in-sample-td4729483.html, and
>> >      >     more
>> >      >      >>> recently in
>> >      >      >>>
>> >      >
>> >     https://www.stat.berkeley.edu/~stark/Preprints/r-random-issues.pdf
>> >     <
>> https://www.stat.berkeley.edu/%7Estark/Preprints/r-random-issues.pdf>
>> >      >
>> >       <
>> https://www.stat.berkeley.edu/%7Estark/Preprints/r-random-issues.pdf>,
>> >      >      >>> pointing to the python implementation for comparison:
>> >      >      >>>
>> >      >
>> >
>> https://github.com/statlab/cryptorandom/blob/master/cryptorandom/cryptorandom.py#L265
>> >      >      >>
>> >      >      >> I think the analyses are correct, but I doubt if a change
>> >     to the
>> >      >     default
>> >      >      >> is likely to be accepted as it would make it more
>> >     difficult to
>> >      >     reproduce
>> >      >      >> older results.
>> >      >      >>
>> >      >      >> On the other hand, a contribution of a new function like
>> >      >     sample() but
>> >      >      >> not suffering from the bias would be good.  The normal
>> way to
>> >      >     make such
>> >      >      >> a contribution is in a user contributed package.
>> >      >      >>
>> >      >      >> By the way, R code illustrating the bias is probably not
>> very
>> >      >     hard to
>> >      >      >> put together.  I believe the bias manifests itself in
>> >     sample()
>> >      >     producing
>> >      >      >> values with two different probabilities (instead of all
>> equal
>> >      >      >> probabilities).  Those may differ by as much as one part
>> in
>> >      >     2^32.  It's
>> >      >      >
>> >      >      > According to Kellie and Philip, in the attachment of the
>> >     thread
>> >      >      > referenced by Carl, "The maximum ratio of selection
>> >     probabilities can
>> >      >      > get as large as 1.5 if n is just below 2^31".
>> >      >
>> >      >     Sorry, I didn't write very well.  I meant to say that the
>> >     difference in
>> >      >     probabilities would be 2^-32, not that the ratio of
>> >     probabilities would
>> >      >     be 1 + 2^-32.
>> >      >
>> >      >     By the way, I don't see the statement giving the ratio as
>> >     1.5, but
>> >      >     maybe
>> >      >     I was looking in the wrong place.  In Theorem 1 of the paper
>> >     I was
>> >      >     looking in the ratio was "1 + m 2^{-w + 1}".  In that formula
>> >     m is your
>> >      >     n.  If it is near 2^31, R uses w = 57 random bits, so the
>> ratio
>> >      >     would be
>> >      >     very, very small (one part in 2^25).
>> >      >
>> >      >     The worst case for R would happen when m  is just below
>> >     2^25, where w
>> >      >     is at least 31 for the default generators.  In that case the
>> >     ratio
>> >      >     could
>> >      >     be about 1.03.
>> >      >
>> >      >     Duncan Murdoch
>> >      >
>> >      >
>> >      >
>> >      > --
>> >      > Philip B. Stark | Associate Dean, Mathematical and Physical
>> >     Sciences |
>> >      > Professor,  Department of Statistics |
>> >      > University of California
>> >      > Berkeley, CA 94720-3860 | 510-394-5077 |
>> >     statistics.berkeley.edu/~stark
>> >     <http://statistics.berkeley.edu/%7Estark>
>> >      > <http://statistics.berkeley.edu/%7Estark> |
>> >      > @philipbstark
>> >      >
>> >
>> >
>> >
>> > --
>> > Philip B. Stark | Associate Dean, Mathematical and Physical Sciences |
>> > Professor,  Department of Statistics |
>> > University of California
>> > Berkeley, CA 94720-3860 | 510-394-5077 | statistics.berkeley.edu/~stark
>> > <http://statistics.berkeley.edu/%7Estark> |
>> > @philipbstark
>> >
>>
>>
>
> --
> Philip B. Stark | Associate Dean, Mathematical and Physical Sciences |
> Professor,  Department of Statistics |
> University of California
> Berkeley, CA 94720-3860 | 510-394-5077 | statistics.berkeley.edu/~stark |
> @philipbstark
>
>

-- 
Philip B. Stark | Associate Dean, Mathematical and Physical Sciences |
Professor,  Department of Statistics |
University of California
Berkeley, CA 94720-3860 | 510-394-5077 | statistics.berkeley.edu/~stark |
@philipbstark

	[[alternative HTML version deleted]]


From murdoch@dunc@n @ending from gm@il@com  Wed Sep 19 22:19:49 2018
From: murdoch@dunc@n @ending from gm@il@com (Duncan Murdoch)
Date: Wed, 19 Sep 2018 16:19:49 -0400
Subject: [Rd] Bias in R's random integers?
In-Reply-To: <CAA7KnHE9OSxML3Cp8Y-gd7CbBj-86UP9qAy6yzgvfcb+=Vg86Q@mail.gmail.com>
References: <CAN_1p9wVX=AR8aYtC2LJo0bgN9cRW3XcbYJqnVHdCzA-QA1ceg@mail.gmail.com>
 <ffb74021-8d95-a820-8013-f0e51dc1a9b3@gmail.com>
 <CALEXWq3booRRTSYvYqcjV7qPZGOC1oh5o45b5o=J2ypQi5s-ag@mail.gmail.com>
 <ece52410-ddb2-2198-9291-201475d3f46e@gmail.com>
 <CAA7KnHHEejYVn8NtKrY1KuKcJUq8n4LoJSDx9BZv-LJyC=r9yQ@mail.gmail.com>
 <73c4667d-38ad-f5ef-468c-4c368e0fdc12@gmail.com>
 <CAA7KnHFcY5YJ2kTAfPJJ0PHwcWx7CumE49A4NgJWC2NK5CV=-A@mail.gmail.com>
 <c3590da8-2e92-4e46-3a81-0d0dbfc81845@gmail.com>
 <CAA7KnHE9OSxML3Cp8Y-gd7CbBj-86UP9qAy6yzgvfcb+=Vg86Q@mail.gmail.com>
Message-ID: <e82cfff0-3a8e-1d7b-de5c-02b69974ae16@gmail.com>

On 19/09/2018 3:52 PM, Philip B. Stark wrote:
> Hi Duncan--
> 
> Nice simulation!
> 
> The absolute difference in probabilities is small, but the maximum 
> relative difference grows from something negligible to almost 2 as m 
> approaches 2**31.
> 
> Because the L_1 distance between the uniform distribution on {1, ..., m} 
> and what you actually get is large, there have to be test functions 
> whose expectations are quite different under the two distributions. 

That is a mathematically true statement, but I suspect it is not very 
relevant.  Pseudo-random number generators always have test functions 
whose sample averages are quite different from the expectation under the 
true distribution.  Remember Von Neumann's "state of sin" quote.  The 
bug in sample() just means it is easier to find such a function than it 
would otherwise be.

The practical question is whether such a function is likely to arise in 
practice or not.

 > Whether those correspond to commonly used statistics or not, I have no
 > idea.

I am pretty confident that this bug rarely matters.

> Regarding backwards compatibility: as a user, I'd rather the default 
> sample() do the best possible thing, and take an extra step to use 
> something like sample(..., legacy=TRUE) if I want to reproduce old results.

I suspect there's a good chance the bug I discovered today (non-integer 
x values not being truncated) will be declared to be a feature, and the 
documentation will be changed.  Then the rejection sampling approach 
would need to be quite a bit more complicated.

I think a documentation warning about the accuracy of sampling 
probabilities would also be a sufficient fix here, and would be quite a 
bit less trouble than changing the default sample().  But as I said in 
my original post, a contribution of a function without this bug would be 
a nice addition.

Duncan Murdoch

> 
> Regards,
> Philip
> 
> On Wed, Sep 19, 2018 at 9:50 AM Duncan Murdoch <murdoch.duncan at gmail.com 
> <mailto:murdoch.duncan at gmail.com>> wrote:
> 
>     On 19/09/2018 12:23 PM, Philip B. Stark wrote:
>      > No, the 2nd call only happens when m > 2**31. Here's the code:
> 
>     Yes, you're right. Sorry!
> 
>     So the ratio really does come close to 2.? However, the difference in
>     probabilities between outcomes is still at most 2^-32 when m is less
>     than that cutoff.? That's not feasible to detect; the only detectable
>     difference would happen if some event was constructed to hold an
>     abundance of outcomes with especially low (or especially high)
>     probability.
> 
>     As I said in my original post, it's probably not hard to construct such
>     a thing, but as I've said more recently, it probably wouldn't happen by
>     chance.? Here's one attempt to do it:
> 
>     Call the values from unif_rand() "the unif_rand() outcomes".? Call the
>     values from sample() the sample outcomes.
> 
>     It would be easiest to see the error if half of the sample() outcomes
>     used two unif_rand() outcomes, and half used just one.? That would mean
>     m should be (2/3) * 2^32, but that's too big and would trigger the
>     other
>     version.
> 
>     So how about half use 2 unif_rands(), and half use 3?? That means m =
>     (2/5) * 2^32 = 1717986918.? A good guess is that sample() outcomes
>     would
>     alternate between the two possibilities, so our event could be even
>     versus odd outcomes.
> 
>     Let's try it:
> 
>      ?> m <- (2/5)*2^32
>      ?> m > 2^31
>     [1] FALSE
>      ?> x <- sample(m, 1000000, replace = TRUE)
>      ?> table(x %% 2)
> 
>      ? ? ? 0? ? ? 1
>     399850 600150
> 
>     Since m is an even number, the true proportions of evens and odds
>     should
>     be exactly 0.5.? That's some pretty strong evidence of the bug in the
>     generator.? (Note that the ratio of the observed probabilities is about
>     1.5, so I may not be the first person to have done this.)
> 
>     I'm still not convinced that there has ever been a simulation run with
>     detectable bias compared to Monte Carlo error unless it (like this one)
>     was designed specifically to show the problem.
> 
>     Duncan Murdoch
> 
>      >
>      > (RNG.c, lines 793ff)
>      >
>      > double R_unif_index(double dn)
>      > {
>      >? ? ? double cut = INT_MAX;
>      >
>      >? ? ? switch(RNG_kind) {
>      >? ? ? case KNUTH_TAOCP:
>      >? ? ? case USER_UNIF:
>      >? ? ? case KNUTH_TAOCP2:
>      > cut = 33554431.0; /* 2^25 - 1 */
>      > break;
>      >? ? ? default:
>      > break;
>      >? ? ?}
>      >
>      >? ? ? double u = dn > cut ? ru() : unif_rand();
>      >? ? ? return floor(dn * u);
>      > }
>      >
>      > On Wed, Sep 19, 2018 at 9:20 AM Duncan Murdoch
>     <murdoch.duncan at gmail.com <mailto:murdoch.duncan at gmail.com>
>      > <mailto:murdoch.duncan at gmail.com
>     <mailto:murdoch.duncan at gmail.com>>> wrote:
>      >
>      >? ? ?On 19/09/2018 12:09 PM, Philip B. Stark wrote:
>      >? ? ? > The 53 bits only encode at most 2^{32} possible values,
>     because the
>      >? ? ? > source of the float is the output of a 32-bit PRNG (the
>     obsolete
>      >? ? ?version
>      >? ? ? > of MT). 53 bits isn't the relevant number here.
>      >
>      >? ? ?No, two calls to unif_rand() are used.? There are two 32 bit
>     values,
>      >? ? ?but
>      >? ? ?some of the bits are thrown away.
>      >
>      >? ? ?Duncan Murdoch
>      >
>      >? ? ? >
>      >? ? ? > The selection ratios can get close to 2. Computer scientists
>      >? ? ?don't do it
>      >? ? ? > the way R does, for a reason.
>      >? ? ? >
>      >? ? ? > Regards,
>      >? ? ? > Philip
>      >? ? ? >
>      >? ? ? > On Wed, Sep 19, 2018 at 9:05 AM Duncan Murdoch
>      >? ? ?<murdoch.duncan at gmail.com <mailto:murdoch.duncan at gmail.com>
>     <mailto:murdoch.duncan at gmail.com <mailto:murdoch.duncan at gmail.com>>
>      >? ? ? > <mailto:murdoch.duncan at gmail.com
>     <mailto:murdoch.duncan at gmail.com>
>      >? ? ?<mailto:murdoch.duncan at gmail.com
>     <mailto:murdoch.duncan at gmail.com>>>> wrote:
>      >? ? ? >
>      >? ? ? >? ? ?On 19/09/2018 9:09 AM, I?aki Ucar wrote:
>      >? ? ? >? ? ? > El mi?., 19 sept. 2018 a las 14:43, Duncan Murdoch
>      >? ? ? >? ? ? > (<murdoch.duncan at gmail.com
>     <mailto:murdoch.duncan at gmail.com>
>      >? ? ?<mailto:murdoch.duncan at gmail.com
>     <mailto:murdoch.duncan at gmail.com>> <mailto:murdoch.duncan at gmail.com
>     <mailto:murdoch.duncan at gmail.com>
>      >? ? ?<mailto:murdoch.duncan at gmail.com
>     <mailto:murdoch.duncan at gmail.com>>>>)
>      >? ? ? >? ? ?escribi?:
>      >? ? ? >? ? ? >>
>      >? ? ? >? ? ? >> On 18/09/2018 5:46 PM, Carl Boettiger wrote:
>      >? ? ? >? ? ? >>> Dear list,
>      >? ? ? >? ? ? >>>
>      >? ? ? >? ? ? >>> It looks to me that R samples random integers
>     using an
>      >? ? ? >? ? ?intuitive but biased
>      >? ? ? >? ? ? >>> algorithm by going from a random number on [0,1) from
>      >? ? ?the PRNG
>      >? ? ? >? ? ?to a random
>      >? ? ? >? ? ? >>> integer, e.g.
>      >? ? ? >? ? ? >>>
>      >? ? ? >
>     https://github.com/wch/r-source/blob/tags/R-3-5-1/src/main/RNG.c#L808
>      >? ? ? >? ? ? >>>
>      >? ? ? >? ? ? >>> Many other languages use various rejection sampling
>      >? ? ?approaches
>      >? ? ? >? ? ?which
>      >? ? ? >? ? ? >>> provide an unbiased method for sampling, such as
>     in Go,
>      >? ? ?python,
>      >? ? ? >? ? ?and others
>      >? ? ? >? ? ? >>> described here: https://arxiv.org/abs/1805.10941 (I
>      >? ? ?believe the
>      >? ? ? >? ? ?biased
>      >? ? ? >? ? ? >>> algorithm currently used in R is also described
>     there).? I'm
>      >? ? ? >? ? ?not an expert
>      >? ? ? >? ? ? >>> in this area, but does it make sense for the R to
>     adopt
>      >? ? ?one of
>      >? ? ? >? ? ?the unbiased
>      >? ? ? >? ? ? >>> random sample algorithms outlined there and used
>     in other
>      >? ? ? >? ? ?languages?? Would
>      >? ? ? >? ? ? >>> a patch providing such an algorithm be welcome? What
>      >? ? ?concerns
>      >? ? ? >? ? ?would need to
>      >? ? ? >? ? ? >>> be addressed first?
>      >? ? ? >? ? ? >>>
>      >? ? ? >? ? ? >>> I believe this issue was also raised by Killie &
>     Philip in
>      >? ? ? >? ? ? >>>
>      > http://r.789695.n4.nabble.com/Bug-in-sample-td4729483.html, and
>      >? ? ? >? ? ?more
>      >? ? ? >? ? ? >>> recently in
>      >? ? ? >? ? ? >>>
>      >? ? ? >
>      >
>     https://www.stat.berkeley.edu/~stark/Preprints/r-random-issues.pdf
>     <https://www.stat.berkeley.edu/%7Estark/Preprints/r-random-issues.pdf>
>      >   
>      ?<https://www.stat.berkeley.edu/%7Estark/Preprints/r-random-issues.pdf>
>      >? ? ? >
>      >     
>      ?<https://www.stat.berkeley.edu/%7Estark/Preprints/r-random-issues.pdf>,
>      >? ? ? >? ? ? >>> pointing to the python implementation for comparison:
>      >? ? ? >? ? ? >>>
>      >? ? ? >
>      >
>     https://github.com/statlab/cryptorandom/blob/master/cryptorandom/cryptorandom.py#L265
>      >? ? ? >? ? ? >>
>      >? ? ? >? ? ? >> I think the analyses are correct, but I doubt if a
>     change
>      >? ? ?to the
>      >? ? ? >? ? ?default
>      >? ? ? >? ? ? >> is likely to be accepted as it would make it more
>      >? ? ?difficult to
>      >? ? ? >? ? ?reproduce
>      >? ? ? >? ? ? >> older results.
>      >? ? ? >? ? ? >>
>      >? ? ? >? ? ? >> On the other hand, a contribution of a new
>     function like
>      >? ? ? >? ? ?sample() but
>      >? ? ? >? ? ? >> not suffering from the bias would be good.? The
>     normal way to
>      >? ? ? >? ? ?make such
>      >? ? ? >? ? ? >> a contribution is in a user contributed package.
>      >? ? ? >? ? ? >>
>      >? ? ? >? ? ? >> By the way, R code illustrating the bias is
>     probably not very
>      >? ? ? >? ? ?hard to
>      >? ? ? >? ? ? >> put together.? I believe the bias manifests itself in
>      >? ? ?sample()
>      >? ? ? >? ? ?producing
>      >? ? ? >? ? ? >> values with two different probabilities (instead
>     of all equal
>      >? ? ? >? ? ? >> probabilities).? Those may differ by as much as
>     one part in
>      >? ? ? >? ? ?2^32.? It's
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? > According to Kellie and Philip, in the attachment
>     of the
>      >? ? ?thread
>      >? ? ? >? ? ? > referenced by Carl, "The maximum ratio of selection
>      >? ? ?probabilities can
>      >? ? ? >? ? ? > get as large as 1.5 if n is just below 2^31".
>      >? ? ? >
>      >? ? ? >? ? ?Sorry, I didn't write very well.? I meant to say that the
>      >? ? ?difference in
>      >? ? ? >? ? ?probabilities would be 2^-32, not that the ratio of
>      >? ? ?probabilities would
>      >? ? ? >? ? ?be 1 + 2^-32.
>      >? ? ? >
>      >? ? ? >? ? ?By the way, I don't see the statement giving the ratio as
>      >? ? ?1.5, but
>      >? ? ? >? ? ?maybe
>      >? ? ? >? ? ?I was looking in the wrong place.? In Theorem 1 of the
>     paper
>      >? ? ?I was
>      >? ? ? >? ? ?looking in the ratio was "1 + m 2^{-w + 1}".? In that
>     formula
>      >? ? ?m is your
>      >? ? ? >? ? ?n.? If it is near 2^31, R uses w = 57 random bits, so
>     the ratio
>      >? ? ? >? ? ?would be
>      >? ? ? >? ? ?very, very small (one part in 2^25).
>      >? ? ? >
>      >? ? ? >? ? ?The worst case for R would happen when m? is just below
>      >? ? ?2^25, where w
>      >? ? ? >? ? ?is at least 31 for the default generators.? In that
>     case the
>      >? ? ?ratio
>      >? ? ? >? ? ?could
>      >? ? ? >? ? ?be about 1.03.
>      >? ? ? >
>      >? ? ? >? ? ?Duncan Murdoch
>      >? ? ? >
>      >? ? ? >
>      >? ? ? >
>      >? ? ? > --
>      >? ? ? > Philip B. Stark | Associate Dean, Mathematical and Physical
>      >? ? ?Sciences |
>      >? ? ? > Professor, ?Department of Statistics |
>      >? ? ? > University of California
>      >? ? ? > Berkeley, CA 94720-3860 | 510-394-5077 |
>      > statistics.berkeley.edu/~stark
>     <http://statistics.berkeley.edu/%7Estark>
>      >? ? ?<http://statistics.berkeley.edu/%7Estark>
>      >? ? ? > <http://statistics.berkeley.edu/%7Estark> |
>      >? ? ? > @philipbstark
>      >? ? ? >
>      >
>      >
>      >
>      > --
>      > Philip B. Stark | Associate Dean, Mathematical and Physical
>     Sciences |
>      > Professor, ?Department of Statistics |
>      > University of California
>      > Berkeley, CA 94720-3860 | 510-394-5077 |
>     statistics.berkeley.edu/~stark
>     <http://statistics.berkeley.edu/%7Estark>
>      > <http://statistics.berkeley.edu/%7Estark> |
>      > @philipbstark
>      >
> 
> 
> 
> -- 
> Philip B. Stark | Associate Dean, Mathematical and Physical Sciences | 
> Professor, ?Department of Statistics |
> University of California
> Berkeley, CA 94720-3860 | 510-394-5077 | statistics.berkeley.edu/~stark 
> <http://statistics.berkeley.edu/%7Estark> |
> @philipbstark
>


From d@vidhughjone@ @ending from gm@il@com  Wed Sep 19 23:57:10 2018
From: d@vidhughjone@ @ending from gm@il@com (David Hugh-Jones)
Date: Wed, 19 Sep 2018 22:57:10 +0100
Subject: [Rd] Bias in R's random integers?
In-Reply-To: <e82cfff0-3a8e-1d7b-de5c-02b69974ae16@gmail.com>
References: <CAN_1p9wVX=AR8aYtC2LJo0bgN9cRW3XcbYJqnVHdCzA-QA1ceg@mail.gmail.com>
 <ffb74021-8d95-a820-8013-f0e51dc1a9b3@gmail.com>
 <CALEXWq3booRRTSYvYqcjV7qPZGOC1oh5o45b5o=J2ypQi5s-ag@mail.gmail.com>
 <ece52410-ddb2-2198-9291-201475d3f46e@gmail.com>
 <CAA7KnHHEejYVn8NtKrY1KuKcJUq8n4LoJSDx9BZv-LJyC=r9yQ@mail.gmail.com>
 <73c4667d-38ad-f5ef-468c-4c368e0fdc12@gmail.com>
 <CAA7KnHFcY5YJ2kTAfPJJ0PHwcWx7CumE49A4NgJWC2NK5CV=-A@mail.gmail.com>
 <c3590da8-2e92-4e46-3a81-0d0dbfc81845@gmail.com>
 <CAA7KnHE9OSxML3Cp8Y-gd7CbBj-86UP9qAy6yzgvfcb+=Vg86Q@mail.gmail.com>
 <e82cfff0-3a8e-1d7b-de5c-02b69974ae16@gmail.com>
Message-ID: <CAARY7kgM5WLkF7e1jnj1Pn_9WQzeHmuibROz58HDwWDUTuUmeQ@mail.gmail.com>

It doesn't seem too hard to come up with plausible ways in which this could
give bad results. Suppose I sample rows from a large dataset, maybe for
bootstrapping. Suppose the rows are non-randomly ordered, e.g. odd rows are
males, even rows are females. Oops! Very non-representative sample,
bootstrap p values are garbage.

David

On Wed, 19 Sep 2018 at 21:20, Duncan Murdoch <murdoch.duncan at gmail.com>
wrote:

> On 19/09/2018 3:52 PM, Philip B. Stark wrote:
> > Hi Duncan--
> >
> >
>
> That is a mathematically true statement, but I suspect it is not very
> relevant.  Pseudo-random number generators always have test functions
> whose sample averages are quite different from the expectation under the
> true distribution.  Remember Von Neumann's "state of sin" quote.  The
> bug in sample() just means it is easier to find such a function than it
> would otherwise be.
>
> The practical question is whether such a function is likely to arise in
> practice or not.
>
>  > Whether those correspond to commonly used statistics or not, I have no
>  > idea.
>
> I am pretty confident that this bug rarely matters.
>
> > Regarding backwards compatibility: as a user, I'd rather the default
> > sample() do the best possible thing, and take an extra step to use
> > something like sample(..., legacy=TRUE) if I want to reproduce old
> results.
>
> I suspect there's a good chance the bug I discovered today (non-integer
> x values not being truncated) will be declared to be a feature, and the
> documentation will be changed.  Then the rejection sampling approach
> would need to be quite a bit more complicated.
>
> I think a documentation warning about the accuracy of sampling
> probabilities would also be a sufficient fix here, and would be quite a
> bit less trouble than changing the default sample().  But as I said in
> my original post, a contribution of a function without this bug would be
> a nice addition.
>
> Duncan Murdoch
>
> >
> > Regards,
> > Philip
> >
> > On Wed, Sep 19, 2018 at 9:50 AM Duncan Murdoch <murdoch.duncan at gmail.com
> > <mailto:murdoch.duncan at gmail.com>> wrote:
> >
> >     On 19/09/2018 12:23 PM, Philip B. Stark wrote:
> >      > No, the 2nd call only happens when m > 2**31. Here's the code:
> >
> >     Yes, you're right. Sorry!
> >
> >     So the ratio really does come close to 2.  However, the difference in
> >     probabilities between outcomes is still at most 2^-32 when m is less
> >     than that cutoff.  That's not feasible to detect; the only detectable
> >     difference would happen if some event was constructed to hold an
> >     abundance of outcomes with especially low (or especially high)
> >     probability.
> >
> >     As I said in my original post, it's probably not hard to construct
> such
> >     a thing, but as I've said more recently, it probably wouldn't happen
> by
> >     chance.  Here's one attempt to do it:
> >
> >     Call the values from unif_rand() "the unif_rand() outcomes".  Call
> the
> >     values from sample() the sample outcomes.
> >
> >     It would be easiest to see the error if half of the sample() outcomes
> >     used two unif_rand() outcomes, and half used just one.  That would
> mean
> >     m should be (2/3) * 2^32, but that's too big and would trigger the
> >     other
> >     version.
> >
> >     So how about half use 2 unif_rands(), and half use 3?  That means m =
> >     (2/5) * 2^32 = 1717986918.  A good guess is that sample() outcomes
> >     would
> >     alternate between the two possibilities, so our event could be even
> >     versus odd outcomes.
> >
> >     Let's try it:
> >
> >       > m <- (2/5)*2^32
> >       > m > 2^31
> >     [1] FALSE
> >       > x <- sample(m, 1000000, replace = TRUE)
> >       > table(x %% 2)
> >
> >            0      1
> >     399850 600150
> >
> >     Since m is an even number, the true proportions of evens and odds
> >     should
> >     be exactly 0.5.  That's some pretty strong evidence of the bug in the
> >     generator.  (Note that the ratio of the observed probabilities is
> about
> >     1.5, so I may not be the first person to have done this.)
> >
> >     I'm still not convinced that there has ever been a simulation run
> with
> >     detectable bias compared to Monte Carlo error unless it (like this
> one)
> >     was designed specifically to show the problem.
> >
> >     Duncan Murdoch
> >
> >      >
> >      > (RNG.c, lines 793ff)
> >      >
> >      > double R_unif_index(double dn)
> >      > {
> >      >      double cut = INT_MAX;
> >      >
> >      >      switch(RNG_kind) {
> >      >      case KNUTH_TAOCP:
> >      >      case USER_UNIF:
> >      >      case KNUTH_TAOCP2:
> >      > cut = 33554431.0; /* 2^25 - 1 */
> >      > break;
> >      >      default:
> >      > break;
> >      >     }
> >      >
> >      >      double u = dn > cut ? ru() : unif_rand();
> >      >      return floor(dn * u);
> >      > }
> >      >
> >      > On Wed, Sep 19, 2018 at 9:20 AM Duncan Murdoch
> >     <murdoch.duncan at gmail.com <mailto:murdoch.duncan at gmail.com>
> >      > <mailto:murdoch.duncan at gmail.com
> >     <mailto:murdoch.duncan at gmail.com>>> wrote:
> >      >
> >      >     On 19/09/2018 12:09 PM, Philip B. Stark wrote:
> >      >      > The 53 bits only encode at most 2^{32} possible values,
> >     because the
> >      >      > source of the float is the output of a 32-bit PRNG (the
> >     obsolete
> >      >     version
> >      >      > of MT). 53 bits isn't the relevant number here.
> >      >
> >      >     No, two calls to unif_rand() are used.  There are two 32 bit
> >     values,
> >      >     but
> >      >     some of the bits are thrown away.
> >      >
> >      >     Duncan Murdoch
> >      >
> >      >      >
> >      >      > The selection ratios can get close to 2. Computer
> scientists
> >      >     don't do it
> >      >      > the way R does, for a reason.
> >      >      >
> >      >      > Regards,
> >      >      > Philip
> >      >      >
> >      >      > On Wed, Sep 19, 2018 at 9:05 AM Duncan Murdoch
> >      >     <murdoch.duncan at gmail.com <mailto:murdoch.duncan at gmail.com>
> >     <mailto:murdoch.duncan at gmail.com <mailto:murdoch.duncan at gmail.com>>
> >      >      > <mailto:murdoch.duncan at gmail.com
> >     <mailto:murdoch.duncan at gmail.com>
> >      >     <mailto:murdoch.duncan at gmail.com
> >     <mailto:murdoch.duncan at gmail.com>>>> wrote:
> >      >      >
> >      >      >     On 19/09/2018 9:09 AM, I?aki Ucar wrote:
> >      >      >      > El mi?., 19 sept. 2018 a las 14:43, Duncan Murdoch
> >      >      >      > (<murdoch.duncan at gmail.com
> >     <mailto:murdoch.duncan at gmail.com>
> >      >     <mailto:murdoch.duncan at gmail.com
> >     <mailto:murdoch.duncan at gmail.com>> <mailto:murdoch.duncan at gmail.com
> >     <mailto:murdoch.duncan at gmail.com>
> >      >     <mailto:murdoch.duncan at gmail.com
> >     <mailto:murdoch.duncan at gmail.com>>>>)
> >      >      >     escribi?:
> >      >      >      >>
> >      >      >      >> On 18/09/2018 5:46 PM, Carl Boettiger wrote:
> >      >      >      >>> Dear list,
> >      >      >      >>>
> >      >      >      >>> It looks to me that R samples random integers
> >     using an
> >      >      >     intuitive but biased
> >      >      >      >>> algorithm by going from a random number on [0,1)
> from
> >      >     the PRNG
> >      >      >     to a random
> >      >      >      >>> integer, e.g.
> >      >      >      >>>
> >      >      >
> >
> https://github.com/wch/r-source/blob/tags/R-3-5-1/src/main/RNG.c#L808
> >      >      >      >>>
> >      >      >      >>> Many other languages use various rejection
> sampling
> >      >     approaches
> >      >      >     which
> >      >      >      >>> provide an unbiased method for sampling, such as
> >     in Go,
> >      >     python,
> >      >      >     and others
> >      >      >      >>> described here: https://arxiv.org/abs/1805.10941
> (I
> >      >     believe the
> >      >      >     biased
> >      >      >      >>> algorithm currently used in R is also described
> >     there).  I'm
> >      >      >     not an expert
> >      >      >      >>> in this area, but does it make sense for the R to
> >     adopt
> >      >     one of
> >      >      >     the unbiased
> >      >      >      >>> random sample algorithms outlined there and used
> >     in other
> >      >      >     languages?  Would
> >      >      >      >>> a patch providing such an algorithm be welcome?
> What
> >      >     concerns
> >      >      >     would need to
> >      >      >      >>> be addressed first?
> >      >      >      >>>
> >      >      >      >>> I believe this issue was also raised by Killie &
> >     Philip in
> >      >      >      >>>
> >      > http://r.789695.n4.nabble.com/Bug-in-sample-td4729483.html, and
> >      >      >     more
> >      >      >      >>> recently in
> >      >      >      >>>
> >      >      >
> >      >
> >     https://www.stat.berkeley.edu/~stark/Preprints/r-random-issues.pdf
> >     <
> https://www.stat.berkeley.edu/%7Estark/Preprints/r-random-issues.pdf>
> >      >
> >       <
> https://www.stat.berkeley.edu/%7Estark/Preprints/r-random-issues.pdf>
> >      >      >
> >      >
> >       <
> https://www.stat.berkeley.edu/%7Estark/Preprints/r-random-issues.pdf>,
> >      >      >      >>> pointing to the python implementation for
> comparison:
> >      >      >      >>>
> >      >      >
> >      >
> >
> https://github.com/statlab/cryptorandom/blob/master/cryptorandom/cryptorandom.py#L265
> >      >      >      >>
> >      >      >      >> I think the analyses are correct, but I doubt if a
> >     change
> >      >     to the
> >      >      >     default
> >      >      >      >> is likely to be accepted as it would make it more
> >      >     difficult to
> >      >      >     reproduce
> >      >      >      >> older results.
> >      >      >      >>
> >      >      >      >> On the other hand, a contribution of a new
> >     function like
> >      >      >     sample() but
> >      >      >      >> not suffering from the bias would be good.  The
> >     normal way to
> >      >      >     make such
> >      >      >      >> a contribution is in a user contributed package.
> >      >      >      >>
> >      >      >      >> By the way, R code illustrating the bias is
> >     probably not very
> >      >      >     hard to
> >      >      >      >> put together.  I believe the bias manifests itself
> in
> >      >     sample()
> >      >      >     producing
> >      >      >      >> values with two different probabilities (instead
> >     of all equal
> >      >      >      >> probabilities).  Those may differ by as much as
> >     one part in
> >      >      >     2^32.  It's
> >      >      >      >
> >      >      >      > According to Kellie and Philip, in the attachment
> >     of the
> >      >     thread
> >      >      >      > referenced by Carl, "The maximum ratio of selection
> >      >     probabilities can
> >      >      >      > get as large as 1.5 if n is just below 2^31".
> >      >      >
> >      >      >     Sorry, I didn't write very well.  I meant to say that
> the
> >      >     difference in
> >      >      >     probabilities would be 2^-32, not that the ratio of
> >      >     probabilities would
> >      >      >     be 1 + 2^-32.
> >      >      >
> >      >      >     By the way, I don't see the statement giving the ratio
> as
> >      >     1.5, but
> >      >      >     maybe
> >      >      >     I was looking in the wrong place.  In Theorem 1 of the
> >     paper
> >      >     I was
> >      >      >     looking in the ratio was "1 + m 2^{-w + 1}".  In that
> >     formula
> >      >     m is your
> >      >      >     n.  If it is near 2^31, R uses w = 57 random bits, so
> >     the ratio
> >      >      >     would be
> >      >      >     very, very small (one part in 2^25).
> >      >      >
> >      >      >     The worst case for R would happen when m  is just below
> >      >     2^25, where w
> >      >      >     is at least 31 for the default generators.  In that
> >     case the
> >      >     ratio
> >      >      >     could
> >      >      >     be about 1.03.
> >      >      >
> >      >      >     Duncan Murdoch
> >      >      >
> >      >      >
> >      >      >
> >      >      > --
> >      >      > Philip B. Stark | Associate Dean, Mathematical and Physical
> >      >     Sciences |
> >      >      > Professor,  Department of Statistics |
> >      >      > University of California
> >      >      > Berkeley, CA 94720-3860 | 510-394-5077 |
> >      > statistics.berkeley.edu/~stark
> >     <http://statistics.berkeley.edu/%7Estark>
> >      >     <http://statistics.berkeley.edu/%7Estark>
> >      >      > <http://statistics.berkeley.edu/%7Estark> |
> >      >      > @philipbstark
> >      >      >
> >      >
> >      >
> >      >
> >      > --
> >      > Philip B. Stark | Associate Dean, Mathematical and Physical
> >     Sciences |
> >      > Professor,  Department of Statistics |
> >      > University of California
> >      > Berkeley, CA 94720-3860 | 510-394-5077 |
> >     statistics.berkeley.edu/~stark
> >     <http://statistics.berkeley.edu/%7Estark>
> >      > <http://statistics.berkeley.edu/%7Estark> |
> >      > @philipbstark
> >      >
> >
> >
> >
> > --
> > Philip B. Stark | Associate Dean, Mathematical and Physical Sciences |
> > Professor,  Department of Statistics |
> > University of California
> > Berkeley, CA 94720-3860 | 510-394-5077 | statistics.berkeley.edu/~stark
> > <http://statistics.berkeley.edu/%7Estark> |
> > @philipbstark
> >
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>
-- 
Sent from Gmail Mobile

	[[alternative HTML version deleted]]


From murdoch@dunc@n @ending from gm@il@com  Thu Sep 20 00:08:28 2018
From: murdoch@dunc@n @ending from gm@il@com (Duncan Murdoch)
Date: Wed, 19 Sep 2018 18:08:28 -0400
Subject: [Rd] Bias in R's random integers?
In-Reply-To: <CAARY7kgM5WLkF7e1jnj1Pn_9WQzeHmuibROz58HDwWDUTuUmeQ@mail.gmail.com>
References: <CAN_1p9wVX=AR8aYtC2LJo0bgN9cRW3XcbYJqnVHdCzA-QA1ceg@mail.gmail.com>
 <ffb74021-8d95-a820-8013-f0e51dc1a9b3@gmail.com>
 <CALEXWq3booRRTSYvYqcjV7qPZGOC1oh5o45b5o=J2ypQi5s-ag@mail.gmail.com>
 <ece52410-ddb2-2198-9291-201475d3f46e@gmail.com>
 <CAA7KnHHEejYVn8NtKrY1KuKcJUq8n4LoJSDx9BZv-LJyC=r9yQ@mail.gmail.com>
 <73c4667d-38ad-f5ef-468c-4c368e0fdc12@gmail.com>
 <CAA7KnHFcY5YJ2kTAfPJJ0PHwcWx7CumE49A4NgJWC2NK5CV=-A@mail.gmail.com>
 <c3590da8-2e92-4e46-3a81-0d0dbfc81845@gmail.com>
 <CAA7KnHE9OSxML3Cp8Y-gd7CbBj-86UP9qAy6yzgvfcb+=Vg86Q@mail.gmail.com>
 <e82cfff0-3a8e-1d7b-de5c-02b69974ae16@gmail.com>
 <CAARY7kgM5WLkF7e1jnj1Pn_9WQzeHmuibROz58HDwWDUTuUmeQ@mail.gmail.com>
Message-ID: <e6898826-464a-50dc-3906-009c37b4819a@gmail.com>

On 19/09/2018 5:57 PM, David Hugh-Jones wrote:
> 
> It doesn't seem too hard to come up with plausible ways in which this 
> could give bad results. Suppose I sample rows from a large dataset, 
> maybe for bootstrapping. Suppose the rows are non-randomly ordered, e.g. 
> odd rows are males, even rows are females. Oops! Very non-representative 
> sample, bootstrap p values are garbage.

That would only happen if your dataset was exactly 1717986918 elements 
in size. (And in fact, it will be less extreme than I posted:  I had x 
set to 1717986918.4, as described in another thread.  If you use an 
integer value you need a different pattern; add or subtract an element 
or two and the pattern needed to see a problem changes drastically.)

But if you're sampling from a dataset of that exact size, then you 
should worry about this bug. Don't use sample().  Use the algorithm that 
Carl described.

Duncan Murdoch

> 
> David
> 
> On Wed, 19 Sep 2018 at 21:20, Duncan Murdoch <murdoch.duncan at gmail.com 
> <mailto:murdoch.duncan at gmail.com>> wrote:
> 
>     On 19/09/2018 3:52 PM, Philip B. Stark wrote:
>      > Hi Duncan--
>      >
>      >
> 
>     That is a mathematically true statement, but I suspect it is not very
>     relevant.? Pseudo-random number generators always have test functions
>     whose sample averages are quite different from the expectation under
>     the
>     true distribution.? Remember Von Neumann's "state of sin" quote.? The
>     bug in sample() just means it is easier to find such a function than it
>     would otherwise be.
> 
>     The practical question is whether such a function is likely to arise in
>     practice or not.
> 
>      ?> Whether those correspond to commonly used statistics or not, I
>     have no
>      ?> idea.
> 
>     I am pretty confident that this bug rarely matters.
> 
>      > Regarding backwards compatibility: as a user, I'd rather the default
>      > sample() do the best possible thing, and take an extra step to use
>      > something like sample(..., legacy=TRUE) if I want to reproduce
>     old results.
> 
>     I suspect there's a good chance the bug I discovered today (non-integer
>     x values not being truncated) will be declared to be a feature, and the
>     documentation will be changed.? Then the rejection sampling approach
>     would need to be quite a bit more complicated.
> 
>     I think a documentation warning about the accuracy of sampling
>     probabilities would also be a sufficient fix here, and would be quite a
>     bit less trouble than changing the default sample().? But as I said in
>     my original post, a contribution of a function without this bug
>     would be
>     a nice addition.
> 
>     Duncan Murdoch
> 
>      >
>      > Regards,
>      > Philip
>      >
>      > On Wed, Sep 19, 2018 at 9:50 AM Duncan Murdoch
>     <murdoch.duncan at gmail.com <mailto:murdoch.duncan at gmail.com>
>      > <mailto:murdoch.duncan at gmail.com
>     <mailto:murdoch.duncan at gmail.com>>> wrote:
>      >
>      >? ? ?On 19/09/2018 12:23 PM, Philip B. Stark wrote:
>      >? ? ? > No, the 2nd call only happens when m > 2**31. Here's the code:
>      >
>      >? ? ?Yes, you're right. Sorry!
>      >
>      >? ? ?So the ratio really does come close to 2.? However, the
>     difference in
>      >? ? ?probabilities between outcomes is still at most 2^-32 when m
>     is less
>      >? ? ?than that cutoff.? That's not feasible to detect; the only
>     detectable
>      >? ? ?difference would happen if some event was constructed to hold an
>      >? ? ?abundance of outcomes with especially low (or especially high)
>      >? ? ?probability.
>      >
>      >? ? ?As I said in my original post, it's probably not hard to
>     construct such
>      >? ? ?a thing, but as I've said more recently, it probably wouldn't
>     happen by
>      >? ? ?chance.? Here's one attempt to do it:
>      >
>      >? ? ?Call the values from unif_rand() "the unif_rand() outcomes". 
>     Call the
>      >? ? ?values from sample() the sample outcomes.
>      >
>      >? ? ?It would be easiest to see the error if half of the sample()
>     outcomes
>      >? ? ?used two unif_rand() outcomes, and half used just one.? That
>     would mean
>      >? ? ?m should be (2/3) * 2^32, but that's too big and would
>     trigger the
>      >? ? ?other
>      >? ? ?version.
>      >
>      >? ? ?So how about half use 2 unif_rands(), and half use 3?? That
>     means m =
>      >? ? ?(2/5) * 2^32 = 1717986918.? A good guess is that sample()
>     outcomes
>      >? ? ?would
>      >? ? ?alternate between the two possibilities, so our event could
>     be even
>      >? ? ?versus odd outcomes.
>      >
>      >? ? ?Let's try it:
>      >
>      >? ? ? ?> m <- (2/5)*2^32
>      >? ? ? ?> m > 2^31
>      >? ? ?[1] FALSE
>      >? ? ? ?> x <- sample(m, 1000000, replace = TRUE)
>      >? ? ? ?> table(x %% 2)
>      >
>      >? ? ? ? ? ? 0? ? ? 1
>      >? ? ?399850 600150
>      >
>      >? ? ?Since m is an even number, the true proportions of evens and odds
>      >? ? ?should
>      >? ? ?be exactly 0.5.? That's some pretty strong evidence of the
>     bug in the
>      >? ? ?generator.? (Note that the ratio of the observed
>     probabilities is about
>      >? ? ?1.5, so I may not be the first person to have done this.)
>      >
>      >? ? ?I'm still not convinced that there has ever been a simulation
>     run with
>      >? ? ?detectable bias compared to Monte Carlo error unless it (like
>     this one)
>      >? ? ?was designed specifically to show the problem.
>      >
>      >? ? ?Duncan Murdoch
>      >
>      >? ? ? >
>      >? ? ? > (RNG.c, lines 793ff)
>      >? ? ? >
>      >? ? ? > double R_unif_index(double dn)
>      >? ? ? > {
>      >? ? ? >? ? ? double cut = INT_MAX;
>      >? ? ? >
>      >? ? ? >? ? ? switch(RNG_kind) {
>      >? ? ? >? ? ? case KNUTH_TAOCP:
>      >? ? ? >? ? ? case USER_UNIF:
>      >? ? ? >? ? ? case KNUTH_TAOCP2:
>      >? ? ? > cut = 33554431.0; /* 2^25 - 1 */
>      >? ? ? > break;
>      >? ? ? >? ? ? default:
>      >? ? ? > break;
>      >? ? ? >? ? ?}
>      >? ? ? >
>      >? ? ? >? ? ? double u = dn > cut ? ru() : unif_rand();
>      >? ? ? >? ? ? return floor(dn * u);
>      >? ? ? > }
>      >? ? ? >
>      >? ? ? > On Wed, Sep 19, 2018 at 9:20 AM Duncan Murdoch
>      >? ? ?<murdoch.duncan at gmail.com <mailto:murdoch.duncan at gmail.com>
>     <mailto:murdoch.duncan at gmail.com <mailto:murdoch.duncan at gmail.com>>
>      >? ? ? > <mailto:murdoch.duncan at gmail.com
>     <mailto:murdoch.duncan at gmail.com>
>      >? ? ?<mailto:murdoch.duncan at gmail.com
>     <mailto:murdoch.duncan at gmail.com>>>> wrote:
>      >? ? ? >
>      >? ? ? >? ? ?On 19/09/2018 12:09 PM, Philip B. Stark wrote:
>      >? ? ? >? ? ? > The 53 bits only encode at most 2^{32} possible values,
>      >? ? ?because the
>      >? ? ? >? ? ? > source of the float is the output of a 32-bit PRNG (the
>      >? ? ?obsolete
>      >? ? ? >? ? ?version
>      >? ? ? >? ? ? > of MT). 53 bits isn't the relevant number here.
>      >? ? ? >
>      >? ? ? >? ? ?No, two calls to unif_rand() are used.? There are two
>     32 bit
>      >? ? ?values,
>      >? ? ? >? ? ?but
>      >? ? ? >? ? ?some of the bits are thrown away.
>      >? ? ? >
>      >? ? ? >? ? ?Duncan Murdoch
>      >? ? ? >
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? > The selection ratios can get close to 2. Computer
>     scientists
>      >? ? ? >? ? ?don't do it
>      >? ? ? >? ? ? > the way R does, for a reason.
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? > Regards,
>      >? ? ? >? ? ? > Philip
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? > On Wed, Sep 19, 2018 at 9:05 AM Duncan Murdoch
>      >? ? ? >? ? ?<murdoch.duncan at gmail.com
>     <mailto:murdoch.duncan at gmail.com> <mailto:murdoch.duncan at gmail.com
>     <mailto:murdoch.duncan at gmail.com>>
>      >? ? ?<mailto:murdoch.duncan at gmail.com
>     <mailto:murdoch.duncan at gmail.com> <mailto:murdoch.duncan at gmail.com
>     <mailto:murdoch.duncan at gmail.com>>>
>      >? ? ? >? ? ? > <mailto:murdoch.duncan at gmail.com
>     <mailto:murdoch.duncan at gmail.com>
>      >? ? ?<mailto:murdoch.duncan at gmail.com
>     <mailto:murdoch.duncan at gmail.com>>
>      >? ? ? >? ? ?<mailto:murdoch.duncan at gmail.com
>     <mailto:murdoch.duncan at gmail.com>
>      >? ? ?<mailto:murdoch.duncan at gmail.com
>     <mailto:murdoch.duncan at gmail.com>>>>> wrote:
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? >? ? ?On 19/09/2018 9:09 AM, I?aki Ucar wrote:
>      >? ? ? >? ? ? >? ? ? > El mi?., 19 sept. 2018 a las 14:43, Duncan
>     Murdoch
>      >? ? ? >? ? ? >? ? ? > (<murdoch.duncan at gmail.com
>     <mailto:murdoch.duncan at gmail.com>
>      >? ? ?<mailto:murdoch.duncan at gmail.com
>     <mailto:murdoch.duncan at gmail.com>>
>      >? ? ? >? ? ?<mailto:murdoch.duncan at gmail.com
>     <mailto:murdoch.duncan at gmail.com>
>      >? ? ?<mailto:murdoch.duncan at gmail.com
>     <mailto:murdoch.duncan at gmail.com>>> <mailto:murdoch.duncan at gmail.com
>     <mailto:murdoch.duncan at gmail.com>
>      >? ? ?<mailto:murdoch.duncan at gmail.com
>     <mailto:murdoch.duncan at gmail.com>>
>      >? ? ? >? ? ?<mailto:murdoch.duncan at gmail.com
>     <mailto:murdoch.duncan at gmail.com>
>      >? ? ?<mailto:murdoch.duncan at gmail.com
>     <mailto:murdoch.duncan at gmail.com>>>>>)
>      >? ? ? >? ? ? >? ? ?escribi?:
>      >? ? ? >? ? ? >? ? ? >>
>      >? ? ? >? ? ? >? ? ? >> On 18/09/2018 5:46 PM, Carl Boettiger wrote:
>      >? ? ? >? ? ? >? ? ? >>> Dear list,
>      >? ? ? >? ? ? >? ? ? >>>
>      >? ? ? >? ? ? >? ? ? >>> It looks to me that R samples random integers
>      >? ? ?using an
>      >? ? ? >? ? ? >? ? ?intuitive but biased
>      >? ? ? >? ? ? >? ? ? >>> algorithm by going from a random number on
>     [0,1) from
>      >? ? ? >? ? ?the PRNG
>      >? ? ? >? ? ? >? ? ?to a random
>      >? ? ? >? ? ? >? ? ? >>> integer, e.g.
>      >? ? ? >? ? ? >? ? ? >>>
>      >? ? ? >? ? ? >
>      > https://github.com/wch/r-source/blob/tags/R-3-5-1/src/main/RNG.c#L808
>      >? ? ? >? ? ? >? ? ? >>>
>      >? ? ? >? ? ? >? ? ? >>> Many other languages use various rejection
>     sampling
>      >? ? ? >? ? ?approaches
>      >? ? ? >? ? ? >? ? ?which
>      >? ? ? >? ? ? >? ? ? >>> provide an unbiased method for sampling,
>     such as
>      >? ? ?in Go,
>      >? ? ? >? ? ?python,
>      >? ? ? >? ? ? >? ? ?and others
>      >? ? ? >? ? ? >? ? ? >>> described here:
>     https://arxiv.org/abs/1805.10941 (I
>      >? ? ? >? ? ?believe the
>      >? ? ? >? ? ? >? ? ?biased
>      >? ? ? >? ? ? >? ? ? >>> algorithm currently used in R is also
>     described
>      >? ? ?there).? I'm
>      >? ? ? >? ? ? >? ? ?not an expert
>      >? ? ? >? ? ? >? ? ? >>> in this area, but does it make sense for
>     the R to
>      >? ? ?adopt
>      >? ? ? >? ? ?one of
>      >? ? ? >? ? ? >? ? ?the unbiased
>      >? ? ? >? ? ? >? ? ? >>> random sample algorithms outlined there
>     and used
>      >? ? ?in other
>      >? ? ? >? ? ? >? ? ?languages?? Would
>      >? ? ? >? ? ? >? ? ? >>> a patch providing such an algorithm be
>     welcome? What
>      >? ? ? >? ? ?concerns
>      >? ? ? >? ? ? >? ? ?would need to
>      >? ? ? >? ? ? >? ? ? >>> be addressed first?
>      >? ? ? >? ? ? >? ? ? >>>
>      >? ? ? >? ? ? >? ? ? >>> I believe this issue was also raised by
>     Killie &
>      >? ? ?Philip in
>      >? ? ? >? ? ? >? ? ? >>>
>      >? ? ? >
>     http://r.789695.n4.nabble.com/Bug-in-sample-td4729483.html, and
>      >? ? ? >? ? ? >? ? ?more
>      >? ? ? >? ? ? >? ? ? >>> recently in
>      >? ? ? >? ? ? >? ? ? >>>
>      >? ? ? >? ? ? >
>      >? ? ? >
>      >
>     https://www.stat.berkeley.edu/~stark/Preprints/r-random-issues.pdf
>     <https://www.stat.berkeley.edu/%7Estark/Preprints/r-random-issues.pdf>
>      >   
>      ?<https://www.stat.berkeley.edu/%7Estark/Preprints/r-random-issues.pdf>
>      >? ? ? >
>      >     
>      ?<https://www.stat.berkeley.edu/%7Estark/Preprints/r-random-issues.pdf>
>      >? ? ? >? ? ? >
>      >? ? ? >
>      >     
>      ?<https://www.stat.berkeley.edu/%7Estark/Preprints/r-random-issues.pdf>,
>      >? ? ? >? ? ? >? ? ? >>> pointing to the python implementation for
>     comparison:
>      >? ? ? >? ? ? >? ? ? >>>
>      >? ? ? >? ? ? >
>      >? ? ? >
>      >
>     https://github.com/statlab/cryptorandom/blob/master/cryptorandom/cryptorandom.py#L265
>      >? ? ? >? ? ? >? ? ? >>
>      >? ? ? >? ? ? >? ? ? >> I think the analyses are correct, but I
>     doubt if a
>      >? ? ?change
>      >? ? ? >? ? ?to the
>      >? ? ? >? ? ? >? ? ?default
>      >? ? ? >? ? ? >? ? ? >> is likely to be accepted as it would make
>     it more
>      >? ? ? >? ? ?difficult to
>      >? ? ? >? ? ? >? ? ?reproduce
>      >? ? ? >? ? ? >? ? ? >> older results.
>      >? ? ? >? ? ? >? ? ? >>
>      >? ? ? >? ? ? >? ? ? >> On the other hand, a contribution of a new
>      >? ? ?function like
>      >? ? ? >? ? ? >? ? ?sample() but
>      >? ? ? >? ? ? >? ? ? >> not suffering from the bias would be good.? The
>      >? ? ?normal way to
>      >? ? ? >? ? ? >? ? ?make such
>      >? ? ? >? ? ? >? ? ? >> a contribution is in a user contributed
>     package.
>      >? ? ? >? ? ? >? ? ? >>
>      >? ? ? >? ? ? >? ? ? >> By the way, R code illustrating the bias is
>      >? ? ?probably not very
>      >? ? ? >? ? ? >? ? ?hard to
>      >? ? ? >? ? ? >? ? ? >> put together.? I believe the bias manifests
>     itself in
>      >? ? ? >? ? ?sample()
>      >? ? ? >? ? ? >? ? ?producing
>      >? ? ? >? ? ? >? ? ? >> values with two different probabilities
>     (instead
>      >? ? ?of all equal
>      >? ? ? >? ? ? >? ? ? >> probabilities).? Those may differ by as much as
>      >? ? ?one part in
>      >? ? ? >? ? ? >? ? ?2^32.? It's
>      >? ? ? >? ? ? >? ? ? >
>      >? ? ? >? ? ? >? ? ? > According to Kellie and Philip, in the
>     attachment
>      >? ? ?of the
>      >? ? ? >? ? ?thread
>      >? ? ? >? ? ? >? ? ? > referenced by Carl, "The maximum ratio of
>     selection
>      >? ? ? >? ? ?probabilities can
>      >? ? ? >? ? ? >? ? ? > get as large as 1.5 if n is just below 2^31".
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? >? ? ?Sorry, I didn't write very well.? I meant to
>     say that the
>      >? ? ? >? ? ?difference in
>      >? ? ? >? ? ? >? ? ?probabilities would be 2^-32, not that the ratio of
>      >? ? ? >? ? ?probabilities would
>      >? ? ? >? ? ? >? ? ?be 1 + 2^-32.
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? >? ? ?By the way, I don't see the statement giving
>     the ratio as
>      >? ? ? >? ? ?1.5, but
>      >? ? ? >? ? ? >? ? ?maybe
>      >? ? ? >? ? ? >? ? ?I was looking in the wrong place.? In Theorem 1
>     of the
>      >? ? ?paper
>      >? ? ? >? ? ?I was
>      >? ? ? >? ? ? >? ? ?looking in the ratio was "1 + m 2^{-w + 1}". 
>     In that
>      >? ? ?formula
>      >? ? ? >? ? ?m is your
>      >? ? ? >? ? ? >? ? ?n.? If it is near 2^31, R uses w = 57 random
>     bits, so
>      >? ? ?the ratio
>      >? ? ? >? ? ? >? ? ?would be
>      >? ? ? >? ? ? >? ? ?very, very small (one part in 2^25).
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? >? ? ?The worst case for R would happen when m? is
>     just below
>      >? ? ? >? ? ?2^25, where w
>      >? ? ? >? ? ? >? ? ?is at least 31 for the default generators.? In that
>      >? ? ?case the
>      >? ? ? >? ? ?ratio
>      >? ? ? >? ? ? >? ? ?could
>      >? ? ? >? ? ? >? ? ?be about 1.03.
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? >? ? ?Duncan Murdoch
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? >
>      >? ? ? >? ? ? > --
>      >? ? ? >? ? ? > Philip B. Stark | Associate Dean, Mathematical and
>     Physical
>      >? ? ? >? ? ?Sciences |
>      >? ? ? >? ? ? > Professor, ?Department of Statistics |
>      >? ? ? >? ? ? > University of California
>      >? ? ? >? ? ? > Berkeley, CA 94720-3860 | 510-394-5077 |
>      >? ? ? > statistics.berkeley.edu/~stark
>     <http://statistics.berkeley.edu/%7Estark>
>      >? ? ?<http://statistics.berkeley.edu/%7Estark>
>      >? ? ? >? ? ?<http://statistics.berkeley.edu/%7Estark>
>      >? ? ? >? ? ? > <http://statistics.berkeley.edu/%7Estark> |
>      >? ? ? >? ? ? > @philipbstark
>      >? ? ? >? ? ? >
>      >? ? ? >
>      >? ? ? >
>      >? ? ? >
>      >? ? ? > --
>      >? ? ? > Philip B. Stark | Associate Dean, Mathematical and Physical
>      >? ? ?Sciences |
>      >? ? ? > Professor, ?Department of Statistics |
>      >? ? ? > University of California
>      >? ? ? > Berkeley, CA 94720-3860 | 510-394-5077 |
>      > statistics.berkeley.edu/~stark
>     <http://statistics.berkeley.edu/%7Estark>
>      >? ? ?<http://statistics.berkeley.edu/%7Estark>
>      >? ? ? > <http://statistics.berkeley.edu/%7Estark> |
>      >? ? ? > @philipbstark
>      >? ? ? >
>      >
>      >
>      >
>      > --
>      > Philip B. Stark | Associate Dean, Mathematical and Physical
>     Sciences |
>      > Professor, ?Department of Statistics |
>      > University of California
>      > Berkeley, CA 94720-3860 | 510-394-5077 |
>     statistics.berkeley.edu/~stark
>     <http://statistics.berkeley.edu/%7Estark>
>      > <http://statistics.berkeley.edu/%7Estark> |
>      > @philipbstark
>      >
> 
>     ______________________________________________
>     R-devel at r-project.org <mailto:R-devel at r-project.org> mailing list
>     https://stat.ethz.ch/mailman/listinfo/r-devel
> 
> -- 
> Sent from Gmail Mobile


From frederik m@ili@g off ofb@@et  Thu Sep 20 00:34:02 2018
From: frederik m@ili@g off ofb@@et (frederik m@ili@g off ofb@@et)
Date: Wed, 19 Sep 2018 15:34:02 -0700
Subject: [Rd] Poor documentation for "adj" and text()
In-Reply-To: <23451.25986.467626.955824@stat.math.ethz.ch>
References: <58EC8486020000A1000258ED@gwsmtp1.uni-regensburg.de>
 <20170411190105.GV3858@ofb.net> <20180914063341.GC28532@ofb.net>
 <23451.25986.467626.955824@stat.math.ethz.ch>
Message-ID: <20180919223401.GF22921@ofb.net>

Thanks Martin.

I wouldn't necessarily fault Ulrich for his subject line - unless you
want to propose a better one... I might fault him for not following up
and checking out the patch that I submitted at his prompting.

I noticed that you committed my patch last Friday, with some welcome
improvements, thanks! I'm assuming there is no easy way to get
notified about commits, I just have to do 'svn update'?

FWIW there are a lot of things that I would want to improve about R if
I had the skill and the ability to do so. The patch hints at what I
would improve about text rotation. Additionally, perhaps some other
documentation issues; the fact that large plots can take forever and
are not interruptable on the command line; I want better compatibility
with tiling window managers (plots shouldn't have to be redrawn every
time I switch to the window; they should come up with correct
dimensions); better readline history facilities. I wouldn't assume
that just because something has been the same way for decades that
people like it that way! On the other hand I can see from the logs
that there are at least several commits per day, so there must be a
trade-off in different directions that the project can take.

I made this graph:

https://i.imgur.com/l4uCOek.png

Best wishes,

Frederick

On Fri, Sep 14, 2018 at 09:38:42AM +0200, Martin Maechler wrote:
> >>>>>   
> >>>>>     on Thu, 13 Sep 2018 23:33:41 -0700 writes:
> 
>     > Hello Core Team, I sent this patch over a year ago. It
>     > looks like it was sent in response to another user's
>     > complaint which echoed some of my own observations about
>     > problems in the documentation for 'text'. Did anyone have
>     > a chance to look it over? 
> 
> I see it marked in my box as some (of too many !) thing I had
> wanted to look at.
> 
> OTOH,   R core  may not be terribly motivated by e-mail threads
> starting with "Poor <something>" notably when that thing has
> been in R (and S before that) for decades.
> OTOH^2,  improving documentation is often a good and helpful
> thing.... and I will look at it now.
> 
> Thank you, Frederick, for trying to help making R better!
> Martin
> 
> 
>     > I'd like to get it out of my queue.
> 
>     > Thanks,
> 
>     > Frederick
> 
>     > On Tue, Apr 11, 2017 at 12:01:05PM -0700, frederik at ofb.net
>     > wrote:
>     >> Thanks Ulrich for sharing your experience.
>     >> 
>     >> I'm attaching a patch which tries to address the issues
>     >> you raised.
>     >> 
>     >> I agree with you in principle, but I think it makes sense
>     >> to leave some details under "Details". However, the
>     >> descriptions in "Arguments" should give enough
>     >> information that a user can get the function to do
>     >> something predictable in at least one situation, and I
>     >> feel this is not the case at present.
>     >> 
>     >> I tried to fix the wording so that 'adj' and 'offset' are
>     >> no longer confusing to new users (or to me, every time I
>     >> forget what they mean).
>     >> 
>     >> I also fixed the paragraph on rotated text; it is more
>     >> correct now, at least for X11-cairo.
>     >> 
>     >> I hope that someone in the Core Team can look this over
>     >> and apply it.
>     >> 
>     >> Thank you,
>     >> 
>     >> Frederick
>     >> 
>     >> On Tue, Apr 11, 2017 at 09:23:50AM +0200, Ulrich Windl
>     >> wrote: > Hi!
>     >> > 
>     >> > (I'd like to be able to access your bugzilla, BTW) >
>     >> The documentation for parameter "adj" of text() in R
>     >> 3.3.3 is hard to understand (unless you know what it does
>     >> already):
>     >> > 
>     >> > "adj > one or two values in [0, 1] which specify the x
>     >> (and optionally y) adjustment of the labels. On most
>     >> devices values outside that interval will also work."
>     >> > 
>     >> > What is the meaning of the values? I think the
>     >> description ("adj allows adjustment of the text with
>     >> respect to (x, y). Values of 0, 0.5, and 1 specify
>     >> left/bottom, middle and right/top alignment,
>     >> respectively. The default is for centered text, i.e., adj
>     >> = c(0.5, NA). Accurate vertical centering needs character
>     >> metric information on individual characters which is only
>     >> available on some devices. Vertical alignment is done
>     >> slightly differently for character strings and for
>     >> expressions: adj = c(0,0) means to left-justify and to
>     >> align on the baseline for strings but on the bottom of
>     >> the bounding box for expressions. This also affects
>     >> vertical centering: for strings the centering excludes
>     >> any descenders whereas for expressions it includes
>     >> them. Using NA for strings centers them, including
>     >> descenders.") should be moved to the parameter.
>     >> > 
>     >> > In general I'd suggest to describe the range, meaning
>     >> and default of every parameter where the parameter is
>     >> listed. "Details" should only give an overview of the
>     >> functions.
>     >> > 
>     >> > Likewise "offset": Will the direction be influenced by
>     >> "pos"? The description is quite silent on that.
>     >> > 
>     >> > Documentation should be structured to help the user to
>     >> find the facts easily without having to read the whole
>     >> page.
>     >> > 
>     >> > Regards, > Ulrich Windl
>     >> > 
>     >> > ______________________________________________ >
>     >> R-devel at r-project.org mailing list >
>     >> https://stat.ethz.ch/mailman/listinfo/r-devel
>     >> >


From bbolker @ending from gm@il@com  Thu Sep 20 01:17:30 2018
From: bbolker @ending from gm@il@com (Ben Bolker)
Date: Wed, 19 Sep 2018 19:17:30 -0400
Subject: [Rd] Bias in R's random integers?
In-Reply-To: <e82cfff0-3a8e-1d7b-de5c-02b69974ae16@gmail.com>
References: <CAN_1p9wVX=AR8aYtC2LJo0bgN9cRW3XcbYJqnVHdCzA-QA1ceg@mail.gmail.com>
 <ffb74021-8d95-a820-8013-f0e51dc1a9b3@gmail.com>
 <CALEXWq3booRRTSYvYqcjV7qPZGOC1oh5o45b5o=J2ypQi5s-ag@mail.gmail.com>
 <ece52410-ddb2-2198-9291-201475d3f46e@gmail.com>
 <CAA7KnHHEejYVn8NtKrY1KuKcJUq8n4LoJSDx9BZv-LJyC=r9yQ@mail.gmail.com>
 <73c4667d-38ad-f5ef-468c-4c368e0fdc12@gmail.com>
 <CAA7KnHFcY5YJ2kTAfPJJ0PHwcWx7CumE49A4NgJWC2NK5CV=-A@mail.gmail.com>
 <c3590da8-2e92-4e46-3a81-0d0dbfc81845@gmail.com>
 <CAA7KnHE9OSxML3Cp8Y-gd7CbBj-86UP9qAy6yzgvfcb+=Vg86Q@mail.gmail.com>
 <e82cfff0-3a8e-1d7b-de5c-02b69974ae16@gmail.com>
Message-ID: <4143568b-c67f-2309-f9ec-676f9e791747@gmail.com>


  A quick point of order here: arguing with Duncan in this forum is
helpful to expose ideas, but probably neither side will convince the
other; eventually, if you want this adopted in core R, you'll need to
convince an R-core member to pursue this fix.

  In the meantime, a good, well-tested implementation in a
user-contributed package (presumably written in C for speed) would be
enormously useful.  Volunteers ... ?



On 2018-09-19 04:19 PM, Duncan Murdoch wrote:
> On 19/09/2018 3:52 PM, Philip B. Stark wrote:
>> Hi Duncan--
>>
>> Nice simulation!
>>
>> The absolute difference in probabilities is small, but the maximum
>> relative difference grows from something negligible to almost 2 as m
>> approaches 2**31.
>>
>> Because the L_1 distance between the uniform distribution on {1, ...,
>> m} and what you actually get is large, there have to be test functions
>> whose expectations are quite different under the two distributions. 
> 
> That is a mathematically true statement, but I suspect it is not very
> relevant.? Pseudo-random number generators always have test functions
> whose sample averages are quite different from the expectation under the
> true distribution.? Remember Von Neumann's "state of sin" quote.? The
> bug in sample() just means it is easier to find such a function than it
> would otherwise be.
> 
> The practical question is whether such a function is likely to arise in
> practice or not.
> 
>> Whether those correspond to commonly used statistics or not, I have no
>> idea.
> 
> I am pretty confident that this bug rarely matters.
> 
>> Regarding backwards compatibility: as a user, I'd rather the default
>> sample() do the best possible thing, and take an extra step to use
>> something like sample(..., legacy=TRUE) if I want to reproduce old
>> results.
> 
> I suspect there's a good chance the bug I discovered today (non-integer
> x values not being truncated) will be declared to be a feature, and the
> documentation will be changed.? Then the rejection sampling approach
> would need to be quite a bit more complicated.
> 
> I think a documentation warning about the accuracy of sampling
> probabilities would also be a sufficient fix here, and would be quite a
> bit less trouble than changing the default sample().? But as I said in
> my original post, a contribution of a function without this bug would be
> a nice addition.
> 
> Duncan Murdoch
> 
>>
>> Regards,
>> Philip
>>
>> On Wed, Sep 19, 2018 at 9:50 AM Duncan Murdoch
>> <murdoch.duncan at gmail.com <mailto:murdoch.duncan at gmail.com>> wrote:
>>
>> ??? On 19/09/2018 12:23 PM, Philip B. Stark wrote:
>> ???? > No, the 2nd call only happens when m > 2**31. Here's the code:
>>
>> ??? Yes, you're right. Sorry!
>>
>> ??? So the ratio really does come close to 2.? However, the difference in
>> ??? probabilities between outcomes is still at most 2^-32 when m is less
>> ??? than that cutoff.? That's not feasible to detect; the only detectable
>> ??? difference would happen if some event was constructed to hold an
>> ??? abundance of outcomes with especially low (or especially high)
>> ??? probability.
>>
>> ??? As I said in my original post, it's probably not hard to construct
>> such
>> ??? a thing, but as I've said more recently, it probably wouldn't
>> happen by
>> ??? chance.? Here's one attempt to do it:
>>
>> ??? Call the values from unif_rand() "the unif_rand() outcomes".? Call
>> the
>> ??? values from sample() the sample outcomes.
>>
>> ??? It would be easiest to see the error if half of the sample() outcomes
>> ??? used two unif_rand() outcomes, and half used just one.? That would
>> mean
>> ??? m should be (2/3) * 2^32, but that's too big and would trigger the
>> ??? other
>> ??? version.
>>
>> ??? So how about half use 2 unif_rands(), and half use 3?? That means m =
>> ??? (2/5) * 2^32 = 1717986918.? A good guess is that sample() outcomes
>> ??? would
>> ??? alternate between the two possibilities, so our event could be even
>> ??? versus odd outcomes.
>>
>> ??? Let's try it:
>>
>> ???? ?> m <- (2/5)*2^32
>> ???? ?> m > 2^31
>> ??? [1] FALSE
>> ???? ?> x <- sample(m, 1000000, replace = TRUE)
>> ???? ?> table(x %% 2)
>>
>> ???? ? ? ? 0? ? ? 1
>> ??? 399850 600150
>>
>> ??? Since m is an even number, the true proportions of evens and odds
>> ??? should
>> ??? be exactly 0.5.? That's some pretty strong evidence of the bug in the
>> ??? generator.? (Note that the ratio of the observed probabilities is
>> about
>> ??? 1.5, so I may not be the first person to have done this.)
>>
>> ??? I'm still not convinced that there has ever been a simulation run
>> with
>> ??? detectable bias compared to Monte Carlo error unless it (like this
>> one)
>> ??? was designed specifically to show the problem.
>>
>> ??? Duncan Murdoch
>>
>> ???? >
>> ???? > (RNG.c, lines 793ff)
>> ???? >
>> ???? > double R_unif_index(double dn)
>> ???? > {
>> ???? >? ? ? double cut = INT_MAX;
>> ???? >
>> ???? >? ? ? switch(RNG_kind) {
>> ???? >? ? ? case KNUTH_TAOCP:
>> ???? >? ? ? case USER_UNIF:
>> ???? >? ? ? case KNUTH_TAOCP2:
>> ???? > cut = 33554431.0; /* 2^25 - 1 */
>> ???? > break;
>> ???? >? ? ? default:
>> ???? > break;
>> ???? >? ? ?}
>> ???? >
>> ???? >? ? ? double u = dn > cut ? ru() : unif_rand();
>> ???? >? ? ? return floor(dn * u);
>> ???? > }
>> ???? >
>> ???? > On Wed, Sep 19, 2018 at 9:20 AM Duncan Murdoch
>> ??? <murdoch.duncan at gmail.com <mailto:murdoch.duncan at gmail.com>
>> ???? > <mailto:murdoch.duncan at gmail.com
>> ??? <mailto:murdoch.duncan at gmail.com>>> wrote:
>> ???? >
>> ???? >? ? ?On 19/09/2018 12:09 PM, Philip B. Stark wrote:
>> ???? >? ? ? > The 53 bits only encode at most 2^{32} possible values,
>> ??? because the
>> ???? >? ? ? > source of the float is the output of a 32-bit PRNG (the
>> ??? obsolete
>> ???? >? ? ?version
>> ???? >? ? ? > of MT). 53 bits isn't the relevant number here.
>> ???? >
>> ???? >? ? ?No, two calls to unif_rand() are used.? There are two 32 bit
>> ??? values,
>> ???? >? ? ?but
>> ???? >? ? ?some of the bits are thrown away.
>> ???? >
>> ???? >? ? ?Duncan Murdoch
>> ???? >
>> ???? >? ? ? >
>> ???? >? ? ? > The selection ratios can get close to 2. Computer
>> scientists
>> ???? >? ? ?don't do it
>> ???? >? ? ? > the way R does, for a reason.
>> ???? >? ? ? >
>> ???? >? ? ? > Regards,
>> ???? >? ? ? > Philip
>> ???? >? ? ? >
>> ???? >? ? ? > On Wed, Sep 19, 2018 at 9:05 AM Duncan Murdoch
>> ???? >? ? ?<murdoch.duncan at gmail.com <mailto:murdoch.duncan at gmail.com>
>> ??? <mailto:murdoch.duncan at gmail.com <mailto:murdoch.duncan at gmail.com>>
>> ???? >? ? ? > <mailto:murdoch.duncan at gmail.com
>> ??? <mailto:murdoch.duncan at gmail.com>
>> ???? >? ? ?<mailto:murdoch.duncan at gmail.com
>> ??? <mailto:murdoch.duncan at gmail.com>>>> wrote:
>> ???? >? ? ? >
>> ???? >? ? ? >? ? ?On 19/09/2018 9:09 AM, I?aki Ucar wrote:
>> ???? >? ? ? >? ? ? > El mi?., 19 sept. 2018 a las 14:43, Duncan Murdoch
>> ???? >? ? ? >? ? ? > (<murdoch.duncan at gmail.com
>> ??? <mailto:murdoch.duncan at gmail.com>
>> ???? >? ? ?<mailto:murdoch.duncan at gmail.com
>> ??? <mailto:murdoch.duncan at gmail.com>> <mailto:murdoch.duncan at gmail.com
>> ??? <mailto:murdoch.duncan at gmail.com>
>> ???? >? ? ?<mailto:murdoch.duncan at gmail.com
>> ??? <mailto:murdoch.duncan at gmail.com>>>>)
>> ???? >? ? ? >? ? ?escribi?:
>> ???? >? ? ? >? ? ? >>
>> ???? >? ? ? >? ? ? >> On 18/09/2018 5:46 PM, Carl Boettiger wrote:
>> ???? >? ? ? >? ? ? >>> Dear list,
>> ???? >? ? ? >? ? ? >>>
>> ???? >? ? ? >? ? ? >>> It looks to me that R samples random integers
>> ??? using an
>> ???? >? ? ? >? ? ?intuitive but biased
>> ???? >? ? ? >? ? ? >>> algorithm by going from a random number on
>> [0,1) from
>> ???? >? ? ?the PRNG
>> ???? >? ? ? >? ? ?to a random
>> ???? >? ? ? >? ? ? >>> integer, e.g.
>> ???? >? ? ? >? ? ? >>>
>> ???? >? ? ? >
>> ??? https://github.com/wch/r-source/blob/tags/R-3-5-1/src/main/RNG.c#L808
>> ???? >? ? ? >? ? ? >>>
>> ???? >? ? ? >? ? ? >>> Many other languages use various rejection
>> sampling
>> ???? >? ? ?approaches
>> ???? >? ? ? >? ? ?which
>> ???? >? ? ? >? ? ? >>> provide an unbiased method for sampling, such as
>> ??? in Go,
>> ???? >? ? ?python,
>> ???? >? ? ? >? ? ?and others
>> ???? >? ? ? >? ? ? >>> described here:
>> https://arxiv.org/abs/1805.10941 (I
>> ???? >? ? ?believe the
>> ???? >? ? ? >? ? ?biased
>> ???? >? ? ? >? ? ? >>> algorithm currently used in R is also described
>> ??? there).? I'm
>> ???? >? ? ? >? ? ?not an expert
>> ???? >? ? ? >? ? ? >>> in this area, but does it make sense for the R to
>> ??? adopt
>> ???? >? ? ?one of
>> ???? >? ? ? >? ? ?the unbiased
>> ???? >? ? ? >? ? ? >>> random sample algorithms outlined there and used
>> ??? in other
>> ???? >? ? ? >? ? ?languages?? Would
>> ???? >? ? ? >? ? ? >>> a patch providing such an algorithm be welcome?
>> What
>> ???? >? ? ?concerns
>> ???? >? ? ? >? ? ?would need to
>> ???? >? ? ? >? ? ? >>> be addressed first?
>> ???? >? ? ? >? ? ? >>>
>> ???? >? ? ? >? ? ? >>> I believe this issue was also raised by Killie &
>> ??? Philip in
>> ???? >? ? ? >? ? ? >>>
>> ???? > http://r.789695.n4.nabble.com/Bug-in-sample-td4729483.html, and
>> ???? >? ? ? >? ? ?more
>> ???? >? ? ? >? ? ? >>> recently in
>> ???? >? ? ? >? ? ? >>>
>> ???? >? ? ? >
>> ???? >
>> ??? https://www.stat.berkeley.edu/~stark/Preprints/r-random-issues.pdf
>> ???
>> <https://www.stat.berkeley.edu/%7Estark/Preprints/r-random-issues.pdf>
>> ???? >?? ????
>> ?<https://www.stat.berkeley.edu/%7Estark/Preprints/r-random-issues.pdf>
>> ???? >? ? ? >
>> ???? >???? ????
>> ?<https://www.stat.berkeley.edu/%7Estark/Preprints/r-random-issues.pdf>,
>> ???? >? ? ? >? ? ? >>> pointing to the python implementation for
>> comparison:
>> ???? >? ? ? >? ? ? >>>
>> ???? >? ? ? >
>> ???? >
>> ???
>> https://github.com/statlab/cryptorandom/blob/master/cryptorandom/cryptorandom.py#L265
>>
>> ???? >? ? ? >? ? ? >>
>> ???? >? ? ? >? ? ? >> I think the analyses are correct, but I doubt if a
>> ??? change
>> ???? >? ? ?to the
>> ???? >? ? ? >? ? ?default
>> ???? >? ? ? >? ? ? >> is likely to be accepted as it would make it more
>> ???? >? ? ?difficult to
>> ???? >? ? ? >? ? ?reproduce
>> ???? >? ? ? >? ? ? >> older results.
>> ???? >? ? ? >? ? ? >>
>> ???? >? ? ? >? ? ? >> On the other hand, a contribution of a new
>> ??? function like
>> ???? >? ? ? >? ? ?sample() but
>> ???? >? ? ? >? ? ? >> not suffering from the bias would be good.? The
>> ??? normal way to
>> ???? >? ? ? >? ? ?make such
>> ???? >? ? ? >? ? ? >> a contribution is in a user contributed package.
>> ???? >? ? ? >? ? ? >>
>> ???? >? ? ? >? ? ? >> By the way, R code illustrating the bias is
>> ??? probably not very
>> ???? >? ? ? >? ? ?hard to
>> ???? >? ? ? >? ? ? >> put together.? I believe the bias manifests
>> itself in
>> ???? >? ? ?sample()
>> ???? >? ? ? >? ? ?producing
>> ???? >? ? ? >? ? ? >> values with two different probabilities (instead
>> ??? of all equal
>> ???? >? ? ? >? ? ? >> probabilities).? Those may differ by as much as
>> ??? one part in
>> ???? >? ? ? >? ? ?2^32.? It's
>> ???? >? ? ? >? ? ? >
>> ???? >? ? ? >? ? ? > According to Kellie and Philip, in the attachment
>> ??? of the
>> ???? >? ? ?thread
>> ???? >? ? ? >? ? ? > referenced by Carl, "The maximum ratio of selection
>> ???? >? ? ?probabilities can
>> ???? >? ? ? >? ? ? > get as large as 1.5 if n is just below 2^31".
>> ???? >? ? ? >
>> ???? >? ? ? >? ? ?Sorry, I didn't write very well.? I meant to say
>> that the
>> ???? >? ? ?difference in
>> ???? >? ? ? >? ? ?probabilities would be 2^-32, not that the ratio of
>> ???? >? ? ?probabilities would
>> ???? >? ? ? >? ? ?be 1 + 2^-32.
>> ???? >? ? ? >
>> ???? >? ? ? >? ? ?By the way, I don't see the statement giving the
>> ratio as
>> ???? >? ? ?1.5, but
>> ???? >? ? ? >? ? ?maybe
>> ???? >? ? ? >? ? ?I was looking in the wrong place.? In Theorem 1 of the
>> ??? paper
>> ???? >? ? ?I was
>> ???? >? ? ? >? ? ?looking in the ratio was "1 + m 2^{-w + 1}".? In that
>> ??? formula
>> ???? >? ? ?m is your
>> ???? >? ? ? >? ? ?n.? If it is near 2^31, R uses w = 57 random bits, so
>> ??? the ratio
>> ???? >? ? ? >? ? ?would be
>> ???? >? ? ? >? ? ?very, very small (one part in 2^25).
>> ???? >? ? ? >
>> ???? >? ? ? >? ? ?The worst case for R would happen when m? is just below
>> ???? >? ? ?2^25, where w
>> ???? >? ? ? >? ? ?is at least 31 for the default generators.? In that
>> ??? case the
>> ???? >? ? ?ratio
>> ???? >? ? ? >? ? ?could
>> ???? >? ? ? >? ? ?be about 1.03.
>> ???? >? ? ? >
>> ???? >? ? ? >? ? ?Duncan Murdoch
>> ???? >? ? ? >
>> ???? >? ? ? >
>> ???? >? ? ? >
>> ???? >? ? ? > --
>> ???? >? ? ? > Philip B. Stark | Associate Dean, Mathematical and Physical
>> ???? >? ? ?Sciences |
>> ???? >? ? ? > Professor, ?Department of Statistics |
>> ???? >? ? ? > University of California
>> ???? >? ? ? > Berkeley, CA 94720-3860 | 510-394-5077 |
>> ???? > statistics.berkeley.edu/~stark
>> ??? <http://statistics.berkeley.edu/%7Estark>
>> ???? >? ? ?<http://statistics.berkeley.edu/%7Estark>
>> ???? >? ? ? > <http://statistics.berkeley.edu/%7Estark> |
>> ???? >? ? ? > @philipbstark
>> ???? >? ? ? >
>> ???? >
>> ???? >
>> ???? >
>> ???? > --
>> ???? > Philip B. Stark | Associate Dean, Mathematical and Physical
>> ??? Sciences |
>> ???? > Professor, ?Department of Statistics |
>> ???? > University of California
>> ???? > Berkeley, CA 94720-3860 | 510-394-5077 |
>> ??? statistics.berkeley.edu/~stark
>> ??? <http://statistics.berkeley.edu/%7Estark>
>> ???? > <http://statistics.berkeley.edu/%7Estark> |
>> ???? > @philipbstark
>> ???? >
>>
>>
>>
>> --?
>> Philip B. Stark | Associate Dean, Mathematical and Physical Sciences |
>> Professor, ?Department of Statistics |
>> University of California
>> Berkeley, CA 94720-3860 | 510-394-5077 |
>> statistics.berkeley.edu/~stark
>> <http://statistics.berkeley.edu/%7Estark> |
>> @philipbstark
>>
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From cboettig @ending from gm@il@com  Thu Sep 20 01:43:38 2018
From: cboettig @ending from gm@il@com (Carl Boettiger)
Date: Wed, 19 Sep 2018 16:43:38 -0700
Subject: [Rd] Bias in R's random integers?
In-Reply-To: <4143568b-c67f-2309-f9ec-676f9e791747@gmail.com>
References: <CAN_1p9wVX=AR8aYtC2LJo0bgN9cRW3XcbYJqnVHdCzA-QA1ceg@mail.gmail.com>
 <ffb74021-8d95-a820-8013-f0e51dc1a9b3@gmail.com>
 <CALEXWq3booRRTSYvYqcjV7qPZGOC1oh5o45b5o=J2ypQi5s-ag@mail.gmail.com>
 <ece52410-ddb2-2198-9291-201475d3f46e@gmail.com>
 <CAA7KnHHEejYVn8NtKrY1KuKcJUq8n4LoJSDx9BZv-LJyC=r9yQ@mail.gmail.com>
 <73c4667d-38ad-f5ef-468c-4c368e0fdc12@gmail.com>
 <CAA7KnHFcY5YJ2kTAfPJJ0PHwcWx7CumE49A4NgJWC2NK5CV=-A@mail.gmail.com>
 <c3590da8-2e92-4e46-3a81-0d0dbfc81845@gmail.com>
 <CAA7KnHE9OSxML3Cp8Y-gd7CbBj-86UP9qAy6yzgvfcb+=Vg86Q@mail.gmail.com>
 <e82cfff0-3a8e-1d7b-de5c-02b69974ae16@gmail.com>
 <4143568b-c67f-2309-f9ec-676f9e791747@gmail.com>
Message-ID: <CAN_1p9znLss3QKuF3WB5ucM_F2=paNx28eCdJecB8fj0p_w-6Q@mail.gmail.com>

For a well-tested C algorithm, based on my reading of Lemire, the unbiased
"algorithm 3" in https://arxiv.org/abs/1805.10941 is part already of the C
standard library in OpenBSD and macOS (as arc4random_uniform), and in the
GNU standard library.  Lemire also provides C++ code in the appendix of his
piece for both this and the faster "nearly divisionless" algorithm.

It would be excellent if any R core members were interested in considering
bindings to these algorithms as a patch, or might express expectations for
how that patch would have to operate (e.g. re Duncan's comment about
non-integer arguments to sample size).  Otherwise, an R package binding
seems like a good starting point, but I'm not the right volunteer.


On Wed, Sep 19, 2018 at 4:22 PM Ben Bolker <bbolker at gmail.com> wrote:

>
>   A quick point of order here: arguing with Duncan in this forum is
> helpful to expose ideas, but probably neither side will convince the
> other; eventually, if you want this adopted in core R, you'll need to
> convince an R-core member to pursue this fix.
>
>   In the meantime, a good, well-tested implementation in a
> user-contributed package (presumably written in C for speed) would be
> enormously useful.  Volunteers ... ?
>
>
>
> On 2018-09-19 04:19 PM, Duncan Murdoch wrote:
> > On 19/09/2018 3:52 PM, Philip B. Stark wrote:
> >> Hi Duncan--
> >>
> >> Nice simulation!
> >>
> >> The absolute difference in probabilities is small, but the maximum
> >> relative difference grows from something negligible to almost 2 as m
> >> approaches 2**31.
> >>
> >> Because the L_1 distance between the uniform distribution on {1, ...,
> >> m} and what you actually get is large, there have to be test functions
> >> whose expectations are quite different under the two distributions.
> >
> > That is a mathematically true statement, but I suspect it is not very
> > relevant.  Pseudo-random number generators always have test functions
> > whose sample averages are quite different from the expectation under the
> > true distribution.  Remember Von Neumann's "state of sin" quote.  The
> > bug in sample() just means it is easier to find such a function than it
> > would otherwise be.
> >
> > The practical question is whether such a function is likely to arise in
> > practice or not.
> >
> >> Whether those correspond to commonly used statistics or not, I have no
> >> idea.
> >
> > I am pretty confident that this bug rarely matters.
> >
> >> Regarding backwards compatibility: as a user, I'd rather the default
> >> sample() do the best possible thing, and take an extra step to use
> >> something like sample(..., legacy=TRUE) if I want to reproduce old
> >> results.
> >
> > I suspect there's a good chance the bug I discovered today (non-integer
> > x values not being truncated) will be declared to be a feature, and the
> > documentation will be changed.  Then the rejection sampling approach
> > would need to be quite a bit more complicated.
> >
> > I think a documentation warning about the accuracy of sampling
> > probabilities would also be a sufficient fix here, and would be quite a
> > bit less trouble than changing the default sample().  But as I said in
> > my original post, a contribution of a function without this bug would be
> > a nice addition.
> >
> > Duncan Murdoch
> >
> >>
> >> Regards,
> >> Philip
> >>
> >> On Wed, Sep 19, 2018 at 9:50 AM Duncan Murdoch
> >> <murdoch.duncan at gmail.com <mailto:murdoch.duncan at gmail.com>> wrote:
> >>
> >>     On 19/09/2018 12:23 PM, Philip B. Stark wrote:
> >>      > No, the 2nd call only happens when m > 2**31. Here's the code:
> >>
> >>     Yes, you're right. Sorry!
> >>
> >>     So the ratio really does come close to 2.  However, the difference
> in
> >>     probabilities between outcomes is still at most 2^-32 when m is less
> >>     than that cutoff.  That's not feasible to detect; the only
> detectable
> >>     difference would happen if some event was constructed to hold an
> >>     abundance of outcomes with especially low (or especially high)
> >>     probability.
> >>
> >>     As I said in my original post, it's probably not hard to construct
> >> such
> >>     a thing, but as I've said more recently, it probably wouldn't
> >> happen by
> >>     chance.  Here's one attempt to do it:
> >>
> >>     Call the values from unif_rand() "the unif_rand() outcomes".  Call
> >> the
> >>     values from sample() the sample outcomes.
> >>
> >>     It would be easiest to see the error if half of the sample()
> outcomes
> >>     used two unif_rand() outcomes, and half used just one.  That would
> >> mean
> >>     m should be (2/3) * 2^32, but that's too big and would trigger the
> >>     other
> >>     version.
> >>
> >>     So how about half use 2 unif_rands(), and half use 3?  That means m
> =
> >>     (2/5) * 2^32 = 1717986918.  A good guess is that sample() outcomes
> >>     would
> >>     alternate between the two possibilities, so our event could be even
> >>     versus odd outcomes.
> >>
> >>     Let's try it:
> >>
> >>       > m <- (2/5)*2^32
> >>       > m > 2^31
> >>     [1] FALSE
> >>       > x <- sample(m, 1000000, replace = TRUE)
> >>       > table(x %% 2)
> >>
> >>            0      1
> >>     399850 600150
> >>
> >>     Since m is an even number, the true proportions of evens and odds
> >>     should
> >>     be exactly 0.5.  That's some pretty strong evidence of the bug in
> the
> >>     generator.  (Note that the ratio of the observed probabilities is
> >> about
> >>     1.5, so I may not be the first person to have done this.)
> >>
> >>     I'm still not convinced that there has ever been a simulation run
> >> with
> >>     detectable bias compared to Monte Carlo error unless it (like this
> >> one)
> >>     was designed specifically to show the problem.
> >>
> >>     Duncan Murdoch
> >>
> >>      >
> >>      > (RNG.c, lines 793ff)
> >>      >
> >>      > double R_unif_index(double dn)
> >>      > {
> >>      >      double cut = INT_MAX;
> >>      >
> >>      >      switch(RNG_kind) {
> >>      >      case KNUTH_TAOCP:
> >>      >      case USER_UNIF:
> >>      >      case KNUTH_TAOCP2:
> >>      > cut = 33554431.0; /* 2^25 - 1 */
> >>      > break;
> >>      >      default:
> >>      > break;
> >>      >     }
> >>      >
> >>      >      double u = dn > cut ? ru() : unif_rand();
> >>      >      return floor(dn * u);
> >>      > }
> >>      >
> >>      > On Wed, Sep 19, 2018 at 9:20 AM Duncan Murdoch
> >>     <murdoch.duncan at gmail.com <mailto:murdoch.duncan at gmail.com>
> >>      > <mailto:murdoch.duncan at gmail.com
> >>     <mailto:murdoch.duncan at gmail.com>>> wrote:
> >>      >
> >>      >     On 19/09/2018 12:09 PM, Philip B. Stark wrote:
> >>      >      > The 53 bits only encode at most 2^{32} possible values,
> >>     because the
> >>      >      > source of the float is the output of a 32-bit PRNG (the
> >>     obsolete
> >>      >     version
> >>      >      > of MT). 53 bits isn't the relevant number here.
> >>      >
> >>      >     No, two calls to unif_rand() are used.  There are two 32 bit
> >>     values,
> >>      >     but
> >>      >     some of the bits are thrown away.
> >>      >
> >>      >     Duncan Murdoch
> >>      >
> >>      >      >
> >>      >      > The selection ratios can get close to 2. Computer
> >> scientists
> >>      >     don't do it
> >>      >      > the way R does, for a reason.
> >>      >      >
> >>      >      > Regards,
> >>      >      > Philip
> >>      >      >
> >>      >      > On Wed, Sep 19, 2018 at 9:05 AM Duncan Murdoch
> >>      >     <murdoch.duncan at gmail.com <mailto:murdoch.duncan at gmail.com>
> >>     <mailto:murdoch.duncan at gmail.com <mailto:murdoch.duncan at gmail.com>>
> >>      >      > <mailto:murdoch.duncan at gmail.com
> >>     <mailto:murdoch.duncan at gmail.com>
> >>      >     <mailto:murdoch.duncan at gmail.com
> >>     <mailto:murdoch.duncan at gmail.com>>>> wrote:
> >>      >      >
> >>      >      >     On 19/09/2018 9:09 AM, I?aki Ucar wrote:
> >>      >      >      > El mi?., 19 sept. 2018 a las 14:43, Duncan Murdoch
> >>      >      >      > (<murdoch.duncan at gmail.com
> >>     <mailto:murdoch.duncan at gmail.com>
> >>      >     <mailto:murdoch.duncan at gmail.com
> >>     <mailto:murdoch.duncan at gmail.com>> <mailto:murdoch.duncan at gmail.com
> >>     <mailto:murdoch.duncan at gmail.com>
> >>      >     <mailto:murdoch.duncan at gmail.com
> >>     <mailto:murdoch.duncan at gmail.com>>>>)
> >>      >      >     escribi?:
> >>      >      >      >>
> >>      >      >      >> On 18/09/2018 5:46 PM, Carl Boettiger wrote:
> >>      >      >      >>> Dear list,
> >>      >      >      >>>
> >>      >      >      >>> It looks to me that R samples random integers
> >>     using an
> >>      >      >     intuitive but biased
> >>      >      >      >>> algorithm by going from a random number on
> >> [0,1) from
> >>      >     the PRNG
> >>      >      >     to a random
> >>      >      >      >>> integer, e.g.
> >>      >      >      >>>
> >>      >      >
> >>
> https://github.com/wch/r-source/blob/tags/R-3-5-1/src/main/RNG.c#L808
> >>      >      >      >>>
> >>      >      >      >>> Many other languages use various rejection
> >> sampling
> >>      >     approaches
> >>      >      >     which
> >>      >      >      >>> provide an unbiased method for sampling, such as
> >>     in Go,
> >>      >     python,
> >>      >      >     and others
> >>      >      >      >>> described here:
> >> https://arxiv.org/abs/1805.10941 (I
> >>      >     believe the
> >>      >      >     biased
> >>      >      >      >>> algorithm currently used in R is also described
> >>     there).  I'm
> >>      >      >     not an expert
> >>      >      >      >>> in this area, but does it make sense for the R to
> >>     adopt
> >>      >     one of
> >>      >      >     the unbiased
> >>      >      >      >>> random sample algorithms outlined there and used
> >>     in other
> >>      >      >     languages?  Would
> >>      >      >      >>> a patch providing such an algorithm be welcome?
> >> What
> >>      >     concerns
> >>      >      >     would need to
> >>      >      >      >>> be addressed first?
> >>      >      >      >>>
> >>      >      >      >>> I believe this issue was also raised by Killie &
> >>     Philip in
> >>      >      >      >>>
> >>      > http://r.789695.n4.nabble.com/Bug-in-sample-td4729483.html, and
> >>      >      >     more
> >>      >      >      >>> recently in
> >>      >      >      >>>
> >>      >      >
> >>      >
> >>     https://www.stat.berkeley.edu/~stark/Preprints/r-random-issues.pdf
> >>
> >> <https://www.stat.berkeley.edu/%7Estark/Preprints/r-random-issues.pdf>
> >>      >
> >>  <https://www.stat.berkeley.edu/%7Estark/Preprints/r-random-issues.pdf>
> >>      >      >
> >>      >
> >>  <https://www.stat.berkeley.edu/%7Estark/Preprints/r-random-issues.pdf
> >,
> >>      >      >      >>> pointing to the python implementation for
> >> comparison:
> >>      >      >      >>>
> >>      >      >
> >>      >
> >>
> >>
> https://github.com/statlab/cryptorandom/blob/master/cryptorandom/cryptorandom.py#L265
> >>
> >>      >      >      >>
> >>      >      >      >> I think the analyses are correct, but I doubt if a
> >>     change
> >>      >     to the
> >>      >      >     default
> >>      >      >      >> is likely to be accepted as it would make it more
> >>      >     difficult to
> >>      >      >     reproduce
> >>      >      >      >> older results.
> >>      >      >      >>
> >>      >      >      >> On the other hand, a contribution of a new
> >>     function like
> >>      >      >     sample() but
> >>      >      >      >> not suffering from the bias would be good.  The
> >>     normal way to
> >>      >      >     make such
> >>      >      >      >> a contribution is in a user contributed package.
> >>      >      >      >>
> >>      >      >      >> By the way, R code illustrating the bias is
> >>     probably not very
> >>      >      >     hard to
> >>      >      >      >> put together.  I believe the bias manifests
> >> itself in
> >>      >     sample()
> >>      >      >     producing
> >>      >      >      >> values with two different probabilities (instead
> >>     of all equal
> >>      >      >      >> probabilities).  Those may differ by as much as
> >>     one part in
> >>      >      >     2^32.  It's
> >>      >      >      >
> >>      >      >      > According to Kellie and Philip, in the attachment
> >>     of the
> >>      >     thread
> >>      >      >      > referenced by Carl, "The maximum ratio of selection
> >>      >     probabilities can
> >>      >      >      > get as large as 1.5 if n is just below 2^31".
> >>      >      >
> >>      >      >     Sorry, I didn't write very well.  I meant to say
> >> that the
> >>      >     difference in
> >>      >      >     probabilities would be 2^-32, not that the ratio of
> >>      >     probabilities would
> >>      >      >     be 1 + 2^-32.
> >>      >      >
> >>      >      >     By the way, I don't see the statement giving the
> >> ratio as
> >>      >     1.5, but
> >>      >      >     maybe
> >>      >      >     I was looking in the wrong place.  In Theorem 1 of the
> >>     paper
> >>      >     I was
> >>      >      >     looking in the ratio was "1 + m 2^{-w + 1}".  In that
> >>     formula
> >>      >     m is your
> >>      >      >     n.  If it is near 2^31, R uses w = 57 random bits, so
> >>     the ratio
> >>      >      >     would be
> >>      >      >     very, very small (one part in 2^25).
> >>      >      >
> >>      >      >     The worst case for R would happen when m  is just
> below
> >>      >     2^25, where w
> >>      >      >     is at least 31 for the default generators.  In that
> >>     case the
> >>      >     ratio
> >>      >      >     could
> >>      >      >     be about 1.03.
> >>      >      >
> >>      >      >     Duncan Murdoch
> >>      >      >
> >>      >      >
> >>      >      >
> >>      >      > --
> >>      >      > Philip B. Stark | Associate Dean, Mathematical and
> Physical
> >>      >     Sciences |
> >>      >      > Professor,  Department of Statistics |
> >>      >      > University of California
> >>      >      > Berkeley, CA 94720-3860 | 510-394-5077 <(510)%20394-5077>
> |
> >>      > statistics.berkeley.edu/~stark
> >>     <http://statistics.berkeley.edu/%7Estark>
> >>      >     <http://statistics.berkeley.edu/%7Estark>
> >>      >      > <http://statistics.berkeley.edu/%7Estark> |
> >>      >      > @philipbstark
> >>      >      >
> >>      >
> >>      >
> >>      >
> >>      > --
> >>      > Philip B. Stark | Associate Dean, Mathematical and Physical
> >>     Sciences |
> >>      > Professor,  Department of Statistics |
> >>      > University of California
> >>      > Berkeley, CA 94720-3860 | 510-394-5077 <(510)%20394-5077> |
> >>     statistics.berkeley.edu/~stark
> >>     <http://statistics.berkeley.edu/%7Estark>
> >>      > <http://statistics.berkeley.edu/%7Estark> |
> >>      > @philipbstark
> >>      >
> >>
> >>
> >>
> >> --
> >> Philip B. Stark | Associate Dean, Mathematical and Physical Sciences |
> >> Professor,  Department of Statistics |
> >> University of California
> >> Berkeley, CA 94720-3860 | 510-394-5077 <(510)%20394-5077> |
> >> statistics.berkeley.edu/~stark
> >> <http://statistics.berkeley.edu/%7Estark> |
> >> @philipbstark
> >>
> >
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>
-- 

http://carlboettiger.info

	[[alternative HTML version deleted]]


From d@tr7320 @ending from uni@@ydney@edu@@u  Thu Sep 20 03:00:11 2018
From: d@tr7320 @ending from uni@@ydney@edu@@u (Dario Strbenac)
Date: Thu, 20 Sep 2018 01:00:11 +0000
Subject: [Rd] A different error in sample()
In-Reply-To: <CAO1zAVbHXAz9o7uf98Q5pUm7R8pSHFigAAtQn90FrGdPfqUn1A@mail.gmail.com>
References: <5b6950ce-ac7a-4f6a-da61-8e70d25bb08c@gmail.com>,
 <CAO1zAVbHXAz9o7uf98Q5pUm7R8pSHFigAAtQn90FrGdPfqUn1A@mail.gmail.com>
Message-ID: <SYCPR01MB41112555F67E1CAD438FCB6BCD1C0@SYCPR01MB4111.ausprd01.prod.outlook.com>

Good day,

The use of "rounding" also doesn't make sense. If The number is halfway between two integers, it is rounded to the nearest even integer.

> round(2.5)
[1] 2

--------------------------------------
Dario Strbenac
University of Sydney
Camperdown NSW 2050
Australia

From wolfg@ng@huber @ending from embl@de  Thu Sep 20 08:19:53 2018
From: wolfg@ng@huber @ending from embl@de (Wolfgang Huber)
Date: Thu, 20 Sep 2018 08:19:53 +0200
Subject: [Rd] A different error in sample()
In-Reply-To: <SYCPR01MB41112555F67E1CAD438FCB6BCD1C0@SYCPR01MB4111.ausprd01.prod.outlook.com>
References: <5b6950ce-ac7a-4f6a-da61-8e70d25bb08c@gmail.com>
 <CAO1zAVbHXAz9o7uf98Q5pUm7R8pSHFigAAtQn90FrGdPfqUn1A@mail.gmail.com>
 <SYCPR01MB41112555F67E1CAD438FCB6BCD1C0@SYCPR01MB4111.ausprd01.prod.outlook.com>
Message-ID: <90332569-2e20-e645-26de-9c2cd43eaea9@embl.de>

Besides wording of the documentation re truncating vs rounding, there 
is something peculiar going on with the fractional part of n:

 > table(sample.int(2.5, 1e6, replace = TRUE))

      1      2      3
399051 401035 199914

 > table(sample.int(3, 1e6, replace = TRUE))

      1      2      3
332956 332561 334483

 > table(sample.int(2.01, 1e6, replace = TRUE))

      1      2      3
497173 497866   4961

 > sessionInfo()
R Under development (unstable) (2018-09-17 r75319)
Platform: x86_64-apple-darwin17.7.0 (64-bit)
Running under: macOS High Sierra 10.13.6

Matrix products: default
BLAS: /Users/whuber/R/lib/libRblas.dylib
LAPACK: /Users/whuber/R/lib/libRlapack.dylib

locale:
[1] en_US.UTF-8/UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] fortunes_1.5-4

loaded via a namespace (and not attached):
[1] compiler_3.6.0 tools_3.6.0


20.9.18 03:00, Dario Strbenac scripsit:
> Good day,
> 
> The use of "rounding" also doesn't make sense. If The number is halfway between two integers, it is rounded to the nearest even integer.
> 
>> round(2.5)
> [1] 2
> 
> --------------------------------------
> Dario Strbenac
> University of Sydney
> Camperdown NSW 2050
> Australia

-- 
With thanks in advance-

Wolfgang

-------
Wolfgang Huber
Principal Investigator, EMBL Senior Scientist
European Molecular Biology Laboratory (EMBL)
Heidelberg, Germany

wolfgang.huber at embl.de
http://www.huber.embl.de

My book with Susan Holmes: http://www.huber.embl.de/msmb


From wolfg@ng@huber @ending from embl@de  Thu Sep 20 08:47:47 2018
From: wolfg@ng@huber @ending from embl@de (Wolfgang Huber)
Date: Thu, 20 Sep 2018 08:47:47 +0200
Subject: [Rd] A different error in sample()
In-Reply-To: <90332569-2e20-e645-26de-9c2cd43eaea9@embl.de>
References: <5b6950ce-ac7a-4f6a-da61-8e70d25bb08c@gmail.com>
 <CAO1zAVbHXAz9o7uf98Q5pUm7R8pSHFigAAtQn90FrGdPfqUn1A@mail.gmail.com>
 <SYCPR01MB41112555F67E1CAD438FCB6BCD1C0@SYCPR01MB4111.ausprd01.prod.outlook.com>
 <90332569-2e20-e645-26de-9c2cd43eaea9@embl.de>
Message-ID: <a90aa1d8-6f6d-4be1-140a-5f776df36fd0@embl.de>

FWIW, I suspect this is related to the function R_unif_index that was 
introduced in src/main/RNG.c around revision 72356, or the way this 
function is used in do_sample in src/main/random.c.

20.9.18 08:19, Wolfgang Huber scripsit:
> Besides wording of the documentation re truncating vs rounding, there is 
> something peculiar going on with the fractional part of n:
> 
>  > table(sample.int(2.5, 1e6, replace = TRUE))
> 
>  ???? 1????? 2????? 3
> 399051 401035 199914
> 
>  > table(sample.int(3, 1e6, replace = TRUE))
> 
>  ???? 1????? 2????? 3
> 332956 332561 334483
> 
>  > table(sample.int(2.01, 1e6, replace = TRUE))
> 
>  ???? 1????? 2????? 3
> 497173 497866?? 4961
> 
>  > sessionInfo()
> R Under development (unstable) (2018-09-17 r75319)
> Platform: x86_64-apple-darwin17.7.0 (64-bit)
> Running under: macOS High Sierra 10.13.6
> 
> Matrix products: default
> BLAS: /Users/whuber/R/lib/libRblas.dylib
> LAPACK: /Users/whuber/R/lib/libRlapack.dylib
> 
> locale:
> [1] en_US.UTF-8/UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
> 
> attached base packages:
> [1] stats???? graphics? grDevices utils???? datasets? methods?? base
> 
> other attached packages:
> [1] fortunes_1.5-4
> 
> loaded via a namespace (and not attached):
> [1] compiler_3.6.0 tools_3.6.0
> 
> 
> 20.9.18 03:00, Dario Strbenac scripsit:
>> Good day,
>>
>> The use of "rounding" also doesn't make sense. If The number is 
>> halfway between two integers, it is rounded to the nearest even integer.
>>
>>> round(2.5)
>> [1] 2
>>
>> --------------------------------------
>> Dario Strbenac
>> University of Sydney
>> Camperdown NSW 2050
>> Australia
> 

-- 
With thanks in advance-

Wolfgang

-------
Wolfgang Huber
Principal Investigator, EMBL Senior Scientist
European Molecular Biology Laboratory (EMBL)
Heidelberg, Germany

wolfgang.huber at embl.de
http://www.huber.embl.de

My book with Susan Holmes: http://www.huber.embl.de/msmb


From m@echler @ending from @t@t@m@th@ethz@ch  Thu Sep 20 09:20:46 2018
From: m@echler @ending from @t@t@m@th@ethz@ch (Martin Maechler)
Date: Thu, 20 Sep 2018 09:20:46 +0200
Subject: [Rd] A different error in sample()
In-Reply-To: <a90aa1d8-6f6d-4be1-140a-5f776df36fd0@embl.de>
References: <5b6950ce-ac7a-4f6a-da61-8e70d25bb08c@gmail.com>
 <CAO1zAVbHXAz9o7uf98Q5pUm7R8pSHFigAAtQn90FrGdPfqUn1A@mail.gmail.com>
 <SYCPR01MB41112555F67E1CAD438FCB6BCD1C0@SYCPR01MB4111.ausprd01.prod.outlook.com>
 <90332569-2e20-e645-26de-9c2cd43eaea9@embl.de>
 <a90aa1d8-6f6d-4be1-140a-5f776df36fd0@embl.de>
Message-ID: <23459.19022.858260.194660@stat.math.ethz.ch>

>>>>> Wolfgang Huber 
>>>>>     on Thu, 20 Sep 2018 08:47:47 +0200 writes:

    > FWIW, I suspect this is related to the function
    > R_unif_index that was introduced in src/main/RNG.c around
    > revision 72356, or the way this function is used in
    > do_sample in src/main/random.c.

Yes, it is just the use of 'dn' instead of 'n'
- a one letter thinko I'd say.

But *no*, it's much older than revision 72356; e.g., it's already in

    R version 3.0.0 (2013-04-03) -- "Masked Marvel"

but not yet in

    R version 2.15.3 (2013-03-01) -- "Security Blanket"

----

Here, I clearly think we see a regression bug, and hopefully not
one that should trigger often in practice...
and  -- without any statistics about the consequences out in
package space --
I do think we should fix this in code and let the documentation
become "great again" ;-)

Martin





    > 20.9.18 08:19, Wolfgang Huber scripsit:
    >> Besides wording of the documentation re truncating vs
    >> rounding, there is something peculiar going on with the
    >> fractional part of n:
    >> 
    >> > table(sample.int(2.5, 1e6, replace = TRUE))
    >> 
    >> ???? 1????? 2????? 3 399051 401035 199914
    >> 
    >> > table(sample.int(3, 1e6, replace = TRUE))
    >> 
    >> ???? 1????? 2????? 3 332956 332561 334483
    >> 
    >> > table(sample.int(2.01, 1e6, replace = TRUE))
    >> 
    >> ???? 1????? 2????? 3 497173 497866?? 4961
    >>


From @eth@ru@@ell @ending from gm@il@com  Wed Sep 19 23:19:48 2018
From: @eth@ru@@ell @ending from gm@il@com (Seth Russell)
Date: Wed, 19 Sep 2018 15:19:48 -0600
Subject: [Rd] segfault issue with parallel::mclapply and download.file() on
 Mac OS X
Message-ID: <CAB75gRUv1LitTgWSbMFunqp7P9wfWG_BJ_xYBPN4K5KJkDu+_g@mail.gmail.com>

I have an lapply function call that I want to parallelize. Below is a very
simplified version of the code:

url_base <- "https://cloud.r-project.org/src/contrib/"
files <- c("A3_1.0.0.tar.gz", "ABC.RAP_0.9.0.tar.gz")
res <- parallel::mclapply(files, function(s) download.file(paste0(url_base,
s), s))

Instead of download a couple of files in parallel, I get a segfault per
process with a 'memory not mapped' message. I've been working with Henrik
Bengtsson on resolving this issue and he recommended I send a message to
the R-Devel mailing list.

Here's the output:

trying URL 'https://cloud.r-project.org/src/contrib/A3_1.0.0.tar.gz'
trying URL 'https://cloud.r-project.org/src/contrib/ABC.RAP_0.9.0.tar.gz'

 *** caught segfault ***
address 0x11575ba3a, cause 'memory not mapped'

 *** caught segfault ***
address 0x11575ba3a, cause 'memory not mapped'

Traceback:
 1: download.file(paste0(url_base, s), s)
 2: FUN(X[[i]], ...)
 3: lapply(X = S, FUN = FUN, ...)
 4: doTryCatch(return(expr), name, parentenv, handler)
 5: tryCatchOne(expr, names, parentenv, handlers[[1L]])
 6: tryCatchList(expr, classes, parentenv, handlers)
 7: tryCatch(expr, error = function(e) {    call <- conditionCall(e)    if
(!is.null(call)) {        if (identical(call[[1L]], quote(doTryCatch)))
        call <- sys.call(-4L)        dcall <- deparse(call)[1L]
 prefix <- paste("Error in", dcall, ": ")
        LONG <- 75LTraceback:
        sm <- strsplit(conditionMessage(e), "\n")[[1L]] 1:         w <- 14L
+ nchar(dcall, type = "w") + nchar(sm[1L], type = "w")        if (is.na(w))
download.file(paste0(url_base, s), s)            w <- 14L + nchar(dcall,
type = "b") + nchar(sm[1L],
                type = "b")        if (w > LONG)  2: FUN(X[[i]], ...)
 3: lapply(X = S, FUN = FUN, ...)
 4: doTryCatch(return(expr), name, parentenv, handler)
 5: tryCatchOne(expr, names, parentenv, handlers[[1L]])
 6:             prefix <- paste0(prefix, "\n  ")tryCatchList(expr, classes,
parentenv, handlers)
    }    else prefix <- "Error : " 7:     msg <- paste0(prefix,
conditionMessage(e), "\n")tryCatch(expr, error = function(e) {
 .Internal(seterrmessage(msg[1L]))    call <- conditionCall(e)    if
(!silent && isTRUE(getOption("show.error.messages"))) {    if
(!is.null(call)) {        cat(msg, file = outFile)        if
(identical(call[[1L]], quote(doTryCatch)))
.Internal(printDeferredWarnings())            call <- sys.call(-4L)    }
     dcall <- deparse(call)[1L]    invisible(structure(msg, class =
"try-error", condition = e))        prefix <- paste("Error in", dcall, ":
")})        LONG <- 75L        sm <- strsplit(conditionMessage(e),
"\n")[[1L]]
        w <- 14L + nchar(dcall, type = "w") + nchar(sm[1L], type = "w")
   if (is.na(w))  8:             w <- 14L + nchar(dcall, type = "b") +
nchar(sm[1L], try(lapply(X = S, FUN = FUN, ...), silent = TRUE)
   type = "b")
        if (w > LONG)             prefix <- paste0(prefix, "\n  ") 9:
}sendMaster(try(lapply(X = S, FUN = FUN, ...), silent = TRUE))    else
prefix <- "Error : "
    msg <- paste0(prefix, conditionMessage(e), "\n")
 .Internal(seterrmessage(msg[1L]))10:     if (!silent &&
isTRUE(getOption("show.error.messages"))) {FUN(X[[i]], ...)        cat(msg,
file = outFile)
        .Internal(printDeferredWarnings())    }11:
invisible(structure(msg, class = "try-error", condition =
e))lapply(seq_len(cores), inner.do)})

12:  8: parallel::mclapply(files, function(s)
download.file(paste0(url_base, try(lapply(X = S, FUN = FUN, ...), silent =
TRUE)    s), s))

 9:
sendMaster(try(lapply(X = S, FUN = FUN, ...), silent = TRUE))Possible
actions:

1: abort (with core dump, if enabled)
2: normal R exit
10: 3: exit R without saving workspace
FUN(X[[i]], ...)4: exit R saving workspace

11: lapply(seq_len(cores), inner.do)
12: parallel::mclapply(files, function(s) download.file(paste0(url_base,
  s), s))

Here's my sessionInfo()

> sessionInfo()
R version 3.5.1 (2018-07-02)
Platform: x86_64-apple-darwin16.7.0 (64-bit)
Running under: macOS Sierra 10.12.6

Matrix products: default
BLAS/LAPACK: /usr/local/Cellar/openblas/0.3.3/lib/libopenblasp-r0.3.3.dylib

locale:
[1] en_US/en_US/en_US/C/en_US/en_US

attached base packages:
[1] parallel  stats     graphics  grDevices utils     datasets  methods
[8] base

loaded via a namespace (and not attached):
[1] compiler_3.5.1

My version of R I'm running was installed via homebrew with "brew install r
--with-java --with-openblas"

Also, the provided example code works as expected on Linux. Also, if I
provide a non-default download method to the download.file() call such as:

res <- parallel::mclapply(files, function(s) download.file(paste0(url_base,
s), s, method="wget"))
res <- parallel::mclapply(files, function(s) download.file(paste0(url_base,
s), s, method="curl"))

It works correctly - no segfault. If I use method="libcurl" it does
segfault.

I'm not sure what steps to take to further narrow down the source of the
error.

Is this a known bug? if not, is this a new bug or an unexpected feature?

Thanks,
Seth

	[[alternative HTML version deleted]]


From lukemol@on @ending from y@hoo@com  Thu Sep 20 01:53:21 2018
From: lukemol@on @ending from y@hoo@com (lmo)
Date: Wed, 19 Sep 2018 23:53:21 +0000 (UTC)
Subject: [Rd] A different error in sample()
References: <1899129802.76058.1537401201170.ref@mail.yahoo.com>
Message-ID: <1899129802.76058.1537401201170@mail.yahoo.com>

Although it seems to be pretty weird to enter a numeric vector of length one that is not an integer as the first argument to sample(), the results do not seem to match what is documented in the manual. In addition, the results below do not support the use of round rather than truncate in the documentation. Consider the code below.
The first sentence in the details section says: "If x has length 1, is numeric (in the sense of is.numeric) and x >= 1, sampling via sample takes place from 1:x."
In the console:> 1:2.001
[1] 1 2
> 1:2.9
[1] 1 2

truncation:
> trunc(2.9)
[1] 2

So, this seems to support the quote from in previous emails: "Non-integer positive numerical values of n or x will be truncated to the next smallest integer, which has to be no larger than .Machine$integer.max."
However, again in the console:> set.seed(123)
 > table(sample(2.001, 10000, replace=TRUE))

?? 1??? 2??? 3 
5052 4941??? 7

So, neither rounding nor truncation is occurring. Next, define a sequence.
> x <- seq(2.001, 2.51, length.out=20)
Now, grab all of the threes from sample()-ing this sequence.

 > set.seed(123)
> threes <- sapply(x, function(y) table(sample(y, 10000, replace=TRUE))[3])

Check for NAs (I cheated here and found a nice seed).> any(is.na(threes))
[1] FALSE
Now, the (to me) disturbing result.

> is.unsorted(threes)
[1] FALSE

or equivalently

> all(diff(threes) > 0)
[1] TRUE

So the number of threes grows monotonically as 2.001 moves to 2.5. As I hinted above, the monotonic growth is not assured. My guess is that the growth is stochastic and relates to some "probability weighting" based on how close the element of x is to 3. Perhaps this has been brought up before, but it seems relevant to the current discussion.
A potential aid to this issue would be something like
if(length(x) == 1 && !all.equal(x, as.integer(x))) warning("It is a bad idea to use vectors of length 1 in the x argument that are not integers.")
Hope that helps,luke

	[[alternative HTML version deleted]]


From kim@@eonghyun @ending from eric@@on@com  Thu Sep 20 09:27:22 2018
From: kim@@eonghyun @ending from eric@@on@com (Kim Seonghyun)
Date: Thu, 20 Sep 2018 07:27:22 +0000
Subject: [Rd] A different error in sample()
In-Reply-To: <SYCPR01MB41112555F67E1CAD438FCB6BCD1C0@SYCPR01MB4111.ausprd01.prod.outlook.com>
References: <5b6950ce-ac7a-4f6a-da61-8e70d25bb08c@gmail.com>,
 <CAO1zAVbHXAz9o7uf98Q5pUm7R8pSHFigAAtQn90FrGdPfqUn1A@mail.gmail.com>
 <SYCPR01MB41112555F67E1CAD438FCB6BCD1C0@SYCPR01MB4111.ausprd01.prod.outlook.com>
Message-ID: <VI1PR07MB1616238B31968EBEA911B6F292130@VI1PR07MB1616.eurprd07.prod.outlook.com>

Hi,

I have not checked the source code, but I think it is because of banker's round.

https://en.wikipedia.org/wiki/Rounding#Round_half_to_even

Best regards,
Kim

-----Original Message-----
From: R-devel <r-devel-bounces at r-project.org> On Behalf Of Dario Strbenac
Sent: den 20 september 2018 03:00
To: r-devel <r-devel at r-project.org>
Subject: Re: [Rd] A different error in sample()

Good day,

The use of "rounding" also doesn't make sense. If The number is halfway between two integers, it is rounded to the nearest even integer.

> round(2.5)
[1] 2

--------------------------------------
Dario Strbenac
University of Sydney
Camperdown NSW 2050
Australia
______________________________________________
R-devel at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-devel


From emil@bode @ending from d@n@@kn@w@nl  Thu Sep 20 10:17:53 2018
From: emil@bode @ending from d@n@@kn@w@nl (Emil Bode)
Date: Thu, 20 Sep 2018 08:17:53 +0000
Subject: [Rd] A different error in sample()
In-Reply-To: <1899129802.76058.1537401201170@mail.yahoo.com>
References: <1899129802.76058.1537401201170.ref@mail.yahoo.com>
 <1899129802.76058.1537401201170@mail.yahoo.com>
Message-ID: <B0F9D2DB-7823-4C0D-BEB6-90F68B92F3F2@dans.knaw.nl>

But do we handle it as an error in what sample does, or how the documentation is?
I think what is done now would be best described as "ceilinged", i.e. what ceiling() does. But is there an English word to describe this?
Or just use "converted to the next smallest integer"?

But then again, what happens is that the answer is ceilinged, not the input.
I guess the rationale is that multiplying by any integer and then dividing should give the same results:
ceiling(sample(n * x, size=1e6, replace = TRUE) / x) gives the same results for any integer n and x, it's nice that this also holds for non-integer n.
The most important thing is why people would use sample with a non-integer x, I don?t see many use cases.
So I agree with Luke that a warning would be best, regardless of what the docs say.

Best regards, 
Emil Bode

    Although it seems to be pretty weird to enter a numeric vector of length one that is not an integer as the first argument to sample(), the results do not seem to match what is documented in the manual. In addition, the results below do not support the use of round rather than truncate in the documentation. Consider the code below.
    The first sentence in the details section says: "If x has length 1, is numeric (in the sense of is.numeric) and x >= 1, sampling via sample takes place from 1:x."
    In the console:> 1:2.001
    [1] 1 2
    > 1:2.9
    [1] 1 2
    
    truncation:
    > trunc(2.9)
    [1] 2
    
    So, this seems to support the quote from in previous emails: "Non-integer positive numerical values of n or x will be truncated to the next smallest integer, which has to be no larger than .Machine$integer.max."
    However, again in the console:> set.seed(123)
     > table(sample(2.001, 10000, replace=TRUE))
    
       1    2    3 
    5052 4941    7
    
    So, neither rounding nor truncation is occurring. Next, define a sequence.
    > x <- seq(2.001, 2.51, length.out=20)
    Now, grab all of the threes from sample()-ing this sequence.
    
     > set.seed(123)
    > threes <- sapply(x, function(y) table(sample(y, 10000, replace=TRUE))[3])
    
    Check for NAs (I cheated here and found a nice seed).> any(is.na(threes))
    [1] FALSE
    Now, the (to me) disturbing result.
    
    > is.unsorted(threes)
    [1] FALSE
    
    or equivalently
    
    > all(diff(threes) > 0)
    [1] TRUE
    
    So the number of threes grows monotonically as 2.001 moves to 2.5. As I hinted above, the monotonic growth is not assured. My guess is that the growth is stochastic and relates to some "probability weighting" based on how close the element of x is to 3. Perhaps this has been brought up before, but it seems relevant to the current discussion.
    A potential aid to this issue would be something like
    if(length(x) == 1 && !all.equal(x, as.integer(x))) warning("It is a bad idea to use vectors of length 1 in the x argument that are not integers.")
    Hope that helps,luke
    
    	[[alternative HTML version deleted]]
    
    ______________________________________________
    R-devel at r-project.org mailing list
    https://stat.ethz.ch/mailman/listinfo/r-devel
    


From jori@mey@ @ending from gm@il@com  Thu Sep 20 10:31:22 2018
From: jori@mey@ @ending from gm@il@com (Joris Meys)
Date: Thu, 20 Sep 2018 10:31:22 +0200
Subject: [Rd] A different error in sample()
In-Reply-To: <1899129802.76058.1537401201170@mail.yahoo.com>
References: <1899129802.76058.1537401201170.ref@mail.yahoo.com>
 <1899129802.76058.1537401201170@mail.yahoo.com>
Message-ID: <CAO1zAVZW8PFV5A2VfAz464bYJyZ5VEasxbv5xxp7-53gERQvGg@mail.gmail.com>

To be more clear: I do NOT state that the function "round" is used. I read
the documentation as "non integer positive numerical values will be
replaced by the next smallest integer", the important part being the NEXT
smallest integer, i.e. how ceiling() does it. So that's exactly what I
would expect. If "replaced by" causes less confusion than "rounded to" or
"truncated to", then use that.

I do agree that this wording would still indicate that this happens prior
to the sampling, whereas the output indicates that this is done after the
sampling. I can reproduce the sample() outcome using runif() as follows:

> table(ceiling(runif(10000,0,2.1)))
   1    2    3
4774 4756  470

> table(ceiling(runif(10000,0,3)))
   1    2    3
3273 3440 3287

I don't know if that's the intended behaviour, but there is some logic in
it. It's up to the R core team to decide if this is OK and rephrase the
help page so it becomes more clear what actually happens, or simply add
something like

if( (x%%1) != 0) x <- ceiling(x)

prior to the sampling algorithm.

Cheers
Joris

On Thu, Sep 20, 2018 at 9:44 AM lmo via R-devel <r-devel at r-project.org>
wrote:

> Although it seems to be pretty weird to enter a numeric vector of length
> one that is not an integer as the first argument to sample(), the results
> do not seem to match what is documented in the manual. In addition, the
> results below do not support the use of round rather than truncate in the
> documentation. Consider the code below.
> The first sentence in the details section says: "If x has length 1, is
> numeric (in the sense of is.numeric) and x >= 1, sampling via sample takes
> place from 1:x."
> In the console:> 1:2.001
> [1] 1 2
> > 1:2.9
> [1] 1 2
>
> truncation:
> > trunc(2.9)
> [1] 2
>
> So, this seems to support the quote from in previous emails: "Non-integer
> positive numerical values of n or x will be truncated to the next smallest
> integer, which has to be no larger than .Machine$integer.max."
> However, again in the console:> set.seed(123)
>  > table(sample(2.001, 10000, replace=TRUE))
>
>    1    2    3
> 5052 4941    7
>
> So, neither rounding nor truncation is occurring. Next, define a sequence.
> > x <- seq(2.001, 2.51, length.out=20)
> Now, grab all of the threes from sample()-ing this sequence.
>
>  > set.seed(123)
> > threes <- sapply(x, function(y) table(sample(y, 10000, replace=TRUE))[3])
>
> Check for NAs (I cheated here and found a nice seed).> any(is.na(threes))
> [1] FALSE
> Now, the (to me) disturbing result.
>
> > is.unsorted(threes)
> [1] FALSE
>
> or equivalently
>
> > all(diff(threes) > 0)
> [1] TRUE
>
> So the number of threes grows monotonically as 2.001 moves to 2.5. As I
> hinted above, the monotonic growth is not assured. My guess is that the
> growth is stochastic and relates to some "probability weighting" based on
> how close the element of x is to 3. Perhaps this has been brought up
> before, but it seems relevant to the current discussion.
> A potential aid to this issue would be something like
> if(length(x) == 1 && !all.equal(x, as.integer(x))) warning("It is a bad
> idea to use vectors of length 1 in the x argument that are not integers.")
> Hope that helps,luke
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


-- 
Joris Meys
Statistical consultant

Department of Data Analysis and Mathematical Modelling
Ghent University
Coupure Links 653, B-9000 Gent (Belgium)
<https://maps.google.com/?q=Coupure+links+653,%C2%A0B-9000+Gent,%C2%A0Belgium&entry=gmail&source=g>

-----------
Biowiskundedagen 2017-2018
http://www.biowiskundedagen.ugent.be/

-------------------------------
Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php

	[[alternative HTML version deleted]]


From m@echler @ending from @t@t@m@th@ethz@ch  Thu Sep 20 10:33:38 2018
From: m@echler @ending from @t@t@m@th@ethz@ch (Martin Maechler)
Date: Thu, 20 Sep 2018 10:33:38 +0200
Subject: [Rd] 
 segfault issue with parallel::mclapply and download.file() on
 Mac OS X
In-Reply-To: <CAB75gRUv1LitTgWSbMFunqp7P9wfWG_BJ_xYBPN4K5KJkDu+_g@mail.gmail.com>
References: <CAB75gRUv1LitTgWSbMFunqp7P9wfWG_BJ_xYBPN4K5KJkDu+_g@mail.gmail.com>
Message-ID: <23459.23394.977877.156700@stat.math.ethz.ch>

>>>>> Seth Russell 
>>>>>     on Wed, 19 Sep 2018 15:19:48 -0600 writes:

    > I have an lapply function call that I want to parallelize. Below is a very
    > simplified version of the code:

    > url_base <- "https://cloud.r-project.org/src/contrib/"
    > files <- c("A3_1.0.0.tar.gz", "ABC.RAP_0.9.0.tar.gz")
    > res <- parallel::mclapply(files, function(s) download.file(paste0(url_base,
    > s), s))

    > Instead of download a couple of files in parallel, I get a segfault per
    > process with a 'memory not mapped' message. I've been working with Henrik
    > Bengtsson on resolving this issue and he recommended I send a message to
    > the R-Devel mailing list.

Thank you for the simple reproducible (*) example.

If I run the above in either R-devel  or R 3.5.1, it works
flawlessly [on Linux Fedora 28]. .... ah, now I see you say so
much later... also that other methods than "libcurl" work.

To note here is that "libcurl" is also the default method on
Linux where things work.

I've also tried it on the Windows server I've easily access and
the following code -- also explicitly using  "libcurl" --

##--------------------------------------------------------------
url_base <- "https://cloud.r-project.org/src/contrib/"
files <- c("A3_1.0.0.tar.gz", "ABC.RAP_0.9.0.tar.gz")
res <- parallel::mclapply(files, function(s)
            download.file(paste0(url_base, s), s, method="libcurl"))
##--------------------------------------------------------------

works fine there too.

- So maybe this should have gone to the R-SIG-Mac mailing list
  instead of this one ??

- Can other MacOS R users try and see?

--
*) at least till one of the 2 packages gets updated ! ;-)

    > Here's the output:

    > trying URL 'https://cloud.r-project.org/src/contrib/A3_1.0.0.tar.gz'
    > trying URL 'https://cloud.r-project.org/src/contrib/ABC.RAP_0.9.0.tar.gz'

    > *** caught segfault ***
    > address 0x11575ba3a, cause 'memory not mapped'

    > *** caught segfault ***
    > address 0x11575ba3a, cause 'memory not mapped'

    > Traceback:
    > 1: download.file(paste0(url_base, s), s)
    > 2: FUN(X[[i]], ...)
    > 3: lapply(X = S, FUN = FUN, ...)
    > 4: doTryCatch(return(expr), name, parentenv, handler)
    > 5: tryCatchOne(expr, names, parentenv, handlers[[1L]])
    > 6: tryCatchList(expr, classes, parentenv, handlers)
    > 7: tryCatch(expr, error = function(e) {    call <- conditionCall(e)    if
    > (!is.null(call)) {        if (identical(call[[1L]], quote(doTryCatch)))
    > call <- sys.call(-4L)        dcall <- deparse(call)[1L]
    > prefix <- paste("Error in", dcall, ": ")
    > LONG <- 75LTraceback:
    > sm <- strsplit(conditionMessage(e), "\n")[[1L]] 1:         w <- 14L
    > + nchar(dcall, type = "w") + nchar(sm[1L], type = "w")        if (is.na(w))
    > download.file(paste0(url_base, s), s)            w <- 14L + nchar(dcall,
    > type = "b") + nchar(sm[1L],
    > type = "b")        if (w > LONG)  2: FUN(X[[i]], ...)
    > 3: lapply(X = S, FUN = FUN, ...)
    > 4: doTryCatch(return(expr), name, parentenv, handler)
    > 5: tryCatchOne(expr, names, parentenv, handlers[[1L]])
    > 6:             prefix <- paste0(prefix, "\n  ")tryCatchList(expr, classes,
    > parentenv, handlers)
    > }    else prefix <- "Error : " 7:     msg <- paste0(prefix,
    > conditionMessage(e), "\n")tryCatch(expr, error = function(e) {
    > .Internal(seterrmessage(msg[1L]))    call <- conditionCall(e)    if
    > (!silent && isTRUE(getOption("show.error.messages"))) {    if
    > (!is.null(call)) {        cat(msg, file = outFile)        if
    > (identical(call[[1L]], quote(doTryCatch)))
    > .Internal(printDeferredWarnings())            call <- sys.call(-4L)    }
    > dcall <- deparse(call)[1L]    invisible(structure(msg, class =
    > "try-error", condition = e))        prefix <- paste("Error in", dcall, ":
    > ")})        LONG <- 75L        sm <- strsplit(conditionMessage(e),
    > "\n")[[1L]]
    > w <- 14L + nchar(dcall, type = "w") + nchar(sm[1L], type = "w")
    > if (is.na(w))  8:             w <- 14L + nchar(dcall, type = "b") +
    > nchar(sm[1L], try(lapply(X = S, FUN = FUN, ...), silent = TRUE)
    > type = "b")
    > if (w > LONG)             prefix <- paste0(prefix, "\n  ") 9:
    > }sendMaster(try(lapply(X = S, FUN = FUN, ...), silent = TRUE))    else
    > prefix <- "Error : "
    > msg <- paste0(prefix, conditionMessage(e), "\n")
    > .Internal(seterrmessage(msg[1L]))10:     if (!silent &&
    > isTRUE(getOption("show.error.messages"))) {FUN(X[[i]], ...)        cat(msg,
    > file = outFile)
    > .Internal(printDeferredWarnings())    }11:
    > invisible(structure(msg, class = "try-error", condition =
    > e))lapply(seq_len(cores), inner.do)})

    > 12:  8: parallel::mclapply(files, function(s)
    > download.file(paste0(url_base, try(lapply(X = S, FUN = FUN, ...), silent =
    > TRUE)    s), s))

    > 9:
    > sendMaster(try(lapply(X = S, FUN = FUN, ...), silent = TRUE))Possible
    > actions:

    > 1: abort (with core dump, if enabled)
    > 2: normal R exit
    > 10: 3: exit R without saving workspace
    > FUN(X[[i]], ...)4: exit R saving workspace

    > 11: lapply(seq_len(cores), inner.do)
    > 12: parallel::mclapply(files, function(s) download.file(paste0(url_base,
    > s), s))

    > Here's my sessionInfo()

    >> sessionInfo()
    > R version 3.5.1 (2018-07-02)
    > Platform: x86_64-apple-darwin16.7.0 (64-bit)
    > Running under: macOS Sierra 10.12.6

    > Matrix products: default
    > BLAS/LAPACK: /usr/local/Cellar/openblas/0.3.3/lib/libopenblasp-r0.3.3.dylib

    > locale:
    > [1] en_US/en_US/en_US/C/en_US/en_US

    > attached base packages:
    > [1] parallel  stats     graphics  grDevices utils     datasets  methods
    > [8] base

    > loaded via a namespace (and not attached):
    > [1] compiler_3.5.1

    > My version of R I'm running was installed via homebrew with "brew install r
    > --with-java --with-openblas"

    > Also, the provided example code works as expected on Linux. Also, if I
    > provide a non-default download method to the download.file() call such as:

    > res <- parallel::mclapply(files, function(s) download.file(paste0(url_base,
    > s), s, method="wget"))
    > res <- parallel::mclapply(files, function(s) download.file(paste0(url_base,
    > s), s, method="curl"))

    > It works correctly - no segfault. If I use method="libcurl" it does
    > segfault.

    > I'm not sure what steps to take to further narrow down the source of the
    > error.

    > Is this a known bug? if not, is this a new bug or an unexpected feature?

    > Thanks,
    > Seth

    > [[alternative HTML version deleted]]

    > ______________________________________________
    > R-devel at r-project.org mailing list
    > https://stat.ethz.ch/mailman/listinfo/r-devel


From pd@lgd @ending from gm@il@com  Thu Sep 20 10:42:10 2018
From: pd@lgd @ending from gm@il@com (peter dalgaard)
Date: Thu, 20 Sep 2018 10:42:10 +0200
Subject: [Rd] A different error in sample()
In-Reply-To: <1899129802.76058.1537401201170@mail.yahoo.com>
References: <1899129802.76058.1537401201170.ref@mail.yahoo.com>
 <1899129802.76058.1537401201170@mail.yahoo.com>
Message-ID: <06798949-8B14-4DC4-9B43-B436D7232A29@gmail.com>

Yup, that is a bug, at least in the documentation. Probably a clearer example is 

x <- seq(2.001, 2.999, length.out=999)
threes <- sapply(x, function(y) table(sample(y, 10000, replace=TRUE))[3])
plot(threes, type="l")
curve(10000*(x-2)/x, add=TRUE, col="red")

which is entirely consistent with what you'd expect from floor(runif(10000, 0, y)) + 1, and as far as I can tell from the source, that is what is happening internally. 

(Strict monotonicity is a bit of a red herring, it is jut a matter of having spaced the y so far apart that the probability of an order reversal becomes negligible.)

So either we should do what the documentation says we do, or the documentation should not say that we do what we do not actually do...

The suspect code is this snippet from do_sample:

            int n = (int) dn;
            .....

            if (replace || k < 2) {
                for (int i = 0; i < k; i++) iy[i] = (int)(R_unif_index(dn) + 1);
            } else {
                int *x = (int *)R_alloc(n, sizeof(int));
                for (int i = 0; i < n; i++) x[i] = i;
                for (int i = 0; i < k; i++) {
                    int j = (int)(R_unif_index(n));
                    iy[i] = x[j] + 1;
                    x[j] = x[--n];
                }
            }

(notice arguments to R_unif_index)

-pd

> On 20 Sep 2018, at 01:53 , lmo via R-devel <r-devel at r-project.org> wrote:
> 
> Although it seems to be pretty weird to enter a numeric vector of length one that is not an integer as the first argument to sample(), the results do not seem to match what is documented in the manual. In addition, the results below do not support the use of round rather than truncate in the documentation. Consider the code below.
> The first sentence in the details section says: "If x has length 1, is numeric (in the sense of is.numeric) and x >= 1, sampling via sample takes place from 1:x."
> In the console:> 1:2.001
> [1] 1 2
>> 1:2.9
> [1] 1 2
> 
> truncation:
>> trunc(2.9)
> [1] 2
> 
> So, this seems to support the quote from in previous emails: "Non-integer positive numerical values of n or x will be truncated to the next smallest integer, which has to be no larger than .Machine$integer.max."
> However, again in the console:> set.seed(123)
>> table(sample(2.001, 10000, replace=TRUE))
> 
>    1    2    3 
> 5052 4941    7
> 
> So, neither rounding nor truncation is occurring. Next, define a sequence.
>> x <- seq(2.001, 2.51, length.out=20)
> Now, grab all of the threes from sample()-ing this sequence.
> 
>> set.seed(123)
>> threes <- sapply(x, function(y) table(sample(y, 10000, replace=TRUE))[3])
> 
> Check for NAs (I cheated here and found a nice seed).> any(is.na(threes))
> [1] FALSE
> Now, the (to me) disturbing result.
> 
>> is.unsorted(threes)
> [1] FALSE
> 
> or equivalently
> 
>> all(diff(threes) > 0)
> [1] TRUE
> 
> So the number of threes grows monotonically as 2.001 moves to 2.5. As I hinted above, the monotonic growth is not assured. My guess is that the growth is stochastic and relates to some "probability weighting" based on how close the element of x is to 3. Perhaps this has been brought up before, but it seems relevant to the current discussion.
> A potential aid to this issue would be something like
> if(length(x) == 1 && !all.equal(x, as.integer(x))) warning("It is a bad idea to use vectors of length 1 in the x argument that are not integers.")
> Hope that helps,luke
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From c@@rdi@g@bor @ending from gm@il@com  Thu Sep 20 10:53:26 2018
From: c@@rdi@g@bor @ending from gm@il@com (=?UTF-8?B?R8OhYm9yIENzw6FyZGk=?=)
Date: Thu, 20 Sep 2018 09:53:26 +0100
Subject: [Rd] 
 segfault issue with parallel::mclapply and download.file() on
 Mac OS X
In-Reply-To: <CAB75gRUv1LitTgWSbMFunqp7P9wfWG_BJ_xYBPN4K5KJkDu+_g@mail.gmail.com>
References: <CAB75gRUv1LitTgWSbMFunqp7P9wfWG_BJ_xYBPN4K5KJkDu+_g@mail.gmail.com>
Message-ID: <CABtg=KktP1KkbGwfwD+Qa4ouCBVwL=snkSqJp85HhiJA927xUw@mail.gmail.com>

This code actually happens to work for me on macOS, but I think in
general you cannot rely on performing HTTP requests in fork clusters,
i.e. with mclapply().

Fork clusters create worker processes by forking the R process and
then _not_ executing another R binary. (Which is often convenient,
because the new processes will inherit the memory image of the parent
process.)

Fork without exec is not supported by macOS, basically any calls to
system libraries might crash. (Ie. not just HTTP-related calls.) For
HTTP calls I have seen errors, crashes, and sometimes it works.
Depends on the combination of libcurl version, macOS version and
probably luck.

It usually (always?) works on Linux, but I would not rely on that, either.

So, yes, this is a known issue.

Creating new processes to perform HTTP in parallel is very often bad
practice, actually. Whenever you can, use I/O multiplexing instead,
since the main R process is not doing anything, anyway, just waiting
for the data to come in. So you don't need more processes, you need
parallel I/O. Take a look at the curl::multi_add() etc. functions.

Btw. download.file() can actually download files in parallel if the
liburl method is used, just give it a list of URLs in a character
vector. This API is very restricted, though, so I suggest to look at
the curl package.

GaborOn Thu, Sep 20, 2018 at 8:44 AM Seth Russell
<seth.russell at gmail.com> wrote:
>
> I have an lapply function call that I want to parallelize. Below is a very
> simplified version of the code:
>
> url_base <- "https://cloud.r-project.org/src/contrib/"
> files <- c("A3_1.0.0.tar.gz", "ABC.RAP_0.9.0.tar.gz")
> res <- parallel::mclapply(files, function(s) download.file(paste0(url_base,
> s), s))
>
> Instead of download a couple of files in parallel, I get a segfault per
> process with a 'memory not mapped' message. I've been working with Henrik
> Bengtsson on resolving this issue and he recommended I send a message to
> the R-Devel mailing list.
>
> Here's the output:
>
> trying URL 'https://cloud.r-project.org/src/contrib/A3_1.0.0.tar.gz'
> trying URL 'https://cloud.r-project.org/src/contrib/ABC.RAP_0.9.0.tar.gz'
>
>  *** caught segfault ***
> address 0x11575ba3a, cause 'memory not mapped'
>
>  *** caught segfault ***
> address 0x11575ba3a, cause 'memory not mapped'
>
> Traceback:
>  1: download.file(paste0(url_base, s), s)
>  2: FUN(X[[i]], ...)
>  3: lapply(X = S, FUN = FUN, ...)
>  4: doTryCatch(return(expr), name, parentenv, handler)
>  5: tryCatchOne(expr, names, parentenv, handlers[[1L]])
>  6: tryCatchList(expr, classes, parentenv, handlers)
>  7: tryCatch(expr, error = function(e) {    call <- conditionCall(e)    if
> (!is.null(call)) {        if (identical(call[[1L]], quote(doTryCatch)))
>         call <- sys.call(-4L)        dcall <- deparse(call)[1L]
>  prefix <- paste("Error in", dcall, ": ")
>         LONG <- 75LTraceback:
>         sm <- strsplit(conditionMessage(e), "\n")[[1L]] 1:         w <- 14L
> + nchar(dcall, type = "w") + nchar(sm[1L], type = "w")        if (is.na(w))
> download.file(paste0(url_base, s), s)            w <- 14L + nchar(dcall,
> type = "b") + nchar(sm[1L],
>                 type = "b")        if (w > LONG)  2: FUN(X[[i]], ...)
>  3: lapply(X = S, FUN = FUN, ...)
>  4: doTryCatch(return(expr), name, parentenv, handler)
>  5: tryCatchOne(expr, names, parentenv, handlers[[1L]])
>  6:             prefix <- paste0(prefix, "\n  ")tryCatchList(expr, classes,
> parentenv, handlers)
>     }    else prefix <- "Error : " 7:     msg <- paste0(prefix,
> conditionMessage(e), "\n")tryCatch(expr, error = function(e) {
>  .Internal(seterrmessage(msg[1L]))    call <- conditionCall(e)    if
> (!silent && isTRUE(getOption("show.error.messages"))) {    if
> (!is.null(call)) {        cat(msg, file = outFile)        if
> (identical(call[[1L]], quote(doTryCatch)))
> .Internal(printDeferredWarnings())            call <- sys.call(-4L)    }
>      dcall <- deparse(call)[1L]    invisible(structure(msg, class =
> "try-error", condition = e))        prefix <- paste("Error in", dcall, ":
> ")})        LONG <- 75L        sm <- strsplit(conditionMessage(e),
> "\n")[[1L]]
>         w <- 14L + nchar(dcall, type = "w") + nchar(sm[1L], type = "w")
>    if (is.na(w))  8:             w <- 14L + nchar(dcall, type = "b") +
> nchar(sm[1L], try(lapply(X = S, FUN = FUN, ...), silent = TRUE)
>    type = "b")
>         if (w > LONG)             prefix <- paste0(prefix, "\n  ") 9:
> }sendMaster(try(lapply(X = S, FUN = FUN, ...), silent = TRUE))    else
> prefix <- "Error : "
>     msg <- paste0(prefix, conditionMessage(e), "\n")
>  .Internal(seterrmessage(msg[1L]))10:     if (!silent &&
> isTRUE(getOption("show.error.messages"))) {FUN(X[[i]], ...)        cat(msg,
> file = outFile)
>         .Internal(printDeferredWarnings())    }11:
> invisible(structure(msg, class = "try-error", condition =
> e))lapply(seq_len(cores), inner.do)})
>
> 12:  8: parallel::mclapply(files, function(s)
> download.file(paste0(url_base, try(lapply(X = S, FUN = FUN, ...), silent =
> TRUE)    s), s))
>
>  9:
> sendMaster(try(lapply(X = S, FUN = FUN, ...), silent = TRUE))Possible
> actions:
>
> 1: abort (with core dump, if enabled)
> 2: normal R exit
> 10: 3: exit R without saving workspace
> FUN(X[[i]], ...)4: exit R saving workspace
>
> 11: lapply(seq_len(cores), inner.do)
> 12: parallel::mclapply(files, function(s) download.file(paste0(url_base,
>   s), s))
>
> Here's my sessionInfo()
>
> > sessionInfo()
> R version 3.5.1 (2018-07-02)
> Platform: x86_64-apple-darwin16.7.0 (64-bit)
> Running under: macOS Sierra 10.12.6
>
> Matrix products: default
> BLAS/LAPACK: /usr/local/Cellar/openblas/0.3.3/lib/libopenblasp-r0.3.3.dylib
>
> locale:
> [1] en_US/en_US/en_US/C/en_US/en_US
>
> attached base packages:
> [1] parallel  stats     graphics  grDevices utils     datasets  methods
> [8] base
>
> loaded via a namespace (and not attached):
> [1] compiler_3.5.1
>
> My version of R I'm running was installed via homebrew with "brew install r
> --with-java --with-openblas"
>
> Also, the provided example code works as expected on Linux. Also, if I
> provide a non-default download method to the download.file() call such as:
>
> res <- parallel::mclapply(files, function(s) download.file(paste0(url_base,
> s), s, method="wget"))
> res <- parallel::mclapply(files, function(s) download.file(paste0(url_base,
> s), s, method="curl"))
>
> It works correctly - no segfault. If I use method="libcurl" it does
> segfault.
>
> I'm not sure what steps to take to further narrow down the source of the
> error.
>
> Is this a known bug? if not, is this a new bug or an unexpected feature?
>
> Thanks,
> Seth
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From leo@l@hti @ending from iki@fi  Thu Sep 20 12:56:29 2018
From: leo@l@hti @ending from iki@fi (Leo Lahti)
Date: Thu, 20 Sep 2018 12:56:29 +0200
Subject: [Rd] future time stamps warning
Message-ID: <CAHm4+KDgui7QFaLNDCGZm7K9q3a3xCAC19rWjfPGZ3EEeON-sA@mail.gmail.com>

Dear developers,

Upon CRAN submission I have bumped into "future file timestamps" warning
that I can't solve. I have updated the package as usual, and all checks go
through in my system. CRAN reports the following warning however.

* checking for future file timestanps ... WARNING
Files with future time stamps:
  DESCRIPTION
  NAMESPACE
  README.md

The build log is at
https://win-builder.r-project.org/incoming_pretest/eurostat_3.2.7_20180920_004954/Debian/00check.log

The package is at
https://github.com/rOpenGov/eurostat/


I did not find guidance (search engines, R extensions manual)
on how to solve this. Perhaps we would need to generate the
files in a certain order but not sure what is the correct order.

Any tips?

kind regards

Leo Lahti

	[[alternative HTML version deleted]]


From murdoch@dunc@n @ending from gm@il@com  Thu Sep 20 12:07:14 2018
From: murdoch@dunc@n @ending from gm@il@com (Duncan Murdoch)
Date: Thu, 20 Sep 2018 06:07:14 -0400
Subject: [Rd] future time stamps warning
In-Reply-To: <CAHm4+KDgui7QFaLNDCGZm7K9q3a3xCAC19rWjfPGZ3EEeON-sA@mail.gmail.com>
References: <CAHm4+KDgui7QFaLNDCGZm7K9q3a3xCAC19rWjfPGZ3EEeON-sA@mail.gmail.com>
Message-ID: <ee162471-a18f-e01b-7abc-1e5f8b770d4e@gmail.com>

On 20/09/2018 6:56 AM, Leo Lahti wrote:
> Dear developers,
> 
> Upon CRAN submission I have bumped into "future file timestamps" warning
> that I can't solve. I have updated the package as usual, and all checks go
> through in my system. CRAN reports the following warning however.
> 
> * checking for future file timestanps ... WARNING
> Files with future time stamps:
>    DESCRIPTION
>    NAMESPACE
>    README.md
> 
> The build log is at
> https://win-builder.r-project.org/incoming_pretest/eurostat_3.2.7_20180920_004954/Debian/00check.log
> 
> The package is at
> https://github.com/rOpenGov/eurostat/
> 
> 
> I did not find guidance (search engines, R extensions manual)
> on how to solve this. Perhaps we would need to generate the
> files in a certain order but not sure what is the correct order.
> 
> Any tips?

What are the time stamps on those files?  You can list the contents of 
the tarball at the command line using

tar ztvf wCorr_1.9.0.tar.gz

Are the timestamps correct?

Duncan Murdoch


From c@@rdi@g@bor @ending from gm@il@com  Thu Sep 20 12:05:12 2018
From: c@@rdi@g@bor @ending from gm@il@com (=?UTF-8?B?R8OhYm9yIENzw6FyZGk=?=)
Date: Thu, 20 Sep 2018 11:05:12 +0100
Subject: [Rd] future time stamps warning
In-Reply-To: <CAHm4+KDgui7QFaLNDCGZm7K9q3a3xCAC19rWjfPGZ3EEeON-sA@mail.gmail.com>
References: <CAHm4+KDgui7QFaLNDCGZm7K9q3a3xCAC19rWjfPGZ3EEeON-sA@mail.gmail.com>
Message-ID: <CABtg=Km-B+kaMkmaGafZgxv73O0jU1iUoeWfyRupvXwC9mCVkQ@mail.gmail.com>

Have you tried setting your system clock correctly and then Sys.setFileTime()?

Gabor
On Thu, Sep 20, 2018 at 10:58 AM Leo Lahti <leo.lahti at iki.fi> wrote:
>
> Dear developers,
>
> Upon CRAN submission I have bumped into "future file timestamps" warning
> that I can't solve. I have updated the package as usual, and all checks go
> through in my system. CRAN reports the following warning however.
>
> * checking for future file timestanps ... WARNING
> Files with future time stamps:
>   DESCRIPTION
>   NAMESPACE
>   README.md
>
> The build log is at
> https://win-builder.r-project.org/incoming_pretest/eurostat_3.2.7_20180920_004954/Debian/00check.log
>
> The package is at
> https://github.com/rOpenGov/eurostat/
>
>
> I did not find guidance (search engines, R extensions manual)
> on how to solve this. Perhaps we would need to generate the
> files in a certain order but not sure what is the correct order.
>
> Any tips?
>
> kind regards
>
> Leo Lahti
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From leo@l@hti @ending from iki@fi  Thu Sep 20 13:44:33 2018
From: leo@l@hti @ending from iki@fi (Leo Lahti)
Date: Thu, 20 Sep 2018 13:44:33 +0200
Subject: [Rd] future time stamps warning
In-Reply-To: <ee162471-a18f-e01b-7abc-1e5f8b770d4e@gmail.com>
References: <CAHm4+KDgui7QFaLNDCGZm7K9q3a3xCAC19rWjfPGZ3EEeON-sA@mail.gmail.com>
 <ee162471-a18f-e01b-7abc-1e5f8b770d4e@gmail.com>
Message-ID: <CAHm4+KCEhp++HcNGpZkMK7DGed8rNe4H+1NbRv_2A4B3i1Ji5g@mail.gmail.com>

Time stamps are correct and my system time is correct.

I am now tried to use Sys.setFileTime() to update time stamps as proposed.
This does not help.

The windows and debian builds give different reports on the time stamp
issue.
https://win-builder.r-project.org/incoming_pretest/eurostat_3.2.8_20180920_122655/Windows/00check.log
https://win-builder.r-project.org/incoming_pretest/eurostat_3.2.8_20180920_122655/Debian/00check.log

I attached the time stamp listing below.

Leo



lei at kone:~/Rpackages/eurostat/inst/extras$ tar zvtf eurostat_3.2.8.tar.gz
drwxr-xr-x lei/lei           0 2018-09-20 13:23 eurostat/build/
-rw-r--r-- lei/lei         301 2018-09-20 13:23 eurostat/build/vignette.rds
drwxr-xr-x lei/lei           0 2018-09-20 13:23 eurostat/data/
-rw-r--r-- lei/lei          98 2018-09-20 13:23 eurostat/data/datalist
-rwxr-xr-x lei/lei         340 2018-06-17 20:44
eurostat/data/ea_countries.rda
-rwxr-xr-x lei/lei         214 2018-06-17 20:44
eurostat/data/efta_countries.rda
-rwxr-xr-x lei/lei         252 2018-06-17 20:44
eurostat/data/eu_candidate_countries.rda
-rwxr-xr-x lei/lei         431 2018-06-17 20:44
eurostat/data/eu_countries.rda
-rw-r--r-- lei/lei     3651661 2018-08-28 18:44
eurostat/data/eurostat_geodata_60_2016.rda
-rwxr-xr-x lei/lei        5596 2018-06-17 20:44 eurostat/data/tgs00026.rda
-rwxr-xr-x lei/lei        1447 2018-09-20 13:23 eurostat/DESCRIPTION
drwxr-xr-x lei/lei           0 2018-09-20 13:23 eurostat/inst/
-rwxr-xr-x lei/lei        1060 2018-06-17 20:44 eurostat/inst/CITATION
drwxr-xr-x lei/lei           0 2018-09-20 13:23 eurostat/inst/doc/
-rw-r--r-- lei/lei        6361 2018-09-20 13:22
eurostat/inst/doc/blogposts.html
-rwxr-xr-x lei/lei         518 2018-06-17 20:44
eurostat/inst/doc/blogposts.Rmd
-rw-r--r-- lei/lei        6455 2018-09-20 13:22
eurostat/inst/doc/cheatsheet.html
-rwxr-xr-x lei/lei         610 2018-06-17 20:44
eurostat/inst/doc/cheatsheet.Rmd
-rw-r--r-- lei/lei      246912 2018-09-20 01:43
eurostat/inst/doc/eurostat_tutorial.pdf
-rw-r--r-- lei/lei       10565 2018-09-20 13:23
eurostat/inst/doc/eurostat_tutorial.R
-rwxr-xr-x lei/lei       17177 2018-09-20 00:07
eurostat/inst/doc/eurostat_tutorial.Rmd
-rw-r--r-- lei/lei        6834 2018-09-20 13:23
eurostat/inst/doc/publications.html
-rwxr-xr-x lei/lei         942 2018-06-17 20:44
eurostat/inst/doc/publications.Rmd
-rwxr-xr-x lei/lei          92 2018-06-17 20:44 eurostat/LICENSE
drwxr-xr-x lei/lei           0 2018-08-28 18:44 eurostat/man/
-rwxr-xr-x lei/lei         697 2018-06-17 20:44
eurostat/man/clean_eurostat_cache.Rd
-rwxr-xr-x lei/lei         395 2018-06-17 20:44
eurostat/man/convert_time_col.Rd
-rwxr-xr-x lei/lei        1276 2018-06-17 20:44
eurostat/man/cut_to_classes.Rd
-rwxr-xr-x lei/lei        1133 2018-06-17 20:44 eurostat/man/dic_order.Rd
-rwxr-xr-x lei/lei         788 2018-06-17 20:44 eurostat/man/eu_countries.Rd
-rw-r--r-- lei/lei         826 2018-08-28 18:44
eurostat/man/eurostat_geodata_60_2016.Rd
-rwxr-xr-x lei/lei         805 2018-06-17 20:44
eurostat/man/eurostat-package.Rd
-rwxr-xr-x lei/lei        1220 2018-06-17 20:44
eurostat/man/eurotime2date.Rd
-rwxr-xr-x lei/lei         978 2018-06-17 20:44 eurostat/man/eurotime2num.Rd
-rwxr-xr-x lei/lei        1169 2018-06-17 20:44
eurostat/man/get_eurostat_dic.Rd
-rwxr-xr-x lei/lei        2321 2018-08-28 18:44
eurostat/man/get_eurostat_geospatial.Rd
-rwxr-xr-x lei/lei        2675 2018-08-28 18:44
eurostat/man/get_eurostat_json.Rd
-rwxr-xr-x lei/lei        1172 2018-06-17 20:44
eurostat/man/get_eurostat_raw.Rd
-rwxr-xr-x lei/lei        1191 2018-06-17 20:44
eurostat/man/get_eurostat_toc.Rd
-rwxr-xr-x lei/lei        6351 2018-08-28 18:44 eurostat/man/get_eurostat.Rd
-rwxr-xr-x lei/lei         783 2018-06-17 20:44
eurostat/man/harmonize_country_code.Rd
-rwxr-xr-x lei/lei        2518 2018-06-17 20:44
eurostat/man/label_eurostat.Rd
-rwxr-xr-x lei/lei        2015 2018-06-17 20:44
eurostat/man/search_eurostat.Rd
-rwxr-xr-x lei/lei         453 2018-06-17 20:44
eurostat/man/set_eurostat_toc.Rd
-rwxr-xr-x lei/lei         344 2018-06-17 20:44 eurostat/man/tgs00026.Rd
-rwxr-xr-x lei/lei        1696 2018-06-17 20:44
eurostat/man/tidy_eurostat.Rd
-rwxr-xr-x lei/lei        1273 2018-09-20 13:21 eurostat/NAMESPACE
drwxr-xr-x lei/lei           0 2018-09-20 13:23 eurostat/R/
-rwxr-xr-x lei/lei        1140 2018-06-17 20:44
eurostat/R/clean_eurostat_cache.R
-rwxr-xr-x lei/lei        3191 2018-06-17 20:44 eurostat/R/cut_to_classes.R
-rwxr-xr-x lei/lei         638 2018-06-17 20:44 eurostat/R/data_countries.R
-rwxr-xr-x lei/lei        4571 2018-08-28 18:44 eurostat/R/data_spatial.R
-rwxr-xr-x lei/lei         210 2018-06-17 20:44 eurostat/R/data.R
-rwxr-xr-x lei/lei        1310 2018-06-17 20:44 eurostat/R/dic_order.R
-rwxr-xr-x lei/lei         721 2018-06-17 20:44
eurostat/R/eurostat-package.R
-rwxr-xr-x lei/lei        2725 2018-06-17 20:44 eurostat/R/eurotime2date.R
-rwxr-xr-x lei/lei        1918 2018-06-17 20:44 eurostat/R/eurotime2num.R
-rwxr-xr-x lei/lei         967 2018-07-23 17:01 eurostat/R/firstlib.R
-rwxr-xr-x lei/lei        1668 2018-08-28 18:44
eurostat/R/get_eurostat_dic.R
-rwxr-xr-x lei/lei        9550 2018-08-28 18:44
eurostat/R/get_eurostat_geospatial.R
-rwxr-xr-x lei/lei        4986 2018-06-17 20:44
eurostat/R/get_eurostat_json.R
-rwxr-xr-x lei/lei        2111 2018-08-28 18:44
eurostat/R/get_eurostat_raw.R
-rwxr-xr-x lei/lei        1315 2018-06-17 20:44
eurostat/R/get_eurostat_toc.R
-rwxr-xr-x lei/lei        9401 2018-08-28 18:44 eurostat/R/get_eurostat.R
-rwxr-xr-x lei/lei         816 2018-06-17 20:44
eurostat/R/harmonize_country_code.R
-rwxr-xr-x lei/lei        6208 2018-06-17 20:44 eurostat/R/label_eurostat.R
-rwxr-xr-x lei/lei        2185 2018-06-17 20:44 eurostat/R/search_eurostat.R
-rwxr-xr-x lei/lei         752 2018-08-28 18:44
eurostat/R/set_eurostat_toc.R
-rwxr-xr-x lei/lei        5090 2018-06-17 20:44 eurostat/R/tidy_eurostat.R
-rwxr-xr-x lei/lei         433 2018-06-17 20:44 eurostat/R/utils.R
-rw-r--r-- lei/lei         311 2018-08-28 18:44 eurostat/R/zzz.R
-rwxr-xr-x lei/lei        5342 2018-09-20 01:48 eurostat/README.md
drwxr-xr-x lei/lei           0 2018-06-17 20:44 eurostat/tests/
drwxr-xr-x lei/lei           0 2018-06-17 20:44 eurostat/tests/testthat/
-rwxr-xr-x lei/lei          60 2018-06-17 20:44 eurostat/tests/testthat.R
-rwxr-xr-x lei/lei        5021 2018-06-17 20:44
eurostat/tests/testthat/test-all.R
drwxr-xr-x lei/lei           0 2018-09-20 13:23 eurostat/vignettes/
drwxr-xr-x lei/lei           0 2018-09-20 13:23
eurostat/vignettes/2017_RJournal_manuscript/
-rwxr-xr-x lei/lei      677086 2018-06-17 20:44
eurostat/vignettes/2017_RJournal_manuscript/2017_RJournal_manuscript.zip
-rwxr-xr-x lei/lei      397875 2018-06-17 20:44
eurostat/vignettes/2017_RJournal_manuscript/Proofs.zip
-rwxr-xr-x lei/lei         620 2018-06-17 20:44
eurostat/vignettes/2017_RJournal_manuscript/README.md
-rwxr-xr-x lei/lei         518 2018-06-17 20:44
eurostat/vignettes/blogposts.Rmd
drwxr-xr-x lei/lei           0 2018-06-17 20:44
eurostat/vignettes/cheatsheet/
-rwxr-xr-x lei/lei         610 2018-06-17 20:44
eurostat/vignettes/cheatsheet.Rmd
-rwxr-xr-x lei/lei      790957 2018-06-17 20:44
eurostat/vignettes/cheatsheet/eurostat_cheatsheet.ai
-rwxr-xr-x lei/lei      830868 2018-06-17 20:44
eurostat/vignettes/cheatsheet/eurostat_cheatsheet.pdf
-rwxr-xr-x lei/lei      482250 2018-09-20 13:23
eurostat/vignettes/eurostat_tutorial.html
-rwxr-xr-x lei/lei       43215 2018-09-20 01:48
eurostat/vignettes/eurostat_tutorial.md
-rwxr-xr-x lei/lei       17177 2018-09-20 00:07
eurostat/vignettes/eurostat_tutorial.Rmd
drwxr-xr-x lei/lei           0 2018-09-20 01:48 eurostat/vignettes/fig/
-rwxr-xr-x lei/lei       63101 2018-09-20 01:42
eurostat/vignettes/fig/map1ex-1.pdf
-rwxr-xr-x lei/lei        2244 2018-09-20 13:23
eurostat/vignettes/fig/map1ex-1.png
-rwxr-xr-x lei/lei       10157 2018-06-17 20:44
eurostat/vignettes/fig/maps1-1-1.pdf
-rwxr-xr-x lei/lei      217962 2018-07-22 14:49
eurostat/vignettes/fig/maps1-1-1.png
-rwxr-xr-x lei/lei       46180 2018-09-20 01:48
eurostat/vignettes/fig/maps2-1.pdf
-rwxr-xr-x lei/lei        5716 2018-09-20 13:23
eurostat/vignettes/fig/maps2-1.png
-rwxr-xr-x lei/lei      189518 2018-07-22 14:49
eurostat/vignettes/fig/maps3-1.pdf
-rwxr-xr-x lei/lei      136509 2018-09-20 13:23
eurostat/vignettes/fig/maps3-1.png
-rwxr-xr-x lei/lei      145304 2018-09-20 13:23
eurostat/vignettes/fig/maps4-1.png
-rwxr-xr-x lei/lei        7482 2018-09-20 01:48
eurostat/vignettes/fig/plotGallery-1.pdf
-rwxr-xr-x lei/lei        9111 2018-09-20 13:22
eurostat/vignettes/fig/plotGallery-1.png
-rwxr-xr-x lei/lei        5101 2018-09-20 01:48
eurostat/vignettes/fig/trains_plot-1.pdf
-rwxr-xr-x lei/lei        3984 2018-09-20 13:22
eurostat/vignettes/fig/trains_plot-1.png
-rwxr-xr-x lei/lei         510 2018-06-17 20:44 eurostat/vignettes/main.R
-rwxr-xr-x lei/lei         942 2018-06-17 20:44
eurostat/vignettes/publications.Rmd




On Thu, Sep 20, 2018 at 12:07 PM Duncan Murdoch <murdoch.duncan at gmail.com>
wrote:

> On 20/09/2018 6:56 AM, Leo Lahti wrote:
> > Dear developers,
> >
> > Upon CRAN submission I have bumped into "future file timestamps" warning
> > that I can't solve. I have updated the package as usual, and all checks
> go
> > through in my system. CRAN reports the following warning however.
> >
> > * checking for future file timestanps ... WARNING
> > Files with future time stamps:
> >    DESCRIPTION
> >    NAMESPACE
> >    README.md
> >
> > The build log is at
> >
> https://win-builder.r-project.org/incoming_pretest/eurostat_3.2.7_20180920_004954/Debian/00check.log
> >
> > The package is at
> > https://github.com/rOpenGov/eurostat/
> >
> >
> > I did not find guidance (search engines, R extensions manual)
> > on how to solve this. Perhaps we would need to generate the
> > files in a certain order but not sure what is the correct order.
> >
> > Any tips?
>
> What are the time stamps on those files?  You can list the contents of
> the tarball at the command line using
>
> tar ztvf wCorr_1.9.0.tar.gz
>
> Are the timestamps correct?
>
> Duncan Murdoch
>

	[[alternative HTML version deleted]]


From c@@rdi@g@bor @ending from gm@il@com  Thu Sep 20 12:49:14 2018
From: c@@rdi@g@bor @ending from gm@il@com (=?UTF-8?B?R8OhYm9yIENzw6FyZGk=?=)
Date: Thu, 20 Sep 2018 11:49:14 +0100
Subject: [Rd] future time stamps warning
In-Reply-To: <CAHm4+KCEhp++HcNGpZkMK7DGed8rNe4H+1NbRv_2A4B3i1Ji5g@mail.gmail.com>
References: <CAHm4+KDgui7QFaLNDCGZm7K9q3a3xCAC19rWjfPGZ3EEeON-sA@mail.gmail.com>
 <ee162471-a18f-e01b-7abc-1e5f8b770d4e@gmail.com>
 <CAHm4+KCEhp++HcNGpZkMK7DGed8rNe4H+1NbRv_2A4B3i1Ji5g@mail.gmail.com>
Message-ID: <CABtg=Kk44-iM00JHTjD8ByUuj=VTQRnHe-+hG8OFCbBd5UBcSQ@mail.gmail.com>

On Thu, Sep 20, 2018 at 11:46 AM Leo Lahti <leo.lahti at iki.fi> wrote:
>
> Time stamps are correct and my system time is correct.
>
> I am now tried to use Sys.setFileTime() to update time stamps as proposed.
> This does not help.
>
> The windows and debian builds give different reports on the time stamp
> issue.
> https://win-builder.r-project.org/incoming_pretest/eurostat_3.2.8_20180920_122655/Windows/00check.log
> https://win-builder.r-project.org/incoming_pretest/eurostat_3.2.8_20180920_122655/Debian/00check.log
>
> I attached the time stamp listing below.

Well, maybe it is an inaccurate system clock on your machine, or on
CRAN, but nevertheless,
for this submission, you can just set the time stamps to a time in the past.

Gabor

> Leo
>
>
>
> lei at kone:~/Rpackages/eurostat/inst/extras$ tar zvtf eurostat_3.2.8.tar.gz
> drwxr-xr-x lei/lei           0 2018-09-20 13:23 eurostat/build/
> -rw-r--r-- lei/lei         301 2018-09-20 13:23 eurostat/build/vignette.rds
> drwxr-xr-x lei/lei           0 2018-09-20 13:23 eurostat/data/
> -rw-r--r-- lei/lei          98 2018-09-20 13:23 eurostat/data/datalist
> -rwxr-xr-x lei/lei         340 2018-06-17 20:44
> eurostat/data/ea_countries.rda
> -rwxr-xr-x lei/lei         214 2018-06-17 20:44
> eurostat/data/efta_countries.rda
> -rwxr-xr-x lei/lei         252 2018-06-17 20:44
> eurostat/data/eu_candidate_countries.rda
> -rwxr-xr-x lei/lei         431 2018-06-17 20:44
> eurostat/data/eu_countries.rda
> -rw-r--r-- lei/lei     3651661 2018-08-28 18:44
> eurostat/data/eurostat_geodata_60_2016.rda
> -rwxr-xr-x lei/lei        5596 2018-06-17 20:44 eurostat/data/tgs00026.rda
> -rwxr-xr-x lei/lei        1447 2018-09-20 13:23 eurostat/DESCRIPTION
> drwxr-xr-x lei/lei           0 2018-09-20 13:23 eurostat/inst/
> -rwxr-xr-x lei/lei        1060 2018-06-17 20:44 eurostat/inst/CITATION
> drwxr-xr-x lei/lei           0 2018-09-20 13:23 eurostat/inst/doc/
> -rw-r--r-- lei/lei        6361 2018-09-20 13:22
> eurostat/inst/doc/blogposts.html
> -rwxr-xr-x lei/lei         518 2018-06-17 20:44
> eurostat/inst/doc/blogposts.Rmd
> -rw-r--r-- lei/lei        6455 2018-09-20 13:22
> eurostat/inst/doc/cheatsheet.html
> -rwxr-xr-x lei/lei         610 2018-06-17 20:44
> eurostat/inst/doc/cheatsheet.Rmd
> -rw-r--r-- lei/lei      246912 2018-09-20 01:43
> eurostat/inst/doc/eurostat_tutorial.pdf
> -rw-r--r-- lei/lei       10565 2018-09-20 13:23
> eurostat/inst/doc/eurostat_tutorial.R
> -rwxr-xr-x lei/lei       17177 2018-09-20 00:07
> eurostat/inst/doc/eurostat_tutorial.Rmd
> -rw-r--r-- lei/lei        6834 2018-09-20 13:23
> eurostat/inst/doc/publications.html
> -rwxr-xr-x lei/lei         942 2018-06-17 20:44
> eurostat/inst/doc/publications.Rmd
> -rwxr-xr-x lei/lei          92 2018-06-17 20:44 eurostat/LICENSE
> drwxr-xr-x lei/lei           0 2018-08-28 18:44 eurostat/man/
> -rwxr-xr-x lei/lei         697 2018-06-17 20:44
> eurostat/man/clean_eurostat_cache.Rd
> -rwxr-xr-x lei/lei         395 2018-06-17 20:44
> eurostat/man/convert_time_col.Rd
> -rwxr-xr-x lei/lei        1276 2018-06-17 20:44
> eurostat/man/cut_to_classes.Rd
> -rwxr-xr-x lei/lei        1133 2018-06-17 20:44 eurostat/man/dic_order.Rd
> -rwxr-xr-x lei/lei         788 2018-06-17 20:44 eurostat/man/eu_countries.Rd
> -rw-r--r-- lei/lei         826 2018-08-28 18:44
> eurostat/man/eurostat_geodata_60_2016.Rd
> -rwxr-xr-x lei/lei         805 2018-06-17 20:44
> eurostat/man/eurostat-package.Rd
> -rwxr-xr-x lei/lei        1220 2018-06-17 20:44
> eurostat/man/eurotime2date.Rd
> -rwxr-xr-x lei/lei         978 2018-06-17 20:44 eurostat/man/eurotime2num.Rd
> -rwxr-xr-x lei/lei        1169 2018-06-17 20:44
> eurostat/man/get_eurostat_dic.Rd
> -rwxr-xr-x lei/lei        2321 2018-08-28 18:44
> eurostat/man/get_eurostat_geospatial.Rd
> -rwxr-xr-x lei/lei        2675 2018-08-28 18:44
> eurostat/man/get_eurostat_json.Rd
> -rwxr-xr-x lei/lei        1172 2018-06-17 20:44
> eurostat/man/get_eurostat_raw.Rd
> -rwxr-xr-x lei/lei        1191 2018-06-17 20:44
> eurostat/man/get_eurostat_toc.Rd
> -rwxr-xr-x lei/lei        6351 2018-08-28 18:44 eurostat/man/get_eurostat.Rd
> -rwxr-xr-x lei/lei         783 2018-06-17 20:44
> eurostat/man/harmonize_country_code.Rd
> -rwxr-xr-x lei/lei        2518 2018-06-17 20:44
> eurostat/man/label_eurostat.Rd
> -rwxr-xr-x lei/lei        2015 2018-06-17 20:44
> eurostat/man/search_eurostat.Rd
> -rwxr-xr-x lei/lei         453 2018-06-17 20:44
> eurostat/man/set_eurostat_toc.Rd
> -rwxr-xr-x lei/lei         344 2018-06-17 20:44 eurostat/man/tgs00026.Rd
> -rwxr-xr-x lei/lei        1696 2018-06-17 20:44
> eurostat/man/tidy_eurostat.Rd
> -rwxr-xr-x lei/lei        1273 2018-09-20 13:21 eurostat/NAMESPACE
> drwxr-xr-x lei/lei           0 2018-09-20 13:23 eurostat/R/
> -rwxr-xr-x lei/lei        1140 2018-06-17 20:44
> eurostat/R/clean_eurostat_cache.R
> -rwxr-xr-x lei/lei        3191 2018-06-17 20:44 eurostat/R/cut_to_classes.R
> -rwxr-xr-x lei/lei         638 2018-06-17 20:44 eurostat/R/data_countries.R
> -rwxr-xr-x lei/lei        4571 2018-08-28 18:44 eurostat/R/data_spatial.R
> -rwxr-xr-x lei/lei         210 2018-06-17 20:44 eurostat/R/data.R
> -rwxr-xr-x lei/lei        1310 2018-06-17 20:44 eurostat/R/dic_order.R
> -rwxr-xr-x lei/lei         721 2018-06-17 20:44
> eurostat/R/eurostat-package.R
> -rwxr-xr-x lei/lei        2725 2018-06-17 20:44 eurostat/R/eurotime2date.R
> -rwxr-xr-x lei/lei        1918 2018-06-17 20:44 eurostat/R/eurotime2num.R
> -rwxr-xr-x lei/lei         967 2018-07-23 17:01 eurostat/R/firstlib.R
> -rwxr-xr-x lei/lei        1668 2018-08-28 18:44
> eurostat/R/get_eurostat_dic.R
> -rwxr-xr-x lei/lei        9550 2018-08-28 18:44
> eurostat/R/get_eurostat_geospatial.R
> -rwxr-xr-x lei/lei        4986 2018-06-17 20:44
> eurostat/R/get_eurostat_json.R
> -rwxr-xr-x lei/lei        2111 2018-08-28 18:44
> eurostat/R/get_eurostat_raw.R
> -rwxr-xr-x lei/lei        1315 2018-06-17 20:44
> eurostat/R/get_eurostat_toc.R
> -rwxr-xr-x lei/lei        9401 2018-08-28 18:44 eurostat/R/get_eurostat.R
> -rwxr-xr-x lei/lei         816 2018-06-17 20:44
> eurostat/R/harmonize_country_code.R
> -rwxr-xr-x lei/lei        6208 2018-06-17 20:44 eurostat/R/label_eurostat.R
> -rwxr-xr-x lei/lei        2185 2018-06-17 20:44 eurostat/R/search_eurostat.R
> -rwxr-xr-x lei/lei         752 2018-08-28 18:44
> eurostat/R/set_eurostat_toc.R
> -rwxr-xr-x lei/lei        5090 2018-06-17 20:44 eurostat/R/tidy_eurostat.R
> -rwxr-xr-x lei/lei         433 2018-06-17 20:44 eurostat/R/utils.R
> -rw-r--r-- lei/lei         311 2018-08-28 18:44 eurostat/R/zzz.R
> -rwxr-xr-x lei/lei        5342 2018-09-20 01:48 eurostat/README.md
> drwxr-xr-x lei/lei           0 2018-06-17 20:44 eurostat/tests/
> drwxr-xr-x lei/lei           0 2018-06-17 20:44 eurostat/tests/testthat/
> -rwxr-xr-x lei/lei          60 2018-06-17 20:44 eurostat/tests/testthat.R
> -rwxr-xr-x lei/lei        5021 2018-06-17 20:44
> eurostat/tests/testthat/test-all.R
> drwxr-xr-x lei/lei           0 2018-09-20 13:23 eurostat/vignettes/
> drwxr-xr-x lei/lei           0 2018-09-20 13:23
> eurostat/vignettes/2017_RJournal_manuscript/
> -rwxr-xr-x lei/lei      677086 2018-06-17 20:44
> eurostat/vignettes/2017_RJournal_manuscript/2017_RJournal_manuscript.zip
> -rwxr-xr-x lei/lei      397875 2018-06-17 20:44
> eurostat/vignettes/2017_RJournal_manuscript/Proofs.zip
> -rwxr-xr-x lei/lei         620 2018-06-17 20:44
> eurostat/vignettes/2017_RJournal_manuscript/README.md
> -rwxr-xr-x lei/lei         518 2018-06-17 20:44
> eurostat/vignettes/blogposts.Rmd
> drwxr-xr-x lei/lei           0 2018-06-17 20:44
> eurostat/vignettes/cheatsheet/
> -rwxr-xr-x lei/lei         610 2018-06-17 20:44
> eurostat/vignettes/cheatsheet.Rmd
> -rwxr-xr-x lei/lei      790957 2018-06-17 20:44
> eurostat/vignettes/cheatsheet/eurostat_cheatsheet.ai
> -rwxr-xr-x lei/lei      830868 2018-06-17 20:44
> eurostat/vignettes/cheatsheet/eurostat_cheatsheet.pdf
> -rwxr-xr-x lei/lei      482250 2018-09-20 13:23
> eurostat/vignettes/eurostat_tutorial.html
> -rwxr-xr-x lei/lei       43215 2018-09-20 01:48
> eurostat/vignettes/eurostat_tutorial.md
> -rwxr-xr-x lei/lei       17177 2018-09-20 00:07
> eurostat/vignettes/eurostat_tutorial.Rmd
> drwxr-xr-x lei/lei           0 2018-09-20 01:48 eurostat/vignettes/fig/
> -rwxr-xr-x lei/lei       63101 2018-09-20 01:42
> eurostat/vignettes/fig/map1ex-1.pdf
> -rwxr-xr-x lei/lei        2244 2018-09-20 13:23
> eurostat/vignettes/fig/map1ex-1.png
> -rwxr-xr-x lei/lei       10157 2018-06-17 20:44
> eurostat/vignettes/fig/maps1-1-1.pdf
> -rwxr-xr-x lei/lei      217962 2018-07-22 14:49
> eurostat/vignettes/fig/maps1-1-1.png
> -rwxr-xr-x lei/lei       46180 2018-09-20 01:48
> eurostat/vignettes/fig/maps2-1.pdf
> -rwxr-xr-x lei/lei        5716 2018-09-20 13:23
> eurostat/vignettes/fig/maps2-1.png
> -rwxr-xr-x lei/lei      189518 2018-07-22 14:49
> eurostat/vignettes/fig/maps3-1.pdf
> -rwxr-xr-x lei/lei      136509 2018-09-20 13:23
> eurostat/vignettes/fig/maps3-1.png
> -rwxr-xr-x lei/lei      145304 2018-09-20 13:23
> eurostat/vignettes/fig/maps4-1.png
> -rwxr-xr-x lei/lei        7482 2018-09-20 01:48
> eurostat/vignettes/fig/plotGallery-1.pdf
> -rwxr-xr-x lei/lei        9111 2018-09-20 13:22
> eurostat/vignettes/fig/plotGallery-1.png
> -rwxr-xr-x lei/lei        5101 2018-09-20 01:48
> eurostat/vignettes/fig/trains_plot-1.pdf
> -rwxr-xr-x lei/lei        3984 2018-09-20 13:22
> eurostat/vignettes/fig/trains_plot-1.png
> -rwxr-xr-x lei/lei         510 2018-06-17 20:44 eurostat/vignettes/main.R
> -rwxr-xr-x lei/lei         942 2018-06-17 20:44
> eurostat/vignettes/publications.Rmd
>
>
>
>
> On Thu, Sep 20, 2018 at 12:07 PM Duncan Murdoch <murdoch.duncan at gmail.com>
> wrote:
>
> > On 20/09/2018 6:56 AM, Leo Lahti wrote:
> > > Dear developers,
> > >
> > > Upon CRAN submission I have bumped into "future file timestamps" warning
> > > that I can't solve. I have updated the package as usual, and all checks
> > go
> > > through in my system. CRAN reports the following warning however.
> > >
> > > * checking for future file timestanps ... WARNING
> > > Files with future time stamps:
> > >    DESCRIPTION
> > >    NAMESPACE
> > >    README.md
> > >
> > > The build log is at
> > >
> > https://win-builder.r-project.org/incoming_pretest/eurostat_3.2.7_20180920_004954/Debian/00check.log
> > >
> > > The package is at
> > > https://github.com/rOpenGov/eurostat/
> > >
> > >
> > > I did not find guidance (search engines, R extensions manual)
> > > on how to solve this. Perhaps we would need to generate the
> > > files in a certain order but not sure what is the correct order.
> > >
> > > Any tips?
> >
> > What are the time stamps on those files?  You can list the contents of
> > the tarball at the command line using
> >
> > tar ztvf wCorr_1.9.0.tar.gz
> >
> > Are the timestamps correct?
> >
> > Duncan Murdoch
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From r@lf@@tubner @ending from d@q@n@@com  Thu Sep 20 12:59:00 2018
From: r@lf@@tubner @ending from d@q@n@@com (Ralf Stubner)
Date: Thu, 20 Sep 2018 12:59:00 +0200
Subject: [Rd] Bias in R's random integers?
In-Reply-To: <CAN_1p9znLss3QKuF3WB5ucM_F2=paNx28eCdJecB8fj0p_w-6Q@mail.gmail.com>
References: <CAN_1p9wVX=AR8aYtC2LJo0bgN9cRW3XcbYJqnVHdCzA-QA1ceg@mail.gmail.com>
 <ffb74021-8d95-a820-8013-f0e51dc1a9b3@gmail.com>
 <CALEXWq3booRRTSYvYqcjV7qPZGOC1oh5o45b5o=J2ypQi5s-ag@mail.gmail.com>
 <ece52410-ddb2-2198-9291-201475d3f46e@gmail.com>
 <CAA7KnHHEejYVn8NtKrY1KuKcJUq8n4LoJSDx9BZv-LJyC=r9yQ@mail.gmail.com>
 <73c4667d-38ad-f5ef-468c-4c368e0fdc12@gmail.com>
 <CAA7KnHFcY5YJ2kTAfPJJ0PHwcWx7CumE49A4NgJWC2NK5CV=-A@mail.gmail.com>
 <c3590da8-2e92-4e46-3a81-0d0dbfc81845@gmail.com>
 <CAA7KnHE9OSxML3Cp8Y-gd7CbBj-86UP9qAy6yzgvfcb+=Vg86Q@mail.gmail.com>
 <e82cfff0-3a8e-1d7b-de5c-02b69974ae16@gmail.com>
 <4143568b-c67f-2309-f9ec-676f9e791747@gmail.com>
 <CAN_1p9znLss3QKuF3WB5ucM_F2=paNx28eCdJecB8fj0p_w-6Q@mail.gmail.com>
Message-ID: <5245f770-a010-b976-28c8-13a2ec74a088@daqana.com>

On 9/20/18 1:43 AM, Carl Boettiger wrote:
> For a well-tested C algorithm, based on my reading of Lemire, the unbiased
> "algorithm 3" in https://arxiv.org/abs/1805.10941 is part already of the C
> standard library in OpenBSD and macOS (as arc4random_uniform), and in the
> GNU standard library.  Lemire also provides C++ code in the appendix of his
> piece for both this and the faster "nearly divisionless" algorithm.
> 
> It would be excellent if any R core members were interested in considering
> bindings to these algorithms as a patch, or might express expectations for
> how that patch would have to operate (e.g. re Duncan's comment about
> non-integer arguments to sample size).  Otherwise, an R package binding
> seems like a good starting point, but I'm not the right volunteer.
It is difficult to do this in a package, since R does not provide access
to the random bits generated by the RNG. Only a float in (0,1) is
available via unif_rand(). However, if one is willing to use an external
RNG, it is of course possible. After reading about Lemire's work [1], I
had planned to integrate such an unbiased sampling scheme into the dqrng
package, which I have now started. [2]

Using Duncan's example, the results look much better:

> library(dqrng)
> m <- (2/5)*2^32
> y <- dqsample(m, 1000000, replace = TRUE)
> table(y %% 2)

     0      1
500252 499748

Currently I am taking the other interpretation of "truncated":

> table(dqsample(2.5, 1000000, replace = TRUE))

     0      1
499894 500106

I will adjust this to whatever is decided for base R.


However, there is currently neither long vector nor weighted sampling
support. And the performance without replacement is quite bad compared
to R's algorithm with hashing.

cheerio
ralf

[1] via http://www.pcg-random.org/posts/bounded-rands.html
[2] https://github.com/daqana/dqrng/tree/feature/sample

-- 
Ralf Stubner
Senior Software Engineer / Trainer

daqana GmbH
Dortustra?e 48
14467 Potsdam

T: +49 331 23 61 93 11
F: +49 331 23 61 93 90
M: +49 162 20 91 196
Mail: ralf.stubner at daqana.com

Sitz: Potsdam
Register: AG Potsdam HRB 27966 P
Ust.-IdNr.: DE300072622
Gesch?ftsf?hrer: Prof. Dr. Dr. Karl-Kuno Kunze


-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 833 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20180920/cce85e63/attachment.sig>

From emil@bode @ending from d@n@@kn@w@nl  Thu Sep 20 13:13:03 2018
From: emil@bode @ending from d@n@@kn@w@nl (Emil Bode)
Date: Thu, 20 Sep 2018 11:13:03 +0000
Subject: [Rd] future time stamps warning
In-Reply-To: <CABtg=Kk44-iM00JHTjD8ByUuj=VTQRnHe-+hG8OFCbBd5UBcSQ@mail.gmail.com>
References: <CAHm4+KDgui7QFaLNDCGZm7K9q3a3xCAC19rWjfPGZ3EEeON-sA@mail.gmail.com>
 <ee162471-a18f-e01b-7abc-1e5f8b770d4e@gmail.com>
 <CAHm4+KCEhp++HcNGpZkMK7DGed8rNe4H+1NbRv_2A4B3i1Ji5g@mail.gmail.com>
 <CABtg=Kk44-iM00JHTjD8ByUuj=VTQRnHe-+hG8OFCbBd5UBcSQ@mail.gmail.com>
Message-ID: <0D3D3E31-916F-4FDE-85A2-7AEF3A30CE5F@dans.knaw.nl>


?    On Thu, Sep 20, 2018 at 11:46 AM Leo Lahti <leo.lahti at iki.fi> wrote:
    >
    > Time stamps are correct and my system time is correct.

How is your timezone set?
When I look at your github I see as timestamp for DESCRIPTION today, 1:25 PM GMT+2. (and as I'm writing this, it's 1:12 PM GMT+2)
GMT+2 is CEST, if I see your mail-adress I guess you're in Finland, EEST, GMT+3.


    >
    > I am now tried to use Sys.setFileTime() to update time stamps as proposed.
    > This does not help.
    >
    > The windows and debian builds give different reports on the time stamp
    > issue.
    > https://win-builder.r-project.org/incoming_pretest/eurostat_3.2.8_20180920_122655/Windows/00check.log
    > https://win-builder.r-project.org/incoming_pretest/eurostat_3.2.8_20180920_122655/Debian/00check.log
    >
    > I attached the time stamp listing below.
    
    Well, maybe it is an inaccurate system clock on your machine, or on
    CRAN, but nevertheless,
    for this submission, you can just set the time stamps to a time in the past.
    
    Gabor
    
    > Leo
    >
    >
    >
    > lei at kone:~/Rpackages/eurostat/inst/extras$ tar zvtf eurostat_3.2.8.tar.gz
    > drwxr-xr-x lei/lei           0 2018-09-20 13:23 eurostat/build/
    > -rw-r--r-- lei/lei         301 2018-09-20 13:23 eurostat/build/vignette.rds
    > drwxr-xr-x lei/lei           0 2018-09-20 13:23 eurostat/data/
    > -rw-r--r-- lei/lei          98 2018-09-20 13:23 eurostat/data/datalist
    > -rwxr-xr-x lei/lei         340 2018-06-17 20:44
    > eurostat/data/ea_countries.rda
    > -rwxr-xr-x lei/lei         214 2018-06-17 20:44
    > eurostat/data/efta_countries.rda
    > -rwxr-xr-x lei/lei         252 2018-06-17 20:44
    > eurostat/data/eu_candidate_countries.rda
    > -rwxr-xr-x lei/lei         431 2018-06-17 20:44
    > eurostat/data/eu_countries.rda
    > -rw-r--r-- lei/lei     3651661 2018-08-28 18:44
    > eurostat/data/eurostat_geodata_60_2016.rda
    > -rwxr-xr-x lei/lei        5596 2018-06-17 20:44 eurostat/data/tgs00026.rda
    > -rwxr-xr-x lei/lei        1447 2018-09-20 13:23 eurostat/DESCRIPTION
    > drwxr-xr-x lei/lei           0 2018-09-20 13:23 eurostat/inst/
    > -rwxr-xr-x lei/lei        1060 2018-06-17 20:44 eurostat/inst/CITATION
    > drwxr-xr-x lei/lei           0 2018-09-20 13:23 eurostat/inst/doc/
    > -rw-r--r-- lei/lei        6361 2018-09-20 13:22
    > eurostat/inst/doc/blogposts.html
    > -rwxr-xr-x lei/lei         518 2018-06-17 20:44
    > eurostat/inst/doc/blogposts.Rmd
    > -rw-r--r-- lei/lei        6455 2018-09-20 13:22
    > eurostat/inst/doc/cheatsheet.html
    > -rwxr-xr-x lei/lei         610 2018-06-17 20:44
    > eurostat/inst/doc/cheatsheet.Rmd
    > -rw-r--r-- lei/lei      246912 2018-09-20 01:43
    > eurostat/inst/doc/eurostat_tutorial.pdf
    > -rw-r--r-- lei/lei       10565 2018-09-20 13:23
    > eurostat/inst/doc/eurostat_tutorial.R
    > -rwxr-xr-x lei/lei       17177 2018-09-20 00:07
    > eurostat/inst/doc/eurostat_tutorial.Rmd
    > -rw-r--r-- lei/lei        6834 2018-09-20 13:23
    > eurostat/inst/doc/publications.html
    > -rwxr-xr-x lei/lei         942 2018-06-17 20:44
    > eurostat/inst/doc/publications.Rmd
    > -rwxr-xr-x lei/lei          92 2018-06-17 20:44 eurostat/LICENSE
    > drwxr-xr-x lei/lei           0 2018-08-28 18:44 eurostat/man/
    > -rwxr-xr-x lei/lei         697 2018-06-17 20:44
    > eurostat/man/clean_eurostat_cache.Rd
    > -rwxr-xr-x lei/lei         395 2018-06-17 20:44
    > eurostat/man/convert_time_col.Rd
    > -rwxr-xr-x lei/lei        1276 2018-06-17 20:44
    > eurostat/man/cut_to_classes.Rd
    > -rwxr-xr-x lei/lei        1133 2018-06-17 20:44 eurostat/man/dic_order.Rd
    > -rwxr-xr-x lei/lei         788 2018-06-17 20:44 eurostat/man/eu_countries.Rd
    > -rw-r--r-- lei/lei         826 2018-08-28 18:44
    > eurostat/man/eurostat_geodata_60_2016.Rd
    > -rwxr-xr-x lei/lei         805 2018-06-17 20:44
    > eurostat/man/eurostat-package.Rd
    > -rwxr-xr-x lei/lei        1220 2018-06-17 20:44
    > eurostat/man/eurotime2date.Rd
    > -rwxr-xr-x lei/lei         978 2018-06-17 20:44 eurostat/man/eurotime2num.Rd
    > -rwxr-xr-x lei/lei        1169 2018-06-17 20:44
    > eurostat/man/get_eurostat_dic.Rd
    > -rwxr-xr-x lei/lei        2321 2018-08-28 18:44
    > eurostat/man/get_eurostat_geospatial.Rd
    > -rwxr-xr-x lei/lei        2675 2018-08-28 18:44
    > eurostat/man/get_eurostat_json.Rd
    > -rwxr-xr-x lei/lei        1172 2018-06-17 20:44
    > eurostat/man/get_eurostat_raw.Rd
    > -rwxr-xr-x lei/lei        1191 2018-06-17 20:44
    > eurostat/man/get_eurostat_toc.Rd
    > -rwxr-xr-x lei/lei        6351 2018-08-28 18:44 eurostat/man/get_eurostat.Rd
    > -rwxr-xr-x lei/lei         783 2018-06-17 20:44
    > eurostat/man/harmonize_country_code.Rd
    > -rwxr-xr-x lei/lei        2518 2018-06-17 20:44
    > eurostat/man/label_eurostat.Rd
    > -rwxr-xr-x lei/lei        2015 2018-06-17 20:44
    > eurostat/man/search_eurostat.Rd
    > -rwxr-xr-x lei/lei         453 2018-06-17 20:44
    > eurostat/man/set_eurostat_toc.Rd
    > -rwxr-xr-x lei/lei         344 2018-06-17 20:44 eurostat/man/tgs00026.Rd
    > -rwxr-xr-x lei/lei        1696 2018-06-17 20:44
    > eurostat/man/tidy_eurostat.Rd
    > -rwxr-xr-x lei/lei        1273 2018-09-20 13:21 eurostat/NAMESPACE
    > drwxr-xr-x lei/lei           0 2018-09-20 13:23 eurostat/R/
    > -rwxr-xr-x lei/lei        1140 2018-06-17 20:44
    > eurostat/R/clean_eurostat_cache.R
    > -rwxr-xr-x lei/lei        3191 2018-06-17 20:44 eurostat/R/cut_to_classes.R
    > -rwxr-xr-x lei/lei         638 2018-06-17 20:44 eurostat/R/data_countries.R
    > -rwxr-xr-x lei/lei        4571 2018-08-28 18:44 eurostat/R/data_spatial.R
    > -rwxr-xr-x lei/lei         210 2018-06-17 20:44 eurostat/R/data.R
    > -rwxr-xr-x lei/lei        1310 2018-06-17 20:44 eurostat/R/dic_order.R
    > -rwxr-xr-x lei/lei         721 2018-06-17 20:44
    > eurostat/R/eurostat-package.R
    > -rwxr-xr-x lei/lei        2725 2018-06-17 20:44 eurostat/R/eurotime2date.R
    > -rwxr-xr-x lei/lei        1918 2018-06-17 20:44 eurostat/R/eurotime2num.R
    > -rwxr-xr-x lei/lei         967 2018-07-23 17:01 eurostat/R/firstlib.R
    > -rwxr-xr-x lei/lei        1668 2018-08-28 18:44
    > eurostat/R/get_eurostat_dic.R
    > -rwxr-xr-x lei/lei        9550 2018-08-28 18:44
    > eurostat/R/get_eurostat_geospatial.R
    > -rwxr-xr-x lei/lei        4986 2018-06-17 20:44
    > eurostat/R/get_eurostat_json.R
    > -rwxr-xr-x lei/lei        2111 2018-08-28 18:44
    > eurostat/R/get_eurostat_raw.R
    > -rwxr-xr-x lei/lei        1315 2018-06-17 20:44
    > eurostat/R/get_eurostat_toc.R
    > -rwxr-xr-x lei/lei        9401 2018-08-28 18:44 eurostat/R/get_eurostat.R
    > -rwxr-xr-x lei/lei         816 2018-06-17 20:44
    > eurostat/R/harmonize_country_code.R
    > -rwxr-xr-x lei/lei        6208 2018-06-17 20:44 eurostat/R/label_eurostat.R
    > -rwxr-xr-x lei/lei        2185 2018-06-17 20:44 eurostat/R/search_eurostat.R
    > -rwxr-xr-x lei/lei         752 2018-08-28 18:44
    > eurostat/R/set_eurostat_toc.R
    > -rwxr-xr-x lei/lei        5090 2018-06-17 20:44 eurostat/R/tidy_eurostat.R
    > -rwxr-xr-x lei/lei         433 2018-06-17 20:44 eurostat/R/utils.R
    > -rw-r--r-- lei/lei         311 2018-08-28 18:44 eurostat/R/zzz.R
    > -rwxr-xr-x lei/lei        5342 2018-09-20 01:48 eurostat/README.md
    > drwxr-xr-x lei/lei           0 2018-06-17 20:44 eurostat/tests/
    > drwxr-xr-x lei/lei           0 2018-06-17 20:44 eurostat/tests/testthat/
    > -rwxr-xr-x lei/lei          60 2018-06-17 20:44 eurostat/tests/testthat.R
    > -rwxr-xr-x lei/lei        5021 2018-06-17 20:44
    > eurostat/tests/testthat/test-all.R
    > drwxr-xr-x lei/lei           0 2018-09-20 13:23 eurostat/vignettes/
    > drwxr-xr-x lei/lei           0 2018-09-20 13:23
    > eurostat/vignettes/2017_RJournal_manuscript/
    > -rwxr-xr-x lei/lei      677086 2018-06-17 20:44
    > eurostat/vignettes/2017_RJournal_manuscript/2017_RJournal_manuscript.zip
    > -rwxr-xr-x lei/lei      397875 2018-06-17 20:44
    > eurostat/vignettes/2017_RJournal_manuscript/Proofs.zip
    > -rwxr-xr-x lei/lei         620 2018-06-17 20:44
    > eurostat/vignettes/2017_RJournal_manuscript/README.md
    > -rwxr-xr-x lei/lei         518 2018-06-17 20:44
    > eurostat/vignettes/blogposts.Rmd
    > drwxr-xr-x lei/lei           0 2018-06-17 20:44
    > eurostat/vignettes/cheatsheet/
    > -rwxr-xr-x lei/lei         610 2018-06-17 20:44
    > eurostat/vignettes/cheatsheet.Rmd
    > -rwxr-xr-x lei/lei      790957 2018-06-17 20:44
    > eurostat/vignettes/cheatsheet/eurostat_cheatsheet.ai
    > -rwxr-xr-x lei/lei      830868 2018-06-17 20:44
    > eurostat/vignettes/cheatsheet/eurostat_cheatsheet.pdf
    > -rwxr-xr-x lei/lei      482250 2018-09-20 13:23
    > eurostat/vignettes/eurostat_tutorial.html
    > -rwxr-xr-x lei/lei       43215 2018-09-20 01:48
    > eurostat/vignettes/eurostat_tutorial.md
    > -rwxr-xr-x lei/lei       17177 2018-09-20 00:07
    > eurostat/vignettes/eurostat_tutorial.Rmd
    > drwxr-xr-x lei/lei           0 2018-09-20 01:48 eurostat/vignettes/fig/
    > -rwxr-xr-x lei/lei       63101 2018-09-20 01:42
    > eurostat/vignettes/fig/map1ex-1.pdf
    > -rwxr-xr-x lei/lei        2244 2018-09-20 13:23
    > eurostat/vignettes/fig/map1ex-1.png
    > -rwxr-xr-x lei/lei       10157 2018-06-17 20:44
    > eurostat/vignettes/fig/maps1-1-1.pdf
    > -rwxr-xr-x lei/lei      217962 2018-07-22 14:49
    > eurostat/vignettes/fig/maps1-1-1.png
    > -rwxr-xr-x lei/lei       46180 2018-09-20 01:48
    > eurostat/vignettes/fig/maps2-1.pdf
    > -rwxr-xr-x lei/lei        5716 2018-09-20 13:23
    > eurostat/vignettes/fig/maps2-1.png
    > -rwxr-xr-x lei/lei      189518 2018-07-22 14:49
    > eurostat/vignettes/fig/maps3-1.pdf
    > -rwxr-xr-x lei/lei      136509 2018-09-20 13:23
    > eurostat/vignettes/fig/maps3-1.png
    > -rwxr-xr-x lei/lei      145304 2018-09-20 13:23
    > eurostat/vignettes/fig/maps4-1.png
    > -rwxr-xr-x lei/lei        7482 2018-09-20 01:48
    > eurostat/vignettes/fig/plotGallery-1.pdf
    > -rwxr-xr-x lei/lei        9111 2018-09-20 13:22
    > eurostat/vignettes/fig/plotGallery-1.png
    > -rwxr-xr-x lei/lei        5101 2018-09-20 01:48
    > eurostat/vignettes/fig/trains_plot-1.pdf
    > -rwxr-xr-x lei/lei        3984 2018-09-20 13:22
    > eurostat/vignettes/fig/trains_plot-1.png
    > -rwxr-xr-x lei/lei         510 2018-06-17 20:44 eurostat/vignettes/main.R
    > -rwxr-xr-x lei/lei         942 2018-06-17 20:44
    > eurostat/vignettes/publications.Rmd
    >
    >
    >
    >
    > On Thu, Sep 20, 2018 at 12:07 PM Duncan Murdoch <murdoch.duncan at gmail.com>
    > wrote:
    >
    > > On 20/09/2018 6:56 AM, Leo Lahti wrote:
    > > > Dear developers,
    > > >
    > > > Upon CRAN submission I have bumped into "future file timestamps" warning
    > > > that I can't solve. I have updated the package as usual, and all checks
    > > go
    > > > through in my system. CRAN reports the following warning however.
    > > >
    > > > * checking for future file timestanps ... WARNING
    > > > Files with future time stamps:
    > > >    DESCRIPTION
    > > >    NAMESPACE
    > > >    README.md
    > > >
    > > > The build log is at
    > > >
    > > https://win-builder.r-project.org/incoming_pretest/eurostat_3.2.7_20180920_004954/Debian/00check.log
    > > >
    > > > The package is at
    > > > https://github.com/rOpenGov/eurostat/
    > > >
    > > >
    > > > I did not find guidance (search engines, R extensions manual)
    > > > on how to solve this. Perhaps we would need to generate the
    > > > files in a certain order but not sure what is the correct order.
    > > >
    > > > Any tips?
    > >
    > > What are the time stamps on those files?  You can list the contents of
    > > the tarball at the command line using
    > >
    > > tar ztvf wCorr_1.9.0.tar.gz
    > >
    > > Are the timestamps correct?
    > >
    > > Duncan Murdoch
    > >
    >
    >         [[alternative HTML version deleted]]
    >
    > ______________________________________________
    > R-devel at r-project.org mailing list
    > https://stat.ethz.ch/mailman/listinfo/r-devel
    
    ______________________________________________
    R-devel at r-project.org mailing list
    https://stat.ethz.ch/mailman/listinfo/r-devel
    


From j@ri@ok@@nen @ending from oulu@fi  Thu Sep 20 13:24:09 2018
From: j@ri@ok@@nen @ending from oulu@fi (Jari Oksanen)
Date: Thu, 20 Sep 2018 14:24:09 +0300
Subject: [Rd] future time stamps warning
In-Reply-To: <CABtg=Kk44-iM00JHTjD8ByUuj=VTQRnHe-+hG8OFCbBd5UBcSQ@mail.gmail.com>
References: <CAHm4+KDgui7QFaLNDCGZm7K9q3a3xCAC19rWjfPGZ3EEeON-sA@mail.gmail.com>
 <ee162471-a18f-e01b-7abc-1e5f8b770d4e@gmail.com>
 <CAHm4+KCEhp++HcNGpZkMK7DGed8rNe4H+1NbRv_2A4B3i1Ji5g@mail.gmail.com>
 <CABtg=Kk44-iM00JHTjD8ByUuj=VTQRnHe-+hG8OFCbBd5UBcSQ@mail.gmail.com>
Message-ID: <88a4cd4b-ff42-cd46-971b-c442a2af75e1@oulu.fi>

Could this be a timezone issue (setting the timezone in local computer 
and communicating this to CRAN): when I look at the email in my computer 
I see:

> On Thu, Sep 20, 2018 at 11:46 AM Leo Lahti <leo.lahti at iki.fi> wrote:
>> -rwxr-xr-x lei/lei        1447 2018-09-20 13:23 eurostat/DESCRIPTION

Which seems to claim that eurostats/DESCRIPTION was nearly three hours 
younger than the email. This clearly was in the future back then.

If so, waiting a couple of hours before submission could help, and there 
should be an optimal solution, too (i.e., CRAN and you communicate the 
timezone or both use the same like UTC).

Cheers, Jari Oksanen


From r@dford @ending from c@@toronto@edu  Thu Sep 20 17:01:33 2018
From: r@dford @ending from c@@toronto@edu (Radford Neal)
Date: Thu, 20 Sep 2018 11:01:33 -0400
Subject: [Rd] Bias in R's random integers?
In-Reply-To: <mailman.47730.5.1537438086.4546.r-devel@r-project.org>
References: <mailman.47730.5.1537438086.4546.r-devel@r-project.org>
Message-ID: <20180920150133.GA29697@mail.cs.toronto.edu>

> From: Duncan Murdoch <murdoch.duncan at gmail.com>

> Let's try it:
> 
>  > m <- (2/5)*2^32
>  > m > 2^31
> [1] FALSE
>  > x <- sample(m, 1000000, replace = TRUE)
>  > table(x %% 2)
> 
>       0      1
> 399850 600150
> 
> Since m is an even number, the true proportions of evens and odds should 
> be exactly 0.5.  That's some pretty strong evidence of the bug in the 
> generator. 


It seems to be a recently-introduced bug.  Here's output with R-2.15.1:

  R version 2.15.1 (2012-06-22) -- "Roasted Marshmallows"
  Copyright (C) 2012 The R Foundation for Statistical Computing
  ISBN 3-900051-07-0
  Platform: x86_64-unknown-linux-gnu (64-bit)
  
  R is free software and comes with ABSOLUTELY NO WARRANTY.
  You are welcome to redistribute it under certain conditions.
  Type 'license()' or 'licence()' for distribution details.
  
  R is a collaborative project with many contributors.
  Type 'contributors()' for more information and
  'citation()' on how to cite R or R packages in publications.
  
  Type 'demo()' for some demos, 'help()' for on-line help, or
  'help.start()' for an HTML browser interface to help.
  Type 'q()' to quit R.
  
  > set.seed(123)
  > m <- (2/5)*2^32
  > m > 2^31
  [1] FALSE
  > x <- sample(m, 1000000, replace = TRUE)
  > table(x %% 2)
  
       0      1 
  499412 500588 
  
So I doubt that this has anything to do with bias from using 32-bit
random values.

   Radford Neal


From @eth@ru@@ell @ending from gm@il@com  Thu Sep 20 17:09:00 2018
From: @eth@ru@@ell @ending from gm@il@com (Seth Russell)
Date: Thu, 20 Sep 2018 09:09:00 -0600
Subject: [Rd] 
 segfault issue with parallel::mclapply and download.file() on
 Mac OS X
In-Reply-To: <CABtg=KktP1KkbGwfwD+Qa4ouCBVwL=snkSqJp85HhiJA927xUw@mail.gmail.com>
References: <CAB75gRUv1LitTgWSbMFunqp7P9wfWG_BJ_xYBPN4K5KJkDu+_g@mail.gmail.com>
 <CABtg=KktP1KkbGwfwD+Qa4ouCBVwL=snkSqJp85HhiJA927xUw@mail.gmail.com>
Message-ID: <CAB75gRW+6+5UX1aAm99jrV90QE8h00q4PE+hJZkqGTwe91n++Q@mail.gmail.com>

Thanks for the warning about fork without exec(). A co-worker of mine, also
on Mac, ran the sample code and got an error about that exact problem.

Thanks also for the pointer to try curl::multi_add() or download.file()
with a vector of files.

My actual use case includes downloading the files and then untar() for
analysis of files contained in the tar.gz file. I'm currently parallelizing
both the download and untar operation and found that using a parallel form
of lapply resulted in 4x - 8x improvement depending on hardware, network
latency, etc. I'll see how much of that improvement can be attributed to
I/O multiplexing for the downloading portion using your recommendations.

Seth

Trimmed reply from G?bor Cs?rdi <csardi.gabor at gmail.com>:

>
> Fork without exec is not supported by macOS, basically any calls to
> system libraries might crash. (Ie. not just HTTP-related calls.) For
> HTTP calls I have seen errors, crashes, and sometimes it works.
> Depends on the combination of libcurl version, macOS version and
> probably luck.
>
> It usually (always?) works on Linux, but I would not rely on that, either.
>
> So, yes, this is a known issue.
>
> Creating new processes to perform HTTP in parallel is very often bad
> practice, actually. Whenever you can, use I/O multiplexing instead,
> since the main R process is not doing anything, anyway, just waiting
> for the data to come in. So you don't need more processes, you need
> parallel I/O. Take a look at the curl::multi_add() etc. functions.
>
> Btw. download.file() can actually download files in parallel if the
> liburl method is used, just give it a list of URLs in a character
> vector. This API is very restricted, though, so I suggest to look at
> the curl package.
>
>
>

	[[alternative HTML version deleted]]


From murdoch@dunc@n @ending from gm@il@com  Thu Sep 20 17:15:04 2018
From: murdoch@dunc@n @ending from gm@il@com (Duncan Murdoch)
Date: Thu, 20 Sep 2018 11:15:04 -0400
Subject: [Rd] Bias in R's random integers?
In-Reply-To: <5245f770-a010-b976-28c8-13a2ec74a088@daqana.com>
References: <CAN_1p9wVX=AR8aYtC2LJo0bgN9cRW3XcbYJqnVHdCzA-QA1ceg@mail.gmail.com>
 <ffb74021-8d95-a820-8013-f0e51dc1a9b3@gmail.com>
 <CALEXWq3booRRTSYvYqcjV7qPZGOC1oh5o45b5o=J2ypQi5s-ag@mail.gmail.com>
 <ece52410-ddb2-2198-9291-201475d3f46e@gmail.com>
 <CAA7KnHHEejYVn8NtKrY1KuKcJUq8n4LoJSDx9BZv-LJyC=r9yQ@mail.gmail.com>
 <73c4667d-38ad-f5ef-468c-4c368e0fdc12@gmail.com>
 <CAA7KnHFcY5YJ2kTAfPJJ0PHwcWx7CumE49A4NgJWC2NK5CV=-A@mail.gmail.com>
 <c3590da8-2e92-4e46-3a81-0d0dbfc81845@gmail.com>
 <CAA7KnHE9OSxML3Cp8Y-gd7CbBj-86UP9qAy6yzgvfcb+=Vg86Q@mail.gmail.com>
 <e82cfff0-3a8e-1d7b-de5c-02b69974ae16@gmail.com>
 <4143568b-c67f-2309-f9ec-676f9e791747@gmail.com>
 <CAN_1p9znLss3QKuF3WB5ucM_F2=paNx28eCdJecB8fj0p_w-6Q@mail.gmail.com>
 <5245f770-a010-b976-28c8-13a2ec74a088@daqana.com>
Message-ID: <a590bfc6-f172-b227-00d9-9eacb126fd63@gmail.com>

On 20/09/2018 6:59 AM, Ralf Stubner wrote:
> On 9/20/18 1:43 AM, Carl Boettiger wrote:
>> For a well-tested C algorithm, based on my reading of Lemire, the unbiased
>> "algorithm 3" in https://arxiv.org/abs/1805.10941 is part already of the C
>> standard library in OpenBSD and macOS (as arc4random_uniform), and in the
>> GNU standard library.  Lemire also provides C++ code in the appendix of his
>> piece for both this and the faster "nearly divisionless" algorithm.
>>
>> It would be excellent if any R core members were interested in considering
>> bindings to these algorithms as a patch, or might express expectations for
>> how that patch would have to operate (e.g. re Duncan's comment about
>> non-integer arguments to sample size).  Otherwise, an R package binding
>> seems like a good starting point, but I'm not the right volunteer.
> It is difficult to do this in a package, since R does not provide access
> to the random bits generated by the RNG. Only a float in (0,1) is
> available via unif_rand(). 

I believe it is safe to multiply the unif_rand() value by 2^32, and take 
the whole number part as an unsigned 32 bit integer.  Depending on the 
RNG in use, that will give at least 25 random bits.  (The low order bits 
are the questionable ones.  25 is just a guess, not a guarantee.)

However, if one is willing to use an external
> RNG, it is of course possible. After reading about Lemire's work [1], I
> had planned to integrate such an unbiased sampling scheme into the dqrng
> package, which I have now started. [2]
> 
> Using Duncan's example, the results look much better:
> 
>> library(dqrng)
>> m <- (2/5)*2^32
>> y <- dqsample(m, 1000000, replace = TRUE)
>> table(y %% 2)
> 
>       0      1
> 500252 499748

Another useful diagnostic is

   plot(density(y[y %% 2 == 0]))

Obviously that should give a more or less uniform density, but for 
values near m, the default sample() gives some nice pretty pictures of 
quite non-uniform densities.

By the way, there are actually quite a few examples of very large m 
besides m = (2/5)*2^32 where performance of sample() is noticeably bad. 
You'll see problems in y %% 2 for any integer a > 1 with m = 2/(1 + 2a) 
* 2^32, problems in y %% 3 for m = 3/(1 + 3a)*2^32 or m = 3/(2 + 
3a)*2^32, etc.

So perhaps I'm starting to be convinced that the default sample() should 
be fixed.

Duncan Murdoch


> 
> Currently I am taking the other interpretation of "truncated":
> 
>> table(dqsample(2.5, 1000000, replace = TRUE))
> 
>       0      1
> 499894 500106
> 
> I will adjust this to whatever is decided for base R.
> 
> 
> However, there is currently neither long vector nor weighted sampling
> support. And the performance without replacement is quite bad compared
> to R's algorithm with hashing.
> 
> cheerio
> ralf
> 
> [1] via http://www.pcg-random.org/posts/bounded-rands.html
> [2] https://github.com/daqana/dqrng/tree/feature/sample
> 
> 
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From m@echler @ending from @t@t@m@th@ethz@ch  Thu Sep 20 17:46:18 2018
From: m@echler @ending from @t@t@m@th@ethz@ch (Martin Maechler)
Date: Thu, 20 Sep 2018 17:46:18 +0200
Subject: [Rd] A different error in sample()
In-Reply-To: <23459.19022.858260.194660@stat.math.ethz.ch>
References: <5b6950ce-ac7a-4f6a-da61-8e70d25bb08c@gmail.com>
 <CAO1zAVbHXAz9o7uf98Q5pUm7R8pSHFigAAtQn90FrGdPfqUn1A@mail.gmail.com>
 <SYCPR01MB41112555F67E1CAD438FCB6BCD1C0@SYCPR01MB4111.ausprd01.prod.outlook.com>
 <90332569-2e20-e645-26de-9c2cd43eaea9@embl.de>
 <a90aa1d8-6f6d-4be1-140a-5f776df36fd0@embl.de>
 <23459.19022.858260.194660@stat.math.ethz.ch>
Message-ID: <23459.49354.466423.813313@stat.math.ethz.ch>

>>>>> Martin Maechler 
>>>>>     on Thu, 20 Sep 2018 09:20:46 +0200 writes:

>>>>> Wolfgang Huber 
>>>>>     on Thu, 20 Sep 2018 08:47:47 +0200 writes:

    >> FWIW, I suspect this is related to the function
    >> R_unif_index that was introduced in src/main/RNG.c around
    >> revision 72356, or the way this function is used in
    >> do_sample in src/main/random.c.

    > Yes, it is just the use of 'dn' instead of 'n'
    > - a one letter thinko I'd say.

    > But *no*, it's much older than revision 72356; e.g., it's already in

    > R version 3.0.0 (2013-04-03) -- "Masked Marvel"

    > but not yet in

    > R version 2.15.3 (2013-03-01) -- "Security Blanket"

    > ----

    > Here, I clearly think we see a regression bug, and hopefully not
    > one that should trigger often in practice...
    > and  -- without any statistics about the consequences out in
    > package space --
    > I do think we should fix this in code and let the documentation
    > become "great again" ;-)

We have agreed that this is simply a regression and should be
fixed without a change to the documenation.

Consequently, ~ 5 minutes ago

$ svn log -v -c75338
------------------------------------------------------------------------
r75338 | maechler | 2018-09-20 17:38:46 +0200 (Thu, 20 Sep 2018) | 1 line
Changed paths:
   M /trunk/doc/NEWS.Rd
   M /trunk/src/main/random.c
   M /trunk/tests/reg-tests-1d.R

revert sample.int(<non-integer>, k, replace=TRUE) to sane pre_R-3.0.0 behaviour
------------------------------------------------------------------------

This is now back to "correct" behaviour  in  "R-devel (>= 75338)"

(and, as Duncan Murdoch also said by choosing this thread's
 Subject, this is really a different issue than the  "Bias in R's....")

Martin


From murdoch@dunc@n @ending from gm@il@com  Thu Sep 20 18:03:02 2018
From: murdoch@dunc@n @ending from gm@il@com (Duncan Murdoch)
Date: Thu, 20 Sep 2018 12:03:02 -0400
Subject: [Rd] Bias in R's random integers?
In-Reply-To: <20180920150133.GA29697@mail.cs.toronto.edu>
References: <mailman.47730.5.1537438086.4546.r-devel@r-project.org>
 <20180920150133.GA29697@mail.cs.toronto.edu>
Message-ID: <ca3bb3d2-79c3-91e8-0548-8d691161b2a1@gmail.com>

On 20/09/2018 11:01 AM, Radford Neal wrote:
>> From: Duncan Murdoch <murdoch.duncan at gmail.com>
> 
>> Let's try it:
>>
>>   > m <- (2/5)*2^32
>>   > m > 2^31
>> [1] FALSE
>>   > x <- sample(m, 1000000, replace = TRUE)
>>   > table(x %% 2)
>>
>>        0      1
>> 399850 600150
>>
>> Since m is an even number, the true proportions of evens and odds should
>> be exactly 0.5.  That's some pretty strong evidence of the bug in the
>> generator.
> 
> 
> It seems to be a recently-introduced bug.  Here's output with R-2.15.1:
> 
>    R version 2.15.1 (2012-06-22) -- "Roasted Marshmallows"
>    Copyright (C) 2012 The R Foundation for Statistical Computing
>    ISBN 3-900051-07-0
>    Platform: x86_64-unknown-linux-gnu (64-bit)
>    
>    R is free software and comes with ABSOLUTELY NO WARRANTY.
>    You are welcome to redistribute it under certain conditions.
>    Type 'license()' or 'licence()' for distribution details.
>    
>    R is a collaborative project with many contributors.
>    Type 'contributors()' for more information and
>    'citation()' on how to cite R or R packages in publications.
>    
>    Type 'demo()' for some demos, 'help()' for on-line help, or
>    'help.start()' for an HTML browser interface to help.
>    Type 'q()' to quit R.
>    
>    > set.seed(123)
>    > m <- (2/5)*2^32
>    > m > 2^31
>    [1] FALSE
>    > x <- sample(m, 1000000, replace = TRUE)
>    > table(x %% 2)
>    
>         0      1
>    499412 500588
>    
> So I doubt that this has anything to do with bias from using 32-bit
> random values.

There are two bugs, one recent one (allowing fractional m to slip 
through) and one old one (the bias).  The test above shows the bias with 
m+1, i.e. in R 2.15.1

 > x <- sample(m + 1, 1000000, replace = TRUE)
 > table(x %% 2)

      0      1
466739 533261


and you can see it in the original m with

   x <- sample(m, 1000000, replace = TRUE)
   plot(density(x[x %% 2 == 0]))

Duncan Murdoch


From pgilbert902 @ending from gm@il@com  Thu Sep 20 18:30:31 2018
From: pgilbert902 @ending from gm@il@com (Paul Gilbert)
Date: Thu, 20 Sep 2018 12:30:31 -0400
Subject: [Rd] Bias in R's random integers?
In-Reply-To: <3d7901f0-6b06-89a9-d841-738d2a8307ec@gmail.com>
References: <CAN_1p9wVX=AR8aYtC2LJo0bgN9cRW3XcbYJqnVHdCzA-QA1ceg@mail.gmail.com>
 <ffb74021-8d95-a820-8013-f0e51dc1a9b3@gmail.com>
 <CAARY7khVi==mZs5o_8fZkTUDDuY-ABwE=gA45COX7qbRgBLhsg@mail.gmail.com>
 <3d7901f0-6b06-89a9-d841-738d2a8307ec@gmail.com>
Message-ID: <01a56b8e-4e9e-9f3c-1ed7-dd458e0a929f@gmail.com>

On 09/19/2018 10:03 AM, Ben Bolker wrote:
...
>     Balancing backward compatibility and correctness is a tough problem
> here.  

I think improvements in the RNG is a situation where backward 
compatibility is not really going to be lost, because people can specify 
the old generator, they just will not get it by default. My opinion is 
that the default needs to generally be the best option available because 
too many people will be expecting that, or not know better, in which 
case that is what they should get.

There are only two small problems that occur to me:

1/ Researchers that want to have reproducible results (all I hope) need 
to be aware the change has happened. In theory they should have recorded 
the RNG they were using, along with the seed (and, BTW, the number of 
nodes if they generate with a parallel generator). If they have not done 
that then they can figure out the RNG from knowing what version of R 
they used. If they haven't recorded that then they can figure it out by 
some experimentation and knowing roughly when they did the research. If 
none of this works then the research probably should be lost.

As an exercise, researchers might also want to experiment with whether 
the new default qualitatively changes their results. That might lead to 
publishable research, so no one should complain.

2/ Package maintainers that have used the default RNG to generate tests 
may need to change their tests to specify the old generator, or modify 
results used for comparisons in the tests. Since package testing is 
usually for code checking rather than statistical results, not using the 
best available generator is not usually an issue.

Most of my own package testing already specifies the generator, lots 
uses "buggy Kinderman-Ramage" because tests were set up a long time ago. 
I will have to change package setRNG which warns when the default 
generator changes. (This warning is intentional because I was bitten 
badly by a small change in the S generator circa 1990.)


> If this goes into base R, what's the best way to do it?  What was
> the protocol for migrating away from the "buggy Kinderman-Ramage"
> generator, back in the day?   (Version 1.7 was sometime between 2001 and
> 2004).

I think there may have been a change in R 0.99 too. At least my notes 
suggest that the code I changed for  R 1.7.0 had worked with the default 
generator from R 0.99 to 1.6.2.

I don't recall the protocol, I think it just happened and was announced 
in the NEWS. (Has this protocol changed?) The ramification for me was 
that I had to go through all of my packages' testing and change the name 
of the explicitly specified RNG to "buggy Kinderman-Ramage".

Perhaps there does need to be a protocol for testing before release. 
When my package setRNG fails then many of my other packages will also 
fail because they depend on it. This is a simple fix but reverse 
dependencies may make it look like lots of things are broken.

Paul Gilbert

>    I couldn't find the exact commit in the GitHub mirror: this is related ...
> 
> https://github.com/wch/r-source/commit/7ad3044639fd1fe093c655e573fd1a67aa7f55f6#diff-dbcad570d4fb9b7005550ff630543b37
> 
> 
> 
> ===
> ?normal.kind? can be ?"Kinderman-Ramage"?, ?"Buggy
>       Kinderman-Ramage"? (not for ?set.seed?), ?"Ahrens-Dieter"?,
>       ?"Box-Muller"?, ?"Inversion"? (the default), or ?"user-supplied"?.
>       (For inversion, see the reference in ?qnorm?.)  The
>       Kinderman-Ramage generator used in versions prior to 1.7.0 (now
>       called ?"Buggy"?) had several approximation errors and should only
>       be used for reproduction of old results.
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From becker@g@be @ending from gene@com  Thu Sep 20 19:02:35 2018
From: becker@g@be @ending from gene@com (Gabe Becker)
Date: Thu, 20 Sep 2018 10:02:35 -0700
Subject: [Rd] Bias in R's random integers?
In-Reply-To: <01a56b8e-4e9e-9f3c-1ed7-dd458e0a929f@gmail.com>
References: <CAN_1p9wVX=AR8aYtC2LJo0bgN9cRW3XcbYJqnVHdCzA-QA1ceg@mail.gmail.com>
 <ffb74021-8d95-a820-8013-f0e51dc1a9b3@gmail.com>
 <CAARY7khVi==mZs5o_8fZkTUDDuY-ABwE=gA45COX7qbRgBLhsg@mail.gmail.com>
 <3d7901f0-6b06-89a9-d841-738d2a8307ec@gmail.com>
 <01a56b8e-4e9e-9f3c-1ed7-dd458e0a929f@gmail.com>
Message-ID: <CAMFmJsnDQ+D=kMKdTypScbPtF=o8G6PTRfWkunkbRYy7c5XPnQ@mail.gmail.com>

Hi all,

On Thu, Sep 20, 2018 at 9:30 AM, Paul Gilbert <pgilbert902 at gmail.com> wrote:
>
>
> There are only two small problems that occur to me:
>
> 1/ Researchers that want to have reproducible results (all I hope) need to
> be aware the change has happened. In theory they should have recorded the
> RNG they were using, along with the seed (and, BTW, the number of nodes if
> they generate with a parallel generator). If they have not done that then
> they can figure out the RNG from knowing what version of R they used. If
> they haven't recorded that then they can figure it out by some
> experimentation and knowing roughly when they did the research. If none of
> this works then the research probably should be lost.
>
> As an exercise, researchers might also want to experiment with whether the
> new default qualitatively changes their results. That might lead to
> publishable research, so no one should complain.
>

I was going to suggest helper/convenience functions for this but looking at
?RNG I see that someone has already put in RNGversion which *sets* the RNG
kind to what was used by default in an older version. I do wonder if there
is still value in a function that would *return* it, e.g. for comparisons.
Perhaps RNGversionstr?

Also, would R-core members be interested in a small patch to sessionInfo()
and print.sessionInfo() which makes it so that the current RNGkind is
captured and displayed (respectively) by the sessionInfo machinery? I can
prepare one if so.

Best,
~G

-- 
Gabriel Becker, Ph.D
Scientist
Bioinformatics and Computational Biology
Genentech Research

	[[alternative HTML version deleted]]


From hp@ge@ @ending from fredhutch@org  Thu Sep 20 19:57:00 2018
From: hp@ge@ @ending from fredhutch@org (=?UTF-8?B?SGVydsOpIFBhZ8Oocw==?=)
Date: Thu, 20 Sep 2018 10:57:00 -0700
Subject: [Rd] Bias in R's random integers?
In-Reply-To: <a590bfc6-f172-b227-00d9-9eacb126fd63@gmail.com>
References: <CAN_1p9wVX=AR8aYtC2LJo0bgN9cRW3XcbYJqnVHdCzA-QA1ceg@mail.gmail.com>
 <ffb74021-8d95-a820-8013-f0e51dc1a9b3@gmail.com>
 <CALEXWq3booRRTSYvYqcjV7qPZGOC1oh5o45b5o=J2ypQi5s-ag@mail.gmail.com>
 <ece52410-ddb2-2198-9291-201475d3f46e@gmail.com>
 <CAA7KnHHEejYVn8NtKrY1KuKcJUq8n4LoJSDx9BZv-LJyC=r9yQ@mail.gmail.com>
 <73c4667d-38ad-f5ef-468c-4c368e0fdc12@gmail.com>
 <CAA7KnHFcY5YJ2kTAfPJJ0PHwcWx7CumE49A4NgJWC2NK5CV=-A@mail.gmail.com>
 <c3590da8-2e92-4e46-3a81-0d0dbfc81845@gmail.com>
 <CAA7KnHE9OSxML3Cp8Y-gd7CbBj-86UP9qAy6yzgvfcb+=Vg86Q@mail.gmail.com>
 <e82cfff0-3a8e-1d7b-de5c-02b69974ae16@gmail.com>
 <4143568b-c67f-2309-f9ec-676f9e791747@gmail.com>
 <CAN_1p9znLss3QKuF3WB5ucM_F2=paNx28eCdJecB8fj0p_w-6Q@mail.gmail.com>
 <5245f770-a010-b976-28c8-13a2ec74a088@daqana.com>
 <a590bfc6-f172-b227-00d9-9eacb126fd63@gmail.com>
Message-ID: <3d999ffd-c1b2-d94f-7b7e-baa2bd289a6f@fredhutch.org>

Hi,

Note that it wouldn't be the first time that sample() changes behavior
in a non-backward compatible way:

   https://stat.ethz.ch/pipermail/r-devel/2012-October/065049.html

Cheers,
H.


On 09/20/2018 08:15 AM, Duncan Murdoch wrote:
> On 20/09/2018 6:59 AM, Ralf Stubner wrote:
>> On 9/20/18 1:43 AM, Carl Boettiger wrote:
>>> For a well-tested C algorithm, based on my reading of Lemire, the 
>>> unbiased
>>> "algorithm 3" in 
>>> https://urldefense.proofpoint.com/v2/url?u=https-3A__arxiv.org_abs_1805.10941&d=DwICAg&c=eRAMFD45gAfqt84VtBcfhQ&r=BK7q3XeAvimeWdGbWY_wJYbW0WYiZvSXAJJKaaPhzWA&m=tVt5ARiRzaOYr7BgOc0nC_hDq80BUkAUKNwcowN5W1k&s=TtofIDsvWasBZGzOl9J0kBQnJMksr2Rg3u1l8CM5-qE&e= 
>>> is part already of the C
>>> standard library in OpenBSD and macOS (as arc4random_uniform), and in 
>>> the
>>> GNU standard library.? Lemire also provides C++ code in the appendix 
>>> of his
>>> piece for both this and the faster "nearly divisionless" algorithm.
>>>
>>> It would be excellent if any R core members were interested in 
>>> considering
>>> bindings to these algorithms as a patch, or might express 
>>> expectations for
>>> how that patch would have to operate (e.g. re Duncan's comment about
>>> non-integer arguments to sample size).? Otherwise, an R package binding
>>> seems like a good starting point, but I'm not the right volunteer.
>> It is difficult to do this in a package, since R does not provide access
>> to the random bits generated by the RNG. Only a float in (0,1) is
>> available via unif_rand(). 
> 
> I believe it is safe to multiply the unif_rand() value by 2^32, and take 
> the whole number part as an unsigned 32 bit integer.? Depending on the 
> RNG in use, that will give at least 25 random bits.? (The low order bits 
> are the questionable ones.? 25 is just a guess, not a guarantee.)
> 
> However, if one is willing to use an external
>> RNG, it is of course possible. After reading about Lemire's work [1], I
>> had planned to integrate such an unbiased sampling scheme into the dqrng
>> package, which I have now started. [2]
>>
>> Using Duncan's example, the results look much better:
>>
>>> library(dqrng)
>>> m <- (2/5)*2^32
>>> y <- dqsample(m, 1000000, replace = TRUE)
>>> table(y %% 2)
>>
>> ????? 0????? 1
>> 500252 499748
> 
> Another useful diagnostic is
> 
>  ? plot(density(y[y %% 2 == 0]))
> 
> Obviously that should give a more or less uniform density, but for 
> values near m, the default sample() gives some nice pretty pictures of 
> quite non-uniform densities.
> 
> By the way, there are actually quite a few examples of very large m 
> besides m = (2/5)*2^32 where performance of sample() is noticeably bad. 
> You'll see problems in y %% 2 for any integer a > 1 with m = 2/(1 + 2a) 
> * 2^32, problems in y %% 3 for m = 3/(1 + 3a)*2^32 or m = 3/(2 + 
> 3a)*2^32, etc.
> 
> So perhaps I'm starting to be convinced that the default sample() should 
> be fixed.
> 
> Duncan Murdoch
> 
> 
>>
>> Currently I am taking the other interpretation of "truncated":
>>
>>> table(dqsample(2.5, 1000000, replace = TRUE))
>>
>> ????? 0????? 1
>> 499894 500106
>>
>> I will adjust this to whatever is decided for base R.
>>
>>
>> However, there is currently neither long vector nor weighted sampling
>> support. And the performance without replacement is quite bad compared
>> to R's algorithm with hashing.
>>
>> cheerio
>> ralf
>>
>> [1] via 
>> https://urldefense.proofpoint.com/v2/url?u=http-3A__www.pcg-2Drandom.org_posts_bounded-2Drands.html&d=DwICAg&c=eRAMFD45gAfqt84VtBcfhQ&r=BK7q3XeAvimeWdGbWY_wJYbW0WYiZvSXAJJKaaPhzWA&m=tVt5ARiRzaOYr7BgOc0nC_hDq80BUkAUKNwcowN5W1k&s=OlX-dzwoOeFlod3Gofa_1TQaZwmjsCH9C9v3lM5Y2rY&e= 
>>
>> [2] 
>> https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_daqana_dqrng_tree_feature_sample&d=DwICAg&c=eRAMFD45gAfqt84VtBcfhQ&r=BK7q3XeAvimeWdGbWY_wJYbW0WYiZvSXAJJKaaPhzWA&m=tVt5ARiRzaOYr7BgOc0nC_hDq80BUkAUKNwcowN5W1k&s=DNaSqRCy89Hvbg1G0SpyEL0kkr9_RqWXi9pTy75V32M&e= 
>>
>>
>>
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Ddevel&d=DwICAg&c=eRAMFD45gAfqt84VtBcfhQ&r=BK7q3XeAvimeWdGbWY_wJYbW0WYiZvSXAJJKaaPhzWA&m=tVt5ARiRzaOYr7BgOc0nC_hDq80BUkAUKNwcowN5W1k&s=WOx4NyeYmWxpDG3tBRQ9-_Y3_7YAlKUKOP6gZLs0BrQ&e= 
>>
>>
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Ddevel&d=DwICAg&c=eRAMFD45gAfqt84VtBcfhQ&r=BK7q3XeAvimeWdGbWY_wJYbW0WYiZvSXAJJKaaPhzWA&m=tVt5ARiRzaOYr7BgOc0nC_hDq80BUkAUKNwcowN5W1k&s=WOx4NyeYmWxpDG3tBRQ9-_Y3_7YAlKUKOP6gZLs0BrQ&e= 
> 

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fredhutch.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From @grubb @ending from redh@t@com  Thu Sep 20 23:09:23 2018
From: @grubb @ending from redh@t@com (Steve Grubb)
Date: Thu, 20 Sep 2018 17:09:23 -0400
Subject: [Rd] Bias in R's random integers?
In-Reply-To: <a590bfc6-f172-b227-00d9-9eacb126fd63@gmail.com>
References: <CAN_1p9wVX=AR8aYtC2LJo0bgN9cRW3XcbYJqnVHdCzA-QA1ceg@mail.gmail.com>
 <5245f770-a010-b976-28c8-13a2ec74a088@daqana.com>
 <a590bfc6-f172-b227-00d9-9eacb126fd63@gmail.com>
Message-ID: <2357485.GhJh1lojO1@x2>

Hello,

On Thursday, September 20, 2018 11:15:04 AM EDT Duncan Murdoch wrote:
> On 20/09/2018 6:59 AM, Ralf Stubner wrote:
> > On 9/20/18 1:43 AM, Carl Boettiger wrote:
> >> For a well-tested C algorithm, based on my reading of Lemire, the
> >> unbiased "algorithm 3" in https://arxiv.org/abs/1805.10941 is part
> >> already of the C standard library in OpenBSD and macOS (as
> >> arc4random_uniform), and in the GNU standard library.  Lemire also
> >> provides C++ code in the appendix of his piece for both this and the
> >> faster "nearly divisionless" algorithm.
> >> 
> >> It would be excellent if any R core members were interested in
> >> considering bindings to these algorithms as a patch, or might express
> >> expectations for how that patch would have to operate (e.g. re Duncan's
> >> comment about non-integer arguments to sample size).  Otherwise, an R
> >> package binding seems like a good starting point, but I'm not the right
> >> volunteer.
> > 
> > It is difficult to do this in a package, since R does not provide access
> > to the random bits generated by the RNG. Only a float in (0,1) is
> > available via unif_rand().
> 
> I believe it is safe to multiply the unif_rand() value by 2^32, and take
> the whole number part as an unsigned 32 bit integer.  Depending on the
> RNG in use, that will give at least 25 random bits.  (The low order bits
> are the questionable ones.  25 is just a guess, not a guarantee.)
> 
> However, if one is willing to use an external
> 
> > RNG, it is of course possible. After reading about Lemire's work [1], I
> > had planned to integrate such an unbiased sampling scheme into the dqrng
> > package, which I have now started. [2]
> > 
> > Using Duncan's example, the results look much better:
> >> library(dqrng)
> >> m <- (2/5)*2^32
> >> y <- dqsample(m, 1000000, replace = TRUE)
> >> table(y %% 2)
> >> 
> >       0      1
> > 
> > 500252 499748
> 
> Another useful diagnostic is
> 
>    plot(density(y[y %% 2 == 0]))
> 
> Obviously that should give a more or less uniform density, but for
> values near m, the default sample() gives some nice pretty pictures of
> quite non-uniform densities.
> 
> By the way, there are actually quite a few examples of very large m
> besides m = (2/5)*2^32 where performance of sample() is noticeably bad.
> You'll see problems in y %% 2 for any integer a > 1 with m = 2/(1 + 2a)
> * 2^32, problems in y %% 3 for m = 3/(1 + 3a)*2^32 or m = 3/(2 +
> 3a)*2^32, etc.
> 
> So perhaps I'm starting to be convinced that the default sample() should
> be fixed.

I find this discussion fascinating. I normally test random numbers in 
different languages every now and again using various methods. One simple 
check that I do is to use Michal Zalewski's method when he studied Strange 
Attractors and Initial TCP/IP Sequence Numbers:

http://lcamtuf.coredump.cx/newtcp/
https://pdfs.semanticscholar.org/
adb7/069984e3fa48505cd5081ec118ccb95529a3.pdf

The technique works by mapping the dynamics of the generated numbers into a 
three-dimensional phase space. This is then plotted in a graph so that you 
can visually see if something odd is going on.

I used   runif(10000, min = 0, max = 65535)  to get a set of numbers. This is 
the resulting plot that was generated from R's numbers using this technique:

http://people.redhat.com/sgrubb/files/r-random.jpg

And for comparison this was generated by collecting the same number of 
samples from the bash shell:

http://people.redhat.com/sgrubb/files/bash-random.jpg

The net result is that it shows some banding in the R generated random 
numbers where bash has uniform random numbers with no discernible pattern.

Best Regards,
-Steve


From @t@rk @ending from @t@t@berkeley@edu  Fri Sep 21 04:58:07 2018
From: @t@rk @ending from @t@t@berkeley@edu (Philip B. Stark)
Date: Thu, 20 Sep 2018 19:58:07 -0700
Subject: [Rd] Bias in R's random integers?
In-Reply-To: <e6898826-464a-50dc-3906-009c37b4819a@gmail.com>
References: <CAN_1p9wVX=AR8aYtC2LJo0bgN9cRW3XcbYJqnVHdCzA-QA1ceg@mail.gmail.com>
 <ffb74021-8d95-a820-8013-f0e51dc1a9b3@gmail.com>
 <CALEXWq3booRRTSYvYqcjV7qPZGOC1oh5o45b5o=J2ypQi5s-ag@mail.gmail.com>
 <ece52410-ddb2-2198-9291-201475d3f46e@gmail.com>
 <CAA7KnHHEejYVn8NtKrY1KuKcJUq8n4LoJSDx9BZv-LJyC=r9yQ@mail.gmail.com>
 <73c4667d-38ad-f5ef-468c-4c368e0fdc12@gmail.com>
 <CAA7KnHFcY5YJ2kTAfPJJ0PHwcWx7CumE49A4NgJWC2NK5CV=-A@mail.gmail.com>
 <c3590da8-2e92-4e46-3a81-0d0dbfc81845@gmail.com>
 <CAA7KnHE9OSxML3Cp8Y-gd7CbBj-86UP9qAy6yzgvfcb+=Vg86Q@mail.gmail.com>
 <e82cfff0-3a8e-1d7b-de5c-02b69974ae16@gmail.com>
 <CAARY7kgM5WLkF7e1jnj1Pn_9WQzeHmuibROz58HDwWDUTuUmeQ@mail.gmail.com>
 <e6898826-464a-50dc-3906-009c37b4819a@gmail.com>
Message-ID: <CAA7KnHGb9gA0Ybui0m2x8108xyKZPmWF+MReSQ866d=9REQ=Aw@mail.gmail.com>

The same issue occurs in walker_ProbSampleReplace() in random.c, lines
386-387:

rU = unif_rand() * n;
k = (int) rU;

Cheers,
Philip

On Wed, Sep 19, 2018 at 3:08 PM Duncan Murdoch <murdoch.duncan at gmail.com>
wrote:

> On 19/09/2018 5:57 PM, David Hugh-Jones wrote:
> >
> > It doesn't seem too hard to come up with plausible ways in which this
> > could give bad results. Suppose I sample rows from a large dataset,
> > maybe for bootstrapping. Suppose the rows are non-randomly ordered, e.g.
> > odd rows are males, even rows are females. Oops! Very non-representative
> > sample, bootstrap p values are garbage.
>
> That would only happen if your dataset was exactly 1717986918 elements
> in size. (And in fact, it will be less extreme than I posted:  I had x
> set to 1717986918.4, as described in another thread.  If you use an
> integer value you need a different pattern; add or subtract an element
> or two and the pattern needed to see a problem changes drastically.)
>
> But if you're sampling from a dataset of that exact size, then you
> should worry about this bug. Don't use sample().  Use the algorithm that
> Carl described.
>
> Duncan Murdoch
>
> >
> > David
> >
> > On Wed, 19 Sep 2018 at 21:20, Duncan Murdoch <murdoch.duncan at gmail.com
> > <mailto:murdoch.duncan at gmail.com>> wrote:
> >
> >     On 19/09/2018 3:52 PM, Philip B. Stark wrote:
> >      > Hi Duncan--
> >      >
> >      >
> >
> >     That is a mathematically true statement, but I suspect it is not very
> >     relevant.  Pseudo-random number generators always have test functions
> >     whose sample averages are quite different from the expectation under
> >     the
> >     true distribution.  Remember Von Neumann's "state of sin" quote.  The
> >     bug in sample() just means it is easier to find such a function than
> it
> >     would otherwise be.
> >
> >     The practical question is whether such a function is likely to arise
> in
> >     practice or not.
> >
> >       > Whether those correspond to commonly used statistics or not, I
> >     have no
> >       > idea.
> >
> >     I am pretty confident that this bug rarely matters.
> >
> >      > Regarding backwards compatibility: as a user, I'd rather the
> default
> >      > sample() do the best possible thing, and take an extra step to use
> >      > something like sample(..., legacy=TRUE) if I want to reproduce
> >     old results.
> >
> >     I suspect there's a good chance the bug I discovered today
> (non-integer
> >     x values not being truncated) will be declared to be a feature, and
> the
> >     documentation will be changed.  Then the rejection sampling approach
> >     would need to be quite a bit more complicated.
> >
> >     I think a documentation warning about the accuracy of sampling
> >     probabilities would also be a sufficient fix here, and would be
> quite a
> >     bit less trouble than changing the default sample().  But as I said
> in
> >     my original post, a contribution of a function without this bug
> >     would be
> >     a nice addition.
> >
> >     Duncan Murdoch
> >
> >      >
> >      > Regards,
> >      > Philip
> >      >
> >      > On Wed, Sep 19, 2018 at 9:50 AM Duncan Murdoch
> >     <murdoch.duncan at gmail.com <mailto:murdoch.duncan at gmail.com>
> >      > <mailto:murdoch.duncan at gmail.com
> >     <mailto:murdoch.duncan at gmail.com>>> wrote:
> >      >
> >      >     On 19/09/2018 12:23 PM, Philip B. Stark wrote:
> >      >      > No, the 2nd call only happens when m > 2**31. Here's the
> code:
> >      >
> >      >     Yes, you're right. Sorry!
> >      >
> >      >     So the ratio really does come close to 2.  However, the
> >     difference in
> >      >     probabilities between outcomes is still at most 2^-32 when m
> >     is less
> >      >     than that cutoff.  That's not feasible to detect; the only
> >     detectable
> >      >     difference would happen if some event was constructed to hold
> an
> >      >     abundance of outcomes with especially low (or especially high)
> >      >     probability.
> >      >
> >      >     As I said in my original post, it's probably not hard to
> >     construct such
> >      >     a thing, but as I've said more recently, it probably wouldn't
> >     happen by
> >      >     chance.  Here's one attempt to do it:
> >      >
> >      >     Call the values from unif_rand() "the unif_rand() outcomes".
> >     Call the
> >      >     values from sample() the sample outcomes.
> >      >
> >      >     It would be easiest to see the error if half of the sample()
> >     outcomes
> >      >     used two unif_rand() outcomes, and half used just one.  That
> >     would mean
> >      >     m should be (2/3) * 2^32, but that's too big and would
> >     trigger the
> >      >     other
> >      >     version.
> >      >
> >      >     So how about half use 2 unif_rands(), and half use 3?  That
> >     means m =
> >      >     (2/5) * 2^32 = 1717986918.  A good guess is that sample()
> >     outcomes
> >      >     would
> >      >     alternate between the two possibilities, so our event could
> >     be even
> >      >     versus odd outcomes.
> >      >
> >      >     Let's try it:
> >      >
> >      >       > m <- (2/5)*2^32
> >      >       > m > 2^31
> >      >     [1] FALSE
> >      >       > x <- sample(m, 1000000, replace = TRUE)
> >      >       > table(x %% 2)
> >      >
> >      >            0      1
> >      >     399850 600150
> >      >
> >      >     Since m is an even number, the true proportions of evens and
> odds
> >      >     should
> >      >     be exactly 0.5.  That's some pretty strong evidence of the
> >     bug in the
> >      >     generator.  (Note that the ratio of the observed
> >     probabilities is about
> >      >     1.5, so I may not be the first person to have done this.)
> >      >
> >      >     I'm still not convinced that there has ever been a simulation
> >     run with
> >      >     detectable bias compared to Monte Carlo error unless it (like
> >     this one)
> >      >     was designed specifically to show the problem.
> >      >
> >      >     Duncan Murdoch
> >      >
> >      >      >
> >      >      > (RNG.c, lines 793ff)
> >      >      >
> >      >      > double R_unif_index(double dn)
> >      >      > {
> >      >      >      double cut = INT_MAX;
> >      >      >
> >      >      >      switch(RNG_kind) {
> >      >      >      case KNUTH_TAOCP:
> >      >      >      case USER_UNIF:
> >      >      >      case KNUTH_TAOCP2:
> >      >      > cut = 33554431.0; /* 2^25 - 1 */
> >      >      > break;
> >      >      >      default:
> >      >      > break;
> >      >      >     }
> >      >      >
> >      >      >      double u = dn > cut ? ru() : unif_rand();
> >      >      >      return floor(dn * u);
> >      >      > }
> >      >      >
> >      >      > On Wed, Sep 19, 2018 at 9:20 AM Duncan Murdoch
> >      >     <murdoch.duncan at gmail.com <mailto:murdoch.duncan at gmail.com>
> >     <mailto:murdoch.duncan at gmail.com <mailto:murdoch.duncan at gmail.com>>
> >      >      > <mailto:murdoch.duncan at gmail.com
> >     <mailto:murdoch.duncan at gmail.com>
> >      >     <mailto:murdoch.duncan at gmail.com
> >     <mailto:murdoch.duncan at gmail.com>>>> wrote:
> >      >      >
> >      >      >     On 19/09/2018 12:09 PM, Philip B. Stark wrote:
> >      >      >      > The 53 bits only encode at most 2^{32} possible
> values,
> >      >     because the
> >      >      >      > source of the float is the output of a 32-bit PRNG
> (the
> >      >     obsolete
> >      >      >     version
> >      >      >      > of MT). 53 bits isn't the relevant number here.
> >      >      >
> >      >      >     No, two calls to unif_rand() are used.  There are two
> >     32 bit
> >      >     values,
> >      >      >     but
> >      >      >     some of the bits are thrown away.
> >      >      >
> >      >      >     Duncan Murdoch
> >      >      >
> >      >      >      >
> >      >      >      > The selection ratios can get close to 2. Computer
> >     scientists
> >      >      >     don't do it
> >      >      >      > the way R does, for a reason.
> >      >      >      >
> >      >      >      > Regards,
> >      >      >      > Philip
> >      >      >      >
> >      >      >      > On Wed, Sep 19, 2018 at 9:05 AM Duncan Murdoch
> >      >      >     <murdoch.duncan at gmail.com
> >     <mailto:murdoch.duncan at gmail.com> <mailto:murdoch.duncan at gmail.com
> >     <mailto:murdoch.duncan at gmail.com>>
> >      >     <mailto:murdoch.duncan at gmail.com
> >     <mailto:murdoch.duncan at gmail.com> <mailto:murdoch.duncan at gmail.com
> >     <mailto:murdoch.duncan at gmail.com>>>
> >      >      >      > <mailto:murdoch.duncan at gmail.com
> >     <mailto:murdoch.duncan at gmail.com>
> >      >     <mailto:murdoch.duncan at gmail.com
> >     <mailto:murdoch.duncan at gmail.com>>
> >      >      >     <mailto:murdoch.duncan at gmail.com
> >     <mailto:murdoch.duncan at gmail.com>
> >      >     <mailto:murdoch.duncan at gmail.com
> >     <mailto:murdoch.duncan at gmail.com>>>>> wrote:
> >      >      >      >
> >      >      >      >     On 19/09/2018 9:09 AM, I?aki Ucar wrote:
> >      >      >      >      > El mi?., 19 sept. 2018 a las 14:43, Duncan
> >     Murdoch
> >      >      >      >      > (<murdoch.duncan at gmail.com
> >     <mailto:murdoch.duncan at gmail.com>
> >      >     <mailto:murdoch.duncan at gmail.com
> >     <mailto:murdoch.duncan at gmail.com>>
> >      >      >     <mailto:murdoch.duncan at gmail.com
> >     <mailto:murdoch.duncan at gmail.com>
> >      >     <mailto:murdoch.duncan at gmail.com
> >     <mailto:murdoch.duncan at gmail.com>>> <mailto:murdoch.duncan at gmail.com
> >     <mailto:murdoch.duncan at gmail.com>
> >      >     <mailto:murdoch.duncan at gmail.com
> >     <mailto:murdoch.duncan at gmail.com>>
> >      >      >     <mailto:murdoch.duncan at gmail.com
> >     <mailto:murdoch.duncan at gmail.com>
> >      >     <mailto:murdoch.duncan at gmail.com
> >     <mailto:murdoch.duncan at gmail.com>>>>>)
> >      >      >      >     escribi?:
> >      >      >      >      >>
> >      >      >      >      >> On 18/09/2018 5:46 PM, Carl Boettiger wrote:
> >      >      >      >      >>> Dear list,
> >      >      >      >      >>>
> >      >      >      >      >>> It looks to me that R samples random
> integers
> >      >     using an
> >      >      >      >     intuitive but biased
> >      >      >      >      >>> algorithm by going from a random number on
> >     [0,1) from
> >      >      >     the PRNG
> >      >      >      >     to a random
> >      >      >      >      >>> integer, e.g.
> >      >      >      >      >>>
> >      >      >      >
> >      >
> https://github.com/wch/r-source/blob/tags/R-3-5-1/src/main/RNG.c#L808
> >      >      >      >      >>>
> >      >      >      >      >>> Many other languages use various rejection
> >     sampling
> >      >      >     approaches
> >      >      >      >     which
> >      >      >      >      >>> provide an unbiased method for sampling,
> >     such as
> >      >     in Go,
> >      >      >     python,
> >      >      >      >     and others
> >      >      >      >      >>> described here:
> >     https://arxiv.org/abs/1805.10941 (I
> >      >      >     believe the
> >      >      >      >     biased
> >      >      >      >      >>> algorithm currently used in R is also
> >     described
> >      >     there).  I'm
> >      >      >      >     not an expert
> >      >      >      >      >>> in this area, but does it make sense for
> >     the R to
> >      >     adopt
> >      >      >     one of
> >      >      >      >     the unbiased
> >      >      >      >      >>> random sample algorithms outlined there
> >     and used
> >      >     in other
> >      >      >      >     languages?  Would
> >      >      >      >      >>> a patch providing such an algorithm be
> >     welcome? What
> >      >      >     concerns
> >      >      >      >     would need to
> >      >      >      >      >>> be addressed first?
> >      >      >      >      >>>
> >      >      >      >      >>> I believe this issue was also raised by
> >     Killie &
> >      >     Philip in
> >      >      >      >      >>>
> >      >      >
> >     http://r.789695.n4.nabble.com/Bug-in-sample-td4729483.html, and
> >      >      >      >     more
> >      >      >      >      >>> recently in
> >      >      >      >      >>>
> >      >      >      >
> >      >      >
> >      >
> >     https://www.stat.berkeley.edu/~stark/Preprints/r-random-issues.pdf
> >     <
> https://www.stat.berkeley.edu/%7Estark/Preprints/r-random-issues.pdf>
> >      >
> >       <
> https://www.stat.berkeley.edu/%7Estark/Preprints/r-random-issues.pdf>
> >      >      >
> >      >
> >       <
> https://www.stat.berkeley.edu/%7Estark/Preprints/r-random-issues.pdf>
> >      >      >      >
> >      >      >
> >      >
> >       <
> https://www.stat.berkeley.edu/%7Estark/Preprints/r-random-issues.pdf>,
> >      >      >      >      >>> pointing to the python implementation for
> >     comparison:
> >      >      >      >      >>>
> >      >      >      >
> >      >      >
> >      >
> >
> https://github.com/statlab/cryptorandom/blob/master/cryptorandom/cryptorandom.py#L265
> >      >      >      >      >>
> >      >      >      >      >> I think the analyses are correct, but I
> >     doubt if a
> >      >     change
> >      >      >     to the
> >      >      >      >     default
> >      >      >      >      >> is likely to be accepted as it would make
> >     it more
> >      >      >     difficult to
> >      >      >      >     reproduce
> >      >      >      >      >> older results.
> >      >      >      >      >>
> >      >      >      >      >> On the other hand, a contribution of a new
> >      >     function like
> >      >      >      >     sample() but
> >      >      >      >      >> not suffering from the bias would be good.
> The
> >      >     normal way to
> >      >      >      >     make such
> >      >      >      >      >> a contribution is in a user contributed
> >     package.
> >      >      >      >      >>
> >      >      >      >      >> By the way, R code illustrating the bias is
> >      >     probably not very
> >      >      >      >     hard to
> >      >      >      >      >> put together.  I believe the bias manifests
> >     itself in
> >      >      >     sample()
> >      >      >      >     producing
> >      >      >      >      >> values with two different probabilities
> >     (instead
> >      >     of all equal
> >      >      >      >      >> probabilities).  Those may differ by as
> much as
> >      >     one part in
> >      >      >      >     2^32.  It's
> >      >      >      >      >
> >      >      >      >      > According to Kellie and Philip, in the
> >     attachment
> >      >     of the
> >      >      >     thread
> >      >      >      >      > referenced by Carl, "The maximum ratio of
> >     selection
> >      >      >     probabilities can
> >      >      >      >      > get as large as 1.5 if n is just below 2^31".
> >      >      >      >
> >      >      >      >     Sorry, I didn't write very well.  I meant to
> >     say that the
> >      >      >     difference in
> >      >      >      >     probabilities would be 2^-32, not that the
> ratio of
> >      >      >     probabilities would
> >      >      >      >     be 1 + 2^-32.
> >      >      >      >
> >      >      >      >     By the way, I don't see the statement giving
> >     the ratio as
> >      >      >     1.5, but
> >      >      >      >     maybe
> >      >      >      >     I was looking in the wrong place.  In Theorem 1
> >     of the
> >      >     paper
> >      >      >     I was
> >      >      >      >     looking in the ratio was "1 + m 2^{-w + 1}".
> >     In that
> >      >     formula
> >      >      >     m is your
> >      >      >      >     n.  If it is near 2^31, R uses w = 57 random
> >     bits, so
> >      >     the ratio
> >      >      >      >     would be
> >      >      >      >     very, very small (one part in 2^25).
> >      >      >      >
> >      >      >      >     The worst case for R would happen when m  is
> >     just below
> >      >      >     2^25, where w
> >      >      >      >     is at least 31 for the default generators.  In
> that
> >      >     case the
> >      >      >     ratio
> >      >      >      >     could
> >      >      >      >     be about 1.03.
> >      >      >      >
> >      >      >      >     Duncan Murdoch
> >      >      >      >
> >      >      >      >
> >      >      >      >
> >      >      >      > --
> >      >      >      > Philip B. Stark | Associate Dean, Mathematical and
> >     Physical
> >      >      >     Sciences |
> >      >      >      > Professor,  Department of Statistics |
> >      >      >      > University of California
> >      >      >      > Berkeley, CA 94720-3860 | 510-394-5077 |
> >      >      > statistics.berkeley.edu/~stark
> >     <http://statistics.berkeley.edu/%7Estark>
> >      >     <http://statistics.berkeley.edu/%7Estark>
> >      >      >     <http://statistics.berkeley.edu/%7Estark>
> >      >      >      > <http://statistics.berkeley.edu/%7Estark> |
> >      >      >      > @philipbstark
> >      >      >      >
> >      >      >
> >      >      >
> >      >      >
> >      >      > --
> >      >      > Philip B. Stark | Associate Dean, Mathematical and Physical
> >      >     Sciences |
> >      >      > Professor,  Department of Statistics |
> >      >      > University of California
> >      >      > Berkeley, CA 94720-3860 | 510-394-5077 |
> >      > statistics.berkeley.edu/~stark
> >     <http://statistics.berkeley.edu/%7Estark>
> >      >     <http://statistics.berkeley.edu/%7Estark>
> >      >      > <http://statistics.berkeley.edu/%7Estark> |
> >      >      > @philipbstark
> >      >      >
> >      >
> >      >
> >      >
> >      > --
> >      > Philip B. Stark | Associate Dean, Mathematical and Physical
> >     Sciences |
> >      > Professor,  Department of Statistics |
> >      > University of California
> >      > Berkeley, CA 94720-3860 | 510-394-5077 |
> >     statistics.berkeley.edu/~stark
> >     <http://statistics.berkeley.edu/%7Estark>
> >      > <http://statistics.berkeley.edu/%7Estark> |
> >      > @philipbstark
> >      >
> >
> >     ______________________________________________
> >     R-devel at r-project.org <mailto:R-devel at r-project.org> mailing list
> >     https://stat.ethz.ch/mailman/listinfo/r-devel
> >
> > --
> > Sent from Gmail Mobile
>
>

-- 
Philip B. Stark | Associate Dean, Mathematical and Physical Sciences |
Professor,  Department of Statistics |
University of California
Berkeley, CA 94720-3860 | 510-394-5077 | statistics.berkeley.edu/~stark |
@philipbstark

	[[alternative HTML version deleted]]


From r@lf@@tubner @ending from d@q@n@@com  Fri Sep 21 15:15:44 2018
From: r@lf@@tubner @ending from d@q@n@@com (Ralf Stubner)
Date: Fri, 21 Sep 2018 15:15:44 +0200
Subject: [Rd] Bias in R's random integers?
In-Reply-To: <a590bfc6-f172-b227-00d9-9eacb126fd63@gmail.com>
References: <CAN_1p9wVX=AR8aYtC2LJo0bgN9cRW3XcbYJqnVHdCzA-QA1ceg@mail.gmail.com>
 <ffb74021-8d95-a820-8013-f0e51dc1a9b3@gmail.com>
 <CALEXWq3booRRTSYvYqcjV7qPZGOC1oh5o45b5o=J2ypQi5s-ag@mail.gmail.com>
 <ece52410-ddb2-2198-9291-201475d3f46e@gmail.com>
 <CAA7KnHHEejYVn8NtKrY1KuKcJUq8n4LoJSDx9BZv-LJyC=r9yQ@mail.gmail.com>
 <73c4667d-38ad-f5ef-468c-4c368e0fdc12@gmail.com>
 <CAA7KnHFcY5YJ2kTAfPJJ0PHwcWx7CumE49A4NgJWC2NK5CV=-A@mail.gmail.com>
 <c3590da8-2e92-4e46-3a81-0d0dbfc81845@gmail.com>
 <CAA7KnHE9OSxML3Cp8Y-gd7CbBj-86UP9qAy6yzgvfcb+=Vg86Q@mail.gmail.com>
 <e82cfff0-3a8e-1d7b-de5c-02b69974ae16@gmail.com>
 <4143568b-c67f-2309-f9ec-676f9e791747@gmail.com>
 <CAN_1p9znLss3QKuF3WB5ucM_F2=paNx28eCdJecB8fj0p_w-6Q@mail.gmail.com>
 <5245f770-a010-b976-28c8-13a2ec74a088@daqana.com>
 <a590bfc6-f172-b227-00d9-9eacb126fd63@gmail.com>
Message-ID: <05cffd00-d64d-3fd3-78b6-fae862946c26@daqana.com>

On 9/20/18 5:15 PM, Duncan Murdoch wrote:
> On 20/09/2018 6:59 AM, Ralf Stubner wrote:
>> It is difficult to do this in a package, since R does not provide access
>> to the random bits generated by the RNG. Only a float in (0,1) is
>> available via unif_rand(). 
> 
> I believe it is safe to multiply the unif_rand() value by 2^32, and take
> the whole number part as an unsigned 32 bit integer.? Depending on the
> RNG in use, that will give at least 25 random bits.? (The low order bits
> are the questionable ones.? 25 is just a guess, not a guarantee.)

Right, the RNGs in R produce no more than 32bits, so the conversion to a
double can be reverted. If we ignore those RNGs that produce less than
32bits for the moment, then the attached file contains a sample
implementation (without long vectors, weighted sampling or hashing). It
uses Rcpp for convenience, but I have tried to keep the C++ low.
Interesting results:

The results for "simple" sampling are the same.

> set.seed(42)

> sample.int(6, 10, replace = TRUE)
 [1] 6 6 2 5 4 4 5 1 4 5

> sample.int(100, 10)
 [1] 46 72 92 25 45 90 98 11 44 51

> set.seed(42)

> sample_int(6, 10, replace = TRUE)
 [1] 6 6 2 5 4 4 5 1 4 5

> sample_int(100, 10)
 [1] 46 72 92 25 45 90 98 11 44 51


But there is no bias with the alternative method:

> m <- ceiling((2/5)*2^32)

> set.seed(42)

> x <- sample.int(m, 1000000, replace = TRUE)

> table(x %% 2)

     0      1
467768 532232

> set.seed(42)

> y <- sample_int(m, 1000000, replace = TRUE)

> table(y %% 2)

     0      1
500586 499414


The differences are also visible when sampling only a few values from
'm' possible values:

> set.seed(42)

> sample.int(m, 6, replace = TRUE)
[1] 1571624817 1609883303  491583978 1426698159 1102510407  891800051

> set.seed(42)

> sample_int(m, 6, replace = TRUE)
[1]  491583978 1426698159 1102510407  891800051 1265449090  231355453


When sampling from 'm', performance is not so good since we often have
to get a second random number:

> bench::mark(orig = sample.int(m, 1000000, replace = TRUE),
+             new  = sample_int(m, 1000000, replace = TRUE),
+             check = FALSE)
# A tibble: 2 x 14
  expression     min    mean  median   max `itr/sec` mem_alloc  n_gc n_itr
  <chr>      <bch:t> <bch:t> <bch:t> <bch>     <dbl> <bch:byt> <dbl> <int>
1 orig        8.15ms  8.67ms  8.43ms  10ms     115.     3.82MB     4    52
2 new        25.21ms 25.58ms 25.45ms  27ms      39.1    3.82MB     2    18
# ... with 5 more variables: total_time <bch:tm>, result <list>, memory
<list>,
#   time <list>, gc <list>


When sampling from fewer values, the difference is much less pronounced:

> bench::mark(orig = sample.int(6, 1000000, replace = TRUE),
+             new  = sample_int(6, 1000000, replace = TRUE),
+             check = FALSE)
# A tibble: 2 x 14
  expression     min    mean  median     max `itr/sec` mem_alloc  n_gc n_itr
  <chr>      <bch:t> <bch:t> <bch:t> <bch:t>     <dbl> <bch:byt> <dbl> <int>
1 orig        8.14ms  8.44ms  8.29ms  9.58ms     118.     3.82MB     4    54
2 new        11.13ms 11.66ms 11.23ms 12.98ms      85.8    3.82MB     3    39
# ... with 5 more variables: total_time <bch:tm>, result <list>, memory
<list>,
#   time <list>, gc <list>


> Another useful diagnostic is
> 
> ? plot(density(y[y %% 2 == 0]))
> 
> Obviously that should give a more or less uniform density, but for
> values near m, the default sample() gives some nice pretty pictures of
> quite non-uniform densities.

Indeed. Adding/subtracting numbers < 10 to/from 'm'  gives "interesting"
curves.

> By the way, there are actually quite a few examples of very large m
> besides m = (2/5)*2^32 where performance of sample() is noticeably bad.
> You'll see problems in y %% 2 for any integer a > 1 with m = 2/(1 + 2a)
> * 2^32, problems in y %% 3 for m = 3/(1 + 3a)*2^32 or m = 3/(2 +
> 3a)*2^32, etc.
> 
> So perhaps I'm starting to be convinced that the default sample() should
> be fixed.

I have the impression that Lemire's method gives the same results unless
it is correcting for the bias that exists in the current method. If that
is really the case, then the disruption should be rather minor. The
ability to fall back to the old behavior would still be useful, though.

cheerio
ralf

-- 
Ralf Stubner
Senior Software Engineer / Trainer

daqana GmbH
Dortustra?e 48
14467 Potsdam

T: +49 331 23 61 93 11
F: +49 331 23 61 93 90
M: +49 162 20 91 196
Mail: ralf.stubner at daqana.com

Sitz: Potsdam
Register: AG Potsdam HRB 27966 P
Ust.-IdNr.: DE300072622
Gesch?ftsf?hrer: Prof. Dr. Dr. Karl-Kuno Kunze

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 833 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20180921/10d5df07/attachment.sig>

From @grubb @ending from redh@t@com  Fri Sep 21 15:50:30 2018
From: @grubb @ending from redh@t@com (Steve Grubb)
Date: Fri, 21 Sep 2018 09:50:30 -0400
Subject: [Rd] Bias in R's random integers?
In-Reply-To: <2357485.GhJh1lojO1@x2>
References: <CAN_1p9wVX=AR8aYtC2LJo0bgN9cRW3XcbYJqnVHdCzA-QA1ceg@mail.gmail.com>
 <a590bfc6-f172-b227-00d9-9eacb126fd63@gmail.com> <2357485.GhJh1lojO1@x2>
Message-ID: <5268718.ryNPvsxLyr@x2>

Hello,

Top posting. Several people have asked about the code to replicate my 
results. I have cleaned up the code to remove an x/y coordinate bias for 
displaying the results directly on a 640 x 480 VGA adapter. You can find the 
code here:

http://people.redhat.com/sgrubb/files/vseq.c

To collect R samples:
X <- runif(10000, min = 0, max = 65535)
write.table(X, file = "~/r-rand.txt", sep = "\n", row.names = FALSE)

Then:
cat ~/r-rand.txt | ./vseq > ~/r-rand.csv

And then to create the chart:

library(ggplot2);
num.csv <- read.csv("~/random.csv", header=T)
qplot(X, Y, data=num.csv);

Hope this helps sort this out.

Best Regards,
-Steve

On Thursday, September 20, 2018 5:09:23 PM EDT Steve Grubb wrote:
> On Thursday, September 20, 2018 11:15:04 AM EDT Duncan Murdoch wrote:
> > On 20/09/2018 6:59 AM, Ralf Stubner wrote:
> > > On 9/20/18 1:43 AM, Carl Boettiger wrote:
> > >> For a well-tested C algorithm, based on my reading of Lemire, the
> > >> unbiased "algorithm 3" in https://arxiv.org/abs/1805.10941 is part
> > >> already of the C standard library in OpenBSD and macOS (as
> > >> arc4random_uniform), and in the GNU standard library.  Lemire also
> > >> provides C++ code in the appendix of his piece for both this and the
> > >> faster "nearly divisionless" algorithm.
> > >> 
> > >> It would be excellent if any R core members were interested in
> > >> considering bindings to these algorithms as a patch, or might express
> > >> expectations for how that patch would have to operate (e.g. re
> > >> Duncan's
> > >> comment about non-integer arguments to sample size).  Otherwise, an R
> > >> package binding seems like a good starting point, but I'm not the
> > >> right
> > >> volunteer.
> > > 
> > > It is difficult to do this in a package, since R does not provide
> > > access
> > > to the random bits generated by the RNG. Only a float in (0,1) is
> > > available via unif_rand().
> > 
> > I believe it is safe to multiply the unif_rand() value by 2^32, and take
> > the whole number part as an unsigned 32 bit integer.  Depending on the
> > RNG in use, that will give at least 25 random bits.  (The low order bits
> > are the questionable ones.  25 is just a guess, not a guarantee.)
> > 
> > However, if one is willing to use an external
> > 
> > > RNG, it is of course possible. After reading about Lemire's work [1], I
> > > had planned to integrate such an unbiased sampling scheme into the
> > > dqrng
> > > package, which I have now started. [2]
> > > 
> > > Using Duncan's example, the results look much better:
> > >> library(dqrng)
> > >> m <- (2/5)*2^32
> > >> y <- dqsample(m, 1000000, replace = TRUE)
> > >> table(y %% 2)
> > >> 
> > >       0      1
> > > 
> > > 500252 499748
> > 
> > Another useful diagnostic is
> > 
> >    plot(density(y[y %% 2 == 0]))
> > 
> > Obviously that should give a more or less uniform density, but for
> > values near m, the default sample() gives some nice pretty pictures of
> > quite non-uniform densities.
> > 
> > By the way, there are actually quite a few examples of very large m
> > besides m = (2/5)*2^32 where performance of sample() is noticeably bad.
> > You'll see problems in y %% 2 for any integer a > 1 with m = 2/(1 + 2a)
> > * 2^32, problems in y %% 3 for m = 3/(1 + 3a)*2^32 or m = 3/(2 +
> > 3a)*2^32, etc.
> > 
> > So perhaps I'm starting to be convinced that the default sample() should
> > be fixed.
> 
> I find this discussion fascinating. I normally test random numbers in
> different languages every now and again using various methods. One simple
> check that I do is to use Michal Zalewski's method when he studied Strange
> Attractors and Initial TCP/IP Sequence Numbers:
> 
> http://lcamtuf.coredump.cx/newtcp/
> https://pdfs.semanticscholar.org/
> adb7/069984e3fa48505cd5081ec118ccb95529a3.pdf
> 
> The technique works by mapping the dynamics of the generated numbers into a
> three-dimensional phase space. This is then plotted in a graph so that you
> can visually see if something odd is going on.
> 
> I used   runif(10000, min = 0, max = 65535)  to get a set of numbers. This
> is the resulting plot that was generated from R's numbers using this
> technique:
> 
> http://people.redhat.com/sgrubb/files/r-random.jpg
> 
> And for comparison this was generated by collecting the same number of
> samples from the bash shell:
> 
> http://people.redhat.com/sgrubb/files/bash-random.jpg
> 
> The net result is that it shows some banding in the R generated random
> numbers where bash has uniform random numbers with no discernible pattern.
> 
> Best Regards,
> -Steve
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From r@dford @ending from c@@toronto@edu  Fri Sep 21 16:15:27 2018
From: r@dford @ending from c@@toronto@edu (Radford Neal)
Date: Fri, 21 Sep 2018 10:15:27 -0400
Subject: [Rd] Bias in R's random integers?
In-Reply-To: <mailman.47747.5.1537524158.51011.r-devel@r-project.org>
References: <mailman.47747.5.1537524158.51011.r-devel@r-project.org>
Message-ID: <20180921141527.GA15079@mail.cs.toronto.edu>

> Duncan Murdoch:
>
> and you can see it in the original m with
> 
>    x <- sample(m, 1000000, replace = TRUE)
>    plot(density(x[x %% 2 == 0]))

OK.  Thanks.  I see there is a real problem.

One option to fix it while mostly retaining backwards-compatibility
would be to add extra bits from a second RNG call only when m is large
- eg, larger than 2^27.  That would retain reproducibility for most
analyses of small to moderate size data sets.  Of course, there would
still be some small, detectable error for values a bit less than 2^27,
but perhaps that's tolerable.  (The 2^27 threshold could obviously be
debated.)

R Core made a similar decision in the case of sampling with
replacement when implementing a new hashing algorithm that produces
different results.  It is enabled by default only when m > 1e7 and no
more than half the values are to be sampled, as was noted:

> Note that it wouldn't be the first time that sample() changes behavior
> in a non-backward compatible way:
>
>   https://stat.ethz.ch/pipermail/r-devel/2012-October/065049.html
>
> Cheers,
> H.

That incompatibility could have been avoided.  A year ago I posted
a fast hashing algorithm that produces the same results as the simple
algorithm, here:

  https://stat.ethz.ch/pipermail/r-devel/2017-October/075012.html

The latest version of this will be in the soon-to-be new release of
pqR, and will of course enabled automatically whenever it seems
desirable, for a considerable speed gain in many cases.

  Radford Neal


From edd @ending from debi@n@org  Fri Sep 21 16:20:00 2018
From: edd @ending from debi@n@org (Dirk Eddelbuettel)
Date: Fri, 21 Sep 2018 09:20:00 -0500
Subject: [Rd] Bias in R's random integers?
In-Reply-To: <5268718.ryNPvsxLyr@x2>
References: <CAN_1p9wVX=AR8aYtC2LJo0bgN9cRW3XcbYJqnVHdCzA-QA1ceg@mail.gmail.com>
 <a590bfc6-f172-b227-00d9-9eacb126fd63@gmail.com>
 <2357485.GhJh1lojO1@x2> <5268718.ryNPvsxLyr@x2>
Message-ID: <23460.65040.561425.838748@rob.eddelbuettel.com>


On 21 September 2018 at 09:50, Steve Grubb wrote:
| Hello,
| 
| Top posting. Several people have asked about the code to replicate my 
| results. I have cleaned up the code to remove an x/y coordinate bias for 
| displaying the results directly on a 640 x 480 VGA adapter. You can find the 
| code here:
| 
| http://people.redhat.com/sgrubb/files/vseq.c
| 
| To collect R samples:
| X <- runif(10000, min = 0, max = 65535)
| write.table(X, file = "~/r-rand.txt", sep = "\n", row.names = FALSE)
| 
| Then:
| cat ~/r-rand.txt | ./vseq > ~/r-rand.csv
| 
| And then to create the chart:
| 
| library(ggplot2);
| num.csv <- read.csv("~/random.csv", header=T)
| qplot(X, Y, data=num.csv);
| 
| Hope this helps sort this out.

Here is a simpler version in one file, combining the operations in a bit of
C++ and also an auto-executed piece of R to create the data, call the
transformation you created, and plot.  Store as eg /tmp/stevegrubb.cpp
and then call   Rcpp::sourceCpp("/tmp/stevegrubb.cpp")

Dirk

--- snip --------------------------------------------------------------------
#include <Rcpp.h>

#define SCALING 0.5

// [[Rcpp::export]]
Rcpp::NumericMatrix stevePlot(Rcpp::NumericVector itable) {
  size_t cnt = itable.size();
  Rcpp::NumericMatrix M(cnt,2);
  size_t num = 0;
  double nth, n1th = 0, n2th = 0, n3th = 0;
  double x, y;
  double usex, usey, pha = 0;

  
  usex = sin(pha);
  usey = cos(pha);
  while (num < cnt) {
    double x1, y1, z1;
    nth = itable[num];
    x1 = (nth - n1th)  * SCALING;
    y1 = (n1th - n2th) * SCALING;
    z1 = (n2th - n3th) * SCALING;
    x = (x1*usey+z1*usex);
    y = y1 + (z1*usey-x1*usex) / 2;
    if (num > 4) {
      M(num,0) = x;
      M(num,1) = y;
    } else {
      M(num,0) = M(num,1) = NA_REAL;
    }
    num++;
    n3th=n2th; n2th=n1th; n1th=nth;
  }
  Rcpp::colnames(M) = Rcpp::CharacterVector::create("X", "Y");
  return(M);
}

/*** R
library(ggplot2);
Xin <- runif(10000, min = 0, max = 65535)
Xout <- stevePlot(Xin);
qplot(X, Y, data=as.data.frame(Xout));
*/
--- snip --------------------------------------------------------------------


-- 
http://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From edd @ending from debi@n@org  Fri Sep 21 16:25:42 2018
From: edd @ending from debi@n@org (Dirk Eddelbuettel)
Date: Fri, 21 Sep 2018 09:25:42 -0500
Subject: [Rd] Bias in R's random integers?
In-Reply-To: <5268718.ryNPvsxLyr@x2>
References: <CAN_1p9wVX=AR8aYtC2LJo0bgN9cRW3XcbYJqnVHdCzA-QA1ceg@mail.gmail.com>
 <a590bfc6-f172-b227-00d9-9eacb126fd63@gmail.com>
 <2357485.GhJh1lojO1@x2> <5268718.ryNPvsxLyr@x2>
Message-ID: <23460.65382.277446.718386@rob.eddelbuettel.com>


Slightly nicer version:

--- snip --------------------------------------------------------------------
#include <Rcpp.h>

// [[Rcpp::export]]
Rcpp::DataFrame stevePlot(Rcpp::NumericVector itable) {
  size_t cnt = itable.size(), num = 0;
  double nth, n1th = 0, n2th = 0, n3th = 0;
  double x, y, usex, usey, pha = 0;
  std::vector<double> xv, yv;
  const double scaling = 0.5;
  usex = sin(pha);
  usey = cos(pha);
  while (num < cnt) {
    double x1, y1, z1;
    nth = itable[num];
    num++;
    x1 = (nth - n1th)  * scaling;
    y1 = (n1th - n2th) * scaling;
    z1 = (n2th - n3th) * scaling;
    x = (x1*usey+z1*usex);
    y = y1 + (z1*usey-x1*usex) / 2;
    if (num > 4) {
      xv.push_back(x);
      yv.push_back(y);
    }
    n3th=n2th; n2th=n1th; n1th=nth;
  }
  return(Rcpp::DataFrame::create(Rcpp::Named("X") = xv,
                                 Rcpp::Named("Y") = yv));
}

/*** R
library(ggplot2)
Xin <- runif(10000, min = 0, max = 65535)
Xout <- stevePlot(Xin)
qplot(X, Y, data=Xout)
*/
--- snip --------------------------------------------------------------------

Still just   Rcpp::sourceCpp("filenamehere.cpp")   it.

Dirk

-- 
http://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From luke-tierney @ending from uiow@@edu  Fri Sep 21 18:38:15 2018
From: luke-tierney @ending from uiow@@edu (Tierney, Luke)
Date: Fri, 21 Sep 2018 16:38:15 +0000
Subject: [Rd] Bias in R's random integers?
In-Reply-To: <5268718.ryNPvsxLyr@x2>
References: <CAN_1p9wVX=AR8aYtC2LJo0bgN9cRW3XcbYJqnVHdCzA-QA1ceg@mail.gmail.com>
 <a590bfc6-f172-b227-00d9-9eacb126fd63@gmail.com> <2357485.GhJh1lojO1@x2>
 <5268718.ryNPvsxLyr@x2>
Message-ID: <alpine.DEB.2.21.1809211112290.18675@luke-Latitude-7480>

Not sure what should happen theoretically for the code in vseq.c, but
I see the same pattern with the R generators I tried (default,
Super-Duper, and L'Ecuyer) and with with bash $RANDOM using

N <- 10000
X1 <- replicate(N, as.integer(system("bash -c 'echo $RANDOM'", intern = TRUE)))
X2 <- replicate(N, as.integer(system("bash -c 'echo $RANDOM'", intern = TRUE)))
X <- X1 + 2 ^ 15 * (X2 > 2^14)

and with numbers from random.org

library(random)
X <- randomNumbers(N, 0, 2^16-1, col = 1)

So I'm not convinced there is an issue.

Best,

luke

On Fri, 21 Sep 2018, Steve Grubb wrote:

> Hello,
>
> Top posting. Several people have asked about the code to replicate my
> results. I have cleaned up the code to remove an x/y coordinate bias for
> displaying the results directly on a 640 x 480 VGA adapter. You can find the
> code here:
>
> http://people.redhat.com/sgrubb/files/vseq.c
>
> To collect R samples:
> X <- runif(10000, min = 0, max = 65535)
> write.table(X, file = "~/r-rand.txt", sep = "\n", row.names = FALSE)
>
> Then:
> cat ~/r-rand.txt | ./vseq > ~/r-rand.csv
>
> And then to create the chart:
>
> library(ggplot2);
> num.csv <- read.csv("~/random.csv", header=T)
> qplot(X, Y, data=num.csv);
>
> Hope this helps sort this out.
>
> Best Regards,
> -Steve
>
> On Thursday, September 20, 2018 5:09:23 PM EDT Steve Grubb wrote:
>> On Thursday, September 20, 2018 11:15:04 AM EDT Duncan Murdoch wrote:
>>> On 20/09/2018 6:59 AM, Ralf Stubner wrote:
>>>> On 9/20/18 1:43 AM, Carl Boettiger wrote:
>>>>> For a well-tested C algorithm, based on my reading of Lemire, the
>>>>> unbiased "algorithm 3" in https://arxiv.org/abs/1805.10941 is part
>>>>> already of the C standard library in OpenBSD and macOS (as
>>>>> arc4random_uniform), and in the GNU standard library.  Lemire also
>>>>> provides C++ code in the appendix of his piece for both this and the
>>>>> faster "nearly divisionless" algorithm.
>>>>>
>>>>> It would be excellent if any R core members were interested in
>>>>> considering bindings to these algorithms as a patch, or might express
>>>>> expectations for how that patch would have to operate (e.g. re
>>>>> Duncan's
>>>>> comment about non-integer arguments to sample size).  Otherwise, an R
>>>>> package binding seems like a good starting point, but I'm not the
>>>>> right
>>>>> volunteer.
>>>>
>>>> It is difficult to do this in a package, since R does not provide
>>>> access
>>>> to the random bits generated by the RNG. Only a float in (0,1) is
>>>> available via unif_rand().
>>>
>>> I believe it is safe to multiply the unif_rand() value by 2^32, and take
>>> the whole number part as an unsigned 32 bit integer.  Depending on the
>>> RNG in use, that will give at least 25 random bits.  (The low order bits
>>> are the questionable ones.  25 is just a guess, not a guarantee.)
>>>
>>> However, if one is willing to use an external
>>>
>>>> RNG, it is of course possible. After reading about Lemire's work [1], I
>>>> had planned to integrate such an unbiased sampling scheme into the
>>>> dqrng
>>>> package, which I have now started. [2]
>>>>
>>>> Using Duncan's example, the results look much better:
>>>>> library(dqrng)
>>>>> m <- (2/5)*2^32
>>>>> y <- dqsample(m, 1000000, replace = TRUE)
>>>>> table(y %% 2)
>>>>>
>>>>       0      1
>>>>
>>>> 500252 499748
>>>
>>> Another useful diagnostic is
>>>
>>>    plot(density(y[y %% 2 == 0]))
>>>
>>> Obviously that should give a more or less uniform density, but for
>>> values near m, the default sample() gives some nice pretty pictures of
>>> quite non-uniform densities.
>>>
>>> By the way, there are actually quite a few examples of very large m
>>> besides m = (2/5)*2^32 where performance of sample() is noticeably bad.
>>> You'll see problems in y %% 2 for any integer a > 1 with m = 2/(1 + 2a)
>>> * 2^32, problems in y %% 3 for m = 3/(1 + 3a)*2^32 or m = 3/(2 +
>>> 3a)*2^32, etc.
>>>
>>> So perhaps I'm starting to be convinced that the default sample() should
>>> be fixed.
>>
>> I find this discussion fascinating. I normally test random numbers in
>> different languages every now and again using various methods. One simple
>> check that I do is to use Michal Zalewski's method when he studied Strange
>> Attractors and Initial TCP/IP Sequence Numbers:
>>
>> http://lcamtuf.coredump.cx/newtcp/
>> https://pdfs.semanticscholar.org/
>> adb7/069984e3fa48505cd5081ec118ccb95529a3.pdf
>>
>> The technique works by mapping the dynamics of the generated numbers into a
>> three-dimensional phase space. This is then plotted in a graph so that you
>> can visually see if something odd is going on.
>>
>> I used   runif(10000, min = 0, max = 65535)  to get a set of numbers. This
>> is the resulting plot that was generated from R's numbers using this
>> technique:
>>
>> http://people.redhat.com/sgrubb/files/r-random.jpg
>>
>> And for comparison this was generated by collecting the same number of
>> samples from the bash shell:
>>
>> http://people.redhat.com/sgrubb/files/bash-random.jpg
>>
>> The net result is that it shows some banding in the R generated random
>> numbers where bash has uniform random numbers with no discernible pattern.
>>
>> Best Regards,
>> -Steve
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

-- 
Luke Tierney
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu


From r@lf@@tubner @ending from d@q@n@@com  Fri Sep 21 23:28:38 2018
From: r@lf@@tubner @ending from d@q@n@@com (Ralf Stubner)
Date: Fri, 21 Sep 2018 23:28:38 +0200
Subject: [Rd] Bias in R's random integers?
In-Reply-To: <alpine.DEB.2.21.1809211112290.18675@luke-Latitude-7480>
References: <CAN_1p9wVX=AR8aYtC2LJo0bgN9cRW3XcbYJqnVHdCzA-QA1ceg@mail.gmail.com>
 <a590bfc6-f172-b227-00d9-9eacb126fd63@gmail.com> <2357485.GhJh1lojO1@x2>
 <5268718.ryNPvsxLyr@x2>
 <alpine.DEB.2.21.1809211112290.18675@luke-Latitude-7480>
Message-ID: <3861f9c1-a0c9-5490-3fcd-4ee22324f089@daqana.com>

On 9/21/18 6:38 PM, Tierney, Luke wrote:
> Not sure what should happen theoretically for the code in vseq.c, but
> I see the same pattern with the R generators I tried (default,
> Super-Duper, and L'Ecuyer) and with with bash $RANDOM using
> 
> N <- 10000
> X1 <- replicate(N, as.integer(system("bash -c 'echo $RANDOM'", intern = TRUE)))
> X2 <- replicate(N, as.integer(system("bash -c 'echo $RANDOM'", intern = TRUE)))
> X <- X1 + 2 ^ 15 * (X2 > 2^14)
> 
> and with numbers from random.org
> 
> library(random)
> X <- randomNumbers(N, 0, 2^16-1, col = 1)
> 
> So I'm not convinced there is an issue.
There is an issue, but it is in vseq.c.

The plot I found striking was this:

http://people.redhat.com/sgrubb/files/r-random.jpg

It shows a scatter plot that is bounded to some rectangle where the
upper right and lower left corner are empty. Roughly speaking, X and Y
correspond to *consecutive differences* between random draws. It is
obvious that differences between random draws are bounded by the range
of the RNG, and that there cannot be two *differences in a row* that are
close to the maximum (or minimum). Hence the expected shape for such a
scatter plot is a rectangle with two corners being forbidden.

Within the allowed region, there should be no structure what so ever
(given enough draws). And that was striking about the above picture: It
showed clear vertical bands which should not be there. MT does fail some
statistical tests, but it cannot be brought down that easily.

Interestingly, I first used Dirk's C++ function for convenience, and
that did *not* show these bands. But when I compiled vseq.c I could
reproduce this. To cut this short: There is an error in vseq.c when the
numbers are read in:

    tmp = strtoul(buf, NULL, 16);

The third argument to strtoul is the base in which the numbers should be
interpreted. However, R has written numbers with base 10. Those can be
interpreted as base 16, but they will mean something different. Once one
changes the above line to

    tmp = strtoul(buf, NULL, 10);

the bands do disappear.

cheerio
ralf

-- 
Ralf Stubner
Senior Software Engineer / Trainer

daqana GmbH
Dortustra?e 48
14467 Potsdam

T: +49 331 23 61 93 11
F: +49 331 23 61 93 90
M: +49 162 20 91 196
Mail: ralf.stubner at daqana.com

Sitz: Potsdam
Register: AG Potsdam HRB 27966 P
Ust.-IdNr.: DE300072622
Gesch?ftsf?hrer: Prof. Dr. Dr. Karl-Kuno Kunze


-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 833 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20180921/b4bc371f/attachment.sig>

From @grubb @ending from redh@t@com  Fri Sep 21 23:33:52 2018
From: @grubb @ending from redh@t@com (Steve Grubb)
Date: Fri, 21 Sep 2018 17:33:52 -0400
Subject: [Rd] Bias in R's random integers?
In-Reply-To: <alpine.DEB.2.21.1809211112290.18675@luke-Latitude-7480>
References: <CAN_1p9wVX=AR8aYtC2LJo0bgN9cRW3XcbYJqnVHdCzA-QA1ceg@mail.gmail.com>
 <5268718.ryNPvsxLyr@x2>
 <alpine.DEB.2.21.1809211112290.18675@luke-Latitude-7480>
Message-ID: <25211226.9na0BstKmx@x2>

On Friday, September 21, 2018 12:38:15 PM EDT Tierney, Luke wrote:
> Not sure what should happen theoretically for the code in vseq.c, but
> I see the same pattern with the R generators I tried (default,
> Super-Duper, and L'Ecuyer) and with with bash $RANDOM using
> 
> N <- 10000
> X1 <- replicate(N, as.integer(system("bash -c 'echo $RANDOM'", intern =
> TRUE))) X2 <- replicate(N, as.integer(system("bash -c 'echo $RANDOM'",
> intern = TRUE))) X <- X1 + 2 ^ 15 * (X2 > 2^14)
> 
> and with numbers from random.org
> 
> library(random)
> X <- randomNumbers(N, 0, 2^16-1, col = 1)
> 
> So I'm not convinced there is an issue.

Indeed. I found the issue: Hex vs Decimal output. If the bash command was
printf "0x%08x\n" $RANDOM, then everything is fine. So, the code in vseq.c 
needs to be  strtoul(buf, NULL, 0);  so that it handles Hex, Decimal, or 
Octal. With this correction to vseq all is fine. No banding in any generated 
image. Corrected program is uploaded.

So, the technique works. When the numbers had a problem, due to a conversion 
error, it showed a distinct pattern. :-)

Best Regards,
-Steve


From @grubb @ending from redh@t@com  Fri Sep 21 23:38:05 2018
From: @grubb @ending from redh@t@com (Steve Grubb)
Date: Fri, 21 Sep 2018 17:38:05 -0400
Subject: [Rd] Bias in R's random integers?
In-Reply-To: <3861f9c1-a0c9-5490-3fcd-4ee22324f089@daqana.com>
References: <CAN_1p9wVX=AR8aYtC2LJo0bgN9cRW3XcbYJqnVHdCzA-QA1ceg@mail.gmail.com>
 <alpine.DEB.2.21.1809211112290.18675@luke-Latitude-7480>
 <3861f9c1-a0c9-5490-3fcd-4ee22324f089@daqana.com>
Message-ID: <16021544.Q7Xe3GdlRP@x2>

On Friday, September 21, 2018 5:28:38 PM EDT Ralf Stubner wrote:
> On 9/21/18 6:38 PM, Tierney, Luke wrote:
> > Not sure what should happen theoretically for the code in vseq.c, but
> > I see the same pattern with the R generators I tried (default,
> > Super-Duper, and L'Ecuyer) and with with bash $RANDOM using
> > 
> > N <- 10000
> > X1 <- replicate(N, as.integer(system("bash -c 'echo $RANDOM'", intern =
> > TRUE))) X2 <- replicate(N, as.integer(system("bash -c 'echo $RANDOM'",
> > intern = TRUE))) X <- X1 + 2 ^ 15 * (X2 > 2^14)
> > 
> > and with numbers from random.org
> > 
> > library(random)
> > X <- randomNumbers(N, 0, 2^16-1, col = 1)
> > 
> > So I'm not convinced there is an issue.
> 
> There is an issue, but it is in vseq.c.
> 
> The plot I found striking was this:
> 
> http://people.redhat.com/sgrubb/files/r-random.jpg
> 
> It shows a scatter plot that is bounded to some rectangle where the
> upper right and lower left corner are empty. Roughly speaking, X and Y
> correspond to *consecutive differences* between random draws. It is
> obvious that differences between random draws are bounded by the range
> of the RNG, and that there cannot be two *differences in a row* that are
> close to the maximum (or minimum). Hence the expected shape for such a
> scatter plot is a rectangle with two corners being forbidden.
> 
> Within the allowed region, there should be no structure what so ever
> (given enough draws). And that was striking about the above picture: It
> showed clear vertical bands which should not be there. MT does fail some
> statistical tests, but it cannot be brought down that easily.
> 
> Interestingly, I first used Dirk's C++ function for convenience, and
> that did *not* show these bands. But when I compiled vseq.c I could
> reproduce this. To cut this short: There is an error in vseq.c when the
> numbers are read in:
> 
>     tmp = strtoul(buf, NULL, 16);
> 
> The third argument to strtoul is the base in which the numbers should be
> interpreted. However, R has written numbers with base 10. Those can be
> interpreted as base 16, but they will mean something different. Once one
> changes the above line to
> 
>     tmp = strtoul(buf, NULL, 10);
> 
> the bands do disappear.

Yes. I just discovered the problem also. I was looking at how my bash script 
worked fine and how the example Luke gave had a problem. I was using the 
print command to keep things in hex. A corrected copy was uploaded so no one 
else runs across this.

Best Regards,
-Steve


From c@@rdi@g@bor @ending from gm@il@com  Sat Sep 22 23:46:13 2018
From: c@@rdi@g@bor @ending from gm@il@com (=?UTF-8?B?R8OhYm9yIENzw6FyZGk=?=)
Date: Sat, 22 Sep 2018 22:46:13 +0100
Subject: [Rd] Possible bug, max argument in print.default(),
 on R-3.5.1-patched
Message-ID: <CABtg=K=w06_s_tXcMdpHSq7YGCBK-5+2Bi8KgUenb50Vp3LGzg@mail.gmail.com>

The max argument of print.default() does not override
options(max.print), see below.
R 3.5.1 and  R-devel both seem good.

G?bor

> options(max.print = 1)
> print(data.frame(a=1:10))
  a
1 1
 [ reached 'max' / getOption("max.print") -- omitted 9 rows ]
> print(data.frame(a=1:10), max  = 100)
    a
1   1
 [ reached getOption("max.print") -- omitted 9 rows ]
> options(max.print = 1000)
> R.version
               _
platform       x86_64-w64-mingw32
arch           x86_64
os             mingw32
system         x86_64, mingw32
status         Patched
major          3
minor          5.1
year           2018
month          09
day            11
svn rev        75286
language       R
version.string R version 3.5.1 Patched (2018-09-11 r75286)
nickname       Feather Spray
>


From ggrothendieck @ending from gm@il@com  Sun Sep 23 15:23:52 2018
From: ggrothendieck @ending from gm@il@com (Gabor Grothendieck)
Date: Sun, 23 Sep 2018 09:23:52 -0400
Subject: [Rd] Recall
Message-ID: <CAP01uRm5Cuu0RgU_BznfrUHmQLRz9re92ND6uqeR+8vaP8pN3Q@mail.gmail.com>

This works:

  my.compose <- function(f, ...) {
    if (missing(f)) identity
    else function(x) f(my.compose(...)(x))
  }

  my.compose(sin, cos, tan)(pi/4)
  ## [1] 0.5143953

  sin(cos(tan(pi/4)))
  ## [1] 0.5143953

But replacing my.compose with Recall in the else causes it to fail:

  my.compose2 <- function(f, ...) {
    if (missing(f)) identity
    else function(x) f(Recall(...)(x))
  }

  my.compose2(sin, cos, tan)(pi/4)
  ## Error in my.compose2(sin, cos, tan)(pi/4) : unused argument (tan)

Seems like a bug in R.

This is taken from:
https://stackoverflow.com/questions/52463170/a-recursive-compose-function-in-r

-- 
Statistics & Software Consulting
GKX Group, GKX Associates Inc.
tel: 1-877-GKX-GROUP
email: ggrothendieck at gmail.com


From ggrothendieck @ending from gm@il@com  Sun Sep 23 19:06:01 2018
From: ggrothendieck @ending from gm@il@com (Gabor Grothendieck)
Date: Sun, 23 Sep 2018 13:06:01 -0400
Subject: [Rd] Recall
In-Reply-To: <CAP01uRm5Cuu0RgU_BznfrUHmQLRz9re92ND6uqeR+8vaP8pN3Q@mail.gmail.com>
References: <CAP01uRm5Cuu0RgU_BznfrUHmQLRz9re92ND6uqeR+8vaP8pN3Q@mail.gmail.com>
Message-ID: <CAP01uR=XoLfW6nYj_Pw2okrtGwKZKSNFpGYkiydY5D7y6AQ7Ug@mail.gmail.com>

Please ignore. Looking at this again I realize the problem is that
Recall is not direclty within my.compose2 but rather is within the
anonymous function in the else.
On Sun, Sep 23, 2018 at 9:23 AM Gabor Grothendieck
<ggrothendieck at gmail.com> wrote:
>
> This works:
>
>   my.compose <- function(f, ...) {
>     if (missing(f)) identity
>     else function(x) f(my.compose(...)(x))
>   }
>
>   my.compose(sin, cos, tan)(pi/4)
>   ## [1] 0.5143953
>
>   sin(cos(tan(pi/4)))
>   ## [1] 0.5143953
>
> But replacing my.compose with Recall in the else causes it to fail:
>
>   my.compose2 <- function(f, ...) {
>     if (missing(f)) identity
>     else function(x) f(Recall(...)(x))
>   }
>
>   my.compose2(sin, cos, tan)(pi/4)
>   ## Error in my.compose2(sin, cos, tan)(pi/4) : unused argument (tan)
>
> Seems like a bug in R.
>
> This is taken from:
> https://stackoverflow.com/questions/52463170/a-recursive-compose-function-in-r
>
> --
> Statistics & Software Consulting
> GKX Group, GKX Associates Inc.
> tel: 1-877-GKX-GROUP
> email: ggrothendieck at gmail.com



-- 
Statistics & Software Consulting
GKX Group, GKX Associates Inc.
tel: 1-877-GKX-GROUP
email: ggrothendieck at gmail.com


From pd@lgd @ending from gm@il@com  Mon Sep 24 10:16:07 2018
From: pd@lgd @ending from gm@il@com (peter dalgaard)
Date: Mon, 24 Sep 2018 10:16:07 +0200
Subject: [Rd] Possible bug, max argument in print.default(),
 on R-3.5.1-patched
In-Reply-To: <CABtg=K=w06_s_tXcMdpHSq7YGCBK-5+2Bi8KgUenb50Vp3LGzg@mail.gmail.com>
References: <CABtg=K=w06_s_tXcMdpHSq7YGCBK-5+2Bi8KgUenb50Vp3LGzg@mail.gmail.com>
Message-ID: <1657FDAE-7C6A-48AF-AA7F-9BEAAFF54823@gmail.com>

Not in print.default(), but in print.data.frame(), which is now doing its own max= handling but not passing max to print.default (maechler, r75122 --- was this really for r-patched?

-pd

> On 22 Sep 2018, at 23:46 , G?bor Cs?rdi <csardi.gabor at gmail.com> wrote:
> 
> The max argument of print.default() does not override
> options(max.print), see below.
> R 3.5.1 and  R-devel both seem good.
> 
> G?bor
> 
>> options(max.print = 1)
>> print(data.frame(a=1:10))
>  a
> 1 1
> [ reached 'max' / getOption("max.print") -- omitted 9 rows ]
>> print(data.frame(a=1:10), max  = 100)
>    a
> 1   1
> [ reached getOption("max.print") -- omitted 9 rows ]
>> options(max.print = 1000)
>> R.version
>               _
> platform       x86_64-w64-mingw32
> arch           x86_64
> os             mingw32
> system         x86_64, mingw32
> status         Patched
> major          3
> minor          5.1
> year           2018
> month          09
> day            11
> svn rev        75286
> language       R
> version.string R version 3.5.1 Patched (2018-09-11 r75286)
> nickname       Feather Spray
>> 
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From jtelleri@@rproject @ending from gm@il@com  Mon Sep 24 22:50:32 2018
From: jtelleri@@rproject @ending from gm@il@com (Juan Telleria Ruiz de Aguirre)
Date: Mon, 24 Sep 2018 22:50:32 +0200
Subject: [Rd] Native 64 Integers
Message-ID: <CAJXDcw0TBZmS-psQm2YSHyv+1F_P2W_hoStpUNVyr2sCGRJrMQ@mail.gmail.com>

Dear R Developers,

I would like to pick up back again the issue of 64 bits integers with R:

http://r.789695.n4.nabble.com/Re-R-support-for-64-bit-integers-td2320024.html

*** CURRENT SITUATION ***

At the moment, as regards integers, all the following are the same type:

* length of an R vector
* R integer type
* C int type (Fixed at 32 bits: In practice)
 * Fortran INTEGER type (Fixed at 32 bits: By Standard)

*** OBJECTIVE ***

Introducing 64-bit integers natively into "base R", notably if it was
also allowed using them for indices.

And, ideally, we would like:
* length of an R vector.
* R integer type.
To become 64bit.

This would allow to free ourselves from the increasingly relevant
maximum-atomic-object-length = 2^31 problem.

*** DIFFICULTIES ***
a) If both the R length type and the R integer type become the same
64bit type and replace the current integer type -> Then every compiled
package would have to change to declare the arguments as int64 (or
long, on most 64bit systems) and INTEGER*8.

b) If the R length type changes to something /different/ from the
integer type then any compiled code has to be checked to see if C int
arguments are lengths or integers, which is more work and more
error-prone.

c) On the other hand, changing the integer type to 64bit -> Will
presumably make integer code run noticeably more slowly on 32bit
systems.

In any case, the changes could be postponed by having an option to
.C/.Call forcing lengths and integers to be passed as 32-bit -> This
would mean that: The code couldn't use large integers or large
vectors, but it would keep working indefinitely.

*** 2010 SOLUTION***

There were 2 possibilities at the time:
a) Using 64-bit integers.
b) Using "double precision integers": Solution Finally Chosen at 2010.
Reason: In order that not all R packages using compiled code had to be
patched extensively.

*** BIT64 PACKAGE***
Nowdays, we have 'bit64' Package,  which provides serializable S3
atomic 64bit (signed) integers (+-2^63).

But this are not a replacement for 32bit integers, as integer64 are:
* Not supported for subscripting.
* Have different semantics when combined with double, e.g. integer64 +
double => integer64.

https://cran.r-project.org/web/packages/bit64/index.html

*** PROPOSAL ***

Instead of seeing 64 integers as a substitution to 32 bit integers,
these could be included into base R as a new / additional data type,
which co-exists with:
a) Using 64-bit integers.
b) Using "double precision integers".

This new data type could:
* Be based (ported) from "bit64" package: https://github.com/cran/bit64
 * Allow to use int64 Data Type for Subscripting.
* Have Coercion Rules such as:
as.integer64()
is.integer64()
integer + integer64 => integer
double + integer64 => double
* Be included with a double "L". e.g.: 34783274893274892334279LL (This
would be integer64, not double).

By doing so, existing packages would not need to be recompiled, and
could keep on working as already do. So we would not introduce
backward incompatible change.

*** FINAL KEY IDEA ***

Take already developed "bit64" Package (https://github.com/cran/bit64)
as base for building a new Integer64 Type System which co-exists
natively in R with Integer32 (Just as "parallel" package was included
in the past into base R for example), and build on top of it
improvements.


From mikefc @ending from coolbutu@ele@@@com  Mon Sep 24 23:38:26 2018
From: mikefc @ending from coolbutu@ele@@@com (mikefc)
Date: Tue, 25 Sep 2018 07:38:26 +1000
Subject: [Rd] Fwd: Bug report: cbind with numeric and raw gives incorrect
 result
In-Reply-To: <CABtA9mEaP5mgixWdby4_jbi8gNTP7ns+XkbtK16-D+zASSJUFw@mail.gmail.com>
References: <CABtA9mEaP5mgixWdby4_jbi8gNTP7ns+XkbtK16-D+zASSJUFw@mail.gmail.com>
Message-ID: <CABtA9mFtAY1Qsj2BKyS2X9-hyVERXr97O3FvEWuMPEOTxiRiVQ@mail.gmail.com>

Hi there,

using cbind with a numeric and raw argument produces an incorrect result.

I've posted some details below,

kind regards,
Mike.



e.g.
> cbind(0, as.raw(0))
     [,1]          [,2]
[1,]    0 6.950136e-310



A longer example shows that the result is not a rounding error, is not
consistent, and repeated applications get different results.

> cbind(0, as.raw(1:10))
               [,1]           [,2]
 [1,]  0.000000e+00   0.000000e+00
 [2,]  0.000000e+00   0.000000e+00
 [3,]  0.000000e+00   0.000000e+00
 [4,]  0.000000e+00   0.000000e+00
 [5,]  0.000000e+00  6.950135e-310
 [6,] 4.243992e-314  6.950135e-310
 [7,] 8.487983e-314  6.324040e-322
 [8,] 1.273197e-313   0.000000e+00
 [9,] 1.697597e-313 -4.343725e-311
[10,] 2.121996e-313  1.812216e-308


This bug occurs on
* mac os (with R 3.5.1)
* linux (with R 3.4.4)
* Windows (with R 3.5.0)




My Session Info
R version 3.5.1 (2018-07-02)
Platform: x86_64-apple-darwin15.6.0 (64-bit)
Running under: macOS High Sierra 10.13.6

Matrix products: default
BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/
A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib
LAPACK: /Library/Frameworks/R.framework/Versions/3.5/
Resources/lib/libRlapack.dylib

locale:
[1] en_AU.UTF-8/en_AU.UTF-8/en_AU.UTF-8/C/en_AU.UTF-8/en_AU.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] memoise_1.1.0   ggplot2_3.0.0   nonogramp_0.1.0 purrr_0.2.5
dplyr_0.7.6

loaded via a namespace (and not attached):
 [1] Rcpp_0.12.18     rstudioapi_0.7   bindr_0.1.1      magrittr_1.5
tidyselect_0.2.4 munsell_0.5.0    colorspace_1.3-2 R6_2.2.2
rlang_0.2.1.9000 stringr_1.3.1    plyr_1.8.4       tools_3.5.1
grid_3.5.1
[14] packrat_0.4.9-3  gtable_0.2.0     withr_2.1.2      digest_0.6.15
lazyeval_0.2.1   assertthat_0.2.0 tibble_1.4.2     crayon_1.3.4
bindrcpp_0.2.2   pryr_0.1.4       codetools_0.2-15 glue_1.3.0
labeling_0.3
[27] stringi_1.2.4    compiler_3.5.1   pillar_1.3.0     scales_0.5.0
pkgconfig_2.0.1

	[[alternative HTML version deleted]]


From m@b2 @ending from @fu@c@  Mon Sep 24 08:19:43 2018
From: m@b2 @ending from @fu@c@ (Marie-Helene Burle)
Date: Sun, 23 Sep 2018 23:19:43 -0700
Subject: [Rd] 2 minor typos
Message-ID: <87va6vv2m8.fsf@sfu.ca>

Hello,

I would like to report 2 very minor typos:


1. help file for package:base function:function

The last sentence of the "Technical details" section reads:

"This is not normally user-visible, but it indicated when functions are printed."

Either "is" is missing ("but it is indicated") or "it" should be replaced by "is" ("but is indicated") if the subject is omitted.


2. R FAQ, mailing list section (https://cran.r-project.org/doc/FAQ/R-FAQ.html#What-mailing-lists-exist-for-R_003f)

For the R-package-devel list, the section reads:

"A list which which provides a forum for learning about the R package development process."

(Double "which").


If this was not the proper list or the proper way to report such minor typos, please do let me know so that I will know better in the future.

Thank you!

Best,

-----
Marie-Helene Burle (Marie)

PhD candidate (Centre for Wildlife Ecology)
R Data Peer (Research Commons)
Writing Facilitator (Student Learning Commons)
Data and Software Carpentry instructor
RStudio Community sustainer
Scientific Programming Study Group admin
-----
Simon Fraser University
(+1) 778 782-5618
http://www.sfu.ca/content/sfu/biology/people/profiles/msb2.html
https://github.com/prosoitos
https://twitter.com/MHBurle


From m@echler @ending from @t@t@m@th@ethz@ch  Tue Sep 25 14:39:52 2018
From: m@echler @ending from @t@t@m@th@ethz@ch (Martin Maechler)
Date: Tue, 25 Sep 2018 14:39:52 +0200
Subject: [Rd] Possible bug, max argument in print.default(),
 on R-3.5.1-patched
In-Reply-To: <1657FDAE-7C6A-48AF-AA7F-9BEAAFF54823@gmail.com>
References: <CABtg=K=w06_s_tXcMdpHSq7YGCBK-5+2Bi8KgUenb50Vp3LGzg@mail.gmail.com>
 <1657FDAE-7C6A-48AF-AA7F-9BEAAFF54823@gmail.com>
Message-ID: <23466.11416.344132.932176@stat.math.ethz.ch>

>>>>> peter dalgaard 
>>>>>     on Mon, 24 Sep 2018 10:16:07 +0200 writes:

    > Not in print.default(), but in print.data.frame(), which
    > is now doing its own max= handling but not passing max to
    > print.default (maechler, r75122 --- was this really for
    > r-patched?  -pd

Yes, because it solved an embarrasing thinko which lead to
enormous time spent for printing a few lines in extreme cases.

This was an incomplete bug-fix backport.. which I now have
completed (R-patched, svn rev >= 75359).

I'm sorry for the hassle.
Thank you, G?bor, for the report, and Peter for the digging!

Martin

    >> On 22 Sep 2018, at 23:46 , G?bor Cs?rdi
    >> <csardi.gabor at gmail.com> wrote:
    >> 
    >> The max argument of print.default() does not override
    >> options(max.print), see below.  R 3.5.1 and R-devel both
    >> seem good.
    >> 
    >> G?bor
    >> 
    >>> options(max.print = 1) print(data.frame(a=1:10))
    >> a 1 1 [ reached 'max' / getOption("max.print") -- omitted
    >> 9 rows ]
    >>> print(data.frame(a=1:10), max = 100)
    >> a 1 1 [ reached getOption("max.print") -- omitted 9 rows
    >> ]
    >>> options(max.print = 1000) R.version
    >> _ platform x86_64-w64-mingw32 arch x86_64 os mingw32
    >> system x86_64, mingw32 status Patched major 3 minor 5.1
    >> year 2018 month 09 day 11 svn rev 75286 language R
    >> version.string R version 3.5.1 Patched (2018-09-11
    >> r75286) nickname Feather Spray
    >>> 
    >> 
    >> ______________________________________________
    >> R-devel at r-project.org mailing list
    >> https://stat.ethz.ch/mailman/listinfo/r-devel

    > -- 
    > Peter Dalgaard, Professor, Center for Statistics,
    > Copenhagen Business School Solbjerg Plads 3, 2000
    > Frederiksberg, Denmark Phone: (+45)38153501 Office: A 4.23
    > Email: pd.mes at cbs.dk Priv: PDalgd at gmail.com


From brodie@g@@l@m @ending from y@hoo@com  Tue Sep 25 14:52:16 2018
From: brodie@g@@l@m @ending from y@hoo@com (brodie gaslam)
Date: Tue, 25 Sep 2018 12:52:16 +0000 (UTC)
Subject: [Rd] 
 Fwd: Bug report: cbind with numeric and raw gives incorrect result
In-Reply-To: <CABtA9mFtAY1Qsj2BKyS2X9-hyVERXr97O3FvEWuMPEOTxiRiVQ@mail.gmail.com>
References: <CABtA9mEaP5mgixWdby4_jbi8gNTP7ns+XkbtK16-D+zASSJUFw@mail.gmail.com>
 <CABtA9mFtAY1Qsj2BKyS2X9-hyVERXr97O3FvEWuMPEOTxiRiVQ@mail.gmail.com>
Message-ID: <676088014.2066943.1537879936696@mail.yahoo.com>



For what it's worth the following patch fixes that particular problem on my system.? I have not checked very carefully to make sure this does not cause other problems, but at a high level it seems to make sense.? In this particular part of the code I believe `mode` is taken to be the highest type of "column" encountered by `ctype` and based on conditionals it can (I think) be up to REALSXP here.? This leads to a `INTEGER(REALSXP)` call, which presumably messes up the underlying double bit representation.

Again, I looked at this very quickly so I could be completely wrong, but I did at least build R with this patch and then no longer observed the odd behavior reported by mikefc.

Index: src/main/bind.c
===================================================================
--- src/main/bind.c?? ?(revision 75340)
+++ src/main/bind.c?? ?(working copy)
@@ -1381,11 +1381,16 @@
??? ??? ??? ?MOD_ITERATE1(idx, k, i, i1, {
??? ??? ??? ???? LOGICAL(result)[n++] = RAW(u)[i1] ? TRUE : FALSE;
??? ??? ??? ?});
-?? ??? ???? } else {
+?? ??? ???? } else if (mode == INTSXP) {
??? ??? ??? ?R_xlen_t i, i1;
??? ??? ??? ?MOD_ITERATE1(idx, k, i, i1, {
??? ??? ??? ???? INTEGER(result)[n++] = (unsigned char) RAW(u)[i1];
??? ??? ??? ?});
+?? ??? ???? } else {
+?? ??? ??? ?R_xlen_t i, i1;
+?? ??? ??? ?MOD_ITERATE1(idx, k, i, i1, {
+?? ??? ??? ???? REAL(result)[n++] = (unsigned char) RAW(u)[i1];
+?? ??? ??? ?});
??? ??? ???? }
??? ??? ?}
??? ???? }






On Tuesday, September 25, 2018, 7:58:31 AM EDT, mikefc <mikefc at coolbutuseless.com> wrote: 





Hi there,

using cbind with a numeric and raw argument produces an incorrect result.

I've posted some details below,

kind regards,
Mike.



e.g.
> cbind(0, as.raw(0))
? ? [,1]? ? ? ? ? [,2]
[1,]? ? 0 6.950136e-310



A longer example shows that the result is not a rounding error, is not
consistent, and repeated applications get different results.

> cbind(0, as.raw(1:10))
? ? ? ? ? ? ? [,1]? ? ? ? ? [,2]
[1,]? 0.000000e+00? 0.000000e+00
[2,]? 0.000000e+00? 0.000000e+00
[3,]? 0.000000e+00? 0.000000e+00
[4,]? 0.000000e+00? 0.000000e+00
[5,]? 0.000000e+00? 6.950135e-310
[6,] 4.243992e-314? 6.950135e-310
[7,] 8.487983e-314? 6.324040e-322
[8,] 1.273197e-313? 0.000000e+00
[9,] 1.697597e-313 -4.343725e-311
[10,] 2.121996e-313? 1.812216e-308


This bug occurs on
* mac os (with R 3.5.1)
* linux (with R 3.4.4)
* Windows (with R 3.5.0)




My Session Info
R version 3.5.1 (2018-07-02)
Platform: x86_64-apple-darwin15.6.0 (64-bit)
Running under: macOS High Sierra 10.13.6

Matrix products: default
BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/
A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib
LAPACK: /Library/Frameworks/R.framework/Versions/3.5/
Resources/lib/libRlapack.dylib

locale:
[1] en_AU.UTF-8/en_AU.UTF-8/en_AU.UTF-8/C/en_AU.UTF-8/en_AU.UTF-8

attached base packages:
[1] stats? ? graphics? grDevices utils? ? datasets? methods? base

other attached packages:
[1] memoise_1.1.0? ggplot2_3.0.0? nonogramp_0.1.0 purrr_0.2.5
dplyr_0.7.6

loaded via a namespace (and not attached):
[1] Rcpp_0.12.18? ? rstudioapi_0.7? bindr_0.1.1? ? ? magrittr_1.5
tidyselect_0.2.4 munsell_0.5.0? ? colorspace_1.3-2 R6_2.2.2
rlang_0.2.1.9000 stringr_1.3.1? ? plyr_1.8.4? ? ? tools_3.5.1
grid_3.5.1
[14] packrat_0.4.9-3? gtable_0.2.0? ? withr_2.1.2? ? ? digest_0.6.15
lazyeval_0.2.1? assertthat_0.2.0 tibble_1.4.2? ? crayon_1.3.4
bindrcpp_0.2.2? pryr_0.1.4? ? ? codetools_0.2-15 glue_1.3.0
labeling_0.3
[27] stringi_1.2.4? ? compiler_3.5.1? pillar_1.3.0? ? scales_0.5.0
pkgconfig_2.0.1

??? [[alternative HTML version deleted]]

______________________________________________
R-devel at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-devel


From mikefc @ending from coolbutu@ele@@@com  Tue Sep 25 14:59:33 2018
From: mikefc @ending from coolbutu@ele@@@com (mikefc)
Date: Tue, 25 Sep 2018 22:59:33 +1000
Subject: [Rd] 
 Fwd: Bug report: cbind with numeric and raw gives incorrect result
In-Reply-To: <676088014.2066943.1537879936696@mail.yahoo.com>
References: <CABtA9mEaP5mgixWdby4_jbi8gNTP7ns+XkbtK16-D+zASSJUFw@mail.gmail.com>
 <CABtA9mFtAY1Qsj2BKyS2X9-hyVERXr97O3FvEWuMPEOTxiRiVQ@mail.gmail.com>
 <676088014.2066943.1537879936696@mail.yahoo.com>
Message-ID: <CABtA9mF7iDri64mgH6kybTriTi3Ds_Srg8xjhbuo7nyMHzAe_g@mail.gmail.com>

Thanks Brodie, that's some nice detective work.

If someone wanted to grant me access to Bugzilla, I'll be happy to post the
bug and patch there (with your permission Brodie?) and help this bug get
fixed.

Mike.

On Tue., 25 Sep. 2018, 10:53 pm brodie gaslam, <brodie.gaslam at yahoo.com>
wrote:

>
>
> For what it's worth the following patch fixes that particular problem on
> my system.  I have not checked very carefully to make sure this does not
> cause other problems, but at a high level it seems to make sense.  In this
> particular part of the code I believe `mode` is taken to be the highest
> type of "column" encountered by `ctype` and based on conditionals it can (I
> think) be up to REALSXP here.  This leads to a `INTEGER(REALSXP)` call,
> which presumably messes up the underlying double bit representation.
>
> Again, I looked at this very quickly so I could be completely wrong, but I
> did at least build R with this patch and then no longer observed the odd
> behavior reported by mikefc.
>
> Index: src/main/bind.c
> ===================================================================
> --- src/main/bind.c    (revision 75340)
> +++ src/main/bind.c    (working copy)
> @@ -1381,11 +1381,16 @@
>              MOD_ITERATE1(idx, k, i, i1, {
>                  LOGICAL(result)[n++] = RAW(u)[i1] ? TRUE : FALSE;
>              });
> -            } else {
> +            } else if (mode == INTSXP) {
>              R_xlen_t i, i1;
>              MOD_ITERATE1(idx, k, i, i1, {
>                  INTEGER(result)[n++] = (unsigned char) RAW(u)[i1];
>              });
> +            } else {
> +            R_xlen_t i, i1;
> +            MOD_ITERATE1(idx, k, i, i1, {
> +                REAL(result)[n++] = (unsigned char) RAW(u)[i1];
> +            });
>              }
>          }
>          }
>
>
>
>
>
>
> On Tuesday, September 25, 2018, 7:58:31 AM EDT, mikefc <
> mikefc at coolbutuseless.com> wrote:
>
>
>
>
>
> Hi there,
>
> using cbind with a numeric and raw argument produces an incorrect result.
>
> I've posted some details below,
>
> kind regards,
> Mike.
>
>
>
> e.g.
> > cbind(0, as.raw(0))
>     [,1]          [,2]
> [1,]    0 6.950136e-310
>
>
>
> A longer example shows that the result is not a rounding error, is not
> consistent, and repeated applications get different results.
>
> > cbind(0, as.raw(1:10))
>               [,1]          [,2]
> [1,]  0.000000e+00  0.000000e+00
> [2,]  0.000000e+00  0.000000e+00
> [3,]  0.000000e+00  0.000000e+00
> [4,]  0.000000e+00  0.000000e+00
> [5,]  0.000000e+00  6.950135e-310
> [6,] 4.243992e-314  6.950135e-310
> [7,] 8.487983e-314  6.324040e-322
> [8,] 1.273197e-313  0.000000e+00
> [9,] 1.697597e-313 -4.343725e-311
> [10,] 2.121996e-313  1.812216e-308
>
>
> This bug occurs on
> * mac os (with R 3.5.1)
> * linux (with R 3.4.4)
> * Windows (with R 3.5.0)
>
>
>
>
> My Session Info
> R version 3.5.1 (2018-07-02)
> Platform: x86_64-apple-darwin15.6.0 (64-bit)
> Running under: macOS High Sierra 10.13.6
>
> Matrix products: default
> BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/
> A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib
> LAPACK: /Library/Frameworks/R.framework/Versions/3.5/
> Resources/lib/libRlapack.dylib
>
> locale:
> [1] en_AU.UTF-8/en_AU.UTF-8/en_AU.UTF-8/C/en_AU.UTF-8/en_AU.UTF-8
>
> attached base packages:
> [1] stats    graphics  grDevices utils    datasets  methods  base
>
> other attached packages:
> [1] memoise_1.1.0  ggplot2_3.0.0  nonogramp_0.1.0 purrr_0.2.5
> dplyr_0.7.6
>
> loaded via a namespace (and not attached):
> [1] Rcpp_0.12.18    rstudioapi_0.7  bindr_0.1.1      magrittr_1.5
> tidyselect_0.2.4 munsell_0.5.0    colorspace_1.3-2 R6_2.2.2
> rlang_0.2.1.9000 stringr_1.3.1    plyr_1.8.4      tools_3.5.1
> grid_3.5.1
> [14] packrat_0.4.9-3  gtable_0.2.0    withr_2.1.2      digest_0.6.15
> lazyeval_0.2.1  assertthat_0.2.0 tibble_1.4.2    crayon_1.3.4
> bindrcpp_0.2.2  pryr_0.1.4      codetools_0.2-15 glue_1.3.0
> labeling_0.3
> [27] stringi_1.2.4    compiler_3.5.1  pillar_1.3.0    scales_0.5.0
> pkgconfig_2.0.1
>
>     [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From edd @ending from debi@n@org  Tue Sep 25 15:38:33 2018
From: edd @ending from debi@n@org (Dirk Eddelbuettel)
Date: Tue, 25 Sep 2018 08:38:33 -0500
Subject: [Rd] 
 Fwd: Bug report: cbind with numeric and raw gives incorrect result
In-Reply-To: <CABtA9mF7iDri64mgH6kybTriTi3Ds_Srg8xjhbuo7nyMHzAe_g@mail.gmail.com>
References: <CABtA9mEaP5mgixWdby4_jbi8gNTP7ns+XkbtK16-D+zASSJUFw@mail.gmail.com>
 <CABtA9mFtAY1Qsj2BKyS2X9-hyVERXr97O3FvEWuMPEOTxiRiVQ@mail.gmail.com>
 <676088014.2066943.1537879936696@mail.yahoo.com>
 <CABtA9mF7iDri64mgH6kybTriTi3Ds_Srg8xjhbuo7nyMHzAe_g@mail.gmail.com>
Message-ID: <23466.14937.599753.341839@rob.eddelbuettel.com>


On 25 September 2018 at 22:59, mikefc wrote:
| Thanks Brodie, that's some nice detective work.
| 
| If someone wanted to grant me access to Bugzilla, I'll be happy to post the
| bug and patch there (with your permission Brodie?) and help this bug get
| fixed.

I think it would help your cause if you were identifiable.

You know who you are dealing with. We do not. All we get is a first name, and
a "cool but useless" (so true, for once) handle on social media and email.

I don't set the rules for the bugzilla account, but if were me, I'd ask you
to give your name.

Sincerely,

Dirk

| Mike.
| 
| On Tue., 25 Sep. 2018, 10:53 pm brodie gaslam, <brodie.gaslam at yahoo.com>
| wrote:
| 
| >
| >
| > For what it's worth the following patch fixes that particular problem on
| > my system.  I have not checked very carefully to make sure this does not
| > cause other problems, but at a high level it seems to make sense.  In this
| > particular part of the code I believe `mode` is taken to be the highest
| > type of "column" encountered by `ctype` and based on conditionals it can (I
| > think) be up to REALSXP here.  This leads to a `INTEGER(REALSXP)` call,
| > which presumably messes up the underlying double bit representation.
| >
| > Again, I looked at this very quickly so I could be completely wrong, but I
| > did at least build R with this patch and then no longer observed the odd
| > behavior reported by mikefc.
| >
| > Index: src/main/bind.c
| > ===================================================================
| > --- src/main/bind.c    (revision 75340)
| > +++ src/main/bind.c    (working copy)
| > @@ -1381,11 +1381,16 @@
| >              MOD_ITERATE1(idx, k, i, i1, {
| >                  LOGICAL(result)[n++] = RAW(u)[i1] ? TRUE : FALSE;
| >              });
| > -            } else {
| > +            } else if (mode == INTSXP) {
| >              R_xlen_t i, i1;
| >              MOD_ITERATE1(idx, k, i, i1, {
| >                  INTEGER(result)[n++] = (unsigned char) RAW(u)[i1];
| >              });
| > +            } else {
| > +            R_xlen_t i, i1;
| > +            MOD_ITERATE1(idx, k, i, i1, {
| > +                REAL(result)[n++] = (unsigned char) RAW(u)[i1];
| > +            });
| >              }
| >          }
| >          }
| >
| >
| >
| >
| >
| >
| > On Tuesday, September 25, 2018, 7:58:31 AM EDT, mikefc <
| > mikefc at coolbutuseless.com> wrote:
| >
| >
| >
| >
| >
| > Hi there,
| >
| > using cbind with a numeric and raw argument produces an incorrect result.
| >
| > I've posted some details below,
| >
| > kind regards,
| > Mike.
| >
| >
| >
| > e.g.
| > > cbind(0, as.raw(0))
| >     [,1]          [,2]
| > [1,]    0 6.950136e-310
| >
| >
| >
| > A longer example shows that the result is not a rounding error, is not
| > consistent, and repeated applications get different results.
| >
| > > cbind(0, as.raw(1:10))
| >               [,1]          [,2]
| > [1,]  0.000000e+00  0.000000e+00
| > [2,]  0.000000e+00  0.000000e+00
| > [3,]  0.000000e+00  0.000000e+00
| > [4,]  0.000000e+00  0.000000e+00
| > [5,]  0.000000e+00  6.950135e-310
| > [6,] 4.243992e-314  6.950135e-310
| > [7,] 8.487983e-314  6.324040e-322
| > [8,] 1.273197e-313  0.000000e+00
| > [9,] 1.697597e-313 -4.343725e-311
| > [10,] 2.121996e-313  1.812216e-308
| >
| >
| > This bug occurs on
| > * mac os (with R 3.5.1)
| > * linux (with R 3.4.4)
| > * Windows (with R 3.5.0)
| >
| >
| >
| >
| > My Session Info
| > R version 3.5.1 (2018-07-02)
| > Platform: x86_64-apple-darwin15.6.0 (64-bit)
| > Running under: macOS High Sierra 10.13.6
| >
| > Matrix products: default
| > BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/
| > A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib
| > LAPACK: /Library/Frameworks/R.framework/Versions/3.5/
| > Resources/lib/libRlapack.dylib
| >
| > locale:
| > [1] en_AU.UTF-8/en_AU.UTF-8/en_AU.UTF-8/C/en_AU.UTF-8/en_AU.UTF-8
| >
| > attached base packages:
| > [1] stats    graphics  grDevices utils    datasets  methods  base
| >
| > other attached packages:
| > [1] memoise_1.1.0  ggplot2_3.0.0  nonogramp_0.1.0 purrr_0.2.5
| > dplyr_0.7.6
| >
| > loaded via a namespace (and not attached):
| > [1] Rcpp_0.12.18    rstudioapi_0.7  bindr_0.1.1      magrittr_1.5
| > tidyselect_0.2.4 munsell_0.5.0    colorspace_1.3-2 R6_2.2.2
| > rlang_0.2.1.9000 stringr_1.3.1    plyr_1.8.4      tools_3.5.1
| > grid_3.5.1
| > [14] packrat_0.4.9-3  gtable_0.2.0    withr_2.1.2      digest_0.6.15
| > lazyeval_0.2.1  assertthat_0.2.0 tibble_1.4.2    crayon_1.3.4
| > bindrcpp_0.2.2  pryr_0.1.4      codetools_0.2-15 glue_1.3.0
| > labeling_0.3
| > [27] stringi_1.2.4    compiler_3.5.1  pillar_1.3.0    scales_0.5.0
| > pkgconfig_2.0.1
| >
| >     [[alternative HTML version deleted]]
| >
| > ______________________________________________
| > R-devel at r-project.org mailing list
| > https://stat.ethz.ch/mailman/listinfo/r-devel
| >
| 
| 	[[alternative HTML version deleted]]
| 
| ______________________________________________
| R-devel at r-project.org mailing list
| https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
http://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From luke-tierney @ending from uiow@@edu  Tue Sep 25 16:59:39 2018
From: luke-tierney @ending from uiow@@edu (Tierney, Luke)
Date: Tue, 25 Sep 2018 14:59:39 +0000
Subject: [Rd] 
 Fwd: Bug report: cbind with numeric and raw gives incorrect result
In-Reply-To: <676088014.2066943.1537879936696@mail.yahoo.com>
References: <CABtA9mEaP5mgixWdby4_jbi8gNTP7ns+XkbtK16-D+zASSJUFw@mail.gmail.com>
 <CABtA9mFtAY1Qsj2BKyS2X9-hyVERXr97O3FvEWuMPEOTxiRiVQ@mail.gmail.com>
 <676088014.2066943.1537879936696@mail.yahoo.com>
Message-ID: <alpine.DEB.2.21.1809250958560.18675@luke-Latitude-7480>

Thanks for the report and patch. Fixed in R-devel and R_patched.

Best,

luke

On Tue, 25 Sep 2018, brodie gaslam via R-devel wrote:

>
>
> For what it's worth the following patch fixes that particular problem on my system.? I have not checked very carefully to make sure this does not cause other problems, but at a high level it seems to make sense.? In this particular part of the code I believe `mode` is taken to be the highest type of "column" encountered by `ctype` and based on conditionals it can (I think) be up to REALSXP here.? This leads to a `INTEGER(REALSXP)` call, which presumably messes up the underlying double bit representation.
>
> Again, I looked at this very quickly so I could be completely wrong, but I did at least build R with this patch and then no longer observed the odd behavior reported by mikefc.
>
> Index: src/main/bind.c
> ===================================================================
> --- src/main/bind.c?? ?(revision 75340)
> +++ src/main/bind.c?? ?(working copy)
> @@ -1381,11 +1381,16 @@
> ??? ??? ??? ?MOD_ITERATE1(idx, k, i, i1, {
> ??? ??? ??? ???? LOGICAL(result)[n++] = RAW(u)[i1] ? TRUE : FALSE;
> ??? ??? ??? ?});
> -?? ??? ???? } else {
> +?? ??? ???? } else if (mode == INTSXP) {
> ??? ??? ??? ?R_xlen_t i, i1;
> ??? ??? ??? ?MOD_ITERATE1(idx, k, i, i1, {
> ??? ??? ??? ???? INTEGER(result)[n++] = (unsigned char) RAW(u)[i1];
> ??? ??? ??? ?});
> +?? ??? ???? } else {
> +?? ??? ??? ?R_xlen_t i, i1;
> +?? ??? ??? ?MOD_ITERATE1(idx, k, i, i1, {
> +?? ??? ??? ???? REAL(result)[n++] = (unsigned char) RAW(u)[i1];
> +?? ??? ??? ?});
> ??? ??? ???? }
> ??? ??? ?}
> ??? ???? }
>
>
>
>
>
>
> On Tuesday, September 25, 2018, 7:58:31 AM EDT, mikefc <mikefc at coolbutuseless.com> wrote:
>
>
>
>
>
> Hi there,
>
> using cbind with a numeric and raw argument produces an incorrect result.
>
> I've posted some details below,
>
> kind regards,
> Mike.
>
>
>
> e.g.
>> cbind(0, as.raw(0))
> ? ? [,1]? ? ? ? ? [,2]
> [1,]? ? 0 6.950136e-310
>
>
>
> A longer example shows that the result is not a rounding error, is not
> consistent, and repeated applications get different results.
>
>> cbind(0, as.raw(1:10))
> ? ? ? ? ? ? ? [,1]? ? ? ? ? [,2]
> [1,]? 0.000000e+00? 0.000000e+00
> [2,]? 0.000000e+00? 0.000000e+00
> [3,]? 0.000000e+00? 0.000000e+00
> [4,]? 0.000000e+00? 0.000000e+00
> [5,]? 0.000000e+00? 6.950135e-310
> [6,] 4.243992e-314? 6.950135e-310
> [7,] 8.487983e-314? 6.324040e-322
> [8,] 1.273197e-313? 0.000000e+00
> [9,] 1.697597e-313 -4.343725e-311
> [10,] 2.121996e-313? 1.812216e-308
>
>
> This bug occurs on
> * mac os (with R 3.5.1)
> * linux (with R 3.4.4)
> * Windows (with R 3.5.0)
>
>
>
>
> My Session Info
> R version 3.5.1 (2018-07-02)
> Platform: x86_64-apple-darwin15.6.0 (64-bit)
> Running under: macOS High Sierra 10.13.6
>
> Matrix products: default
> BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/
> A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib
> LAPACK: /Library/Frameworks/R.framework/Versions/3.5/
> Resources/lib/libRlapack.dylib
>
> locale:
> [1] en_AU.UTF-8/en_AU.UTF-8/en_AU.UTF-8/C/en_AU.UTF-8/en_AU.UTF-8
>
> attached base packages:
> [1] stats? ? graphics? grDevices utils? ? datasets? methods? base
>
> other attached packages:
> [1] memoise_1.1.0? ggplot2_3.0.0? nonogramp_0.1.0 purrr_0.2.5
> dplyr_0.7.6
>
> loaded via a namespace (and not attached):
> [1] Rcpp_0.12.18? ? rstudioapi_0.7? bindr_0.1.1? ? ? magrittr_1.5
> tidyselect_0.2.4 munsell_0.5.0? ? colorspace_1.3-2 R6_2.2.2
> rlang_0.2.1.9000 stringr_1.3.1? ? plyr_1.8.4? ? ? tools_3.5.1
> grid_3.5.1
> [14] packrat_0.4.9-3? gtable_0.2.0? ? withr_2.1.2? ? ? digest_0.6.15
> lazyeval_0.2.1? assertthat_0.2.0 tibble_1.4.2? ? crayon_1.3.4
> bindrcpp_0.2.2? pryr_0.1.4? ? ? codetools_0.2-15 glue_1.3.0
> labeling_0.3
> [27] stringi_1.2.4? ? compiler_3.5.1? pillar_1.3.0? ? scales_0.5.0
> pkgconfig_2.0.1
>
> ??? [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

-- 
Luke Tierney
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu

From hp@ge@ @ending from fredhutch@org  Wed Sep 26 08:27:19 2018
From: hp@ge@ @ending from fredhutch@org (=?UTF-8?B?SGVydsOpIFBhZ8Oocw==?=)
Date: Tue, 25 Sep 2018 23:27:19 -0700
Subject: [Rd] as.vector() broken on a matrix or array of type "list"
Message-ID: <7e68eb24-d5bb-f949-c5ac-673f4f8508ae@fredhutch.org>

Hi,

Unlike on an atomic matrix, as.vector() doesn't drop the "dim"
attribute of matrix or array of type "list":

   m <- matrix(list(), nrow=2, ncol=3)
   m
   #      [,1] [,2] [,3]
   # [1,] NULL NULL NULL
   # [2,] NULL NULL NULL

   as.vector(m)
   #      [,1] [,2] [,3]
   # [1,] NULL NULL NULL
   # [2,] NULL NULL NULL

   is.vector(as.vector(m))
   # [1] FALSE

Thanks,
H.

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fredhutch.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From m@echler @ending from @t@t@m@th@ethz@ch  Wed Sep 26 09:41:50 2018
From: m@echler @ending from @t@t@m@th@ethz@ch (Martin Maechler)
Date: Wed, 26 Sep 2018 09:41:50 +0200
Subject: [Rd] as.vector() broken on a matrix or array of type "list"
In-Reply-To: <7e68eb24-d5bb-f949-c5ac-673f4f8508ae@fredhutch.org>
References: <7e68eb24-d5bb-f949-c5ac-673f4f8508ae@fredhutch.org>
Message-ID: <23467.14398.984686.196196@stat.math.ethz.ch>

>>>>> Herv? Pag?s 
>>>>>     on Tue, 25 Sep 2018 23:27:19 -0700 writes:

    > Hi, Unlike on an atomic matrix, as.vector() doesn't drop
    > the "dim" attribute of matrix or array of type "list":


>    m <- matrix(list(), nrow=2, ncol=3)
>    m
>    #      [,1] [,2] [,3]
>    # [1,] NULL NULL NULL
>    # [2,] NULL NULL NULL

> 
>    as.vector(m)
>    #      [,1] [,2] [,3]
>    # [1,] NULL NULL NULL
>    # [2,] NULL NULL NULL

as documented and as always, including (probably all) versions of S and S-plus.

>    is.vector(as.vector(m))
>    # [1] FALSE

as bad is that looks, that's also "known" and has been the case
forever as well... 

I agree that the semantics of as.vector(.)  are not what you
would expect, and probably neither what we would do when
creating R today. *)
The help page {the same for as.vector() and is.vector()}
mentions that as.vector() behavior more than once, notably at
the end of 'Details' and its 'Note's....
... with one exception where you have a strong point, and the documenation
is incomplete at least -- under the heading

 Methods for 'as.vector()':

   ....... follow the conventions of the default method.  In particular

   ... 
   ... 
   ... 

   ? ?is.vector(as.vector(x, m), m)? should be true for any mode ?m?, 
      including the default ?"any"?.

and you are right that this is not fulfilled in the case the
list has a 'dim' attribute.  

But I don't think we "can" change as.vector(.) for that case
(where it is a no-op).
Rather  possibly is.vector(.) should not return FALSE but TRUE -- with
the reasoning (I think most experienced R programmers would
agree) that the foremost property of 'm' is to be
 - a list() {with a dim attribute and matrix-like indexing possibility}
   rather than
 - a 'matrix' {where every matrix entry is a list()}.

At the moment my gut feeling would propose to only update the
documentation, adding that one case as "an exception for historic reasons".

Martin

-----
*) {Possibly such an R we would create today would be much closer to
    julia, where every function is generic / a multi-dispach method
    "a la S4" .... and still be blazingly fast, thanks to JIT
    compilation, method caching and more smart things.}
But as you know one of the strength of (base) R is its stability
and reliability.  You can only use something as a "the language
of applied statistics and data science" and rely that published
code still works 10 years later if the language is not
changed/redesigned from scratch every few years ((as some ... are)).


From m@cqueen1 @ending from llnl@gov  Wed Sep 26 17:32:07 2018
From: m@cqueen1 @ending from llnl@gov (MacQueen, Don)
Date: Wed, 26 Sep 2018 15:32:07 +0000
Subject: [Rd] Stability and reliability of R (comment on Re: as.vector()
 broken on a matrix or array of type "list")
Message-ID: <22EE66C1-D009-49A3-8184-A4081849F6DD@llnl.gov>

With regard to Martin's  comment about the strength of (base) R:

I have R code I wrote 15+ years ago that has been used regularly ever since with only a few minor changes needed due to changes in R. Within that code, I find particularly impressive for its stability a simple custom GUI that uses the tcltk package that has needed no updates whatsoever in all that time.

Such stability and reliability have been extremely valuable to me.

-Don

--
Don MacQueen
Lawrence Livermore National Laboratory
7000 East Ave., L-627
Livermore, CA 94550
925-423-1062
Lab cell 925-724-7509
 
 

?On 9/26/18, 12:41 AM, "R-devel on behalf of Martin Maechler" <r-devel-bounces at r-project.org on behalf of maechler at stat.math.ethz.ch> wrote:

[-- most of original message omitted, so as to comment on the following --]
    
    -----
    *) {Possibly such an R we would create today would be much closer to
        julia, where every function is generic / a multi-dispach method
        "a la S4" .... and still be blazingly fast, thanks to JIT
        compilation, method caching and more smart things.}
    But as you know one of the strength of (base) R is its stability
    and reliability.  You can only use something as a "the language
    of applied statistics and data science" and rely that published
    code still works 10 years later if the language is not
    changed/redesigned from scratch every few years ((as some ... are)).
    
    ______________________________________________
    R-devel at r-project.org mailing list
    https://stat.ethz.ch/mailman/listinfo/r-devel
    


From @pencer@gr@ve@ @ending from prod@y@e@com  Wed Sep 26 17:50:23 2018
From: @pencer@gr@ve@ @ending from prod@y@e@com (Spencer Graves)
Date: Wed, 26 Sep 2018 10:50:23 -0500
Subject: [Rd] Stability and reliability of R (comment on Re: as.vector()
 broken on a matrix or array of type "list")
In-Reply-To: <22EE66C1-D009-49A3-8184-A4081849F6DD@llnl.gov>
References: <22EE66C1-D009-49A3-8184-A4081849F6DD@llnl.gov>
Message-ID: <e7e77fa3-3dfe-352a-1487-5fac4dbafdf7@prodsyse.com>



On 2018-09-26 10:32, MacQueen, Don via R-devel wrote:
> With regard to Martin's  comment about the strength of (base) R:
>
> I have R code I wrote 15+ years ago that has been used regularly ever since with only a few minor changes needed due to changes in R. Within that code, I find particularly impressive for its stability a simple custom GUI that uses the tcltk package that has needed no updates whatsoever in all that time.
>
> Such stability and reliability have been extremely valuable to me.


 ????? How much of R's stability is due to the unit tests encouraged by 
the examples in the help pages, the vast majority of which are run 
repeatedly with each new change?


 ????? More generally, what are the lessons the computer science 
discipline can take from R's experience in this regard?


 ????? I discussed this eight years ago in an article on "Package 
development process"? that I posted to Wikipedia eight years ago that 
has attracted 9 views per day since.? I also added a table discussing 
this to the Wikipedia article on "Software repository". That article has 
attracted over 300 views per day for at least the past 3 years.? Both 
these articles could doubtless be improved by someone more knowledgeable 
than I.


 ????? Many thanks and kudos to Ross Ihaka, Bob Gentleman, Martin 
Maechler and the rest of the R Core team, who have managed this project 
so successfully for more than two decades now.


 ????? Spencer Graves
>
> -Don
>
> --
> Don MacQueen
> Lawrence Livermore National Laboratory
> 7000 East Ave., L-627
> Livermore, CA 94550
> 925-423-1062
> Lab cell 925-724-7509
>   
>   
>
> ?On 9/26/18, 12:41 AM, "R-devel on behalf of Martin Maechler" <r-devel-bounces at r-project.org on behalf of maechler at stat.math.ethz.ch> wrote:
>
> [-- most of original message omitted, so as to comment on the following --]
>      
>      -----
>      *) {Possibly such an R we would create today would be much closer to
>          julia, where every function is generic / a multi-dispach method
>          "a la S4" .... and still be blazingly fast, thanks to JIT
>          compilation, method caching and more smart things.}
>      But as you know one of the strength of (base) R is its stability
>      and reliability.  You can only use something as a "the language
>      of applied statistics and data science" and rely that published
>      code still works 10 years later if the language is not
>      changed/redesigned from scratch every few years ((as some ... are)).
>      
>      ______________________________________________
>      R-devel at r-project.org mailing list
>      https://stat.ethz.ch/mailman/listinfo/r-devel
>      
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From hp@ge@ @ending from fredhutch@org  Wed Sep 26 21:47:01 2018
From: hp@ge@ @ending from fredhutch@org (=?UTF-8?B?SGVydsOpIFBhZ8Oocw==?=)
Date: Wed, 26 Sep 2018 12:47:01 -0700
Subject: [Rd] as.vector() broken on a matrix or array of type "list"
In-Reply-To: <23467.14398.984686.196196@stat.math.ethz.ch>
References: <7e68eb24-d5bb-f949-c5ac-673f4f8508ae@fredhutch.org>
 <23467.14398.984686.196196@stat.math.ethz.ch>
Message-ID: <a1c834c5-3e70-fcd2-b7ba-38a42735ab83@fredhutch.org>

Hi Martin,

On 09/26/2018 12:41 AM, Martin Maechler wrote:
>>>>>> Herv? Pag?s
>>>>>>      on Tue, 25 Sep 2018 23:27:19 -0700 writes:
> 
>      > Hi, Unlike on an atomic matrix, as.vector() doesn't drop
>      > the "dim" attribute of matrix or array of type "list":
> 
> 
>>     m <- matrix(list(), nrow=2, ncol=3)
>>     m
>>     #      [,1] [,2] [,3]
>>     # [1,] NULL NULL NULL
>>     # [2,] NULL NULL NULL
> 
>>
>>     as.vector(m)
>>     #      [,1] [,2] [,3]
>>     # [1,] NULL NULL NULL
>>     # [2,] NULL NULL NULL
> 
> as documented and as always, including (probably all) versions of S and S-plus.
> 
>>     is.vector(as.vector(m))
>>     # [1] FALSE
> 
> as bad is that looks, that's also "known" and has been the case
> forever as well...
> 
> I agree that the semantics of as.vector(.)  are not what you
> would expect, and probably neither what we would do when
> creating R today. *)
> The help page {the same for as.vector() and is.vector()}
> mentions that as.vector() behavior more than once, notably at
> the end of 'Details' and its 'Note's....
> ... with one exception where you have a strong point, and the documenation
> is incomplete at least -- under the heading
> 
>   Methods for 'as.vector()':
> 
>     ....... follow the conventions of the default method.  In particular
> 
>     ...
>     ...
>     ...
> 
>     ? ?is.vector(as.vector(x, m), m)? should be true for any mode ?m?,
>        including the default ?"any"?.
> 
> and you are right that this is not fulfilled in the case the
> list has a 'dim' attribute.
> 
> But I don't think we "can" change as.vector(.) for that case
> (where it is a no-op).
> Rather  possibly is.vector(.) should not return FALSE but TRUE -- with
> the reasoning (I think most experienced R programmers would
> agree) that the foremost property of 'm' is to be
>   - a list() {with a dim attribute and matrix-like indexing possibility}
>     rather than
>   - a 'matrix' {where every matrix entry is a list()}.

Note that this change would break all the code around that uses
is.vector() to distinguish between an array (of mode "atomic" or
"list") and a non-array. Arguably is.array() should preferably be
used for that but I'm sure there is a lot of code around that uses
is.vector().

The bottom of the problem is that as.vector() doesn't drop attributes
that is.vector() sees as "vector breakers" i.e. as breaking the vector
nature of an object. So for example is.vector() considers the "dim"
attribute to be a vector breaker but as.vector() doesn't drop it.

So yes in order to bring is.vector() and as.vector() in agreement you
can either change one or the other, or both. My gut feeling though is
that it would be less disruptive to not change what is.vector() thinks
about the "dim" attribute and to make sure that as.vector() **always**
drops it (together with "dimnames" if present). How much code around
could there be that calls as.vector() on an array and expects the "dim"
attribute to be dropped **except** when the mode() of the array is
"list"? It is more likely that the code around that calls as.vector()
on an array doesn't expect such exception and so is broken. This was
actually the case for my code ;-)

Thanks,
H.

> 
> At the moment my gut feeling would propose to only update the
> documentation, adding that one case as "an exception for historic reasons".
> 
> Martin
> 
> -----
> *) {Possibly such an R we would create today would be much closer to
>      julia, where every function is generic / a multi-dispach method
>      "a la S4" .... and still be blazingly fast, thanks to JIT
>      compilation, method caching and more smart things.}
> But as you know one of the strength of (base) R is its stability
> and reliability.  You can only use something as a "the language
> of applied statistics and data science" and rely that published
> code still works 10 years later if the language is not
> changed/redesigned from scratch every few years ((as some ... are)).
> 
> 
> 

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fredhutch.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From hp@ge@ @ending from fredhutch@org  Wed Sep 26 22:03:40 2018
From: hp@ge@ @ending from fredhutch@org (=?UTF-8?B?SGVydsOpIFBhZ8Oocw==?=)
Date: Wed, 26 Sep 2018 13:03:40 -0700
Subject: [Rd] Bug in printing array of type "list"
Message-ID: <8610e0fe-ea37-8fd6-9cb3-8be9654b81ad@fredhutch.org>

Hi,

This array is of type "list" but print() reports otherwise:

   a1 <- array(list(1), 2:0)

   typeof(a1)
   # [1] "list"

   a1
   # <2 x 1 x 0 array of character>
   #      [,1]
   # [1,]
   # [2,]

No such problem with an array of type "logical":

   a2 <- array(NA, 2:0)

   typeof(a2)
   # [1] "logical"

   a2
   # <2 x 1 x 0 array of logical>
   #      [,1]
   # [1,]
   # [2,]

Thanks,
H.

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fredhutch.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From Kurt@Hornik @ending from wu@@c@@t  Thu Sep 27 18:54:58 2018
From: Kurt@Hornik @ending from wu@@c@@t (Kurt Hornik)
Date: Thu, 27 Sep 2018 18:54:58 +0200
Subject: [Rd] 2 minor typos
In-Reply-To: <87va6vv2m8.fsf@sfu.ca>
References: <87va6vv2m8.fsf@sfu.ca>
Message-ID: <23469.2914.624574.275647@hornik.net>

>>>>> Marie-Helene Burle writes:

Thanks, will fix!

Best
-k

> Hello,
> I would like to report 2 very minor typos:


> 1. help file for package:base function:function

> The last sentence of the "Technical details" section reads:

> "This is not normally user-visible, but it indicated when functions are printed."

> Either "is" is missing ("but it is indicated") or "it" should be replaced by "is" ("but is indicated") if the subject is omitted.


> 2. R FAQ, mailing list section (https://cran.r-project.org/doc/FAQ/R-FAQ.html#What-mailing-lists-exist-for-R_003f)

> For the R-package-devel list, the section reads:

> "A list which which provides a forum for learning about the R package development process."

> (Double "which").


> If this was not the proper list or the proper way to report such minor typos, please do let me know so that I will know better in the future.

> Thank you!

> Best,

> -----
> Marie-Helene Burle (Marie)

> PhD candidate (Centre for Wildlife Ecology)
> R Data Peer (Research Commons)
> Writing Facilitator (Student Learning Commons)
> Data and Software Carpentry instructor
> RStudio Community sustainer
> Scientific Programming Study Group admin
> -----
> Simon Fraser University
> (+1) 778 782-5618
> http://www.sfu.ca/content/sfu/biology/people/profiles/msb2.html
> https://github.com/prosoitos
> https://twitter.com/MHBurle

> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From r@lf@@tubner @ending from d@q@n@@com  Thu Sep 27 21:58:34 2018
From: r@lf@@tubner @ending from d@q@n@@com (Ralf Stubner)
Date: Thu, 27 Sep 2018 21:58:34 +0200
Subject: [Rd] Bias in R's random integers?
In-Reply-To: <05cffd00-d64d-3fd3-78b6-fae862946c26@daqana.com>
References: <CAN_1p9wVX=AR8aYtC2LJo0bgN9cRW3XcbYJqnVHdCzA-QA1ceg@mail.gmail.com>
 <ffb74021-8d95-a820-8013-f0e51dc1a9b3@gmail.com>
 <CALEXWq3booRRTSYvYqcjV7qPZGOC1oh5o45b5o=J2ypQi5s-ag@mail.gmail.com>
 <ece52410-ddb2-2198-9291-201475d3f46e@gmail.com>
 <CAA7KnHHEejYVn8NtKrY1KuKcJUq8n4LoJSDx9BZv-LJyC=r9yQ@mail.gmail.com>
 <73c4667d-38ad-f5ef-468c-4c368e0fdc12@gmail.com>
 <CAA7KnHFcY5YJ2kTAfPJJ0PHwcWx7CumE49A4NgJWC2NK5CV=-A@mail.gmail.com>
 <c3590da8-2e92-4e46-3a81-0d0dbfc81845@gmail.com>
 <CAA7KnHE9OSxML3Cp8Y-gd7CbBj-86UP9qAy6yzgvfcb+=Vg86Q@mail.gmail.com>
 <e82cfff0-3a8e-1d7b-de5c-02b69974ae16@gmail.com>
 <4143568b-c67f-2309-f9ec-676f9e791747@gmail.com>
 <CAN_1p9znLss3QKuF3WB5ucM_F2=paNx28eCdJecB8fj0p_w-6Q@mail.gmail.com>
 <5245f770-a010-b976-28c8-13a2ec74a088@daqana.com>
 <a590bfc6-f172-b227-00d9-9eacb126fd63@gmail.com>
 <05cffd00-d64d-3fd3-78b6-fae862946c26@daqana.com>
Message-ID: <898b00c3-d7c3-990f-5238-401f9e62879e@daqana.com>

On 9/21/18 3:15 PM, Ralf Stubner wrote:
> Right, the RNGs in R produce no more than 32bits, so the conversion to a
> double can be reverted. If we ignore those RNGs that produce less than
> 32bits for the moment, then the attached file contains a sample
> implementation (without long vectors, weighted sampling or hashing). It
> uses Rcpp for convenience, but I have tried to keep the C++ low.

I just realized that the mailing list had stripped the attachment. I am
therefore including it at the end for reference. Meanwhile I have also
cast this into a package (feedback welcome):

	https://www.daqana.org/dqsample/

cheerio
ralf

// [[Rcpp::plugins(cpp11)]]
#include <cstdint>
#include <Rcpp.h>

/* Daniel Lemire
   Fast Random Integer Generation in an Interval
   https://arxiv.org/abs/1805.10941
 */
uint32_t  nearlydivisionless32(uint32_t s, uint32_t (*random32)(void)) {
    uint32_t x = random32();
    uint64_t m = (uint64_t) x * (uint64_t) s;
    uint32_t l = (uint32_t) m;
    if (l < s) {
        uint32_t t = -s % s;
        while (l < t) {
            x = random32();
            m = (uint64_t) x * (uint64_t) s;
            l = (uint32_t) m;
        }
    }
    return m >> 32;
}

uint32_t random32() {
    return R::runif(0, 1) * 4294967296; /* 2^32 */
}

// [[Rcpp::export]]
Rcpp::IntegerVector sample_int(int n, int size, bool replace = false) {
    Rcpp::IntegerVector result(Rcpp::no_init(size));
    if (replace) {
        for (int i = 0; i < size; ++i)
            result[i] = nearlydivisionless32(n, random32) + 1;
    } else {
        Rcpp::IntegerVector tmp(Rcpp::no_init(n));
        for (int i = 0; i < n; ++i)
            tmp[i] = i;
        for (int i = 0; i < size; ++i) {
            int j = nearlydivisionless32(n, random32);
            result[i] = tmp[j] + 1;
            tmp[j] = tmp[--n];
        }
    }
    return result;
}

/*** R
set.seed(42)
sample.int(6, 10, replace = TRUE)
sample.int(100, 10)
set.seed(42)
sample_int(6, 10, replace = TRUE)
sample_int(100, 10)

m <- ceiling((2/5)*2^32)

set.seed(42)
x <- sample.int(m, 1000000, replace = TRUE)
table(x %% 2)

set.seed(42)
y <- sample_int(m, 1000000, replace = TRUE)
table(y %% 2)

set.seed(42)
sample.int(m, 6, replace = TRUE)

set.seed(42)
sample_int(m, 6, replace = TRUE)

bench::mark(orig = sample.int(m, 1000000, replace = TRUE),
            new  = sample_int(m, 1000000, replace = TRUE),
            check = FALSE)
bench::mark(orig = sample.int(6, 1000000, replace = TRUE),
            new  = sample_int(6, 1000000, replace = TRUE),
            check = FALSE)
 */


-- 
Ralf Stubner
Senior Software Engineer / Trainer

daqana GmbH
Dortustra?e 48
14467 Potsdam

T: +49 331 23 61 93 11
F: +49 331 23 61 93 90
M: +49 162 20 91 196
Mail: ralf.stubner at daqana.com

Sitz: Potsdam
Register: AG Potsdam HRB 27966 P
Ust.-IdNr.: DE300072622
Gesch?ftsf?hrer: Prof. Dr. Dr. Karl-Kuno Kunze


-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 833 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20180927/2648621a/attachment.sig>

From mikko@korpel@ @ending from m@@nmitt@u@l@ito@@fi  Sat Sep 29 08:49:25 2018
From: mikko@korpel@ @ending from m@@nmitt@u@l@ito@@fi (Korpela Mikko (MML))
Date: Sat, 29 Sep 2018 06:49:25 +0000
Subject: [Rd] Improvement to documentation in dput.Rd
Message-ID: <3e8ce8c001334129991999e5f68aa332@C119S212VM016.msvyvi.vaha.local>

If the "Matrix" package is attached, 'example(dget)' fails:

  > library(Matrix)
  > example(dget)
  
  dget> fil <- tempfile()
  
  dget> ## Write an ASCII version of function mean to our temp file
  dget> dput(mean, fil)

  dget> ## And read it back into 'bar'
  dget> bar <- dget(fil)
  Error in initialize(value, ...) : '...' used in an incorrect context

A possible fix would be to use 'dput(base::mean, fil)' instead of
'dput(mean, fil)' in src/library/base/man/dput.Rd .

-- 
Mikko Korpela


From m@echler @ending from @t@t@m@th@ethz@ch  Sat Sep 29 15:40:21 2018
From: m@echler @ending from @t@t@m@th@ethz@ch (Martin Maechler)
Date: Sat, 29 Sep 2018 15:40:21 +0200
Subject: [Rd] Improvement to documentation in dput.Rd
In-Reply-To: <3e8ce8c001334129991999e5f68aa332@C119S212VM016.msvyvi.vaha.local>
References: <3e8ce8c001334129991999e5f68aa332@C119S212VM016.msvyvi.vaha.local>
Message-ID: <23471.32965.252988.444727@stat.math.ethz.ch>

>>>>> Korpela Mikko (MML) 
>>>>>     on Sat, 29 Sep 2018 06:49:25 +0000 writes:

    > If the "Matrix" package is attached, 'example(dget)'
    > fails:
    >> library(Matrix) example(dget)
  
    dget> fil <- tempfile()
  
    dget> ## Write an ASCII version of function mean to our temp
    dget> file dput(mean, fil)

    dget> ## And read it back into 'bar' bar <- dget(fil)
    >   Error in initialize(value, ...) : '...' used in an
    > incorrect context

    > A possible fix would be to use 'dput(base::mean, fil)'
    > instead of 'dput(mean, fil)' in
    > src/library/base/man/dput.Rd .

    > -- 
    > Mikko Korpela

Thank you, Mikko,
indeed that's an improvement which I have done now (svn 75380).

---------

In an ideal world the deparse()'ing  [equivalent: dput()] of S4
generics and methods would give an expression which is not only
parse()able -- which it is finely here -- but would also be
correct when eval()'ed subsequently.

So in this sense, one could also for a "bug" fix which would provide that.
OTOH we do say in the docs that parse() typically works as
inverse of deparse()  {and hence dget() as inverse of dput()}
but not always...

Martin


From henrik@bengt@@on @ending from gm@il@com  Sat Sep 29 18:41:31 2018
From: henrik@bengt@@on @ending from gm@il@com (Henrik Bengtsson)
Date: Sat, 29 Sep 2018 09:41:31 -0700
Subject: [Rd] Query the pointer protection stack size (--max-ppsize=N) from
 within R?
Message-ID: <CAFDcVCTOJk+YFMej9Jg+BW4ggjAVqtRDJV-ZfGoaFvkrmQ=fxA@mail.gmail.com>

Hi,

for simply for troubleshooting purposes (e.g. making sure that
--max-ppsize=N) is set, is there a way to query the pointer protection
stack size from R?  I tried to grep the source code and couldn't find
anything, so I expect the answer is no.

Thxs,

Henrik


