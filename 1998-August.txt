From jimrc@mathfs.math.montana.edu  Tue Aug  4 22:23:29 1998
From: jimrc@mathfs.math.montana.edu (Jim Robison-Cox)
Date: Tue, 4 Aug 1998 15:23:29 -0600 (MDT)
Subject: aov with Error terms
Message-ID: <Pine.GSO.4.02.9808041517520.8073-100000@gauss.math.montana.edu>

 To R-devel:
 
   If anyone is testing the aov() function and it's relatives which I
 posted to the list, thanks for the effort, but I would ask you to hold
off now.  Brian Ripley is developing better and more complete versions, so
you should save your effort for looking over his functions.

  His preliminary version is located at:
http://www.stats.ox.ac.uk/pub/R/aov.tar.gz

Prof. Ripley says it contains:
BR> my current versions of  aov, proj, model.tables, eff.aovlist etc.
BR> 
BR> These are probably as complete as I want to make them, and handle
BR> Error terms and (to some extent) multiple responses.  It is hard to
BR> find sufficiently wierd designs to test this, so I would be grateful
BR> for further testing. There are some datasets and tests in the file,
BR> which is packaged as an R package.

 His note about our recent exchange is related to these functions, though
I was also asking about a summary.mlm which is not in current R nor in
today's aov library.


Jim Robison-Cox                 ____________            
Department of Math Sciences    |            |           phone: (406)994-5340
2-214 Wilson Hall               \   BZN, MT |           FAX:   (406)994-1789
Montana State University         |  *_______|
Bozeman, MT 59717                 \_|         e-mail: jimrc@math.montana.edu 




-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From Andreas.Weingessel@ci.tuwien.ac.at  Thu Aug  6 12:06:11 1998
From: Andreas.Weingessel@ci.tuwien.ac.at (Andreas Weingessel)
Date: Thu, 6 Aug 1998 13:06:11 +0200 (CEST)
Subject: memory-management
Message-ID: <13769.36387.261923.629675@elendil.ci.tuwien.ac.at>


R has some strange behavior in its memory management. (R-0.62.2 and
R-0.63.0 of July 22)

If I start R on a PC with 256MB with the option -v 200, and want to
see the free memory I get

R> gc()
Garbage collection ...
111801 cons cells free (55%)
204554k bytes of heap free (-63%)

So, I have suddenly a negative percentage of memory free. This value
becomes not positive, unless a certain amount of memory is used.

R> x <- matrix(0,1000,1000)
R> gc()
Garbage collection ...
111797 cons cells free (55%)
196741k bytes of heap free (-67%)
R> x <- matrix(0,4000,1000)
R> gc()
Garbage collection ...
111797 cons cells free (55%)
173304k bytes of heap free (-79%)
R> x <- matrix(0,5000,1000)
R> gc()
Garbage collection ...
111797 cons cells free (55%)
165491k bytes of heap free (80%)



The second point I came across is the following. R is now started with
-v 100 -n 1000000. After

R>  x <- matrix(0,5000,900)

I have still enough memory free.

R>  gc()
Garbage collection ...
911797 cons cells free (91%)
66997k bytes of heap free (65%)

So, if x uses 35% of heap memory, there should still be enough memory
for a second matrix of this size, but the command

R>  y <- matrix(0,5000,900)

runs out of heap memory.

Similarly, I can create a 5000x1300-matrix on a newly started R with
-v 100 and have still 50% heap memory free, but I can not create a
5000x1400-matrix, if R is started with -v 100. So, I suppose that R
needs a complete copy of this matrix while creating it and therefore I
can only create matrices which use less than half of the free
memory. Is there any way to avoid this memory usage?

************************************************************************
*                          Andreas Weingessel                          *
************************************************************************
* Institut f=FCr Statistik      *                Tel: (+43 1) 58801 4541 =
*
* Technische Universit=E4t Wien *                Fax: (+43 1)  504 14 98 =
*
* Wiedner Hauptstr. 8-10/1071 *     Andreas.Weingessel@ci.tuwien.ac.at *
* A-1040 Wien, Austria        * http://www.ci.tuwien.ac.at/~weingessel *
************************************************************************


-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From pgilbert@bank-banque-canada.ca  Tue Aug 11 15:02:21 1998
From: pgilbert@bank-banque-canada.ca (Paul Gilbert)
Date: Tue, 11 Aug 1998 10:02:21 -0400
Subject: R-beta: base not loading
References: <199808111314.PAA13233@sophie.ethz.ch>
Message-ID: <98Aug11.100657edt.13447@mailgate.bank-banque-canada.ca>

Martin

No I didn't solve it. I've been on holidays for a couple of weeks. I'll try to
look at it again soon, but any hints would be appreciated.

I can compile on SunOS 5.5 and then run on 5.6, which may be a sort term
solution to your user's problem.

Paul

-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From pgilbert@bank-banque-canada.ca  Tue Aug 11 15:27:03 1998
From: pgilbert@bank-banque-canada.ca (Paul Gilbert)
Date: Tue, 11 Aug 1998 10:27:03 -0400
Subject: R-beta: base not loading
References: <199808111314.PAA13233@sophie.ethz.ch> <x2btprvcmk.fsf@blueberry.kubism.ku.dk>
Message-ID: <98Aug11.103140edt.13458@mailgate.bank-banque-canada.ca>

I just remembered, another thing which complicates this is that older gcc's do
not work with Solaris 2.6 so I had to upgrade to gcc 2.8.1. The result is that I
am not sure if this is

1/  gcc 2.8.1 is not good for the current version of R

2/ my gcc 2.8.1 is not correctly configured

3/ I need to be more careful to upgrade *all* parts of the toolchain

My gut feeling is the same as Peter's (3/), but I haven't got a clue how to
track down the problem.

Paul

(PS. There also seem to be some problems making the f2c with gcc 2.8.1. I've
been using g77 so that isn't the problem.)

-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From Martin Maechler <maechler@stat.math.ethz.ch>  Tue Aug 11 15:34:30 1998
From: Martin Maechler <maechler@stat.math.ethz.ch> (Martin Maechler)
Date: Tue, 11 Aug 1998 16:34:30 +0200
Subject: R-beta: base not loading
In-Reply-To: <98Aug11.103140edt.13458@mailgate.bank-banque-canada.ca> (message
 from Paul Gilbert on Tue, 11 Aug 1998 10:27:03 -0400)
Message-ID: <199808111434.QAA19036@sophie.ethz.ch>

>>>>> "PaulG" == Paul Gilbert <pgilbert@bank-banque-canada.ca> writes:

    PaulG> I just remembered, another thing which complicates this is that
    PaulG> older gcc's do not work with Solaris 2.6 so I had to upgrade to
    PaulG> gcc 2.8.1. The result is that I am not sure if this is

    PaulG> 1/ gcc 2.8.1 is not good for the current version of R

This is certainly not the case.
We have been using gcc 2.8.1 here (Solaris 2.5.x) for quite a while
with no problems.

    PaulG> 2/ my gcc 2.8.1 is not correctly configured

    PaulG> 3/ I need to be more careful to upgrade *all* parts of the
    PaulG> toolchain

    PaulG> My gut feeling is the same as Peter's (3/), but I haven't got a
    PaulG> clue how to track down the problem.

    PaulG> Paul

    PaulG> (PS. There also seem to be some problems making the f2c with gcc
    PaulG> 2.8.1. I've been using g77 so that isn't the problem.)

NOTE: Looking at  RHOME/library/base/R/base (which is the R file that is
	NOT correctly loaded by the internal C code),
      I now bet, that it is  .Alias(.)  which is failing;  more exactly,
      line 64
	------------------------------------------------------
	colours <- .Alias(colors)
	------------------------------------------------------
   
      That's (the only way?) how the error message
	------------------------------------------------------
	Error: Object "colors" not found 
	------------------------------------------------------
      can emerge...

[[this may help find gcc's problem, maybe...]]

Martin
-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From pgilbert@bank-banque-canada.ca  Tue Aug 11 15:59:51 1998
From: pgilbert@bank-banque-canada.ca (Paul Gilbert)
Date: Tue, 11 Aug 1998 10:59:51 -0400
Subject: R-beta: base not loading
References: <199808111434.QAA19036@sophie.ethz.ch>
Message-ID: <98Aug11.110432edt.13451@mailgate.bank-banque-canada.ca>

>PaulG> 1/ gcc 2.8.1 is not good for the current version of R

>This is certainly not the case.
>We have been using gcc 2.8.1 here (Solaris 2.5.x) for quite a while
>with no problems.

To do this I think you would have made gcc with Solaris 2.5 and we made it with
Solaris 2.6, so I don't think we can be certain that there are not problems with
gcc, in combination with Solaris 2.6 libraries, etc, etc.

Paul

-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From p.dalgaard@biostat.ku.dk  Sun Aug 16 11:36:50 1998
From: p.dalgaard@biostat.ku.dk (Peter Dalgaard BSA)
Date: 16 Aug 1998 12:36:50 +0200
Subject: R-beta: base not loading
In-Reply-To: Martin Maechler's message of Tue, 11 Aug 1998 16:34:30 +0200
References: <199808111434.QAA19036@sophie.ethz.ch>
Message-ID: <x27m091ad9.fsf@biostat.ku.dk>

Martin Maechler <maechler@stat.math.ethz.ch> writes:

>     PaulG> (PS. There also seem to be some problems making the f2c with gcc
>     PaulG> 2.8.1. I've been using g77 so that isn't the problem.)
> 
> NOTE: Looking at  RHOME/library/base/R/base (which is the R file that is
> 	NOT correctly loaded by the internal C code),
>       I now bet, that it is  .Alias(.)  which is failing;  more exactly,
>       line 64
> 	------------------------------------------------------
> 	colours <- .Alias(colors)
> 	------------------------------------------------------
>    
>       That's (the only way?) how the error message
> 	------------------------------------------------------
> 	Error: Object "colors" not found 
> 	------------------------------------------------------
>       can emerge...
> 
> [[this may help find gcc's problem, maybe...]]

While messing around with a Windows cross-compile, I realized that
this can also happen if colorstuff.R occurs before New-Internal.R in
the base file. The former has:

## nice to the English
colours <- colors

And the latter has

colors <- function().Internal(colors())
colours <- .Alias(colors)

Now, if ls gets the idea of sorting differently on some platforms,
which is not at all unlikely - Ripley reported something about locale
settings resulting in the collating sequence AaBbCc..., and he *is*
using Sol2.6! - then all hell breaks loose.

The thing in colorstuff.R is simply wrong, so I'll remove it.

PS: No, the 0.62.3 crosscompile doesn't work (yet?).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard@biostat.ku.dk)             FAX: (+45) 35327907
-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From mani@ee.iitb.ernet.in  Sun Aug 16 22:07:07 1998
From: mani@ee.iitb.ernet.in (R. MANIVASAKAN)
Date: Mon, 17 Aug 1998 02:37:07 +0530 (IST)
Subject: Bug tracking system
In-Reply-To: <x2yasoajuj.fsf@biostat.ku.dk>
Message-ID: <Pine.SUN.3.91.980817023536.21259A-100000@bhairav.ee.iitb.ernet.in>

Hi all,
	is there any code in R implimenting the Whittle's MLE estimator 
for Hurst parameter H of the given dataset?.

	thanks,
	mani.

-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From pgilbert@bank-banque-canada.ca  Mon Aug 17 17:13:28 1998
From: pgilbert@bank-banque-canada.ca (Paul Gilbert)
Date: Mon, 17 Aug 1998 12:13:28 -0400
Subject: R-beta: base not loading
References: <199808111434.QAA19036@sophie.ethz.ch> <x27m091ad9.fsf@biostat.ku.dk>
Message-ID: <98Aug17.122121edt.13448@mailgate.bank-banque-canada.ca>

> NOTE: Looking at  RHOME/library/base/R/base (which is the R file that is
>       NOT correctly loaded by the internal C code),
>       I now bet, that it is  .Alias(.)  which is failing;  more exactly,
>       line 64
>       colours <- .Alias(colors)

In my RHOME/library/base/R/base this is line 7105. Based on hints from Peter
Dalgaard and Brian Ripley I discovered that the setting of LC_COLLATE affects
the result of ls, but does not seem to be responsible for the very different
order in which things are arranged in RHOME/library/base/R/base. (Apparently the
setting of LC_COLLATE is part of a choice about locale made during the
installation of Solaris 2.6, so not everyone will have it set.)

One can
    unsetenv LC_COLLATE
which changes the result of ls, but on re-installing R this does not seem to
affect the order of things in RHOME/library/base/R/base (and I have still not
figured out why it is different between 2.5 and 2.6).

On the other hand, removing
    ## nice to the English
    colours <- colors
from RHOME/library/base/R/base seems to fix everything. This assignment seems
to be replicated by
   colours <- .Alias(colors)
and so I guess Peter's fix is the right one.

Paul Gilbert



-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From ihaka@stat.auckland.ac.nz  Mon Aug 17 21:32:43 1998
From: ihaka@stat.auckland.ac.nz (Ross Ihaka)
Date: Tue, 18 Aug 1998 08:32:43 +1200 (NZST)
Subject: Grammar Changes
Message-ID: <199808172032.IAA17509@stat1.stat.auckland.ac.nz>

I would like to make a couple of small changes to the R grammar.
At present, operators like ~, ==, !=, <, <=, >, >= are declared
to be non-associative.  This means that things like

	a < b <= c

produce a "syntax error" message when typed.

I would like to change the grammar so that these operators are
left-associative.  I want to make this change so that mathematical
annotation in the graphics will work a little better.

Interestingly, these operators are left-associative in S, so this
would be a move toward compatibility.

Does anyone have any strong thoughts on this?

	Ross
-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From p.dalgaard@biostat.ku.dk  Mon Aug 17 22:32:50 1998
From: p.dalgaard@biostat.ku.dk (Peter Dalgaard BSA)
Date: 17 Aug 1998 23:32:50 +0200
Subject: Grammar Changes
In-Reply-To: Ross Ihaka's message of Tue, 18 Aug 1998 08:32:43 +1200 (NZST)
References: <199808172032.IAA17509@stat1.stat.auckland.ac.nz>
Message-ID: <x2ogtjgupp.fsf@biostat.ku.dk>

Ross Ihaka <ihaka@stat.auckland.ac.nz> writes:

> 
> I would like to make a couple of small changes to the R grammar.
> At present, operators like ~, ==, !=, <, <=, >, >= are declared
> to be non-associative.  This means that things like
> 
> 	a < b <= c
> 
> produce a "syntax error" message when typed.
> 
> I would like to change the grammar so that these operators are
> left-associative.  I want to make this change so that mathematical
> annotation in the graphics will work a little better.
> 
> Interestingly, these operators are left-associative in S, so this
> would be a move toward compatibility.
> 
> Does anyone have any strong thoughts on this?

Sounds harmless enough to me. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard@biostat.ku.dk)             FAX: (+45) 35327907
-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From Martin Maechler <maechler@stat.math.ethz.ch>  Tue Aug 18 14:13:46 1998
From: Martin Maechler <maechler@stat.math.ethz.ch> (Martin Maechler)
Date: Tue, 18 Aug 1998 15:13:46 +0200
Subject: Problem in "configure" for Solaris (cc) ?!
Message-ID: <199808181313.PAA28150@sophie.ethz.ch>

	[[0.62.3, already 0.62.2]]

This bug report is overdue,
but I really didn't test these things for weeks 
(have always used gcc, but only yesterday, someone told me that he saw a
speed gain of a factor 2 when using Sun's cc over gcc)

I just found that the same problem is already in 0.62.2

If I take yesterday's  
	R-release.tar.gz  (or also R-0.62.2.tar.gz)
unpack
and add
	CC=cc
	-----
to  config.site,
then
	./configure
	-----------
it ends VERY badly:

% ./configure
creating cache ./config.cache
checking for a BSD compatible install... /usr/local/bin/ginstall -c
checking whether ln -s works... yes
checking for ranlib... ranlib
checking for bison... bison -y
checking for ar... ar
checking for ratfor... ratfor
checking for latex... /afs/ethz.ch/public/teTeX/teTeX/bin/sparc-solaris2.5/latex
checking for dvips... /afs/ethz.ch/public/teTeX/teTeX/bin/sparc-solaris2.5/dvips
checking for perl... /usr/local/bin/perl
checking whether perl is perl 5... yes
checking for cc... cc
checking for ranlib... (cached) ranlib
checking for f77... f77
checking for underscore after Fortran symbols... configure: error: Nothing worked - cannot use FORTRAN

-----------
and then  STOP, nothing, ....

When I omit the "CC=cc", gcc is used (together with f77), and all is well.

  % cc -V
  cc: SC3.0.1 13 Jul 1994
  usage: cc [ options] files.  Use 'cc -flags' for details

  % f77 -V
  f77: SC3.0.1 13 Jul 1994
  Usage: f77 [ options ] files.  Use 'f77 -flags' for details

  % uname -a
  SunOS florence 5.5 Generic sun4u sparc SUNW,Ultra-1

------------------------------------------------

So yes, the  SC.. compilers are somewhat dated, but still...

-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From Martin Maechler <maechler@stat.math.ethz.ch>  Tue Aug 18 14:54:22 1998
From: Martin Maechler <maechler@stat.math.ethz.ch> (Martin Maechler)
Date: Tue, 18 Aug 1998 15:54:22 +0200
Subject: Problem in "configure" for Solaris (cc) -- solved (partly) --
In-Reply-To: <199808181313.PAA28150@sophie.ethz.ch> (message from Martin
 Maechler on Tue, 18 Aug 1998 15:13:46 +0200)
Message-ID: <199808181354.PAA01678@sophie.ethz.ch>

(answering myself, after a suggestion by Kurt Hornik
		 which actually was *not* the solution.. but...)

The configure problem I've just reported
(( setting   CC=cc  in  config.site ))

actually does not show when I either leave the CFLAGS commented out
(which results in CFLAGS=-g) or set CFLAGS="-g -O",

however, it *DOES* show when I set
	CFLAGS=-O2 or CFLAGS="-g -O2"

----

Reading the long output of  ``cc -flags'',
I see that more than average optimization is done using
	cc -xO[1-4]
	    #
Still very funny that "-O2" leads to such bad results with ./configure.
Maybe configure could become smarter here...

Martin
-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From p.dalgaard@biostat.ku.dk  Tue Aug 18 15:25:20 1998
From: p.dalgaard@biostat.ku.dk (Peter Dalgaard BSA)
Date: 18 Aug 1998 16:25:20 +0200
Subject: Problem in "configure" for Solaris (cc) -- solved (partly) --
In-Reply-To: Martin Maechler's message of Tue, 18 Aug 1998 15:54:22 +0200
References: <199808181354.PAA01678@sophie.ethz.ch>
Message-ID: <x2lnom2wq7.fsf@biostat.ku.dk>

Martin Maechler <maechler@stat.math.ethz.ch> writes:

> Reading the long output of  ``cc -flags'',
> I see that more than average optimization is done using
> 	cc -xO[1-4]
> 	    #
> Still very funny that "-O2" leads to such bad results with ./configure.
> Maybe configure could become smarter here...

This is a generic problem with autoconf: It goes looking for X, the
compile doesn't work because of Y, and it reports that X is missing.
Of course, one should always check config.log when something goes
wrong, it contains the details of the compiler failure and the failed
programs. 

Our configure starts off with a compiler sanity check, but apparently
it isn't enough?


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard@biostat.ku.dk)             FAX: (+45) 35327907
-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From bates@stat.wisc.edu  Tue Aug 18 15:45:22 1998
From: bates@stat.wisc.edu (Douglas Bates)
Date: 18 Aug 1998 09:45:22 -0500
Subject: Problem in "configure" for Solaris (cc) -- solved (partly) --
In-Reply-To: Peter Dalgaard BSA's message of "18 Aug 1998 16:25:20 +0200"
References: <199808181354.PAA01678@sophie.ethz.ch> <x2lnom2wq7.fsf@biostat.ku.dk>
Message-ID: <6rww86nybh.fsf@verdi.stat.wisc.edu>

>>>>> "PD" == Peter Dalgaard BSA <p.dalgaard@biostat.ku.dk> writes:

  PD> This is a generic problem with autoconf: It goes looking for X,
  PD> the compile doesn't work because of Y, and it reports that X is
  PD> missing.  Of course, one should always check config.log when
  PD> something goes wrong, it contains the details of the compiler
  PD> failure and the failed programs.

I have seen another such problem.  The check for -lblas is done with
the C compiler invoked on a short C program stub.  For me the blas
library is found but the program does not link properly because of
missing Fortran library routines.  Configure then decides that there
is no blas library present.
-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From Prof Brian Ripley <ripley@stats.ox.ac.uk>  Tue Aug 18 18:28:50 1998
From: Prof Brian Ripley <ripley@stats.ox.ac.uk> (Prof Brian Ripley)
Date: Tue, 18 Aug 1998 18:28:50 +0100 (BST)
Subject: Problem in "configure" for Solaris (cc) -- solved (partly) --
Message-ID: <199808181728.SAA04981@toucan.stats.ox.ac.uk>

> From: Peter Dalgaard BSA <p.dalgaard@biostat.ku.dk>
> 
> Martin Maechler <maechler@stat.math.ethz.ch> writes:
> 
> > Reading the long output of  ``cc -flags'',
> > I see that more than average optimization is done using
> > 	cc -xO[1-4]
> > 	    #

And -O is -xO2. If you do want higher speed, you need to use
other flags too, and -fast is a good start.

> > Still very funny that "-O2" leads to such bad results with ./configure.
> > Maybe configure could become smarter here...
> 
> This is a generic problem with autoconf: It goes looking for X, the
> compile doesn't work because of Y, and it reports that X is missing.
> Of course, one should always check config.log when something goes
> wrong, it contains the details of the compiler failure and the failed
> programs. 

Not this time. See below.

> Our configure starts off with a compiler sanity check, but apparently
> it isn't enough?

No, as it is not invoked if the compiler is specified via CC. In
Martin's configuration, config.log gives

$ CC=cc CFLAGS=-O2 ./configure
$ cat config.log
This file contains any messages produced by compilers while
running configure, to aid debugging if configure makes a mistake.

configure:708: checking for a BSD compatible install
configure:758: checking whether ln -s works
configure:781: checking for ranlib
configure:812: checking for bison
configure:845: checking for ar
configure:873: checking for ratfor
configure:902: checking for latex
configure:934: checking for dvips
configure:974: checking for perl
configure:1004: checking whether perl is perl 5
configure:1200: checking for cc
configure:1261: checking for ranlib
configure:1325: checking for f77
configure:1402: checking for underscore after Fortran symbols

Note, no details of the compiler failure appear ....


As for speed comparisons:

A piece of the ch14 script from V&R2 (highly C-dependent), Solaris 2.6, 
Sparc 20 (about 4 years old)

library(MASS)
library(tree)
data(fgl)
unix.time({
fgl.tr <- tree(type ~ ., fgl)
fgl.cv <- cv.tree(fgl.tr,, prune.misclass)
for(i in 2:5)  fgl.cv$dev <- fgl.cv$dev + 
     cv.tree(fgl.tr,, prune.misclass)$dev
fgl.cv$dev <- fgl.cv$dev/5})

gcc2.8.1 60.26secs
cc SC4.0 60.35secs

(the close agreement is fortuitous: I had to tune each to get about
10% improvement in speed).

Brian

-- 
Brian D. Ripley,                  ripley@stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From p.dalgaard@biostat.ku.dk  Tue Aug 18 20:39:34 1998
From: p.dalgaard@biostat.ku.dk (Peter Dalgaard BSA)
Date: 18 Aug 1998 21:39:34 +0200
Subject: Problem in "configure" for Solaris (cc) -- solved (partly) --
In-Reply-To: Prof Brian Ripley's message of Tue, 18 Aug 1998 18:28:50 +0100 (BST)
References: <199808181728.SAA04981@toucan.stats.ox.ac.uk>
Message-ID: <x2n292xeo9.fsf@biostat.ku.dk>

Prof Brian Ripley <ripley@stats.ox.ac.uk> writes:

> $ CC=cc CFLAGS=-O2 ./configure
> $ cat config.log
> This file contains any messages produced by compilers while
> running configure, to aid debugging if configure makes a mistake.
> 
> configure:708: checking for a BSD compatible install
...
> 
> Note, no details of the compiler failure appear ....

Hmmm. Yes. Easily reproduced on other systems by using:

CC=/bin/false ./configure

The basic problem here is that this part of configure.in is handwritten,
and doesn't use the "autoconf conventions" (which are probably as
well-documented as certain aspects of R...) of sending the compile
commands and in the case of failure also the program contents to file
descriptor 5.  I.e. what yields all the constructions of the type

if { (eval echo configure:1475: \"$ac_link\") 1>&5; (eval $ac_link)
2>&5; } && test -s conftest; then
  rm -rf conftest*
  eval "ac_cv_lib_$ac_lib_var=yes"
else
  echo "configure: failed program was:" >&5
  cat conftest.$ac_ext >&5
  rm -rf conftest*
  eval "ac_cv_lib_$ac_lib_var=no"
fi

There must be a relevant macro for this somewhere?

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard@biostat.ku.dk)             FAX: (+45) 35327907
-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From p.dalgaard@biostat.ku.dk  Wed Aug 19 15:01:05 1998
From: p.dalgaard@biostat.ku.dk (Peter Dalgaard BSA)
Date: 19 Aug 1998 16:01:05 +0200
Subject: R 0.62.3 to be released soon
Message-ID: <x2n291xe8u.fsf@blueberry.kubism.ku.dk>

For the (slightly) adventurous among you:

We plan to roll out R 0.62.3 next Friday. In order to root out
remaining bugs before the release, I'd like to encourage you to try it
out on your system. 

Since there is a risk of destabilising things whenever you mess with
the configure scripts, we won't attack installation problems on
unusual platforms after this week.

We also won't take on any complicated bugs, unless they have very bad
effects. (This is a good time to pick nits...)

Note that the "horses mouth" for snapshot releases of R is 

ftp.stat.math.ethz.ch:/pub/Software/R/

The daily snapshots are also mirrored to the CRAN sites, but not
always on a daily basis.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard@biostat.ku.dk)             FAX: (+45) 35327907
-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From pgilbert@bank-banque-canada.ca  Wed Aug 19 19:36:09 1998
From: pgilbert@bank-banque-canada.ca (Paul Gilbert)
Date: Wed, 19 Aug 1998 14:36:09 -0400
Subject: R 0.62.3 to be released soon
References: <x2n291xe8u.fsf@blueberry.kubism.ku.dk>
 <98Aug19.120624edt.13485@mailgate.bank-banque-canada.ca> <x2d89xx7sp.fsf@blueberry.kubism.ku.dk>
Message-ID: <98Aug19.144503edt.13473@mailgate.bank-banque-canada.ca>

I've compiled and installed pre R 0.62.3 (Aug. 19) under Solaris 2.6 using gcc
version 2.8.1 and g77 0.5.23. Everything went smoothly and my DSE library tests
work ok. There is a very occasional problem with a "Segmentation Fault - core
dumped", which also occured in 0.62.2, but I have not been able to reproduce it
reliably. I also had to increase R -v from 250000 cells (which worked with
0.62.2) to 300000 cells for one of my tests.

There is however, a small problem with installing the HTML documention in a
private (i.e. not RHOME/library) location. The file

   RHOME/doc/html/packages.html

linking the main documention to the packages is written (BTW this will probably
fail if the package owner and the R owner are not the same) but the links in it
are not quite correct. For me the link indicated in that file is

  <A HREF="../../library/dse/html/00Index.html">
which would be RHOME/library/dse/html/00Index.html, but it should be
 <A HREF="{package_location}/dse/html/00Index.html">

Paul Gilbert

-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From pgilbert@bank-banque-canada.ca  Thu Aug 20 19:57:57 1998
From: pgilbert@bank-banque-canada.ca (Paul Gilbert)
Date: Thu, 20 Aug 1998 14:57:57 -0400
Subject: R 0.62.3 to be released soon
Message-ID: <98Aug20.145941edt.13442@mailgate.bank-banque-canada.ca>

Installing a package with documentation seems to clobber the file

RHOME/doc/html/function.html

which has the links to all the HTML  function documentation, so access to the R
base documentation is almost impossible.

Paul Gilbert

-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From p.dalgaard@biostat.ku.dk  Thu Aug 20 22:28:07 1998
From: p.dalgaard@biostat.ku.dk (Peter Dalgaard BSA)
Date: 20 Aug 1998 23:28:07 +0200
Subject: R 0.62.3 to be released soon
In-Reply-To: Paul Gilbert's message of Thu, 20 Aug 1998 14:57:57 -0400
References: <98Aug20.145941edt.13442@mailgate.bank-banque-canada.ca>
Message-ID: <x2r9ybibrs.fsf@biostat.ku.dk>

Paul Gilbert <pgilbert@bank-banque-canada.ca> writes:

> 
> Installing a package with documentation seems to clobber the file
> 
> RHOME/doc/html/function.html
> 
> which has the links to all the HTML  function documentation, so access to the R
> base documentation is almost impossible.
> 
> Paul Gilbert

I took the liberty of forwarding this to r-bugs and ditto with your
note about 'private' libraries.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard@biostat.ku.dk)             FAX: (+45) 35327907
-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From p.dalgaard@biostat.ku.dk  Fri Aug 21 15:30:25 1998
From: p.dalgaard@biostat.ku.dk (Peter Dalgaard BSA)
Date: 21 Aug 1998 16:30:25 +0200
Subject: R 0.62.3 to be released soon
In-Reply-To: Paul Gilbert's message of Fri, 21 Aug 1998 10:04:14 -0400
References: <98Aug20.145941edt.13442@mailgate.bank-banque-canada.ca>
 <x2r9ybibrs.fsf@biostat.ku.dk>
 <98Aug21.100624edt.13448@mailgate.bank-banque-canada.ca>
Message-ID: <x2btpe9zlq.fsf@blueberry.kubism.ku.dk>

Paul Gilbert <pgilbert@bank-banque-canada.ca> writes:

> 
> >I took the liberty of forwarding this to r-bugs and ditto with your
> >note about 'private' libraries.
> 
> Peter
> 
> Please remind me again what r-bugs is. When should I be posting there instead?

Sorry, I was very tired when I wrote that...

R-bugs@biostat.ku.dk is the mail interface to our new bug-tracking
system. So what I did was to file the letters as official bug reports.
There was no error on your behalf, we still need to figure out what
the practical way of using this system really is. 

The purpose of forwarding the notes was just to get them filed so that
we won't (easily) forget about them. I did it using 'resend' so they
appear to come from you, and I though I'd better tell you.

Also, I was testing some parts of the interface at the same time,
namely what happens with the e-mail notification of the core team when
you do a forward like that. (It turns out that it recognises the 'Re:'
string and skips the notification, which would seem to be a bit of a
bug in the bug tracking system...)

(Note the cc: to r-devel in *this* note, by the way)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard@biostat.ku.dk)             FAX: (+45) 35327907
-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From pgilbert@bank-banque-canada.ca  Fri Aug 21 17:19:13 1998
From: pgilbert@bank-banque-canada.ca (Paul Gilbert)
Date: Fri, 21 Aug 1998 12:19:13 -0400
Subject: couldn't find FUN
Message-ID: <98Aug21.122130edt.13441@mailgate.bank-banque-canada.ca>

The call to sweep in this function which was working in 0.62.2 is giving me
trouble in 62.3:

prcomponents <- function(x, center=TRUE, scale=TRUE, N=nrow(x)-1)
   {if (center) center <- apply(x,2,mean)
    else        center <- rep(0, ncol(x))
    if (scale)  scale  <- sqrt(apply(x,2,var))
    else        scale  <- rep(1, ncol(x))
    s <- svd(sweep(sweep(x,2, center),2, scale, FUN="/"))
    # remove anything corresponding to effectively zero singular values.
    rank <- sum(s$d > (s$d[1]*sqrt(.Machine$double.eps)))
    if (rank < ncol(x)) s$v <- s$v[,1:rank, drop=FALSE]
    s$d <- s$d/sqrt(max(1,N))

#   r <- list(sdev=s$d, proj=s$v,x=x %*% s$v, center=center, scale=scale)
    r <- list(sdev=s$d, proj=s$v, center=center, scale=scale)
    r
}

> data(crimes)
> prcomponents(crimes)
Error: couldn't find function "FUN"
> traceback()
[1] "eval(f)"
[2] "Ops.data.frame(x, aperm(array(STATS, dims[perm]), order(perm)), "
[3] "\t    ...)"
[4] "sweep(x, 2, center)"
[5] "sweep(sweep(x, 2, center), 2, scale, FUN = \"/\")"
[6] "svd(sweep(sweep(x, 2, center), 2, scale, FUN = \"/\"))"
[7] "prcomponents(crimes)"
>

Paul Gilbert

-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From pgilbert@bank-banque-canada.ca  Fri Aug 21 18:30:23 1998
From: pgilbert@bank-banque-canada.ca (Paul Gilbert)
Date: Fri, 21 Aug 1998 13:30:23 -0400
Subject: couldn't find FUN
References: <98Aug21.122130edt.13441@mailgate.bank-banque-canada.ca>
Message-ID: <98Aug21.133236edt.13450@mailgate.bank-banque-canada.ca>

>> data(crimes)
>> prcomponents(crimes)
>Error: couldn't find function "FUN"

This does not work in 0.62.2 as I previously reported. The error is associated
with using
data(crimes) which is not what I've usually done. It is fixed by replacing

   s <- svd(sweep(sweep(x,2, center),2, scale, FUN="/"))

with

   s <- svd(sweep(sweep(as.matrix(x),2, center),2, scale, FUN="/"))

So, I'm not sure if this is a bug or if this is the way it should work? For my
purposes it is fixed already.

Paul Gilbert



-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From ripley@stats.ox.ac.uk  Fri Aug 21 18:37:01 1998
From: ripley@stats.ox.ac.uk (Prof Brian D Ripley)
Date: Fri, 21 Aug 1998 18:37:01 +0100 (BST)
Subject: couldn't find FUN
In-Reply-To: <98Aug21.122130edt.13441@mailgate.bank-banque-canada.ca>
Message-ID: <Pine.GSO.3.96.980821182855.4962E-100000@localhost>

On Fri, 21 Aug 1998, Paul Gilbert wrote:

> The call to sweep in this function which was working in 0.62.2 is giving me
> trouble in 62.3:

The call is invalid:

> ?sweep

Sweep out Array Summaries

     sweep(x, MARGIN, STATS, FUN="-", ...)

Arguments:

       x: an array.

> class(crimes)
[1] "data.frame"

If you call sweep on a data frame, as in

> sweep(crimes, 2, "-")

it does not work: it should not have done so before.  On the other hand, in
0.62.2, valid arithmetic operations on data frames gave matrices,
which helped your illegal example but were incorrect on legal ones.
You need an as.matrix in your code, I believe.


This example also fails in S.

-- 
Brian D. Ripley,                  ripley@stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From p.dalgaard@biostat.ku.dk  Fri Aug 21 23:34:02 1998
From: p.dalgaard@biostat.ku.dk (Peter Dalgaard BSA)
Date: 22 Aug 1998 00:34:02 +0200
Subject: couldn't find FUN
In-Reply-To: Prof Brian D Ripley's message of Fri, 21 Aug 1998 18:37:01 +0100 (BST)
References: <Pine.GSO.3.96.980821182855.4962E-100000@localhost>
Message-ID: <x2ww82ars5.fsf@biostat.ku.dk>

Prof Brian D Ripley <ripley@stats.ox.ac.uk> writes:

> 
> On Fri, 21 Aug 1998, Paul Gilbert wrote:
> 
> > The call to sweep in this function which was working in 0.62.2 is giving me
> > trouble in 62.3:
..
> 
> If you call sweep on a data frame, as in
> 
> > sweep(crimes, 2, "-")

Um, you need some STATS in there...

> 
> it does not work: it should not have done so before.  On the other hand, in
> 0.62.2, valid arithmetic operations on data frames gave matrices,
> which helped your illegal example but were incorrect on legal ones.
> You need an as.matrix in your code, I believe.
> 
> 
> This example also fails in S.

Still, something weird is going on. First of all, the error message
about not finding "FUN" is less than obvious and secondly, it's not in
general a problem to do arithmetic on data frames as if they were
matrices. (E.g. crimes - as.matrix(crimes)) I think we have some of
the explanation in:

> f<-get("-")                         
> f(crimes,as.matrix(crimes))
Warning: ignored non function "f"
Warning: ignored non function "f"
Warning: ignored non function "f"
Warning: ignored non function "f"
               Murder Assault UrbanPop Rape
Alabama             0       0        0    0
Alaska              0       0        0    0
Arizona             0       0        0    0

i.e. "something" applies "f" for each column and there's some kind of
mess related to scoping (there really is no other "f" around before
the function is called!)

It happens only with .Primitive functions like "-". Stuff like

sweep(crimes, 2, apply(crimes, 2, mean), function(x, y) x - y)

works perfectly.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard@biostat.ku.dk)             FAX: (+45) 35327907
-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From ripley@stats.ox.ac.uk  Sat Aug 22 18:06:00 1998
From: ripley@stats.ox.ac.uk (Prof Brian D Ripley)
Date: Sat, 22 Aug 1998 18:06:00 +0100 (BST)
Subject: Handling of offsets in glm is really inconsistent.
Message-ID: <Pine.GSO.3.96.980822172731.6251A-100000@localhost>

[Copied to R-devel for information]

This applies to all versions of R I have: 0.62.2, 0.62.3, 0.63.
Great care seems needed with glms with offsets, as many things seem
wrong.

Consider the following:

> data(freeny)
> freeny.glm <- glm(y ~ offset(lag.quarterly.revenue) + price.index +
    income.level + market.potential, data=freeny, subset=1:30)
> predict(freeny.glm)
            Qtr1       Qtr2       Qtr3       Qtr4
1962:         NA 0.01040457 0.01073223 0.01233351
1963: 0.01211730 0.02744293 0.03259214 0.03225403
1964: 0.03235081 0.03272723 0.03361984 0.03114351
1965: 0.03128098 0.03371820 0.02869783 0.02670215
1966: 0.02346093 0.02172193 0.02269731 0.01927851
1967: 0.02516107 0.02496217 0.02679227 0.03099460
1968: 0.03225807 0.03418791 0.03857815 0.03324247
1969: 0.02575733 0.02702418 0.02988582         NA
> predict(freeny.glm, type="response")
          Qtr1     Qtr2     Qtr3     Qtr4
1962:       NA 8.806765 8.803092 8.803704
1963: 8.826977 8.840453 8.940102 8.968984
1964: 8.993961 8.993167 9.042300 9.061634
1965: 9.100341 9.092428 9.135678 9.153552
1966: 9.194421 9.208372 9.260927 9.284149
1967: 9.309521 9.338742 9.377042 9.389345
1968: 9.429928 9.455688 9.480808 9.520452
1969: 9.549497 9.566824 9.611116       NA

so one might think that prediction of a glm with an offset should
include the offset on the response scale but not link scale. However,

> predict(freeny.glm, newdata=freeny)
   1962.25     1962.5    1962.75       1963    1963.25 
0.01040457 0.01073223 0.01233351 0.01211730 0.02744293 
    1963.5    1963.75       1964    1964.25     1964.5 
0.03259214 0.03225403 0.03235081 0.03272723 0.03361984 
   1964.75       1965    1965.25     1965.5    1965.75 
0.03114351 0.03128098 0.03371820 0.02869783 0.02670215 
      1966    1966.25     1966.5    1966.75       1967 
0.02346093 0.02172193 0.02269731 0.01927851 0.02516107 
   1967.25     1967.5    1967.75       1968    1968.25 
0.02496217 0.02679227 0.03099460 0.03225807 0.03418791 
    1968.5    1968.75       1969    1969.25     1969.5 
0.03857815 0.03324247 0.02575733 0.02702418 0.02988582 
   1969.75       1970    1970.25     1970.5    1970.75 
0.02686281 0.03816228 0.03910487 0.04325638 0.03599068 
      1971    1971.25     1971.5    1971.75 
0.03357946 0.03890549 0.04196893 0.03952385 

> predict(freeny.glm, newdata=freeny, type="response")
   1962.25     1962.5    1962.75       1963    1963.25 
0.01040457 0.01073223 0.01233351 0.01211730 0.02744293 
    1963.5    1963.75       1964    1964.25     1964.5 
0.03259214 0.03225403 0.03235081 0.03272723 0.03361984 
   1964.75       1965    1965.25     1965.5    1965.75 
0.03114351 0.03128098 0.03371820 0.02869783 0.02670215 
      1966    1966.25     1966.5    1966.75       1967 
0.02346093 0.02172193 0.02269731 0.01927851 0.02516107 
   1967.25     1967.5    1967.75       1968    1968.25 
0.02496217 0.02679227 0.03099460 0.03225807 0.03418791 
    1968.5    1968.75       1969    1969.25     1969.5 
0.03857815 0.03324247 0.02575733 0.02702418 0.02988582 
   1969.75       1970    1970.25     1970.5    1970.75 
0.02686281 0.03816228 0.03910487 0.04325638 0.03599068 
      1971    1971.25     1971.5    1971.75 
0.03357946 0.03890549 0.04196893 0.03952385 

and prediction on either scale with newdata ignores the offset. Now, S is
also inconsistent (prior to S-PLUS 4.5 rel2), but at least it does usually
include the offset (except for prediction on link scale with no newdata,
where the offset was omitted until recently). [The different print layout
is due to the original response being a time series of class "ts"; the
predict method cannot know that.]

The discrepancy is in predict.lm (not predict.glm) which ignores offsets in
R but includes them in S. Correcting it is made difficult by the way
delete.response also deletes offsets in R: 

> terms(freeny.glm)
y ~ offset(lag.quarterly.revenue) + price.index + income.level + 
            market.potential
attr(,"variables")
list(y, offset(lag.quarterly.revenue), price.index, income.level, 
            market.potential)
attr(,"factors")
                              price.index income.level market.potential
y                                       0            0                0
offset(lag.quarterly.revenue)           0            0                0
price.index                             1            0                0
income.level                            0            1                0
market.potential                        0            0                1
attr(,"term.labels")
[1] "price.index"      "income.level"     "market.potential"
attr(,"order")
[1] 1 1 1
attr(,"intercept")
[1] 1
attr(,"response")
[1] 1
attr(,"offset")
[1] 2

> delete.response(terms(freeny.glm))
~price.index + income.level + market.potential
attr(,"variables")
list(price.index, income.level, market.potential)
attr(,"factors")
                 price.index income.level market.potential
price.index                1            0                0
income.level               0            1                0
market.potential           0            0                1
attr(,"term.labels")
[1] "price.index"      "income.level"     "market.potential"
attr(,"order")
[1] 1 1 1
attr(,"intercept")
[1] 1
attr(,"response")
[1] 0

and that is definitely not what is documented to happen.  I do not begin to
understand why delete.response is written the way it is: it looks like it
needs a complete re-design. 

So

- delete.response needs to only delete responses.
- predict.lm needs to handle offsets (or predict.glm, but it is
  much cleaner in predict.lm).
- glm.fit should return  linear.predictors = eta + offset.

There is another small problem:

> predict(freeny.glm, se=T)
Error: Object "price.index" not found

as predict.lm does not recognize that newdata was missing in the caller
(how lazy is lazy evaluation?)  The simplest way out is to have

   if (missing(newdata) || is.null(newdata)) X <- model.matrix(object)

the alternative is to test missing(newdata) in predict.glm.

While this is being done, I wonder why R does not implement offsets
for lm()?  It `looks like an easy exercise'.

-- 
Brian D. Ripley,                  ripley@stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From pgilbert@bank-banque-canada.ca  Mon Aug 24 14:24:17 1998
From: pgilbert@bank-banque-canada.ca (Paul Gilbert)
Date: Mon, 24 Aug 1998 09:24:17 -0400
Subject: R-beta: re -n -v wr0613b - windows dynload
References: <35DE9A90.166E324E@mail1.stofanet.dk>
Message-ID: <98Aug24.092850edt.13455@mailgate.bank-banque-canada.ca>

>When I use the -v I can modify the size of the heap, as assessed by
>gc(), but the -n key seems to be without effect ?

In src/include/Defn.h the following are set and if you exceed these values I
believe your setting is ignored. (I don't think there is any warning but it
would be nice.)

#ifndef R_NSIZE
#define R_NSIZE  200000L  /* number of cons cells */
#endif

#ifndef R_VSIZE
#define R_VSIZE  2000000L /* vector heap size in bytes */
#endif

Paul Gilbert

-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From pgilbert@bank-banque-canada.ca  Mon Aug 24 16:08:38 1998
From: pgilbert@bank-banque-canada.ca (Paul Gilbert)
Date: Mon, 24 Aug 1998 11:08:38 -0400
Subject: prcomp and princomp
Message-ID: <98Aug24.111223edt.13450@mailgate.bank-banque-canada.ca>

Below are mva functions prcomp and princomp as well as my own version
prcomponents. Also included are contents for prcomp.Rd and princomp.Rd.
Functions prcomp and princomp are intended to replicate Splus results. (I think
this princomp is already in the R distribution, but the version of prcomp
already in the R distribution does not reproduce Splus results.)

My version, prcomponent, is preliminary and as yet undocumented, but I would
appreciate comments. It uses the preferred svd as in prcomp, not eigen as in
princomp,  but returns some extra information as in princomp. Following Bill
Venables suggestion, it also allows the user to specify the effective scaling
factor (e.g. N or N-1).

Also, I have just noticed that there is some very small duplication
(print.princomp, summary.princomp, plot.princomp) between the functions  below
and the file princomp-add.R already in the R distribution. If someone could pick
the best versions I would appreciate it. I am about to get buried with some
other work and would like to get this in the R distribution before it gets lost
on my desk (again).

Paul Gilbert

############## prcomp ##############

prcomp <- function(x, retx=TRUE, center=TRUE, scale=FALSE) {
# s <- svd(scale(x, center=center, scale=scale),nu=0)
# above produces warning since scale is both a function and a variable so
 s <- svd(get("scale",envir=.GlobalEnv)
   (x, center=center, scale=scale),nu=0)
# rank <- sum(s$d > 0)
 rank <- sum(s$d > (s$d[1]*sqrt(.Machine$double.eps)))
 if (rank < ncol(x)) s$v <- s$v[,1:rank, drop=F]
 s$d <- s$d/sqrt(max(1,nrow(x) -1))
 if(retx) r <- list(sdev=s$d, rotation=s$v, x=as.matrix(x) %*% s$v)
 else     r <- list(sdev=s$d, rotation=s$v)
 class(r) <- "prcomp"
 r
}

print.prcomp <- function(x) {
 cat("Standard deviations:\n")
 print(x$sdev)
 cat("\nRotation:\n")
 print(x$rotation)
 if (!is.null(x$x))
  {cat("\nRotated variables:\n")
   print(x$x)
  }
 invisible(x)
}

plot.prcomp <- function(obj, x=1, y=2, main="Scree Plot",
  xlab=paste("Principle component", x),
  ylab=paste("Principle component", y), ...) {
    if (is.null(obj$x))
 stop("Rotated x has not be calculated by prcomp. Use retx=T in prcomp.")
    plot(obj$x[,c(x,y)], main=main, xlab=xlab, ylab=ylab, ...)
}

############### prcomp.Rd ###############

\name{prcomp}
\title{Principal Components Analysis}
\usage{
prcomp(x, retx=TRUE, center=TRUE, scale=FALSE)

}
\arguments{
\item{x}{a matrix (or data frame) which provides the data
for the principal components analysis.}
\item{retx}{a logical value indicating whether the rotated variables should
be returned.}
\item{center}{a logical value indicating whether the variables should
be shifted to be zero centered.}
\item{scale}{a logical value indicating whether the variables should
be scaled to have unit variance before the analysis takes place. The default
is F for consistency with S, but in general scaling is advisable.}
}
\description{
This function performs a principal components analysis on
the given data matrix and returns the results as a
\code{prcomp} object. The calculation is done with svd on the data matrix, not
by using eigen on the covariance matrix. This is generally the preferred method
for numerical accuracy.  The print method for the these
objects prints the results in a nice format and the
plot method produces a scree plot.
}
\value{
\code{prcomp} returns an list with class \code{"prcomp"}
containing the following components:
\item{sdev}{the standard deviation of the principal components
(i.e. the eigenvalues of the cov matrix - though the calculation is actually
done with the singular values of the data matrix.)}
\item{rotation}{the matrix of variable loadings (i.e. a matrix
whose columns contain the eigenvectors). The function princomp returns this
in the element \code{loadings}.}
\item{x}{if retx is true the value of the rotated data (the data multiplied by
the \code{rotation} matrix) is returned.}
}
\references{
Mardia, K. V., J. T. Kent, J and M. Bibby (1979),
\emph{Multivariate Analysis}, London: Academic Press.

Venerables, W. N. and B. D. Ripley (1994).
\emph{Modern Applied Statistics with S-Plus}, Springer-Verlag.
}

\seealso{
\code{\link{princomp}}, \code{\link{prcomponents}}, \code{\link{cor}},
\code{\link{cov}},
\code{\link{svd}}, \code{\link{eigen}}.
}
\examples{
# the variances of the variables in the
# crimes data vary by orders of magnitude
data(crimes)
prcomp(crimes)
prcomp(crimes,scale=TRUE)
}




############## princomp ##############

princomp <- function(x, cor=FALSE, scores=TRUE,
  subset=rep(TRUE, nrow(as.matrix(x)))) {
 # it is tempting to add  use="all.obs" which could be passed to cov or
 # cor but then the calculation of N is complicated.
 z<- as.matrix(x)[subset,, drop=F]
 N <- nrow(z)
 if(cor) cv <- get("cor",envir=.GlobalEnv)(z)
 else    cv <- cov(z)
#  (svd can be used but gives different signs for some vectors)
 edc <- eigen(cv)
 cn <- paste("Comp.", 1:ncol(cv), sep="")
 names(edc$values) <- cn
 dimnames(edc$vectors) <- list(dimnames(x)[[2]], cn)
 scr<- NULL
 if (cor)
   {sdev <- sqrt(edc$values)
    sc <- (apply(z,2,var)*(N-1)/N)^0.5
    if (scores)
        scr<-(scale(z,center=T,scale=T) %*% edc$vectors)*sqrt(N/(N-1))
   }
 else
   {sdev <- sqrt(edc$values*(N-1)/N)
    sc <- rep(1, ncol(z))
    if (scores)
        scr<- (scale(z,center=T,scale=F) %*% edc$vectors)
   }
 names(sc) <- dimnames(x)[[2]]
 edc <-list(sdev=sdev, loadings=edc$vectors,
     center=apply(z,2,mean), scale=sc, n.obs=N, scores=scr)
# The Splus function also return list elements factor.sdev, correlations
# and coef, but these are not documented in the help. coef seems to equal
# load. The Splus function also return list elements call and terms which
# are not supported here.
 class(edc) <- "princomp"
 edc
}

print.princomp <- function(x) {
 cat("Standard deviations:\n")
 print(x$sdev)
 cat(length(x$scale), " variables and ", x$n.obs, "observations.\n")
 cat("Scale:\n")
 print(x$scale)
 invisible(x)
}

summary.princomp <- function(x) {
 per.var <- (x$sdev^2)/sum(x$sdev^2)
 r <- list(sdev=x$sdev, per.var= per.var, cum.var=cumsum(per.var))
 class(r) <- "summary.princomp"
 r
}

print.summary.princomp <- function(x) {
 cat("                        ",names(x$sdev),"\n")
 cat("Standard deviations   : ", x$sdev,      "\n")
 cat("Proportion of variance: ", x$per.var,   "\n")
 cat("Cumulative proportion : ", x$cum.var,   "\n")
 invisible(x)
}

plot.princomp <- function(obj, x=1, y=2, main="Scree Plot",
  xlab=paste("Principle component", x),
  ylab=paste("Principle component", y), ...) {
    plot(obj$scores[,c(x,y)], main=main, xlab=xlab, ylab=ylab, ...)
}

############### princomp.Rd ###############

\name{princomp}
\title{Principal Components Analysis}
\usage{
princomp(x, cor=FALSE, scores=TRUE, subset=rep(TRUE, nrow(as.matrix(x))))
print.princomp(obj)
summary.princomp(obj)
plot.princomp(obj)
}
\alias{print.princomp}
\alias{plot.princomp}
\arguments{
\item{x}{a matrix (or data frame) which provides the data
for the principal components analysis.}
\item{cor}{a logical value indicating whether the calculation should use the
correlation matrix or the covariance matrix.}
\item{score}{a logical value indicating whether the score on each principal
component should be calculated.}
\item{subset}{a vector used to select rows (observations) of the
data matrix \code{x}.}
}
\description{
This function performs a principal components analysis on
the given data matrix and returns the results as a \code{princomp} object.
The calculation is done using \code{eigen} on the correlation or
covariance matrix, as determined by \code{cor}. This is done for compatibility
with the Splus result (even though alternate forms for \code{x} - e.g. a
covariance matrix - are not supported as they are in Splus). A preferred method
of calculation is to use svd on \code{x}, as is done in \code{prcomp}.

Note that the scaling of results is affected by the setting of \code{cor}.
If \code{cor} is T then the divisor in the calculation of the sdev is N-1,
otherwise it is N. This has the effect that the result is slightly different
depending on whether scaling is done first on the data and cor set to F, or
done automatically in \code{princomp} with cor=T.

The print method for the these
objects prints the results in a nice format and the
plot method produces a scree plot.
}
\value{
\code{princomp} returns an list with class \code{"princomp"}
containing the following components:
\item{var}{the variances of the principal components
(i.e. the eigenvalues)}
\item{load}{the matrix of variable loadings (i.e. a matrix
whose columns contain the eigenvectors).}
\item{scale}{the value of the \code{scale} argument.}
}
\references{
Mardia, K. V., J. T. Kent and J. M. Bibby (1979).
\emph{Multivariate Analysis}, London: Academic Press.

Venerables, W. N. and B. D. Ripley (1994).
\emph{Modern Applied Statistics with S-Plus}, Springer-Verlag.
}
\seealso{
\code{\link{prcomp}}, \code{\link{prcomponents}}, \code{\link{cor}},
\code{\link{cov}}, \code{\link{eigen}}.
}
\examples{
# the variances of the variables in the
# crimes data vary by orders of magnitude
data(crimes)
princomp(crimes)
princomp(crimes,cor=TRUE)
princomp(scale(crimes, scale=T, center=T), cor=F)
}


############## prcomponents ##############

prcomponents <- function(x, center=TRUE, scale=TRUE, N=nrow(x)-1)
   {if (center) center <- apply(x,2,mean)
    else        center <- rep(0, ncol(x))
    if (scale)  scale  <- sqrt(apply(x,2,var))
    else        scale  <- rep(1, ncol(x))
    s <- svd(sweep(sweep(as.matrix(x),2, center),2, scale, FUN="/"))
    # remove anything corresponding to effectively zero singular values.
    rank <- sum(s$d > (s$d[1]*sqrt(.Machine$double.eps)))
    if (rank < ncol(x)) s$v <- s$v[,1:rank, drop=FALSE]
    s$d <- s$d/sqrt(N)

#   r <- list(sdev=s$d, proj=s$v,x=x %*% s$v, center=center, scale=scale)
    r <- list(sdev=s$d, proj=s$v, center=center, scale=scale)
    r
}



-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From Prof Brian Ripley <ripley@stats.ox.ac.uk>  Mon Aug 24 16:35:17 1998
From: Prof Brian Ripley <ripley@stats.ox.ac.uk> (Prof Brian Ripley)
Date: Mon, 24 Aug 1998 16:35:17 +0100 (BST)
Subject: prcomp and princomp
Message-ID: <199808241535.QAA21899@toucan.stats.ox.ac.uk>

> plot.prcomp <- function(obj, x=1, y=2, main="Scree Plot",
>   xlab=paste("Principle component", x),
>   ylab=paste("Principle component", y), ...) {
>     if (is.null(obj$x))
>  stop("Rotated x has not be calculated by prcomp. Use retx=T in prcomp.")
>     plot(obj$x[,c(x,y)], main=main, xlab=xlab, ylab=ylab, ...)
> }

> plot.princomp <- function(obj, x=1, y=2, main="Scree Plot",
>   xlab=paste("Principle component", x),
>   ylab=paste("Principle component", y), ...) {
>     plot(obj$scores[,c(x,y)], main=main, xlab=xlab, ylab=ylab, ...)
> }

Ouch. Enough of my students think that it is the spelling already!
This is _not_ what is meant by a scree plot, which plots the singular
values or eigenvalues.

-- 
Brian D. Ripley,                  ripley@stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From pgilbert@bank-banque-canada.ca  Mon Aug 24 17:50:17 1998
From: pgilbert@bank-banque-canada.ca (Paul Gilbert)
Date: Mon, 24 Aug 1998 12:50:17 -0400
Subject: prcomp and princomp
References: <199808241535.QAA21899@toucan.stats.ox.ac.uk>
Message-ID: <98Aug24.125405edt.13441@mailgate.bank-banque-canada.ca>

>Ouch. Enough of my students think that it is the spelling already!

Apologies. I thought I had eradicated this.

>This is _not_ what is meant by a scree plot, which plots the singular
>values or eigenvalues.

My mistake here was in the title main="Scree Plot", which in the end was not
what I decided to plot. At one point I thought the default plot method should be
the scree plot, but later decided the default plot method should plot the
principal components.  I noticed that in princomp-add.R you have

plot.princomp <- function(...) screeplot(...)

so perhaps you have different thoughts on this? I would propose having the
default plot the principal components (with the correct title) and use your
function scree, but not as the default.

Paul Gilbert


-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From p.dalgaard@biostat.ku.dk  Tue Aug 25 23:01:45 1998
From: p.dalgaard@biostat.ku.dk (Peter Dalgaard BSA)
Date: 26 Aug 1998 00:01:45 +0200
Subject: Variations on the t test
Message-ID: <x2g1ekk9fa.fsf@biostat.ku.dk>

One of the things that have been annoying me with both Splus and R is
that the "simple tests" are inconsistent with the lm/glm/dataframe
conventions, and that they become quite awkward to use when data have
to be extracted from a dataframe:

t.test(data$bp[data$sex=="F" & data$age>25], data$bp[data$sex=="M" &
data$age>25])

OK, so it's better to do 

eval(t.test(bp[sex=="F"],bp[sex=="M"]),subset(data,age>25))

but try explaining that to a class of MD's!


Below is a first sketch of a set of functions that extend t.test to
allow specification in terms of a list of vectors or a vector and a
grouping variable or a model formula, allowing data= and subset=
arguments as well. The paired test can also be specified using a
Pairs(x,y) construction, which should help prevent people from using
the wrong test. It's supposed to be downward compatible with the Splus
syntax.

I've been pushing Kurt to use a similar interface for all the ctest
functions, so I thought you'd all like to see them. If nothing else,
they should be interesting to study (they were definitely fun to
write), since they use some *really* dirty tricks relating to R's
evaluation model.

Anyway, here goes (remember, I said *first* sketch - there is at least
one bug in it, can you spot it?):

----
###
### The first couple of lines just guard against multiple loading
###

if(exists("t.test",envir=.GlobalEnv))rm(t.test)
if(exists("t.test.default",envir=.GlobalEnv))rm(t.test.default)

### Utility function to extract the environment the call is evaluated
### in. If you do eval(x,list), list is found as the variable "envir"
### in the stack frame immediately below the local frame

.ParentEnv<-function()
{
  parent<-sys.parent()
  grandparent <- sys.parent(2)
  if ( parent - grandparent == 1 )
    sys.frame(grandparent)
  else
    get("envir",envir=sys.frame(parent-1))
}

.t.test<-t.test

t.test.default<-function(x,...,group)
{
  e<-.ParentEnv()
  call <- if ( is.list(x) )
    substitute(t.test.list(x,...))
  else if (missing(group))
    substitute(.t.test(x,...))
  else 
    substitute(t.test.list(split(x, group),...))
  eval(call,e)
}

t.test<-function(...,data=sys.frame(sys.parent()),subset)
{  
  dname.add<-
  if (!missing(data)) 
    paste(", data frame:", deparse(substitute(data))) 
  else
    ""
  if (!missing(subset)){ 
    dname.add<-paste(dname.add, ", subset:", deparse(substitute(subset))) 
    subset <- eval(substitute(subset),data)
    data <- data[subset,] # had better be a data frame...
  }
  res<-eval(substitute(t.test.generic(...)), data)
  res$data.name <- paste(res$data.name, dname.add)
  res
}


t.test.generic<-function(x,...)
{
  UseMethod("t.test")
}

t.test.list<-function(l,...)
{
  if (length(l) != 2)
    stop("need exactly 2 groups")
  res<-.t.test(l[[1]],l[[2]],...)
  res$data.name<-deparse(substitute(l))
  nn<-names(l)
  if (length(nn) != 2)
    nn<-c("group 1", "group 2")
  names(res$estimate)<-nn
  res
}

t.test.formula<-function(f,...)
{
  e<-.ParentEnv()
  f[[1]]<-as.name("split")
  call<-as.call(list(as.name("t.test.list"), f, ...))
    eval(call,e)
}

Pairs<-function(x,y)
  structure(match.call(),class="paired")

t.test.paired<-function(l,...)
{
  e<-.ParentEnv()
  call<-c(as.list(l),list(...),list(paired=T))
  call[[1]]<-as.name(".t.test")
  call<-as.call(call)
  eval(call,e)
}




-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard@biostat.ku.dk)             FAX: (+45) 35327907
-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From pgilbert@bank-banque-canada.ca  Wed Aug 26 19:33:11 1998
From: pgilbert@bank-banque-canada.ca (Paul Gilbert)
Date: Wed, 26 Aug 1998 14:33:11 -0400
Subject: prcomp & princomp - revised
Message-ID: <98Aug26.143800edt.13450@mailgate.bank-banque-canada.ca>

My previous post about prcomp and princomp was done in some haste as I had long
ago indicated to Kurt that I would try to have this ready for the June release,
and it appeared that I would miss yet another release. I also need to get it out
before it becomes hopelessly buried by other work.

Brian Ripley kindly pointed out some errors, and also pointed out that I was
suggesting replacing some functions which were already working well in R. Other
than changing prcomp and princomp this was unintentional on my part. It seems
that some of these functions have either appeared since I first started this, or
that I was just careless (hard to imagine but it happens) and perhaps checked
only for prcomp methods and not princomp methods. In any event, I decided it was
necessary to check and indicate the changes I am proposing more carefully,
despite my time pressures, and also, first of all, to indicate my interest and
intentions more clearly.

I have no special attachment to principal components and there are many people
on this list who could do a better job at this than I can. I want to use PCA for
another problem I am working on, and I noticed several months ago that prcomp
and princomp were returning different results in R and Splus. Unless this is
fixed it puts me in the difficult position of having to choose between Splus and
R, which I would prefer not to do for reasons I have expressed from time to
time. (Another solution is to over-ride the mva library with my own functions,
but this has undesirable consequences when others use my library.) My intention
at this point is to submit these changes to the mva library and then not do any
more work on it. I am not suggesting that these methods are better, only that
they are more consistent with Splus. There is room for improvement (in both
Splus and R) and perhaps someone else would like to work on it. Of all the
changes below, the only ones I consider really important are the documentation
and the new version of prcomp which gives results like Splus.

I have some misgivings about adding yet another principal components function
(prcomponents below). However, I would like prcomp and princomp to return the
same results as Splus, and yet I see that there is need for improvement. I would
like this improvement to happen in a function which has a different name from
the functions in Splus, and perhaps be moved into an expanded "compatibility
library" at some point.

Paul Gilbert

_______

The code below makes the following changes relative to R Version 0.62.3 in
progress-release (August 19, 1998).

- prcomp is changed so that it takes similar arguments and usually reproduces
      Splus results. The criterion used for determining if a singular value
      is zero (following suggestions of Martin Maechler) is slightly different
      and this can occasionally give a different result.
- prcomp.Rd is changed to reflect the changes to prcomp
- summary.prcomp is defined to give a result more like summary.princomp. In
      Splus summary.prcomp does not exist and the default method is applied,
      but this difference seems reasonable?
- screeplot is made generic
- function(...) is changed to function(x, ...) for some plot methods (to be
         consistent with plot).
- print.prcomp is changed to be more like the result from Splus (Splus just
     uses print.default, so this is slightly different)
- plot.prcomp is renamed screeplot.prcomp to be consistent with the way
     this is done for princomp, but I have not worked on this
     function and it does not appear to do scaling or other nice things
     like screeplot.princomp. (Hopefully Brian Ripley will volunteer
     to fix it.)
- plot.prcomp calls - screeplot.prcomp


- princomp is still the function I submitted previously but has a
     few additional comments. It reproduces Splus results when the
     argument is a data matrix, but does not work with a covariance
     argument and the Splus version does.
- princomp.Rd is new.
- print.princomp is defined in princomp.R. This version is a bit more like
    the result from Splus and the function of the same name is removed
    from princomp-add.R.

- programs in the file princomp-add.R could be included in princomp.R but I
    was unsure how to deal with the V&R copyright. Perhaps it should
    be included in each function? I've left princomp-add.R as a
    separate file and done only the minimal number of changes necessary to
    make functions work with the changes in other files. (As far as my
    contributions in the other files are concerned I am happy to assign
    copyright to "The R Development Core Team".)


- a tentative and so far undocumented function prcomponents in the file
    prcomponents.R has been added. This is an incomplete attempt to deal
    with shortcomings of prcomp and princomp, the main problems with those
    functions being that the preferred computational method, svd, is used
    in prcomp, but princomp returns additional useful information. The
    function princomp uses eigen so that it returns results consistent
    with Splus, but does not yet support a covariance as an argument,
    which Splus does (and is the reason eigen is used). My function
    prcomponents also tries to give the user the ability to control
    what is used in the normalization (e.g. N or N-1), as suggested by
    Bill Venables. It does not include use="all.obs" which was in the
    prcomp I am proposing to replace, but it should. The difficulty is
    that this argument is passed to cor or cov, which are not
    called if svd is used.



############## prcomp.R replacement file ##############

screeplot    <- function(x, ...)  {UseMethod("screeplot")}

plot.prcomp   <- function(x, ...) {screeplot(x, ...)}


prcomp <- function(x, retx=TRUE, center=TRUE, scale=FALSE) {
# s <- svd(scale(x, center=center, scale=scale),nu=0)
# above produces warning since scale is both a function and a variable so
 s <- svd(get("scale",envir=.GlobalEnv)
   (x, center=center, scale=scale),nu=0)
# rank <- sum(s$d > 0)
 rank <- sum(s$d > (s$d[1]*sqrt(.Machine$double.eps)))
 if (rank < ncol(x)) s$v <- s$v[,1:rank, drop=F]
 s$d <- s$d/sqrt(max(1,nrow(x) -1))
 if(retx) r <- list(sdev=s$d, rotation=s$v, x=as.matrix(x) %*% s$v)
 else     r <- list(sdev=s$d, rotation=s$v)
 class(r) <- "prcomp"
 r
}

print.prcomp <- function(x) {
 cat("Standard deviations:\n")
 print(x$sdev)
 cat("\nRotation:\n")
 print(x$rotation)
 if (!is.null(x$x))
  {cat("\nRotated variables:\n")
   print(x$x)
  }
 invisible(x)
}

summary.prcomp <- function(obj, digits=3 )
{ vars <- obj$sdev^2
  vars <- vars/sum(vars)
  cat("Importance of components:\n")
  print(rbind("Standard deviation" = obj$sdev,
              "Proportion of Variance" = vars,
              "Cumulative Proportion" = cumsum(vars)))
  invisible(obj)
}

screeplot.prcomp <- function(x, main="Scree Plot", ylab="Variance",
  xlab="Component", ...) {
 plot(x$var, main=main, xlab=xlab, ylab=ylab, ...)
}

############### prcomp.Rd replacement file ###############

\name{prcomp}
\title{Principal Components Analysis}
\usage{
prcomp(x, retx=TRUE, center=TRUE, scale=FALSE)

}
\arguments{
\item{x}{a matrix (or data frame) which provides the data
for the principal components analysis.}
\item{retx}{a logical value indicating whether the rotated variables should
be returned.}
\item{center}{a logical value indicating whether the variables should
be shifted to be zero centered.}
\item{scale}{a logical value indicating whether the variables should
be scaled to have unit variance before the analysis takes place. The default
is F for consistency with S, but in general scaling is advisable.}
}
\description{
This function performs a principal components analysis on
the given data matrix and returns the results as a
\code{prcomp} object. The calculation is done with svd on the data matrix, not
by using eigen on the covariance matrix. This is generally the preferred method
for numerical accuracy.  The print method for the these
objects prints the results in a nice format and the
plot method produces a scree plot. The method \code{biplot} plots two selected
components against one another.
}
\value{
\code{prcomp} returns an list with class \code{"prcomp"}
containing the following components:
\item{sdev}{the standard deviation of the principal components
(i.e. the eigenvalues of the cov matrix - though the calculation is actually
done with the singular values of the data matrix.)}
\item{rotation}{the matrix of variable loadings (i.e. a matrix
whose columns contain the eigenvectors). The function princomp returns this
in the element \code{loadings}.}
\item{x}{if retx is true the value of the rotated data (the data multiplied by
the \code{rotation} matrix) is returned.}
}
\references{
Mardia, K. V., J. T. Kent, J and M. Bibby (1979),
\emph{Multivariate Analysis}, London: Academic Press.

Venerables, W. N. and B. D. Ripley (1994).
\emph{Modern Applied Statistics with S-Plus}, Springer-Verlag.
}

\seealso{
\code{\link{princomp}}, \code{\link{prcomponents}}, \code{\link{cor}},
\code{\link{cov}},
\code{\link{svd}}, \code{\link{eigen}}.
}
\examples{
# the variances of the variables in the
# crimes data vary by orders of magnitude
data(crimes)
prcomp(crimes)
prcomp(crimes,scale=TRUE)
plot(prcomp(crimes))
summary(prcomp(crimes))
}



############## princomp.R replacement file ##############

princomp <- function(x, cor=FALSE, scores=TRUE,
  subset=rep(TRUE, nrow(as.matrix(x)))) {
 # it is tempting to add  use="all.obs" which could be passed to cov or
 # cor but then the calculation of N is complicated.
 z<- as.matrix(x)[subset,, drop=F]
 N <- nrow(z)
 if(cor) cv <- get("cor",envir=.GlobalEnv)(z)
 else    cv <- cov(z)
#  (svd can be used but gives different signs for some vectors)
 edc <- eigen(cv)
 cn <- paste("Comp.", 1:ncol(cv), sep="")
 names(edc$values) <- cn
 dimnames(edc$vectors) <- list(dimnames(x)[[2]], cn)
 scr<- NULL
 if (cor)
   {sdev <- sqrt(edc$values)
    sc <- (apply(z,2,var)*(N-1)/N)^0.5
    if (scores)
        scr<-(scale(z,center=T,scale=T) %*% edc$vectors)*sqrt(N/(N-1))
   }
 else
   {sdev <- sqrt(edc$values*(N-1)/N)
    sc <- rep(1, ncol(z))
    if (scores)
        scr<- (scale(z,center=T,scale=F) %*% edc$vectors)
   }
 names(sc) <- dimnames(x)[[2]]
 edc <-list(sdev=sdev, loadings=edc$vectors,
     center=apply(z,2,mean), scale=sc, n.obs=N, scores=scr)
# The Splus function also return list elements factor.sdev, correlations
# and coef, but these are not documented in the help. coef seems to equal
# load. The Splus function also return list elements call and terms which
# are not supported here.
 class(edc) <- "princomp"
 edc
}

print.princomp <- function(x) {
 cat("Standard deviations:\n")
 print(x$sdev)
 cat(length(x$scale), " variables and ", x$n.obs, "observations.\n")
 cat("Scale:\n")
 print(x$scale)
 invisible(x)
}


############### princomp.Rd new file ###############

\name{princomp}
\title{Principal Components Analysis}
\usage{
princomp(x, cor=FALSE, scores=TRUE, subset=rep(TRUE, nrow(as.matrix(x))))
print.princomp(obj)
summary.princomp(obj)
plot.princomp(obj)
}
\alias{print.princomp}
\alias{plot.princomp}
\arguments{
\item{x}{a matrix (or data frame) which provides the data
for the principal components analysis.}
\item{cor}{a logical value indicating whether the calculation should use the
correlation matrix or the covariance matrix.}
\item{score}{a logical value indicating whether the score on each principal
component should be calculated.}
\item{subset}{a vector used to select rows (observations) of the
data matrix \code{x}.}
}
\description{
This function performs a principal components analysis on
the given data matrix and returns the results as a \code{princomp} object.
The calculation is done using \code{eigen} on the correlation or
covariance matrix, as determined by \code{cor}. This is done for compatibility
with the Splus result (even though alternate forms for \code{x} - e.g. a
covariance matrix - are not supported as they are in Splus). A preferred method
of calculation is to use svd on \code{x}, as is done in \code{prcomp}.

Note that the scaling of results is affected by the setting of \code{cor}.
If \code{cor} is T then the divisor in the calculation of the sdev is N-1,
otherwise it is N. This has the effect that the result is slightly different
depending on whether scaling is done first on the data and cor set to F, or
done automatically in \code{princomp} with cor=T.

The print method for the these
objects prints the results in a nice format and the
plot method produces a scree plot.
}
\value{
\code{princomp} returns an list with class \code{"princomp"}
containing the following components:
\item{var}{the variances of the principal components
(i.e. the eigenvalues)}
\item{load}{the matrix of variable loadings (i.e. a matrix
whose columns contain the eigenvectors).}
\item{scale}{the value of the \code{scale} argument.}
}
\references{
Mardia, K. V., J. T. Kent and J. M. Bibby (1979).
\emph{Multivariate Analysis}, London: Academic Press.

Venerables, W. N. and B. D. Ripley (1994).
\emph{Modern Applied Statistics with S-Plus}, Springer-Verlag.
}
\seealso{
\code{\link{prcomp}}, \code{\link{prcomponents}}, \code{\link{cor}},
\code{\link{cov}}, \code{\link{eigen}}.
}
\examples{
# the variances of the variables in the
# crimes data vary by orders of magnitude
data(crimes)
princomp(crimes)
princomp(crimes,cor=TRUE)
princomp(scale(crimes, scale=T, center=T), cor=F)
plot(princomp(crimes))
biplot(princomp(crimes))
summary(princomp(crimes))
loadings(princomp(crimes))
}


############## princomp-add.R replacement file ##############

# copyright (C) 1998 W. N. Venables and B. D. Ripley
#
predict.princomp <- function(object, newdata,...)
{
  if (missing(newdata)) return(object$scores)
  scale(newdata, object$center, object$scale) %*% object$loadings
}

summary.princomp <-
function(object, loadings = F, cutoff = 0.1, digits=3, ...)
{
  vars <- object$sdev^2
  vars <- vars/sum(vars)
  cat("Importance of components:\n")
  print(rbind("Standard deviation" = object$sdev,
              "Proportion of Variance" = vars,
              "Cumulative Proportion" = cumsum(vars)))
  if(loadings) {
    cat("\nLoadings:\n")
    cx <- format(round(object$loadings, digits=digits))
    cx[abs(object$loadings) < cutoff] <-
      substring("       ", 1, nchar(cx[1,1]))
    print(cx, quote = F, ...)
  }
  invisible(object)
}


plot.princomp <- function(x, ...) {screeplot(x, ...)}

screeplot.prcomp <- screeplot.princomp <-
function(x, npcs=min(10, length(x$sdev)), type=c("barplot", "lines"),
         main = deparse(substitute(x)), ...)
{
  eval(main)
  type <- match.arg(type)
  pcs <- x$sdev^2
  xp <- seq(length=npcs)
  if(type=="barplot") barplot(pcs[xp], names = names(pcs[xp]),
       main = main, ylab = "Variances", ...)
  else {
    plot(xp, pcs[xp], type = "b", axes = F, main = main, xlab="",
            ylab = "Variances", ...)
    axis(2)
    axis(1, at = xp, labels = names(pcs[xp]))
  }
  invisible()
}

loadings <- function(x) x$loadings



############## prcomponents.R  new file ##############

prcomponents <- function(x, center=TRUE, scale=TRUE, N=nrow(x)-1)
   {if (center) center <- apply(x,2,mean)
    else        center <- rep(0, ncol(x))
    if (scale)  scale  <- sqrt(apply(x,2,var))
    else        scale  <- rep(1, ncol(x))
    s <- svd(sweep(sweep(as.matrix(x),2, center),2, scale, FUN="/"))
    # remove anything corresponding to effectively zero singular values.
    rank <- sum(s$d > (s$d[1]*sqrt(.Machine$double.eps)))
    if (rank < ncol(x)) s$v <- s$v[,1:rank, drop=FALSE]
    s$d <- s$d/sqrt(N)

#   r <- list(sdev=s$d, proj=s$v,x=x %*% s$v, center=center, scale=scale)
    r <- list(sdev=s$d, proj=s$v, center=center, scale=scale)
    r
}




-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From Prof Brian Ripley <ripley@stats.ox.ac.uk>  Mon Aug 31 15:19:26 1998
From: Prof Brian Ripley <ripley@stats.ox.ac.uk> (Prof Brian Ripley)
Date: Mon, 31 Aug 1998 15:19:26 +0100 (BST)
Subject: Packages aov, modreg, lqs, psplines
Message-ID: <199808311419.PAA22293@toucan.stats.ox.ac.uk>

I now have versions of code that is destined (I believe) for 0.63 which
is in a suitable state for comment. The files are at

	ftp://ftp.stats.ox.ac.uk/pub/R  

(Our www server is being moved, so may be intermittently down, but this
ftp server should be stable.)  All are R packages, for the moment for
personal use only (no re-distribution). Use with 0.62.3 or 0.63 (although
I am aware of some problems with use with 0.63 that I have reported).


aov.tar.gz:
==========

aov with Error terms, proj, model.tables, se.contrast, replications, 
eff.aovlist, C, dummy.coef, add1, drop1, step, kappa, labels.

This is in I believe close to final form.


modreg.tar.gz:
=============

ksmooth                  Kernel Regression Smoother
loess                    Local Polynomial Regression Fitting
loess.control            Set Parameters for Loess
plot.ppr                 Plot Ridge Functions for Projection Pursuit
                            Regression Fit
ppr                      Projection Pursuit Regression
predict.loess            Predict Loess Curve or Surface
predict.smooth.spline    Predict from Smoothing Spline Fit
scatter.smooth           Scatter Plot with Smooth Curve Fitted by Loess
smooth.spline            Fit a Smoothing Spline
supsmu                   Friedmans's SuperSmoother

Probably only bug-fixing left, but I would welcome comments about the
extent of the loess functionality.


lqs.tar.gz:
==========

cov.rob                  Robust Estimation of Multivariate Location and
                            Scatter
lqs                      Resistant Regression by Least Trimmed and Least
                            Quantile Sum of Squares
lmsreg, ltsreg, cov.mve  Compatibility functions

This is much less complete (and the claimed mcd method is not yet
operational).  Comments please both on the design and from any experts
out there on the methodology used. (BTW, as all the programs I have give
different answers, it is very hard to establish the true answer. I am
fairly convinced that both S-PLUS versions are slightly wrong.)

I plan to add S-estimation, a general M-estimator (based on my function
rlm) and MM-estimation to this over the next few weeks.


While I am posting, a R port of Jim Ramsay's psplines package is also there
en route to CRAN, file (pspline_1.0-1.tar.gz) as well as an update of the
V&R `R' Complements to 0.62.3.

-- 
Brian D. Ripley,                  ripley@stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From pgilbert@bank-banque-canada.ca  Mon Aug 31 22:18:27 1998
From: pgilbert@bank-banque-canada.ca (Paul Gilbert)
Date: Mon, 31 Aug 1998 17:18:27 -0400
Subject: isolating R/S and operating system
 differences
Message-ID: <98Aug31.172601edt.13446@mailgate.bank-banque-canada.ca>

Below is a revised version of my kernel of functions for isolating R/S and
operating system  differences. The main change is "date" which I've renamed
"date.parsed" to avoid conflicts with the R and S date functions. The R call now
uses system() rather than unix() to avoid warning messages in R 0.62.3.

Paul Gilbert

##############################################################################


# This file has code which contains operating system and S/R specific
#   functions. They are intended to be used as a kernel to help
#   protect other code from these problems.

# The MSwindows versions are not done.

# The following functions are attempted:
#   For S/R differences:
#      global.assign, system.info, exists.graphics.device, unlink,
#      synchronize,  list.add for [["new.element"]]<-
#   For OS differences:
#     system.call, sleep, present.working.directory, whoami, file.copy,
#     file.date.info, date.parsed, mail, unlink, local.host.netname,

# Also a number of is.xxx functions are defined to identify systems.

# The variable  .SPAWN is also set to be used to identify if Splus "For" loops
#    should be used. (It is sometimes better not to use these even in Splus.)

##############################################################################

#            General Logic and organization of these functions

# 1/ The first group of functions are for identifying S or R and flavours.
# 2/ The second group of functions are for identifying the operating system.
# 3/ The third group specify a few functions which depend only on the
#         differences between S and R.
# 4/ The fourth group specify functions which depend only on the
#         differences among operating system.
# 5/ The fifth group specify a few functions which depend on both R/S and the
#         differences among operating system.

#  >>> I would very much like any input WRT  MS Windows / Win95 / NT / Mac <<<

# The function system.call is defined in order to provide a generic way to
#  make a call to the operating system. When the calls are specific
#  to Unix then the function unix() might be used (though that is now
#  deprecated in R and produces a warning messsage). However, in general the
#  purpose of these functions is not to give a generic way to call the operating

#  system, but rather a generic way to do things that require a call to the
#  operating system (like date, mail, sleep, whoami).


##############################################################################

system.info <- function()
     {if( !exists("version"))
         { #-- `Vanilla' S (i.e. here "S version 4")
           #- this now works for  S version 4  (this is not S-plus 4.0, maybe
           #             part of S-plus 5.0 !):
           lv <- nchar(Sv <- Sversion())
           r <- list(
       major = substring(Sv, 1,1),
       minor = substring(Sv, lv,lv))
         }
   else
     {r <- version
      r$minor <- as.numeric(r$minor)
      r$major <- as.numeric(r$major)
     }
   if      (is.Splus())    r$language <- "Splus"
   else if (is.Svanilla()) r$language <- "S"
   r$OSversion <- OSversion()
   r$OStype    <- OStype()
   r
  }


###########################################################

#    1/  Functions are for identifying S or R and flavours.

###########################################################

#Note It is tempting to use system.info as defined above, but there is a
#        bootstrapping problem to solve.

if (! exists("is.R"))
 {is.R <- function()
     {exists("version") && !is.null(vl <- version$language) && vl == "R" }
 }

is.R.pre0.60 <- function()
  {is.R() && ((as.numeric(version$major)+.01*as.numeric(version$minor)) <0.60) }

is.R.pre0.63.2 <- function()
  {is.R() && ((as.numeric(version$major)+.01*as.numeric(version$minor)) <0.623)}



is.S <- function(){is.Svanilla() | is.Splus() }
is.Svanilla <- function(){!exists("version")}
is.Splus <- function(){exists("version") && is.null(version$language)}
is.Splus.pre3.3 <- function()
   { ## <= 3.2
    is.Splus() &&  ((system.info()$major+.1*system.info()$minor) < 3.3)
   }

###########################################################

#    2/  Functions are for identifying the operating system.

###########################################################

if (is.R())
   {OStype <- function()
      {if("Win32"== machine())          return("MS Windows")
       else if("Macintosh"== machine()) return("Macintosh") #needs to be checked

       else if("Unix"== machine())      return ("Unix")
      }
   }

if (is.S())
   {OStype <- function()
      {if(charmatch("MS Windows", version$os, nomatch=0))
                                return("MS Windows")
       else if(charmatch("Macintosh",  version$os, nomatch=0))
                                return("Macintosh") # needs to be checked
       else if(exists("unix"))  return ("Unix")
      }
   }


is.MSwindows <- function(){OStype() == "MS Windows"}
is.Mac       <- function(){OStype() == "Macintosh" }
is.unix      <- function(){OStype() == "Unix" }

{
if (is.unix())
  {OSversion <- function()
    {paste(system.call("uname -s"),
           system.call("uname -r | sed -e 's/\\.\.\*//'"), sep="") }
  }
else if(is.MSwindows())
  {if (is.R())
     {OSversion <- function()
        {# This is not great since NT is not distinguished but
         #    is.Win32() below will work ok
         if("Win32"== machine()) return("MS Windows 95")
         else return ("unkown")
        }
     }
   if (is.S())
     {OSversion <- function()
        {if("MS Windows 3.1"==version$os) return("MS Windows 3.1")
         if("MS Windows 95" ==version$os) return("MS Windows 95")
         if("MS Windows 98" ==version$os) return("MS Windows 98")
         if("MS Windows NT" ==version$os) return("MS Windows NT")
         else return ("unkown")
        }
     }
  }
else OSversion <- function() "unknown"
}


# Other is.xxx() should be added here.

# determining Unix flavours doesn't seem to be too important but ...
is.Sun4 <- function() {is.unix() && OSversion() == "SunOS4" }
is.Sun5 <- function() {is.unix() && OSversion() == "SunOS5" }
is.Linux <- function(){is.unix() && OSversion() == "linux"}

# Windows flavours may be more important but these are untested !!!
is.Win3.1 <- function(){is.MSwindows() && OSversion() == "MS Windows 3.1"}
is.Win95  <- function(){is.MSwindows() && OSversion() == "MS Windows 95"}
is.WinNT  <- function(){is.MSwindows() && OSversion() == "MS Windows NT"}
is.Win32  <- function(){is.Win95() | is.WinNT() }





###########################################################

#    3/  Functions depending only on the
#         differences between S and R

###########################################################

if(is.S())
   {if(is.unix())system.call  <- unix
    global.assign <- function(name, value) {assign(name,value, where = 1)}
    .SPAWN <- TRUE
    exists.graphics.device <- function(){dev.cur() !=1 }
    open.graphics.device  <- function(display=getenv("DISPLAY"))
                                 {openlook(display) }
    #                            {motif(display) }
    close.graphics.device <- function(){dev.off() }
    if (!exists("set.seed.Splus")) set.seed.Splus <- set.seed
    set.seed <- function(seed=NULL)
      {if (is.null(seed))
          seed <-.Random.seed
       else
         {if (1==length(seed)) set.seed.Splus(seed)
          else global.assign(".Random.seed", seed)
         }
       seed
      }

    "list.add<-" <- function(x, replace, value)
       {# replace or add elements to a list.
        x[replace] <- value
        # x[[replace]] <- value  would be more logical but doesn't work
        x
       }
   }

if(is.R())
   {#tempfile <- function(f)
    #   {# Requires C code also from Friedrich Leisch not in version 0.15 of R.
    #    d<-"This is simply a string long enough to hold the name of a tmpfile";

    #     .C("tmpf", as.character(d))[[1]]
    #    }

    if (is.R.pre0.60())
        {tempfile <- function(pattern = "file")
                {system(paste("for p in", paste(pattern, collapse = " "), ";",
                       "do echo /tmp/$p$$; done"),
                 intern = TRUE)
                }
        }

#    unlink <- function(file) system.call(paste("rm -fr ", file))
    global.assign <- function(name, value)
                          {assign(name,value, envir=.GlobalEnv)}
    synchronize<- function(x){NULL} # perhaps this should do something?
    .SPAWN <- FALSE
    dev.ask <- function(ask=T){par(ask=ask)}
    if (is.R.pre0.63.2())
         exists.graphics.device <- function(){exists(".Device")}
    else exists.graphics.device <- function(){dev.cur() !=1 }
    open.graphics.device  <- function(display=getenv("DISPLAY")) {x11(display)}
    close.graphics.device <- function(){F} # how do I do this?
    set.seed <- function(seed=NULL)
      {if (is.null(seed))
         {if (!exists(".Random.seed")) zzz <- runif(1) # seed may not yet exist
          seed <-.Random.seed
         }
       else
         {if (1==length(seed))
             global.assign(".Random.seed",round(runif(3,min=seed,max=1e5*seed)))

          else global.assign(".Random.seed", seed)
         }
       seed
      }

   "list.add<-" <- function(x, replace, value)
     {# replace or add elements to a list.
      if (is.numeric(replace))
        {# x<- do.call("default.[[<-", list(x,replace,value))   # use default
         x[[replace]] <- value
         return(x)
        }
      if (is.null(value))  value <- list(NULL)
      if (!is.list(value)) value <- list(value)
      if (1 == length(value))
       {for (i in seq(length(replace)))
          x<- do.call("$<-", list(x,replace[i],value[[1]]))
       }
      else
        {if(length(value) != length(replace) )
         stop("number of replacement values != number of elements to replace")
         for (i in seq(length(replace)))
            x<- do.call("$<-", list(x,replace[i],value[[i]]))
        }
      x
     }
 }


###########################################################

#    4/  Functions depending only on the
#         differences among operating system.

###########################################################

if(is.unix())
  {sleep <- function(n) {unix(paste("sleep ", n))} # pause for n seconds
   present.working.directory <- function(){unix("pwd")} # present directory
   whoami <- function(){unix("whoami")} # return user id (for mail)
   local.host.netname <-function() {unix("uname -n")}

   mail <- function(to, subject="", text="")
    {# If to is null then mail is not sent (useful for testing).
     file <- tempfile()
     write(text, file=file)
   if(!is.null(to)) unix(paste("cat ",file, " | mail  -s '", subject, "' ", to))

     unlink(file)
     invisible()
    }

   file.copy <- function(from, to)unix(paste("cp ", from, to)) # copy file

   file.date.info <- function(file.name)
     {# This could be a lot better. It will fail for files older than a year.
      # Also, a returned format like date() below would be better.
      mo <- (1:12)[c("Jan","Feb","Mar","Apr","May", "Jun","Jul","Aug", "Sep",
         "Oct","Nov","Dec") ==substring(unix(paste("ls -l ",file)),33,35)]
      day <- as.integer(substring(unix(paste("ls -l ",file.name)),37,38))
      hr  <- as.integer(substring(unix(paste("ls -l ",file.name)),40,41))
      sec <- as.integer(substring(unix(paste("ls -l ",file.name)),43,44))
      c(mo,day,hr,sec)
     }

}

if(is.MSwindows())
  {system.call  <- function(cmd)
         {stop("system calls must be modified for this operating system.")}
   sleep <- system.call
   present.working.directory <- system.call
   whoami <- system.call
   file.copy <- system.call
   file.date.info <- system.call
  }



###########################################################

#    5/  Functions depending on both R/S and the
#         differences among operating system.

###########################################################

if(is.unix())
  {if(is.R())
     {#unix <- function(cmd) system(cmd, intern=T)
      # unix() is now a function in R but deprecated in favour of system()
      # (This is a bit dangerous, as these calls may be system dependent.)

      system.call <- function(cmd) system(cmd, intern=T)

  # the following date function might be made system independent as a C call.
      date.parsed <-function()
        {d<-parse(text=strsplit(
              system.call("date \'+%Y %m %d %H %M %S\'")," ")[[1]])
         list(y=  eval(d[1]),
              m=eval(d[2]),
              d= eval(d[3]),
              h= eval(d[4]),
              m= eval(d[5]),
              s= eval(d[6]),
              tz=system.call("date '+%Z'"))
        }
     }
   if(is.S())
     {system.call <- function(cmd) unix(cmd)

      date.parsed <-function()
        {d <- parse(text=unix("date '+%Y %m %d %H %M %S'"),white=T)
         list(y=  eval(d[1]),
              m=eval(d[2]),
              d= eval(d[3]),
              h= eval(d[4]),
              m= eval(d[5]),
              s= eval(d[6]),
              tz=unix("date '+%Z'"))
        }
     }
  }




##############################################################################



-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From jimrc@mathfs.math.montana.edu  Tue Aug  4 22:23:29 1998
From: jimrc@mathfs.math.montana.edu (Jim Robison-Cox)
Date: Tue, 4 Aug 1998 15:23:29 -0600 (MDT)
Subject: aov with Error terms
Message-ID: <Pine.GSO.4.02.9808041517520.8073-100000@gauss.math.montana.edu>

 To R-devel:
 
   If anyone is testing the aov() function and it's relatives which I
 posted to the list, thanks for the effort, but I would ask you to hold
off now.  Brian Ripley is developing better and more complete versions, so
you should save your effort for looking over his functions.

  His preliminary version is located at:
http://www.stats.ox.ac.uk/pub/R/aov.tar.gz

Prof. Ripley says it contains:
BR> my current versions of  aov, proj, model.tables, eff.aovlist etc.
BR> 
BR> These are probably as complete as I want to make them, and handle
BR> Error terms and (to some extent) multiple responses.  It is hard to
BR> find sufficiently wierd designs to test this, so I would be grateful
BR> for further testing. There are some datasets and tests in the file,
BR> which is packaged as an R package.

 His note about our recent exchange is related to these functions, though
I was also asking about a summary.mlm which is not in current R nor in
today's aov library.


Jim Robison-Cox                 ____________            
Department of Math Sciences    |            |           phone: (406)994-5340
2-214 Wilson Hall               \   BZN, MT |           FAX:   (406)994-1789
Montana State University         |  *_______|
Bozeman, MT 59717                 \_|         e-mail: jimrc@math.montana.edu 




-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From Andreas.Weingessel@ci.tuwien.ac.at  Thu Aug  6 12:06:11 1998
From: Andreas.Weingessel@ci.tuwien.ac.at (Andreas Weingessel)
Date: Thu, 6 Aug 1998 13:06:11 +0200 (CEST)
Subject: memory-management
Message-ID: <13769.36387.261923.629675@elendil.ci.tuwien.ac.at>


R has some strange behavior in its memory management. (R-0.62.2 and
R-0.63.0 of July 22)

If I start R on a PC with 256MB with the option -v 200, and want to
see the free memory I get

R> gc()
Garbage collection ...
111801 cons cells free (55%)
204554k bytes of heap free (-63%)

So, I have suddenly a negative percentage of memory free. This value
becomes not positive, unless a certain amount of memory is used.

R> x <- matrix(0,1000,1000)
R> gc()
Garbage collection ...
111797 cons cells free (55%)
196741k bytes of heap free (-67%)
R> x <- matrix(0,4000,1000)
R> gc()
Garbage collection ...
111797 cons cells free (55%)
173304k bytes of heap free (-79%)
R> x <- matrix(0,5000,1000)
R> gc()
Garbage collection ...
111797 cons cells free (55%)
165491k bytes of heap free (80%)



The second point I came across is the following. R is now started with
-v 100 -n 1000000. After

R>  x <- matrix(0,5000,900)

I have still enough memory free.

R>  gc()
Garbage collection ...
911797 cons cells free (91%)
66997k bytes of heap free (65%)

So, if x uses 35% of heap memory, there should still be enough memory
for a second matrix of this size, but the command

R>  y <- matrix(0,5000,900)

runs out of heap memory.

Similarly, I can create a 5000x1300-matrix on a newly started R with
-v 100 and have still 50% heap memory free, but I can not create a
5000x1400-matrix, if R is started with -v 100. So, I suppose that R
needs a complete copy of this matrix while creating it and therefore I
can only create matrices which use less than half of the free
memory. Is there any way to avoid this memory usage?

************************************************************************
*                          Andreas Weingessel                          *
************************************************************************
* Institut f=FCr Statistik      *                Tel: (+43 1) 58801 4541 =
*
* Technische Universit=E4t Wien *                Fax: (+43 1)  504 14 98 =
*
* Wiedner Hauptstr. 8-10/1071 *     Andreas.Weingessel@ci.tuwien.ac.at *
* A-1040 Wien, Austria        * http://www.ci.tuwien.ac.at/~weingessel *
************************************************************************


-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From pgilbert@bank-banque-canada.ca  Tue Aug 11 15:02:21 1998
From: pgilbert@bank-banque-canada.ca (Paul Gilbert)
Date: Tue, 11 Aug 1998 10:02:21 -0400
Subject: R-beta: base not loading
References: <199808111314.PAA13233@sophie.ethz.ch>
Message-ID: <98Aug11.100657edt.13447@mailgate.bank-banque-canada.ca>

Martin

No I didn't solve it. I've been on holidays for a couple of weeks. I'll try to
look at it again soon, but any hints would be appreciated.

I can compile on SunOS 5.5 and then run on 5.6, which may be a sort term
solution to your user's problem.

Paul

-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From pgilbert@bank-banque-canada.ca  Tue Aug 11 15:27:03 1998
From: pgilbert@bank-banque-canada.ca (Paul Gilbert)
Date: Tue, 11 Aug 1998 10:27:03 -0400
Subject: R-beta: base not loading
References: <199808111314.PAA13233@sophie.ethz.ch> <x2btprvcmk.fsf@blueberry.kubism.ku.dk>
Message-ID: <98Aug11.103140edt.13458@mailgate.bank-banque-canada.ca>

I just remembered, another thing which complicates this is that older gcc's do
not work with Solaris 2.6 so I had to upgrade to gcc 2.8.1. The result is that I
am not sure if this is

1/  gcc 2.8.1 is not good for the current version of R

2/ my gcc 2.8.1 is not correctly configured

3/ I need to be more careful to upgrade *all* parts of the toolchain

My gut feeling is the same as Peter's (3/), but I haven't got a clue how to
track down the problem.

Paul

(PS. There also seem to be some problems making the f2c with gcc 2.8.1. I've
been using g77 so that isn't the problem.)

-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From Martin Maechler <maechler@stat.math.ethz.ch>  Tue Aug 11 15:34:30 1998
From: Martin Maechler <maechler@stat.math.ethz.ch> (Martin Maechler)
Date: Tue, 11 Aug 1998 16:34:30 +0200
Subject: R-beta: base not loading
In-Reply-To: <98Aug11.103140edt.13458@mailgate.bank-banque-canada.ca> (message
 from Paul Gilbert on Tue, 11 Aug 1998 10:27:03 -0400)
Message-ID: <199808111434.QAA19036@sophie.ethz.ch>

>>>>> "PaulG" == Paul Gilbert <pgilbert@bank-banque-canada.ca> writes:

    PaulG> I just remembered, another thing which complicates this is that
    PaulG> older gcc's do not work with Solaris 2.6 so I had to upgrade to
    PaulG> gcc 2.8.1. The result is that I am not sure if this is

    PaulG> 1/ gcc 2.8.1 is not good for the current version of R

This is certainly not the case.
We have been using gcc 2.8.1 here (Solaris 2.5.x) for quite a while
with no problems.

    PaulG> 2/ my gcc 2.8.1 is not correctly configured

    PaulG> 3/ I need to be more careful to upgrade *all* parts of the
    PaulG> toolchain

    PaulG> My gut feeling is the same as Peter's (3/), but I haven't got a
    PaulG> clue how to track down the problem.

    PaulG> Paul

    PaulG> (PS. There also seem to be some problems making the f2c with gcc
    PaulG> 2.8.1. I've been using g77 so that isn't the problem.)

NOTE: Looking at  RHOME/library/base/R/base (which is the R file that is
	NOT correctly loaded by the internal C code),
      I now bet, that it is  .Alias(.)  which is failing;  more exactly,
      line 64
	------------------------------------------------------
	colours <- .Alias(colors)
	------------------------------------------------------
   
      That's (the only way?) how the error message
	------------------------------------------------------
	Error: Object "colors" not found 
	------------------------------------------------------
      can emerge...

[[this may help find gcc's problem, maybe...]]

Martin
-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From pgilbert@bank-banque-canada.ca  Tue Aug 11 15:59:51 1998
From: pgilbert@bank-banque-canada.ca (Paul Gilbert)
Date: Tue, 11 Aug 1998 10:59:51 -0400
Subject: R-beta: base not loading
References: <199808111434.QAA19036@sophie.ethz.ch>
Message-ID: <98Aug11.110432edt.13451@mailgate.bank-banque-canada.ca>

>PaulG> 1/ gcc 2.8.1 is not good for the current version of R

>This is certainly not the case.
>We have been using gcc 2.8.1 here (Solaris 2.5.x) for quite a while
>with no problems.

To do this I think you would have made gcc with Solaris 2.5 and we made it with
Solaris 2.6, so I don't think we can be certain that there are not problems with
gcc, in combination with Solaris 2.6 libraries, etc, etc.

Paul

-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From p.dalgaard@biostat.ku.dk  Sun Aug 16 11:36:50 1998
From: p.dalgaard@biostat.ku.dk (Peter Dalgaard BSA)
Date: 16 Aug 1998 12:36:50 +0200
Subject: R-beta: base not loading
In-Reply-To: Martin Maechler's message of Tue, 11 Aug 1998 16:34:30 +0200
References: <199808111434.QAA19036@sophie.ethz.ch>
Message-ID: <x27m091ad9.fsf@biostat.ku.dk>

Martin Maechler <maechler@stat.math.ethz.ch> writes:

>     PaulG> (PS. There also seem to be some problems making the f2c with gcc
>     PaulG> 2.8.1. I've been using g77 so that isn't the problem.)
> 
> NOTE: Looking at  RHOME/library/base/R/base (which is the R file that is
> 	NOT correctly loaded by the internal C code),
>       I now bet, that it is  .Alias(.)  which is failing;  more exactly,
>       line 64
> 	------------------------------------------------------
> 	colours <- .Alias(colors)
> 	------------------------------------------------------
>    
>       That's (the only way?) how the error message
> 	------------------------------------------------------
> 	Error: Object "colors" not found 
> 	------------------------------------------------------
>       can emerge...
> 
> [[this may help find gcc's problem, maybe...]]

While messing around with a Windows cross-compile, I realized that
this can also happen if colorstuff.R occurs before New-Internal.R in
the base file. The former has:

## nice to the English
colours <- colors

And the latter has

colors <- function().Internal(colors())
colours <- .Alias(colors)

Now, if ls gets the idea of sorting differently on some platforms,
which is not at all unlikely - Ripley reported something about locale
settings resulting in the collating sequence AaBbCc..., and he *is*
using Sol2.6! - then all hell breaks loose.

The thing in colorstuff.R is simply wrong, so I'll remove it.

PS: No, the 0.62.3 crosscompile doesn't work (yet?).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard@biostat.ku.dk)             FAX: (+45) 35327907
-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From mani@ee.iitb.ernet.in  Sun Aug 16 22:07:07 1998
From: mani@ee.iitb.ernet.in (R. MANIVASAKAN)
Date: Mon, 17 Aug 1998 02:37:07 +0530 (IST)
Subject: Bug tracking system
In-Reply-To: <x2yasoajuj.fsf@biostat.ku.dk>
Message-ID: <Pine.SUN.3.91.980817023536.21259A-100000@bhairav.ee.iitb.ernet.in>

Hi all,
	is there any code in R implimenting the Whittle's MLE estimator 
for Hurst parameter H of the given dataset?.

	thanks,
	mani.

-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From pgilbert@bank-banque-canada.ca  Mon Aug 17 17:13:28 1998
From: pgilbert@bank-banque-canada.ca (Paul Gilbert)
Date: Mon, 17 Aug 1998 12:13:28 -0400
Subject: R-beta: base not loading
References: <199808111434.QAA19036@sophie.ethz.ch> <x27m091ad9.fsf@biostat.ku.dk>
Message-ID: <98Aug17.122121edt.13448@mailgate.bank-banque-canada.ca>

> NOTE: Looking at  RHOME/library/base/R/base (which is the R file that is
>       NOT correctly loaded by the internal C code),
>       I now bet, that it is  .Alias(.)  which is failing;  more exactly,
>       line 64
>       colours <- .Alias(colors)

In my RHOME/library/base/R/base this is line 7105. Based on hints from Peter
Dalgaard and Brian Ripley I discovered that the setting of LC_COLLATE affects
the result of ls, but does not seem to be responsible for the very different
order in which things are arranged in RHOME/library/base/R/base. (Apparently the
setting of LC_COLLATE is part of a choice about locale made during the
installation of Solaris 2.6, so not everyone will have it set.)

One can
    unsetenv LC_COLLATE
which changes the result of ls, but on re-installing R this does not seem to
affect the order of things in RHOME/library/base/R/base (and I have still not
figured out why it is different between 2.5 and 2.6).

On the other hand, removing
    ## nice to the English
    colours <- colors
from RHOME/library/base/R/base seems to fix everything. This assignment seems
to be replicated by
   colours <- .Alias(colors)
and so I guess Peter's fix is the right one.

Paul Gilbert



-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From ihaka@stat.auckland.ac.nz  Mon Aug 17 21:32:43 1998
From: ihaka@stat.auckland.ac.nz (Ross Ihaka)
Date: Tue, 18 Aug 1998 08:32:43 +1200 (NZST)
Subject: Grammar Changes
Message-ID: <199808172032.IAA17509@stat1.stat.auckland.ac.nz>

I would like to make a couple of small changes to the R grammar.
At present, operators like ~, ==, !=, <, <=, >, >= are declared
to be non-associative.  This means that things like

	a < b <= c

produce a "syntax error" message when typed.

I would like to change the grammar so that these operators are
left-associative.  I want to make this change so that mathematical
annotation in the graphics will work a little better.

Interestingly, these operators are left-associative in S, so this
would be a move toward compatibility.

Does anyone have any strong thoughts on this?

	Ross
-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From p.dalgaard@biostat.ku.dk  Mon Aug 17 22:32:50 1998
From: p.dalgaard@biostat.ku.dk (Peter Dalgaard BSA)
Date: 17 Aug 1998 23:32:50 +0200
Subject: Grammar Changes
In-Reply-To: Ross Ihaka's message of Tue, 18 Aug 1998 08:32:43 +1200 (NZST)
References: <199808172032.IAA17509@stat1.stat.auckland.ac.nz>
Message-ID: <x2ogtjgupp.fsf@biostat.ku.dk>

Ross Ihaka <ihaka@stat.auckland.ac.nz> writes:

> 
> I would like to make a couple of small changes to the R grammar.
> At present, operators like ~, ==, !=, <, <=, >, >= are declared
> to be non-associative.  This means that things like
> 
> 	a < b <= c
> 
> produce a "syntax error" message when typed.
> 
> I would like to change the grammar so that these operators are
> left-associative.  I want to make this change so that mathematical
> annotation in the graphics will work a little better.
> 
> Interestingly, these operators are left-associative in S, so this
> would be a move toward compatibility.
> 
> Does anyone have any strong thoughts on this?

Sounds harmless enough to me. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard@biostat.ku.dk)             FAX: (+45) 35327907
-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From Martin Maechler <maechler@stat.math.ethz.ch>  Tue Aug 18 14:13:46 1998
From: Martin Maechler <maechler@stat.math.ethz.ch> (Martin Maechler)
Date: Tue, 18 Aug 1998 15:13:46 +0200
Subject: Problem in "configure" for Solaris (cc) ?!
Message-ID: <199808181313.PAA28150@sophie.ethz.ch>

	[[0.62.3, already 0.62.2]]

This bug report is overdue,
but I really didn't test these things for weeks 
(have always used gcc, but only yesterday, someone told me that he saw a
speed gain of a factor 2 when using Sun's cc over gcc)

I just found that the same problem is already in 0.62.2

If I take yesterday's  
	R-release.tar.gz  (or also R-0.62.2.tar.gz)
unpack
and add
	CC=cc
	-----
to  config.site,
then
	./configure
	-----------
it ends VERY badly:

% ./configure
creating cache ./config.cache
checking for a BSD compatible install... /usr/local/bin/ginstall -c
checking whether ln -s works... yes
checking for ranlib... ranlib
checking for bison... bison -y
checking for ar... ar
checking for ratfor... ratfor
checking for latex... /afs/ethz.ch/public/teTeX/teTeX/bin/sparc-solaris2.5/latex
checking for dvips... /afs/ethz.ch/public/teTeX/teTeX/bin/sparc-solaris2.5/dvips
checking for perl... /usr/local/bin/perl
checking whether perl is perl 5... yes
checking for cc... cc
checking for ranlib... (cached) ranlib
checking for f77... f77
checking for underscore after Fortran symbols... configure: error: Nothing worked - cannot use FORTRAN

-----------
and then  STOP, nothing, ....

When I omit the "CC=cc", gcc is used (together with f77), and all is well.

  % cc -V
  cc: SC3.0.1 13 Jul 1994
  usage: cc [ options] files.  Use 'cc -flags' for details

  % f77 -V
  f77: SC3.0.1 13 Jul 1994
  Usage: f77 [ options ] files.  Use 'f77 -flags' for details

  % uname -a
  SunOS florence 5.5 Generic sun4u sparc SUNW,Ultra-1

------------------------------------------------

So yes, the  SC.. compilers are somewhat dated, but still...

-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From Martin Maechler <maechler@stat.math.ethz.ch>  Tue Aug 18 14:54:22 1998
From: Martin Maechler <maechler@stat.math.ethz.ch> (Martin Maechler)
Date: Tue, 18 Aug 1998 15:54:22 +0200
Subject: Problem in "configure" for Solaris (cc) -- solved (partly) --
In-Reply-To: <199808181313.PAA28150@sophie.ethz.ch> (message from Martin
 Maechler on Tue, 18 Aug 1998 15:13:46 +0200)
Message-ID: <199808181354.PAA01678@sophie.ethz.ch>

(answering myself, after a suggestion by Kurt Hornik
		 which actually was *not* the solution.. but...)

The configure problem I've just reported
(( setting   CC=cc  in  config.site ))

actually does not show when I either leave the CFLAGS commented out
(which results in CFLAGS=-g) or set CFLAGS="-g -O",

however, it *DOES* show when I set
	CFLAGS=-O2 or CFLAGS="-g -O2"

----

Reading the long output of  ``cc -flags'',
I see that more than average optimization is done using
	cc -xO[1-4]
	    #
Still very funny that "-O2" leads to such bad results with ./configure.
Maybe configure could become smarter here...

Martin
-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From p.dalgaard@biostat.ku.dk  Tue Aug 18 15:25:20 1998
From: p.dalgaard@biostat.ku.dk (Peter Dalgaard BSA)
Date: 18 Aug 1998 16:25:20 +0200
Subject: Problem in "configure" for Solaris (cc) -- solved (partly) --
In-Reply-To: Martin Maechler's message of Tue, 18 Aug 1998 15:54:22 +0200
References: <199808181354.PAA01678@sophie.ethz.ch>
Message-ID: <x2lnom2wq7.fsf@biostat.ku.dk>

Martin Maechler <maechler@stat.math.ethz.ch> writes:

> Reading the long output of  ``cc -flags'',
> I see that more than average optimization is done using
> 	cc -xO[1-4]
> 	    #
> Still very funny that "-O2" leads to such bad results with ./configure.
> Maybe configure could become smarter here...

This is a generic problem with autoconf: It goes looking for X, the
compile doesn't work because of Y, and it reports that X is missing.
Of course, one should always check config.log when something goes
wrong, it contains the details of the compiler failure and the failed
programs. 

Our configure starts off with a compiler sanity check, but apparently
it isn't enough?


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard@biostat.ku.dk)             FAX: (+45) 35327907
-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From bates@stat.wisc.edu  Tue Aug 18 15:45:22 1998
From: bates@stat.wisc.edu (Douglas Bates)
Date: 18 Aug 1998 09:45:22 -0500
Subject: Problem in "configure" for Solaris (cc) -- solved (partly) --
In-Reply-To: Peter Dalgaard BSA's message of "18 Aug 1998 16:25:20 +0200"
References: <199808181354.PAA01678@sophie.ethz.ch> <x2lnom2wq7.fsf@biostat.ku.dk>
Message-ID: <6rww86nybh.fsf@verdi.stat.wisc.edu>

>>>>> "PD" == Peter Dalgaard BSA <p.dalgaard@biostat.ku.dk> writes:

  PD> This is a generic problem with autoconf: It goes looking for X,
  PD> the compile doesn't work because of Y, and it reports that X is
  PD> missing.  Of course, one should always check config.log when
  PD> something goes wrong, it contains the details of the compiler
  PD> failure and the failed programs.

I have seen another such problem.  The check for -lblas is done with
the C compiler invoked on a short C program stub.  For me the blas
library is found but the program does not link properly because of
missing Fortran library routines.  Configure then decides that there
is no blas library present.
-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From Prof Brian Ripley <ripley@stats.ox.ac.uk>  Tue Aug 18 18:28:50 1998
From: Prof Brian Ripley <ripley@stats.ox.ac.uk> (Prof Brian Ripley)
Date: Tue, 18 Aug 1998 18:28:50 +0100 (BST)
Subject: Problem in "configure" for Solaris (cc) -- solved (partly) --
Message-ID: <199808181728.SAA04981@toucan.stats.ox.ac.uk>

> From: Peter Dalgaard BSA <p.dalgaard@biostat.ku.dk>
> 
> Martin Maechler <maechler@stat.math.ethz.ch> writes:
> 
> > Reading the long output of  ``cc -flags'',
> > I see that more than average optimization is done using
> > 	cc -xO[1-4]
> > 	    #

And -O is -xO2. If you do want higher speed, you need to use
other flags too, and -fast is a good start.

> > Still very funny that "-O2" leads to such bad results with ./configure.
> > Maybe configure could become smarter here...
> 
> This is a generic problem with autoconf: It goes looking for X, the
> compile doesn't work because of Y, and it reports that X is missing.
> Of course, one should always check config.log when something goes
> wrong, it contains the details of the compiler failure and the failed
> programs. 

Not this time. See below.

> Our configure starts off with a compiler sanity check, but apparently
> it isn't enough?

No, as it is not invoked if the compiler is specified via CC. In
Martin's configuration, config.log gives

$ CC=cc CFLAGS=-O2 ./configure
$ cat config.log
This file contains any messages produced by compilers while
running configure, to aid debugging if configure makes a mistake.

configure:708: checking for a BSD compatible install
configure:758: checking whether ln -s works
configure:781: checking for ranlib
configure:812: checking for bison
configure:845: checking for ar
configure:873: checking for ratfor
configure:902: checking for latex
configure:934: checking for dvips
configure:974: checking for perl
configure:1004: checking whether perl is perl 5
configure:1200: checking for cc
configure:1261: checking for ranlib
configure:1325: checking for f77
configure:1402: checking for underscore after Fortran symbols

Note, no details of the compiler failure appear ....


As for speed comparisons:

A piece of the ch14 script from V&R2 (highly C-dependent), Solaris 2.6, 
Sparc 20 (about 4 years old)

library(MASS)
library(tree)
data(fgl)
unix.time({
fgl.tr <- tree(type ~ ., fgl)
fgl.cv <- cv.tree(fgl.tr,, prune.misclass)
for(i in 2:5)  fgl.cv$dev <- fgl.cv$dev + 
     cv.tree(fgl.tr,, prune.misclass)$dev
fgl.cv$dev <- fgl.cv$dev/5})

gcc2.8.1 60.26secs
cc SC4.0 60.35secs

(the close agreement is fortuitous: I had to tune each to get about
10% improvement in speed).

Brian

-- 
Brian D. Ripley,                  ripley@stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From p.dalgaard@biostat.ku.dk  Tue Aug 18 20:39:34 1998
From: p.dalgaard@biostat.ku.dk (Peter Dalgaard BSA)
Date: 18 Aug 1998 21:39:34 +0200
Subject: Problem in "configure" for Solaris (cc) -- solved (partly) --
In-Reply-To: Prof Brian Ripley's message of Tue, 18 Aug 1998 18:28:50 +0100 (BST)
References: <199808181728.SAA04981@toucan.stats.ox.ac.uk>
Message-ID: <x2n292xeo9.fsf@biostat.ku.dk>

Prof Brian Ripley <ripley@stats.ox.ac.uk> writes:

> $ CC=cc CFLAGS=-O2 ./configure
> $ cat config.log
> This file contains any messages produced by compilers while
> running configure, to aid debugging if configure makes a mistake.
> 
> configure:708: checking for a BSD compatible install
...
> 
> Note, no details of the compiler failure appear ....

Hmmm. Yes. Easily reproduced on other systems by using:

CC=/bin/false ./configure

The basic problem here is that this part of configure.in is handwritten,
and doesn't use the "autoconf conventions" (which are probably as
well-documented as certain aspects of R...) of sending the compile
commands and in the case of failure also the program contents to file
descriptor 5.  I.e. what yields all the constructions of the type

if { (eval echo configure:1475: \"$ac_link\") 1>&5; (eval $ac_link)
2>&5; } && test -s conftest; then
  rm -rf conftest*
  eval "ac_cv_lib_$ac_lib_var=yes"
else
  echo "configure: failed program was:" >&5
  cat conftest.$ac_ext >&5
  rm -rf conftest*
  eval "ac_cv_lib_$ac_lib_var=no"
fi

There must be a relevant macro for this somewhere?

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard@biostat.ku.dk)             FAX: (+45) 35327907
-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From p.dalgaard@biostat.ku.dk  Wed Aug 19 15:01:05 1998
From: p.dalgaard@biostat.ku.dk (Peter Dalgaard BSA)
Date: 19 Aug 1998 16:01:05 +0200
Subject: R 0.62.3 to be released soon
Message-ID: <x2n291xe8u.fsf@blueberry.kubism.ku.dk>

For the (slightly) adventurous among you:

We plan to roll out R 0.62.3 next Friday. In order to root out
remaining bugs before the release, I'd like to encourage you to try it
out on your system. 

Since there is a risk of destabilising things whenever you mess with
the configure scripts, we won't attack installation problems on
unusual platforms after this week.

We also won't take on any complicated bugs, unless they have very bad
effects. (This is a good time to pick nits...)

Note that the "horses mouth" for snapshot releases of R is 

ftp.stat.math.ethz.ch:/pub/Software/R/

The daily snapshots are also mirrored to the CRAN sites, but not
always on a daily basis.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard@biostat.ku.dk)             FAX: (+45) 35327907
-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From pgilbert@bank-banque-canada.ca  Wed Aug 19 19:36:09 1998
From: pgilbert@bank-banque-canada.ca (Paul Gilbert)
Date: Wed, 19 Aug 1998 14:36:09 -0400
Subject: R 0.62.3 to be released soon
References: <x2n291xe8u.fsf@blueberry.kubism.ku.dk>
 <98Aug19.120624edt.13485@mailgate.bank-banque-canada.ca> <x2d89xx7sp.fsf@blueberry.kubism.ku.dk>
Message-ID: <98Aug19.144503edt.13473@mailgate.bank-banque-canada.ca>

I've compiled and installed pre R 0.62.3 (Aug. 19) under Solaris 2.6 using gcc
version 2.8.1 and g77 0.5.23. Everything went smoothly and my DSE library tests
work ok. There is a very occasional problem with a "Segmentation Fault - core
dumped", which also occured in 0.62.2, but I have not been able to reproduce it
reliably. I also had to increase R -v from 250000 cells (which worked with
0.62.2) to 300000 cells for one of my tests.

There is however, a small problem with installing the HTML documention in a
private (i.e. not RHOME/library) location. The file

   RHOME/doc/html/packages.html

linking the main documention to the packages is written (BTW this will probably
fail if the package owner and the R owner are not the same) but the links in it
are not quite correct. For me the link indicated in that file is

  <A HREF="../../library/dse/html/00Index.html">
which would be RHOME/library/dse/html/00Index.html, but it should be
 <A HREF="{package_location}/dse/html/00Index.html">

Paul Gilbert

-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From pgilbert@bank-banque-canada.ca  Thu Aug 20 19:57:57 1998
From: pgilbert@bank-banque-canada.ca (Paul Gilbert)
Date: Thu, 20 Aug 1998 14:57:57 -0400
Subject: R 0.62.3 to be released soon
Message-ID: <98Aug20.145941edt.13442@mailgate.bank-banque-canada.ca>

Installing a package with documentation seems to clobber the file

RHOME/doc/html/function.html

which has the links to all the HTML  function documentation, so access to the R
base documentation is almost impossible.

Paul Gilbert

-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From p.dalgaard@biostat.ku.dk  Thu Aug 20 22:28:07 1998
From: p.dalgaard@biostat.ku.dk (Peter Dalgaard BSA)
Date: 20 Aug 1998 23:28:07 +0200
Subject: R 0.62.3 to be released soon
In-Reply-To: Paul Gilbert's message of Thu, 20 Aug 1998 14:57:57 -0400
References: <98Aug20.145941edt.13442@mailgate.bank-banque-canada.ca>
Message-ID: <x2r9ybibrs.fsf@biostat.ku.dk>

Paul Gilbert <pgilbert@bank-banque-canada.ca> writes:

> 
> Installing a package with documentation seems to clobber the file
> 
> RHOME/doc/html/function.html
> 
> which has the links to all the HTML  function documentation, so access to the R
> base documentation is almost impossible.
> 
> Paul Gilbert

I took the liberty of forwarding this to r-bugs and ditto with your
note about 'private' libraries.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard@biostat.ku.dk)             FAX: (+45) 35327907
-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From p.dalgaard@biostat.ku.dk  Fri Aug 21 15:30:25 1998
From: p.dalgaard@biostat.ku.dk (Peter Dalgaard BSA)
Date: 21 Aug 1998 16:30:25 +0200
Subject: R 0.62.3 to be released soon
In-Reply-To: Paul Gilbert's message of Fri, 21 Aug 1998 10:04:14 -0400
References: <98Aug20.145941edt.13442@mailgate.bank-banque-canada.ca>
 <x2r9ybibrs.fsf@biostat.ku.dk>
 <98Aug21.100624edt.13448@mailgate.bank-banque-canada.ca>
Message-ID: <x2btpe9zlq.fsf@blueberry.kubism.ku.dk>

Paul Gilbert <pgilbert@bank-banque-canada.ca> writes:

> 
> >I took the liberty of forwarding this to r-bugs and ditto with your
> >note about 'private' libraries.
> 
> Peter
> 
> Please remind me again what r-bugs is. When should I be posting there instead?

Sorry, I was very tired when I wrote that...

R-bugs@biostat.ku.dk is the mail interface to our new bug-tracking
system. So what I did was to file the letters as official bug reports.
There was no error on your behalf, we still need to figure out what
the practical way of using this system really is. 

The purpose of forwarding the notes was just to get them filed so that
we won't (easily) forget about them. I did it using 'resend' so they
appear to come from you, and I though I'd better tell you.

Also, I was testing some parts of the interface at the same time,
namely what happens with the e-mail notification of the core team when
you do a forward like that. (It turns out that it recognises the 'Re:'
string and skips the notification, which would seem to be a bit of a
bug in the bug tracking system...)

(Note the cc: to r-devel in *this* note, by the way)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard@biostat.ku.dk)             FAX: (+45) 35327907
-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From pgilbert@bank-banque-canada.ca  Fri Aug 21 17:19:13 1998
From: pgilbert@bank-banque-canada.ca (Paul Gilbert)
Date: Fri, 21 Aug 1998 12:19:13 -0400
Subject: couldn't find FUN
Message-ID: <98Aug21.122130edt.13441@mailgate.bank-banque-canada.ca>

The call to sweep in this function which was working in 0.62.2 is giving me
trouble in 62.3:

prcomponents <- function(x, center=TRUE, scale=TRUE, N=nrow(x)-1)
   {if (center) center <- apply(x,2,mean)
    else        center <- rep(0, ncol(x))
    if (scale)  scale  <- sqrt(apply(x,2,var))
    else        scale  <- rep(1, ncol(x))
    s <- svd(sweep(sweep(x,2, center),2, scale, FUN="/"))
    # remove anything corresponding to effectively zero singular values.
    rank <- sum(s$d > (s$d[1]*sqrt(.Machine$double.eps)))
    if (rank < ncol(x)) s$v <- s$v[,1:rank, drop=FALSE]
    s$d <- s$d/sqrt(max(1,N))

#   r <- list(sdev=s$d, proj=s$v,x=x %*% s$v, center=center, scale=scale)
    r <- list(sdev=s$d, proj=s$v, center=center, scale=scale)
    r
}

> data(crimes)
> prcomponents(crimes)
Error: couldn't find function "FUN"
> traceback()
[1] "eval(f)"
[2] "Ops.data.frame(x, aperm(array(STATS, dims[perm]), order(perm)), "
[3] "\t    ...)"
[4] "sweep(x, 2, center)"
[5] "sweep(sweep(x, 2, center), 2, scale, FUN = \"/\")"
[6] "svd(sweep(sweep(x, 2, center), 2, scale, FUN = \"/\"))"
[7] "prcomponents(crimes)"
>

Paul Gilbert

-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From pgilbert@bank-banque-canada.ca  Fri Aug 21 18:30:23 1998
From: pgilbert@bank-banque-canada.ca (Paul Gilbert)
Date: Fri, 21 Aug 1998 13:30:23 -0400
Subject: couldn't find FUN
References: <98Aug21.122130edt.13441@mailgate.bank-banque-canada.ca>
Message-ID: <98Aug21.133236edt.13450@mailgate.bank-banque-canada.ca>

>> data(crimes)
>> prcomponents(crimes)
>Error: couldn't find function "FUN"

This does not work in 0.62.2 as I previously reported. The error is associated
with using
data(crimes) which is not what I've usually done. It is fixed by replacing

   s <- svd(sweep(sweep(x,2, center),2, scale, FUN="/"))

with

   s <- svd(sweep(sweep(as.matrix(x),2, center),2, scale, FUN="/"))

So, I'm not sure if this is a bug or if this is the way it should work? For my
purposes it is fixed already.

Paul Gilbert



-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From ripley@stats.ox.ac.uk  Fri Aug 21 18:37:01 1998
From: ripley@stats.ox.ac.uk (Prof Brian D Ripley)
Date: Fri, 21 Aug 1998 18:37:01 +0100 (BST)
Subject: couldn't find FUN
In-Reply-To: <98Aug21.122130edt.13441@mailgate.bank-banque-canada.ca>
Message-ID: <Pine.GSO.3.96.980821182855.4962E-100000@localhost>

On Fri, 21 Aug 1998, Paul Gilbert wrote:

> The call to sweep in this function which was working in 0.62.2 is giving me
> trouble in 62.3:

The call is invalid:

> ?sweep

Sweep out Array Summaries

     sweep(x, MARGIN, STATS, FUN="-", ...)

Arguments:

       x: an array.

> class(crimes)
[1] "data.frame"

If you call sweep on a data frame, as in

> sweep(crimes, 2, "-")

it does not work: it should not have done so before.  On the other hand, in
0.62.2, valid arithmetic operations on data frames gave matrices,
which helped your illegal example but were incorrect on legal ones.
You need an as.matrix in your code, I believe.


This example also fails in S.

-- 
Brian D. Ripley,                  ripley@stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From p.dalgaard@biostat.ku.dk  Fri Aug 21 23:34:02 1998
From: p.dalgaard@biostat.ku.dk (Peter Dalgaard BSA)
Date: 22 Aug 1998 00:34:02 +0200
Subject: couldn't find FUN
In-Reply-To: Prof Brian D Ripley's message of Fri, 21 Aug 1998 18:37:01 +0100 (BST)
References: <Pine.GSO.3.96.980821182855.4962E-100000@localhost>
Message-ID: <x2ww82ars5.fsf@biostat.ku.dk>

Prof Brian D Ripley <ripley@stats.ox.ac.uk> writes:

> 
> On Fri, 21 Aug 1998, Paul Gilbert wrote:
> 
> > The call to sweep in this function which was working in 0.62.2 is giving me
> > trouble in 62.3:
..
> 
> If you call sweep on a data frame, as in
> 
> > sweep(crimes, 2, "-")

Um, you need some STATS in there...

> 
> it does not work: it should not have done so before.  On the other hand, in
> 0.62.2, valid arithmetic operations on data frames gave matrices,
> which helped your illegal example but were incorrect on legal ones.
> You need an as.matrix in your code, I believe.
> 
> 
> This example also fails in S.

Still, something weird is going on. First of all, the error message
about not finding "FUN" is less than obvious and secondly, it's not in
general a problem to do arithmetic on data frames as if they were
matrices. (E.g. crimes - as.matrix(crimes)) I think we have some of
the explanation in:

> f<-get("-")                         
> f(crimes,as.matrix(crimes))
Warning: ignored non function "f"
Warning: ignored non function "f"
Warning: ignored non function "f"
Warning: ignored non function "f"
               Murder Assault UrbanPop Rape
Alabama             0       0        0    0
Alaska              0       0        0    0
Arizona             0       0        0    0

i.e. "something" applies "f" for each column and there's some kind of
mess related to scoping (there really is no other "f" around before
the function is called!)

It happens only with .Primitive functions like "-". Stuff like

sweep(crimes, 2, apply(crimes, 2, mean), function(x, y) x - y)

works perfectly.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard@biostat.ku.dk)             FAX: (+45) 35327907
-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From ripley@stats.ox.ac.uk  Sat Aug 22 18:06:00 1998
From: ripley@stats.ox.ac.uk (Prof Brian D Ripley)
Date: Sat, 22 Aug 1998 18:06:00 +0100 (BST)
Subject: Handling of offsets in glm is really inconsistent.
Message-ID: <Pine.GSO.3.96.980822172731.6251A-100000@localhost>

[Copied to R-devel for information]

This applies to all versions of R I have: 0.62.2, 0.62.3, 0.63.
Great care seems needed with glms with offsets, as many things seem
wrong.

Consider the following:

> data(freeny)
> freeny.glm <- glm(y ~ offset(lag.quarterly.revenue) + price.index +
    income.level + market.potential, data=freeny, subset=1:30)
> predict(freeny.glm)
            Qtr1       Qtr2       Qtr3       Qtr4
1962:         NA 0.01040457 0.01073223 0.01233351
1963: 0.01211730 0.02744293 0.03259214 0.03225403
1964: 0.03235081 0.03272723 0.03361984 0.03114351
1965: 0.03128098 0.03371820 0.02869783 0.02670215
1966: 0.02346093 0.02172193 0.02269731 0.01927851
1967: 0.02516107 0.02496217 0.02679227 0.03099460
1968: 0.03225807 0.03418791 0.03857815 0.03324247
1969: 0.02575733 0.02702418 0.02988582         NA
> predict(freeny.glm, type="response")
          Qtr1     Qtr2     Qtr3     Qtr4
1962:       NA 8.806765 8.803092 8.803704
1963: 8.826977 8.840453 8.940102 8.968984
1964: 8.993961 8.993167 9.042300 9.061634
1965: 9.100341 9.092428 9.135678 9.153552
1966: 9.194421 9.208372 9.260927 9.284149
1967: 9.309521 9.338742 9.377042 9.389345
1968: 9.429928 9.455688 9.480808 9.520452
1969: 9.549497 9.566824 9.611116       NA

so one might think that prediction of a glm with an offset should
include the offset on the response scale but not link scale. However,

> predict(freeny.glm, newdata=freeny)
   1962.25     1962.5    1962.75       1963    1963.25 
0.01040457 0.01073223 0.01233351 0.01211730 0.02744293 
    1963.5    1963.75       1964    1964.25     1964.5 
0.03259214 0.03225403 0.03235081 0.03272723 0.03361984 
   1964.75       1965    1965.25     1965.5    1965.75 
0.03114351 0.03128098 0.03371820 0.02869783 0.02670215 
      1966    1966.25     1966.5    1966.75       1967 
0.02346093 0.02172193 0.02269731 0.01927851 0.02516107 
   1967.25     1967.5    1967.75       1968    1968.25 
0.02496217 0.02679227 0.03099460 0.03225807 0.03418791 
    1968.5    1968.75       1969    1969.25     1969.5 
0.03857815 0.03324247 0.02575733 0.02702418 0.02988582 
   1969.75       1970    1970.25     1970.5    1970.75 
0.02686281 0.03816228 0.03910487 0.04325638 0.03599068 
      1971    1971.25     1971.5    1971.75 
0.03357946 0.03890549 0.04196893 0.03952385 

> predict(freeny.glm, newdata=freeny, type="response")
   1962.25     1962.5    1962.75       1963    1963.25 
0.01040457 0.01073223 0.01233351 0.01211730 0.02744293 
    1963.5    1963.75       1964    1964.25     1964.5 
0.03259214 0.03225403 0.03235081 0.03272723 0.03361984 
   1964.75       1965    1965.25     1965.5    1965.75 
0.03114351 0.03128098 0.03371820 0.02869783 0.02670215 
      1966    1966.25     1966.5    1966.75       1967 
0.02346093 0.02172193 0.02269731 0.01927851 0.02516107 
   1967.25     1967.5    1967.75       1968    1968.25 
0.02496217 0.02679227 0.03099460 0.03225807 0.03418791 
    1968.5    1968.75       1969    1969.25     1969.5 
0.03857815 0.03324247 0.02575733 0.02702418 0.02988582 
   1969.75       1970    1970.25     1970.5    1970.75 
0.02686281 0.03816228 0.03910487 0.04325638 0.03599068 
      1971    1971.25     1971.5    1971.75 
0.03357946 0.03890549 0.04196893 0.03952385 

and prediction on either scale with newdata ignores the offset. Now, S is
also inconsistent (prior to S-PLUS 4.5 rel2), but at least it does usually
include the offset (except for prediction on link scale with no newdata,
where the offset was omitted until recently). [The different print layout
is due to the original response being a time series of class "ts"; the
predict method cannot know that.]

The discrepancy is in predict.lm (not predict.glm) which ignores offsets in
R but includes them in S. Correcting it is made difficult by the way
delete.response also deletes offsets in R: 

> terms(freeny.glm)
y ~ offset(lag.quarterly.revenue) + price.index + income.level + 
            market.potential
attr(,"variables")
list(y, offset(lag.quarterly.revenue), price.index, income.level, 
            market.potential)
attr(,"factors")
                              price.index income.level market.potential
y                                       0            0                0
offset(lag.quarterly.revenue)           0            0                0
price.index                             1            0                0
income.level                            0            1                0
market.potential                        0            0                1
attr(,"term.labels")
[1] "price.index"      "income.level"     "market.potential"
attr(,"order")
[1] 1 1 1
attr(,"intercept")
[1] 1
attr(,"response")
[1] 1
attr(,"offset")
[1] 2

> delete.response(terms(freeny.glm))
~price.index + income.level + market.potential
attr(,"variables")
list(price.index, income.level, market.potential)
attr(,"factors")
                 price.index income.level market.potential
price.index                1            0                0
income.level               0            1                0
market.potential           0            0                1
attr(,"term.labels")
[1] "price.index"      "income.level"     "market.potential"
attr(,"order")
[1] 1 1 1
attr(,"intercept")
[1] 1
attr(,"response")
[1] 0

and that is definitely not what is documented to happen.  I do not begin to
understand why delete.response is written the way it is: it looks like it
needs a complete re-design. 

So

- delete.response needs to only delete responses.
- predict.lm needs to handle offsets (or predict.glm, but it is
  much cleaner in predict.lm).
- glm.fit should return  linear.predictors = eta + offset.

There is another small problem:

> predict(freeny.glm, se=T)
Error: Object "price.index" not found

as predict.lm does not recognize that newdata was missing in the caller
(how lazy is lazy evaluation?)  The simplest way out is to have

   if (missing(newdata) || is.null(newdata)) X <- model.matrix(object)

the alternative is to test missing(newdata) in predict.glm.

While this is being done, I wonder why R does not implement offsets
for lm()?  It `looks like an easy exercise'.

-- 
Brian D. Ripley,                  ripley@stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From pgilbert@bank-banque-canada.ca  Mon Aug 24 14:24:17 1998
From: pgilbert@bank-banque-canada.ca (Paul Gilbert)
Date: Mon, 24 Aug 1998 09:24:17 -0400
Subject: R-beta: re -n -v wr0613b - windows dynload
References: <35DE9A90.166E324E@mail1.stofanet.dk>
Message-ID: <98Aug24.092850edt.13455@mailgate.bank-banque-canada.ca>

>When I use the -v I can modify the size of the heap, as assessed by
>gc(), but the -n key seems to be without effect ?

In src/include/Defn.h the following are set and if you exceed these values I
believe your setting is ignored. (I don't think there is any warning but it
would be nice.)

#ifndef R_NSIZE
#define R_NSIZE  200000L  /* number of cons cells */
#endif

#ifndef R_VSIZE
#define R_VSIZE  2000000L /* vector heap size in bytes */
#endif

Paul Gilbert

-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From pgilbert@bank-banque-canada.ca  Mon Aug 24 16:08:38 1998
From: pgilbert@bank-banque-canada.ca (Paul Gilbert)
Date: Mon, 24 Aug 1998 11:08:38 -0400
Subject: prcomp and princomp
Message-ID: <98Aug24.111223edt.13450@mailgate.bank-banque-canada.ca>

Below are mva functions prcomp and princomp as well as my own version
prcomponents. Also included are contents for prcomp.Rd and princomp.Rd.
Functions prcomp and princomp are intended to replicate Splus results. (I think
this princomp is already in the R distribution, but the version of prcomp
already in the R distribution does not reproduce Splus results.)

My version, prcomponent, is preliminary and as yet undocumented, but I would
appreciate comments. It uses the preferred svd as in prcomp, not eigen as in
princomp,  but returns some extra information as in princomp. Following Bill
Venables suggestion, it also allows the user to specify the effective scaling
factor (e.g. N or N-1).

Also, I have just noticed that there is some very small duplication
(print.princomp, summary.princomp, plot.princomp) between the functions  below
and the file princomp-add.R already in the R distribution. If someone could pick
the best versions I would appreciate it. I am about to get buried with some
other work and would like to get this in the R distribution before it gets lost
on my desk (again).

Paul Gilbert

############## prcomp ##############

prcomp <- function(x, retx=TRUE, center=TRUE, scale=FALSE) {
# s <- svd(scale(x, center=center, scale=scale),nu=0)
# above produces warning since scale is both a function and a variable so
 s <- svd(get("scale",envir=.GlobalEnv)
   (x, center=center, scale=scale),nu=0)
# rank <- sum(s$d > 0)
 rank <- sum(s$d > (s$d[1]*sqrt(.Machine$double.eps)))
 if (rank < ncol(x)) s$v <- s$v[,1:rank, drop=F]
 s$d <- s$d/sqrt(max(1,nrow(x) -1))
 if(retx) r <- list(sdev=s$d, rotation=s$v, x=as.matrix(x) %*% s$v)
 else     r <- list(sdev=s$d, rotation=s$v)
 class(r) <- "prcomp"
 r
}

print.prcomp <- function(x) {
 cat("Standard deviations:\n")
 print(x$sdev)
 cat("\nRotation:\n")
 print(x$rotation)
 if (!is.null(x$x))
  {cat("\nRotated variables:\n")
   print(x$x)
  }
 invisible(x)
}

plot.prcomp <- function(obj, x=1, y=2, main="Scree Plot",
  xlab=paste("Principle component", x),
  ylab=paste("Principle component", y), ...) {
    if (is.null(obj$x))
 stop("Rotated x has not be calculated by prcomp. Use retx=T in prcomp.")
    plot(obj$x[,c(x,y)], main=main, xlab=xlab, ylab=ylab, ...)
}

############### prcomp.Rd ###############

\name{prcomp}
\title{Principal Components Analysis}
\usage{
prcomp(x, retx=TRUE, center=TRUE, scale=FALSE)

}
\arguments{
\item{x}{a matrix (or data frame) which provides the data
for the principal components analysis.}
\item{retx}{a logical value indicating whether the rotated variables should
be returned.}
\item{center}{a logical value indicating whether the variables should
be shifted to be zero centered.}
\item{scale}{a logical value indicating whether the variables should
be scaled to have unit variance before the analysis takes place. The default
is F for consistency with S, but in general scaling is advisable.}
}
\description{
This function performs a principal components analysis on
the given data matrix and returns the results as a
\code{prcomp} object. The calculation is done with svd on the data matrix, not
by using eigen on the covariance matrix. This is generally the preferred method
for numerical accuracy.  The print method for the these
objects prints the results in a nice format and the
plot method produces a scree plot.
}
\value{
\code{prcomp} returns an list with class \code{"prcomp"}
containing the following components:
\item{sdev}{the standard deviation of the principal components
(i.e. the eigenvalues of the cov matrix - though the calculation is actually
done with the singular values of the data matrix.)}
\item{rotation}{the matrix of variable loadings (i.e. a matrix
whose columns contain the eigenvectors). The function princomp returns this
in the element \code{loadings}.}
\item{x}{if retx is true the value of the rotated data (the data multiplied by
the \code{rotation} matrix) is returned.}
}
\references{
Mardia, K. V., J. T. Kent, J and M. Bibby (1979),
\emph{Multivariate Analysis}, London: Academic Press.

Venerables, W. N. and B. D. Ripley (1994).
\emph{Modern Applied Statistics with S-Plus}, Springer-Verlag.
}

\seealso{
\code{\link{princomp}}, \code{\link{prcomponents}}, \code{\link{cor}},
\code{\link{cov}},
\code{\link{svd}}, \code{\link{eigen}}.
}
\examples{
# the variances of the variables in the
# crimes data vary by orders of magnitude
data(crimes)
prcomp(crimes)
prcomp(crimes,scale=TRUE)
}




############## princomp ##############

princomp <- function(x, cor=FALSE, scores=TRUE,
  subset=rep(TRUE, nrow(as.matrix(x)))) {
 # it is tempting to add  use="all.obs" which could be passed to cov or
 # cor but then the calculation of N is complicated.
 z<- as.matrix(x)[subset,, drop=F]
 N <- nrow(z)
 if(cor) cv <- get("cor",envir=.GlobalEnv)(z)
 else    cv <- cov(z)
#  (svd can be used but gives different signs for some vectors)
 edc <- eigen(cv)
 cn <- paste("Comp.", 1:ncol(cv), sep="")
 names(edc$values) <- cn
 dimnames(edc$vectors) <- list(dimnames(x)[[2]], cn)
 scr<- NULL
 if (cor)
   {sdev <- sqrt(edc$values)
    sc <- (apply(z,2,var)*(N-1)/N)^0.5
    if (scores)
        scr<-(scale(z,center=T,scale=T) %*% edc$vectors)*sqrt(N/(N-1))
   }
 else
   {sdev <- sqrt(edc$values*(N-1)/N)
    sc <- rep(1, ncol(z))
    if (scores)
        scr<- (scale(z,center=T,scale=F) %*% edc$vectors)
   }
 names(sc) <- dimnames(x)[[2]]
 edc <-list(sdev=sdev, loadings=edc$vectors,
     center=apply(z,2,mean), scale=sc, n.obs=N, scores=scr)
# The Splus function also return list elements factor.sdev, correlations
# and coef, but these are not documented in the help. coef seems to equal
# load. The Splus function also return list elements call and terms which
# are not supported here.
 class(edc) <- "princomp"
 edc
}

print.princomp <- function(x) {
 cat("Standard deviations:\n")
 print(x$sdev)
 cat(length(x$scale), " variables and ", x$n.obs, "observations.\n")
 cat("Scale:\n")
 print(x$scale)
 invisible(x)
}

summary.princomp <- function(x) {
 per.var <- (x$sdev^2)/sum(x$sdev^2)
 r <- list(sdev=x$sdev, per.var= per.var, cum.var=cumsum(per.var))
 class(r) <- "summary.princomp"
 r
}

print.summary.princomp <- function(x) {
 cat("                        ",names(x$sdev),"\n")
 cat("Standard deviations   : ", x$sdev,      "\n")
 cat("Proportion of variance: ", x$per.var,   "\n")
 cat("Cumulative proportion : ", x$cum.var,   "\n")
 invisible(x)
}

plot.princomp <- function(obj, x=1, y=2, main="Scree Plot",
  xlab=paste("Principle component", x),
  ylab=paste("Principle component", y), ...) {
    plot(obj$scores[,c(x,y)], main=main, xlab=xlab, ylab=ylab, ...)
}

############### princomp.Rd ###############

\name{princomp}
\title{Principal Components Analysis}
\usage{
princomp(x, cor=FALSE, scores=TRUE, subset=rep(TRUE, nrow(as.matrix(x))))
print.princomp(obj)
summary.princomp(obj)
plot.princomp(obj)
}
\alias{print.princomp}
\alias{plot.princomp}
\arguments{
\item{x}{a matrix (or data frame) which provides the data
for the principal components analysis.}
\item{cor}{a logical value indicating whether the calculation should use the
correlation matrix or the covariance matrix.}
\item{score}{a logical value indicating whether the score on each principal
component should be calculated.}
\item{subset}{a vector used to select rows (observations) of the
data matrix \code{x}.}
}
\description{
This function performs a principal components analysis on
the given data matrix and returns the results as a \code{princomp} object.
The calculation is done using \code{eigen} on the correlation or
covariance matrix, as determined by \code{cor}. This is done for compatibility
with the Splus result (even though alternate forms for \code{x} - e.g. a
covariance matrix - are not supported as they are in Splus). A preferred method
of calculation is to use svd on \code{x}, as is done in \code{prcomp}.

Note that the scaling of results is affected by the setting of \code{cor}.
If \code{cor} is T then the divisor in the calculation of the sdev is N-1,
otherwise it is N. This has the effect that the result is slightly different
depending on whether scaling is done first on the data and cor set to F, or
done automatically in \code{princomp} with cor=T.

The print method for the these
objects prints the results in a nice format and the
plot method produces a scree plot.
}
\value{
\code{princomp} returns an list with class \code{"princomp"}
containing the following components:
\item{var}{the variances of the principal components
(i.e. the eigenvalues)}
\item{load}{the matrix of variable loadings (i.e. a matrix
whose columns contain the eigenvectors).}
\item{scale}{the value of the \code{scale} argument.}
}
\references{
Mardia, K. V., J. T. Kent and J. M. Bibby (1979).
\emph{Multivariate Analysis}, London: Academic Press.

Venerables, W. N. and B. D. Ripley (1994).
\emph{Modern Applied Statistics with S-Plus}, Springer-Verlag.
}
\seealso{
\code{\link{prcomp}}, \code{\link{prcomponents}}, \code{\link{cor}},
\code{\link{cov}}, \code{\link{eigen}}.
}
\examples{
# the variances of the variables in the
# crimes data vary by orders of magnitude
data(crimes)
princomp(crimes)
princomp(crimes,cor=TRUE)
princomp(scale(crimes, scale=T, center=T), cor=F)
}


############## prcomponents ##############

prcomponents <- function(x, center=TRUE, scale=TRUE, N=nrow(x)-1)
   {if (center) center <- apply(x,2,mean)
    else        center <- rep(0, ncol(x))
    if (scale)  scale  <- sqrt(apply(x,2,var))
    else        scale  <- rep(1, ncol(x))
    s <- svd(sweep(sweep(as.matrix(x),2, center),2, scale, FUN="/"))
    # remove anything corresponding to effectively zero singular values.
    rank <- sum(s$d > (s$d[1]*sqrt(.Machine$double.eps)))
    if (rank < ncol(x)) s$v <- s$v[,1:rank, drop=FALSE]
    s$d <- s$d/sqrt(N)

#   r <- list(sdev=s$d, proj=s$v,x=x %*% s$v, center=center, scale=scale)
    r <- list(sdev=s$d, proj=s$v, center=center, scale=scale)
    r
}



-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From Prof Brian Ripley <ripley@stats.ox.ac.uk>  Mon Aug 24 16:35:17 1998
From: Prof Brian Ripley <ripley@stats.ox.ac.uk> (Prof Brian Ripley)
Date: Mon, 24 Aug 1998 16:35:17 +0100 (BST)
Subject: prcomp and princomp
Message-ID: <199808241535.QAA21899@toucan.stats.ox.ac.uk>

> plot.prcomp <- function(obj, x=1, y=2, main="Scree Plot",
>   xlab=paste("Principle component", x),
>   ylab=paste("Principle component", y), ...) {
>     if (is.null(obj$x))
>  stop("Rotated x has not be calculated by prcomp. Use retx=T in prcomp.")
>     plot(obj$x[,c(x,y)], main=main, xlab=xlab, ylab=ylab, ...)
> }

> plot.princomp <- function(obj, x=1, y=2, main="Scree Plot",
>   xlab=paste("Principle component", x),
>   ylab=paste("Principle component", y), ...) {
>     plot(obj$scores[,c(x,y)], main=main, xlab=xlab, ylab=ylab, ...)
> }

Ouch. Enough of my students think that it is the spelling already!
This is _not_ what is meant by a scree plot, which plots the singular
values or eigenvalues.

-- 
Brian D. Ripley,                  ripley@stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From pgilbert@bank-banque-canada.ca  Mon Aug 24 17:50:17 1998
From: pgilbert@bank-banque-canada.ca (Paul Gilbert)
Date: Mon, 24 Aug 1998 12:50:17 -0400
Subject: prcomp and princomp
References: <199808241535.QAA21899@toucan.stats.ox.ac.uk>
Message-ID: <98Aug24.125405edt.13441@mailgate.bank-banque-canada.ca>

>Ouch. Enough of my students think that it is the spelling already!

Apologies. I thought I had eradicated this.

>This is _not_ what is meant by a scree plot, which plots the singular
>values or eigenvalues.

My mistake here was in the title main="Scree Plot", which in the end was not
what I decided to plot. At one point I thought the default plot method should be
the scree plot, but later decided the default plot method should plot the
principal components.  I noticed that in princomp-add.R you have

plot.princomp <- function(...) screeplot(...)

so perhaps you have different thoughts on this? I would propose having the
default plot the principal components (with the correct title) and use your
function scree, but not as the default.

Paul Gilbert


-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From p.dalgaard@biostat.ku.dk  Tue Aug 25 23:01:45 1998
From: p.dalgaard@biostat.ku.dk (Peter Dalgaard BSA)
Date: 26 Aug 1998 00:01:45 +0200
Subject: Variations on the t test
Message-ID: <x2g1ekk9fa.fsf@biostat.ku.dk>

One of the things that have been annoying me with both Splus and R is
that the "simple tests" are inconsistent with the lm/glm/dataframe
conventions, and that they become quite awkward to use when data have
to be extracted from a dataframe:

t.test(data$bp[data$sex=="F" & data$age>25], data$bp[data$sex=="M" &
data$age>25])

OK, so it's better to do 

eval(t.test(bp[sex=="F"],bp[sex=="M"]),subset(data,age>25))

but try explaining that to a class of MD's!


Below is a first sketch of a set of functions that extend t.test to
allow specification in terms of a list of vectors or a vector and a
grouping variable or a model formula, allowing data= and subset=
arguments as well. The paired test can also be specified using a
Pairs(x,y) construction, which should help prevent people from using
the wrong test. It's supposed to be downward compatible with the Splus
syntax.

I've been pushing Kurt to use a similar interface for all the ctest
functions, so I thought you'd all like to see them. If nothing else,
they should be interesting to study (they were definitely fun to
write), since they use some *really* dirty tricks relating to R's
evaluation model.

Anyway, here goes (remember, I said *first* sketch - there is at least
one bug in it, can you spot it?):

----
###
### The first couple of lines just guard against multiple loading
###

if(exists("t.test",envir=.GlobalEnv))rm(t.test)
if(exists("t.test.default",envir=.GlobalEnv))rm(t.test.default)

### Utility function to extract the environment the call is evaluated
### in. If you do eval(x,list), list is found as the variable "envir"
### in the stack frame immediately below the local frame

.ParentEnv<-function()
{
  parent<-sys.parent()
  grandparent <- sys.parent(2)
  if ( parent - grandparent == 1 )
    sys.frame(grandparent)
  else
    get("envir",envir=sys.frame(parent-1))
}

.t.test<-t.test

t.test.default<-function(x,...,group)
{
  e<-.ParentEnv()
  call <- if ( is.list(x) )
    substitute(t.test.list(x,...))
  else if (missing(group))
    substitute(.t.test(x,...))
  else 
    substitute(t.test.list(split(x, group),...))
  eval(call,e)
}

t.test<-function(...,data=sys.frame(sys.parent()),subset)
{  
  dname.add<-
  if (!missing(data)) 
    paste(", data frame:", deparse(substitute(data))) 
  else
    ""
  if (!missing(subset)){ 
    dname.add<-paste(dname.add, ", subset:", deparse(substitute(subset))) 
    subset <- eval(substitute(subset),data)
    data <- data[subset,] # had better be a data frame...
  }
  res<-eval(substitute(t.test.generic(...)), data)
  res$data.name <- paste(res$data.name, dname.add)
  res
}


t.test.generic<-function(x,...)
{
  UseMethod("t.test")
}

t.test.list<-function(l,...)
{
  if (length(l) != 2)
    stop("need exactly 2 groups")
  res<-.t.test(l[[1]],l[[2]],...)
  res$data.name<-deparse(substitute(l))
  nn<-names(l)
  if (length(nn) != 2)
    nn<-c("group 1", "group 2")
  names(res$estimate)<-nn
  res
}

t.test.formula<-function(f,...)
{
  e<-.ParentEnv()
  f[[1]]<-as.name("split")
  call<-as.call(list(as.name("t.test.list"), f, ...))
    eval(call,e)
}

Pairs<-function(x,y)
  structure(match.call(),class="paired")

t.test.paired<-function(l,...)
{
  e<-.ParentEnv()
  call<-c(as.list(l),list(...),list(paired=T))
  call[[1]]<-as.name(".t.test")
  call<-as.call(call)
  eval(call,e)
}




-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard@biostat.ku.dk)             FAX: (+45) 35327907
-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From pgilbert@bank-banque-canada.ca  Wed Aug 26 19:33:11 1998
From: pgilbert@bank-banque-canada.ca (Paul Gilbert)
Date: Wed, 26 Aug 1998 14:33:11 -0400
Subject: prcomp & princomp - revised
Message-ID: <98Aug26.143800edt.13450@mailgate.bank-banque-canada.ca>

My previous post about prcomp and princomp was done in some haste as I had long
ago indicated to Kurt that I would try to have this ready for the June release,
and it appeared that I would miss yet another release. I also need to get it out
before it becomes hopelessly buried by other work.

Brian Ripley kindly pointed out some errors, and also pointed out that I was
suggesting replacing some functions which were already working well in R. Other
than changing prcomp and princomp this was unintentional on my part. It seems
that some of these functions have either appeared since I first started this, or
that I was just careless (hard to imagine but it happens) and perhaps checked
only for prcomp methods and not princomp methods. In any event, I decided it was
necessary to check and indicate the changes I am proposing more carefully,
despite my time pressures, and also, first of all, to indicate my interest and
intentions more clearly.

I have no special attachment to principal components and there are many people
on this list who could do a better job at this than I can. I want to use PCA for
another problem I am working on, and I noticed several months ago that prcomp
and princomp were returning different results in R and Splus. Unless this is
fixed it puts me in the difficult position of having to choose between Splus and
R, which I would prefer not to do for reasons I have expressed from time to
time. (Another solution is to over-ride the mva library with my own functions,
but this has undesirable consequences when others use my library.) My intention
at this point is to submit these changes to the mva library and then not do any
more work on it. I am not suggesting that these methods are better, only that
they are more consistent with Splus. There is room for improvement (in both
Splus and R) and perhaps someone else would like to work on it. Of all the
changes below, the only ones I consider really important are the documentation
and the new version of prcomp which gives results like Splus.

I have some misgivings about adding yet another principal components function
(prcomponents below). However, I would like prcomp and princomp to return the
same results as Splus, and yet I see that there is need for improvement. I would
like this improvement to happen in a function which has a different name from
the functions in Splus, and perhaps be moved into an expanded "compatibility
library" at some point.

Paul Gilbert

_______

The code below makes the following changes relative to R Version 0.62.3 in
progress-release (August 19, 1998).

- prcomp is changed so that it takes similar arguments and usually reproduces
      Splus results. The criterion used for determining if a singular value
      is zero (following suggestions of Martin Maechler) is slightly different
      and this can occasionally give a different result.
- prcomp.Rd is changed to reflect the changes to prcomp
- summary.prcomp is defined to give a result more like summary.princomp. In
      Splus summary.prcomp does not exist and the default method is applied,
      but this difference seems reasonable?
- screeplot is made generic
- function(...) is changed to function(x, ...) for some plot methods (to be
         consistent with plot).
- print.prcomp is changed to be more like the result from Splus (Splus just
     uses print.default, so this is slightly different)
- plot.prcomp is renamed screeplot.prcomp to be consistent with the way
     this is done for princomp, but I have not worked on this
     function and it does not appear to do scaling or other nice things
     like screeplot.princomp. (Hopefully Brian Ripley will volunteer
     to fix it.)
- plot.prcomp calls - screeplot.prcomp


- princomp is still the function I submitted previously but has a
     few additional comments. It reproduces Splus results when the
     argument is a data matrix, but does not work with a covariance
     argument and the Splus version does.
- princomp.Rd is new.
- print.princomp is defined in princomp.R. This version is a bit more like
    the result from Splus and the function of the same name is removed
    from princomp-add.R.

- programs in the file princomp-add.R could be included in princomp.R but I
    was unsure how to deal with the V&R copyright. Perhaps it should
    be included in each function? I've left princomp-add.R as a
    separate file and done only the minimal number of changes necessary to
    make functions work with the changes in other files. (As far as my
    contributions in the other files are concerned I am happy to assign
    copyright to "The R Development Core Team".)


- a tentative and so far undocumented function prcomponents in the file
    prcomponents.R has been added. This is an incomplete attempt to deal
    with shortcomings of prcomp and princomp, the main problems with those
    functions being that the preferred computational method, svd, is used
    in prcomp, but princomp returns additional useful information. The
    function princomp uses eigen so that it returns results consistent
    with Splus, but does not yet support a covariance as an argument,
    which Splus does (and is the reason eigen is used). My function
    prcomponents also tries to give the user the ability to control
    what is used in the normalization (e.g. N or N-1), as suggested by
    Bill Venables. It does not include use="all.obs" which was in the
    prcomp I am proposing to replace, but it should. The difficulty is
    that this argument is passed to cor or cov, which are not
    called if svd is used.



############## prcomp.R replacement file ##############

screeplot    <- function(x, ...)  {UseMethod("screeplot")}

plot.prcomp   <- function(x, ...) {screeplot(x, ...)}


prcomp <- function(x, retx=TRUE, center=TRUE, scale=FALSE) {
# s <- svd(scale(x, center=center, scale=scale),nu=0)
# above produces warning since scale is both a function and a variable so
 s <- svd(get("scale",envir=.GlobalEnv)
   (x, center=center, scale=scale),nu=0)
# rank <- sum(s$d > 0)
 rank <- sum(s$d > (s$d[1]*sqrt(.Machine$double.eps)))
 if (rank < ncol(x)) s$v <- s$v[,1:rank, drop=F]
 s$d <- s$d/sqrt(max(1,nrow(x) -1))
 if(retx) r <- list(sdev=s$d, rotation=s$v, x=as.matrix(x) %*% s$v)
 else     r <- list(sdev=s$d, rotation=s$v)
 class(r) <- "prcomp"
 r
}

print.prcomp <- function(x) {
 cat("Standard deviations:\n")
 print(x$sdev)
 cat("\nRotation:\n")
 print(x$rotation)
 if (!is.null(x$x))
  {cat("\nRotated variables:\n")
   print(x$x)
  }
 invisible(x)
}

summary.prcomp <- function(obj, digits=3 )
{ vars <- obj$sdev^2
  vars <- vars/sum(vars)
  cat("Importance of components:\n")
  print(rbind("Standard deviation" = obj$sdev,
              "Proportion of Variance" = vars,
              "Cumulative Proportion" = cumsum(vars)))
  invisible(obj)
}

screeplot.prcomp <- function(x, main="Scree Plot", ylab="Variance",
  xlab="Component", ...) {
 plot(x$var, main=main, xlab=xlab, ylab=ylab, ...)
}

############### prcomp.Rd replacement file ###############

\name{prcomp}
\title{Principal Components Analysis}
\usage{
prcomp(x, retx=TRUE, center=TRUE, scale=FALSE)

}
\arguments{
\item{x}{a matrix (or data frame) which provides the data
for the principal components analysis.}
\item{retx}{a logical value indicating whether the rotated variables should
be returned.}
\item{center}{a logical value indicating whether the variables should
be shifted to be zero centered.}
\item{scale}{a logical value indicating whether the variables should
be scaled to have unit variance before the analysis takes place. The default
is F for consistency with S, but in general scaling is advisable.}
}
\description{
This function performs a principal components analysis on
the given data matrix and returns the results as a
\code{prcomp} object. The calculation is done with svd on the data matrix, not
by using eigen on the covariance matrix. This is generally the preferred method
for numerical accuracy.  The print method for the these
objects prints the results in a nice format and the
plot method produces a scree plot. The method \code{biplot} plots two selected
components against one another.
}
\value{
\code{prcomp} returns an list with class \code{"prcomp"}
containing the following components:
\item{sdev}{the standard deviation of the principal components
(i.e. the eigenvalues of the cov matrix - though the calculation is actually
done with the singular values of the data matrix.)}
\item{rotation}{the matrix of variable loadings (i.e. a matrix
whose columns contain the eigenvectors). The function princomp returns this
in the element \code{loadings}.}
\item{x}{if retx is true the value of the rotated data (the data multiplied by
the \code{rotation} matrix) is returned.}
}
\references{
Mardia, K. V., J. T. Kent, J and M. Bibby (1979),
\emph{Multivariate Analysis}, London: Academic Press.

Venerables, W. N. and B. D. Ripley (1994).
\emph{Modern Applied Statistics with S-Plus}, Springer-Verlag.
}

\seealso{
\code{\link{princomp}}, \code{\link{prcomponents}}, \code{\link{cor}},
\code{\link{cov}},
\code{\link{svd}}, \code{\link{eigen}}.
}
\examples{
# the variances of the variables in the
# crimes data vary by orders of magnitude
data(crimes)
prcomp(crimes)
prcomp(crimes,scale=TRUE)
plot(prcomp(crimes))
summary(prcomp(crimes))
}



############## princomp.R replacement file ##############

princomp <- function(x, cor=FALSE, scores=TRUE,
  subset=rep(TRUE, nrow(as.matrix(x)))) {
 # it is tempting to add  use="all.obs" which could be passed to cov or
 # cor but then the calculation of N is complicated.
 z<- as.matrix(x)[subset,, drop=F]
 N <- nrow(z)
 if(cor) cv <- get("cor",envir=.GlobalEnv)(z)
 else    cv <- cov(z)
#  (svd can be used but gives different signs for some vectors)
 edc <- eigen(cv)
 cn <- paste("Comp.", 1:ncol(cv), sep="")
 names(edc$values) <- cn
 dimnames(edc$vectors) <- list(dimnames(x)[[2]], cn)
 scr<- NULL
 if (cor)
   {sdev <- sqrt(edc$values)
    sc <- (apply(z,2,var)*(N-1)/N)^0.5
    if (scores)
        scr<-(scale(z,center=T,scale=T) %*% edc$vectors)*sqrt(N/(N-1))
   }
 else
   {sdev <- sqrt(edc$values*(N-1)/N)
    sc <- rep(1, ncol(z))
    if (scores)
        scr<- (scale(z,center=T,scale=F) %*% edc$vectors)
   }
 names(sc) <- dimnames(x)[[2]]
 edc <-list(sdev=sdev, loadings=edc$vectors,
     center=apply(z,2,mean), scale=sc, n.obs=N, scores=scr)
# The Splus function also return list elements factor.sdev, correlations
# and coef, but these are not documented in the help. coef seems to equal
# load. The Splus function also return list elements call and terms which
# are not supported here.
 class(edc) <- "princomp"
 edc
}

print.princomp <- function(x) {
 cat("Standard deviations:\n")
 print(x$sdev)
 cat(length(x$scale), " variables and ", x$n.obs, "observations.\n")
 cat("Scale:\n")
 print(x$scale)
 invisible(x)
}


############### princomp.Rd new file ###############

\name{princomp}
\title{Principal Components Analysis}
\usage{
princomp(x, cor=FALSE, scores=TRUE, subset=rep(TRUE, nrow(as.matrix(x))))
print.princomp(obj)
summary.princomp(obj)
plot.princomp(obj)
}
\alias{print.princomp}
\alias{plot.princomp}
\arguments{
\item{x}{a matrix (or data frame) which provides the data
for the principal components analysis.}
\item{cor}{a logical value indicating whether the calculation should use the
correlation matrix or the covariance matrix.}
\item{score}{a logical value indicating whether the score on each principal
component should be calculated.}
\item{subset}{a vector used to select rows (observations) of the
data matrix \code{x}.}
}
\description{
This function performs a principal components analysis on
the given data matrix and returns the results as a \code{princomp} object.
The calculation is done using \code{eigen} on the correlation or
covariance matrix, as determined by \code{cor}. This is done for compatibility
with the Splus result (even though alternate forms for \code{x} - e.g. a
covariance matrix - are not supported as they are in Splus). A preferred method
of calculation is to use svd on \code{x}, as is done in \code{prcomp}.

Note that the scaling of results is affected by the setting of \code{cor}.
If \code{cor} is T then the divisor in the calculation of the sdev is N-1,
otherwise it is N. This has the effect that the result is slightly different
depending on whether scaling is done first on the data and cor set to F, or
done automatically in \code{princomp} with cor=T.

The print method for the these
objects prints the results in a nice format and the
plot method produces a scree plot.
}
\value{
\code{princomp} returns an list with class \code{"princomp"}
containing the following components:
\item{var}{the variances of the principal components
(i.e. the eigenvalues)}
\item{load}{the matrix of variable loadings (i.e. a matrix
whose columns contain the eigenvectors).}
\item{scale}{the value of the \code{scale} argument.}
}
\references{
Mardia, K. V., J. T. Kent and J. M. Bibby (1979).
\emph{Multivariate Analysis}, London: Academic Press.

Venerables, W. N. and B. D. Ripley (1994).
\emph{Modern Applied Statistics with S-Plus}, Springer-Verlag.
}
\seealso{
\code{\link{prcomp}}, \code{\link{prcomponents}}, \code{\link{cor}},
\code{\link{cov}}, \code{\link{eigen}}.
}
\examples{
# the variances of the variables in the
# crimes data vary by orders of magnitude
data(crimes)
princomp(crimes)
princomp(crimes,cor=TRUE)
princomp(scale(crimes, scale=T, center=T), cor=F)
plot(princomp(crimes))
biplot(princomp(crimes))
summary(princomp(crimes))
loadings(princomp(crimes))
}


############## princomp-add.R replacement file ##############

# copyright (C) 1998 W. N. Venables and B. D. Ripley
#
predict.princomp <- function(object, newdata,...)
{
  if (missing(newdata)) return(object$scores)
  scale(newdata, object$center, object$scale) %*% object$loadings
}

summary.princomp <-
function(object, loadings = F, cutoff = 0.1, digits=3, ...)
{
  vars <- object$sdev^2
  vars <- vars/sum(vars)
  cat("Importance of components:\n")
  print(rbind("Standard deviation" = object$sdev,
              "Proportion of Variance" = vars,
              "Cumulative Proportion" = cumsum(vars)))
  if(loadings) {
    cat("\nLoadings:\n")
    cx <- format(round(object$loadings, digits=digits))
    cx[abs(object$loadings) < cutoff] <-
      substring("       ", 1, nchar(cx[1,1]))
    print(cx, quote = F, ...)
  }
  invisible(object)
}


plot.princomp <- function(x, ...) {screeplot(x, ...)}

screeplot.prcomp <- screeplot.princomp <-
function(x, npcs=min(10, length(x$sdev)), type=c("barplot", "lines"),
         main = deparse(substitute(x)), ...)
{
  eval(main)
  type <- match.arg(type)
  pcs <- x$sdev^2
  xp <- seq(length=npcs)
  if(type=="barplot") barplot(pcs[xp], names = names(pcs[xp]),
       main = main, ylab = "Variances", ...)
  else {
    plot(xp, pcs[xp], type = "b", axes = F, main = main, xlab="",
            ylab = "Variances", ...)
    axis(2)
    axis(1, at = xp, labels = names(pcs[xp]))
  }
  invisible()
}

loadings <- function(x) x$loadings



############## prcomponents.R  new file ##############

prcomponents <- function(x, center=TRUE, scale=TRUE, N=nrow(x)-1)
   {if (center) center <- apply(x,2,mean)
    else        center <- rep(0, ncol(x))
    if (scale)  scale  <- sqrt(apply(x,2,var))
    else        scale  <- rep(1, ncol(x))
    s <- svd(sweep(sweep(as.matrix(x),2, center),2, scale, FUN="/"))
    # remove anything corresponding to effectively zero singular values.
    rank <- sum(s$d > (s$d[1]*sqrt(.Machine$double.eps)))
    if (rank < ncol(x)) s$v <- s$v[,1:rank, drop=FALSE]
    s$d <- s$d/sqrt(N)

#   r <- list(sdev=s$d, proj=s$v,x=x %*% s$v, center=center, scale=scale)
    r <- list(sdev=s$d, proj=s$v, center=center, scale=scale)
    r
}




-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From Prof Brian Ripley <ripley@stats.ox.ac.uk>  Mon Aug 31 15:19:26 1998
From: Prof Brian Ripley <ripley@stats.ox.ac.uk> (Prof Brian Ripley)
Date: Mon, 31 Aug 1998 15:19:26 +0100 (BST)
Subject: Packages aov, modreg, lqs, psplines
Message-ID: <199808311419.PAA22293@toucan.stats.ox.ac.uk>

I now have versions of code that is destined (I believe) for 0.63 which
is in a suitable state for comment. The files are at

	ftp://ftp.stats.ox.ac.uk/pub/R  

(Our www server is being moved, so may be intermittently down, but this
ftp server should be stable.)  All are R packages, for the moment for
personal use only (no re-distribution). Use with 0.62.3 or 0.63 (although
I am aware of some problems with use with 0.63 that I have reported).


aov.tar.gz:
==========

aov with Error terms, proj, model.tables, se.contrast, replications, 
eff.aovlist, C, dummy.coef, add1, drop1, step, kappa, labels.

This is in I believe close to final form.


modreg.tar.gz:
=============

ksmooth                  Kernel Regression Smoother
loess                    Local Polynomial Regression Fitting
loess.control            Set Parameters for Loess
plot.ppr                 Plot Ridge Functions for Projection Pursuit
                            Regression Fit
ppr                      Projection Pursuit Regression
predict.loess            Predict Loess Curve or Surface
predict.smooth.spline    Predict from Smoothing Spline Fit
scatter.smooth           Scatter Plot with Smooth Curve Fitted by Loess
smooth.spline            Fit a Smoothing Spline
supsmu                   Friedmans's SuperSmoother

Probably only bug-fixing left, but I would welcome comments about the
extent of the loess functionality.


lqs.tar.gz:
==========

cov.rob                  Robust Estimation of Multivariate Location and
                            Scatter
lqs                      Resistant Regression by Least Trimmed and Least
                            Quantile Sum of Squares
lmsreg, ltsreg, cov.mve  Compatibility functions

This is much less complete (and the claimed mcd method is not yet
operational).  Comments please both on the design and from any experts
out there on the methodology used. (BTW, as all the programs I have give
different answers, it is very hard to establish the true answer. I am
fairly convinced that both S-PLUS versions are slightly wrong.)

I plan to add S-estimation, a general M-estimator (based on my function
rlm) and MM-estimation to this over the next few weeks.


While I am posting, a R port of Jim Ramsay's psplines package is also there
en route to CRAN, file (pspline_1.0-1.tar.gz) as well as an update of the
V&R `R' Complements to 0.62.3.

-- 
Brian D. Ripley,                  ripley@stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From pgilbert@bank-banque-canada.ca  Mon Aug 31 22:18:27 1998
From: pgilbert@bank-banque-canada.ca (Paul Gilbert)
Date: Mon, 31 Aug 1998 17:18:27 -0400
Subject: isolating R/S and operating system
 differences
Message-ID: <98Aug31.172601edt.13446@mailgate.bank-banque-canada.ca>

Below is a revised version of my kernel of functions for isolating R/S and
operating system  differences. The main change is "date" which I've renamed
"date.parsed" to avoid conflicts with the R and S date functions. The R call now
uses system() rather than unix() to avoid warning messages in R 0.62.3.

Paul Gilbert

##############################################################################


# This file has code which contains operating system and S/R specific
#   functions. They are intended to be used as a kernel to help
#   protect other code from these problems.

# The MSwindows versions are not done.

# The following functions are attempted:
#   For S/R differences:
#      global.assign, system.info, exists.graphics.device, unlink,
#      synchronize,  list.add for [["new.element"]]<-
#   For OS differences:
#     system.call, sleep, present.working.directory, whoami, file.copy,
#     file.date.info, date.parsed, mail, unlink, local.host.netname,

# Also a number of is.xxx functions are defined to identify systems.

# The variable  .SPAWN is also set to be used to identify if Splus "For" loops
#    should be used. (It is sometimes better not to use these even in Splus.)

##############################################################################

#            General Logic and organization of these functions

# 1/ The first group of functions are for identifying S or R and flavours.
# 2/ The second group of functions are for identifying the operating system.
# 3/ The third group specify a few functions which depend only on the
#         differences between S and R.
# 4/ The fourth group specify functions which depend only on the
#         differences among operating system.
# 5/ The fifth group specify a few functions which depend on both R/S and the
#         differences among operating system.

#  >>> I would very much like any input WRT  MS Windows / Win95 / NT / Mac <<<

# The function system.call is defined in order to provide a generic way to
#  make a call to the operating system. When the calls are specific
#  to Unix then the function unix() might be used (though that is now
#  deprecated in R and produces a warning messsage). However, in general the
#  purpose of these functions is not to give a generic way to call the operating

#  system, but rather a generic way to do things that require a call to the
#  operating system (like date, mail, sleep, whoami).


##############################################################################

system.info <- function()
     {if( !exists("version"))
         { #-- `Vanilla' S (i.e. here "S version 4")
           #- this now works for  S version 4  (this is not S-plus 4.0, maybe
           #             part of S-plus 5.0 !):
           lv <- nchar(Sv <- Sversion())
           r <- list(
       major = substring(Sv, 1,1),
       minor = substring(Sv, lv,lv))
         }
   else
     {r <- version
      r$minor <- as.numeric(r$minor)
      r$major <- as.numeric(r$major)
     }
   if      (is.Splus())    r$language <- "Splus"
   else if (is.Svanilla()) r$language <- "S"
   r$OSversion <- OSversion()
   r$OStype    <- OStype()
   r
  }


###########################################################

#    1/  Functions are for identifying S or R and flavours.

###########################################################

#Note It is tempting to use system.info as defined above, but there is a
#        bootstrapping problem to solve.

if (! exists("is.R"))
 {is.R <- function()
     {exists("version") && !is.null(vl <- version$language) && vl == "R" }
 }

is.R.pre0.60 <- function()
  {is.R() && ((as.numeric(version$major)+.01*as.numeric(version$minor)) <0.60) }

is.R.pre0.63.2 <- function()
  {is.R() && ((as.numeric(version$major)+.01*as.numeric(version$minor)) <0.623)}



is.S <- function(){is.Svanilla() | is.Splus() }
is.Svanilla <- function(){!exists("version")}
is.Splus <- function(){exists("version") && is.null(version$language)}
is.Splus.pre3.3 <- function()
   { ## <= 3.2
    is.Splus() &&  ((system.info()$major+.1*system.info()$minor) < 3.3)
   }

###########################################################

#    2/  Functions are for identifying the operating system.

###########################################################

if (is.R())
   {OStype <- function()
      {if("Win32"== machine())          return("MS Windows")
       else if("Macintosh"== machine()) return("Macintosh") #needs to be checked

       else if("Unix"== machine())      return ("Unix")
      }
   }

if (is.S())
   {OStype <- function()
      {if(charmatch("MS Windows", version$os, nomatch=0))
                                return("MS Windows")
       else if(charmatch("Macintosh",  version$os, nomatch=0))
                                return("Macintosh") # needs to be checked
       else if(exists("unix"))  return ("Unix")
      }
   }


is.MSwindows <- function(){OStype() == "MS Windows"}
is.Mac       <- function(){OStype() == "Macintosh" }
is.unix      <- function(){OStype() == "Unix" }

{
if (is.unix())
  {OSversion <- function()
    {paste(system.call("uname -s"),
           system.call("uname -r | sed -e 's/\\.\.\*//'"), sep="") }
  }
else if(is.MSwindows())
  {if (is.R())
     {OSversion <- function()
        {# This is not great since NT is not distinguished but
         #    is.Win32() below will work ok
         if("Win32"== machine()) return("MS Windows 95")
         else return ("unkown")
        }
     }
   if (is.S())
     {OSversion <- function()
        {if("MS Windows 3.1"==version$os) return("MS Windows 3.1")
         if("MS Windows 95" ==version$os) return("MS Windows 95")
         if("MS Windows 98" ==version$os) return("MS Windows 98")
         if("MS Windows NT" ==version$os) return("MS Windows NT")
         else return ("unkown")
        }
     }
  }
else OSversion <- function() "unknown"
}


# Other is.xxx() should be added here.

# determining Unix flavours doesn't seem to be too important but ...
is.Sun4 <- function() {is.unix() && OSversion() == "SunOS4" }
is.Sun5 <- function() {is.unix() && OSversion() == "SunOS5" }
is.Linux <- function(){is.unix() && OSversion() == "linux"}

# Windows flavours may be more important but these are untested !!!
is.Win3.1 <- function(){is.MSwindows() && OSversion() == "MS Windows 3.1"}
is.Win95  <- function(){is.MSwindows() && OSversion() == "MS Windows 95"}
is.WinNT  <- function(){is.MSwindows() && OSversion() == "MS Windows NT"}
is.Win32  <- function(){is.Win95() | is.WinNT() }





###########################################################

#    3/  Functions depending only on the
#         differences between S and R

###########################################################

if(is.S())
   {if(is.unix())system.call  <- unix
    global.assign <- function(name, value) {assign(name,value, where = 1)}
    .SPAWN <- TRUE
    exists.graphics.device <- function(){dev.cur() !=1 }
    open.graphics.device  <- function(display=getenv("DISPLAY"))
                                 {openlook(display) }
    #                            {motif(display) }
    close.graphics.device <- function(){dev.off() }
    if (!exists("set.seed.Splus")) set.seed.Splus <- set.seed
    set.seed <- function(seed=NULL)
      {if (is.null(seed))
          seed <-.Random.seed
       else
         {if (1==length(seed)) set.seed.Splus(seed)
          else global.assign(".Random.seed", seed)
         }
       seed
      }

    "list.add<-" <- function(x, replace, value)
       {# replace or add elements to a list.
        x[replace] <- value
        # x[[replace]] <- value  would be more logical but doesn't work
        x
       }
   }

if(is.R())
   {#tempfile <- function(f)
    #   {# Requires C code also from Friedrich Leisch not in version 0.15 of R.
    #    d<-"This is simply a string long enough to hold the name of a tmpfile";

    #     .C("tmpf", as.character(d))[[1]]
    #    }

    if (is.R.pre0.60())
        {tempfile <- function(pattern = "file")
                {system(paste("for p in", paste(pattern, collapse = " "), ";",
                       "do echo /tmp/$p$$; done"),
                 intern = TRUE)
                }
        }

#    unlink <- function(file) system.call(paste("rm -fr ", file))
    global.assign <- function(name, value)
                          {assign(name,value, envir=.GlobalEnv)}
    synchronize<- function(x){NULL} # perhaps this should do something?
    .SPAWN <- FALSE
    dev.ask <- function(ask=T){par(ask=ask)}
    if (is.R.pre0.63.2())
         exists.graphics.device <- function(){exists(".Device")}
    else exists.graphics.device <- function(){dev.cur() !=1 }
    open.graphics.device  <- function(display=getenv("DISPLAY")) {x11(display)}
    close.graphics.device <- function(){F} # how do I do this?
    set.seed <- function(seed=NULL)
      {if (is.null(seed))
         {if (!exists(".Random.seed")) zzz <- runif(1) # seed may not yet exist
          seed <-.Random.seed
         }
       else
         {if (1==length(seed))
             global.assign(".Random.seed",round(runif(3,min=seed,max=1e5*seed)))

          else global.assign(".Random.seed", seed)
         }
       seed
      }

   "list.add<-" <- function(x, replace, value)
     {# replace or add elements to a list.
      if (is.numeric(replace))
        {# x<- do.call("default.[[<-", list(x,replace,value))   # use default
         x[[replace]] <- value
         return(x)
        }
      if (is.null(value))  value <- list(NULL)
      if (!is.list(value)) value <- list(value)
      if (1 == length(value))
       {for (i in seq(length(replace)))
          x<- do.call("$<-", list(x,replace[i],value[[1]]))
       }
      else
        {if(length(value) != length(replace) )
         stop("number of replacement values != number of elements to replace")
         for (i in seq(length(replace)))
            x<- do.call("$<-", list(x,replace[i],value[[i]]))
        }
      x
     }
 }


###########################################################

#    4/  Functions depending only on the
#         differences among operating system.

###########################################################

if(is.unix())
  {sleep <- function(n) {unix(paste("sleep ", n))} # pause for n seconds
   present.working.directory <- function(){unix("pwd")} # present directory
   whoami <- function(){unix("whoami")} # return user id (for mail)
   local.host.netname <-function() {unix("uname -n")}

   mail <- function(to, subject="", text="")
    {# If to is null then mail is not sent (useful for testing).
     file <- tempfile()
     write(text, file=file)
   if(!is.null(to)) unix(paste("cat ",file, " | mail  -s '", subject, "' ", to))

     unlink(file)
     invisible()
    }

   file.copy <- function(from, to)unix(paste("cp ", from, to)) # copy file

   file.date.info <- function(file.name)
     {# This could be a lot better. It will fail for files older than a year.
      # Also, a returned format like date() below would be better.
      mo <- (1:12)[c("Jan","Feb","Mar","Apr","May", "Jun","Jul","Aug", "Sep",
         "Oct","Nov","Dec") ==substring(unix(paste("ls -l ",file)),33,35)]
      day <- as.integer(substring(unix(paste("ls -l ",file.name)),37,38))
      hr  <- as.integer(substring(unix(paste("ls -l ",file.name)),40,41))
      sec <- as.integer(substring(unix(paste("ls -l ",file.name)),43,44))
      c(mo,day,hr,sec)
     }

}

if(is.MSwindows())
  {system.call  <- function(cmd)
         {stop("system calls must be modified for this operating system.")}
   sleep <- system.call
   present.working.directory <- system.call
   whoami <- system.call
   file.copy <- system.call
   file.date.info <- system.call
  }



###########################################################

#    5/  Functions depending on both R/S and the
#         differences among operating system.

###########################################################

if(is.unix())
  {if(is.R())
     {#unix <- function(cmd) system(cmd, intern=T)
      # unix() is now a function in R but deprecated in favour of system()
      # (This is a bit dangerous, as these calls may be system dependent.)

      system.call <- function(cmd) system(cmd, intern=T)

  # the following date function might be made system independent as a C call.
      date.parsed <-function()
        {d<-parse(text=strsplit(
              system.call("date \'+%Y %m %d %H %M %S\'")," ")[[1]])
         list(y=  eval(d[1]),
              m=eval(d[2]),
              d= eval(d[3]),
              h= eval(d[4]),
              m= eval(d[5]),
              s= eval(d[6]),
              tz=system.call("date '+%Z'"))
        }
     }
   if(is.S())
     {system.call <- function(cmd) unix(cmd)

      date.parsed <-function()
        {d <- parse(text=unix("date '+%Y %m %d %H %M %S'"),white=T)
         list(y=  eval(d[1]),
              m=eval(d[2]),
              d= eval(d[3]),
              h= eval(d[4]),
              m= eval(d[5]),
              s= eval(d[6]),
              tz=unix("date '+%Z'"))
        }
     }
  }




##############################################################################



-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From jimrc@mathfs.math.montana.edu  Tue Aug  4 22:23:29 1998
From: jimrc@mathfs.math.montana.edu (Jim Robison-Cox)
Date: Tue, 4 Aug 1998 15:23:29 -0600 (MDT)
Subject: aov with Error terms
Message-ID: <Pine.GSO.4.02.9808041517520.8073-100000@gauss.math.montana.edu>

 To R-devel:
 
   If anyone is testing the aov() function and it's relatives which I
 posted to the list, thanks for the effort, but I would ask you to hold
off now.  Brian Ripley is developing better and more complete versions, so
you should save your effort for looking over his functions.

  His preliminary version is located at:
http://www.stats.ox.ac.uk/pub/R/aov.tar.gz

Prof. Ripley says it contains:
BR> my current versions of  aov, proj, model.tables, eff.aovlist etc.
BR> 
BR> These are probably as complete as I want to make them, and handle
BR> Error terms and (to some extent) multiple responses.  It is hard to
BR> find sufficiently wierd designs to test this, so I would be grateful
BR> for further testing. There are some datasets and tests in the file,
BR> which is packaged as an R package.

 His note about our recent exchange is related to these functions, though
I was also asking about a summary.mlm which is not in current R nor in
today's aov library.


Jim Robison-Cox                 ____________            
Department of Math Sciences    |            |           phone: (406)994-5340
2-214 Wilson Hall               \   BZN, MT |           FAX:   (406)994-1789
Montana State University         |  *_______|
Bozeman, MT 59717                 \_|         e-mail: jimrc@math.montana.edu 




-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From Andreas.Weingessel@ci.tuwien.ac.at  Thu Aug  6 12:06:11 1998
From: Andreas.Weingessel@ci.tuwien.ac.at (Andreas Weingessel)
Date: Thu, 6 Aug 1998 13:06:11 +0200 (CEST)
Subject: memory-management
Message-ID: <13769.36387.261923.629675@elendil.ci.tuwien.ac.at>


R has some strange behavior in its memory management. (R-0.62.2 and
R-0.63.0 of July 22)

If I start R on a PC with 256MB with the option -v 200, and want to
see the free memory I get

R> gc()
Garbage collection ...
111801 cons cells free (55%)
204554k bytes of heap free (-63%)

So, I have suddenly a negative percentage of memory free. This value
becomes not positive, unless a certain amount of memory is used.

R> x <- matrix(0,1000,1000)
R> gc()
Garbage collection ...
111797 cons cells free (55%)
196741k bytes of heap free (-67%)
R> x <- matrix(0,4000,1000)
R> gc()
Garbage collection ...
111797 cons cells free (55%)
173304k bytes of heap free (-79%)
R> x <- matrix(0,5000,1000)
R> gc()
Garbage collection ...
111797 cons cells free (55%)
165491k bytes of heap free (80%)



The second point I came across is the following. R is now started with
-v 100 -n 1000000. After

R>  x <- matrix(0,5000,900)

I have still enough memory free.

R>  gc()
Garbage collection ...
911797 cons cells free (91%)
66997k bytes of heap free (65%)

So, if x uses 35% of heap memory, there should still be enough memory
for a second matrix of this size, but the command

R>  y <- matrix(0,5000,900)

runs out of heap memory.

Similarly, I can create a 5000x1300-matrix on a newly started R with
-v 100 and have still 50% heap memory free, but I can not create a
5000x1400-matrix, if R is started with -v 100. So, I suppose that R
needs a complete copy of this matrix while creating it and therefore I
can only create matrices which use less than half of the free
memory. Is there any way to avoid this memory usage?

************************************************************************
*                          Andreas Weingessel                          *
************************************************************************
* Institut f=FCr Statistik      *                Tel: (+43 1) 58801 4541 =
*
* Technische Universit=E4t Wien *                Fax: (+43 1)  504 14 98 =
*
* Wiedner Hauptstr. 8-10/1071 *     Andreas.Weingessel@ci.tuwien.ac.at *
* A-1040 Wien, Austria        * http://www.ci.tuwien.ac.at/~weingessel *
************************************************************************


-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From pgilbert@bank-banque-canada.ca  Tue Aug 11 15:02:21 1998
From: pgilbert@bank-banque-canada.ca (Paul Gilbert)
Date: Tue, 11 Aug 1998 10:02:21 -0400
Subject: R-beta: base not loading
References: <199808111314.PAA13233@sophie.ethz.ch>
Message-ID: <98Aug11.100657edt.13447@mailgate.bank-banque-canada.ca>

Martin

No I didn't solve it. I've been on holidays for a couple of weeks. I'll try to
look at it again soon, but any hints would be appreciated.

I can compile on SunOS 5.5 and then run on 5.6, which may be a sort term
solution to your user's problem.

Paul

-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From pgilbert@bank-banque-canada.ca  Tue Aug 11 15:27:03 1998
From: pgilbert@bank-banque-canada.ca (Paul Gilbert)
Date: Tue, 11 Aug 1998 10:27:03 -0400
Subject: R-beta: base not loading
References: <199808111314.PAA13233@sophie.ethz.ch> <x2btprvcmk.fsf@blueberry.kubism.ku.dk>
Message-ID: <98Aug11.103140edt.13458@mailgate.bank-banque-canada.ca>

I just remembered, another thing which complicates this is that older gcc's do
not work with Solaris 2.6 so I had to upgrade to gcc 2.8.1. The result is that I
am not sure if this is

1/  gcc 2.8.1 is not good for the current version of R

2/ my gcc 2.8.1 is not correctly configured

3/ I need to be more careful to upgrade *all* parts of the toolchain

My gut feeling is the same as Peter's (3/), but I haven't got a clue how to
track down the problem.

Paul

(PS. There also seem to be some problems making the f2c with gcc 2.8.1. I've
been using g77 so that isn't the problem.)

-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From Martin Maechler <maechler@stat.math.ethz.ch>  Tue Aug 11 15:34:30 1998
From: Martin Maechler <maechler@stat.math.ethz.ch> (Martin Maechler)
Date: Tue, 11 Aug 1998 16:34:30 +0200
Subject: R-beta: base not loading
In-Reply-To: <98Aug11.103140edt.13458@mailgate.bank-banque-canada.ca> (message
 from Paul Gilbert on Tue, 11 Aug 1998 10:27:03 -0400)
Message-ID: <199808111434.QAA19036@sophie.ethz.ch>

>>>>> "PaulG" == Paul Gilbert <pgilbert@bank-banque-canada.ca> writes:

    PaulG> I just remembered, another thing which complicates this is that
    PaulG> older gcc's do not work with Solaris 2.6 so I had to upgrade to
    PaulG> gcc 2.8.1. The result is that I am not sure if this is

    PaulG> 1/ gcc 2.8.1 is not good for the current version of R

This is certainly not the case.
We have been using gcc 2.8.1 here (Solaris 2.5.x) for quite a while
with no problems.

    PaulG> 2/ my gcc 2.8.1 is not correctly configured

    PaulG> 3/ I need to be more careful to upgrade *all* parts of the
    PaulG> toolchain

    PaulG> My gut feeling is the same as Peter's (3/), but I haven't got a
    PaulG> clue how to track down the problem.

    PaulG> Paul

    PaulG> (PS. There also seem to be some problems making the f2c with gcc
    PaulG> 2.8.1. I've been using g77 so that isn't the problem.)

NOTE: Looking at  RHOME/library/base/R/base (which is the R file that is
	NOT correctly loaded by the internal C code),
      I now bet, that it is  .Alias(.)  which is failing;  more exactly,
      line 64
	------------------------------------------------------
	colours <- .Alias(colors)
	------------------------------------------------------
   
      That's (the only way?) how the error message
	------------------------------------------------------
	Error: Object "colors" not found 
	------------------------------------------------------
      can emerge...

[[this may help find gcc's problem, maybe...]]

Martin
-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From pgilbert@bank-banque-canada.ca  Tue Aug 11 15:59:51 1998
From: pgilbert@bank-banque-canada.ca (Paul Gilbert)
Date: Tue, 11 Aug 1998 10:59:51 -0400
Subject: R-beta: base not loading
References: <199808111434.QAA19036@sophie.ethz.ch>
Message-ID: <98Aug11.110432edt.13451@mailgate.bank-banque-canada.ca>

>PaulG> 1/ gcc 2.8.1 is not good for the current version of R

>This is certainly not the case.
>We have been using gcc 2.8.1 here (Solaris 2.5.x) for quite a while
>with no problems.

To do this I think you would have made gcc with Solaris 2.5 and we made it with
Solaris 2.6, so I don't think we can be certain that there are not problems with
gcc, in combination with Solaris 2.6 libraries, etc, etc.

Paul

-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From p.dalgaard@biostat.ku.dk  Sun Aug 16 11:36:50 1998
From: p.dalgaard@biostat.ku.dk (Peter Dalgaard BSA)
Date: 16 Aug 1998 12:36:50 +0200
Subject: R-beta: base not loading
In-Reply-To: Martin Maechler's message of Tue, 11 Aug 1998 16:34:30 +0200
References: <199808111434.QAA19036@sophie.ethz.ch>
Message-ID: <x27m091ad9.fsf@biostat.ku.dk>

Martin Maechler <maechler@stat.math.ethz.ch> writes:

>     PaulG> (PS. There also seem to be some problems making the f2c with gcc
>     PaulG> 2.8.1. I've been using g77 so that isn't the problem.)
> 
> NOTE: Looking at  RHOME/library/base/R/base (which is the R file that is
> 	NOT correctly loaded by the internal C code),
>       I now bet, that it is  .Alias(.)  which is failing;  more exactly,
>       line 64
> 	------------------------------------------------------
> 	colours <- .Alias(colors)
> 	------------------------------------------------------
>    
>       That's (the only way?) how the error message
> 	------------------------------------------------------
> 	Error: Object "colors" not found 
> 	------------------------------------------------------
>       can emerge...
> 
> [[this may help find gcc's problem, maybe...]]

While messing around with a Windows cross-compile, I realized that
this can also happen if colorstuff.R occurs before New-Internal.R in
the base file. The former has:

## nice to the English
colours <- colors

And the latter has

colors <- function().Internal(colors())
colours <- .Alias(colors)

Now, if ls gets the idea of sorting differently on some platforms,
which is not at all unlikely - Ripley reported something about locale
settings resulting in the collating sequence AaBbCc..., and he *is*
using Sol2.6! - then all hell breaks loose.

The thing in colorstuff.R is simply wrong, so I'll remove it.

PS: No, the 0.62.3 crosscompile doesn't work (yet?).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard@biostat.ku.dk)             FAX: (+45) 35327907
-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From mani@ee.iitb.ernet.in  Sun Aug 16 22:07:07 1998
From: mani@ee.iitb.ernet.in (R. MANIVASAKAN)
Date: Mon, 17 Aug 1998 02:37:07 +0530 (IST)
Subject: Bug tracking system
In-Reply-To: <x2yasoajuj.fsf@biostat.ku.dk>
Message-ID: <Pine.SUN.3.91.980817023536.21259A-100000@bhairav.ee.iitb.ernet.in>

Hi all,
	is there any code in R implimenting the Whittle's MLE estimator 
for Hurst parameter H of the given dataset?.

	thanks,
	mani.

-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From pgilbert@bank-banque-canada.ca  Mon Aug 17 17:13:28 1998
From: pgilbert@bank-banque-canada.ca (Paul Gilbert)
Date: Mon, 17 Aug 1998 12:13:28 -0400
Subject: R-beta: base not loading
References: <199808111434.QAA19036@sophie.ethz.ch> <x27m091ad9.fsf@biostat.ku.dk>
Message-ID: <98Aug17.122121edt.13448@mailgate.bank-banque-canada.ca>

> NOTE: Looking at  RHOME/library/base/R/base (which is the R file that is
>       NOT correctly loaded by the internal C code),
>       I now bet, that it is  .Alias(.)  which is failing;  more exactly,
>       line 64
>       colours <- .Alias(colors)

In my RHOME/library/base/R/base this is line 7105. Based on hints from Peter
Dalgaard and Brian Ripley I discovered that the setting of LC_COLLATE affects
the result of ls, but does not seem to be responsible for the very different
order in which things are arranged in RHOME/library/base/R/base. (Apparently the
setting of LC_COLLATE is part of a choice about locale made during the
installation of Solaris 2.6, so not everyone will have it set.)

One can
    unsetenv LC_COLLATE
which changes the result of ls, but on re-installing R this does not seem to
affect the order of things in RHOME/library/base/R/base (and I have still not
figured out why it is different between 2.5 and 2.6).

On the other hand, removing
    ## nice to the English
    colours <- colors
from RHOME/library/base/R/base seems to fix everything. This assignment seems
to be replicated by
   colours <- .Alias(colors)
and so I guess Peter's fix is the right one.

Paul Gilbert



-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From ihaka@stat.auckland.ac.nz  Mon Aug 17 21:32:43 1998
From: ihaka@stat.auckland.ac.nz (Ross Ihaka)
Date: Tue, 18 Aug 1998 08:32:43 +1200 (NZST)
Subject: Grammar Changes
Message-ID: <199808172032.IAA17509@stat1.stat.auckland.ac.nz>

I would like to make a couple of small changes to the R grammar.
At present, operators like ~, ==, !=, <, <=, >, >= are declared
to be non-associative.  This means that things like

	a < b <= c

produce a "syntax error" message when typed.

I would like to change the grammar so that these operators are
left-associative.  I want to make this change so that mathematical
annotation in the graphics will work a little better.

Interestingly, these operators are left-associative in S, so this
would be a move toward compatibility.

Does anyone have any strong thoughts on this?

	Ross
-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From p.dalgaard@biostat.ku.dk  Mon Aug 17 22:32:50 1998
From: p.dalgaard@biostat.ku.dk (Peter Dalgaard BSA)
Date: 17 Aug 1998 23:32:50 +0200
Subject: Grammar Changes
In-Reply-To: Ross Ihaka's message of Tue, 18 Aug 1998 08:32:43 +1200 (NZST)
References: <199808172032.IAA17509@stat1.stat.auckland.ac.nz>
Message-ID: <x2ogtjgupp.fsf@biostat.ku.dk>

Ross Ihaka <ihaka@stat.auckland.ac.nz> writes:

> 
> I would like to make a couple of small changes to the R grammar.
> At present, operators like ~, ==, !=, <, <=, >, >= are declared
> to be non-associative.  This means that things like
> 
> 	a < b <= c
> 
> produce a "syntax error" message when typed.
> 
> I would like to change the grammar so that these operators are
> left-associative.  I want to make this change so that mathematical
> annotation in the graphics will work a little better.
> 
> Interestingly, these operators are left-associative in S, so this
> would be a move toward compatibility.
> 
> Does anyone have any strong thoughts on this?

Sounds harmless enough to me. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard@biostat.ku.dk)             FAX: (+45) 35327907
-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From Martin Maechler <maechler@stat.math.ethz.ch>  Tue Aug 18 14:13:46 1998
From: Martin Maechler <maechler@stat.math.ethz.ch> (Martin Maechler)
Date: Tue, 18 Aug 1998 15:13:46 +0200
Subject: Problem in "configure" for Solaris (cc) ?!
Message-ID: <199808181313.PAA28150@sophie.ethz.ch>

	[[0.62.3, already 0.62.2]]

This bug report is overdue,
but I really didn't test these things for weeks 
(have always used gcc, but only yesterday, someone told me that he saw a
speed gain of a factor 2 when using Sun's cc over gcc)

I just found that the same problem is already in 0.62.2

If I take yesterday's  
	R-release.tar.gz  (or also R-0.62.2.tar.gz)
unpack
and add
	CC=cc
	-----
to  config.site,
then
	./configure
	-----------
it ends VERY badly:

% ./configure
creating cache ./config.cache
checking for a BSD compatible install... /usr/local/bin/ginstall -c
checking whether ln -s works... yes
checking for ranlib... ranlib
checking for bison... bison -y
checking for ar... ar
checking for ratfor... ratfor
checking for latex... /afs/ethz.ch/public/teTeX/teTeX/bin/sparc-solaris2.5/latex
checking for dvips... /afs/ethz.ch/public/teTeX/teTeX/bin/sparc-solaris2.5/dvips
checking for perl... /usr/local/bin/perl
checking whether perl is perl 5... yes
checking for cc... cc
checking for ranlib... (cached) ranlib
checking for f77... f77
checking for underscore after Fortran symbols... configure: error: Nothing worked - cannot use FORTRAN

-----------
and then  STOP, nothing, ....

When I omit the "CC=cc", gcc is used (together with f77), and all is well.

  % cc -V
  cc: SC3.0.1 13 Jul 1994
  usage: cc [ options] files.  Use 'cc -flags' for details

  % f77 -V
  f77: SC3.0.1 13 Jul 1994
  Usage: f77 [ options ] files.  Use 'f77 -flags' for details

  % uname -a
  SunOS florence 5.5 Generic sun4u sparc SUNW,Ultra-1

------------------------------------------------

So yes, the  SC.. compilers are somewhat dated, but still...

-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From Martin Maechler <maechler@stat.math.ethz.ch>  Tue Aug 18 14:54:22 1998
From: Martin Maechler <maechler@stat.math.ethz.ch> (Martin Maechler)
Date: Tue, 18 Aug 1998 15:54:22 +0200
Subject: Problem in "configure" for Solaris (cc) -- solved (partly) --
In-Reply-To: <199808181313.PAA28150@sophie.ethz.ch> (message from Martin
 Maechler on Tue, 18 Aug 1998 15:13:46 +0200)
Message-ID: <199808181354.PAA01678@sophie.ethz.ch>

(answering myself, after a suggestion by Kurt Hornik
		 which actually was *not* the solution.. but...)

The configure problem I've just reported
(( setting   CC=cc  in  config.site ))

actually does not show when I either leave the CFLAGS commented out
(which results in CFLAGS=-g) or set CFLAGS="-g -O",

however, it *DOES* show when I set
	CFLAGS=-O2 or CFLAGS="-g -O2"

----

Reading the long output of  ``cc -flags'',
I see that more than average optimization is done using
	cc -xO[1-4]
	    #
Still very funny that "-O2" leads to such bad results with ./configure.
Maybe configure could become smarter here...

Martin
-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From p.dalgaard@biostat.ku.dk  Tue Aug 18 15:25:20 1998
From: p.dalgaard@biostat.ku.dk (Peter Dalgaard BSA)
Date: 18 Aug 1998 16:25:20 +0200
Subject: Problem in "configure" for Solaris (cc) -- solved (partly) --
In-Reply-To: Martin Maechler's message of Tue, 18 Aug 1998 15:54:22 +0200
References: <199808181354.PAA01678@sophie.ethz.ch>
Message-ID: <x2lnom2wq7.fsf@biostat.ku.dk>

Martin Maechler <maechler@stat.math.ethz.ch> writes:

> Reading the long output of  ``cc -flags'',
> I see that more than average optimization is done using
> 	cc -xO[1-4]
> 	    #
> Still very funny that "-O2" leads to such bad results with ./configure.
> Maybe configure could become smarter here...

This is a generic problem with autoconf: It goes looking for X, the
compile doesn't work because of Y, and it reports that X is missing.
Of course, one should always check config.log when something goes
wrong, it contains the details of the compiler failure and the failed
programs. 

Our configure starts off with a compiler sanity check, but apparently
it isn't enough?


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard@biostat.ku.dk)             FAX: (+45) 35327907
-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From bates@stat.wisc.edu  Tue Aug 18 15:45:22 1998
From: bates@stat.wisc.edu (Douglas Bates)
Date: 18 Aug 1998 09:45:22 -0500
Subject: Problem in "configure" for Solaris (cc) -- solved (partly) --
In-Reply-To: Peter Dalgaard BSA's message of "18 Aug 1998 16:25:20 +0200"
References: <199808181354.PAA01678@sophie.ethz.ch> <x2lnom2wq7.fsf@biostat.ku.dk>
Message-ID: <6rww86nybh.fsf@verdi.stat.wisc.edu>

>>>>> "PD" == Peter Dalgaard BSA <p.dalgaard@biostat.ku.dk> writes:

  PD> This is a generic problem with autoconf: It goes looking for X,
  PD> the compile doesn't work because of Y, and it reports that X is
  PD> missing.  Of course, one should always check config.log when
  PD> something goes wrong, it contains the details of the compiler
  PD> failure and the failed programs.

I have seen another such problem.  The check for -lblas is done with
the C compiler invoked on a short C program stub.  For me the blas
library is found but the program does not link properly because of
missing Fortran library routines.  Configure then decides that there
is no blas library present.
-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From Prof Brian Ripley <ripley@stats.ox.ac.uk>  Tue Aug 18 18:28:50 1998
From: Prof Brian Ripley <ripley@stats.ox.ac.uk> (Prof Brian Ripley)
Date: Tue, 18 Aug 1998 18:28:50 +0100 (BST)
Subject: Problem in "configure" for Solaris (cc) -- solved (partly) --
Message-ID: <199808181728.SAA04981@toucan.stats.ox.ac.uk>

> From: Peter Dalgaard BSA <p.dalgaard@biostat.ku.dk>
> 
> Martin Maechler <maechler@stat.math.ethz.ch> writes:
> 
> > Reading the long output of  ``cc -flags'',
> > I see that more than average optimization is done using
> > 	cc -xO[1-4]
> > 	    #

And -O is -xO2. If you do want higher speed, you need to use
other flags too, and -fast is a good start.

> > Still very funny that "-O2" leads to such bad results with ./configure.
> > Maybe configure could become smarter here...
> 
> This is a generic problem with autoconf: It goes looking for X, the
> compile doesn't work because of Y, and it reports that X is missing.
> Of course, one should always check config.log when something goes
> wrong, it contains the details of the compiler failure and the failed
> programs. 

Not this time. See below.

> Our configure starts off with a compiler sanity check, but apparently
> it isn't enough?

No, as it is not invoked if the compiler is specified via CC. In
Martin's configuration, config.log gives

$ CC=cc CFLAGS=-O2 ./configure
$ cat config.log
This file contains any messages produced by compilers while
running configure, to aid debugging if configure makes a mistake.

configure:708: checking for a BSD compatible install
configure:758: checking whether ln -s works
configure:781: checking for ranlib
configure:812: checking for bison
configure:845: checking for ar
configure:873: checking for ratfor
configure:902: checking for latex
configure:934: checking for dvips
configure:974: checking for perl
configure:1004: checking whether perl is perl 5
configure:1200: checking for cc
configure:1261: checking for ranlib
configure:1325: checking for f77
configure:1402: checking for underscore after Fortran symbols

Note, no details of the compiler failure appear ....


As for speed comparisons:

A piece of the ch14 script from V&R2 (highly C-dependent), Solaris 2.6, 
Sparc 20 (about 4 years old)

library(MASS)
library(tree)
data(fgl)
unix.time({
fgl.tr <- tree(type ~ ., fgl)
fgl.cv <- cv.tree(fgl.tr,, prune.misclass)
for(i in 2:5)  fgl.cv$dev <- fgl.cv$dev + 
     cv.tree(fgl.tr,, prune.misclass)$dev
fgl.cv$dev <- fgl.cv$dev/5})

gcc2.8.1 60.26secs
cc SC4.0 60.35secs

(the close agreement is fortuitous: I had to tune each to get about
10% improvement in speed).

Brian

-- 
Brian D. Ripley,                  ripley@stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From p.dalgaard@biostat.ku.dk  Tue Aug 18 20:39:34 1998
From: p.dalgaard@biostat.ku.dk (Peter Dalgaard BSA)
Date: 18 Aug 1998 21:39:34 +0200
Subject: Problem in "configure" for Solaris (cc) -- solved (partly) --
In-Reply-To: Prof Brian Ripley's message of Tue, 18 Aug 1998 18:28:50 +0100 (BST)
References: <199808181728.SAA04981@toucan.stats.ox.ac.uk>
Message-ID: <x2n292xeo9.fsf@biostat.ku.dk>

Prof Brian Ripley <ripley@stats.ox.ac.uk> writes:

> $ CC=cc CFLAGS=-O2 ./configure
> $ cat config.log
> This file contains any messages produced by compilers while
> running configure, to aid debugging if configure makes a mistake.
> 
> configure:708: checking for a BSD compatible install
...
> 
> Note, no details of the compiler failure appear ....

Hmmm. Yes. Easily reproduced on other systems by using:

CC=/bin/false ./configure

The basic problem here is that this part of configure.in is handwritten,
and doesn't use the "autoconf conventions" (which are probably as
well-documented as certain aspects of R...) of sending the compile
commands and in the case of failure also the program contents to file
descriptor 5.  I.e. what yields all the constructions of the type

if { (eval echo configure:1475: \"$ac_link\") 1>&5; (eval $ac_link)
2>&5; } && test -s conftest; then
  rm -rf conftest*
  eval "ac_cv_lib_$ac_lib_var=yes"
else
  echo "configure: failed program was:" >&5
  cat conftest.$ac_ext >&5
  rm -rf conftest*
  eval "ac_cv_lib_$ac_lib_var=no"
fi

There must be a relevant macro for this somewhere?

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard@biostat.ku.dk)             FAX: (+45) 35327907
-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From p.dalgaard@biostat.ku.dk  Wed Aug 19 15:01:05 1998
From: p.dalgaard@biostat.ku.dk (Peter Dalgaard BSA)
Date: 19 Aug 1998 16:01:05 +0200
Subject: R 0.62.3 to be released soon
Message-ID: <x2n291xe8u.fsf@blueberry.kubism.ku.dk>

For the (slightly) adventurous among you:

We plan to roll out R 0.62.3 next Friday. In order to root out
remaining bugs before the release, I'd like to encourage you to try it
out on your system. 

Since there is a risk of destabilising things whenever you mess with
the configure scripts, we won't attack installation problems on
unusual platforms after this week.

We also won't take on any complicated bugs, unless they have very bad
effects. (This is a good time to pick nits...)

Note that the "horses mouth" for snapshot releases of R is 

ftp.stat.math.ethz.ch:/pub/Software/R/

The daily snapshots are also mirrored to the CRAN sites, but not
always on a daily basis.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard@biostat.ku.dk)             FAX: (+45) 35327907
-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From pgilbert@bank-banque-canada.ca  Wed Aug 19 19:36:09 1998
From: pgilbert@bank-banque-canada.ca (Paul Gilbert)
Date: Wed, 19 Aug 1998 14:36:09 -0400
Subject: R 0.62.3 to be released soon
References: <x2n291xe8u.fsf@blueberry.kubism.ku.dk>
 <98Aug19.120624edt.13485@mailgate.bank-banque-canada.ca> <x2d89xx7sp.fsf@blueberry.kubism.ku.dk>
Message-ID: <98Aug19.144503edt.13473@mailgate.bank-banque-canada.ca>

I've compiled and installed pre R 0.62.3 (Aug. 19) under Solaris 2.6 using gcc
version 2.8.1 and g77 0.5.23. Everything went smoothly and my DSE library tests
work ok. There is a very occasional problem with a "Segmentation Fault - core
dumped", which also occured in 0.62.2, but I have not been able to reproduce it
reliably. I also had to increase R -v from 250000 cells (which worked with
0.62.2) to 300000 cells for one of my tests.

There is however, a small problem with installing the HTML documention in a
private (i.e. not RHOME/library) location. The file

   RHOME/doc/html/packages.html

linking the main documention to the packages is written (BTW this will probably
fail if the package owner and the R owner are not the same) but the links in it
are not quite correct. For me the link indicated in that file is

  <A HREF="../../library/dse/html/00Index.html">
which would be RHOME/library/dse/html/00Index.html, but it should be
 <A HREF="{package_location}/dse/html/00Index.html">

Paul Gilbert

-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From pgilbert@bank-banque-canada.ca  Thu Aug 20 19:57:57 1998
From: pgilbert@bank-banque-canada.ca (Paul Gilbert)
Date: Thu, 20 Aug 1998 14:57:57 -0400
Subject: R 0.62.3 to be released soon
Message-ID: <98Aug20.145941edt.13442@mailgate.bank-banque-canada.ca>

Installing a package with documentation seems to clobber the file

RHOME/doc/html/function.html

which has the links to all the HTML  function documentation, so access to the R
base documentation is almost impossible.

Paul Gilbert

-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From p.dalgaard@biostat.ku.dk  Thu Aug 20 22:28:07 1998
From: p.dalgaard@biostat.ku.dk (Peter Dalgaard BSA)
Date: 20 Aug 1998 23:28:07 +0200
Subject: R 0.62.3 to be released soon
In-Reply-To: Paul Gilbert's message of Thu, 20 Aug 1998 14:57:57 -0400
References: <98Aug20.145941edt.13442@mailgate.bank-banque-canada.ca>
Message-ID: <x2r9ybibrs.fsf@biostat.ku.dk>

Paul Gilbert <pgilbert@bank-banque-canada.ca> writes:

> 
> Installing a package with documentation seems to clobber the file
> 
> RHOME/doc/html/function.html
> 
> which has the links to all the HTML  function documentation, so access to the R
> base documentation is almost impossible.
> 
> Paul Gilbert

I took the liberty of forwarding this to r-bugs and ditto with your
note about 'private' libraries.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard@biostat.ku.dk)             FAX: (+45) 35327907
-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From p.dalgaard@biostat.ku.dk  Fri Aug 21 15:30:25 1998
From: p.dalgaard@biostat.ku.dk (Peter Dalgaard BSA)
Date: 21 Aug 1998 16:30:25 +0200
Subject: R 0.62.3 to be released soon
In-Reply-To: Paul Gilbert's message of Fri, 21 Aug 1998 10:04:14 -0400
References: <98Aug20.145941edt.13442@mailgate.bank-banque-canada.ca>
 <x2r9ybibrs.fsf@biostat.ku.dk>
 <98Aug21.100624edt.13448@mailgate.bank-banque-canada.ca>
Message-ID: <x2btpe9zlq.fsf@blueberry.kubism.ku.dk>

Paul Gilbert <pgilbert@bank-banque-canada.ca> writes:

> 
> >I took the liberty of forwarding this to r-bugs and ditto with your
> >note about 'private' libraries.
> 
> Peter
> 
> Please remind me again what r-bugs is. When should I be posting there instead?

Sorry, I was very tired when I wrote that...

R-bugs@biostat.ku.dk is the mail interface to our new bug-tracking
system. So what I did was to file the letters as official bug reports.
There was no error on your behalf, we still need to figure out what
the practical way of using this system really is. 

The purpose of forwarding the notes was just to get them filed so that
we won't (easily) forget about them. I did it using 'resend' so they
appear to come from you, and I though I'd better tell you.

Also, I was testing some parts of the interface at the same time,
namely what happens with the e-mail notification of the core team when
you do a forward like that. (It turns out that it recognises the 'Re:'
string and skips the notification, which would seem to be a bit of a
bug in the bug tracking system...)

(Note the cc: to r-devel in *this* note, by the way)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard@biostat.ku.dk)             FAX: (+45) 35327907
-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From pgilbert@bank-banque-canada.ca  Fri Aug 21 17:19:13 1998
From: pgilbert@bank-banque-canada.ca (Paul Gilbert)
Date: Fri, 21 Aug 1998 12:19:13 -0400
Subject: couldn't find FUN
Message-ID: <98Aug21.122130edt.13441@mailgate.bank-banque-canada.ca>

The call to sweep in this function which was working in 0.62.2 is giving me
trouble in 62.3:

prcomponents <- function(x, center=TRUE, scale=TRUE, N=nrow(x)-1)
   {if (center) center <- apply(x,2,mean)
    else        center <- rep(0, ncol(x))
    if (scale)  scale  <- sqrt(apply(x,2,var))
    else        scale  <- rep(1, ncol(x))
    s <- svd(sweep(sweep(x,2, center),2, scale, FUN="/"))
    # remove anything corresponding to effectively zero singular values.
    rank <- sum(s$d > (s$d[1]*sqrt(.Machine$double.eps)))
    if (rank < ncol(x)) s$v <- s$v[,1:rank, drop=FALSE]
    s$d <- s$d/sqrt(max(1,N))

#   r <- list(sdev=s$d, proj=s$v,x=x %*% s$v, center=center, scale=scale)
    r <- list(sdev=s$d, proj=s$v, center=center, scale=scale)
    r
}

> data(crimes)
> prcomponents(crimes)
Error: couldn't find function "FUN"
> traceback()
[1] "eval(f)"
[2] "Ops.data.frame(x, aperm(array(STATS, dims[perm]), order(perm)), "
[3] "\t    ...)"
[4] "sweep(x, 2, center)"
[5] "sweep(sweep(x, 2, center), 2, scale, FUN = \"/\")"
[6] "svd(sweep(sweep(x, 2, center), 2, scale, FUN = \"/\"))"
[7] "prcomponents(crimes)"
>

Paul Gilbert

-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From pgilbert@bank-banque-canada.ca  Fri Aug 21 18:30:23 1998
From: pgilbert@bank-banque-canada.ca (Paul Gilbert)
Date: Fri, 21 Aug 1998 13:30:23 -0400
Subject: couldn't find FUN
References: <98Aug21.122130edt.13441@mailgate.bank-banque-canada.ca>
Message-ID: <98Aug21.133236edt.13450@mailgate.bank-banque-canada.ca>

>> data(crimes)
>> prcomponents(crimes)
>Error: couldn't find function "FUN"

This does not work in 0.62.2 as I previously reported. The error is associated
with using
data(crimes) which is not what I've usually done. It is fixed by replacing

   s <- svd(sweep(sweep(x,2, center),2, scale, FUN="/"))

with

   s <- svd(sweep(sweep(as.matrix(x),2, center),2, scale, FUN="/"))

So, I'm not sure if this is a bug or if this is the way it should work? For my
purposes it is fixed already.

Paul Gilbert



-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From ripley@stats.ox.ac.uk  Fri Aug 21 18:37:01 1998
From: ripley@stats.ox.ac.uk (Prof Brian D Ripley)
Date: Fri, 21 Aug 1998 18:37:01 +0100 (BST)
Subject: couldn't find FUN
In-Reply-To: <98Aug21.122130edt.13441@mailgate.bank-banque-canada.ca>
Message-ID: <Pine.GSO.3.96.980821182855.4962E-100000@localhost>

On Fri, 21 Aug 1998, Paul Gilbert wrote:

> The call to sweep in this function which was working in 0.62.2 is giving me
> trouble in 62.3:

The call is invalid:

> ?sweep

Sweep out Array Summaries

     sweep(x, MARGIN, STATS, FUN="-", ...)

Arguments:

       x: an array.

> class(crimes)
[1] "data.frame"

If you call sweep on a data frame, as in

> sweep(crimes, 2, "-")

it does not work: it should not have done so before.  On the other hand, in
0.62.2, valid arithmetic operations on data frames gave matrices,
which helped your illegal example but were incorrect on legal ones.
You need an as.matrix in your code, I believe.


This example also fails in S.

-- 
Brian D. Ripley,                  ripley@stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From p.dalgaard@biostat.ku.dk  Fri Aug 21 23:34:02 1998
From: p.dalgaard@biostat.ku.dk (Peter Dalgaard BSA)
Date: 22 Aug 1998 00:34:02 +0200
Subject: couldn't find FUN
In-Reply-To: Prof Brian D Ripley's message of Fri, 21 Aug 1998 18:37:01 +0100 (BST)
References: <Pine.GSO.3.96.980821182855.4962E-100000@localhost>
Message-ID: <x2ww82ars5.fsf@biostat.ku.dk>

Prof Brian D Ripley <ripley@stats.ox.ac.uk> writes:

> 
> On Fri, 21 Aug 1998, Paul Gilbert wrote:
> 
> > The call to sweep in this function which was working in 0.62.2 is giving me
> > trouble in 62.3:
..
> 
> If you call sweep on a data frame, as in
> 
> > sweep(crimes, 2, "-")

Um, you need some STATS in there...

> 
> it does not work: it should not have done so before.  On the other hand, in
> 0.62.2, valid arithmetic operations on data frames gave matrices,
> which helped your illegal example but were incorrect on legal ones.
> You need an as.matrix in your code, I believe.
> 
> 
> This example also fails in S.

Still, something weird is going on. First of all, the error message
about not finding "FUN" is less than obvious and secondly, it's not in
general a problem to do arithmetic on data frames as if they were
matrices. (E.g. crimes - as.matrix(crimes)) I think we have some of
the explanation in:

> f<-get("-")                         
> f(crimes,as.matrix(crimes))
Warning: ignored non function "f"
Warning: ignored non function "f"
Warning: ignored non function "f"
Warning: ignored non function "f"
               Murder Assault UrbanPop Rape
Alabama             0       0        0    0
Alaska              0       0        0    0
Arizona             0       0        0    0

i.e. "something" applies "f" for each column and there's some kind of
mess related to scoping (there really is no other "f" around before
the function is called!)

It happens only with .Primitive functions like "-". Stuff like

sweep(crimes, 2, apply(crimes, 2, mean), function(x, y) x - y)

works perfectly.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard@biostat.ku.dk)             FAX: (+45) 35327907
-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From ripley@stats.ox.ac.uk  Sat Aug 22 18:06:00 1998
From: ripley@stats.ox.ac.uk (Prof Brian D Ripley)
Date: Sat, 22 Aug 1998 18:06:00 +0100 (BST)
Subject: Handling of offsets in glm is really inconsistent.
Message-ID: <Pine.GSO.3.96.980822172731.6251A-100000@localhost>

[Copied to R-devel for information]

This applies to all versions of R I have: 0.62.2, 0.62.3, 0.63.
Great care seems needed with glms with offsets, as many things seem
wrong.

Consider the following:

> data(freeny)
> freeny.glm <- glm(y ~ offset(lag.quarterly.revenue) + price.index +
    income.level + market.potential, data=freeny, subset=1:30)
> predict(freeny.glm)
            Qtr1       Qtr2       Qtr3       Qtr4
1962:         NA 0.01040457 0.01073223 0.01233351
1963: 0.01211730 0.02744293 0.03259214 0.03225403
1964: 0.03235081 0.03272723 0.03361984 0.03114351
1965: 0.03128098 0.03371820 0.02869783 0.02670215
1966: 0.02346093 0.02172193 0.02269731 0.01927851
1967: 0.02516107 0.02496217 0.02679227 0.03099460
1968: 0.03225807 0.03418791 0.03857815 0.03324247
1969: 0.02575733 0.02702418 0.02988582         NA
> predict(freeny.glm, type="response")
          Qtr1     Qtr2     Qtr3     Qtr4
1962:       NA 8.806765 8.803092 8.803704
1963: 8.826977 8.840453 8.940102 8.968984
1964: 8.993961 8.993167 9.042300 9.061634
1965: 9.100341 9.092428 9.135678 9.153552
1966: 9.194421 9.208372 9.260927 9.284149
1967: 9.309521 9.338742 9.377042 9.389345
1968: 9.429928 9.455688 9.480808 9.520452
1969: 9.549497 9.566824 9.611116       NA

so one might think that prediction of a glm with an offset should
include the offset on the response scale but not link scale. However,

> predict(freeny.glm, newdata=freeny)
   1962.25     1962.5    1962.75       1963    1963.25 
0.01040457 0.01073223 0.01233351 0.01211730 0.02744293 
    1963.5    1963.75       1964    1964.25     1964.5 
0.03259214 0.03225403 0.03235081 0.03272723 0.03361984 
   1964.75       1965    1965.25     1965.5    1965.75 
0.03114351 0.03128098 0.03371820 0.02869783 0.02670215 
      1966    1966.25     1966.5    1966.75       1967 
0.02346093 0.02172193 0.02269731 0.01927851 0.02516107 
   1967.25     1967.5    1967.75       1968    1968.25 
0.02496217 0.02679227 0.03099460 0.03225807 0.03418791 
    1968.5    1968.75       1969    1969.25     1969.5 
0.03857815 0.03324247 0.02575733 0.02702418 0.02988582 
   1969.75       1970    1970.25     1970.5    1970.75 
0.02686281 0.03816228 0.03910487 0.04325638 0.03599068 
      1971    1971.25     1971.5    1971.75 
0.03357946 0.03890549 0.04196893 0.03952385 

> predict(freeny.glm, newdata=freeny, type="response")
   1962.25     1962.5    1962.75       1963    1963.25 
0.01040457 0.01073223 0.01233351 0.01211730 0.02744293 
    1963.5    1963.75       1964    1964.25     1964.5 
0.03259214 0.03225403 0.03235081 0.03272723 0.03361984 
   1964.75       1965    1965.25     1965.5    1965.75 
0.03114351 0.03128098 0.03371820 0.02869783 0.02670215 
      1966    1966.25     1966.5    1966.75       1967 
0.02346093 0.02172193 0.02269731 0.01927851 0.02516107 
   1967.25     1967.5    1967.75       1968    1968.25 
0.02496217 0.02679227 0.03099460 0.03225807 0.03418791 
    1968.5    1968.75       1969    1969.25     1969.5 
0.03857815 0.03324247 0.02575733 0.02702418 0.02988582 
   1969.75       1970    1970.25     1970.5    1970.75 
0.02686281 0.03816228 0.03910487 0.04325638 0.03599068 
      1971    1971.25     1971.5    1971.75 
0.03357946 0.03890549 0.04196893 0.03952385 

and prediction on either scale with newdata ignores the offset. Now, S is
also inconsistent (prior to S-PLUS 4.5 rel2), but at least it does usually
include the offset (except for prediction on link scale with no newdata,
where the offset was omitted until recently). [The different print layout
is due to the original response being a time series of class "ts"; the
predict method cannot know that.]

The discrepancy is in predict.lm (not predict.glm) which ignores offsets in
R but includes them in S. Correcting it is made difficult by the way
delete.response also deletes offsets in R: 

> terms(freeny.glm)
y ~ offset(lag.quarterly.revenue) + price.index + income.level + 
            market.potential
attr(,"variables")
list(y, offset(lag.quarterly.revenue), price.index, income.level, 
            market.potential)
attr(,"factors")
                              price.index income.level market.potential
y                                       0            0                0
offset(lag.quarterly.revenue)           0            0                0
price.index                             1            0                0
income.level                            0            1                0
market.potential                        0            0                1
attr(,"term.labels")
[1] "price.index"      "income.level"     "market.potential"
attr(,"order")
[1] 1 1 1
attr(,"intercept")
[1] 1
attr(,"response")
[1] 1
attr(,"offset")
[1] 2

> delete.response(terms(freeny.glm))
~price.index + income.level + market.potential
attr(,"variables")
list(price.index, income.level, market.potential)
attr(,"factors")
                 price.index income.level market.potential
price.index                1            0                0
income.level               0            1                0
market.potential           0            0                1
attr(,"term.labels")
[1] "price.index"      "income.level"     "market.potential"
attr(,"order")
[1] 1 1 1
attr(,"intercept")
[1] 1
attr(,"response")
[1] 0

and that is definitely not what is documented to happen.  I do not begin to
understand why delete.response is written the way it is: it looks like it
needs a complete re-design. 

So

- delete.response needs to only delete responses.
- predict.lm needs to handle offsets (or predict.glm, but it is
  much cleaner in predict.lm).
- glm.fit should return  linear.predictors = eta + offset.

There is another small problem:

> predict(freeny.glm, se=T)
Error: Object "price.index" not found

as predict.lm does not recognize that newdata was missing in the caller
(how lazy is lazy evaluation?)  The simplest way out is to have

   if (missing(newdata) || is.null(newdata)) X <- model.matrix(object)

the alternative is to test missing(newdata) in predict.glm.

While this is being done, I wonder why R does not implement offsets
for lm()?  It `looks like an easy exercise'.

-- 
Brian D. Ripley,                  ripley@stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From pgilbert@bank-banque-canada.ca  Mon Aug 24 14:24:17 1998
From: pgilbert@bank-banque-canada.ca (Paul Gilbert)
Date: Mon, 24 Aug 1998 09:24:17 -0400
Subject: R-beta: re -n -v wr0613b - windows dynload
References: <35DE9A90.166E324E@mail1.stofanet.dk>
Message-ID: <98Aug24.092850edt.13455@mailgate.bank-banque-canada.ca>

>When I use the -v I can modify the size of the heap, as assessed by
>gc(), but the -n key seems to be without effect ?

In src/include/Defn.h the following are set and if you exceed these values I
believe your setting is ignored. (I don't think there is any warning but it
would be nice.)

#ifndef R_NSIZE
#define R_NSIZE  200000L  /* number of cons cells */
#endif

#ifndef R_VSIZE
#define R_VSIZE  2000000L /* vector heap size in bytes */
#endif

Paul Gilbert

-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From pgilbert@bank-banque-canada.ca  Mon Aug 24 16:08:38 1998
From: pgilbert@bank-banque-canada.ca (Paul Gilbert)
Date: Mon, 24 Aug 1998 11:08:38 -0400
Subject: prcomp and princomp
Message-ID: <98Aug24.111223edt.13450@mailgate.bank-banque-canada.ca>

Below are mva functions prcomp and princomp as well as my own version
prcomponents. Also included are contents for prcomp.Rd and princomp.Rd.
Functions prcomp and princomp are intended to replicate Splus results. (I think
this princomp is already in the R distribution, but the version of prcomp
already in the R distribution does not reproduce Splus results.)

My version, prcomponent, is preliminary and as yet undocumented, but I would
appreciate comments. It uses the preferred svd as in prcomp, not eigen as in
princomp,  but returns some extra information as in princomp. Following Bill
Venables suggestion, it also allows the user to specify the effective scaling
factor (e.g. N or N-1).

Also, I have just noticed that there is some very small duplication
(print.princomp, summary.princomp, plot.princomp) between the functions  below
and the file princomp-add.R already in the R distribution. If someone could pick
the best versions I would appreciate it. I am about to get buried with some
other work and would like to get this in the R distribution before it gets lost
on my desk (again).

Paul Gilbert

############## prcomp ##############

prcomp <- function(x, retx=TRUE, center=TRUE, scale=FALSE) {
# s <- svd(scale(x, center=center, scale=scale),nu=0)
# above produces warning since scale is both a function and a variable so
 s <- svd(get("scale",envir=.GlobalEnv)
   (x, center=center, scale=scale),nu=0)
# rank <- sum(s$d > 0)
 rank <- sum(s$d > (s$d[1]*sqrt(.Machine$double.eps)))
 if (rank < ncol(x)) s$v <- s$v[,1:rank, drop=F]
 s$d <- s$d/sqrt(max(1,nrow(x) -1))
 if(retx) r <- list(sdev=s$d, rotation=s$v, x=as.matrix(x) %*% s$v)
 else     r <- list(sdev=s$d, rotation=s$v)
 class(r) <- "prcomp"
 r
}

print.prcomp <- function(x) {
 cat("Standard deviations:\n")
 print(x$sdev)
 cat("\nRotation:\n")
 print(x$rotation)
 if (!is.null(x$x))
  {cat("\nRotated variables:\n")
   print(x$x)
  }
 invisible(x)
}

plot.prcomp <- function(obj, x=1, y=2, main="Scree Plot",
  xlab=paste("Principle component", x),
  ylab=paste("Principle component", y), ...) {
    if (is.null(obj$x))
 stop("Rotated x has not be calculated by prcomp. Use retx=T in prcomp.")
    plot(obj$x[,c(x,y)], main=main, xlab=xlab, ylab=ylab, ...)
}

############### prcomp.Rd ###############

\name{prcomp}
\title{Principal Components Analysis}
\usage{
prcomp(x, retx=TRUE, center=TRUE, scale=FALSE)

}
\arguments{
\item{x}{a matrix (or data frame) which provides the data
for the principal components analysis.}
\item{retx}{a logical value indicating whether the rotated variables should
be returned.}
\item{center}{a logical value indicating whether the variables should
be shifted to be zero centered.}
\item{scale}{a logical value indicating whether the variables should
be scaled to have unit variance before the analysis takes place. The default
is F for consistency with S, but in general scaling is advisable.}
}
\description{
This function performs a principal components analysis on
the given data matrix and returns the results as a
\code{prcomp} object. The calculation is done with svd on the data matrix, not
by using eigen on the covariance matrix. This is generally the preferred method
for numerical accuracy.  The print method for the these
objects prints the results in a nice format and the
plot method produces a scree plot.
}
\value{
\code{prcomp} returns an list with class \code{"prcomp"}
containing the following components:
\item{sdev}{the standard deviation of the principal components
(i.e. the eigenvalues of the cov matrix - though the calculation is actually
done with the singular values of the data matrix.)}
\item{rotation}{the matrix of variable loadings (i.e. a matrix
whose columns contain the eigenvectors). The function princomp returns this
in the element \code{loadings}.}
\item{x}{if retx is true the value of the rotated data (the data multiplied by
the \code{rotation} matrix) is returned.}
}
\references{
Mardia, K. V., J. T. Kent, J and M. Bibby (1979),
\emph{Multivariate Analysis}, London: Academic Press.

Venerables, W. N. and B. D. Ripley (1994).
\emph{Modern Applied Statistics with S-Plus}, Springer-Verlag.
}

\seealso{
\code{\link{princomp}}, \code{\link{prcomponents}}, \code{\link{cor}},
\code{\link{cov}},
\code{\link{svd}}, \code{\link{eigen}}.
}
\examples{
# the variances of the variables in the
# crimes data vary by orders of magnitude
data(crimes)
prcomp(crimes)
prcomp(crimes,scale=TRUE)
}




############## princomp ##############

princomp <- function(x, cor=FALSE, scores=TRUE,
  subset=rep(TRUE, nrow(as.matrix(x)))) {
 # it is tempting to add  use="all.obs" which could be passed to cov or
 # cor but then the calculation of N is complicated.
 z<- as.matrix(x)[subset,, drop=F]
 N <- nrow(z)
 if(cor) cv <- get("cor",envir=.GlobalEnv)(z)
 else    cv <- cov(z)
#  (svd can be used but gives different signs for some vectors)
 edc <- eigen(cv)
 cn <- paste("Comp.", 1:ncol(cv), sep="")
 names(edc$values) <- cn
 dimnames(edc$vectors) <- list(dimnames(x)[[2]], cn)
 scr<- NULL
 if (cor)
   {sdev <- sqrt(edc$values)
    sc <- (apply(z,2,var)*(N-1)/N)^0.5
    if (scores)
        scr<-(scale(z,center=T,scale=T) %*% edc$vectors)*sqrt(N/(N-1))
   }
 else
   {sdev <- sqrt(edc$values*(N-1)/N)
    sc <- rep(1, ncol(z))
    if (scores)
        scr<- (scale(z,center=T,scale=F) %*% edc$vectors)
   }
 names(sc) <- dimnames(x)[[2]]
 edc <-list(sdev=sdev, loadings=edc$vectors,
     center=apply(z,2,mean), scale=sc, n.obs=N, scores=scr)
# The Splus function also return list elements factor.sdev, correlations
# and coef, but these are not documented in the help. coef seems to equal
# load. The Splus function also return list elements call and terms which
# are not supported here.
 class(edc) <- "princomp"
 edc
}

print.princomp <- function(x) {
 cat("Standard deviations:\n")
 print(x$sdev)
 cat(length(x$scale), " variables and ", x$n.obs, "observations.\n")
 cat("Scale:\n")
 print(x$scale)
 invisible(x)
}

summary.princomp <- function(x) {
 per.var <- (x$sdev^2)/sum(x$sdev^2)
 r <- list(sdev=x$sdev, per.var= per.var, cum.var=cumsum(per.var))
 class(r) <- "summary.princomp"
 r
}

print.summary.princomp <- function(x) {
 cat("                        ",names(x$sdev),"\n")
 cat("Standard deviations   : ", x$sdev,      "\n")
 cat("Proportion of variance: ", x$per.var,   "\n")
 cat("Cumulative proportion : ", x$cum.var,   "\n")
 invisible(x)
}

plot.princomp <- function(obj, x=1, y=2, main="Scree Plot",
  xlab=paste("Principle component", x),
  ylab=paste("Principle component", y), ...) {
    plot(obj$scores[,c(x,y)], main=main, xlab=xlab, ylab=ylab, ...)
}

############### princomp.Rd ###############

\name{princomp}
\title{Principal Components Analysis}
\usage{
princomp(x, cor=FALSE, scores=TRUE, subset=rep(TRUE, nrow(as.matrix(x))))
print.princomp(obj)
summary.princomp(obj)
plot.princomp(obj)
}
\alias{print.princomp}
\alias{plot.princomp}
\arguments{
\item{x}{a matrix (or data frame) which provides the data
for the principal components analysis.}
\item{cor}{a logical value indicating whether the calculation should use the
correlation matrix or the covariance matrix.}
\item{score}{a logical value indicating whether the score on each principal
component should be calculated.}
\item{subset}{a vector used to select rows (observations) of the
data matrix \code{x}.}
}
\description{
This function performs a principal components analysis on
the given data matrix and returns the results as a \code{princomp} object.
The calculation is done using \code{eigen} on the correlation or
covariance matrix, as determined by \code{cor}. This is done for compatibility
with the Splus result (even though alternate forms for \code{x} - e.g. a
covariance matrix - are not supported as they are in Splus). A preferred method
of calculation is to use svd on \code{x}, as is done in \code{prcomp}.

Note that the scaling of results is affected by the setting of \code{cor}.
If \code{cor} is T then the divisor in the calculation of the sdev is N-1,
otherwise it is N. This has the effect that the result is slightly different
depending on whether scaling is done first on the data and cor set to F, or
done automatically in \code{princomp} with cor=T.

The print method for the these
objects prints the results in a nice format and the
plot method produces a scree plot.
}
\value{
\code{princomp} returns an list with class \code{"princomp"}
containing the following components:
\item{var}{the variances of the principal components
(i.e. the eigenvalues)}
\item{load}{the matrix of variable loadings (i.e. a matrix
whose columns contain the eigenvectors).}
\item{scale}{the value of the \code{scale} argument.}
}
\references{
Mardia, K. V., J. T. Kent and J. M. Bibby (1979).
\emph{Multivariate Analysis}, London: Academic Press.

Venerables, W. N. and B. D. Ripley (1994).
\emph{Modern Applied Statistics with S-Plus}, Springer-Verlag.
}
\seealso{
\code{\link{prcomp}}, \code{\link{prcomponents}}, \code{\link{cor}},
\code{\link{cov}}, \code{\link{eigen}}.
}
\examples{
# the variances of the variables in the
# crimes data vary by orders of magnitude
data(crimes)
princomp(crimes)
princomp(crimes,cor=TRUE)
princomp(scale(crimes, scale=T, center=T), cor=F)
}


############## prcomponents ##############

prcomponents <- function(x, center=TRUE, scale=TRUE, N=nrow(x)-1)
   {if (center) center <- apply(x,2,mean)
    else        center <- rep(0, ncol(x))
    if (scale)  scale  <- sqrt(apply(x,2,var))
    else        scale  <- rep(1, ncol(x))
    s <- svd(sweep(sweep(as.matrix(x),2, center),2, scale, FUN="/"))
    # remove anything corresponding to effectively zero singular values.
    rank <- sum(s$d > (s$d[1]*sqrt(.Machine$double.eps)))
    if (rank < ncol(x)) s$v <- s$v[,1:rank, drop=FALSE]
    s$d <- s$d/sqrt(N)

#   r <- list(sdev=s$d, proj=s$v,x=x %*% s$v, center=center, scale=scale)
    r <- list(sdev=s$d, proj=s$v, center=center, scale=scale)
    r
}



-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From Prof Brian Ripley <ripley@stats.ox.ac.uk>  Mon Aug 24 16:35:17 1998
From: Prof Brian Ripley <ripley@stats.ox.ac.uk> (Prof Brian Ripley)
Date: Mon, 24 Aug 1998 16:35:17 +0100 (BST)
Subject: prcomp and princomp
Message-ID: <199808241535.QAA21899@toucan.stats.ox.ac.uk>

> plot.prcomp <- function(obj, x=1, y=2, main="Scree Plot",
>   xlab=paste("Principle component", x),
>   ylab=paste("Principle component", y), ...) {
>     if (is.null(obj$x))
>  stop("Rotated x has not be calculated by prcomp. Use retx=T in prcomp.")
>     plot(obj$x[,c(x,y)], main=main, xlab=xlab, ylab=ylab, ...)
> }

> plot.princomp <- function(obj, x=1, y=2, main="Scree Plot",
>   xlab=paste("Principle component", x),
>   ylab=paste("Principle component", y), ...) {
>     plot(obj$scores[,c(x,y)], main=main, xlab=xlab, ylab=ylab, ...)
> }

Ouch. Enough of my students think that it is the spelling already!
This is _not_ what is meant by a scree plot, which plots the singular
values or eigenvalues.

-- 
Brian D. Ripley,                  ripley@stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From pgilbert@bank-banque-canada.ca  Mon Aug 24 17:50:17 1998
From: pgilbert@bank-banque-canada.ca (Paul Gilbert)
Date: Mon, 24 Aug 1998 12:50:17 -0400
Subject: prcomp and princomp
References: <199808241535.QAA21899@toucan.stats.ox.ac.uk>
Message-ID: <98Aug24.125405edt.13441@mailgate.bank-banque-canada.ca>

>Ouch. Enough of my students think that it is the spelling already!

Apologies. I thought I had eradicated this.

>This is _not_ what is meant by a scree plot, which plots the singular
>values or eigenvalues.

My mistake here was in the title main="Scree Plot", which in the end was not
what I decided to plot. At one point I thought the default plot method should be
the scree plot, but later decided the default plot method should plot the
principal components.  I noticed that in princomp-add.R you have

plot.princomp <- function(...) screeplot(...)

so perhaps you have different thoughts on this? I would propose having the
default plot the principal components (with the correct title) and use your
function scree, but not as the default.

Paul Gilbert


-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From p.dalgaard@biostat.ku.dk  Tue Aug 25 23:01:45 1998
From: p.dalgaard@biostat.ku.dk (Peter Dalgaard BSA)
Date: 26 Aug 1998 00:01:45 +0200
Subject: Variations on the t test
Message-ID: <x2g1ekk9fa.fsf@biostat.ku.dk>

One of the things that have been annoying me with both Splus and R is
that the "simple tests" are inconsistent with the lm/glm/dataframe
conventions, and that they become quite awkward to use when data have
to be extracted from a dataframe:

t.test(data$bp[data$sex=="F" & data$age>25], data$bp[data$sex=="M" &
data$age>25])

OK, so it's better to do 

eval(t.test(bp[sex=="F"],bp[sex=="M"]),subset(data,age>25))

but try explaining that to a class of MD's!


Below is a first sketch of a set of functions that extend t.test to
allow specification in terms of a list of vectors or a vector and a
grouping variable or a model formula, allowing data= and subset=
arguments as well. The paired test can also be specified using a
Pairs(x,y) construction, which should help prevent people from using
the wrong test. It's supposed to be downward compatible with the Splus
syntax.

I've been pushing Kurt to use a similar interface for all the ctest
functions, so I thought you'd all like to see them. If nothing else,
they should be interesting to study (they were definitely fun to
write), since they use some *really* dirty tricks relating to R's
evaluation model.

Anyway, here goes (remember, I said *first* sketch - there is at least
one bug in it, can you spot it?):

----
###
### The first couple of lines just guard against multiple loading
###

if(exists("t.test",envir=.GlobalEnv))rm(t.test)
if(exists("t.test.default",envir=.GlobalEnv))rm(t.test.default)

### Utility function to extract the environment the call is evaluated
### in. If you do eval(x,list), list is found as the variable "envir"
### in the stack frame immediately below the local frame

.ParentEnv<-function()
{
  parent<-sys.parent()
  grandparent <- sys.parent(2)
  if ( parent - grandparent == 1 )
    sys.frame(grandparent)
  else
    get("envir",envir=sys.frame(parent-1))
}

.t.test<-t.test

t.test.default<-function(x,...,group)
{
  e<-.ParentEnv()
  call <- if ( is.list(x) )
    substitute(t.test.list(x,...))
  else if (missing(group))
    substitute(.t.test(x,...))
  else 
    substitute(t.test.list(split(x, group),...))
  eval(call,e)
}

t.test<-function(...,data=sys.frame(sys.parent()),subset)
{  
  dname.add<-
  if (!missing(data)) 
    paste(", data frame:", deparse(substitute(data))) 
  else
    ""
  if (!missing(subset)){ 
    dname.add<-paste(dname.add, ", subset:", deparse(substitute(subset))) 
    subset <- eval(substitute(subset),data)
    data <- data[subset,] # had better be a data frame...
  }
  res<-eval(substitute(t.test.generic(...)), data)
  res$data.name <- paste(res$data.name, dname.add)
  res
}


t.test.generic<-function(x,...)
{
  UseMethod("t.test")
}

t.test.list<-function(l,...)
{
  if (length(l) != 2)
    stop("need exactly 2 groups")
  res<-.t.test(l[[1]],l[[2]],...)
  res$data.name<-deparse(substitute(l))
  nn<-names(l)
  if (length(nn) != 2)
    nn<-c("group 1", "group 2")
  names(res$estimate)<-nn
  res
}

t.test.formula<-function(f,...)
{
  e<-.ParentEnv()
  f[[1]]<-as.name("split")
  call<-as.call(list(as.name("t.test.list"), f, ...))
    eval(call,e)
}

Pairs<-function(x,y)
  structure(match.call(),class="paired")

t.test.paired<-function(l,...)
{
  e<-.ParentEnv()
  call<-c(as.list(l),list(...),list(paired=T))
  call[[1]]<-as.name(".t.test")
  call<-as.call(call)
  eval(call,e)
}




-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard@biostat.ku.dk)             FAX: (+45) 35327907
-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From pgilbert@bank-banque-canada.ca  Wed Aug 26 19:33:11 1998
From: pgilbert@bank-banque-canada.ca (Paul Gilbert)
Date: Wed, 26 Aug 1998 14:33:11 -0400
Subject: prcomp & princomp - revised
Message-ID: <98Aug26.143800edt.13450@mailgate.bank-banque-canada.ca>

My previous post about prcomp and princomp was done in some haste as I had long
ago indicated to Kurt that I would try to have this ready for the June release,
and it appeared that I would miss yet another release. I also need to get it out
before it becomes hopelessly buried by other work.

Brian Ripley kindly pointed out some errors, and also pointed out that I was
suggesting replacing some functions which were already working well in R. Other
than changing prcomp and princomp this was unintentional on my part. It seems
that some of these functions have either appeared since I first started this, or
that I was just careless (hard to imagine but it happens) and perhaps checked
only for prcomp methods and not princomp methods. In any event, I decided it was
necessary to check and indicate the changes I am proposing more carefully,
despite my time pressures, and also, first of all, to indicate my interest and
intentions more clearly.

I have no special attachment to principal components and there are many people
on this list who could do a better job at this than I can. I want to use PCA for
another problem I am working on, and I noticed several months ago that prcomp
and princomp were returning different results in R and Splus. Unless this is
fixed it puts me in the difficult position of having to choose between Splus and
R, which I would prefer not to do for reasons I have expressed from time to
time. (Another solution is to over-ride the mva library with my own functions,
but this has undesirable consequences when others use my library.) My intention
at this point is to submit these changes to the mva library and then not do any
more work on it. I am not suggesting that these methods are better, only that
they are more consistent with Splus. There is room for improvement (in both
Splus and R) and perhaps someone else would like to work on it. Of all the
changes below, the only ones I consider really important are the documentation
and the new version of prcomp which gives results like Splus.

I have some misgivings about adding yet another principal components function
(prcomponents below). However, I would like prcomp and princomp to return the
same results as Splus, and yet I see that there is need for improvement. I would
like this improvement to happen in a function which has a different name from
the functions in Splus, and perhaps be moved into an expanded "compatibility
library" at some point.

Paul Gilbert

_______

The code below makes the following changes relative to R Version 0.62.3 in
progress-release (August 19, 1998).

- prcomp is changed so that it takes similar arguments and usually reproduces
      Splus results. The criterion used for determining if a singular value
      is zero (following suggestions of Martin Maechler) is slightly different
      and this can occasionally give a different result.
- prcomp.Rd is changed to reflect the changes to prcomp
- summary.prcomp is defined to give a result more like summary.princomp. In
      Splus summary.prcomp does not exist and the default method is applied,
      but this difference seems reasonable?
- screeplot is made generic
- function(...) is changed to function(x, ...) for some plot methods (to be
         consistent with plot).
- print.prcomp is changed to be more like the result from Splus (Splus just
     uses print.default, so this is slightly different)
- plot.prcomp is renamed screeplot.prcomp to be consistent with the way
     this is done for princomp, but I have not worked on this
     function and it does not appear to do scaling or other nice things
     like screeplot.princomp. (Hopefully Brian Ripley will volunteer
     to fix it.)
- plot.prcomp calls - screeplot.prcomp


- princomp is still the function I submitted previously but has a
     few additional comments. It reproduces Splus results when the
     argument is a data matrix, but does not work with a covariance
     argument and the Splus version does.
- princomp.Rd is new.
- print.princomp is defined in princomp.R. This version is a bit more like
    the result from Splus and the function of the same name is removed
    from princomp-add.R.

- programs in the file princomp-add.R could be included in princomp.R but I
    was unsure how to deal with the V&R copyright. Perhaps it should
    be included in each function? I've left princomp-add.R as a
    separate file and done only the minimal number of changes necessary to
    make functions work with the changes in other files. (As far as my
    contributions in the other files are concerned I am happy to assign
    copyright to "The R Development Core Team".)


- a tentative and so far undocumented function prcomponents in the file
    prcomponents.R has been added. This is an incomplete attempt to deal
    with shortcomings of prcomp and princomp, the main problems with those
    functions being that the preferred computational method, svd, is used
    in prcomp, but princomp returns additional useful information. The
    function princomp uses eigen so that it returns results consistent
    with Splus, but does not yet support a covariance as an argument,
    which Splus does (and is the reason eigen is used). My function
    prcomponents also tries to give the user the ability to control
    what is used in the normalization (e.g. N or N-1), as suggested by
    Bill Venables. It does not include use="all.obs" which was in the
    prcomp I am proposing to replace, but it should. The difficulty is
    that this argument is passed to cor or cov, which are not
    called if svd is used.



############## prcomp.R replacement file ##############

screeplot    <- function(x, ...)  {UseMethod("screeplot")}

plot.prcomp   <- function(x, ...) {screeplot(x, ...)}


prcomp <- function(x, retx=TRUE, center=TRUE, scale=FALSE) {
# s <- svd(scale(x, center=center, scale=scale),nu=0)
# above produces warning since scale is both a function and a variable so
 s <- svd(get("scale",envir=.GlobalEnv)
   (x, center=center, scale=scale),nu=0)
# rank <- sum(s$d > 0)
 rank <- sum(s$d > (s$d[1]*sqrt(.Machine$double.eps)))
 if (rank < ncol(x)) s$v <- s$v[,1:rank, drop=F]
 s$d <- s$d/sqrt(max(1,nrow(x) -1))
 if(retx) r <- list(sdev=s$d, rotation=s$v, x=as.matrix(x) %*% s$v)
 else     r <- list(sdev=s$d, rotation=s$v)
 class(r) <- "prcomp"
 r
}

print.prcomp <- function(x) {
 cat("Standard deviations:\n")
 print(x$sdev)
 cat("\nRotation:\n")
 print(x$rotation)
 if (!is.null(x$x))
  {cat("\nRotated variables:\n")
   print(x$x)
  }
 invisible(x)
}

summary.prcomp <- function(obj, digits=3 )
{ vars <- obj$sdev^2
  vars <- vars/sum(vars)
  cat("Importance of components:\n")
  print(rbind("Standard deviation" = obj$sdev,
              "Proportion of Variance" = vars,
              "Cumulative Proportion" = cumsum(vars)))
  invisible(obj)
}

screeplot.prcomp <- function(x, main="Scree Plot", ylab="Variance",
  xlab="Component", ...) {
 plot(x$var, main=main, xlab=xlab, ylab=ylab, ...)
}

############### prcomp.Rd replacement file ###############

\name{prcomp}
\title{Principal Components Analysis}
\usage{
prcomp(x, retx=TRUE, center=TRUE, scale=FALSE)

}
\arguments{
\item{x}{a matrix (or data frame) which provides the data
for the principal components analysis.}
\item{retx}{a logical value indicating whether the rotated variables should
be returned.}
\item{center}{a logical value indicating whether the variables should
be shifted to be zero centered.}
\item{scale}{a logical value indicating whether the variables should
be scaled to have unit variance before the analysis takes place. The default
is F for consistency with S, but in general scaling is advisable.}
}
\description{
This function performs a principal components analysis on
the given data matrix and returns the results as a
\code{prcomp} object. The calculation is done with svd on the data matrix, not
by using eigen on the covariance matrix. This is generally the preferred method
for numerical accuracy.  The print method for the these
objects prints the results in a nice format and the
plot method produces a scree plot. The method \code{biplot} plots two selected
components against one another.
}
\value{
\code{prcomp} returns an list with class \code{"prcomp"}
containing the following components:
\item{sdev}{the standard deviation of the principal components
(i.e. the eigenvalues of the cov matrix - though the calculation is actually
done with the singular values of the data matrix.)}
\item{rotation}{the matrix of variable loadings (i.e. a matrix
whose columns contain the eigenvectors). The function princomp returns this
in the element \code{loadings}.}
\item{x}{if retx is true the value of the rotated data (the data multiplied by
the \code{rotation} matrix) is returned.}
}
\references{
Mardia, K. V., J. T. Kent, J and M. Bibby (1979),
\emph{Multivariate Analysis}, London: Academic Press.

Venerables, W. N. and B. D. Ripley (1994).
\emph{Modern Applied Statistics with S-Plus}, Springer-Verlag.
}

\seealso{
\code{\link{princomp}}, \code{\link{prcomponents}}, \code{\link{cor}},
\code{\link{cov}},
\code{\link{svd}}, \code{\link{eigen}}.
}
\examples{
# the variances of the variables in the
# crimes data vary by orders of magnitude
data(crimes)
prcomp(crimes)
prcomp(crimes,scale=TRUE)
plot(prcomp(crimes))
summary(prcomp(crimes))
}



############## princomp.R replacement file ##############

princomp <- function(x, cor=FALSE, scores=TRUE,
  subset=rep(TRUE, nrow(as.matrix(x)))) {
 # it is tempting to add  use="all.obs" which could be passed to cov or
 # cor but then the calculation of N is complicated.
 z<- as.matrix(x)[subset,, drop=F]
 N <- nrow(z)
 if(cor) cv <- get("cor",envir=.GlobalEnv)(z)
 else    cv <- cov(z)
#  (svd can be used but gives different signs for some vectors)
 edc <- eigen(cv)
 cn <- paste("Comp.", 1:ncol(cv), sep="")
 names(edc$values) <- cn
 dimnames(edc$vectors) <- list(dimnames(x)[[2]], cn)
 scr<- NULL
 if (cor)
   {sdev <- sqrt(edc$values)
    sc <- (apply(z,2,var)*(N-1)/N)^0.5
    if (scores)
        scr<-(scale(z,center=T,scale=T) %*% edc$vectors)*sqrt(N/(N-1))
   }
 else
   {sdev <- sqrt(edc$values*(N-1)/N)
    sc <- rep(1, ncol(z))
    if (scores)
        scr<- (scale(z,center=T,scale=F) %*% edc$vectors)
   }
 names(sc) <- dimnames(x)[[2]]
 edc <-list(sdev=sdev, loadings=edc$vectors,
     center=apply(z,2,mean), scale=sc, n.obs=N, scores=scr)
# The Splus function also return list elements factor.sdev, correlations
# and coef, but these are not documented in the help. coef seems to equal
# load. The Splus function also return list elements call and terms which
# are not supported here.
 class(edc) <- "princomp"
 edc
}

print.princomp <- function(x) {
 cat("Standard deviations:\n")
 print(x$sdev)
 cat(length(x$scale), " variables and ", x$n.obs, "observations.\n")
 cat("Scale:\n")
 print(x$scale)
 invisible(x)
}


############### princomp.Rd new file ###############

\name{princomp}
\title{Principal Components Analysis}
\usage{
princomp(x, cor=FALSE, scores=TRUE, subset=rep(TRUE, nrow(as.matrix(x))))
print.princomp(obj)
summary.princomp(obj)
plot.princomp(obj)
}
\alias{print.princomp}
\alias{plot.princomp}
\arguments{
\item{x}{a matrix (or data frame) which provides the data
for the principal components analysis.}
\item{cor}{a logical value indicating whether the calculation should use the
correlation matrix or the covariance matrix.}
\item{score}{a logical value indicating whether the score on each principal
component should be calculated.}
\item{subset}{a vector used to select rows (observations) of the
data matrix \code{x}.}
}
\description{
This function performs a principal components analysis on
the given data matrix and returns the results as a \code{princomp} object.
The calculation is done using \code{eigen} on the correlation or
covariance matrix, as determined by \code{cor}. This is done for compatibility
with the Splus result (even though alternate forms for \code{x} - e.g. a
covariance matrix - are not supported as they are in Splus). A preferred method
of calculation is to use svd on \code{x}, as is done in \code{prcomp}.

Note that the scaling of results is affected by the setting of \code{cor}.
If \code{cor} is T then the divisor in the calculation of the sdev is N-1,
otherwise it is N. This has the effect that the result is slightly different
depending on whether scaling is done first on the data and cor set to F, or
done automatically in \code{princomp} with cor=T.

The print method for the these
objects prints the results in a nice format and the
plot method produces a scree plot.
}
\value{
\code{princomp} returns an list with class \code{"princomp"}
containing the following components:
\item{var}{the variances of the principal components
(i.e. the eigenvalues)}
\item{load}{the matrix of variable loadings (i.e. a matrix
whose columns contain the eigenvectors).}
\item{scale}{the value of the \code{scale} argument.}
}
\references{
Mardia, K. V., J. T. Kent and J. M. Bibby (1979).
\emph{Multivariate Analysis}, London: Academic Press.

Venerables, W. N. and B. D. Ripley (1994).
\emph{Modern Applied Statistics with S-Plus}, Springer-Verlag.
}
\seealso{
\code{\link{prcomp}}, \code{\link{prcomponents}}, \code{\link{cor}},
\code{\link{cov}}, \code{\link{eigen}}.
}
\examples{
# the variances of the variables in the
# crimes data vary by orders of magnitude
data(crimes)
princomp(crimes)
princomp(crimes,cor=TRUE)
princomp(scale(crimes, scale=T, center=T), cor=F)
plot(princomp(crimes))
biplot(princomp(crimes))
summary(princomp(crimes))
loadings(princomp(crimes))
}


############## princomp-add.R replacement file ##############

# copyright (C) 1998 W. N. Venables and B. D. Ripley
#
predict.princomp <- function(object, newdata,...)
{
  if (missing(newdata)) return(object$scores)
  scale(newdata, object$center, object$scale) %*% object$loadings
}

summary.princomp <-
function(object, loadings = F, cutoff = 0.1, digits=3, ...)
{
  vars <- object$sdev^2
  vars <- vars/sum(vars)
  cat("Importance of components:\n")
  print(rbind("Standard deviation" = object$sdev,
              "Proportion of Variance" = vars,
              "Cumulative Proportion" = cumsum(vars)))
  if(loadings) {
    cat("\nLoadings:\n")
    cx <- format(round(object$loadings, digits=digits))
    cx[abs(object$loadings) < cutoff] <-
      substring("       ", 1, nchar(cx[1,1]))
    print(cx, quote = F, ...)
  }
  invisible(object)
}


plot.princomp <- function(x, ...) {screeplot(x, ...)}

screeplot.prcomp <- screeplot.princomp <-
function(x, npcs=min(10, length(x$sdev)), type=c("barplot", "lines"),
         main = deparse(substitute(x)), ...)
{
  eval(main)
  type <- match.arg(type)
  pcs <- x$sdev^2
  xp <- seq(length=npcs)
  if(type=="barplot") barplot(pcs[xp], names = names(pcs[xp]),
       main = main, ylab = "Variances", ...)
  else {
    plot(xp, pcs[xp], type = "b", axes = F, main = main, xlab="",
            ylab = "Variances", ...)
    axis(2)
    axis(1, at = xp, labels = names(pcs[xp]))
  }
  invisible()
}

loadings <- function(x) x$loadings



############## prcomponents.R  new file ##############

prcomponents <- function(x, center=TRUE, scale=TRUE, N=nrow(x)-1)
   {if (center) center <- apply(x,2,mean)
    else        center <- rep(0, ncol(x))
    if (scale)  scale  <- sqrt(apply(x,2,var))
    else        scale  <- rep(1, ncol(x))
    s <- svd(sweep(sweep(as.matrix(x),2, center),2, scale, FUN="/"))
    # remove anything corresponding to effectively zero singular values.
    rank <- sum(s$d > (s$d[1]*sqrt(.Machine$double.eps)))
    if (rank < ncol(x)) s$v <- s$v[,1:rank, drop=FALSE]
    s$d <- s$d/sqrt(N)

#   r <- list(sdev=s$d, proj=s$v,x=x %*% s$v, center=center, scale=scale)
    r <- list(sdev=s$d, proj=s$v, center=center, scale=scale)
    r
}




-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From Prof Brian Ripley <ripley@stats.ox.ac.uk>  Mon Aug 31 15:19:26 1998
From: Prof Brian Ripley <ripley@stats.ox.ac.uk> (Prof Brian Ripley)
Date: Mon, 31 Aug 1998 15:19:26 +0100 (BST)
Subject: Packages aov, modreg, lqs, psplines
Message-ID: <199808311419.PAA22293@toucan.stats.ox.ac.uk>

I now have versions of code that is destined (I believe) for 0.63 which
is in a suitable state for comment. The files are at

	ftp://ftp.stats.ox.ac.uk/pub/R  

(Our www server is being moved, so may be intermittently down, but this
ftp server should be stable.)  All are R packages, for the moment for
personal use only (no re-distribution). Use with 0.62.3 or 0.63 (although
I am aware of some problems with use with 0.63 that I have reported).


aov.tar.gz:
==========

aov with Error terms, proj, model.tables, se.contrast, replications, 
eff.aovlist, C, dummy.coef, add1, drop1, step, kappa, labels.

This is in I believe close to final form.


modreg.tar.gz:
=============

ksmooth                  Kernel Regression Smoother
loess                    Local Polynomial Regression Fitting
loess.control            Set Parameters for Loess
plot.ppr                 Plot Ridge Functions for Projection Pursuit
                            Regression Fit
ppr                      Projection Pursuit Regression
predict.loess            Predict Loess Curve or Surface
predict.smooth.spline    Predict from Smoothing Spline Fit
scatter.smooth           Scatter Plot with Smooth Curve Fitted by Loess
smooth.spline            Fit a Smoothing Spline
supsmu                   Friedmans's SuperSmoother

Probably only bug-fixing left, but I would welcome comments about the
extent of the loess functionality.


lqs.tar.gz:
==========

cov.rob                  Robust Estimation of Multivariate Location and
                            Scatter
lqs                      Resistant Regression by Least Trimmed and Least
                            Quantile Sum of Squares
lmsreg, ltsreg, cov.mve  Compatibility functions

This is much less complete (and the claimed mcd method is not yet
operational).  Comments please both on the design and from any experts
out there on the methodology used. (BTW, as all the programs I have give
different answers, it is very hard to establish the true answer. I am
fairly convinced that both S-PLUS versions are slightly wrong.)

I plan to add S-estimation, a general M-estimator (based on my function
rlm) and MM-estimation to this over the next few weeks.


While I am posting, a R port of Jim Ramsay's psplines package is also there
en route to CRAN, file (pspline_1.0-1.tar.gz) as well as an update of the
V&R `R' Complements to 0.62.3.

-- 
Brian D. Ripley,                  ripley@stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

From pgilbert@bank-banque-canada.ca  Mon Aug 31 22:18:27 1998
From: pgilbert@bank-banque-canada.ca (Paul Gilbert)
Date: Mon, 31 Aug 1998 17:18:27 -0400
Subject: isolating R/S and operating system
 differences
Message-ID: <98Aug31.172601edt.13446@mailgate.bank-banque-canada.ca>

Below is a revised version of my kernel of functions for isolating R/S and
operating system  differences. The main change is "date" which I've renamed
"date.parsed" to avoid conflicts with the R and S date functions. The R call now
uses system() rather than unix() to avoid warning messages in R 0.62.3.

Paul Gilbert

##############################################################################


# This file has code which contains operating system and S/R specific
#   functions. They are intended to be used as a kernel to help
#   protect other code from these problems.

# The MSwindows versions are not done.

# The following functions are attempted:
#   For S/R differences:
#      global.assign, system.info, exists.graphics.device, unlink,
#      synchronize,  list.add for [["new.element"]]<-
#   For OS differences:
#     system.call, sleep, present.working.directory, whoami, file.copy,
#     file.date.info, date.parsed, mail, unlink, local.host.netname,

# Also a number of is.xxx functions are defined to identify systems.

# The variable  .SPAWN is also set to be used to identify if Splus "For" loops
#    should be used. (It is sometimes better not to use these even in Splus.)

##############################################################################

#            General Logic and organization of these functions

# 1/ The first group of functions are for identifying S or R and flavours.
# 2/ The second group of functions are for identifying the operating system.
# 3/ The third group specify a few functions which depend only on the
#         differences between S and R.
# 4/ The fourth group specify functions which depend only on the
#         differences among operating system.
# 5/ The fifth group specify a few functions which depend on both R/S and the
#         differences among operating system.

#  >>> I would very much like any input WRT  MS Windows / Win95 / NT / Mac <<<

# The function system.call is defined in order to provide a generic way to
#  make a call to the operating system. When the calls are specific
#  to Unix then the function unix() might be used (though that is now
#  deprecated in R and produces a warning messsage). However, in general the
#  purpose of these functions is not to give a generic way to call the operating

#  system, but rather a generic way to do things that require a call to the
#  operating system (like date, mail, sleep, whoami).


##############################################################################

system.info <- function()
     {if( !exists("version"))
         { #-- `Vanilla' S (i.e. here "S version 4")
           #- this now works for  S version 4  (this is not S-plus 4.0, maybe
           #             part of S-plus 5.0 !):
           lv <- nchar(Sv <- Sversion())
           r <- list(
       major = substring(Sv, 1,1),
       minor = substring(Sv, lv,lv))
         }
   else
     {r <- version
      r$minor <- as.numeric(r$minor)
      r$major <- as.numeric(r$major)
     }
   if      (is.Splus())    r$language <- "Splus"
   else if (is.Svanilla()) r$language <- "S"
   r$OSversion <- OSversion()
   r$OStype    <- OStype()
   r
  }


###########################################################

#    1/  Functions are for identifying S or R and flavours.

###########################################################

#Note It is tempting to use system.info as defined above, but there is a
#        bootstrapping problem to solve.

if (! exists("is.R"))
 {is.R <- function()
     {exists("version") && !is.null(vl <- version$language) && vl == "R" }
 }

is.R.pre0.60 <- function()
  {is.R() && ((as.numeric(version$major)+.01*as.numeric(version$minor)) <0.60) }

is.R.pre0.63.2 <- function()
  {is.R() && ((as.numeric(version$major)+.01*as.numeric(version$minor)) <0.623)}



is.S <- function(){is.Svanilla() | is.Splus() }
is.Svanilla <- function(){!exists("version")}
is.Splus <- function(){exists("version") && is.null(version$language)}
is.Splus.pre3.3 <- function()
   { ## <= 3.2
    is.Splus() &&  ((system.info()$major+.1*system.info()$minor) < 3.3)
   }

###########################################################

#    2/  Functions are for identifying the operating system.

###########################################################

if (is.R())
   {OStype <- function()
      {if("Win32"== machine())          return("MS Windows")
       else if("Macintosh"== machine()) return("Macintosh") #needs to be checked

       else if("Unix"== machine())      return ("Unix")
      }
   }

if (is.S())
   {OStype <- function()
      {if(charmatch("MS Windows", version$os, nomatch=0))
                                return("MS Windows")
       else if(charmatch("Macintosh",  version$os, nomatch=0))
                                return("Macintosh") # needs to be checked
       else if(exists("unix"))  return ("Unix")
      }
   }


is.MSwindows <- function(){OStype() == "MS Windows"}
is.Mac       <- function(){OStype() == "Macintosh" }
is.unix      <- function(){OStype() == "Unix" }

{
if (is.unix())
  {OSversion <- function()
    {paste(system.call("uname -s"),
           system.call("uname -r | sed -e 's/\\.\.\*//'"), sep="") }
  }
else if(is.MSwindows())
  {if (is.R())
     {OSversion <- function()
        {# This is not great since NT is not distinguished but
         #    is.Win32() below will work ok
         if("Win32"== machine()) return("MS Windows 95")
         else return ("unkown")
        }
     }
   if (is.S())
     {OSversion <- function()
        {if("MS Windows 3.1"==version$os) return("MS Windows 3.1")
         if("MS Windows 95" ==version$os) return("MS Windows 95")
         if("MS Windows 98" ==version$os) return("MS Windows 98")
         if("MS Windows NT" ==version$os) return("MS Windows NT")
         else return ("unkown")
        }
     }
  }
else OSversion <- function() "unknown"
}


# Other is.xxx() should be added here.

# determining Unix flavours doesn't seem to be too important but ...
is.Sun4 <- function() {is.unix() && OSversion() == "SunOS4" }
is.Sun5 <- function() {is.unix() && OSversion() == "SunOS5" }
is.Linux <- function(){is.unix() && OSversion() == "linux"}

# Windows flavours may be more important but these are untested !!!
is.Win3.1 <- function(){is.MSwindows() && OSversion() == "MS Windows 3.1"}
is.Win95  <- function(){is.MSwindows() && OSversion() == "MS Windows 95"}
is.WinNT  <- function(){is.MSwindows() && OSversion() == "MS Windows NT"}
is.Win32  <- function(){is.Win95() | is.WinNT() }





###########################################################

#    3/  Functions depending only on the
#         differences between S and R

###########################################################

if(is.S())
   {if(is.unix())system.call  <- unix
    global.assign <- function(name, value) {assign(name,value, where = 1)}
    .SPAWN <- TRUE
    exists.graphics.device <- function(){dev.cur() !=1 }
    open.graphics.device  <- function(display=getenv("DISPLAY"))
                                 {openlook(display) }
    #                            {motif(display) }
    close.graphics.device <- function(){dev.off() }
    if (!exists("set.seed.Splus")) set.seed.Splus <- set.seed
    set.seed <- function(seed=NULL)
      {if (is.null(seed))
          seed <-.Random.seed
       else
         {if (1==length(seed)) set.seed.Splus(seed)
          else global.assign(".Random.seed", seed)
         }
       seed
      }

    "list.add<-" <- function(x, replace, value)
       {# replace or add elements to a list.
        x[replace] <- value
        # x[[replace]] <- value  would be more logical but doesn't work
        x
       }
   }

if(is.R())
   {#tempfile <- function(f)
    #   {# Requires C code also from Friedrich Leisch not in version 0.15 of R.
    #    d<-"This is simply a string long enough to hold the name of a tmpfile";

    #     .C("tmpf", as.character(d))[[1]]
    #    }

    if (is.R.pre0.60())
        {tempfile <- function(pattern = "file")
                {system(paste("for p in", paste(pattern, collapse = " "), ";",
                       "do echo /tmp/$p$$; done"),
                 intern = TRUE)
                }
        }

#    unlink <- function(file) system.call(paste("rm -fr ", file))
    global.assign <- function(name, value)
                          {assign(name,value, envir=.GlobalEnv)}
    synchronize<- function(x){NULL} # perhaps this should do something?
    .SPAWN <- FALSE
    dev.ask <- function(ask=T){par(ask=ask)}
    if (is.R.pre0.63.2())
         exists.graphics.device <- function(){exists(".Device")}
    else exists.graphics.device <- function(){dev.cur() !=1 }
    open.graphics.device  <- function(display=getenv("DISPLAY")) {x11(display)}
    close.graphics.device <- function(){F} # how do I do this?
    set.seed <- function(seed=NULL)
      {if (is.null(seed))
         {if (!exists(".Random.seed")) zzz <- runif(1) # seed may not yet exist
          seed <-.Random.seed
         }
       else
         {if (1==length(seed))
             global.assign(".Random.seed",round(runif(3,min=seed,max=1e5*seed)))

          else global.assign(".Random.seed", seed)
         }
       seed
      }

   "list.add<-" <- function(x, replace, value)
     {# replace or add elements to a list.
      if (is.numeric(replace))
        {# x<- do.call("default.[[<-", list(x,replace,value))   # use default
         x[[replace]] <- value
         return(x)
        }
      if (is.null(value))  value <- list(NULL)
      if (!is.list(value)) value <- list(value)
      if (1 == length(value))
       {for (i in seq(length(replace)))
          x<- do.call("$<-", list(x,replace[i],value[[1]]))
       }
      else
        {if(length(value) != length(replace) )
         stop("number of replacement values != number of elements to replace")
         for (i in seq(length(replace)))
            x<- do.call("$<-", list(x,replace[i],value[[i]]))
        }
      x
     }
 }


###########################################################

#    4/  Functions depending only on the
#         differences among operating system.

###########################################################

if(is.unix())
  {sleep <- function(n) {unix(paste("sleep ", n))} # pause for n seconds
   present.working.directory <- function(){unix("pwd")} # present directory
   whoami <- function(){unix("whoami")} # return user id (for mail)
   local.host.netname <-function() {unix("uname -n")}

   mail <- function(to, subject="", text="")
    {# If to is null then mail is not sent (useful for testing).
     file <- tempfile()
     write(text, file=file)
   if(!is.null(to)) unix(paste("cat ",file, " | mail  -s '", subject, "' ", to))

     unlink(file)
     invisible()
    }

   file.copy <- function(from, to)unix(paste("cp ", from, to)) # copy file

   file.date.info <- function(file.name)
     {# This could be a lot better. It will fail for files older than a year.
      # Also, a returned format like date() below would be better.
      mo <- (1:12)[c("Jan","Feb","Mar","Apr","May", "Jun","Jul","Aug", "Sep",
         "Oct","Nov","Dec") ==substring(unix(paste("ls -l ",file)),33,35)]
      day <- as.integer(substring(unix(paste("ls -l ",file.name)),37,38))
      hr  <- as.integer(substring(unix(paste("ls -l ",file.name)),40,41))
      sec <- as.integer(substring(unix(paste("ls -l ",file.name)),43,44))
      c(mo,day,hr,sec)
     }

}

if(is.MSwindows())
  {system.call  <- function(cmd)
         {stop("system calls must be modified for this operating system.")}
   sleep <- system.call
   present.working.directory <- system.call
   whoami <- system.call
   file.copy <- system.call
   file.date.info <- system.call
  }



###########################################################

#    5/  Functions depending on both R/S and the
#         differences among operating system.

###########################################################

if(is.unix())
  {if(is.R())
     {#unix <- function(cmd) system(cmd, intern=T)
      # unix() is now a function in R but deprecated in favour of system()
      # (This is a bit dangerous, as these calls may be system dependent.)

      system.call <- function(cmd) system(cmd, intern=T)

  # the following date function might be made system independent as a C call.
      date.parsed <-function()
        {d<-parse(text=strsplit(
              system.call("date \'+%Y %m %d %H %M %S\'")," ")[[1]])
         list(y=  eval(d[1]),
              m=eval(d[2]),
              d= eval(d[3]),
              h= eval(d[4]),
              m= eval(d[5]),
              s= eval(d[6]),
              tz=system.call("date '+%Z'"))
        }
     }
   if(is.S())
     {system.call <- function(cmd) unix(cmd)

      date.parsed <-function()
        {d <- parse(text=unix("date '+%Y %m %d %H %M %S'"),white=T)
         list(y=  eval(d[1]),
              m=eval(d[2]),
              d= eval(d[3]),
              h= eval(d[4]),
              m= eval(d[5]),
              s= eval(d[6]),
              tz=unix("date '+%Z'"))
        }
     }
  }




##############################################################################



-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-
r-devel mailing list -- Read http://www.ci.tuwien.ac.at/~hornik/R/R-FAQ.html
Send "info", "help", or "[un]subscribe"
(in the "body", not the subject !)  To: r-devel-request@stat.math.ethz.ch
_._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._._

