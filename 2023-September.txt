From AHL27 @end|ng |rom p|tt@edu  Fri Sep  1 11:17:57 2023
From: AHL27 @end|ng |rom p|tt@edu (Lakshman, Aidan H)
Date: Fri, 1 Sep 2023 09:17:57 +0000
Subject: [Rd] Updated `dendrapply`
Message-ID: <BL0PR04MB470648B7E324632897EF4EEDD9E5A@BL0PR04MB4706.namprd04.prod.outlook.com>

Hi everyone,
Ivan and I had a few discussions several months ago regarding `dendrapply`, but now that I've had the chance to work on it more specifically and discuss it Martin Maechler at the R Project Sprint, I figured it would be a good idea to start a new email chain.
I've refactored `dendrapply`, and the current implementation is available in my bugzilla report (https://bugs.r-project.org/show_bug.cgi?id=18480). This project started due to the help page for `dendrapply`, which specifically requested users to contribute improvements to the function. I'm including a lot of writeup here because I'm very aware that this is a relatively large contribution for someone that doesn't have a history of contributing a lot of code to base, and I'd like to justify the inclusion.
Feel free to skip everything I've written and instead use the following links. A thorough discussion follows.
- Bugzilla with patch: https://bugs.r-project.org/show_bug.cgi?id=18480
- R Checks: https://github.com/r-devel/r-svn/pull/111
- Discussion at R Project Sprint: https://github.com/r-devel/r-project-sprint-2023/discussions/6
- Original blog post about this (long, code out of date, but has a simpler explanation of the implementation): https://www.ahl27.com/posts/2023/02/dendrapply/

Responses to common questions:
- Why does this project need to be done?
The current implementation in `stats::dendrapply` is recursive, and thus has issues with deeply nested dendrogram objects. As of 2015, users experienced issues with recursive operations on dendrograms causing stack overflow issues (see https://bugs.r-project.org/show_bug.cgi?id=15215). This has been alleviated by better computers and short-term workarounds, but many users have limited resources and/or need for large trees. Even with sufficient memory, a recursive implementation incurs a nontrivial amount of unneccessary computational overhead. I'll also add that this is a feature that was requested in R itself (see Note section in `?dendrapply`), and Martin Maechler has been supportive of the work thus far on it.
- What does this implementation do?
There are a few improvements in this implementation. The first is that function application to dendrogram objects is no longer recursive. This implementation is also based in C, providing a performance boost of at least 4x (see later question for details). Finally, iterative application of functions in C allows for flexibility in *how* the dendrogram is traversed, which gives end-users a significant amount of power to work with tree-like structures in an intuitive way. An easy example is subsetting based on leaf values--if a user wanted to subset a dendrogram to only certain leaves, there isn't a good way to do this in base R (even with dendrapply). `how='post.order'` fixes this problem. I'm happy to provide additional examples if needed.
- Why C? This is harder to maintain than R.
This is a valid point. I did my best to include as much documentation as possible, and I'm also volunteering myself to help maintain this function. C provides a lot of power in working with dendrograms, since R's toolkit for tree-like structures is relatively lacking. This refactor is theoretically doable in R, but the implementation would involve an immense amount of memory overhead to ensure we can preserve the node states as we traverse the tree. There is precedence for a C implementation of dendrapply (see other `*apply` functions). Additionally, this decreases function application overhead, and allows future extensions to be written purely in R in a much simpler way. I think this tradeoff is worth it, but I am happy to discuss implementation specifics with anyone that is interested.
- Ivan previously mentioned issues with user specific `[[.dendrogram` implementations, and it doesn't seem that you've fixed that.
This is correct. I discovered during the R project sprint that `stats::dendrapply` does not respect user-specific implementations of `[[.dendrogram`. stats::`[[.dendrogram` has its own issues; if the user defines multiple classes for a dendrogram object, double bracket subsetting will remove the class (a bug report will be filed for this shortly). My implementation exactly replicates the performance of stats::`[[.dendrogram`, and if users are in need of a function that can respect custom subset overloading, I can address those feature requests if/when they are submitted.
- Backwards compatibility?
>From current testing, this is a drop-in replacement for `stats::dendrapply`. R builds successfully, and all >400 tests in the CRAN package that uses `dendrapply` the most (dendextend) pass with no changes from the original. The additional argument `how=c('pre.order', 'post.order')` is the same syntax as `rapply`, and additional documentation has been added to the `dendrapply.Rd` to reflect this change. This is still an unfinished TODO; the internal R testing for `dendrapply` is very sparse. I haven't been able to find any differences between stats::dendrapply and this implementation, but I am planning to run a full check against all CRAN packages that use `dendrapply`. I'm also planning to add additional regression testing to R either as part of this patch in a separate patch.
- You mentioned there was more to the listed '4x improvement'
Yes. I haven't yet put together a comprehensive benchmark for highly unbalanced trees, and in truth there are so many possible tree structures that it would be challenging to test them all. However, on trees with 5 leaves the performance is roughly identical to that of `stats`, and benchmarking with `microbenchmark` demonstrates performance gains of roughly 5x on fully balanced trees with 10-5000 leaves. This should be a lower bound for performance, since fully balanced trees minimize internal nodes and thus have less recursion...so on reasonably sized trees of arbitrary structure we should have at least around a 4x improvement. I'll also stress that the focus of this patch is not a runtime improvement--it's nice that we get a speedup, but the added value here is the removal of recursive calls.
- Why not just put this in a package?
I think there's value in fleshing out the structures included in base R. Dendrograms are a general tree structure, and few programming languages provide support for these out of the box. Dendrogram objects are currently rarely used, but with a little bit of additional functionality, they could be a very powerful tool for users. These have applications in a variety of fields that are not just phylogenetics; implementations of domain-specific tools (e.g. `ape`, `DECIPHER`) are better suited to 3rd party packages. However, `dendrograms` already exist in base and have poor support, which is even admitted in their help files. `dendrapply`. While it's currently limited to acting on dendrogram objects, a solid implementation would open the door to generalizing dendrapply to work on any nested list. This is my personal opinion, and there is certainly an argument to be made that `dendrapply` (and even `dendrogram` as a whole) could live outside of base.
- Why/how were the included traversal strategies chosen?
The default, pre.order, was chosen because it replicates existing functionality. post.order was included because it lends itself well to a lot of applications. Between these two methods, we have a way to apply a function to trees ensuring that parents are evaluated before children, and ensuring children are evaluated before parents. Future extensions to support an in.order or BFS/level.order traversal is definitely an option, but I don't think the added implementation effort and complexity adds a lot of functionality over the two that have been included.
There's also been previous comments regarding the structure of dendrograms. While hclust will return a bifurcating tree, dendrogram objects (and dendrapply) support arbitrary multifurcations and edge weights.
Apologies for the lengthy email. If you've read even half, thanks for your time. There?s likely some optimizations that could be made in the C code dealing with R structures; I?m still learning the intricacies of some of the more specific points of this. One that comes to mind is converting the repeated `lang2` calls to instead initialize a `lang2` call and then change the symbol in the call, as is done in `lapply` (and was mentioned by Ivan in my last submission). I?ll test that as well.
Further feedback is welcome and much appreciated.
-Aidan

-----------------------
Aidan Lakshman (he/him)<https://www.ahl27.com/>
Doctoral Fellow, Wright Lab<https://www.wrightlabscience.com/>
University of Pittsburgh School of Medicine
Department of Biomedical Informatics
ahl27 at pitt.edu
(724) 612-9940


	[[alternative HTML version deleted]]


From kry|ov@r00t @end|ng |rom gm@||@com  Fri Sep  1 16:24:43 2023
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Fri, 1 Sep 2023 17:24:43 +0300
Subject: [Rd] Updated `dendrapply`
In-Reply-To: <BL0PR04MB470648B7E324632897EF4EEDD9E5A@BL0PR04MB4706.namprd04.prod.outlook.com>
References: <BL0PR04MB470648B7E324632897EF4EEDD9E5A@BL0PR04MB4706.namprd04.prod.outlook.com>
Message-ID: <20230901172443.20f904a2@arachnoid>

? Fri, 1 Sep 2023 09:17:57 +0000
"Lakshman, Aidan H" <AHL27 at pitt.edu> ?????:

> - Ivan previously mentioned issues with user specific `[[.dendrogram`
> implementations, and it doesn't seem that you've fixed that.

> This is correct. I discovered during the R project sprint that
> `stats::dendrapply` does not respect user-specific implementations of
> `[[.dendrogram`. stats::`[[.dendrogram` has its own issues; if the
> user defines multiple classes for a dendrogram object, double bracket
> subsetting will remove the class (a bug report will be filed for this
> shortly).

True, my warning about not handing potential subclasses of dendrogram
was purely theoretical.

(A hypothetical subclass of dendrogram could work with the current
[[.dendrogram if it ensured that its own class name always precedes
'dendrogram' in the class vector, thus never being downstream from
stats:::`[[.dendrogram` in a chain of NextMethod() calls. But that's
still hypothetical.)

I see that your current implementation very nicely bounds the PROTECT()
stack usage and avoids the need to deallocate arbitrary SEXPs, which is
awkward to do with R's garbage collector API. Congratulations!

-- 
Best regards,
Ivan


From murdoch@dunc@n @end|ng |rom gm@||@com  Mon Sep  4 12:01:50 2023
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Mon, 4 Sep 2023 06:01:50 -0400
Subject: [Rd] Bug in PCRE interface code
Message-ID: <c0079cbe-0ec0-4e8a-09d1-b6d3a0bce9cd@gmail.com>

This Stackoverflow question https://stackoverflow.com/q/77036362 turned 
up a bug in the R PCRE interface.

The example (currently in an edit to the original question) tried to use 
named capture with more than 127 named groups.  Here's the code:

append_unique_id <- function(x) {
   for (i in seq_along(x)) {
     x[i] <- paste0("<", paste(sample(letters, 10), collapse = ""), ">", 
x[i])
   }
   x
}

list_regexes <- sample(letters, 128, TRUE) # <<<<<<<<<<< change this to
                                            #             127 and it works
regex2 <- append_unique_id(list_regexes)
regex2 <- paste0("(?", regex2, ")")
regex2 <- paste(regex2, collapse = "|")

out <- gregexpr(regex2, "Cyprus", perl = TRUE, ignore.case = TRUE)
#> Error in gregexpr(regex2, "Cyprus", perl = TRUE, ignore.case = TRUE): 
attempt to set index -129/128 in SET_STRING_ELT

I think the bug is in R, here: 
https://github.com/wch/r-source/blob/57d15d68235dd9bcfaa51fce83aaa71163a020e1/src/main/grep.c#L3079

This is the line

	    int capture_num = (entry[0]<<8) + entry[1] - 1;

where entry is declared as a pointer to a char.  What this is doing is 
extracting a 16 bit number from the first two bytes of a character 
string holding the name of the capture group.  Since char is a signed 
type, the conversion of bytes to integer gets messed up and the value 
comes out wrong.

Duncan Murdoch


From tdhock5 @end|ng |rom gm@||@com  Tue Sep  5 23:06:49 2023
From: tdhock5 @end|ng |rom gm@||@com (Toby Hocking)
Date: Tue, 5 Sep 2023 14:06:49 -0700
Subject: [Rd] Bug in PCRE interface code
In-Reply-To: <c0079cbe-0ec0-4e8a-09d1-b6d3a0bce9cd@gmail.com>
References: <c0079cbe-0ec0-4e8a-09d1-b6d3a0bce9cd@gmail.com>
Message-ID: <CALK03d3c7HMcbF5rZZTNN6yBcX8=-6N-sGK48=GfZu8kiUyGQg@mail.gmail.com>

BTW this is documented here
http://pcre.org/current/doc/html/pcre2api.html#infoaboutpattern with a
helpful example, copied below.

As a simple example of the name/number table, consider the following
pattern after compilation by the 8-bit library (assume PCRE2_EXTENDED
is set, so white space - including newlines - is ignored):

  (?<date> (?<year>(\d\d)?\d\d) - (?<month>\d\d) - (?<day>\d\d) )

There are four named capture groups, so the table has four entries,
and each entry in the table is eight bytes long. The table is as
follows, with non-printing bytes shows in hexadecimal, and undefined
bytes shown as ??:

  00 01 d  a  t  e  00 ??
  00 05 d  a  y  00 ?? ??
  00 04 m  o  n  t  h  00
  00 02 y  e  a  r  00 ??

On Mon, Sep 4, 2023 at 3:02?AM Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
>
> This Stackoverflow question https://stackoverflow.com/q/77036362 turned
> up a bug in the R PCRE interface.
>
> The example (currently in an edit to the original question) tried to use
> named capture with more than 127 named groups.  Here's the code:
>
> append_unique_id <- function(x) {
>    for (i in seq_along(x)) {
>      x[i] <- paste0("<", paste(sample(letters, 10), collapse = ""), ">",
> x[i])
>    }
>    x
> }
>
> list_regexes <- sample(letters, 128, TRUE) # <<<<<<<<<<< change this to
>                                             #             127 and it works
> regex2 <- append_unique_id(list_regexes)
> regex2 <- paste0("(?", regex2, ")")
> regex2 <- paste(regex2, collapse = "|")
>
> out <- gregexpr(regex2, "Cyprus", perl = TRUE, ignore.case = TRUE)
> #> Error in gregexpr(regex2, "Cyprus", perl = TRUE, ignore.case = TRUE):
> attempt to set index -129/128 in SET_STRING_ELT
>
> I think the bug is in R, here:
> https://github.com/wch/r-source/blob/57d15d68235dd9bcfaa51fce83aaa71163a020e1/src/main/grep.c#L3079
>
> This is the line
>
>             int capture_num = (entry[0]<<8) + entry[1] - 1;
>
> where entry is declared as a pointer to a char.  What this is doing is
> extracting a 16 bit number from the first two bytes of a character
> string holding the name of the capture group.  Since char is a signed
> type, the conversion of bytes to integer gets messed up and the value
> comes out wrong.
>
> Duncan Murdoch
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Fri Sep  8 15:44:48 2023
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Fri, 8 Sep 2023 15:44:48 +0200
Subject: [Rd] FYI: daily R source tarballs from ETH: *.xz instead of *.bz2)
Message-ID: <25851.9552.398085.384361@stat.math.ethz.ch>

A quick notice for anyone who uses cron-like scripts to get
R source tarballs from the ETH  R/daily/ s:

I've finally switched to replace *.bz2 by *.xz which does save
quite a bit of bandwidth.

Currently, you can see the 2 day old *.bz2 (and their sizes) and
compare with the new  *.xz one  (sorted newest first):

  https://stat.ethz.ch/R/daily/?C=M;O=D


Best,
Martin


From hp@ge@@on@g|thub @end|ng |rom gm@||@com  Sat Sep  9 02:56:20 2023
From: hp@ge@@on@g|thub @end|ng |rom gm@||@com (=?UTF-8?B?SGVydsOpIFBhZ8Oocw==?=)
Date: Fri, 8 Sep 2023 17:56:20 -0700
Subject: [Rd] 
 FYI: daily R source tarballs from ETH: *.xz instead of *.bz2)
In-Reply-To: <25851.9552.398085.384361@stat.math.ethz.ch>
References: <25851.9552.398085.384361@stat.math.ethz.ch>
Message-ID: <27378572-a2e2-eb29-1e7d-9c8d7020bd3a@gmail.com>

Hi Martin,

Sounds good. Are there any plans to support the xz compression for 
package source tarballs?

Thanks,

H.

On 9/8/23 06:44, Martin Maechler wrote:
> A quick notice for anyone who uses cron-like scripts to get
> R source tarballs from the ETH  R/daily/ s:
>
> I've finally switched to replace *.bz2 by *.xz which does save
> quite a bit of bandwidth.
>
> Currently, you can see the 2 day old *.bz2 (and their sizes) and
> compare with the new  *.xz one  (sorted newest first):
>
>    https://stat.ethz.ch/R/daily/?C=M;O=D
>
>
> Best,
> Martin
>
> ______________________________________________
> R-devel at r-project.org  mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
Herv? Pag?s

Bioconductor Core Team
hpages.on.github at gmail.com

	[[alternative HTML version deleted]]


From trevor@|@d@v|@ @end|ng |rom gm@||@com  Mon Sep 11 18:29:26 2023
From: trevor@|@d@v|@ @end|ng |rom gm@||@com (Trevor Davis)
Date: Mon, 11 Sep 2023 09:29:26 -0700
Subject: [Rd] List's `[[` method with "" tag
Message-ID: <CAMigB8FTRq5BeY8nHfukwyk=kBNQ1k6EHfsKRv6EEZytLC9=og@mail.gmail.com>

Hi,

I notice that one can assign a variable to an R list by using an empty
string key but one cannot get that variable back from the list by using the
empty string key:

```r
l <- list()
l[[""]] <- "An empty string as list key"
names(l)
l[[""]] # Returns a `NULL`
l[[names(l) == ""]] # Returns first value with `""` tag
```

Instead of `l[[""]]` returning a `NULL` I'd "expect" it to instead return
the first variable named `""`  i.e. in this case "An empty string as list
key".  It would be nice if the `[[` method of a list was updated to "fix"
this.

Additionally, I observe that if a list is named but has certain elements
without names then those are currently "named" `""`:

```r
names(list(a = 1, 2, c = 3, 4))
```

This latter change may be a breaking change but I speculate that perhaps it
may be more intuitive if missing names were indicated with `NA_character_`
instead of `""`.

Thanks,

Trevor

	[[alternative HTML version deleted]]


From @oko| @end|ng |rom |n@@-tou|ou@e@|r  Mon Sep 11 18:52:51 2023
From: @oko| @end|ng |rom |n@@-tou|ou@e@|r (=?UTF-8?Q?Sergue=C3=AF_Sokol?=)
Date: Mon, 11 Sep 2023 18:52:51 +0200
Subject: [Rd] List's `[[` method with "" tag
In-Reply-To: <CAMigB8FTRq5BeY8nHfukwyk=kBNQ1k6EHfsKRv6EEZytLC9=og@mail.gmail.com>
References: <CAMigB8FTRq5BeY8nHfukwyk=kBNQ1k6EHfsKRv6EEZytLC9=og@mail.gmail.com>
Message-ID: <fdf54c1464d0e0d9a45fae138c050ef1@198c5bc6.internal>

It's per design: 

?names
...
The name '""' is special: it is used to
indicate that there is no
name associated with an element of a (atomic
or generic) vector.
Subscripting by '""' will match nothing (not even
elements which
have no name).
... 

Best,
Serguei. 

Le 2023-09-11
18:29, Trevor Davis a ?crit :

> Hi,
> 
> I notice that one can assign a
variable to an R list by using an empty
> string key but one cannot get
that variable back from the list by using the
> empty string key:
> 
>
```r
> l <- list()
> l[[""]] <- "An empty string as list key"
>
names(l)
> l[[""]] # Returns a `NULL`
> l[[names(l) == ""]] # Returns
first value with `""` tag
> ```
> 
> Instead of `l[[""]]` returning a
`NULL` I'd "expect" it to instead return
> the first variable named `""`
i.e. in this case "An empty string as list
> key".  It would be nice if
the `[[` method of a list was updated to "fix"
> this.
> 
>
Additionally, I observe that if a list is named but has certain
elements
> without names then those are currently "named" `""`:
> 
>
```r
> names(list(a = 1, 2, c = 3, 4))
> ```
> 
> This latter change may
be a breaking change but I speculate that perhaps it
> may be more
intuitive if missing names were indicated with `NA_character_`
> instead
of `""`.
> 
> Thanks,
> 
> Trevor
> 
> [[alternative HTML version
deleted]]
> 
> ______________________________________________
>
R-devel at r-project.org mailing list
>
https://stat.ethz.ch/mailman/listinfo/r-devel
 
	[[alternative HTML version deleted]]


From r|p|ey @end|ng |rom @t@t@@ox@@c@uk  Tue Sep 12 07:39:47 2023
From: r|p|ey @end|ng |rom @t@t@@ox@@c@uk (Prof Brian Ripley)
Date: Tue, 12 Sep 2023 06:39:47 +0100
Subject: [Rd] 
 FYI: daily R source tarballs from ETH: *.xz instead of *.bz2)
In-Reply-To: <27378572-a2e2-eb29-1e7d-9c8d7020bd3a@gmail.com>
References: <25851.9552.398085.384361@stat.math.ethz.ch>
 <27378572-a2e2-eb29-1e7d-9c8d7020bd3a@gmail.com>
Message-ID: <f84b708a-6ddd-4c02-a97c-8a1331768444@stats.ox.ac.uk>

On 09/09/2023 01:56, Herv? Pag?s wrote:
> Hi Martin,
> 
> Sounds good. Are there any plans to support the xz compression for
> package source tarballs?

What makes you think it is not supported?

R CMD INSTALL happily installs .tar.xz files, and the name is not used 
to detect compression so .tar.gz files could be bzip2- or xz-compressed.

Note that tarball compression is pretty much irrelevant where the 
tarball contains large compressed files, for example .rda files or 
vendor.tar.xz files of Rust sources.  You have to arrange that the first 
compression is the bast possible.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Emeritus Professor of Applied Statistics, University of Oxford


From hp@ge@@on@g|thub @end|ng |rom gm@||@com  Wed Sep 13 02:05:25 2023
From: hp@ge@@on@g|thub @end|ng |rom gm@||@com (=?UTF-8?B?SGVydsOpIFBhZ8Oocw==?=)
Date: Tue, 12 Sep 2023 17:05:25 -0700
Subject: [Rd] 
 FYI: daily R source tarballs from ETH: *.xz instead of *.bz2)
In-Reply-To: <f84b708a-6ddd-4c02-a97c-8a1331768444@stats.ox.ac.uk>
References: <25851.9552.398085.384361@stat.math.ethz.ch>
 <27378572-a2e2-eb29-1e7d-9c8d7020bd3a@gmail.com>
 <f84b708a-6ddd-4c02-a97c-8a1331768444@stats.ox.ac.uk>
Message-ID: <c31fd49d-3249-881b-64e5-a6ad9895be4e@gmail.com>

On 9/11/23 22:39, Prof Brian Ripley wrote:

> On 09/09/2023 01:56, Herv? Pag?s wrote:
>> Hi Martin,
>>
>> Sounds good. Are there any plans to support the xz compression for
>> package source tarballs?
>
> What makes you think it is not supported?

I guess because I've never seen source tarballs distributed as .xz files 
but it's good to know that 'R CMD build' and 'R CMD INSTALL' support that.

So let me reformulate my question: do CRAN have any plans to switch from 
.tar.gz to .xz for the distribution of source tarballs? Is this 
something that tools like write_PACKAGES(), available.packages(), and 
install.packages() would be able to handle? Would they be able to handle 
a mix of .tar.gz and .xz packages? (Which would be important for a 
smooth transition from .tar.gz to .xz across CRAN/Bioconductor.)

I'm just trying to get a sense if the effort to reduce bandwidth will go 
beyond the distribution of R source snapshots.

Thanks,

H.

-- 
Herv? Pag?s

Bioconductor Core Team
hpages.on.github at gmail.com

	[[alternative HTML version deleted]]


From m@rchywk@ @end|ng |rom hotm@||@com  Thu Sep 14 15:22:51 2023
From: m@rchywk@ @end|ng |rom hotm@||@com (Mike Marchywka)
Date: Thu, 14 Sep 2023 13:22:51 +0000
Subject: [Rd] R packages to send plottable data to external apps
In-Reply-To: <CALEXWq15oiL58nitHfNvKH00Chr07DeoMpiFpyXdqwbcxV_rZg@mail.gmail.com>
References: <BL3PR11MB6338324A0CEDD90C3DDD013CBEE1A@BL3PR11MB6338.namprd11.prod.outlook.com>
 <CALEXWq15oiL58nitHfNvKH00Chr07DeoMpiFpyXdqwbcxV_rZg@mail.gmail.com>
Message-ID: <BL3PR11MB6338D11AC8641C479D04F51EBEF7A@BL3PR11MB6338.namprd11.prod.outlook.com>

I put this up on github in current form where it sort of
works and I can use it for my needs but if anyone 
thinks it fills a niche I guess I could clean it up.

https://github.com/mmarchywka/mjmdatascope

Definitely not ready for users but maybe a deverloper.
In terms of the "Trend" package you mentioned 
maybe one interface would be to emulate that action
too as that was my original interest circa 2007 lol. 

fwiw.
Thanks. 


?Mike Marchywka?
44 Crosscreek Trail
Jasper GA 30143
was 306 Charles Cox Drive? Canton, GA 30115
470-758-0799
404-788-1216?




________________________________________
From: I?aki Ucar <iucar at fedoraproject.org>
Sent: Sunday, August 27, 2023 7:12 PM
To: Mike Marchywka
Cc: r-devel; R Package Development
Subject: Re: [Rd] R packages to send plottable data to external apps

I think r-package-devel is a better place for this. CC'ing there.

On Sun, 27 Aug 2023 at 23:50, Mike Marchywka <marchywka at hotmail.com> wrote:
>
> I was curious what R packages, or indeed any other applications, exist
> to plot streamed data from arbitrary data generators. It need not
> be publication quality plotting but it should be easy to use  like
> an oscilloscope.

The last time I checked, there wasn't any R package suitable for
plotting high-throughput streaming data.

There's a nice command-line utility called trend [1] that I
extensively used in the past as an oscilloscope to visualize the
output from a DAQ card. I don't see any new development there, but it
does exactly what it promises; it's easy to use, quite configurable
and very fast. Old but gold.

I also explored VisPy, which is much more ambitious, but at that time
the API had a limitation that didn't allow me to achieve what I
required, and I haven't looked at it ever since, but the project seems
in good shape.

[1] https://www.thregr.org/wavexx/software/trend/
[2] https://vispy.org/

Hope it helps,
I?aki

> I was working on something called datascope that I
> am using for 1D finite difference monitoring and recently interfaced it
> to freefem. I also created an R package. If there is any interest in something
> like this I guess I could put it up somewhere when it is more usable
> or if you can suggest some similar popular packages that would be good
> too. Is there something I could drop-in to the attached code and get
> something like the attached output that could also be switched to other
> data sources?  This right now works via linux fifo and somewhat by UDP.
> It can queue data and stop making it if no one seems to be  consuming
> it depending on the channel.
>
> Thanks.
>
>  Mike Marchywka
> 44 Crosscreek Trail
> Jasper GA 30143
> was 306 Charles Cox Drive  Canton, GA 30115
> 470-758-0799
> 404-788-1216
>
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel



--
I?aki ?car


From kry|ov@r00t @end|ng |rom gm@||@com  Mon Sep 18 23:33:56 2023
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Tue, 19 Sep 2023 00:33:56 +0300
Subject: [Rd] On PRINTNAME() encoding, EncodeChar(),
 and being painted into a corner
Message-ID: <20230919003356.13a97d6e@Tarkus>

Hello R-devel,

I have originally learned about this from the following GitHub issue:
<https://github.com/r-devel/r-project-sprint-2023/issues/65>. In short,
in various places of the R source code, symbol names are accessed using
translateChar(), EncodeChar(), and CHAR(), and it might help to unify
their use.

Currently, R is very careful to only create symbols with names in the
native encoding. I have verified this by tracing the ways a symbol can
be created (allocSExp) or have a name assigned (SET_PRINTNAME) using
static analysis (Coccinelle). While it's possible to create a symbol
with a name in an encoding different from the native encoding using
SET_PRINTNAME(symbol, mkCharCE(...)), neither R nor CRAN packages
invoke code like this for an arbitrary encoding; symbols are always
created using either install() or installTrChar(). (install("invalid
byte sequence") is, of course, still possible, but is a different
problem.)

This means that translateChar(PRINTNAME(...)) is currently unnecessary,
but it may be worth adding a check (opt-in, applicable only during R
CMD check, to avoid a performance hit?) to SET_PRINTNAME() to ensure
that only native-encoding (or ASCII) symbol names are used. I could also
suggest a patch for Writing R Extensions or R Internals to document this
assumption.

The following translateChar() doesn't hurt (it returns CHAR(x) right
away without allocating any memory), but it stands out against most
uses of CHAR(PRINTNAME(.)) and EncodeChar(PRINTNAME(.)):

--- src/main/subscript.c	(revision 85160)
+++ src/main/subscript.c	(working copy)
@@ -186,7 +186,7 @@
 	    PROTECT(names);
 	    for (i = 0; i < nx; i++)
 		if (streql(translateChar(STRING_ELT(names, i)),
-			   translateChar(PRINTNAME(s)))) {
+			   CHAR(PRINTNAME(s)))) {
 		    indx = i;
 		    break;
 		}

The following translateChar() can be safely replaced with EncodeChar(),
correctly printing funnily-named functions in tracemem() reports:

--- src/main/debug.c	(revision 85160)
+++ src/main/debug.c	(working copy)
@@ -203,7 +203,7 @@
 	    && TYPEOF(cptr->call) == LANGSXP) {
 	    SEXP fun = CAR(cptr->call);
 	    Rprintf("%s ",
-		    TYPEOF(fun) == SYMSXP ? translateChar(PRINTNAME(fun)) :
+		    TYPEOF(fun) == SYMSXP ? EncodeChar(PRINTNAME(fun)) : "<Anonymous>");
 	}
     }

tracemem(a <- 1:10)
`\r\v\t\n` <- function(x) x[1] <- 0
`\r\v\t\n`(a)
# Now correctly prints:
# tracemem[0x55fd11e61e00 -> 0x55fd1081d2a8]: \r\v\t\n
# tracemem[0x55fd1081d2a8 -> 0x55fd113277e8]: \r\v\t\n

What about EncodeChar(PRINTNAME(.))? This is the intended way to report
symbols in error messages. Without EncodeChar(),
.Internal(`\r\v\t\n`()) actually prints the newlines to standard output
as part of the error message instead of escaping them. Unfortunately,
EncodeChar() uses a statically-allocated buffer for its return value,
*and* the comments say that it's unsafe to use together with
errorcall(): errorcall_cpy() must be used instead. I think that's
because EncodeChar() may be called while deparsing the call,
overwriting the statically-allocated buffer before the format arguments
(which also contain the return value of EncodeChar()) are processed. In
particular, this means that EncodeChar() is unsafe to use with any kind
of warnings. The following Coccinelle script locates uses of
CHAR(PRINTNAME(.)) inside errors and warnings:

@@
expression x;
expression list arg1, arg2;
identifier fun =~ "(Rf_)?(error|warning)(call)?(_cpy)?";
@@
 fun(
  arg1,
* CHAR(PRINTNAME(x)),
  arg2
 )

Some of these, which already use errorcall(), are trivial to fix by
replacing CHAR() with EncodeChar() and upgrading errorcall() to
errorcall_cpy():

--- src/main/names.c
+++ src/main/names.c
@@ -1367,7 +1367,7 @@ attribute_hidden SEXP do_internal(SEXP c
 	errorcall(call, _("invalid .Internal() argument"));
     if (INTERNAL(fun) == R_NilValue)
-	errorcall(call, _("there is no .Internal function '%s'"),
+	errorcall_cpy(call, _("there is no .Internal function '%s'"),
-		  CHAR(PRINTNAME(fun)));
+		  EncodeChar(PRINTNAME(fun)));
 
 #ifdef CHECK_INTERNALS
     if(R_Is_Running > 1 && getenv("_R_CHECK_INTERNALS2_")) {

--- src/main/eval.c
+++ src/main/eval.c
@@ -1161,7 +1161,7 @@ SEXP eval(SEXP e, SEXP rho)
 	    const char *n = CHAR(PRINTNAME(e));
-	    if(*n) errorcall(getLexicalCall(rho),
+	    if(*n) errorcall_cpy(getLexicalCall(rho),
 			     _("argument \"%s\" is missing, with no default"),
-			     CHAR(PRINTNAME(e)));
+			     EncodeChar(PRINTNAME(e)));
 	    else errorcall(getLexicalCall(rho),
 			   _("argument is missing, with no default"));
 	}

--- src/main/match.c
+++ src/main/match.c
@@ -229,7 +229,7 @@ attribute_hidden SEXP matchArgs_NR(SEXP
 		      if (fargused[arg_i] == 2)
-			  errorcall(call,
+			  errorcall_cpy(call,
 	                      _("formal argument \"%s\" matched by multiple actual arguments"),
-	                      CHAR(PRINTNAME(TAG(f))));
+	                      EncodeChar(PRINTNAME(TAG(f))));
 		      if (ARGUSED(b) == 2)
 			  errorcall(call,
 	                      _("argument %d matches multiple formal arguments"),
@@ -272,12 +271,12 @@ attribute_hidden SEXP matchArgs_NR(SEXP
 			if (fargused[arg_i] == 1)
-			    errorcall(call,
+			    errorcall_cpy(call,
 				_("formal argument \"%s\" matched by multiple actual arguments"),
-				CHAR(PRINTNAME(TAG(f))));
+				EncodeChar(PRINTNAME(TAG(f))));
 			if (R_warn_partial_match_args) {
 			    warningcall(call,
 					_("partial argument match of '%s' to '%s'"), CHAR(PRINTNAME(TAG(b))),
 					CHAR(PRINTNAME(TAG(f))) );
 			}
 			SETCAR(a, CAR(b));
 			if (CAR(b) != R_MissingArg) SET_MISSING(a, 0);

The changes become more complicated with a plain error() (have to
figure out the current call and provide it to errorcall_cpy), still
more complicated with warnings (there's currently no warningcall_cpy(),
though one can be implemented) and even more complicated when multiple
symbols are used in the same warning or error, like in the last
warningcall() above (EncodeChar() can only be called once at a time).

The only solution to the latter problem is an EncodeChar() variant that
allocates its memory dynamically. Would R_alloc() be acceptable in this
context? With errors, the allocation stack would be quickly reset
(except when withCallingHandlers() is in effect?), but with warnings,
the code would have to restore it manually every time. Is it even worth
the effort to try to handle the (pretty rare) non-syntactic symbol names
while constructing error messages? Other languages (like Lua or SQLite)
provide a special printf specifier (typically %q) to create
quoted/escaped string representations, but we're not yet at the point
of providing a C-level printf implementation.

-- 
Best regards,
Ivan


From murdoch@dunc@n @end|ng |rom gm@||@com  Tue Sep 19 16:44:07 2023
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Tue, 19 Sep 2023 10:44:07 -0400
Subject: [Rd] Strange behaviour of do.call()
Message-ID: <a20d1359-7097-42a9-903e-2b505bdb3343@gmail.com>

The knitr::kable() function does some internal setup, including 
determining the target format, and then calls an internal function using

   do.call(paste("kable", format, sep = "_"), list(x = x,
         caption = caption, escape = escape, ...))

I was interested in setting the `vlign` argument to knitr:::kable_latex, 
using this code:

   knitr::kable(head(mtcars), format="latex", align = "c", vlign="")

If I debug knitr::kable, I can see that `vlign = ""` is part of 
list(...).  However, if I debug knitr:::kable_latex, I get weird results:

   > debug(knitr:::kable_latex)
   > knitr::kable(head(mtcars), format="latex", align = "c", vlign="")
   debugging in: kable_latex(x = c("Mazda RX4", "Mazda RX4 Wag", "Datsun 
710",
   "Hornet 4 Drive", "Hornet Sportabout", "Valiant", "21.0", "21.0",
   "22.8", "21.4", "18.7", "18.1", "6", "6", "4", "6", "8", "6",
   "160", "160", "108", "258", "360", "225", "110", "110", "93",
   "110", "175", "105", "3.90", "3.90", "3.85", "3.08", "3.15",
   "2.76", "2.620", "2.875", "2.320", "3.215", "3.440", "3.460",
   "16.46", "17.02", "18.61", "19.44", "17.02", "20.22", "0", "0",
   "1", "1", "0", "1", "1", "1", "1", "0", "0", "0", "4", "4", "4",
   "3", "3", "3", "4", "4", "1", "1", "2", "1"), caption = NULL,
       escape = TRUE, vlign = "")
debug: {

   [rest of function display omitted]

I see here that vlign = "" is being shown as an argument.  However, when 
I print vlign, sometimes I get "object not found", and somethings I get

   Browse[2]> vline
   debug: [1] "|"

(which is what the default value would be).  In the latter case, I also see

   Browse[2]> list(...)
   $vlign
   [1] ""

i.e. vlign remains part of the ... list, it wasn't bound to the argument 
named vlign.

I can't spot anything particularly strange in the way knitr is handling 
this; can anyone else?  My sessionInfo() is below.

Duncan Murdoch

 > sessionInfo()
R version 4.3.1 (2023-06-16)
Platform: x86_64-apple-darwin20 (64-bit)
Running under: macOS Monterey 12.6.9

Matrix products: default
BLAS: 
/Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRblas.0.dylib 

LAPACK: 
/Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRlapack.dylib; 
  LAPACK version 3.11.0

locale:
[1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8

time zone: America/Toronto
tzcode source: internal

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

loaded via a namespace (and not attached):
[1] compiler_4.3.1 tools_4.3.1    knitr_1.44     xfun_0.40


From murdoch@dunc@n @end|ng |rom gm@||@com  Tue Sep 19 16:57:59 2023
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Tue, 19 Sep 2023 10:57:59 -0400
Subject: [Rd] Strange behaviour of do.call()
In-Reply-To: <a20d1359-7097-42a9-903e-2b505bdb3343@gmail.com>
References: <a20d1359-7097-42a9-903e-2b505bdb3343@gmail.com>
Message-ID: <9b386333-6b31-4e7d-8a15-678ed1c8bef6@gmail.com>

Sorry, it's a silly thinko.  I misspelled the vline argument.  Thanks 
Ivan for the gentle nudge!

Duncan Murdoch

On 19/09/2023 10:44 a.m., Duncan Murdoch wrote:
> The knitr::kable() function does some internal setup, including
> determining the target format, and then calls an internal function using
> 
>     do.call(paste("kable", format, sep = "_"), list(x = x,
>           caption = caption, escape = escape, ...))
> 
> I was interested in setting the `vlign` argument to knitr:::kable_latex,
> using this code:
> 
>     knitr::kable(head(mtcars), format="latex", align = "c", vlign="")
> 
> If I debug knitr::kable, I can see that `vlign = ""` is part of
> list(...).  However, if I debug knitr:::kable_latex, I get weird results:
> 
>     > debug(knitr:::kable_latex)
>     > knitr::kable(head(mtcars), format="latex", align = "c", vlign="")
>     debugging in: kable_latex(x = c("Mazda RX4", "Mazda RX4 Wag", "Datsun
> 710",
>     "Hornet 4 Drive", "Hornet Sportabout", "Valiant", "21.0", "21.0",
>     "22.8", "21.4", "18.7", "18.1", "6", "6", "4", "6", "8", "6",
>     "160", "160", "108", "258", "360", "225", "110", "110", "93",
>     "110", "175", "105", "3.90", "3.90", "3.85", "3.08", "3.15",
>     "2.76", "2.620", "2.875", "2.320", "3.215", "3.440", "3.460",
>     "16.46", "17.02", "18.61", "19.44", "17.02", "20.22", "0", "0",
>     "1", "1", "0", "1", "1", "1", "1", "0", "0", "0", "4", "4", "4",
>     "3", "3", "3", "4", "4", "1", "1", "2", "1"), caption = NULL,
>         escape = TRUE, vlign = "")
> debug: {
> 
>     [rest of function display omitted]
> 
> I see here that vlign = "" is being shown as an argument.  However, when
> I print vlign, sometimes I get "object not found", and somethings I get
> 
>     Browse[2]> vline
>     debug: [1] "|"
> 
> (which is what the default value would be).  In the latter case, I also see
> 
>     Browse[2]> list(...)
>     $vlign
>     [1] ""
> 
> i.e. vlign remains part of the ... list, it wasn't bound to the argument
> named vlign.
> 
> I can't spot anything particularly strange in the way knitr is handling
> this; can anyone else?  My sessionInfo() is below.
> 
> Duncan Murdoch
> 
>   > sessionInfo()
> R version 4.3.1 (2023-06-16)
> Platform: x86_64-apple-darwin20 (64-bit)
> Running under: macOS Monterey 12.6.9
> 
> Matrix products: default
> BLAS:
> /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRblas.0.dylib
> 
> LAPACK:
> /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRlapack.dylib;
>    LAPACK version 3.11.0
> 
> locale:
> [1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8
> 
> time zone: America/Toronto
> tzcode source: internal
> 
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
> 
> loaded via a namespace (and not attached):
> [1] compiler_4.3.1 tools_4.3.1    knitr_1.44     xfun_0.40


From @oko| @end|ng |rom |n@@-tou|ou@e@|r  Tue Sep 19 17:05:57 2023
From: @oko| @end|ng |rom |n@@-tou|ou@e@|r (Serguei Sokol)
Date: Tue, 19 Sep 2023 17:05:57 +0200
Subject: [Rd] Strange behaviour of do.call()
In-Reply-To: <a20d1359-7097-42a9-903e-2b505bdb3343@gmail.com>
References: <a20d1359-7097-42a9-903e-2b505bdb3343@gmail.com>
Message-ID: <6c558a4d-d398-28ef-94ed-ab4ee4a639ef@insa-toulouse.fr>

Le 19/09/2023 ? 16:44, Duncan Murdoch a ?crit?:
> The knitr::kable() function does some internal setup, including 
> determining the target format, and then calls an internal function using
>
> ? do.call(paste("kable", format, sep = "_"), list(x = x,
> ??????? caption = caption, escape = escape, ...))
>
> I was interested in setting the `vlign` argument to 
> knitr:::kable_latex, using this code:
>
> ? knitr::kable(head(mtcars), format="latex", align = "c", vlign="")
>
> If I debug knitr::kable, I can see that `vlign = ""` is part of 
> list(...).? However, if I debug knitr:::kable_latex, I get weird results:
>
> ? > debug(knitr:::kable_latex)
> ? > knitr::kable(head(mtcars), format="latex", align = "c", vlign="")
If I do this in my R v4.3.1 on linux, I get:

 > debug(knitr:::kable_latex)
 > knitr::kable(head(mtcars), format="latex", align = "c", vlign="")
Error in kable_latex(x = c("Mazda RX4", "Mazda RX4 Wag", "Datsun 710",? :
 ? unused argument (vlign = "")

By looking at args(knitr:::kable_latex), I see 2 similar arguments 
'valign' and 'vline' but no 'vlign'.
Can it be just a typo in your code?


> debugging in: kable_latex(x = c("Mazda RX4", "Mazda RX4 Wag", "Datsun 
> 710",
> ? "Hornet 4 Drive", "Hornet Sportabout", "Valiant", "21.0", "21.0",
> ? "22.8", "21.4", "18.7", "18.1", "6", "6", "4", "6", "8", "6",
> ? "160", "160", "108", "258", "360", "225", "110", "110", "93",
> ? "110", "175", "105", "3.90", "3.90", "3.85", "3.08", "3.15",
> ? "2.76", "2.620", "2.875", "2.320", "3.215", "3.440", "3.460",
> ? "16.46", "17.02", "18.61", "19.44", "17.02", "20.22", "0", "0",
> ? "1", "1", "0", "1", "1", "1", "1", "0", "0", "0", "4", "4", "4",
> ? "3", "3", "3", "4", "4", "1", "1", "2", "1"), caption = NULL,
> ????? escape = TRUE, vlign = "")
> debug: {
>
> ? [rest of function display omitted]
>
> I see here that vlign = "" is being shown as an argument. However, 
> when I print vlign, sometimes I get "object not found", and somethings 
> I get
>
> ? Browse[2]> vline
> ? debug: [1] "|"
Here again, 'vline' is used on purpose instead of 'vlign'?

Best,
Serguei.

>
> (which is what the default value would be).? In the latter case, I 
> also see
>
> ? Browse[2]> list(...)
> ? $vlign
> ? [1] ""
>
> i.e. vlign remains part of the ... list, it wasn't bound to the 
> argument named vlign.
>
> I can't spot anything particularly strange in the way knitr is 
> handling this; can anyone else?? My sessionInfo() is below.
>
> Duncan Murdoch
>
> > sessionInfo()
> R version 4.3.1 (2023-06-16)
> Platform: x86_64-apple-darwin20 (64-bit)
> Running under: macOS Monterey 12.6.9
>
> Matrix products: default
> BLAS: 
> /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRblas.0.dylib 
>
> LAPACK: 
> /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRlapack.dylib; 
> ?LAPACK version 3.11.0
>
> locale:
> [1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8
>
> time zone: America/Toronto
> tzcode source: internal
>
> attached base packages:
> [1] stats???? graphics? grDevices utils???? datasets? methods base
>
> loaded via a namespace (and not attached):
> [1] compiler_4.3.1 tools_4.3.1??? knitr_1.44???? xfun_0.40
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


-- 
Serguei Sokol
Ingenieur de recherche INRAE

Cellule Math?matiques
TBI, INSA/INRAE UMR 792, INSA/CNRS UMR 5504
135 Avenue de Rangueil
31077 Toulouse Cedex 04

tel: +33 5 61 55 98 49
email: sokol at insa-toulouse.fr
https://www.toulouse-biotechnology-institute.fr/en/plateformes-plateaux/cellule-mathematiques/


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Wed Sep 20 12:39:50 2023
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Wed, 20 Sep 2023 12:39:50 +0200
Subject: [Rd] proposal: 'dev.capabilities()' can also query Unicode
 capabilities of current graphics device
In-Reply-To: <CAMigB8EaoRNacP0Z4QUMXjegnks1jCOrx2eN5Z-EK63wtqsLmA@mail.gmail.com>
References: <CAMigB8EaoRNacP0Z4QUMXjegnks1jCOrx2eN5Z-EK63wtqsLmA@mail.gmail.com>
Message-ID: <25866.52214.850601.243688@stat.math.ethz.ch>

>>>>> Trevor Davis 
>>>>>     on Thu, 31 Aug 2023 13:49:03 -0700 writes:

    > Hi,

    > It would be nice if `grDevices::dev.capabilities()` could also be used to
    > query whether the current graphics device supports Unicode.  In such a case
    > I'd expect it to return `FALSE` if `pdf()` is the current graphics device
    > and something else for the Cairo or Quartz devices.

    > Thanks,
    > Trevor

I agree in principle that this would be useful new feature for
dev.capabilities()

However, pdf()   *does*  support  Unicode.

The problem is that some pdf *viewers*,
notably `evince` on Fedora Linux, for several years now,
do *not* show *some* of the UTF-8 glyphs because they do not use
the correct fonts {which *are* on the machine; good old `xpdf`
does in that case show the glyphs}.

Martin


From trevor@|@d@v|@ @end|ng |rom gm@||@com  Wed Sep 20 18:12:41 2023
From: trevor@|@d@v|@ @end|ng |rom gm@||@com (Trevor Davis)
Date: Wed, 20 Sep 2023 09:12:41 -0700
Subject: [Rd] proposal: 'dev.capabilities()' can also query Unicode
 capabilities of current graphics device
In-Reply-To: <25866.52214.850601.243688@stat.math.ethz.ch>
References: <CAMigB8EaoRNacP0Z4QUMXjegnks1jCOrx2eN5Z-EK63wtqsLmA@mail.gmail.com>
 <25866.52214.850601.243688@stat.math.ethz.ch>
Message-ID: <CAMigB8Fe_j+WBV=rhsiuJuF0WhvwVuQWBkgVhLVk9sJEOwRBMw@mail.gmail.com>

> However, pdf()   *does*  support  Unicode.

When I run a simple Unicode example like:

```
f <- tempfile(fileext = ".pdf")
pdf(f)
# U+2655 ? is found in most (all?) "sans" fonts like Arial, Dejavu Sans,
Arimo, etc.
# However, it is not in the Latin-1 encoding
grid::grid.text("\u2665")
dev.off()
```

I observe the following output:

```
Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y,
 :
  conversion failure on '?' in 'mbcsToSbcs': dot substituted for <e2>
Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y,
 :
  conversion failure on '?' in 'mbcsToSbcs': dot substituted for <99>
Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y,
 :
  conversion failure on '?' in 'mbcsToSbcs': dot substituted for <a5>
Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y,
 :
  conversion failure on '?' in 'mbcsToSbcs': dot substituted for <e2>
Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y,
 :
  conversion failure on '?' in 'mbcsToSbcs': dot substituted for <99>
Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y,
 :
  conversion failure on '?' in 'mbcsToSbcs': dot substituted for <a5>
```

When I open up the pdf file I just see three dots and not a heart as I
expected even if I open it up with `xpdf`.

In contrast the pdf generated by `cairo_pdf()` has a heart without
generating any warnings.

Avoiding such WARNINGs on certain CRAN check machines when I have a Unicode
graphics example that is worth including in a package's examples (if
protected by an appropriate if statement) is my main use case for such a
new feature.  However, a new feature like `dev.capabilities()$unicode`
could certainly return something more sophisticated than a crude `TRUE` and
`FALSE` to distinguish between levels of Unicode support provided by
different graphics devices.

Thanks,

Trevor

On Wed, Sep 20, 2023 at 3:39?AM Martin Maechler <maechler at stat.math.ethz.ch>
wrote:

> >>>>> Trevor Davis
> >>>>>     on Thu, 31 Aug 2023 13:49:03 -0700 writes:
>
>     > Hi,
>
>     > It would be nice if `grDevices::dev.capabilities()` could also be
> used to
>     > query whether the current graphics device supports Unicode.  In such
> a case
>     > I'd expect it to return `FALSE` if `pdf()` is the current graphics
> device
>     > and something else for the Cairo or Quartz devices.
>
>     > Thanks,
>     > Trevor
>
> I agree in principle that this would be useful new feature for
> dev.capabilities()
>
> However, pdf()   *does*  support  Unicode.
>
> The problem is that some pdf *viewers*,
> notably `evince` on Fedora Linux, for several years now,
> do *not* show *some* of the UTF-8 glyphs because they do not use
> the correct fonts {which *are* on the machine; good old `xpdf`
> does in that case show the glyphs}.
>
> Martin
>

	[[alternative HTML version deleted]]


From p@u| @end|ng |rom @t@t@@uck|@nd@@c@nz  Thu Sep 21 04:23:22 2023
From: p@u| @end|ng |rom @t@t@@uck|@nd@@c@nz (Paul Murrell)
Date: Thu, 21 Sep 2023 14:23:22 +1200
Subject: [Rd] proposal: 'dev.capabilities()' can also query Unicode
 capabilities of current graphics device
In-Reply-To: <CAMigB8Fe_j+WBV=rhsiuJuF0WhvwVuQWBkgVhLVk9sJEOwRBMw@mail.gmail.com>
References: <CAMigB8EaoRNacP0Z4QUMXjegnks1jCOrx2eN5Z-EK63wtqsLmA@mail.gmail.com>
 <25866.52214.850601.243688@stat.math.ethz.ch>
 <CAMigB8Fe_j+WBV=rhsiuJuF0WhvwVuQWBkgVhLVk9sJEOwRBMw@mail.gmail.com>
Message-ID: <08d246aa-f267-aab4-7de1-88a9e5b3c704@stat.auckland.ac.nz>

Hi

The problem is what "supports UNICODE" means.
Graphics devices have a 'hasTextUTF8' boolean to indicate that ...

     /* Some devices can plot UTF-8 text directly without converting
        to the native encoding, e.g. windows(), quartz() ....

        If this flag is true, all text *not in the symbol font* is sent
        in UTF8 to the textUTF8/strWidthUTF8 entry points.

... and this is TRUE for the pdf() device for example.
It is also TRUE for Cairo devices, but the support is quite different 
(as your examples demonstrate).
The Cairo devices do not alter UTF8 text at all, but the pdf() device 
attempts to convert to a single-byte representation, which of course 
will not always work.
The situation is only made more complex with the recent dev->glyph() 
support because that offers another possible route to producing generic 
UNICODE characters, including on pdf() devices.

Paul

On 21/09/23 04:12, Trevor Davis wrote:
>  > However, pdf() *does* support Unicode.
> 
> When I run a simple Unicode example like:
> 
> ```
> f <- tempfile(fileext = ".pdf")
> pdf(f)
> # U+2655 ? is found in most (all?) "sans" fonts like Arial, Dejavu Sans,
> Arimo, etc.
> # However, it is not in the Latin-1 encoding
> grid::grid.text("\u2665")
> dev.off()
> ```
> 
> I observe the following output:
> 
> ```
> Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y,
> :
> conversion failure on '?' in 'mbcsToSbcs': dot substituted for <e2>
> Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y,
> :
> conversion failure on '?' in 'mbcsToSbcs': dot substituted for <99>
> Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y,
> :
> conversion failure on '?' in 'mbcsToSbcs': dot substituted for <a5>
> Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y,
> :
> conversion failure on '?' in 'mbcsToSbcs': dot substituted for <e2>
> Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y,
> :
> conversion failure on '?' in 'mbcsToSbcs': dot substituted for <99>
> Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y,
> :
> conversion failure on '?' in 'mbcsToSbcs': dot substituted for <a5>
> ```
> 
> When I open up the pdf file I just see three dots and not a heart as I
> expected even if I open it up with `xpdf`.
> 
> In contrast the pdf generated by `cairo_pdf()` has a heart without
> generating any warnings.
> 
> Avoiding such WARNINGs on certain CRAN check machines when I have a Unicode
> graphics example that is worth including in a package's examples (if
> protected by an appropriate if statement) is my main use case for such a
> new feature. However, a new feature like `dev.capabilities()$unicode`
> could certainly return something more sophisticated than a crude `TRUE` and
> `FALSE` to distinguish between levels of Unicode support provided by
> different graphics devices.
> 
> Thanks,
> 
> Trevor
> 
> On Wed, Sep 20, 2023 at 3:39?AM Martin Maechler <maechler at stat.math.ethz.ch>
> wrote:
> 
>  > >>>>> Trevor Davis
>  > >>>>> on Thu, 31 Aug 2023 13:49:03 -0700 writes:
>  >
>  > > Hi,
>  >
>  > > It would be nice if `grDevices::dev.capabilities()` could also be
>  > used to
>  > > query whether the current graphics device supports Unicode. In such
>  > a case
>  > > I'd expect it to return `FALSE` if `pdf()` is the current graphics
>  > device
>  > > and something else for the Cairo or Quartz devices.
>  >
>  > > Thanks,
>  > > Trevor
>  >
>  > I agree in principle that this would be useful new feature for
>  > dev.capabilities()
>  >
>  > However, pdf() *does* support Unicode.
>  >
>  > The problem is that some pdf *viewers*,
>  > notably `evince` on Fedora Linux, for several years now,
>  > do *not* show *some* of the UTF-8 glyphs because they do not use
>  > the correct fonts {which *are* on the machine; good old `xpdf`
>  > does in that case show the glyphs}.
>  >
>  > Martin
>  >
> 
> [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel 
> <https://stat.ethz.ch/mailman/listinfo/r-devel>

-- 
Dr Paul Murrell
Te Kura Tatauranga | Department of Statistics
Waipapa Taumata Rau | The University of Auckland
Private Bag 92019, Auckland 1142, New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
www.stat.auckland.ac.nz/~paul/


From j@g@nmn2 @end|ng |rom gm@||@com  Thu Sep 21 06:47:39 2023
From: j@g@nmn2 @end|ng |rom gm@||@com (Mikael Jagan)
Date: Thu, 21 Sep 2023 00:47:39 -0400
Subject: [Rd] Recent changes to as.complex(NA_real_)
Message-ID: <1cf40db9-ea70-483a-a547-270da6926935@gmail.com>

Revisiting this thread from April:

     https://stat.ethz.ch/pipermail/r-devel/2023-April/082545.html

where the decision (not yet backported) was made for as.complex(NA_real_)
to give NA_complex_ instead of complex(r=NA_real_, i=0), to be consistent
with help("as.complex") and as.complex(NA) and as.complex(NA_integer_).

Was any consideration given to the alternative?  That is, to changing
as.complex(NA) and as.complex(NA_integer_) to give complex(r=NA_real_, i=0),
consistent with as.complex(NA_real_), then amending help("as.complex")
accordingly?

The principle that Im(as.complex(<real=(double|integer|logical)>)) should
be zero is quite fundamental, in my view, hence the "new" behaviour seems
to really violate the principle of least surprise ...

Another (but maybe weaker) argument is that double->complex coercions happen
more often than logical->complex and integer->complex ones.  Changing the
behaviour of the more frequently performed coercion is more likely to affect
code "out there".

Yet another argument is that one expects

     identical(as.complex(NA_real_), NA_real_ + (0+0i))

to be TRUE, i.e., that coercing from double to complex is equivalent to
adding a complex zero.  The new behaviour makes the above FALSE, since
NA_real_ + (0+0i) gives complex(r=NA_real_, i=0).

Having said that, one might also (but more naively) expect

     identical(as.complex(as.double(NA_complex_)), NA_complex_)

to be TRUE.  Under my proposal it continues to be FALSE.  Well, I'd prefer
if it gave FALSE with a warning "imaginary parts discarded in coercion",
but it seems that as.double(complex(r=a, i=b)) never warns when either of
'a' and 'b' is NA_real_ or NaN, even where "information" {nonzero 'b'} is
clearly lost ...

Whatever decision is made about as.complex(NA_real_), maybe these points
should be weighed before it becomes part of R-release ...

Mikael


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Fri Sep 22 12:38:57 2023
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Fri, 22 Sep 2023 12:38:57 +0200
Subject: [Rd] Recent changes to as.complex(NA_real_)
In-Reply-To: <1cf40db9-ea70-483a-a547-270da6926935@gmail.com>
References: <1cf40db9-ea70-483a-a547-270da6926935@gmail.com>
Message-ID: <25869.28353.595831.807741@stat.math.ethz.ch>

>>>>> Mikael Jagan 
>>>>>     on Thu, 21 Sep 2023 00:47:39 -0400 writes:

    > Revisiting this thread from April:

    >      https://stat.ethz.ch/pipermail/r-devel/2023-April/082545.html

    > where the decision (not yet backported) was made for
    > as.complex(NA_real_) to give NA_complex_ instead of
    > complex(r=NA_real_, i=0), to be consistent with
    > help("as.complex") and as.complex(NA) and as.complex(NA_integer_).

    > Was any consideration given to the alternative?  
    > That is, to changing as.complex(NA) and as.complex(NA_integer_) to
    > give complex(r=NA_real_, i=0), consistent with
    > as.complex(NA_real_), then amending help("as.complex")
    > accordingly?

Hmm, as, from R-core, mostly I was involved, I admit to say "no",
to my knowledge the (above) alternative wasn't considered.

  > The principle that
  > Im(as.complex(<real=(double|integer|logical)>)) should be zero
  > is quite fundamental, in my view, hence the "new" behaviour 
  > seems to really violate the principle of least surprise ...

of course "least surprise"  is somewhat subjective.  Still,
I clearly agree that the above would be one desirable property.

I think that any solution will lead to *some* surprise for some
cases, I think primarily because there are *many* different
values z  for which  is.na(z)  is true,  and in any case
NA_complex_  is only of the many.

I also agree with Mikael that we should reconsider the issue
that was raised by Davis Vaughan here ("on R-devel") last April.

    > Another (but maybe weaker) argument is that
    > double->complex coercions happen more often than
    > logical->complex and integer->complex ones.  Changing the
    > behaviour of the more frequently performed coercion is
    > more likely to affect code "out there".

    > Yet another argument is that one expects

    >      identical(as.complex(NA_real_), NA_real_ + (0+0i))

    > to be TRUE, i.e., that coercing from double to complex is
    > equivalent to adding a complex zero.  The new behaviour
    > makes the above FALSE, since NA_real_ + (0+0i) gives
    > complex(r=NA_real_, i=0).

No!  --- To my own surprise (!) --- in current R-devel the above is TRUE, 
and
      NA_real_ + (0+0i)  , the same as
      NA_real_ + 0i      , really gives  complex(r=NA, i=NA) :

Using showC() from ?complex

  showC <- function(z) noquote(sprintf("(R = %g, I = %g)", Re(z), Im(z)))

we see (in R-devel) quite consistently

> showC(NA_real_ + 0i)
[1] (R = NA, I = NA)
> showC(NA       + 0i)  # NA is 'logical'
[1] (R = NA, I = NA)
> 

where as in R 4.3.1 and "R-patched" -- *in*consistently

> showC(NA_real_ + 0i)
[1] (R = NA, I = 0)
> showC(NA + 0i)
[1] (R = NA, I = NA)
> 

.... and honestly, I do not see *where* (and when) we changed
the underlying code (in arithmetic.c !?)  in R-devel to *also*
produce  NA_complex_  in such complex *arithmetic*


    > Having said that, one might also (but more naively) expect

    >     identical(as.complex(as.double(NA_complex_)), NA_complex_)

    > to be TRUE.  

as in current R-devel

    > Under my proposal it continues to be FALSE.

as in "R-release"

    > Well, I'd prefer if it gave FALSE with a warning
    > "imaginary parts discarded in coercion", but it seems that
    > as.double(complex(r=a, i=b)) never warns when either of
    > 'a' and 'b' is NA_real_ or NaN, even where "information"
    > {nonzero 'b'} is clearly lost ...

The question of *warning* here is related indeed, but I think
we should try to look at it only *secondary* to your first
proposal.

    > Whatever decision is made about as.complex(NA_real_),
    > maybe these points should be weighed before it becomes part of
    > R-release ...

    > Mikael

Indeed.

Can we please get other opinions / ideas here?

Thank you in advance for your thoughts!
Martin

--- 

PS: 

 Our *print()*ing  of complex NA's ("NA" here meaning NA or NaN)
 is also unsatisfactory, e.g. in the case where all entries of a
 vector are NA in the sense of is.na(.), but their
 Re() and Im() are not all NA:
 
  showC <- function(z) noquote(sprintf("(R = %g, I = %g)", Re(z), Im(z)))
  z <- complex(, c(11, NA, NA), c(NA, 99, NA))
  z
  showC(z)

gives

  > z
  [1] NA NA NA
  > showC(z)
  [1] (R = 11, I = NA) (R = NA, I = 99) (R = NA, I = NA)

but that (printing of complex) *is* another issue,
in which we have the re-opened bugzilla PR#16752
    ==>   https://bugs.r-project.org/show_bug.cgi?id=16752

on which we also worked during the R Sprint in Warwick three
weeks ago, and where I want to commit changes in any case {but
think we should change even a bit more than we got to during the
Sprint}.


From j@g@nmn2 @end|ng |rom gm@||@com  Fri Sep 22 14:14:36 2023
From: j@g@nmn2 @end|ng |rom gm@||@com (Mikael Jagan)
Date: Fri, 22 Sep 2023 08:14:36 -0400
Subject: [Rd] Recent changes to as.complex(NA_real_)
In-Reply-To: <25869.28353.595831.807741@stat.math.ethz.ch>
References: <1cf40db9-ea70-483a-a547-270da6926935@gmail.com>
 <25869.28353.595831.807741@stat.math.ethz.ch>
Message-ID: <1aba0839-6412-4fb9-99ff-30118851cf72@gmail.com>



On 2023-09-22 6:38 am, Martin Maechler wrote:
>>>>>> Mikael Jagan
>>>>>>      on Thu, 21 Sep 2023 00:47:39 -0400 writes:
> 
>      > Revisiting this thread from April:
> 
>      >      https://stat.ethz.ch/pipermail/r-devel/2023-April/082545.html
> 
>      > where the decision (not yet backported) was made for
>      > as.complex(NA_real_) to give NA_complex_ instead of
>      > complex(r=NA_real_, i=0), to be consistent with
>      > help("as.complex") and as.complex(NA) and as.complex(NA_integer_).
> 
>      > Was any consideration given to the alternative?
>      > That is, to changing as.complex(NA) and as.complex(NA_integer_) to
>      > give complex(r=NA_real_, i=0), consistent with
>      > as.complex(NA_real_), then amending help("as.complex")
>      > accordingly?
> 
> Hmm, as, from R-core, mostly I was involved, I admit to say "no",
> to my knowledge the (above) alternative wasn't considered.
> 
>    > The principle that
>    > Im(as.complex(<real=(double|integer|logical)>)) should be zero
>    > is quite fundamental, in my view, hence the "new" behaviour
>    > seems to really violate the principle of least surprise ...
> 
> of course "least surprise"  is somewhat subjective.  Still,
> I clearly agree that the above would be one desirable property.
> 
> I think that any solution will lead to *some* surprise for some
> cases, I think primarily because there are *many* different
> values z  for which  is.na(z)  is true,  and in any case
> NA_complex_  is only of the many.
> 
> I also agree with Mikael that we should reconsider the issue
> that was raised by Davis Vaughan here ("on R-devel") last April.
> 
>      > Another (but maybe weaker) argument is that
>      > double->complex coercions happen more often than
>      > logical->complex and integer->complex ones.  Changing the
>      > behaviour of the more frequently performed coercion is
>      > more likely to affect code "out there".
> 
>      > Yet another argument is that one expects
> 
>      >      identical(as.complex(NA_real_), NA_real_ + (0+0i))
> 
>      > to be TRUE, i.e., that coercing from double to complex is
>      > equivalent to adding a complex zero.  The new behaviour
>      > makes the above FALSE, since NA_real_ + (0+0i) gives
>      > complex(r=NA_real_, i=0).
> 
> No!  --- To my own surprise (!) --- in current R-devel the above is TRUE,
> and
>        NA_real_ + (0+0i)  , the same as
>        NA_real_ + 0i      , really gives  complex(r=NA, i=NA) :
> 

Thank you for the correction - indeed, as.complex(NA_real_) and
NA_real_ + (0+0i) are identical in both R-patched and R-devel,
both giving complex(r=NA_real_, i=0) in R-patched and both giving
NA_complex_ in R-devel.  I was hallucating, it seems ...

> Using showC() from ?complex
> 
>    showC <- function(z) noquote(sprintf("(R = %g, I = %g)", Re(z), Im(z)))
> 
> we see (in R-devel) quite consistently
> 
>> showC(NA_real_ + 0i)
> [1] (R = NA, I = NA)
>> showC(NA       + 0i)  # NA is 'logical'
> [1] (R = NA, I = NA)
>>
> 
> where as in R 4.3.1 and "R-patched" -- *in*consistently
> 
>> showC(NA_real_ + 0i)
> [1] (R = NA, I = 0)
>> showC(NA + 0i)
> [1] (R = NA, I = NA)
>>
> 
> .... and honestly, I do not see *where* (and when) we changed
> the underlying code (in arithmetic.c !?)  in R-devel to *also*
> produce  NA_complex_  in such complex *arithmetic*
> 

R_binary() in arithmetic.c has always coerced REALSXP->CPLXSXP when
encountering one of each.  Surely then the changes in coerce.c are the
cause and this arithmetic behaviour is just a (bad, IMO) side effect?

> 
>      > Having said that, one might also (but more naively) expect
> 
>      >     identical(as.complex(as.double(NA_complex_)), NA_complex_)
> 
>      > to be TRUE.
> 
> as in current R-devel
> 
>      > Under my proposal it continues to be FALSE.
> 
> as in "R-release"
> 
>      > Well, I'd prefer if it gave FALSE with a warning
>      > "imaginary parts discarded in coercion", but it seems that
>      > as.double(complex(r=a, i=b)) never warns when either of
>      > 'a' and 'b' is NA_real_ or NaN, even where "information"
>      > {nonzero 'b'} is clearly lost ...
> 
> The question of *warning* here is related indeed, but I think
> we should try to look at it only *secondary* to your first
> proposal.
> 
>      > Whatever decision is made about as.complex(NA_real_),
>      > maybe these points should be weighed before it becomes part of
>      > R-release ...
> 
>      > Mikael
> 
> Indeed.
> 
> Can we please get other opinions / ideas here?
> 

Thank you, Martin, for "reopening".

Mikael

> Thank you in advance for your thoughts!
> Martin
> 
> ---
> 
> PS:
> 
>   Our *print()*ing  of complex NA's ("NA" here meaning NA or NaN)
>   is also unsatisfactory, e.g. in the case where all entries of a
>   vector are NA in the sense of is.na(.), but their
>   Re() and Im() are not all NA:
>   
>    showC <- function(z) noquote(sprintf("(R = %g, I = %g)", Re(z), Im(z)))
>    z <- complex(, c(11, NA, NA), c(NA, 99, NA))
>    z
>    showC(z)
> 
> gives
> 
>    > z
>    [1] NA NA NA
>    > showC(z)
>    [1] (R = 11, I = NA) (R = NA, I = 99) (R = NA, I = NA)
> 
> but that (printing of complex) *is* another issue,
> in which we have the re-opened bugzilla PR#16752
>      ==>   https://bugs.r-project.org/show_bug.cgi?id=16752
> 
> on which we also worked during the R Sprint in Warwick three
> weeks ago, and where I want to commit changes in any case {but
> think we should change even a bit more than we got to during the
> Sprint}.
>


From hp@ge@@on@g|thub @end|ng |rom gm@||@com  Fri Sep 22 20:41:57 2023
From: hp@ge@@on@g|thub @end|ng |rom gm@||@com (=?UTF-8?B?SGVydsOpIFBhZ8Oocw==?=)
Date: Fri, 22 Sep 2023 11:41:57 -0700
Subject: [Rd] Recent changes to as.complex(NA_real_)
In-Reply-To: <25869.28353.595831.807741@stat.math.ethz.ch>
References: <1cf40db9-ea70-483a-a547-270da6926935@gmail.com>
 <25869.28353.595831.807741@stat.math.ethz.ch>
Message-ID: <4abe101e-55be-8800-31ee-e0414c89740e@gmail.com>

We could also question the value of having an infinite number of NA 
representations in the complex space. For example all these complex 
values are displayed the same way (as NA), are considered NAs by 
is.na(), but are not identical or semantically equivalent (from an Re() 
or Im() point of view):

 ??? NA_real_ + 0i

 ??? complex(r=NA_real_, i=Inf)

 ??? complex(r=2, i=NA_real_)

 ??? complex(r=NaN, i=NA_real_)

In other words, using a single representation for complex NA (i.e. 
complex(r=NA_real_, i=NA_real_)) would avoid a lot of unnecessary 
complications and surprises.

Once you do that, whether as.complex(NA_real_) should return 
complex(r=NA_real_, i=0) or complex(r=NA_real_, i=NA_real_) becomes a 
moot point.

Best,

H.

On 9/22/23 03:38, Martin Maechler wrote:
>>>>>> Mikael Jagan
>>>>>>      on Thu, 21 Sep 2023 00:47:39 -0400 writes:
>      > Revisiting this thread from April:
>
>      >https://stat.ethz.ch/pipermail/r-devel/2023-April/082545.html
>
>      > where the decision (not yet backported) was made for
>      > as.complex(NA_real_) to give NA_complex_ instead of
>      > complex(r=NA_real_, i=0), to be consistent with
>      > help("as.complex") and as.complex(NA) and as.complex(NA_integer_).
>
>      > Was any consideration given to the alternative?
>      > That is, to changing as.complex(NA) and as.complex(NA_integer_) to
>      > give complex(r=NA_real_, i=0), consistent with
>      > as.complex(NA_real_), then amending help("as.complex")
>      > accordingly?
>
> Hmm, as, from R-core, mostly I was involved, I admit to say "no",
> to my knowledge the (above) alternative wasn't considered.
>
>    > The principle that
>    > Im(as.complex(<real=(double|integer|logical)>)) should be zero
>    > is quite fundamental, in my view, hence the "new" behaviour
>    > seems to really violate the principle of least surprise ...
>
> of course "least surprise"  is somewhat subjective.  Still,
> I clearly agree that the above would be one desirable property.
>
> I think that any solution will lead to *some* surprise for some
> cases, I think primarily because there are *many* different
> values z  for which  is.na(z)  is true,  and in any case
> NA_complex_  is only of the many.
>
> I also agree with Mikael that we should reconsider the issue
> that was raised by Davis Vaughan here ("on R-devel") last April.
>
>      > Another (but maybe weaker) argument is that
>      > double->complex coercions happen more often than
>      > logical->complex and integer->complex ones.  Changing the
>      > behaviour of the more frequently performed coercion is
>      > more likely to affect code "out there".
>
>      > Yet another argument is that one expects
>
>      >      identical(as.complex(NA_real_), NA_real_ + (0+0i))
>
>      > to be TRUE, i.e., that coercing from double to complex is
>      > equivalent to adding a complex zero.  The new behaviour
>      > makes the above FALSE, since NA_real_ + (0+0i) gives
>      > complex(r=NA_real_, i=0).
>
> No!  --- To my own surprise (!) --- in current R-devel the above is TRUE,
> and
>        NA_real_ + (0+0i)  , the same as
>        NA_real_ + 0i      , really gives  complex(r=NA, i=NA) :
>
> Using showC() from ?complex
>
>    showC <- function(z) noquote(sprintf("(R = %g, I = %g)", Re(z), Im(z)))
>
> we see (in R-devel) quite consistently
>
>> showC(NA_real_ + 0i)
> [1] (R = NA, I = NA)
>> showC(NA       + 0i)  # NA is 'logical'
> [1] (R = NA, I = NA)
> where as in R 4.3.1 and "R-patched" -- *in*consistently
>
>> showC(NA_real_ + 0i)
> [1] (R = NA, I = 0)
>> showC(NA + 0i)
> [1] (R = NA, I = NA)
> .... and honestly, I do not see *where* (and when) we changed
> the underlying code (in arithmetic.c !?)  in R-devel to *also*
> produce  NA_complex_  in such complex *arithmetic*
>
>
>      > Having said that, one might also (but more naively) expect
>
>      >     identical(as.complex(as.double(NA_complex_)), NA_complex_)
>
>      > to be TRUE.
>
> as in current R-devel
>
>      > Under my proposal it continues to be FALSE.
>
> as in "R-release"
>
>      > Well, I'd prefer if it gave FALSE with a warning
>      > "imaginary parts discarded in coercion", but it seems that
>      > as.double(complex(r=a, i=b)) never warns when either of
>      > 'a' and 'b' is NA_real_ or NaN, even where "information"
>      > {nonzero 'b'} is clearly lost ...
>
> The question of *warning* here is related indeed, but I think
> we should try to look at it only *secondary* to your first
> proposal.
>
>      > Whatever decision is made about as.complex(NA_real_),
>      > maybe these points should be weighed before it becomes part of
>      > R-release ...
>
>      > Mikael
>
> Indeed.
>
> Can we please get other opinions / ideas here?
>
> Thank you in advance for your thoughts!
> Martin
>
> ---
>
> PS:
>
>   Our *print()*ing  of complex NA's ("NA" here meaning NA or NaN)
>   is also unsatisfactory, e.g. in the case where all entries of a
>   vector are NA in the sense of is.na(.), but their
>   Re() and Im() are not all NA:
>   
>    showC <- function(z) noquote(sprintf("(R = %g, I = %g)", Re(z), Im(z)))
>    z <- complex(, c(11, NA, NA), c(NA, 99, NA))
>    z
>    showC(z)
>
> gives
>
>    > z
>    [1] NA NA NA
>    > showC(z)
>    [1] (R = 11, I = NA) (R = NA, I = 99) (R = NA, I = NA)
>
> but that (printing of complex) *is* another issue,
> in which we have the re-opened bugzilla PR#16752
>      ==>https://bugs.r-project.org/show_bug.cgi?id=16752
>
> on which we also worked during the R Sprint in Warwick three
> weeks ago, and where I want to commit changes in any case {but
> think we should change even a bit more than we got to during the
> Sprint}.
>
> ______________________________________________
> R-devel at r-project.org  mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
Herv? Pag?s

Bioconductor Core Team
hpages.on.github at gmail.com

	[[alternative HTML version deleted]]


From murdoch@dunc@n @end|ng |rom gm@||@com  Fri Sep 22 22:43:43 2023
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Fri, 22 Sep 2023 16:43:43 -0400
Subject: [Rd] Recent changes to as.complex(NA_real_)
In-Reply-To: <4abe101e-55be-8800-31ee-e0414c89740e@gmail.com>
References: <1cf40db9-ea70-483a-a547-270da6926935@gmail.com>
 <25869.28353.595831.807741@stat.math.ethz.ch>
 <4abe101e-55be-8800-31ee-e0414c89740e@gmail.com>
Message-ID: <57f77592-26ad-43dd-8e36-f9989c915308@gmail.com>

Since the result of is.na(x) is the same on each of those, I don't see a 
problem.  As long as that is consistent, I don't see a problem.  You 
shouldn't be using any other test for NA-ness.  You should never be 
expecting identical() to treat different types as the same (e.g. 
identical(NA, NA_real_) is FALSE, as it should be).  If you are using a 
different test, that's user error.

Duncan Murdoch

On 22/09/2023 2:41 p.m., Herv? Pag?s wrote:
> We could also question the value of having an infinite number of NA
> representations in the complex space. For example all these complex
> values are displayed the same way (as NA), are considered NAs by
> is.na(), but are not identical or semantically equivalent (from an Re()
> or Im() point of view):
> 
>   ??? NA_real_ + 0i
> 
>   ??? complex(r=NA_real_, i=Inf)
> 
>   ??? complex(r=2, i=NA_real_)
> 
>   ??? complex(r=NaN, i=NA_real_)
> 
> In other words, using a single representation for complex NA (i.e.
> complex(r=NA_real_, i=NA_real_)) would avoid a lot of unnecessary
> complications and surprises.
> 
> Once you do that, whether as.complex(NA_real_) should return
> complex(r=NA_real_, i=0) or complex(r=NA_real_, i=NA_real_) becomes a
> moot point.
> 
> Best,
> 
> H.
> 
> On 9/22/23 03:38, Martin Maechler wrote:
>>>>>>> Mikael Jagan
>>>>>>>       on Thu, 21 Sep 2023 00:47:39 -0400 writes:
>>       > Revisiting this thread from April:
>>
>>       >https://stat.ethz.ch/pipermail/r-devel/2023-April/082545.html
>>
>>       > where the decision (not yet backported) was made for
>>       > as.complex(NA_real_) to give NA_complex_ instead of
>>       > complex(r=NA_real_, i=0), to be consistent with
>>       > help("as.complex") and as.complex(NA) and as.complex(NA_integer_).
>>
>>       > Was any consideration given to the alternative?
>>       > That is, to changing as.complex(NA) and as.complex(NA_integer_) to
>>       > give complex(r=NA_real_, i=0), consistent with
>>       > as.complex(NA_real_), then amending help("as.complex")
>>       > accordingly?
>>
>> Hmm, as, from R-core, mostly I was involved, I admit to say "no",
>> to my knowledge the (above) alternative wasn't considered.
>>
>>     > The principle that
>>     > Im(as.complex(<real=(double|integer|logical)>)) should be zero
>>     > is quite fundamental, in my view, hence the "new" behaviour
>>     > seems to really violate the principle of least surprise ...
>>
>> of course "least surprise"  is somewhat subjective.  Still,
>> I clearly agree that the above would be one desirable property.
>>
>> I think that any solution will lead to *some* surprise for some
>> cases, I think primarily because there are *many* different
>> values z  for which  is.na(z)  is true,  and in any case
>> NA_complex_  is only of the many.
>>
>> I also agree with Mikael that we should reconsider the issue
>> that was raised by Davis Vaughan here ("on R-devel") last April.
>>
>>       > Another (but maybe weaker) argument is that
>>       > double->complex coercions happen more often than
>>       > logical->complex and integer->complex ones.  Changing the
>>       > behaviour of the more frequently performed coercion is
>>       > more likely to affect code "out there".
>>
>>       > Yet another argument is that one expects
>>
>>       >      identical(as.complex(NA_real_), NA_real_ + (0+0i))
>>
>>       > to be TRUE, i.e., that coercing from double to complex is
>>       > equivalent to adding a complex zero.  The new behaviour
>>       > makes the above FALSE, since NA_real_ + (0+0i) gives
>>       > complex(r=NA_real_, i=0).
>>
>> No!  --- To my own surprise (!) --- in current R-devel the above is TRUE,
>> and
>>         NA_real_ + (0+0i)  , the same as
>>         NA_real_ + 0i      , really gives  complex(r=NA, i=NA) :
>>
>> Using showC() from ?complex
>>
>>     showC <- function(z) noquote(sprintf("(R = %g, I = %g)", Re(z), Im(z)))
>>
>> we see (in R-devel) quite consistently
>>
>>> showC(NA_real_ + 0i)
>> [1] (R = NA, I = NA)
>>> showC(NA       + 0i)  # NA is 'logical'
>> [1] (R = NA, I = NA)
>> where as in R 4.3.1 and "R-patched" -- *in*consistently
>>
>>> showC(NA_real_ + 0i)
>> [1] (R = NA, I = 0)
>>> showC(NA + 0i)
>> [1] (R = NA, I = NA)
>> .... and honestly, I do not see *where* (and when) we changed
>> the underlying code (in arithmetic.c !?)  in R-devel to *also*
>> produce  NA_complex_  in such complex *arithmetic*
>>
>>
>>       > Having said that, one might also (but more naively) expect
>>
>>       >     identical(as.complex(as.double(NA_complex_)), NA_complex_)
>>
>>       > to be TRUE.
>>
>> as in current R-devel
>>
>>       > Under my proposal it continues to be FALSE.
>>
>> as in "R-release"
>>
>>       > Well, I'd prefer if it gave FALSE with a warning
>>       > "imaginary parts discarded in coercion", but it seems that
>>       > as.double(complex(r=a, i=b)) never warns when either of
>>       > 'a' and 'b' is NA_real_ or NaN, even where "information"
>>       > {nonzero 'b'} is clearly lost ...
>>
>> The question of *warning* here is related indeed, but I think
>> we should try to look at it only *secondary* to your first
>> proposal.
>>
>>       > Whatever decision is made about as.complex(NA_real_),
>>       > maybe these points should be weighed before it becomes part of
>>       > R-release ...
>>
>>       > Mikael
>>
>> Indeed.
>>
>> Can we please get other opinions / ideas here?
>>
>> Thank you in advance for your thoughts!
>> Martin
>>
>> ---
>>
>> PS:
>>
>>    Our *print()*ing  of complex NA's ("NA" here meaning NA or NaN)
>>    is also unsatisfactory, e.g. in the case where all entries of a
>>    vector are NA in the sense of is.na(.), but their
>>    Re() and Im() are not all NA:
>>    
>>     showC <- function(z) noquote(sprintf("(R = %g, I = %g)", Re(z), Im(z)))
>>     z <- complex(, c(11, NA, NA), c(NA, 99, NA))
>>     z
>>     showC(z)
>>
>> gives
>>
>>     > z
>>     [1] NA NA NA
>>     > showC(z)
>>     [1] (R = 11, I = NA) (R = NA, I = 99) (R = NA, I = NA)
>>
>> but that (printing of complex) *is* another issue,
>> in which we have the re-opened bugzilla PR#16752
>>       ==>https://bugs.r-project.org/show_bug.cgi?id=16752
>>
>> on which we also worked during the R Sprint in Warwick three
>> weeks ago, and where I want to commit changes in any case {but
>> think we should change even a bit more than we got to during the
>> Sprint}.
>>
>> ______________________________________________
>> R-devel at r-project.org  mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From iuke-tier@ey m@iii@g oii uiow@@edu  Fri Sep 22 23:14:58 2023
From: iuke-tier@ey m@iii@g oii uiow@@edu (iuke-tier@ey m@iii@g oii uiow@@edu)
Date: Fri, 22 Sep 2023 16:14:58 -0500 (CDT)
Subject: [Rd] [External]  On PRINTNAME() encoding, EncodeChar(),
 and being painted into a corner
In-Reply-To: <20230919003356.13a97d6e@Tarkus>
References: <20230919003356.13a97d6e@Tarkus>
Message-ID: <f357e53a-304e-c61f-8edb-759fda4c052@uiowa.edu>

Thanks for looking into this!

On Mon, 18 Sep 2023, Ivan Krylov wrote:

> Hello R-devel,
>
> I have originally learned about this from the following GitHub issue:
> <https://github.com/r-devel/r-project-sprint-2023/issues/65>. In short,
> in various places of the R source code, symbol names are accessed using
> translateChar(), EncodeChar(), and CHAR(), and it might help to unify
> their use.
>
> Currently, R is very careful to only create symbols with names in the
> native encoding. I have verified this by tracing the ways a symbol can
> be created (allocSExp) or have a name assigned (SET_PRINTNAME) using
> static analysis (Coccinelle). While it's possible to create a symbol
> with a name in an encoding different from the native encoding using
> SET_PRINTNAME(symbol, mkCharCE(...)), neither R nor CRAN packages
> invoke code like this for an arbitrary encoding; symbols are always
> created using either install() or installTrChar(). (install("invalid
> byte sequence") is, of course, still possible, but is a different
> problem.)

SET_PRINTNAME is not in the API and not in the public header files so
this should not be an issue. It would probably be best to refactor
things so SET_PRINTNAME only exists in memory.c

> This means that translateChar(PRINTNAME(...)) is currently unnecessary,
> but it may be worth adding a check (opt-in, applicable only during R
> CMD check, to avoid a performance hit?) to SET_PRINTNAME() to ensure
> that only native-encoding (or ASCII) symbol names are used. I could also
> suggest a patch for Writing R Extensions or R Internals to document this
> assumption.
>
> The following translateChar() doesn't hurt (it returns CHAR(x) right
> away without allocating any memory), but it stands out against most
> uses of CHAR(PRINTNAME(.)) and EncodeChar(PRINTNAME(.)):
>
> --- src/main/subscript.c	(revision 85160)
> +++ src/main/subscript.c	(working copy)
> @@ -186,7 +186,7 @@
> 	    PROTECT(names);
> 	    for (i = 0; i < nx; i++)
> 		if (streql(translateChar(STRING_ELT(names, i)),
> -			   translateChar(PRINTNAME(s)))) {
> +			   CHAR(PRINTNAME(s)))) {
> 		    indx = i;
> 		    break;
> 		}
>
> The following translateChar() can be safely replaced with EncodeChar(),
> correctly printing funnily-named functions in tracemem() reports:
>
> --- src/main/debug.c	(revision 85160)
> +++ src/main/debug.c	(working copy)
> @@ -203,7 +203,7 @@
> 	    && TYPEOF(cptr->call) == LANGSXP) {
> 	    SEXP fun = CAR(cptr->call);
> 	    Rprintf("%s ",
> -		    TYPEOF(fun) == SYMSXP ? translateChar(PRINTNAME(fun)) :
> +		    TYPEOF(fun) == SYMSXP ? EncodeChar(PRINTNAME(fun)) : "<Anonymous>");
> 	}
>     }
>
> tracemem(a <- 1:10)
> `\r\v\t\n` <- function(x) x[1] <- 0
> `\r\v\t\n`(a)
> # Now correctly prints:
> # tracemem[0x55fd11e61e00 -> 0x55fd1081d2a8]: \r\v\t\n
> # tracemem[0x55fd1081d2a8 -> 0x55fd113277e8]: \r\v\t\n

Sounds good. I've made those two changes in the trunk in r85209.

> What about EncodeChar(PRINTNAME(.))? This is the intended way to report
> symbols in error messages. Without EncodeChar(),
> .Internal(`\r\v\t\n`()) actually prints the newlines to standard output
> as part of the error message instead of escaping them. Unfortunately,
> EncodeChar() uses a statically-allocated buffer for its return value,
> *and* the comments say that it's unsafe to use together with
> errorcall(): errorcall_cpy() must be used instead. I think that's
> overwriting the statically-allocated buffer before the format arguments
> (which also contain the return value of EncodeChar()) are processed. In
> particular, this means that EncodeChar() is unsafe to use with any kind
> of warnings. The following Coccinelle script locates uses of
> CHAR(PRINTNAME(.)) inside errors and warnings:
> @@
> expression x;
> expression list arg1, arg2;
> identifier fun =~ "(Rf_)?(error|warning)(call)?(_cpy)?";
> @@
> fun(
>  arg1,
> * CHAR(PRINTNAME(x)),
>  arg2
> )
>
> Some of these, which already use errorcall(), are trivial to fix by
> replacing CHAR() with EncodeChar() and upgrading errorcall() to
> errorcall_cpy():

I think it would be best to modify errorcall so errorcall_cpy is not
necessary. As things are now it is just too easy to forget that
sometimes errorcall_cpy should be used (and this has lead to some bugs
recently).

> --- src/main/names.c
> +++ src/main/names.c
> @@ -1367,7 +1367,7 @@ attribute_hidden SEXP do_internal(SEXP c
> 	errorcall(call, _("invalid .Internal() argument"));
>     if (INTERNAL(fun) == R_NilValue)
> -	errorcall(call, _("there is no .Internal function '%s'"),
> +	errorcall_cpy(call, _("there is no .Internal function '%s'"),
> -		  CHAR(PRINTNAME(fun)));
> +		  EncodeChar(PRINTNAME(fun)));
>
> #ifdef CHECK_INTERNALS
>     if(R_Is_Running > 1 && getenv("_R_CHECK_INTERNALS2_")) {
>
> --- src/main/eval.c
> +++ src/main/eval.c
> @@ -1161,7 +1161,7 @@ SEXP eval(SEXP e, SEXP rho)
> 	    const char *n = CHAR(PRINTNAME(e));
> -	    if(*n) errorcall(getLexicalCall(rho),
> +	    if(*n) errorcall_cpy(getLexicalCall(rho),
> 			     _("argument \"%s\" is missing, with no default"),
> -			     CHAR(PRINTNAME(e)));
> +			     EncodeChar(PRINTNAME(e)));
> 	    else errorcall(getLexicalCall(rho),
> 			   _("argument is missing, with no default"));
> 	}
>
> --- src/main/match.c
> +++ src/main/match.c
> @@ -229,7 +229,7 @@ attribute_hidden SEXP matchArgs_NR(SEXP
> 		      if (fargused[arg_i] == 2)
> -			  errorcall(call,
> +			  errorcall_cpy(call,
> 	                      _("formal argument \"%s\" matched by multiple actual arguments"),
> -	                      CHAR(PRINTNAME(TAG(f))));
> +	                      EncodeChar(PRINTNAME(TAG(f))));
> 		      if (ARGUSED(b) == 2)
> 			  errorcall(call,
> 	                      _("argument %d matches multiple formal arguments"),
> @@ -272,12 +271,12 @@ attribute_hidden SEXP matchArgs_NR(SEXP
> 			if (fargused[arg_i] == 1)
> -			    errorcall(call,
> +			    errorcall_cpy(call,
> 				_("formal argument \"%s\" matched by multiple actual arguments"),
> -				CHAR(PRINTNAME(TAG(f))));
> +				EncodeChar(PRINTNAME(TAG(f))));
> 			if (R_warn_partial_match_args) {
> 			    warningcall(call,
> 					_("partial argument match of '%s' to '%s'"), CHAR(PRINTNAME(TAG(b))),
> 					CHAR(PRINTNAME(TAG(f))) );
> 			}
> 			SETCAR(a, CAR(b));
> 			if (CAR(b) != R_MissingArg) SET_MISSING(a, 0);
>
> The changes become more complicated with a plain error() (have to
> figure out the current call and provide it to errorcall_cpy), still
> more complicated with warnings (there's currently no warningcall_cpy(),
> though one can be implemented) and even more complicated when multiple
> symbols are used in the same warning or error, like in the last
> warningcall() above (EncodeChar() can only be called once at a time).
>
> The only solution to the latter problem is an EncodeChar() variant that
> allocates its memory dynamically. Would R_alloc() be acceptable in this
> context? With errors, the allocation stack would be quickly reset
> (except when withCallingHandlers() is in effect?), but with warnings,
> the code would have to restore it manually every time.

Or allow/require a buffer to be provided. So replacing the calls like

    CHAR(PRINTNAME(sym))

with

    EncodeSymbol(sym, buf, buf_size)

> Is it even worth
> the effort to try to handle the (pretty rare) non-syntactic symbol names
> while constructing error messages? Other languages (like Lua or SQLite)
> provide a special printf specifier (typically %q) to create
> quoted/escaped string representations, but we're not yet at the point
> of providing a C-level printf implementation.

Not clear it is worth it. But the situation now is not good, because
sometimes we encode and sometimes we don't. It would be better to be
consistent, both for the end user and for maintainers who now have to
spend time figuring out which way to go.

Best,

luke

-- 
Luke Tierney
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu


From @pencer@gr@ve@ @end|ng |rom prod@y@e@com  Fri Sep 22 23:42:03 2023
From: @pencer@gr@ve@ @end|ng |rom prod@y@e@com (Spencer Graves)
Date: Fri, 22 Sep 2023 16:42:03 -0500
Subject: [Rd] Recent changes to as.complex(NA_real_)
In-Reply-To: <57f77592-26ad-43dd-8e36-f9989c915308@gmail.com>
References: <1cf40db9-ea70-483a-a547-270da6926935@gmail.com>
 <25869.28353.595831.807741@stat.math.ethz.ch>
 <4abe101e-55be-8800-31ee-e0414c89740e@gmail.com>
 <57f77592-26ad-43dd-8e36-f9989c915308@gmail.com>
Message-ID: <468f3df3-5350-4be0-bed1-f7f1bca0ab23@prodsyse.com>

	  Perhaps I shouldn't comment without having read the entire thread, 
but I will:  I can envision situations where I might want, e.g., 2 from 
complex(r=2, i=NA_real_).


	  Spencer Graves


On 9/22/23 3:43 PM, Duncan Murdoch wrote:
> Since the result of is.na(x) is the same on each of those, I don't see a 
> problem.? As long as that is consistent, I don't see a problem.? You 
> shouldn't be using any other test for NA-ness.? You should never be 
> expecting identical() to treat different types as the same (e.g. 
> identical(NA, NA_real_) is FALSE, as it should be).? If you are using a 
> different test, that's user error.
> 
> Duncan Murdoch
> 
> On 22/09/2023 2:41 p.m., Herv? Pag?s wrote:
>> We could also question the value of having an infinite number of NA
>> representations in the complex space. For example all these complex
>> values are displayed the same way (as NA), are considered NAs by
>> is.na(), but are not identical or semantically equivalent (from an Re()
>> or Im() point of view):
>>
>> ? ??? NA_real_ + 0i
>>
>> ? ??? complex(r=NA_real_, i=Inf)
>>
>> ? ??? complex(r=2, i=NA_real_)
>>
>> ? ??? complex(r=NaN, i=NA_real_)
>>
>> In other words, using a single representation for complex NA (i.e.
>> complex(r=NA_real_, i=NA_real_)) would avoid a lot of unnecessary
>> complications and surprises.
>>
>> Once you do that, whether as.complex(NA_real_) should return
>> complex(r=NA_real_, i=0) or complex(r=NA_real_, i=NA_real_) becomes a
>> moot point.
>>
>> Best,
>>
>> H.
>>
>> On 9/22/23 03:38, Martin Maechler wrote:
>>>>>>>> Mikael Jagan
>>>>>>>> ????? on Thu, 21 Sep 2023 00:47:39 -0400 writes:
>>> ????? > Revisiting this thread from April:
>>>
>>> ????? >https://stat.ethz.ch/pipermail/r-devel/2023-April/082545.html
>>>
>>> ????? > where the decision (not yet backported) was made for
>>> ????? > as.complex(NA_real_) to give NA_complex_ instead of
>>> ????? > complex(r=NA_real_, i=0), to be consistent with
>>> ????? > help("as.complex") and as.complex(NA) and 
>>> as.complex(NA_integer_).
>>>
>>> ????? > Was any consideration given to the alternative?
>>> ????? > That is, to changing as.complex(NA) and 
>>> as.complex(NA_integer_) to
>>> ????? > give complex(r=NA_real_, i=0), consistent with
>>> ????? > as.complex(NA_real_), then amending help("as.complex")
>>> ????? > accordingly?
>>>
>>> Hmm, as, from R-core, mostly I was involved, I admit to say "no",
>>> to my knowledge the (above) alternative wasn't considered.
>>>
>>> ??? > The principle that
>>> ??? > Im(as.complex(<real=(double|integer|logical)>)) should be zero
>>> ??? > is quite fundamental, in my view, hence the "new" behaviour
>>> ??? > seems to really violate the principle of least surprise ...
>>>
>>> of course "least surprise"? is somewhat subjective.? Still,
>>> I clearly agree that the above would be one desirable property.
>>>
>>> I think that any solution will lead to *some* surprise for some
>>> cases, I think primarily because there are *many* different
>>> values z? for which? is.na(z)? is true,? and in any case
>>> NA_complex_? is only of the many.
>>>
>>> I also agree with Mikael that we should reconsider the issue
>>> that was raised by Davis Vaughan here ("on R-devel") last April.
>>>
>>> ????? > Another (but maybe weaker) argument is that
>>> ????? > double->complex coercions happen more often than
>>> ????? > logical->complex and integer->complex ones.? Changing the
>>> ????? > behaviour of the more frequently performed coercion is
>>> ????? > more likely to affect code "out there".
>>>
>>> ????? > Yet another argument is that one expects
>>>
>>> ????? >????? identical(as.complex(NA_real_), NA_real_ + (0+0i))
>>>
>>> ????? > to be TRUE, i.e., that coercing from double to complex is
>>> ????? > equivalent to adding a complex zero.? The new behaviour
>>> ????? > makes the above FALSE, since NA_real_ + (0+0i) gives
>>> ????? > complex(r=NA_real_, i=0).
>>>
>>> No!? --- To my own surprise (!) --- in current R-devel the above is 
>>> TRUE,
>>> and
>>> ??????? NA_real_ + (0+0i)? , the same as
>>> ??????? NA_real_ + 0i????? , really gives? complex(r=NA, i=NA) :
>>>
>>> Using showC() from ?complex
>>>
>>> ??? showC <- function(z) noquote(sprintf("(R = %g, I = %g)", Re(z), 
>>> Im(z)))
>>>
>>> we see (in R-devel) quite consistently
>>>
>>>> showC(NA_real_ + 0i)
>>> [1] (R = NA, I = NA)
>>>> showC(NA?????? + 0i)? # NA is 'logical'
>>> [1] (R = NA, I = NA)
>>> where as in R 4.3.1 and "R-patched" -- *in*consistently
>>>
>>>> showC(NA_real_ + 0i)
>>> [1] (R = NA, I = 0)
>>>> showC(NA + 0i)
>>> [1] (R = NA, I = NA)
>>> .... and honestly, I do not see *where* (and when) we changed
>>> the underlying code (in arithmetic.c !?)? in R-devel to *also*
>>> produce? NA_complex_? in such complex *arithmetic*
>>>
>>>
>>> ????? > Having said that, one might also (but more naively) expect
>>>
>>> ????? >???? identical(as.complex(as.double(NA_complex_)), NA_complex_)
>>>
>>> ????? > to be TRUE.
>>>
>>> as in current R-devel
>>>
>>> ????? > Under my proposal it continues to be FALSE.
>>>
>>> as in "R-release"
>>>
>>> ????? > Well, I'd prefer if it gave FALSE with a warning
>>> ????? > "imaginary parts discarded in coercion", but it seems that
>>> ????? > as.double(complex(r=a, i=b)) never warns when either of
>>> ????? > 'a' and 'b' is NA_real_ or NaN, even where "information"
>>> ????? > {nonzero 'b'} is clearly lost ...
>>>
>>> The question of *warning* here is related indeed, but I think
>>> we should try to look at it only *secondary* to your first
>>> proposal.
>>>
>>> ????? > Whatever decision is made about as.complex(NA_real_),
>>> ????? > maybe these points should be weighed before it becomes part of
>>> ????? > R-release ...
>>>
>>> ????? > Mikael
>>>
>>> Indeed.
>>>
>>> Can we please get other opinions / ideas here?
>>>
>>> Thank you in advance for your thoughts!
>>> Martin
>>>
>>> ---
>>>
>>> PS:
>>>
>>> ?? Our *print()*ing? of complex NA's ("NA" here meaning NA or NaN)
>>> ?? is also unsatisfactory, e.g. in the case where all entries of a
>>> ?? vector are NA in the sense of is.na(.), but their
>>> ?? Re() and Im() are not all NA:
>>> ??? showC <- function(z) noquote(sprintf("(R = %g, I = %g)", Re(z), 
>>> Im(z)))
>>> ??? z <- complex(, c(11, NA, NA), c(NA, 99, NA))
>>> ??? z
>>> ??? showC(z)
>>>
>>> gives
>>>
>>> ??? > z
>>> ??? [1] NA NA NA
>>> ??? > showC(z)
>>> ??? [1] (R = 11, I = NA) (R = NA, I = 99) (R = NA, I = NA)
>>>
>>> but that (printing of complex) *is* another issue,
>>> in which we have the re-opened bugzilla PR#16752
>>> ????? ==>https://bugs.r-project.org/show_bug.cgi?id=16752
>>>
>>> on which we also worked during the R Sprint in Warwick three
>>> weeks ago, and where I want to commit changes in any case {but
>>> think we should change even a bit more than we got to during the
>>> Sprint}.
>>>
>>> ______________________________________________
>>> R-devel at r-project.org? mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From hp@ge@@on@g|thub @end|ng |rom gm@||@com  Sat Sep 23 01:55:05 2023
From: hp@ge@@on@g|thub @end|ng |rom gm@||@com (=?UTF-8?B?SGVydsOpIFBhZ8Oocw==?=)
Date: Fri, 22 Sep 2023 16:55:05 -0700
Subject: [Rd] Recent changes to as.complex(NA_real_)
In-Reply-To: <57f77592-26ad-43dd-8e36-f9989c915308@gmail.com>
References: <1cf40db9-ea70-483a-a547-270da6926935@gmail.com>
 <25869.28353.595831.807741@stat.math.ethz.ch>
 <4abe101e-55be-8800-31ee-e0414c89740e@gmail.com>
 <57f77592-26ad-43dd-8e36-f9989c915308@gmail.com>
Message-ID: <6bcad145-a1f9-32a5-c9e8-97041ed2eab7@gmail.com>

The problem is that you have things that are **semantically** different 
but look exactly the same:

They look the same:

 > x
[1] NA
 > y
[1] NA
 > z
[1] NA

 > is.na(x)
[1] TRUE
 > is.na(y)
[1] TRUE
 > is.na(z)
[1] TRUE

 > str(x)
 ?cplx NA
 > str(y)
 ?num NA
 > str(z)
 ?cplx NA

but they are semantically different e.g.

 > Re(x)
[1] NA
 > Re(y)
[1] -0.5? # surprise!

 > Im(x)? # surprise!
[1] 2
 > Im(z)
[1] NA

so any expression involving Re() or Im() will produce different results 
on input that look the same on the surface.

You can address this either by normalizing the internal representation 
of complex NA to always be complex(r=NaN, i=NA_real_), like for 
NA_complex_, or by allowing the infinite variations that are currently 
allowed and at the same time making sure that both Re() and Im()? always 
return NA_real_ on a complex NA.

My point is that the behavior of complex NA should be predictable. Right 
now it's not. Once it's predictable (with Re() and Im() both returning 
NA_real_ regardless of internal representation), then it no longer 
matters what kind of complex NA is returned by as.complex(NA_real_), 
because they are no onger distinguishable.

H.

On 9/22/23 13:43, Duncan Murdoch wrote:
> Since the result of is.na(x) is the same on each of those, I don't see 
> a problem.? As long as that is consistent, I don't see a problem. You 
> shouldn't be using any other test for NA-ness.? You should never be 
> expecting identical() to treat different types as the same (e.g. 
> identical(NA, NA_real_) is FALSE, as it should be).? If you are using 
> a different test, that's user error.
>
> Duncan Murdoch
>
> On 22/09/2023 2:41 p.m., Herv? Pag?s wrote:
>> We could also question the value of having an infinite number of NA
>> representations in the complex space. For example all these complex
>> values are displayed the same way (as NA), are considered NAs by
>> is.na(), but are not identical or semantically equivalent (from an Re()
>> or Im() point of view):
>>
>> ? ??? NA_real_ + 0i
>>
>> ? ??? complex(r=NA_real_, i=Inf)
>>
>> ? ??? complex(r=2, i=NA_real_)
>>
>> ? ??? complex(r=NaN, i=NA_real_)
>>
>> In other words, using a single representation for complex NA (i.e.
>> complex(r=NA_real_, i=NA_real_)) would avoid a lot of unnecessary
>> complications and surprises.
>>
>> Once you do that, whether as.complex(NA_real_) should return
>> complex(r=NA_real_, i=0) or complex(r=NA_real_, i=NA_real_) becomes a
>> moot point.
>>
>> Best,
>>
>> H.
>>
>> On 9/22/23 03:38, Martin Maechler wrote:
>>>>>>>> Mikael Jagan
>>>>>>>> ????? on Thu, 21 Sep 2023 00:47:39 -0400 writes:
>>> ????? > Revisiting this thread from April:
>>>
>>> >https://stat.ethz.ch/pipermail/r-devel/2023-April/082545.html
>>>
>>> ????? > where the decision (not yet backported) was made for
>>> ????? > as.complex(NA_real_) to give NA_complex_ instead of
>>> ????? > complex(r=NA_real_, i=0), to be consistent with
>>> ????? > help("as.complex") and as.complex(NA) and 
>>> as.complex(NA_integer_).
>>>
>>> ????? > Was any consideration given to the alternative?
>>> ????? > That is, to changing as.complex(NA) and 
>>> as.complex(NA_integer_) to
>>> ????? > give complex(r=NA_real_, i=0), consistent with
>>> ????? > as.complex(NA_real_), then amending help("as.complex")
>>> ????? > accordingly?
>>>
>>> Hmm, as, from R-core, mostly I was involved, I admit to say "no",
>>> to my knowledge the (above) alternative wasn't considered.
>>>
>>> ??? > The principle that
>>> ??? > Im(as.complex(<real=(double|integer|logical)>)) should be zero
>>> ??? > is quite fundamental, in my view, hence the "new" behaviour
>>> ??? > seems to really violate the principle of least surprise ...
>>>
>>> of course "least surprise"? is somewhat subjective.? Still,
>>> I clearly agree that the above would be one desirable property.
>>>
>>> I think that any solution will lead to *some* surprise for some
>>> cases, I think primarily because there are *many* different
>>> values z? for which? is.na(z)? is true,? and in any case
>>> NA_complex_? is only of the many.
>>>
>>> I also agree with Mikael that we should reconsider the issue
>>> that was raised by Davis Vaughan here ("on R-devel") last April.
>>>
>>> ????? > Another (but maybe weaker) argument is that
>>> ????? > double->complex coercions happen more often than
>>> ????? > logical->complex and integer->complex ones. Changing the
>>> ????? > behaviour of the more frequently performed coercion is
>>> ????? > more likely to affect code "out there".
>>>
>>> ????? > Yet another argument is that one expects
>>>
>>> ????? >????? identical(as.complex(NA_real_), NA_real_ + (0+0i))
>>>
>>> ????? > to be TRUE, i.e., that coercing from double to complex is
>>> ????? > equivalent to adding a complex zero.? The new behaviour
>>> ????? > makes the above FALSE, since NA_real_ + (0+0i) gives
>>> ????? > complex(r=NA_real_, i=0).
>>>
>>> No!? --- To my own surprise (!) --- in current R-devel the above is 
>>> TRUE,
>>> and
>>> ??????? NA_real_ + (0+0i)? , the same as
>>> ??????? NA_real_ + 0i????? , really gives? complex(r=NA, i=NA) :
>>>
>>> Using showC() from ?complex
>>>
>>> ??? showC <- function(z) noquote(sprintf("(R = %g, I = %g)", Re(z), 
>>> Im(z)))
>>>
>>> we see (in R-devel) quite consistently
>>>
>>>> showC(NA_real_ + 0i)
>>> [1] (R = NA, I = NA)
>>>> showC(NA?????? + 0i)? # NA is 'logical'
>>> [1] (R = NA, I = NA)
>>> where as in R 4.3.1 and "R-patched" -- *in*consistently
>>>
>>>> showC(NA_real_ + 0i)
>>> [1] (R = NA, I = 0)
>>>> showC(NA + 0i)
>>> [1] (R = NA, I = NA)
>>> .... and honestly, I do not see *where* (and when) we changed
>>> the underlying code (in arithmetic.c !?)? in R-devel to *also*
>>> produce? NA_complex_? in such complex *arithmetic*
>>>
>>>
>>> ????? > Having said that, one might also (but more naively) expect
>>>
>>> ????? >???? identical(as.complex(as.double(NA_complex_)), NA_complex_)
>>>
>>> ????? > to be TRUE.
>>>
>>> as in current R-devel
>>>
>>> ????? > Under my proposal it continues to be FALSE.
>>>
>>> as in "R-release"
>>>
>>> ????? > Well, I'd prefer if it gave FALSE with a warning
>>> ????? > "imaginary parts discarded in coercion", but it seems that
>>> ????? > as.double(complex(r=a, i=b)) never warns when either of
>>> ????? > 'a' and 'b' is NA_real_ or NaN, even where "information"
>>> ????? > {nonzero 'b'} is clearly lost ...
>>>
>>> The question of *warning* here is related indeed, but I think
>>> we should try to look at it only *secondary* to your first
>>> proposal.
>>>
>>> ????? > Whatever decision is made about as.complex(NA_real_),
>>> ????? > maybe these points should be weighed before it becomes part of
>>> ????? > R-release ...
>>>
>>> ????? > Mikael
>>>
>>> Indeed.
>>>
>>> Can we please get other opinions / ideas here?
>>>
>>> Thank you in advance for your thoughts!
>>> Martin
>>>
>>> ---
>>>
>>> PS:
>>>
>>> ?? Our *print()*ing? of complex NA's ("NA" here meaning NA or NaN)
>>> ?? is also unsatisfactory, e.g. in the case where all entries of a
>>> ?? vector are NA in the sense of is.na(.), but their
>>> ?? Re() and Im() are not all NA:
>>> ?? ??? showC <- function(z) noquote(sprintf("(R = %g, I = %g)", 
>>> Re(z), Im(z)))
>>> ??? z <- complex(, c(11, NA, NA), c(NA, 99, NA))
>>> ??? z
>>> ??? showC(z)
>>>
>>> gives
>>>
>>> ??? > z
>>> ??? [1] NA NA NA
>>> ??? > showC(z)
>>> ??? [1] (R = 11, I = NA) (R = NA, I = 99) (R = NA, I = NA)
>>>
>>> but that (printing of complex) *is* another issue,
>>> in which we have the re-opened bugzilla PR#16752
>>> ????? ==>https://bugs.r-project.org/show_bug.cgi?id=16752
>>>
>>> on which we also worked during the R Sprint in Warwick three
>>> weeks ago, and where I want to commit changes in any case {but
>>> think we should change even a bit more than we got to during the
>>> Sprint}.
>>>
>>> ______________________________________________
>>> R-devel at r-project.org? mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>
-- 
Herv? Pag?s

Bioconductor Core Team
hpages.on.github at gmail.com

	[[alternative HTML version deleted]]


From hp@ge@@on@g|thub @end|ng |rom gm@||@com  Sat Sep 23 01:58:06 2023
From: hp@ge@@on@g|thub @end|ng |rom gm@||@com (=?UTF-8?B?SGVydsOpIFBhZ8Oocw==?=)
Date: Fri, 22 Sep 2023 16:58:06 -0700
Subject: [Rd] Recent changes to as.complex(NA_real_)
In-Reply-To: <6bcad145-a1f9-32a5-c9e8-97041ed2eab7@gmail.com>
References: <1cf40db9-ea70-483a-a547-270da6926935@gmail.com>
 <25869.28353.595831.807741@stat.math.ethz.ch>
 <4abe101e-55be-8800-31ee-e0414c89740e@gmail.com>
 <57f77592-26ad-43dd-8e36-f9989c915308@gmail.com>
 <6bcad145-a1f9-32a5-c9e8-97041ed2eab7@gmail.com>
Message-ID: <c903af83-d9a6-59cf-6e60-1eb28300bc9f@gmail.com>

On 9/22/23 16:55, Herv? Pag?s wrote:

> The problem is that you have things that are **semantically** 
> different but look exactly the same:
>
> They look the same:
>
> > x
> [1] NA
> > y
> [1] NA
> > z
> [1] NA
>
> > is.na(x)
> [1] TRUE
> > is.na(y)
> [1] TRUE
> > is.na(z)
> [1] TRUE
>
> > str(x)
> ?cplx NA
> > str(y)
> ?num NA
>
oops, that was supposed to be:

 > str(y)
 ?cplx NA

but somehow I managed to copy/paste the wrong thing, sorry.

H.

-- 
Herv? Pag?s

Bioconductor Core Team
hpages.on.github at gmail.com

	[[alternative HTML version deleted]]


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Sat Sep 23 15:43:01 2023
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Sat, 23 Sep 2023 15:43:01 +0200
Subject: [Rd] Recent changes to as.complex(NA_real_)
In-Reply-To: <6bcad145-a1f9-32a5-c9e8-97041ed2eab7@gmail.com>
References: <1cf40db9-ea70-483a-a547-270da6926935@gmail.com>
 <25869.28353.595831.807741@stat.math.ethz.ch>
 <4abe101e-55be-8800-31ee-e0414c89740e@gmail.com>
 <57f77592-26ad-43dd-8e36-f9989c915308@gmail.com>
 <6bcad145-a1f9-32a5-c9e8-97041ed2eab7@gmail.com>
Message-ID: <25870.60261.24330.115632@stat.math.ethz.ch>

>>>>> Herv? Pag?s 
>>>>>     on Fri, 22 Sep 2023 16:55:05 -0700 writes:

    > The problem is that you have things that are
    > **semantically** different but look exactly the same:

    > They look the same:

    >> x
    > [1] NA
    >> y
    > [1] NA
    >> z
    > [1] NA

    >> is.na(x)
    > [1] TRUE
    >> is.na(y)
    > [1] TRUE
    >> is.na(z)
    > [1] TRUE

    >> str(x)
    >  ?cplx NA
    >> str(y)
    >  ?num NA
    >> str(z)
    >  ?cplx NA

    > but they are semantically different e.g.

    >> Re(x)
    > [1] NA
    >> Re(y)
    > [1] -0.5? # surprise!

    >> Im(x)? # surprise!
    > [1] 2
    >> Im(z)
    > [1] NA

    > so any expression involving Re() or Im() will produce
    > different results on input that look the same on the
    > surface.

    > You can address this either by normalizing the internal
    > representation of complex NA to always be complex(r=NaN,
    > i=NA_real_), like for NA_complex_, or by allowing the
    > infinite variations that are currently allowed and at the
    > same time making sure that both Re() and Im()? always
    > return NA_real_ on a complex NA.

    > My point is that the behavior of complex NA should be
    > predictable. Right now it's not. Once it's predictable
    > (with Re() and Im() both returning NA_real_ regardless of
    > internal representation), then it no longer matters what
    > kind of complex NA is returned by as.complex(NA_real_),
    > because they are no onger distinguishable.

    > H.

    > On 9/22/23 13:43, Duncan Murdoch wrote:
    >> Since the result of is.na(x) is the same on each of
    >> those, I don't see a problem.? As long as that is
    >> consistent, I don't see a problem. You shouldn't be using
    >> any other test for NA-ness.? You should never be
    >> expecting identical() to treat different types as the
    >> same (e.g.  identical(NA, NA_real_) is FALSE, as it
    >> should be).? If you are using a different test, that's
    >> user error.
    >> 
    >> Duncan Murdoch
    >> 
    >> On 22/09/2023 2:41 p.m., Herv? Pag?s wrote:
    >>> We could also question the value of having an infinite
    >>> number of NA representations in the complex space. For
    >>> example all these complex values are displayed the same
    >>> way (as NA), are considered NAs by is.na(), but are not
    >>> identical or semantically equivalent (from an Re() or
    >>> Im() point of view):
    >>> 
    >>> ? ??? NA_real_ + 0i
    >>> 
    >>> ? ??? complex(r=NA_real_, i=Inf)
    >>> 
    >>> ? ??? complex(r=2, i=NA_real_)
    >>> 
    >>> ? ??? complex(r=NaN, i=NA_real_)
    >>> 
    >>> In other words, using a single representation for
    >>> complex NA (i.e.  complex(r=NA_real_, i=NA_real_)) would
    >>> avoid a lot of unnecessary complications and surprises.
    >>> 
    >>> Once you do that, whether as.complex(NA_real_) should
    >>> return complex(r=NA_real_, i=0) or complex(r=NA_real_,
    >>> i=NA_real_) becomes a moot point.
    >>> 
    >>> Best,
    >>> 
    >>> H.

Thank you, Herv?.
Your proposition is yet another one,
to declare that all complex NA's should be treated as identical
(almost/fully?) everywhere.

This would be a possibility, but I think a drastic one.

I think there are too many cases, where I want to keep the
information of the real part independent of the values of the
imaginary part (e.g. think of the Riemann hypothesis), and
typically vice versa.

With your proposal, for a (potentially large) vector of complex numbers,
after
      Re(z)  <-  1/2

I could no longer rely on   Re(z) == 1/2,
because it would be wrong for those z where (the imaginary part/ the number)
was NA/NaN.
Also, in a similar case, a

      Im(z) <- NA

would have to "destroy" all real parts  Re(z);
not really typically in memory, but effectively for the user,  Re(z)
would be all NA/NaN.

And I think there are quite a few other situations
where looking at Re() and Im() separately makes a lot of sense.

Spencer also made a remark in this direction.

All in all I'd be very reluctant to move in this direction;
but yes, I'm just one person ... let's continue musing and
considering !

Martin

    >>> On 9/22/23 03:38, Martin Maechler wrote:
    >>>>>>>>> Mikael Jagan ????? on Thu, 21 Sep 2023 00:47:39
    >>>>>>>>> -0400 writes:
    >>>> ????? > Revisiting this thread from April:
    >>>> 
    >>>> >https://stat.ethz.ch/pipermail/r-devel/2023-April/082545.html
    >>>> 
    >>>> ????? > where the decision (not yet backported) was
    >>>> made for ????? > as.complex(NA_real_) to give
    >>>> NA_complex_ instead of ????? > complex(r=NA_real_,
    >>>> i=0), to be consistent with ????? > help("as.complex")
    >>>> and as.complex(NA) and as.complex(NA_integer_).
    >>>> 
    >>>> ????? > Was any consideration given to the alternative?
    >>>> ????? > That is, to changing as.complex(NA) and
    >>>> as.complex(NA_integer_) to ????? > give
    >>>> complex(r=NA_real_, i=0), consistent with ????? >
    >>>> as.complex(NA_real_), then amending help("as.complex")
    >>>> ????? > accordingly?
    >>>> 
    >>>> Hmm, as, from R-core, mostly I was involved, I admit to
    >>>> say "no", to my knowledge the (above) alternative
    >>>> wasn't considered.
    >>>> 
    >>>> ??? > The principle that ??? >
    >>>> Im(as.complex(<real=(double|integer|logical)>)) should
    >>>> be zero ??? > is quite fundamental, in my view, hence
    >>>> the "new" behaviour ??? > seems to really violate the
    >>>> principle of least surprise ...
    >>>> 
    >>>> of course "least surprise"? is somewhat subjective.?
    >>>> Still, I clearly agree that the above would be one
    >>>> desirable property.
    >>>> 
    >>>> I think that any solution will lead to *some* surprise
    >>>> for some cases, I think primarily because there are
    >>>> *many* different values z? for which? is.na(z)? is
    >>>> true,? and in any case NA_complex_? is only of the
    >>>> many.
    >>>> 
    >>>> I also agree with Mikael that we should reconsider the
    >>>> issue that was raised by Davis Vaughan here ("on
    >>>> R-devel") last April.
    >>>> 
    >>>> ????? > Another (but maybe weaker) argument is that
    >>>> ????? > double->complex coercions happen more often
    >>>> than ????? > logical->complex and integer->complex
    >>>> ones. Changing the ????? > behaviour of the more
    >>>> frequently performed coercion is ????? > more likely to
    >>>> affect code "out there".
    >>>> 
    >>>> ????? > Yet another argument is that one expects
    >>>> 
    >>>> ????? >????? identical(as.complex(NA_real_), NA_real_ +
    >>>> (0+0i))
    >>>> 
    >>>> ????? > to be TRUE, i.e., that coercing from double to
    >>>> complex is ????? > equivalent to adding a complex
    >>>> zero.? The new behaviour ????? > makes the above FALSE,
    >>>> since NA_real_ + (0+0i) gives ????? >
    >>>> complex(r=NA_real_, i=0).
    >>>> 
    >>>> No!? --- To my own surprise (!) --- in current R-devel
    >>>> the above is TRUE, and ??????? NA_real_ + (0+0i)? , the
    >>>> same as ??????? NA_real_ + 0i????? , really gives?
    >>>> complex(r=NA, i=NA) :
    >>>> 
    >>>> Using showC() from ?complex
    >>>> 
    >>>> ??? showC <- function(z) noquote(sprintf("(R = %g, I =
    >>>> %g)", Re(z), Im(z)))
    >>>> 
    >>>> we see (in R-devel) quite consistently
    >>>> 
    >>>>> showC(NA_real_ + 0i)
    >>>> [1] (R = NA, I = NA)
    >>>>> showC(NA?????? + 0i)? # NA is 'logical'
    >>>> [1] (R = NA, I = NA) where as in R 4.3.1 and
    >>>> "R-patched" -- *in*consistently
    >>>> 
    >>>>> showC(NA_real_ + 0i)
    >>>> [1] (R = NA, I = 0)
    >>>>> showC(NA + 0i)
    >>>> [1] (R = NA, I = NA) .... and honestly, I do not see
    >>>> *where* (and when) we changed the underlying code (in
    >>>> arithmetic.c !?)? in R-devel to *also* produce?
    >>>> NA_complex_? in such complex *arithmetic*
    >>>> 
    >>>> 
    >>>> ????? > Having said that, one might also (but more
    >>>> naively) expect
    >>>> 
    >>>> ????? >????
    >>>> identical(as.complex(as.double(NA_complex_)),
    >>>> NA_complex_)
    >>>> 
    >>>> ????? > to be TRUE.
    >>>> 
    >>>> as in current R-devel
    >>>> 
    >>>> ????? > Under my proposal it continues to be FALSE.
    >>>> 
    >>>> as in "R-release"
    >>>> 
    >>>> ????? > Well, I'd prefer if it gave FALSE with a
    >>>> warning ????? > "imaginary parts discarded in
    >>>> coercion", but it seems that ????? >
    >>>> as.double(complex(r=a, i=b)) never warns when either of
    >>>> ????? > 'a' and 'b' is NA_real_ or NaN, even where
    >>>> "information" ????? > {nonzero 'b'} is clearly lost ...
    >>>> 
    >>>> The question of *warning* here is related indeed, but I
    >>>> think we should try to look at it only *secondary* to
    >>>> your first proposal.
    >>>> 
    >>>> ????? > Whatever decision is made about
    >>>> as.complex(NA_real_), ????? > maybe these points should
    >>>> be weighed before it becomes part of ????? > R-release
    >>>> ...
    >>>> 
    >>>> ????? > Mikael
    >>>> 
    >>>> Indeed.
    >>>> 
    >>>> Can we please get other opinions / ideas here?
    >>>> 
    >>>> Thank you in advance for your thoughts!  Martin
    >>>> 
    >>>> ---
    >>>> 
    >>>> PS:
    >>>> 
    >>>> ?? Our *print()*ing? of complex NA's ("NA" here meaning
    >>>> NA or NaN) ?? is also unsatisfactory, e.g. in the case
    >>>> where all entries of a ?? vector are NA in the sense of
    >>>> is.na(.), but their ?? Re() and Im() are not all NA: ??
    >>>> ??? showC <- function(z) noquote(sprintf("(R = %g, I =
    >>>> %g)", Re(z), Im(z))) ??? z <- complex(, c(11, NA, NA),
    >>>> c(NA, 99, NA)) ??? z ??? showC(z)
    >>>> 
    >>>> gives
    >>>> 
    >>>> ??? > z ??? [1] NA NA NA ??? > showC(z) ??? [1] (R =
    >>>> 11, I = NA) (R = NA, I = 99) (R = NA, I = NA)
    >>>> 
    >>>> but that (printing of complex) *is* another issue, in
    >>>> which we have the re-opened bugzilla PR#16752 ?????
    >>>> ==>https://bugs.r-project.org/show_bug.cgi?id=16752
    >>>> 
    >>>> on which we also worked during the R Sprint in Warwick
    >>>> three weeks ago, and where I want to commit changes in
    >>>> any case {but think we should change even a bit more
    >>>> than we got to during the Sprint}.
    >>>> 
    >>>> ______________________________________________
    >>>> R-devel at r-project.org? mailing list
    >>>> https://stat.ethz.ch/mailman/listinfo/r-devel
    >>> 
    >> 
    > -- 
    > Herv? Pag?s

    > Bioconductor Core Team hpages.on.github at gmail.com


From j@g@nmn2 @end|ng |rom gm@||@com  Sat Sep 23 18:36:55 2023
From: j@g@nmn2 @end|ng |rom gm@||@com (Mikael Jagan)
Date: Sat, 23 Sep 2023 12:36:55 -0400
Subject: [Rd] Recent changes to as.complex(NA_real_)
In-Reply-To: <25870.60261.24330.115632@stat.math.ethz.ch>
References: <1cf40db9-ea70-483a-a547-270da6926935@gmail.com>
 <25869.28353.595831.807741@stat.math.ethz.ch>
 <4abe101e-55be-8800-31ee-e0414c89740e@gmail.com>
 <57f77592-26ad-43dd-8e36-f9989c915308@gmail.com>
 <6bcad145-a1f9-32a5-c9e8-97041ed2eab7@gmail.com>
 <25870.60261.24330.115632@stat.math.ethz.ch>
Message-ID: <1b3cd95f-3adb-4cde-b7b4-0ccc83864e65@gmail.com>



On 2023-09-23 9:43 am, Martin Maechler wrote:
>>>>>> Herv? Pag?s
>>>>>>      on Fri, 22 Sep 2023 16:55:05 -0700 writes:
> 
>      > The problem is that you have things that are
>      > **semantically** different but look exactly the same:
> 
>      > They look the same:
> 
>      >> x
>      > [1] NA
>      >> y
>      > [1] NA
>      >> z
>      > [1] NA
> 
>      >> is.na(x)
>      > [1] TRUE
>      >> is.na(y)
>      > [1] TRUE
>      >> is.na(z)
>      > [1] TRUE
> 
>      >> str(x)
>      >  ?cplx NA
>      >> str(y)
>      >  ?num NA
>      >> str(z)
>      >  ?cplx NA
> 
>      > but they are semantically different e.g.
> 
>      >> Re(x)
>      > [1] NA
>      >> Re(y)
>      > [1] -0.5? # surprise!
> 
>      >> Im(x)? # surprise!
>      > [1] 2
>      >> Im(z)
>      > [1] NA
> 
>      > so any expression involving Re() or Im() will produce
>      > different results on input that look the same on the
>      > surface.
> 
>      > You can address this either by normalizing the internal
>      > representation of complex NA to always be complex(r=NaN,
>      > i=NA_real_), like for NA_complex_, or by allowing the
>      > infinite variations that are currently allowed and at the
>      > same time making sure that both Re() and Im()? always
>      > return NA_real_ on a complex NA.
> 
>      > My point is that the behavior of complex NA should be
>      > predictable. Right now it's not. Once it's predictable
>      > (with Re() and Im() both returning NA_real_ regardless of
>      > internal representation), then it no longer matters what
>      > kind of complex NA is returned by as.complex(NA_real_),
>      > because they are no onger distinguishable.
> 
>      > H.
> 
>      > On 9/22/23 13:43, Duncan Murdoch wrote:
>      >> Since the result of is.na(x) is the same on each of
>      >> those, I don't see a problem.? As long as that is
>      >> consistent, I don't see a problem. You shouldn't be using
>      >> any other test for NA-ness.? You should never be
>      >> expecting identical() to treat different types as the
>      >> same (e.g.  identical(NA, NA_real_) is FALSE, as it
>      >> should be).? If you are using a different test, that's
>      >> user error.
>      >>
>      >> Duncan Murdoch
>      >>
>      >> On 22/09/2023 2:41 p.m., Herv? Pag?s wrote:
>      >>> We could also question the value of having an infinite
>      >>> number of NA representations in the complex space. For
>      >>> example all these complex values are displayed the same
>      >>> way (as NA), are considered NAs by is.na(), but are not
>      >>> identical or semantically equivalent (from an Re() or
>      >>> Im() point of view):
>      >>>
>      >>> ? ??? NA_real_ + 0i
>      >>>
>      >>> ? ??? complex(r=NA_real_, i=Inf)
>      >>>
>      >>> ? ??? complex(r=2, i=NA_real_)
>      >>>
>      >>> ? ??? complex(r=NaN, i=NA_real_)
>      >>>
>      >>> In other words, using a single representation for
>      >>> complex NA (i.e.  complex(r=NA_real_, i=NA_real_)) would
>      >>> avoid a lot of unnecessary complications and surprises.
>      >>>
>      >>> Once you do that, whether as.complex(NA_real_) should
>      >>> return complex(r=NA_real_, i=0) or complex(r=NA_real_,
>      >>> i=NA_real_) becomes a moot point.
>      >>>
>      >>> Best,
>      >>>
>      >>> H.
> 
> Thank you, Herv?.
> Your proposition is yet another one,
> to declare that all complex NA's should be treated as identical
> (almost/fully?) everywhere.
> 
> This would be a possibility, but I think a drastic one.
> 
> I think there are too many cases, where I want to keep the
> information of the real part independent of the values of the
> imaginary part (e.g. think of the Riemann hypothesis), and
> typically vice versa.
> 
> With your proposal, for a (potentially large) vector of complex numbers,
> after
>        Re(z)  <-  1/2
> 
> I could no longer rely on   Re(z) == 1/2,
> because it would be wrong for those z where (the imaginary part/ the number)
> was NA/NaN.
> Also, in a similar case, a
> 
>        Im(z) <- NA
> 
> would have to "destroy" all real parts  Re(z);
> not really typically in memory, but effectively for the user,  Re(z)
> would be all NA/NaN.
> 
> And I think there are quite a few other situations
> where looking at Re() and Im() separately makes a lot of sense.

Indeed, and there is no way to "tell" BLAS and LAPACK to treat both the real and
imaginary parts as NA_REAL when either is NA_REAL.  Hence the only reliable way
to implement such a proposal would be to post-process the result of any
computation returning a complex type, testing for NA_REAL and setting both parts
to NA_REAL in that case.  My expectation is that such testing would drastically
slow down basic arithmetic and algebraic operations ...

Mikael

> 
> Spencer also made a remark in this direction.
> 
> All in all I'd be very reluctant to move in this direction;
> but yes, I'm just one person ... let's continue musing and
> considering !
> 
> Martin
> 
>      >>> On 9/22/23 03:38, Martin Maechler wrote:
>      >>>>>>>>> Mikael Jagan ????? on Thu, 21 Sep 2023 00:47:39
>      >>>>>>>>> -0400 writes:
>      >>>> ????? > Revisiting this thread from April:
>      >>>>
>      >>>> >https://stat.ethz.ch/pipermail/r-devel/2023-April/082545.html
>      >>>>
>      >>>> ????? > where the decision (not yet backported) was
>      >>>> made for ????? > as.complex(NA_real_) to give
>      >>>> NA_complex_ instead of ????? > complex(r=NA_real_,
>      >>>> i=0), to be consistent with ????? > help("as.complex")
>      >>>> and as.complex(NA) and as.complex(NA_integer_).
>      >>>>
>      >>>> ????? > Was any consideration given to the alternative?
>      >>>> ????? > That is, to changing as.complex(NA) and
>      >>>> as.complex(NA_integer_) to ????? > give
>      >>>> complex(r=NA_real_, i=0), consistent with ????? >
>      >>>> as.complex(NA_real_), then amending help("as.complex")
>      >>>> ????? > accordingly?
>      >>>>
>      >>>> Hmm, as, from R-core, mostly I was involved, I admit to
>      >>>> say "no", to my knowledge the (above) alternative
>      >>>> wasn't considered.
>      >>>>
>      >>>> ??? > The principle that ??? >
>      >>>> Im(as.complex(<real=(double|integer|logical)>)) should
>      >>>> be zero ??? > is quite fundamental, in my view, hence
>      >>>> the "new" behaviour ??? > seems to really violate the
>      >>>> principle of least surprise ...
>      >>>>
>      >>>> of course "least surprise"? is somewhat subjective.
>      >>>> Still, I clearly agree that the above would be one
>      >>>> desirable property.
>      >>>>
>      >>>> I think that any solution will lead to *some* surprise
>      >>>> for some cases, I think primarily because there are
>      >>>> *many* different values z? for which? is.na(z)? is
>      >>>> true,? and in any case NA_complex_? is only of the
>      >>>> many.
>      >>>>
>      >>>> I also agree with Mikael that we should reconsider the
>      >>>> issue that was raised by Davis Vaughan here ("on
>      >>>> R-devel") last April.
>      >>>>
>      >>>> ????? > Another (but maybe weaker) argument is that
>      >>>> ????? > double->complex coercions happen more often
>      >>>> than ????? > logical->complex and integer->complex
>      >>>> ones. Changing the ????? > behaviour of the more
>      >>>> frequently performed coercion is ????? > more likely to
>      >>>> affect code "out there".
>      >>>>
>      >>>> ????? > Yet another argument is that one expects
>      >>>>
>      >>>> ????? >????? identical(as.complex(NA_real_), NA_real_ +
>      >>>> (0+0i))
>      >>>>
>      >>>> ????? > to be TRUE, i.e., that coercing from double to
>      >>>> complex is ????? > equivalent to adding a complex
>      >>>> zero.? The new behaviour ????? > makes the above FALSE,
>      >>>> since NA_real_ + (0+0i) gives ????? >
>      >>>> complex(r=NA_real_, i=0).
>      >>>>
>      >>>> No!? --- To my own surprise (!) --- in current R-devel
>      >>>> the above is TRUE, and ??????? NA_real_ + (0+0i)? , the
>      >>>> same as ??????? NA_real_ + 0i????? , really gives
>      >>>> complex(r=NA, i=NA) :
>      >>>>
>      >>>> Using showC() from ?complex
>      >>>>
>      >>>> ??? showC <- function(z) noquote(sprintf("(R = %g, I =
>      >>>> %g)", Re(z), Im(z)))
>      >>>>
>      >>>> we see (in R-devel) quite consistently
>      >>>>
>      >>>>> showC(NA_real_ + 0i)
>      >>>> [1] (R = NA, I = NA)
>      >>>>> showC(NA?????? + 0i)? # NA is 'logical'
>      >>>> [1] (R = NA, I = NA) where as in R 4.3.1 and
>      >>>> "R-patched" -- *in*consistently
>      >>>>
>      >>>>> showC(NA_real_ + 0i)
>      >>>> [1] (R = NA, I = 0)
>      >>>>> showC(NA + 0i)
>      >>>> [1] (R = NA, I = NA) .... and honestly, I do not see
>      >>>> *where* (and when) we changed the underlying code (in
>      >>>> arithmetic.c !?)? in R-devel to *also* produce
>      >>>> NA_complex_? in such complex *arithmetic*
>      >>>>
>      >>>>
>      >>>> ????? > Having said that, one might also (but more
>      >>>> naively) expect
>      >>>>
>      >>>> ????? >
>      >>>> identical(as.complex(as.double(NA_complex_)),
>      >>>> NA_complex_)
>      >>>>
>      >>>> ????? > to be TRUE.
>      >>>>
>      >>>> as in current R-devel
>      >>>>
>      >>>> ????? > Under my proposal it continues to be FALSE.
>      >>>>
>      >>>> as in "R-release"
>      >>>>
>      >>>> ????? > Well, I'd prefer if it gave FALSE with a
>      >>>> warning ????? > "imaginary parts discarded in
>      >>>> coercion", but it seems that ????? >
>      >>>> as.double(complex(r=a, i=b)) never warns when either of
>      >>>> ????? > 'a' and 'b' is NA_real_ or NaN, even where
>      >>>> "information" ????? > {nonzero 'b'} is clearly lost ...
>      >>>>
>      >>>> The question of *warning* here is related indeed, but I
>      >>>> think we should try to look at it only *secondary* to
>      >>>> your first proposal.
>      >>>>
>      >>>> ????? > Whatever decision is made about
>      >>>> as.complex(NA_real_), ????? > maybe these points should
>      >>>> be weighed before it becomes part of ????? > R-release
>      >>>> ...
>      >>>>
>      >>>> ????? > Mikael
>      >>>>
>      >>>> Indeed.
>      >>>>
>      >>>> Can we please get other opinions / ideas here?
>      >>>>
>      >>>> Thank you in advance for your thoughts!  Martin
>      >>>>
>      >>>> ---
>      >>>>
>      >>>> PS:
>      >>>>
>      >>>> ?? Our *print()*ing? of complex NA's ("NA" here meaning
>      >>>> NA or NaN) ?? is also unsatisfactory, e.g. in the case
>      >>>> where all entries of a ?? vector are NA in the sense of
>      >>>> is.na(.), but their ?? Re() and Im() are not all NA:
>      >>>> ??? showC <- function(z) noquote(sprintf("(R = %g, I =
>      >>>> %g)", Re(z), Im(z))) ??? z <- complex(, c(11, NA, NA),
>      >>>> c(NA, 99, NA)) ??? z ??? showC(z)
>      >>>>
>      >>>> gives
>      >>>>
>      >>>> ??? > z ??? [1] NA NA NA ??? > showC(z) ??? [1] (R =
>      >>>> 11, I = NA) (R = NA, I = 99) (R = NA, I = NA)
>      >>>>
>      >>>> but that (printing of complex) *is* another issue, in
>      >>>> which we have the re-opened bugzilla PR#16752
>      >>>> ==>https://bugs.r-project.org/show_bug.cgi?id=16752
>      >>>>
>      >>>> on which we also worked during the R Sprint in Warwick
>      >>>> three weeks ago, and where I want to commit changes in
>      >>>> any case {but think we should change even a bit more
>      >>>> than we got to during the Sprint}.
>      >>>>
>      >>>> ______________________________________________
>      >>>> R-devel at r-project.org? mailing list
>      >>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>      >>>
>      >>
>      > --
>      > Herv? Pag?s
> 
>      > Bioconductor Core Team hpages.on.github at gmail.com
> 
> 
>


From greg @end|ng |rom w@rne@@net  Sat Sep 23 19:22:35 2023
From: greg @end|ng |rom w@rne@@net (Gregory R. Warnes)
Date: Sat, 23 Sep 2023 13:22:35 -0400
Subject: [Rd] Recent changes to as.complex(NA_real_)
In-Reply-To: <1b3cd95f-3adb-4cde-b7b4-0ccc83864e65@gmail.com>
References: <1b3cd95f-3adb-4cde-b7b4-0ccc83864e65@gmail.com>
Message-ID: <A967E2AD-2AA3-4C01-9541-A8B417DC9AB3@warnes.net>

It sounds like we need to add arguments (with sensible defaults) to complex(), Re(), Im(), is.na.complex() etc to allow the user to specify the desired behavior.

--  
Change your thoughts and you change the world.
--Dr. Norman Vincent Peale

> On Sep 23, 2023, at 12:37 PM, Mikael Jagan <jaganmn2 at gmail.com> wrote:
> 
> ?
> 
> On 2023-09-23 9:43 am, Martin Maechler wrote:
>>>>>>> Herv? Pag?s
>>>>>>>     on Fri, 22 Sep 2023 16:55:05 -0700 writes:
>>     > The problem is that you have things that are
>>     > **semantically** different but look exactly the same:
>>     > They look the same:
>>     >> x
>>     > [1] NA
>>     >> y
>>     > [1] NA
>>     >> z
>>     > [1] NA
>>     >> is.na(x)
>>     > [1] TRUE
>>     >> is.na(y)
>>     > [1] TRUE
>>     >> is.na(z)
>>     > [1] TRUE
>>     >> str(x)
>>     >   cplx NA
>>     >> str(y)
>>     >   num NA
>>     >> str(z)
>>     >   cplx NA
>>     > but they are semantically different e.g.
>>     >> Re(x)
>>     > [1] NA
>>     >> Re(y)
>>     > [1] -0.5  # surprise!
>>     >> Im(x)  # surprise!
>>     > [1] 2
>>     >> Im(z)
>>     > [1] NA
>>     > so any expression involving Re() or Im() will produce
>>     > different results on input that look the same on the
>>     > surface.
>>     > You can address this either by normalizing the internal
>>     > representation of complex NA to always be complex(r=NaN,
>>     > i=NA_real_), like for NA_complex_, or by allowing the
>>     > infinite variations that are currently allowed and at the
>>     > same time making sure that both Re() and Im()  always
>>     > return NA_real_ on a complex NA.
>>     > My point is that the behavior of complex NA should be
>>     > predictable. Right now it's not. Once it's predictable
>>     > (with Re() and Im() both returning NA_real_ regardless of
>>     > internal representation), then it no longer matters what
>>     > kind of complex NA is returned by as.complex(NA_real_),
>>     > because they are no onger distinguishable.
>>     > H.
>>     > On 9/22/23 13:43, Duncan Murdoch wrote:
>>     >> Since the result of is.na(x) is the same on each of
>>     >> those, I don't see a problem.  As long as that is
>>     >> consistent, I don't see a problem. You shouldn't be using
>>     >> any other test for NA-ness.  You should never be
>>     >> expecting identical() to treat different types as the
>>     >> same (e.g.  identical(NA, NA_real_) is FALSE, as it
>>     >> should be).  If you are using a different test, that's
>>     >> user error.
>>     >>
>>     >> Duncan Murdoch
>>     >>
>>     >> On 22/09/2023 2:41 p.m., Herv? Pag?s wrote:
>>     >>> We could also question the value of having an infinite
>>     >>> number of NA representations in the complex space. For
>>     >>> example all these complex values are displayed the same
>>     >>> way (as NA), are considered NAs by is.na(), but are not
>>     >>> identical or semantically equivalent (from an Re() or
>>     >>> Im() point of view):
>>     >>>
>>     >>>       NA_real_ + 0i
>>     >>>
>>     >>>       complex(r=NA_real_, i=Inf)
>>     >>>
>>     >>>       complex(r=2, i=NA_real_)
>>     >>>
>>     >>>       complex(r=NaN, i=NA_real_)
>>     >>>
>>     >>> In other words, using a single representation for
>>     >>> complex NA (i.e.  complex(r=NA_real_, i=NA_real_)) would
>>     >>> avoid a lot of unnecessary complications and surprises.
>>     >>>
>>     >>> Once you do that, whether as.complex(NA_real_) should
>>     >>> return complex(r=NA_real_, i=0) or complex(r=NA_real_,
>>     >>> i=NA_real_) becomes a moot point.
>>     >>>
>>     >>> Best,
>>     >>>
>>     >>> H.
>> Thank you, Herv?.
>> Your proposition is yet another one,
>> to declare that all complex NA's should be treated as identical
>> (almost/fully?) everywhere.
>> This would be a possibility, but I think a drastic one.
>> I think there are too many cases, where I want to keep the
>> information of the real part independent of the values of the
>> imaginary part (e.g. think of the Riemann hypothesis), and
>> typically vice versa.
>> With your proposal, for a (potentially large) vector of complex numbers,
>> after
>>       Re(z)  <-  1/2
>> I could no longer rely on   Re(z) == 1/2,
>> because it would be wrong for those z where (the imaginary part/ the number)
>> was NA/NaN.
>> Also, in a similar case, a
>>       Im(z) <- NA
>> would have to "destroy" all real parts  Re(z);
>> not really typically in memory, but effectively for the user,  Re(z)
>> would be all NA/NaN.
>> And I think there are quite a few other situations
>> where looking at Re() and Im() separately makes a lot of sense.
> 
> Indeed, and there is no way to "tell" BLAS and LAPACK to treat both the real and
> imaginary parts as NA_REAL when either is NA_REAL.  Hence the only reliable way
> to implement such a proposal would be to post-process the result of any
> computation returning a complex type, testing for NA_REAL and setting both parts
> to NA_REAL in that case.  My expectation is that such testing would drastically
> slow down basic arithmetic and algebraic operations ...
> 
> Mikael
> 
>> Spencer also made a remark in this direction.
>> All in all I'd be very reluctant to move in this direction;
>> but yes, I'm just one person ... let's continue musing and
>> considering !
>> Martin
>>     >>> On 9/22/23 03:38, Martin Maechler wrote:
>>     >>>>>>>>> Mikael Jagan       on Thu, 21 Sep 2023 00:47:39
>>     >>>>>>>>> -0400 writes:
>>     >>>>       > Revisiting this thread from April:
>>     >>>>
>>     >>>> >https://stat.ethz.ch/pipermail/r-devel/2023-April/082545.html
>>     >>>>
>>     >>>>       > where the decision (not yet backported) was
>>     >>>> made for       > as.complex(NA_real_) to give
>>     >>>> NA_complex_ instead of       > complex(r=NA_real_,
>>     >>>> i=0), to be consistent with       > help("as.complex")
>>     >>>> and as.complex(NA) and as.complex(NA_integer_).
>>     >>>>
>>     >>>>       > Was any consideration given to the alternative?
>>     >>>>       > That is, to changing as.complex(NA) and
>>     >>>> as.complex(NA_integer_) to       > give
>>     >>>> complex(r=NA_real_, i=0), consistent with       >
>>     >>>> as.complex(NA_real_), then amending help("as.complex")
>>     >>>>       > accordingly?
>>     >>>>
>>     >>>> Hmm, as, from R-core, mostly I was involved, I admit to
>>     >>>> say "no", to my knowledge the (above) alternative
>>     >>>> wasn't considered.
>>     >>>>
>>     >>>>     > The principle that     >
>>     >>>> Im(as.complex(<real=(double|integer|logical)>)) should
>>     >>>> be zero     > is quite fundamental, in my view, hence
>>     >>>> the "new" behaviour     > seems to really violate the
>>     >>>> principle of least surprise ...
>>     >>>>
>>     >>>> of course "least surprise"  is somewhat subjective.
>>     >>>> Still, I clearly agree that the above would be one
>>     >>>> desirable property.
>>     >>>>
>>     >>>> I think that any solution will lead to *some* surprise
>>     >>>> for some cases, I think primarily because there are
>>     >>>> *many* different values z  for which  is.na(z)  is
>>     >>>> true,  and in any case NA_complex_  is only of the
>>     >>>> many.
>>     >>>>
>>     >>>> I also agree with Mikael that we should reconsider the
>>     >>>> issue that was raised by Davis Vaughan here ("on
>>     >>>> R-devel") last April.
>>     >>>>
>>     >>>>       > Another (but maybe weaker) argument is that
>>     >>>>       > double->complex coercions happen more often
>>     >>>> than       > logical->complex and integer->complex
>>     >>>> ones. Changing the       > behaviour of the more
>>     >>>> frequently performed coercion is       > more likely to
>>     >>>> affect code "out there".
>>     >>>>
>>     >>>>       > Yet another argument is that one expects
>>     >>>>
>>     >>>>       >      identical(as.complex(NA_real_), NA_real_ +
>>     >>>> (0+0i))
>>     >>>>
>>     >>>>       > to be TRUE, i.e., that coercing from double to
>>     >>>> complex is       > equivalent to adding a complex
>>     >>>> zero.  The new behaviour       > makes the above FALSE,
>>     >>>> since NA_real_ + (0+0i) gives       >
>>     >>>> complex(r=NA_real_, i=0).
>>     >>>>
>>     >>>> No!  --- To my own surprise (!) --- in current R-devel
>>     >>>> the above is TRUE, and         NA_real_ + (0+0i)  , the
>>     >>>> same as         NA_real_ + 0i      , really gives
>>     >>>> complex(r=NA, i=NA) :
>>     >>>>
>>     >>>> Using showC() from ?complex
>>     >>>>
>>     >>>>     showC <- function(z) noquote(sprintf("(R = %g, I =
>>     >>>> %g)", Re(z), Im(z)))
>>     >>>>
>>     >>>> we see (in R-devel) quite consistently
>>     >>>>
>>     >>>>> showC(NA_real_ + 0i)
>>     >>>> [1] (R = NA, I = NA)
>>     >>>>> showC(NA       + 0i)  # NA is 'logical'
>>     >>>> [1] (R = NA, I = NA) where as in R 4.3.1 and
>>     >>>> "R-patched" -- *in*consistently
>>     >>>>
>>     >>>>> showC(NA_real_ + 0i)
>>     >>>> [1] (R = NA, I = 0)
>>     >>>>> showC(NA + 0i)
>>     >>>> [1] (R = NA, I = NA) .... and honestly, I do not see
>>     >>>> *where* (and when) we changed the underlying code (in
>>     >>>> arithmetic.c !?)  in R-devel to *also* produce
>>     >>>> NA_complex_  in such complex *arithmetic*
>>     >>>>
>>     >>>>
>>     >>>>       > Having said that, one might also (but more
>>     >>>> naively) expect
>>     >>>>
>>     >>>>       >
>>     >>>> identical(as.complex(as.double(NA_complex_)),
>>     >>>> NA_complex_)
>>     >>>>
>>     >>>>       > to be TRUE.
>>     >>>>
>>     >>>> as in current R-devel
>>     >>>>
>>     >>>>       > Under my proposal it continues to be FALSE.
>>     >>>>
>>     >>>> as in "R-release"
>>     >>>>
>>     >>>>       > Well, I'd prefer if it gave FALSE with a
>>     >>>> warning       > "imaginary parts discarded in
>>     >>>> coercion", but it seems that       >
>>     >>>> as.double(complex(r=a, i=b)) never warns when either of
>>     >>>>       > 'a' and 'b' is NA_real_ or NaN, even where
>>     >>>> "information"       > {nonzero 'b'} is clearly lost ...
>>     >>>>
>>     >>>> The question of *warning* here is related indeed, but I
>>     >>>> think we should try to look at it only *secondary* to
>>     >>>> your first proposal.
>>     >>>>
>>     >>>>       > Whatever decision is made about
>>     >>>> as.complex(NA_real_),       > maybe these points should
>>     >>>> be weighed before it becomes part of       > R-release
>>     >>>> ...
>>     >>>>
>>     >>>>       > Mikael
>>     >>>>
>>     >>>> Indeed.
>>     >>>>
>>     >>>> Can we please get other opinions / ideas here?
>>     >>>>
>>     >>>> Thank you in advance for your thoughts!  Martin
>>     >>>>
>>     >>>> ---
>>     >>>>
>>     >>>> PS:
>>     >>>>
>>     >>>>    Our *print()*ing  of complex NA's ("NA" here meaning
>>     >>>> NA or NaN)    is also unsatisfactory, e.g. in the case
>>     >>>> where all entries of a    vector are NA in the sense of
>>     >>>> is.na(.), but their    Re() and Im() are not all NA:
>>     >>>>     showC <- function(z) noquote(sprintf("(R = %g, I =
>>     >>>> %g)", Re(z), Im(z)))     z <- complex(, c(11, NA, NA),
>>     >>>> c(NA, 99, NA))     z     showC(z)
>>     >>>>
>>     >>>> gives
>>     >>>>
>>     >>>>     > z     [1] NA NA NA     > showC(z)     [1] (R =
>>     >>>> 11, I = NA) (R = NA, I = 99) (R = NA, I = NA)
>>     >>>>
>>     >>>> but that (printing of complex) *is* another issue, in
>>     >>>> which we have the re-opened bugzilla PR#16752
>>     >>>> ==>https://bugs.r-project.org/show_bug.cgi?id=16752
>>     >>>>
>>     >>>> on which we also worked during the R Sprint in Warwick
>>     >>>> three weeks ago, and where I want to commit changes in
>>     >>>> any case {but think we should change even a bit more
>>     >>>> than we got to during the Sprint}.
>>     >>>>
>>     >>>> ______________________________________________
>>     >>>> R-devel at r-project.org  mailing list
>>     >>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>     >>>
>>     >>
>>     > --
>>     > Herv? Pag?s
>>     > Bioconductor Core Team hpages.on.github at gmail.com
>> 
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

	[[alternative HTML version deleted]]


From @g|@nner|n| @end|ng |rom gm@||@com  Sat Sep 23 19:43:35 2023
From: @g|@nner|n| @end|ng |rom gm@||@com (Simone Giannerini)
Date: Sat, 23 Sep 2023 19:43:35 +0200
Subject: [Rd] NROW and NCOL on NULL
Message-ID: <CANcXGizv3sD3kiE__EUkuaESn44mvtQMrtpO_k=FjUwFQDGnVQ@mail.gmail.com>

Dear list,

I do not know what would be the 'correct' answer to the following but
I think that they should return the same value to avoid potential
problems and hard to debug errors.

Regards,

Simone
---------------------------------------

> NCOL(NULL)
[1] 1

> NROW(NULL)
[1] 0

> sessionInfo()
R version 4.3.1 RC (2023-06-08 r84523 ucrt)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 11 x64 (build 22621)

Matrix products: default


locale:
[1] LC_COLLATE=Italian_Italy.utf8  LC_CTYPE=Italian_Italy.utf8
[3] LC_MONETARY=Italian_Italy.utf8 LC_NUMERIC=C
[5] LC_TIME=Italian_Italy.utf8

time zone: Europe/Rome
tzcode source: internal

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

loaded via a namespace (and not attached):
[1] compiler_4.3.1

-- 
___________________________________________________

Simone Giannerini
Dipartimento di Scienze Statistiche "Paolo Fortunati"
Universita' di Bologna
Via delle belle arti 41 - 40126  Bologna,  ITALY
Tel: +39 051 2098262  Fax: +39 051 232153
https://simonegiannerini.net/


From murdoch@dunc@n @end|ng |rom gm@||@com  Sat Sep 23 19:50:14 2023
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Sat, 23 Sep 2023 13:50:14 -0400
Subject: [Rd] NROW and NCOL on NULL
In-Reply-To: <CANcXGizv3sD3kiE__EUkuaESn44mvtQMrtpO_k=FjUwFQDGnVQ@mail.gmail.com>
References: <CANcXGizv3sD3kiE__EUkuaESn44mvtQMrtpO_k=FjUwFQDGnVQ@mail.gmail.com>
Message-ID: <936870db-2e16-4b8b-878b-c2dde317319c@gmail.com>

It's been documented for a long time that NCOL(NULL) is 1.  What 
particular problems did you have in mind?  There might be other ways to 
guard against them.

Duncan Murdoch

On 23/09/2023 1:43 p.m., Simone Giannerini wrote:
> Dear list,
> 
> I do not know what would be the 'correct' answer to the following but
> I think that they should return the same value to avoid potential
> problems and hard to debug errors.
> 
> Regards,
> 
> Simone
> ---------------------------------------
> 
>> NCOL(NULL)
> [1] 1
> 
>> NROW(NULL)
> [1] 0
> 
>> sessionInfo()
> R version 4.3.1 RC (2023-06-08 r84523 ucrt)
> Platform: x86_64-w64-mingw32/x64 (64-bit)
> Running under: Windows 11 x64 (build 22621)
> 
> Matrix products: default
> 
> 
> locale:
> [1] LC_COLLATE=Italian_Italy.utf8  LC_CTYPE=Italian_Italy.utf8
> [3] LC_MONETARY=Italian_Italy.utf8 LC_NUMERIC=C
> [5] LC_TIME=Italian_Italy.utf8
> 
> time zone: Europe/Rome
> tzcode source: internal
> 
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
> 
> loaded via a namespace (and not attached):
> [1] compiler_4.3.1
>


From murdoch@dunc@n @end|ng |rom gm@||@com  Sat Sep 23 20:44:00 2023
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Sat, 23 Sep 2023 14:44:00 -0400
Subject: [Rd] Help requested: writing text to a raster in memory
Message-ID: <35a7f4df-98df-4216-ac67-6d22df1d705e@gmail.com>

I am in the process of updating the rgl package.  One thing I'd like to 
do is to change text support in it when using OpenGL to display to be 
more like the way text is drawn in WebGL displays (i.e. the ones 
rglwidget() produces).

Currently in R, rgl uses the FTGL library to draw text.  That library is 
unsupported these days, and uses the old fixed pipeline in OpenGL.

In WebGL, text is displayed by "shaders", programs that run on the GPU. 
Javascript code prepares bitmap images of the text to display, then the 
shader transfers parts of that bitmap to the output display.

I'd like to duplicate the WebGL process in the C++ code running the 
OpenGL display in R.  The first step in this is to render a character 
vector full of text into an in-memory raster, taking account of font, 
cex, etc.  (I want one raster for the whole vector, with a recording of 
locations from which the shader should get each component of it.)

It looks to me as though I could do this using the ragg::agg_capture 
device in R code, but I'd prefer to do it all in C++ code because I may 
need to make changes to the raster at times when it's not safe to call 
back to R, e.g. if some user interaction requires the axis labels to be 
recomputed and redrawn.

Does anyone with experience doing this kind of thing know of examples I 
can follow, or have advice on how to proceed?  Or want to volunteer to 
help with this?

Duncan Murdoch


From @g|@nner|n| @end|ng |rom gm@||@com  Sat Sep 23 21:41:16 2023
From: @g|@nner|n| @end|ng |rom gm@||@com (Simone Giannerini)
Date: Sat, 23 Sep 2023 21:41:16 +0200
Subject: [Rd] NROW and NCOL on NULL
In-Reply-To: <936870db-2e16-4b8b-878b-c2dde317319c@gmail.com>
References: <CANcXGizv3sD3kiE__EUkuaESn44mvtQMrtpO_k=FjUwFQDGnVQ@mail.gmail.com>
 <936870db-2e16-4b8b-878b-c2dde317319c@gmail.com>
Message-ID: <CANcXGiz9g6mAsnNd3=Uc9iQ5xvE8Mg2ws5fbsEFYVXB_HL=Sog@mail.gmail.com>

I know it's documented and I know there are other ways to guard
against this behaviour, once you know about this.
The point is whether it might be worth it to make NCOL and NROW return
the same value on NULL and make R more consistent/intuitive and
possibly less error prone.

Regards,

Simone

On Sat, Sep 23, 2023 at 7:50?PM Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
>
> It's been documented for a long time that NCOL(NULL) is 1.  What
> particular problems did you have in mind?  There might be other ways to
> guard against them.
>
> Duncan Murdoch
>
> On 23/09/2023 1:43 p.m., Simone Giannerini wrote:
> > Dear list,
> >
> > I do not know what would be the 'correct' answer to the following but
> > I think that they should return the same value to avoid potential
> > problems and hard to debug errors.
> >
> > Regards,
> >
> > Simone
> > ---------------------------------------
> >
> >> NCOL(NULL)
> > [1] 1
> >
> >> NROW(NULL)
> > [1] 0
> >
> >> sessionInfo()
> > R version 4.3.1 RC (2023-06-08 r84523 ucrt)
> > Platform: x86_64-w64-mingw32/x64 (64-bit)
> > Running under: Windows 11 x64 (build 22621)
> >
> > Matrix products: default
> >
> >
> > locale:
> > [1] LC_COLLATE=Italian_Italy.utf8  LC_CTYPE=Italian_Italy.utf8
> > [3] LC_MONETARY=Italian_Italy.utf8 LC_NUMERIC=C
> > [5] LC_TIME=Italian_Italy.utf8
> >
> > time zone: Europe/Rome
> > tzcode source: internal
> >
> > attached base packages:
> > [1] stats     graphics  grDevices utils     datasets  methods   base
> >
> > loaded via a namespace (and not attached):
> > [1] compiler_4.3.1
> >
>


-- 
___________________________________________________

Simone Giannerini
Dipartimento di Scienze Statistiche "Paolo Fortunati"
Universita' di Bologna
Via delle belle arti 41 - 40126  Bologna,  ITALY
Tel: +39 051 2098262  Fax: +39 051 232153
https://simonegiannerini.net/


From bbo|ker @end|ng |rom gm@||@com  Sat Sep 23 21:51:17 2023
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Sat, 23 Sep 2023 15:51:17 -0400
Subject: [Rd] NROW and NCOL on NULL
In-Reply-To: <CANcXGiz9g6mAsnNd3=Uc9iQ5xvE8Mg2ws5fbsEFYVXB_HL=Sog@mail.gmail.com>
References: <CANcXGizv3sD3kiE__EUkuaESn44mvtQMrtpO_k=FjUwFQDGnVQ@mail.gmail.com>
 <936870db-2e16-4b8b-878b-c2dde317319c@gmail.com>
 <CANcXGiz9g6mAsnNd3=Uc9iQ5xvE8Mg2ws5fbsEFYVXB_HL=Sog@mail.gmail.com>
Message-ID: <122c7b31-ee47-9b1f-0baf-d792f29cbfdf@gmail.com>

    This is certainly worth discussing, but there's always a heavy 
burden of back-compatibility; how much better would it be for NCOL and 
NROW to both return zero, vs. the amount of old code that would be broken?

   Furthermore, the reason for this behaviour is justified as 
consistency with the behaviour of as.matrix() and cbind() for 
zero-length vectors, from ?NCOL:

      ## as.matrix() produces 1-column matrices from 0-length vectors,
      ## and so does cbind() :

  (of course you could argue that this behaviour should be changed as 
well ...)


On 2023-09-23 3:41 p.m., Simone Giannerini wrote:
> I know it's documented and I know there are other ways to guard
> against this behaviour, once you know about this.
> The point is whether it might be worth it to make NCOL and NROW return
> the same value on NULL and make R more consistent/intuitive and
> possibly less error prone.
> 
> Regards,
> 
> Simone
> 
> On Sat, Sep 23, 2023 at 7:50?PM Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
>>
>> It's been documented for a long time that NCOL(NULL) is 1.  What
>> particular problems did you have in mind?  There might be other ways to
>> guard against them.
>>
>> Duncan Murdoch
>>
>> On 23/09/2023 1:43 p.m., Simone Giannerini wrote:
>>> Dear list,
>>>
>>> I do not know what would be the 'correct' answer to the following but
>>> I think that they should return the same value to avoid potential
>>> problems and hard to debug errors.
>>>
>>> Regards,
>>>
>>> Simone
>>> ---------------------------------------
>>>
>>>> NCOL(NULL)
>>> [1] 1
>>>
>>>> NROW(NULL)
>>> [1] 0
>>>
>>>> sessionInfo()
>>> R version 4.3.1 RC (2023-06-08 r84523 ucrt)
>>> Platform: x86_64-w64-mingw32/x64 (64-bit)
>>> Running under: Windows 11 x64 (build 22621)
>>>
>>> Matrix products: default
>>>
>>>
>>> locale:
>>> [1] LC_COLLATE=Italian_Italy.utf8  LC_CTYPE=Italian_Italy.utf8
>>> [3] LC_MONETARY=Italian_Italy.utf8 LC_NUMERIC=C
>>> [5] LC_TIME=Italian_Italy.utf8
>>>
>>> time zone: Europe/Rome
>>> tzcode source: internal
>>>
>>> attached base packages:
>>> [1] stats     graphics  grDevices utils     datasets  methods   base
>>>
>>> loaded via a namespace (and not attached):
>>> [1] compiler_4.3.1
>>>
>>
> 
>


From kry|ov@r00t @end|ng |rom gm@||@com  Sat Sep 23 22:43:47 2023
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Sat, 23 Sep 2023 23:43:47 +0300
Subject: [Rd] proposal: 'dev.capabilities()' can also query Unicode
 capabilities of current graphics device
In-Reply-To: <25866.52214.850601.243688@stat.math.ethz.ch>
References: <CAMigB8EaoRNacP0Z4QUMXjegnks1jCOrx2eN5Z-EK63wtqsLmA@mail.gmail.com>
 <25866.52214.850601.243688@stat.math.ethz.ch>
Message-ID: <20230923234347.27eb5def@parabola>

On Wed, 20 Sep 2023 12:39:50 +0200
Martin Maechler <maechler at stat.math.ethz.ch> wrote:

> The problem is that some pdf *viewers*,
> notably `evince` on Fedora Linux, for several years now,
> do *not* show *some* of the UTF-8 glyphs because they do not use
> the correct fonts 

One more problem that makes it nontrivial to use Unicode with pdf() is
the graphics device not knowing some of the font metrics:

x <- '\u410\u411\u412'
pdf()
plot(1:10, main = x)
# Warning messages:
# 1: In title(...) : font width unknown for character 0xb0
# 2: In title(...) : font width unknown for character 0xe4
# 3: In title(...) : font width unknown for character 0xfc
# 4: In title(...) : font width unknown for character 0x7f
dev.off()

In the resulting PDF file, the three letters are visible, at least in
Evince 3.38.2, but they are all positioned in the same space.

I understand that this is strictly speaking not pdf()'s fault
(grDevices contains the font metrics for all standard Adobe fonts and a
few more), but I'm not sure what to do as a user. Should I call
pdfFonts(...), declaring a font with all symbols I need? Where does one
even get Type-1 Cyrillic Helvetica (or any other font) with separate
font metrics files for use with pdf()?

Actually, the wrong number of sometimes random character codes reminds
me of stack garbage. In src/library/grDevices/src/devPS.c, function
static double PostScriptStringWidth, there's this bit of code:

	if(!strIsASCII((char *) str) &&
	   /*
	    * Every fifth font is a symbol font:
	    * see postscriptFonts()
	    */
	   (face % 5) != 0) {
	    R_CheckStack2(strlen((char *)str)+1);
	    char buff[strlen((char *)str)+1];
	    /* Output string cannot be longer */
	    mbcsToSbcs((char *)str, buff, encoding, enc);
	    str1 = (unsigned char *)buff;
	}

Later the characters in str1 are iterated over in order to calculate
the total width of the string. I didn't notice this myself until I saw
in the debugger that after a few iterations of the loop, the contents
of str1 are completely different from the result of mbcsToSbcs((char
*)str, buff, encoding, enc), and went to investigate. Only after the
debugger told me that there's no variable called "buff" I realised that
the VLA pointed to by str1 no longer exists.

--- src/library/grDevices/src/devPS.c	(revision 85214)
+++ src/library/grDevices/src/devPS.c	(working copy)
@@ -721,6 +721,8 @@
     unsigned char p1, p2;
 
     int status;
+    /* May be about to allocate */
+    void *alloc = vmaxget();
     if(!metrics && (face % 5) != 0) {
 	/* This is the CID font case, and should only happen for
 	   non-symbol fonts.  So we assume monospaced with multipliers.
@@ -755,9 +757,8 @@
 	    * Every fifth font is a symbol font:
 	    * see postscriptFonts()
 	    */
-	   (face % 5) != 0) {
-	    R_CheckStack2(strlen((char *)str)+1);
-	    char buff[strlen((char *)str)+1];
+	   (face % 5) != 0 && metrics) {
+	    char *buff = R_alloc(strlen((char *)str)+1, 1);
 	    /* Output string cannot be longer */
 	    mbcsToSbcs((char *)str, buff, encoding, enc);
 	    str1 = (unsigned char *)buff;
@@ -792,6 +793,7 @@
 		}
 	}
     }
+    vmaxset(alloc);
     return 0.001 * sum;
 }
 

 
After this patch, I'm consistently getting the right character codes in
the warnings, but I still don't know how to set up the font metrics.

-- 
Best regards,
Ivan


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Sat Sep 23 23:38:29 2023
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Sat, 23 Sep 2023 22:38:29 +0100
Subject: [Rd] NROW and NCOL on NULL
In-Reply-To: <CANcXGiz9g6mAsnNd3=Uc9iQ5xvE8Mg2ws5fbsEFYVXB_HL=Sog@mail.gmail.com>
References: <CANcXGizv3sD3kiE__EUkuaESn44mvtQMrtpO_k=FjUwFQDGnVQ@mail.gmail.com>
 <936870db-2e16-4b8b-878b-c2dde317319c@gmail.com>
 <CANcXGiz9g6mAsnNd3=Uc9iQ5xvE8Mg2ws5fbsEFYVXB_HL=Sog@mail.gmail.com>
Message-ID: <b5869e21-9b54-4500-9c18-7bd64395aa57@sapo.pt>

?s 20:41 de 23/09/2023, Simone Giannerini escreveu:
> I know it's documented and I know there are other ways to guard
> against this behaviour, once you know about this.
> The point is whether it might be worth it to make NCOL and NROW return
> the same value on NULL and make R more consistent/intuitive and
> possibly less error prone.
> 
> Regards,
> 
> Simone
> 
> On Sat, Sep 23, 2023 at 7:50?PM Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
>>
>> It's been documented for a long time that NCOL(NULL) is 1.  What
>> particular problems did you have in mind?  There might be other ways to
>> guard against them.
>>
>> Duncan Murdoch
>>
>> On 23/09/2023 1:43 p.m., Simone Giannerini wrote:
>>> Dear list,
>>>
>>> I do not know what would be the 'correct' answer to the following but
>>> I think that they should return the same value to avoid potential
>>> problems and hard to debug errors.
>>>
>>> Regards,
>>>
>>> Simone
>>> ---------------------------------------
>>>
>>>> NCOL(NULL)
>>> [1] 1
>>>
>>>> NROW(NULL)
>>> [1] 0
>>>
>>>> sessionInfo()
>>> R version 4.3.1 RC (2023-06-08 r84523 ucrt)
>>> Platform: x86_64-w64-mingw32/x64 (64-bit)
>>> Running under: Windows 11 x64 (build 22621)
>>>
>>> Matrix products: default
>>>
>>>
>>> locale:
>>> [1] LC_COLLATE=Italian_Italy.utf8  LC_CTYPE=Italian_Italy.utf8
>>> [3] LC_MONETARY=Italian_Italy.utf8 LC_NUMERIC=C
>>> [5] LC_TIME=Italian_Italy.utf8
>>>
>>> time zone: Europe/Rome
>>> tzcode source: internal
>>>
>>> attached base packages:
>>> [1] stats     graphics  grDevices utils     datasets  methods   base
>>>
>>> loaded via a namespace (and not attached):
>>> [1] compiler_4.3.1
>>>
>>
> 
> 
Hello,

The way I think of this behavior that made it intuitive is to think that 
in R matrices are column-major, therefore if length(NULL) == 0 then NULL 
can be seen as a matrix with one column and zero rows, no data.

Here are other examples for which that reasoning works:


m <- matrix(integer(0L))
NCOL(m)
NROW(m)

x <- integer(0L)
NCOL(x)
NROW(x)


Not very convincing? Maybe with time...

Hope this helps,

Rui Barradas


From murdoch@dunc@n @end|ng |rom gm@||@com  Sat Sep 23 23:46:18 2023
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Sat, 23 Sep 2023 17:46:18 -0400
Subject: [Rd] NROW and NCOL on NULL
In-Reply-To: <CANcXGiz9g6mAsnNd3=Uc9iQ5xvE8Mg2ws5fbsEFYVXB_HL=Sog@mail.gmail.com>
References: <CANcXGizv3sD3kiE__EUkuaESn44mvtQMrtpO_k=FjUwFQDGnVQ@mail.gmail.com>
 <936870db-2e16-4b8b-878b-c2dde317319c@gmail.com>
 <CANcXGiz9g6mAsnNd3=Uc9iQ5xvE8Mg2ws5fbsEFYVXB_HL=Sog@mail.gmail.com>
Message-ID: <6953a2a1-20df-47ba-885a-058f364d11b2@gmail.com>

On 23/09/2023 3:41 p.m., Simone Giannerini wrote:
> I know it's documented and I know there are other ways to guard
> against this behaviour, once you know about this.
> The point is whether it might be worth it to make NCOL and NROW return
> the same value on NULL and make R more consistent/intuitive and
> possibly less error prone.

If you don't list any examples of problems, then the only possible 
conclusion is that there aren't any except obscure ones, so the answer 
is clearly that it is not worth it to make this change.

Duncan Murdoch

> 
> Regards,
> 
> Simone
> 
> On Sat, Sep 23, 2023 at 7:50?PM Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
>>
>> It's been documented for a long time that NCOL(NULL) is 1.  What
>> particular problems did you have in mind?  There might be other ways to
>> guard against them.
>>
>> Duncan Murdoch
>>
>> On 23/09/2023 1:43 p.m., Simone Giannerini wrote:
>>> Dear list,
>>>
>>> I do not know what would be the 'correct' answer to the following but
>>> I think that they should return the same value to avoid potential
>>> problems and hard to debug errors.
>>>
>>> Regards,
>>>
>>> Simone
>>> ---------------------------------------
>>>
>>>> NCOL(NULL)
>>> [1] 1
>>>
>>>> NROW(NULL)
>>> [1] 0
>>>
>>>> sessionInfo()
>>> R version 4.3.1 RC (2023-06-08 r84523 ucrt)
>>> Platform: x86_64-w64-mingw32/x64 (64-bit)
>>> Running under: Windows 11 x64 (build 22621)
>>>
>>> Matrix products: default
>>>
>>>
>>> locale:
>>> [1] LC_COLLATE=Italian_Italy.utf8  LC_CTYPE=Italian_Italy.utf8
>>> [3] LC_MONETARY=Italian_Italy.utf8 LC_NUMERIC=C
>>> [5] LC_TIME=Italian_Italy.utf8
>>>
>>> time zone: Europe/Rome
>>> tzcode source: internal
>>>
>>> attached base packages:
>>> [1] stats     graphics  grDevices utils     datasets  methods   base
>>>
>>> loaded via a namespace (and not attached):
>>> [1] compiler_4.3.1
>>>
>>
> 
>


From hp@ge@@on@g|thub @end|ng |rom gm@||@com  Sun Sep 24 01:52:21 2023
From: hp@ge@@on@g|thub @end|ng |rom gm@||@com (=?UTF-8?B?SGVydsOpIFBhZ8Oocw==?=)
Date: Sat, 23 Sep 2023 16:52:21 -0700
Subject: [Rd] Recent changes to as.complex(NA_real_)
In-Reply-To: <25870.60261.24330.115632@stat.math.ethz.ch>
References: <1cf40db9-ea70-483a-a547-270da6926935@gmail.com>
 <25869.28353.595831.807741@stat.math.ethz.ch>
 <4abe101e-55be-8800-31ee-e0414c89740e@gmail.com>
 <57f77592-26ad-43dd-8e36-f9989c915308@gmail.com>
 <6bcad145-a1f9-32a5-c9e8-97041ed2eab7@gmail.com>
 <25870.60261.24330.115632@stat.math.ethz.ch>
Message-ID: <52c848e6-80d5-6514-6926-95804ec7562f@gmail.com>

Hi Martin,

On 9/23/23 06:43, Martin Maechler wrote:
>>>>>> Herv? Pag?s
>>>>>>      on Fri, 22 Sep 2023 16:55:05 -0700 writes:
>      > The problem is that you have things that are
>      > **semantically** different but look exactly the same:
>
>      > They look the same:
>
>      >> x
>      > [1] NA
>      >> y
>      > [1] NA
>      >> z
>      > [1] NA
>
>      >> is.na(x)
>      > [1] TRUE
>      >> is.na(y)
>      > [1] TRUE
>      >> is.na(z)
>      > [1] TRUE
>
>      >> str(x)
>      >  ?cplx NA
>      >> str(y)
>      >  ?num NA
>      >> str(z)
>      >  ?cplx NA
>
>      > but they are semantically different e.g.
>
>      >> Re(x)
>      > [1] NA
>      >> Re(y)
>      > [1] -0.5? # surprise!
>
>      >> Im(x)? # surprise!
>      > [1] 2
>      >> Im(z)
>      > [1] NA
>
>      > so any expression involving Re() or Im() will produce
>      > different results on input that look the same on the
>      > surface.
>
>      > You can address this either by normalizing the internal
>      > representation of complex NA to always be complex(r=NaN,
>      > i=NA_real_), like for NA_complex_, or by allowing the
>      > infinite variations that are currently allowed and at the
>      > same time making sure that both Re() and Im()? always
>      > return NA_real_ on a complex NA.
>
>      > My point is that the behavior of complex NA should be
>      > predictable. Right now it's not. Once it's predictable
>      > (with Re() and Im() both returning NA_real_ regardless of
>      > internal representation), then it no longer matters what
>      > kind of complex NA is returned by as.complex(NA_real_),
>      > because they are no onger distinguishable.
>
>      > H.
>
>      > On 9/22/23 13:43, Duncan Murdoch wrote:
>      >> Since the result of is.na(x) is the same on each of
>      >> those, I don't see a problem.? As long as that is
>      >> consistent, I don't see a problem. You shouldn't be using
>      >> any other test for NA-ness.? You should never be
>      >> expecting identical() to treat different types as the
>      >> same (e.g.  identical(NA, NA_real_) is FALSE, as it
>      >> should be).? If you are using a different test, that's
>      >> user error.
>      >>
>      >> Duncan Murdoch
>      >>
>      >> On 22/09/2023 2:41 p.m., Herv? Pag?s wrote:
>      >>> We could also question the value of having an infinite
>      >>> number of NA representations in the complex space. For
>      >>> example all these complex values are displayed the same
>      >>> way (as NA), are considered NAs by is.na(), but are not
>      >>> identical or semantically equivalent (from an Re() or
>      >>> Im() point of view):
>      >>>
>      >>> ? ??? NA_real_ + 0i
>      >>>
>      >>> ? ??? complex(r=NA_real_, i=Inf)
>      >>>
>      >>> ? ??? complex(r=2, i=NA_real_)
>      >>>
>      >>> ? ??? complex(r=NaN, i=NA_real_)
>      >>>
>      >>> In other words, using a single representation for
>      >>> complex NA (i.e.  complex(r=NA_real_, i=NA_real_)) would
>      >>> avoid a lot of unnecessary complications and surprises.
>      >>>
>      >>> Once you do that, whether as.complex(NA_real_) should
>      >>> return complex(r=NA_real_, i=0) or complex(r=NA_real_,
>      >>> i=NA_real_) becomes a moot point.
>      >>>
>      >>> Best,
>      >>>
>      >>> H.
>
> Thank you, Herv?.
> Your proposition is yet another one,
> to declare that all complex NA's should be treated as identical
> (almost/fully?) everywhere.
>
> This would be a possibility, but I think a drastic one.
>
> I think there are too many cases, where I want to keep the
> information of the real part independent of the values of the
> imaginary part (e.g. think of the Riemann hypothesis), and
> typically vice versa.
Use NaN for that, not NA.
>
> With your proposal, for a (potentially large) vector of complex numbers,
> after
>        Re(z)  <-  1/2
>
> I could no longer rely on   Re(z) == 1/2,
> because it would be wrong for those z where (the imaginary part/ the number)
> was NA/NaN.

My proposal is to do this only if the Re and/or Im parts are NAs, not if 
they are NaNs.

BTW the difference between how NAs and NaNs are treated in complex 
vectors is another issue that adds to the confusion:

 ? > complex(r=NA, i=2)
 ? [1] NA
 ? > complex(r=NaN, i=2)
 ? [1] NaN+2i

Not displaying the real + imaginary parts in the NA case kind of 
suggests that somehow they are gone i.e. that Re(z) and Im(z) are both NA.

Note that my proposal is not to change the display but to change Re() 
and Im() to make them consistent with the display.

In your Re(z) <- 1/2 example (which seems to be theoretical only because 
I don't see `Re<-` in base R), any NA in 'z' would be replaced with 
complex(r=NaN, i=1/2), so you could rely on Re(z) == 1/2.

> Also, in a similar case, a
>
>        Im(z) <- NA
>
> would have to "destroy" all real parts  Re(z);
> not really typically in memory, but effectively for the user,  Re(z)
> would be all NA/NaN.
Yes, setting a value to NA destroys it beyond repair in the sense that 
there's no way you can retrieve any original parts of it. I'm fine with 
that. I'm not fine with an NA being used to store hidden information.
>
> And I think there are quite a few other situations
> where looking at Re() and Im() separately makes a lot of sense.
Still doable if the Re or Im parts contain NaNs.
>
> Spencer also made a remark in this direction.
>
> All in all I'd be very reluctant to move in this direction;
> but yes, I'm just one person ... let's continue musing and
> considering !

I understand the reluctance since this would not be a light move, but 
thanks for considering.

Best,

H.

>
> Martin
>
>      >>> On 9/22/23 03:38, Martin Maechler wrote:
>      >>>>>>>>> Mikael Jagan ????? on Thu, 21 Sep 2023 00:47:39
>      >>>>>>>>> -0400 writes:
>      >>>> ????? > Revisiting this thread from April:
>      >>>>
>      >>>> >https://stat.ethz.ch/pipermail/r-devel/2023-April/082545.html
>      >>>>
>      >>>> ????? > where the decision (not yet backported) was
>      >>>> made for ????? > as.complex(NA_real_) to give
>      >>>> NA_complex_ instead of ????? > complex(r=NA_real_,
>      >>>> i=0), to be consistent with ????? > help("as.complex")
>      >>>> and as.complex(NA) and as.complex(NA_integer_).
>      >>>>
>      >>>> ????? > Was any consideration given to the alternative?
>      >>>> ????? > That is, to changing as.complex(NA) and
>      >>>> as.complex(NA_integer_) to ????? > give
>      >>>> complex(r=NA_real_, i=0), consistent with ????? >
>      >>>> as.complex(NA_real_), then amending help("as.complex")
>      >>>> ????? > accordingly?
>      >>>>
>      >>>> Hmm, as, from R-core, mostly I was involved, I admit to
>      >>>> say "no", to my knowledge the (above) alternative
>      >>>> wasn't considered.
>      >>>>
>      >>>> ??? > The principle that ??? >
>      >>>> Im(as.complex(<real=(double|integer|logical)>)) should
>      >>>> be zero ??? > is quite fundamental, in my view, hence
>      >>>> the "new" behaviour ??? > seems to really violate the
>      >>>> principle of least surprise ...
>      >>>>
>      >>>> of course "least surprise"? is somewhat subjective.
>      >>>> Still, I clearly agree that the above would be one
>      >>>> desirable property.
>      >>>>
>      >>>> I think that any solution will lead to *some* surprise
>      >>>> for some cases, I think primarily because there are
>      >>>> *many* different values z? for which? is.na(z)? is
>      >>>> true,? and in any case NA_complex_? is only of the
>      >>>> many.
>      >>>>
>      >>>> I also agree with Mikael that we should reconsider the
>      >>>> issue that was raised by Davis Vaughan here ("on
>      >>>> R-devel") last April.
>      >>>>
>      >>>> ????? > Another (but maybe weaker) argument is that
>      >>>> ????? > double->complex coercions happen more often
>      >>>> than ????? > logical->complex and integer->complex
>      >>>> ones. Changing the ????? > behaviour of the more
>      >>>> frequently performed coercion is ????? > more likely to
>      >>>> affect code "out there".
>      >>>>
>      >>>> ????? > Yet another argument is that one expects
>      >>>>
>      >>>> ????? >????? identical(as.complex(NA_real_), NA_real_ +
>      >>>> (0+0i))
>      >>>>
>      >>>> ????? > to be TRUE, i.e., that coercing from double to
>      >>>> complex is ????? > equivalent to adding a complex
>      >>>> zero.? The new behaviour ????? > makes the above FALSE,
>      >>>> since NA_real_ + (0+0i) gives ????? >
>      >>>> complex(r=NA_real_, i=0).
>      >>>>
>      >>>> No!? --- To my own surprise (!) --- in current R-devel
>      >>>> the above is TRUE, and ??????? NA_real_ + (0+0i)? , the
>      >>>> same as ??????? NA_real_ + 0i????? , really gives
>      >>>> complex(r=NA, i=NA) :
>      >>>>
>      >>>> Using showC() from ?complex
>      >>>>
>      >>>> ??? showC <- function(z) noquote(sprintf("(R = %g, I =
>      >>>> %g)", Re(z), Im(z)))
>      >>>>
>      >>>> we see (in R-devel) quite consistently
>      >>>>
>      >>>>> showC(NA_real_ + 0i)
>      >>>> [1] (R = NA, I = NA)
>      >>>>> showC(NA?????? + 0i)? # NA is 'logical'
>      >>>> [1] (R = NA, I = NA) where as in R 4.3.1 and
>      >>>> "R-patched" -- *in*consistently
>      >>>>
>      >>>>> showC(NA_real_ + 0i)
>      >>>> [1] (R = NA, I = 0)
>      >>>>> showC(NA + 0i)
>      >>>> [1] (R = NA, I = NA) .... and honestly, I do not see
>      >>>> *where* (and when) we changed the underlying code (in
>      >>>> arithmetic.c !?)? in R-devel to *also* produce
>      >>>> NA_complex_? in such complex *arithmetic*
>      >>>>
>      >>>>
>      >>>> ????? > Having said that, one might also (but more
>      >>>> naively) expect
>      >>>>
>      >>>> ????? >
>      >>>> identical(as.complex(as.double(NA_complex_)),
>      >>>> NA_complex_)
>      >>>>
>      >>>> ????? > to be TRUE.
>      >>>>
>      >>>> as in current R-devel
>      >>>>
>      >>>> ????? > Under my proposal it continues to be FALSE.
>      >>>>
>      >>>> as in "R-release"
>      >>>>
>      >>>> ????? > Well, I'd prefer if it gave FALSE with a
>      >>>> warning ????? > "imaginary parts discarded in
>      >>>> coercion", but it seems that ????? >
>      >>>> as.double(complex(r=a, i=b)) never warns when either of
>      >>>> ????? > 'a' and 'b' is NA_real_ or NaN, even where
>      >>>> "information" ????? > {nonzero 'b'} is clearly lost ...
>      >>>>
>      >>>> The question of *warning* here is related indeed, but I
>      >>>> think we should try to look at it only *secondary* to
>      >>>> your first proposal.
>      >>>>
>      >>>> ????? > Whatever decision is made about
>      >>>> as.complex(NA_real_), ????? > maybe these points should
>      >>>> be weighed before it becomes part of ????? > R-release
>      >>>> ...
>      >>>>
>      >>>> ????? > Mikael
>      >>>>
>      >>>> Indeed.
>      >>>>
>      >>>> Can we please get other opinions / ideas here?
>      >>>>
>      >>>> Thank you in advance for your thoughts!  Martin
>      >>>>
>      >>>> ---
>      >>>>
>      >>>> PS:
>      >>>>
>      >>>> ?? Our *print()*ing? of complex NA's ("NA" here meaning
>      >>>> NA or NaN) ?? is also unsatisfactory, e.g. in the case
>      >>>> where all entries of a ?? vector are NA in the sense of
>      >>>> is.na(.), but their ?? Re() and Im() are not all NA:
>      >>>> ??? showC <- function(z) noquote(sprintf("(R = %g, I =
>      >>>> %g)", Re(z), Im(z))) ??? z <- complex(, c(11, NA, NA),
>      >>>> c(NA, 99, NA)) ??? z ??? showC(z)
>      >>>>
>      >>>> gives
>      >>>>
>      >>>> ??? > z ??? [1] NA NA NA ??? > showC(z) ??? [1] (R =
>      >>>> 11, I = NA) (R = NA, I = 99) (R = NA, I = NA)
>      >>>>
>      >>>> but that (printing of complex) *is* another issue, in
>      >>>> which we have the re-opened bugzilla PR#16752
>      >>>> ==>https://bugs.r-project.org/show_bug.cgi?id=16752
>      >>>>
>      >>>> on which we also worked during the R Sprint in Warwick
>      >>>> three weeks ago, and where I want to commit changes in
>      >>>> any case {but think we should change even a bit more
>      >>>> than we got to during the Sprint}.
>      >>>>
>      >>>> ______________________________________________
>      >>>>R-devel at r-project.org ? mailing list
>      >>>>https://stat.ethz.ch/mailman/listinfo/r-devel
>      >>>
>      >>
>      > --
>      > Herv? Pag?s
>
>      > Bioconductor Core Teamhpages.on.github at gmail.com
>
>
>
-- 
Herv? Pag?s

Bioconductor Core Team
hpages.on.github at gmail.com

	[[alternative HTML version deleted]]


From @g|@nner|n| @end|ng |rom gm@||@com  Sun Sep 24 16:57:00 2023
From: @g|@nner|n| @end|ng |rom gm@||@com (Simone Giannerini)
Date: Sun, 24 Sep 2023 16:57:00 +0200
Subject: [Rd] NROW and NCOL on NULL
In-Reply-To: <122c7b31-ee47-9b1f-0baf-d792f29cbfdf@gmail.com>
References: <CANcXGizv3sD3kiE__EUkuaESn44mvtQMrtpO_k=FjUwFQDGnVQ@mail.gmail.com>
 <936870db-2e16-4b8b-878b-c2dde317319c@gmail.com>
 <CANcXGiz9g6mAsnNd3=Uc9iQ5xvE8Mg2ws5fbsEFYVXB_HL=Sog@mail.gmail.com>
 <122c7b31-ee47-9b1f-0baf-d792f29cbfdf@gmail.com>
Message-ID: <CANcXGizameDFVYKp_14BJf-B6BaH3vv-mdQ3_Rf31ynEnFRGfw@mail.gmail.com>

Thank you for your comment,

On Sat, Sep 23, 2023 at 9:51?PM Ben Bolker <bbolker at gmail.com> wrote:
>
>     This is certainly worth discussing, but there's always a heavy
> burden of back-compatibility; how much better would it be for NCOL and
> NROW to both return zero, vs. the amount of old code that would be broken?

I do not have an answer to this question but it seems to me that code
that relies upon NCOL(NULL) being 1 is not extremely good (and
portable).

>
>    Furthermore, the reason for this behaviour is justified as
> consistency with the behaviour of as.matrix() and cbind() for
> zero-length vectors, from ?NCOL:
>
>       ## as.matrix() produces 1-column matrices from 0-length vectors,
>       ## and so does cbind() :
>
>   (of course you could argue that this behaviour should be changed as
> well ...)
>
>

Yes, it is documented and somehow clashes with the more intuitive
behaviour of subsetting matrices

 > a <- matrix(1:4,2,2)
> a
     [,1] [,2]
[1,]    1    3
[2,]    2    4
> a2 <- a[,-(1:2)]
> a2

[1,]
[2,]
> dim(a2)
[1] 2 0

NULL is often used to declare an undefined value for the argument of a
function. If such an argument is potentially a matrix, then using NULL
as the default requires additional code to check for the number of
columns and use it in the code.
The same holds to a lesser extent for functions that are expected to
return a matrix and return NULL instead.

Kind regards,

Simone

> On 2023-09-23 3:41 p.m., Simone Giannerini wrote:
> > I know it's documented and I know there are other ways to guard
> > against this behaviour, once you know about this.
> > The point is whether it might be worth it to make NCOL and NROW return
> > the same value on NULL and make R more consistent/intuitive and
> > possibly less error prone.
> >
> > Regards,
> >
> > Simone
> >
> > On Sat, Sep 23, 2023 at 7:50?PM Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
> >>
> >> It's been documented for a long time that NCOL(NULL) is 1.  What
> >> particular problems did you have in mind?  There might be other ways to
> >> guard against them.
> >>
> >> Duncan Murdoch
> >>
> >> On 23/09/2023 1:43 p.m., Simone Giannerini wrote:
> >>> Dear list,
> >>>
> >>> I do not know what would be the 'correct' answer to the following but
> >>> I think that they should return the same value to avoid potential
> >>> problems and hard to debug errors.
> >>>
> >>> Regards,
> >>>
> >>> Simone
> >>> ---------------------------------------
> >>>
> >>>> NCOL(NULL)
> >>> [1] 1
> >>>
> >>>> NROW(NULL)
> >>> [1] 0
> >>>
> >>>> sessionInfo()
> >>> R version 4.3.1 RC (2023-06-08 r84523 ucrt)
> >>> Platform: x86_64-w64-mingw32/x64 (64-bit)
> >>> Running under: Windows 11 x64 (build 22621)
> >>>
> >>> Matrix products: default
> >>>
> >>>
> >>> locale:
> >>> [1] LC_COLLATE=Italian_Italy.utf8  LC_CTYPE=Italian_Italy.utf8
> >>> [3] LC_MONETARY=Italian_Italy.utf8 LC_NUMERIC=C
> >>> [5] LC_TIME=Italian_Italy.utf8
> >>>
> >>> time zone: Europe/Rome
> >>> tzcode source: internal
> >>>
> >>> attached base packages:
> >>> [1] stats     graphics  grDevices utils     datasets  methods   base
> >>>
> >>> loaded via a namespace (and not attached):
> >>> [1] compiler_4.3.1
> >>>
> >>
> >
> >
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel



-- 
___________________________________________________

Simone Giannerini
Dipartimento di Scienze Statistiche "Paolo Fortunati"
Universita' di Bologna
Via delle belle arti 41 - 40126  Bologna,  ITALY
Tel: +39 051 2098262  Fax: +39 051 232153
https://simonegiannerini.net/


From r@||@@tubner @end|ng |rom gm@||@com  Mon Sep 25 00:30:46 2023
From: r@||@@tubner @end|ng |rom gm@||@com (Ralf Stubner)
Date: Sun, 24 Sep 2023 22:30:46 +0000
Subject: [Rd] Suggestion: User defined sampling
Message-ID: <CAFQwRQz2DK=94LGxHt5Tf50exTXx5iNiw8uyD4e0=hirUJG2BA@mail.gmail.com>

Hi everybody,

Currently one can alter R's random number generation via RNGkind() in
three aspects: the RNG itself, the method for drawing from the normal
distribution and the method for generating integers within a range.
For the first and second aspect it is possible to supply user defined
methods. This is not the case for the last aspect, which is handled in
R_unif_index. I think it would be interesting to add a comparable hook
there as well. What do you think?

Ralf


From @|mon@urb@nek @end|ng |rom R-project@org  Mon Sep 25 00:55:30 2023
From: @|mon@urb@nek @end|ng |rom R-project@org (Simon Urbanek)
Date: Mon, 25 Sep 2023 11:55:30 +1300
Subject: [Rd] Help requested: writing text to a raster in memory
In-Reply-To: <35a7f4df-98df-4216-ac67-6d22df1d705e@gmail.com>
References: <35a7f4df-98df-4216-ac67-6d22df1d705e@gmail.com>
Message-ID: <872DA7B8-D7E6-4D3B-BB72-FD80473E58E0@R-project.org>

Duncan,

drawing text is one of the most complicated things you can do, so it really depends how for you want to go. You can do it badly with a simple cairo show_text API. The steps involved in doing it properly are detecting the direction of the language, finding fonts, finding glyphs (resolving ligatures), applying hints, drawing glyphs etc. Fortunately there are libraries that help with than, but even then it's non-trivial. Probably the most modern pipeline is icu + harfbuzz + freetype + fontconfig + cairo. This is implemented, e.g in https://github.com/s-u/Cairo/blob/master/src/cairotalk.c (the meat is in  L608-) and for all but the drawing part there is an entire R package (in C++) devoted to this: https://github.com/r-lib/textshaping/tree/main/src -- Thomas Lin Pedersen is probably THE expert on this.

Cheers,
Simon


> On 24/09/2023, at 7:44 AM, Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
> 
> I am in the process of updating the rgl package.  One thing I'd like to do is to change text support in it when using OpenGL to display to be more like the way text is drawn in WebGL displays (i.e. the ones rglwidget() produces).
> 
> Currently in R, rgl uses the FTGL library to draw text.  That library is unsupported these days, and uses the old fixed pipeline in OpenGL.
> 
> In WebGL, text is displayed by "shaders", programs that run on the GPU. Javascript code prepares bitmap images of the text to display, then the shader transfers parts of that bitmap to the output display.
> 
> I'd like to duplicate the WebGL process in the C++ code running the OpenGL display in R.  The first step in this is to render a character vector full of text into an in-memory raster, taking account of font, cex, etc.  (I want one raster for the whole vector, with a recording of locations from which the shader should get each component of it.)
> 
> It looks to me as though I could do this using the ragg::agg_capture device in R code, but I'd prefer to do it all in C++ code because I may need to make changes to the raster at times when it's not safe to call back to R, e.g. if some user interaction requires the axis labels to be recomputed and redrawn.
> 
> Does anyone with experience doing this kind of thing know of examples I can follow, or have advice on how to proceed?  Or want to volunteer to help with this?
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 


From murdoch@dunc@n @end|ng |rom gm@||@com  Mon Sep 25 03:01:36 2023
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Sun, 24 Sep 2023 21:01:36 -0400
Subject: [Rd] Help requested: writing text to a raster in memory
In-Reply-To: <872DA7B8-D7E6-4D3B-BB72-FD80473E58E0@R-project.org>
References: <35a7f4df-98df-4216-ac67-6d22df1d705e@gmail.com>
 <872DA7B8-D7E6-4D3B-BB72-FD80473E58E0@R-project.org>
Message-ID: <8eeb8b83-5b84-473d-8f58-9bb126a3491f@gmail.com>

I'm somewhat aware of how tricky it all is.  For now I'm going to do it 
in R (usng textshaping for layout and base graphics on the 
ragg::agg_capture device to draw to the bitmap).  I'll avoid allowing 
changes to happen in the C++ code.

Eventually I'll see if I can translate the code into C++.  I know 
textshaping has a C interface, but for the actual drawing I'll have to 
work something else out.  Or maybe just leave it in R, and only try to 
write a new bitmap when it's safe.

For future reference, will the measurements reported by 
textshaping::shape_text() match the values used by your Cairo package, 
or are equivalent measurements available elsewhere?

Duncan Murdoch

On 24/09/2023 6:55 p.m., Simon Urbanek wrote:
> Duncan,
> 
> drawing text is one of the most complicated things you can do, so it really depends how for you want to go. You can do it badly with a simple cairo show_text API. The steps involved in doing it properly are detecting the direction of the language, finding fonts, finding glyphs (resolving ligatures), applying hints, drawing glyphs etc. Fortunately there are libraries that help with than, but even then it's non-trivial. Probably the most modern pipeline is icu + harfbuzz + freetype + fontconfig + cairo. This is implemented, e.g in https://github.com/s-u/Cairo/blob/master/src/cairotalk.c (the meat is in  L608-) and for all but the drawing part there is an entire R package (in C++) devoted to this: https://github.com/r-lib/textshaping/tree/main/src -- Thomas Lin Pedersen is probably THE expert on this.
> 
> Cheers,
> Simon
> 
> 
>> On 24/09/2023, at 7:44 AM, Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
>>
>> I am in the process of updating the rgl package.  One thing I'd like to do is to change text support in it when using OpenGL to display to be more like the way text is drawn in WebGL displays (i.e. the ones rglwidget() produces).
>>
>> Currently in R, rgl uses the FTGL library to draw text.  That library is unsupported these days, and uses the old fixed pipeline in OpenGL.
>>
>> In WebGL, text is displayed by "shaders", programs that run on the GPU. Javascript code prepares bitmap images of the text to display, then the shader transfers parts of that bitmap to the output display.
>>
>> I'd like to duplicate the WebGL process in the C++ code running the OpenGL display in R.  The first step in this is to render a character vector full of text into an in-memory raster, taking account of font, cex, etc.  (I want one raster for the whole vector, with a recording of locations from which the shader should get each component of it.)
>>
>> It looks to me as though I could do this using the ragg::agg_capture device in R code, but I'd prefer to do it all in C++ code because I may need to make changes to the raster at times when it's not safe to call back to R, e.g. if some user interaction requires the axis labels to be recomputed and redrawn.
>>
>> Does anyone with experience doing this kind of thing know of examples I can follow, or have advice on how to proceed?  Or want to volunteer to help with this?
>>
>> Duncan Murdoch
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>


From @|mon@urb@nek @end|ng |rom R-project@org  Mon Sep 25 06:05:15 2023
From: @|mon@urb@nek @end|ng |rom R-project@org (Simon Urbanek)
Date: Mon, 25 Sep 2023 17:05:15 +1300
Subject: [Rd] Help requested: writing text to a raster in memory
In-Reply-To: <8eeb8b83-5b84-473d-8f58-9bb126a3491f@gmail.com>
References: <35a7f4df-98df-4216-ac67-6d22df1d705e@gmail.com>
 <872DA7B8-D7E6-4D3B-BB72-FD80473E58E0@R-project.org>
 <8eeb8b83-5b84-473d-8f58-9bb126a3491f@gmail.com>
Message-ID: <036F5C9D-103D-477D-A298-7E41DBFE9575@R-project.org>

Since I'm working with Paul on the glyph changes to the R graphics engine I'm quite interested in this so I had the idea to take out the guts from my Cairo package into a self-contained C code. Your request is good to bump is up on my stack. I already have code that draws text into OpenGL textures in Acinonyx, but properly it's only done on macOS so it may be worth combining both approaches to have a decent OpenGL text drawing library.

As for measurements, I didn't look at textshaping, but since both use Harfbuzz+FT they should be the same for the same font and scaling (in theory).

Cheers,
Simon



> On 25/09/2023, at 2:01 PM, Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
> 
> I'm somewhat aware of how tricky it all is.  For now I'm going to do it in R (usng textshaping for layout and base graphics on the ragg::agg_capture device to draw to the bitmap).  I'll avoid allowing changes to happen in the C++ code.
> 
> Eventually I'll see if I can translate the code into C++.  I know textshaping has a C interface, but for the actual drawing I'll have to work something else out.  Or maybe just leave it in R, and only try to write a new bitmap when it's safe.
> 
> For future reference, will the measurements reported by textshaping::shape_text() match the values used by your Cairo package, or are equivalent measurements available elsewhere?
> 
> Duncan Murdoch
> 
> On 24/09/2023 6:55 p.m., Simon Urbanek wrote:
>> Duncan,
>> drawing text is one of the most complicated things you can do, so it really depends how for you want to go. You can do it badly with a simple cairo show_text API. The steps involved in doing it properly are detecting the direction of the language, finding fonts, finding glyphs (resolving ligatures), applying hints, drawing glyphs etc. Fortunately there are libraries that help with than, but even then it's non-trivial. Probably the most modern pipeline is icu + harfbuzz + freetype + fontconfig + cairo. This is implemented, e.g in https://github.com/s-u/Cairo/blob/master/src/cairotalk.c (the meat is in  L608-) and for all but the drawing part there is an entire R package (in C++) devoted to this: https://github.com/r-lib/textshaping/tree/main/src -- Thomas Lin Pedersen is probably THE expert on this.
>> Cheers,
>> Simon
>>> On 24/09/2023, at 7:44 AM, Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
>>> 
>>> I am in the process of updating the rgl package.  One thing I'd like to do is to change text support in it when using OpenGL to display to be more like the way text is drawn in WebGL displays (i.e. the ones rglwidget() produces).
>>> 
>>> Currently in R, rgl uses the FTGL library to draw text.  That library is unsupported these days, and uses the old fixed pipeline in OpenGL.
>>> 
>>> In WebGL, text is displayed by "shaders", programs that run on the GPU. Javascript code prepares bitmap images of the text to display, then the shader transfers parts of that bitmap to the output display.
>>> 
>>> I'd like to duplicate the WebGL process in the C++ code running the OpenGL display in R.  The first step in this is to render a character vector full of text into an in-memory raster, taking account of font, cex, etc.  (I want one raster for the whole vector, with a recording of locations from which the shader should get each component of it.)
>>> 
>>> It looks to me as though I could do this using the ragg::agg_capture device in R code, but I'd prefer to do it all in C++ code because I may need to make changes to the raster at times when it's not safe to call back to R, e.g. if some user interaction requires the axis labels to be recomputed and redrawn.
>>> 
>>> Does anyone with experience doing this kind of thing know of examples I can follow, or have advice on how to proceed?  Or want to volunteer to help with this?
>>> 
>>> Duncan Murdoch
>>> 
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>> 
> 


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Mon Sep 25 10:12:08 2023
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Mon, 25 Sep 2023 10:12:08 +0200
Subject: [Rd] NROW and NCOL on NULL
In-Reply-To: <CANcXGizameDFVYKp_14BJf-B6BaH3vv-mdQ3_Rf31ynEnFRGfw@mail.gmail.com>
References: <CANcXGizv3sD3kiE__EUkuaESn44mvtQMrtpO_k=FjUwFQDGnVQ@mail.gmail.com>
 <936870db-2e16-4b8b-878b-c2dde317319c@gmail.com>
 <CANcXGiz9g6mAsnNd3=Uc9iQ5xvE8Mg2ws5fbsEFYVXB_HL=Sog@mail.gmail.com>
 <122c7b31-ee47-9b1f-0baf-d792f29cbfdf@gmail.com>
 <CANcXGizameDFVYKp_14BJf-B6BaH3vv-mdQ3_Rf31ynEnFRGfw@mail.gmail.com>
Message-ID: <25873.16600.837006.183401@stat.math.ethz.ch>

>>>>> Simone Giannerini 
>>>>>     on Sun, 24 Sep 2023 16:57:00 +0200 writes:

    > Thank you for your comment, On Sat, Sep 23, 2023 at
    > 9:51?PM Ben Bolker <bbolker at gmail.com> wrote:
    >> 
    >> This is certainly worth discussing, but there's always a
    >> heavy burden of back-compatibility; how much better would
    >> it be for NCOL and NROW to both return zero, vs. the
    >> amount of old code that would be broken?

    > I do not have an answer to this question but it seems to
    > me that code that relies upon NCOL(NULL) being 1 is not
    > extremely good (and portable).

Well, it remains *very* portable,  as long as we keep the behavior.
It has worked as it does for more than twenty years, and if you
finally remain convinced that we won't change, it will remain portable
between all versions of R from the very old past to the remote
future  ... ;-)

Martin


    >> Furthermore, the reason for this behaviour is justified
    >> as consistency with the behaviour of as.matrix() and
    >> cbind() for zero-length vectors, from ?NCOL:
    >> 
    >> ## as.matrix() produces 1-column matrices from 0-length
    >> vectors, ## and so does cbind() :
    >> 
    >> (of course you could argue that this behaviour should be
    >> changed as well ...)
    >> 
    >> 

    > Yes, it is documented and somehow clashes with the more
    > intuitive behaviour of subsetting matrices

    >> a <- matrix(1:4,2,2) a
    >      [,1] [,2] [1,] 1 3 [2,] 2 4
    >> a2 <- a[,-(1:2)] a2

    > [1,] [2,]
    >> dim(a2)
    > [1] 2 0

    > NULL is often used to declare an undefined value for the
    > argument of a function. If such an argument is potentially
    > a matrix, then using NULL as the default requires
    > additional code to check for the number of columns and use
    > it in the code.  The same holds to a lesser extent for
    > functions that are expected to return a matrix and return
    > NULL instead.

    > Kind regards,

    > Simone

    >> On 2023-09-23 3:41 p.m., Simone Giannerini wrote: > I
    >> know it's documented and I know there are other ways to
    >> guard > against this behaviour, once you know about this.
    >> > The point is whether it might be worth it to make NCOL
    >> and NROW return > the same value on NULL and make R more
    >> consistent/intuitive and > possibly less error prone.
    >> >
    >> > Regards,
    >> >
    >> > Simone
    >> >
    >> > On Sat, Sep 23, 2023 at 7:50?PM Duncan Murdoch
    >> <murdoch.duncan at gmail.com> wrote:
    >> >>
    >> >> It's been documented for a long time that NCOL(NULL)
    >> is 1.  What >> particular problems did you have in mind?
    >> There might be other ways to >> guard against them.
    >> >>
    >> >> Duncan Murdoch
    >> >>
    >> >> On 23/09/2023 1:43 p.m., Simone Giannerini wrote: >>>
    >> Dear list,
    >> >>>
    >> >>> I do not know what would be the 'correct' answer to
    >> the following but >>> I think that they should return the
    >> same value to avoid potential >>> problems and hard to
    >> debug errors.
    >> >>>
    >> >>> Regards,
    >> >>>
    >> >>> Simone
    >> >>> ---------------------------------------
    >> >>>
    >> >>>> NCOL(NULL) >>> [1] 1
    >> >>>
    >> >>>> NROW(NULL) >>> [1] 0
    >> >>>
    >> >>>> sessionInfo() >>> R version 4.3.1 RC (2023-06-08
    >> r84523 ucrt) >>> Platform: x86_64-w64-mingw32/x64
    >> (64-bit) >>> Running under: Windows 11 x64 (build 22621)
    >> >>>
    >> >>> Matrix products: default
    >> >>>
    >> >>>
    >> >>> locale: >>> [1] LC_COLLATE=Italian_Italy.utf8
    >> LC_CTYPE=Italian_Italy.utf8 >>> [3]
    >> LC_MONETARY=Italian_Italy.utf8 LC_NUMERIC=C >>> [5]
    >> LC_TIME=Italian_Italy.utf8
    >> >>>
    >> >>> time zone: Europe/Rome >>> tzcode source: internal
    >> >>>
    >> >>> attached base packages: >>> [1] stats graphics
    >> grDevices utils datasets methods base
    >> >>>
    >> >>> loaded via a namespace (and not attached): >>> [1]
    >> compiler_4.3.1
    >> >>>
    >> >>
    >> >
    >> >
    >> 
    >> ______________________________________________
    >> R-devel at r-project.org mailing list
    >> https://stat.ethz.ch/mailman/listinfo/r-devel



    > -- 
    > ___________________________________________________

    > Simone Giannerini Dipartimento di Scienze Statistiche
    > "Paolo Fortunati" Universita' di Bologna Via delle belle
    > arti 41 - 40126 Bologna, ITALY Tel: +39 051 2098262 Fax:
    > +39 051 232153 https://simonegiannerini.net/

    > ______________________________________________
    > R-devel at r-project.org mailing list
    > https://stat.ethz.ch/mailman/listinfo/r-devel


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Mon Sep 25 16:05:33 2023
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Mon, 25 Sep 2023 16:05:33 +0200
Subject: [Rd] Recent changes to as.complex(NA_real_)
In-Reply-To: <52c848e6-80d5-6514-6926-95804ec7562f@gmail.com>
References: <1cf40db9-ea70-483a-a547-270da6926935@gmail.com>
 <25869.28353.595831.807741@stat.math.ethz.ch>
 <4abe101e-55be-8800-31ee-e0414c89740e@gmail.com>
 <57f77592-26ad-43dd-8e36-f9989c915308@gmail.com>
 <6bcad145-a1f9-32a5-c9e8-97041ed2eab7@gmail.com>
 <25870.60261.24330.115632@stat.math.ethz.ch>
 <52c848e6-80d5-6514-6926-95804ec7562f@gmail.com>
Message-ID: <25873.37805.824439.995259@stat.math.ethz.ch>

>>>>> Herv? Pag?s 
>>>>>     on Sat, 23 Sep 2023 16:52:21 -0700 writes:

    > Hi Martin,
    > On 9/23/23 06:43, Martin Maechler wrote:
    >>>>>>> Herv? Pag?s
    >>>>>>> on Fri, 22 Sep 2023 16:55:05 -0700 writes:
    >> > The problem is that you have things that are
    >> > **semantically** different but look exactly the same:
    >> 
    >> > They look the same:
    >> 
    >> >> x
    >> > [1] NA
    >> >> y
    >> > [1] NA
    >> >> z
    >> > [1] NA
    >> 
    >> >> is.na(x)
    >> > [1] TRUE
    >> >> is.na(y)
    >> > [1] TRUE
    >> >> is.na(z)
    >> > [1] TRUE
    >> 
    >> >> str(x)
    >> >  ?cplx NA
    >> >> str(y)
    >> >  ?num NA
    >> >> str(z)
    >> >  ?cplx NA
    >> 
    >> > but they are semantically different e.g.
    >> 
    >> >> Re(x)
    >> > [1] NA
    >> >> Re(y)
    >> > [1] -0.5? # surprise!
    >> 
    >> >> Im(x)? # surprise!
    >> > [1] 2
    >> >> Im(z)
    >> > [1] NA
    >> 
    >> > so any expression involving Re() or Im() will produce
    >> > different results on input that look the same on the
    >> > surface.
    >> 
    >> > You can address this either by normalizing the internal
    >> > representation of complex NA to always be complex(r=NaN,
    >> > i=NA_real_), like for NA_complex_, or by allowing the
    >> > infinite variations that are currently allowed and at the
    >> > same time making sure that both Re() and Im()? always
    >> > return NA_real_ on a complex NA.
    >> 
    >> > My point is that the behavior of complex NA should be
    >> > predictable. Right now it's not. Once it's predictable
    >> > (with Re() and Im() both returning NA_real_ regardless of
    >> > internal representation), then it no longer matters what
    >> > kind of complex NA is returned by as.complex(NA_real_),
    >> > because they are no onger distinguishable.
    >> 
    >> > H.
    >> 
    >> > On 9/22/23 13:43, Duncan Murdoch wrote:
    >> >> Since the result of is.na(x) is the same on each of
    >> >> those, I don't see a problem.? As long as that is
    >> >> consistent, I don't see a problem. You shouldn't be using
    >> >> any other test for NA-ness.? You should never be
    >> >> expecting identical() to treat different types as the
    >> >> same (e.g.  identical(NA, NA_real_) is FALSE, as it
    >> >> should be).? If you are using a different test, that's
    >> >> user error.
    >> >>
    >> >> Duncan Murdoch
    >> >>
    >> >> On 22/09/2023 2:41 p.m., Herv? Pag?s wrote:
    >> >>> We could also question the value of having an infinite
    >> >>> number of NA representations in the complex space. For
    >> >>> example all these complex values are displayed the same
    >> >>> way (as NA), are considered NAs by is.na(), but are not
    >> >>> identical or semantically equivalent (from an Re() or
    >> >>> Im() point of view):
    >> >>>
    >> >>> ? ??? NA_real_ + 0i
    >> >>>
    >> >>> ? ??? complex(r=NA_real_, i=Inf)
    >> >>>
    >> >>> ? ??? complex(r=2, i=NA_real_)
    >> >>>
    >> >>> ? ??? complex(r=NaN, i=NA_real_)
    >> >>>
    >> >>> In other words, using a single representation for
    >> >>> complex NA (i.e.  complex(r=NA_real_, i=NA_real_)) would
    >> >>> avoid a lot of unnecessary complications and surprises.
    >> >>>
    >> >>> Once you do that, whether as.complex(NA_real_) should
    >> >>> return complex(r=NA_real_, i=0) or complex(r=NA_real_,
    >> >>> i=NA_real_) becomes a moot point.
    >> >>>
    >> >>> Best,
    >> >>>
    >> >>> H.
    >> 
    >> Thank you, Herv?.
    >> Your proposition is yet another one,
    >> to declare that all complex NA's should be treated as identical
    >> (almost/fully?) everywhere.
    >> 
    >> This would be a possibility, but I think a drastic one.
    >> 
    >> I think there are too many cases, where I want to keep the
    >> information of the real part independent of the values of the
    >> imaginary part (e.g. think of the Riemann hypothesis), and
    >> typically vice versa.
    > Use NaN for that, not NA.

Aa..h, *that* is your point.

Well, I was on exactly this line till a few years ago.

However, very *sadly* to me, note how    example(complex)
nowadays ends :

##----------------------------------------------------------------------------
 showC <- function(z) noquote(sprintf("(R = %g, I = %g)", Re(z), Im(z)))
 
 ## The exact result of this *depends* on the platform, compiler, math-library:
 (NpNA <- NaN + NA_complex_) ; str(NpNA) # *behaves* as 'cplx NA' ..
 stopifnot(is.na(NpNA), is.na(NA_complex_), is.na(Re(NA_complex_)), is.na(Im(NA_complex_)))
 showC(NpNA)# but does not always show '(R = NaN, I = NA)'
 ## and this is not TRUE everywhere:
 identical(NpNA, NA_complex_)
 showC(NA_complex_) # always == (R = NA, I = NA)
##----------------------------------------------------------------------------

Unfortunately --- notably by the appearance of the new (M1, M1 pro, M2, ...)
processors, but not only ---
I (and others, but the real experts) have wrongly assumed  that
NA {which on the C-level is *one* of the many possible internal NaN's}
would be preserved in computations, as they are on the R level
-- well, typically, and as long as we've used intel-compatible
chips and gcc-compilers.

But modern speed optimizations (also seen in accelerated
Blas/Lapack ..) have noticed that no official C standard
requires such preservations (i.e., in our case of NA, *the* special NaN), 
and -- for speed reasons -- now on these accelerated platforms,
R-level NA's "suddenly" turn into R-level NaN's  (all are NaN on
the C level but "with different payload") from quite "trivial" computations.

Consequently, the strict distinction between  NA  and  NaN
even when they are so important for us statisticians / careful data analysts,
nowadays will tend to have to be dismissed eventually.

... and as I have mentioned also mentioned earlier in this thread,
I believe we should also print the complex values of z
fulfilling is.na(z)  by their  Re & Im, i.e., e.g.

 NA+iNA  (or NaN+iNA or NA+iNaN or NaN+iNaN
 NA+0i,  NaN+1i,  3+iNaN, 4+iNA  etc

but note that the exact printing itself should *not* become the topic of this
thread unless by mentioning that I strongly believe the print()ing
of complex vectors in R should change anway *and* for that reason,
the printing / "looks the same as" / ...  should not be strong
reasons in my view for deciding how  *coercion*,
notably  as.complex(.)   should work.

Martin


    >> With your proposal, for a (potentially large) vector of complex numbers,
    >> after
    >> Re(z)  <-  1/2
    >> 
    >> I could no longer rely on   Re(z) == 1/2,
    >> because it would be wrong for those z where (the imaginary part/ the number)
    >> was NA/NaN.

    > My proposal is to do this only if the Re and/or Im parts are NAs, not if 
    > they are NaNs.

    > BTW the difference between how NAs and NaNs are treated in complex 
    > vectors is another issue that adds to the confusion:

    > ? > complex(r=NA, i=2)
    > ? [1] NA
    > ? > complex(r=NaN, i=2)
    > ? [1] NaN+2i

    > Not displaying the real + imaginary parts in the NA case kind of 
    > suggests that somehow they are gone i.e. that Re(z) and Im(z) are both NA.

    > Note that my proposal is not to change the display but to change Re() 
    > and Im() to make them consistent with the display.

    > In your Re(z) <- 1/2 example (which seems to be theoretical only because 
    > I don't see `Re<-` in base R), any NA in 'z' would be replaced with 
    > complex(r=NaN, i=1/2), so you could rely on Re(z) == 1/2.

    >> Also, in a similar case, a
    >> 
    >> Im(z) <- NA
    >> 
    >> would have to "destroy" all real parts  Re(z);
    >> not really typically in memory, but effectively for the user,  Re(z)
    >> would be all NA/NaN.
    > Yes, setting a value to NA destroys it beyond repair in the sense that 
    > there's no way you can retrieve any original parts of it. I'm fine with 
    > that. I'm not fine with an NA being used to store hidden information.
    >> 
    >> And I think there are quite a few other situations
    >> where looking at Re() and Im() separately makes a lot of sense.
    > Still doable if the Re or Im parts contain NaNs.
    >> 
    >> Spencer also made a remark in this direction.
    >> 
    >> All in all I'd be very reluctant to move in this direction;
    >> but yes, I'm just one person ... let's continue musing and
    >> considering !

    > I understand the reluctance since this would not be a light move, but 
    > thanks for considering.

    > Best,

    > H.

    >> 
    >> Martin
    >> 
    >> >>> On 9/22/23 03:38, Martin Maechler wrote:
    >> >>>>>>>>> Mikael Jagan ????? on Thu, 21 Sep 2023 00:47:39
    >> >>>>>>>>> -0400 writes:
    >> >>>> ????? > Revisiting this thread from April:
    >> >>>>
    >> >>>> >https://stat.ethz.ch/pipermail/r-devel/2023-April/082545.html
    >> >>>>
    >> >>>> ????? > where the decision (not yet backported) was
    >> >>>> made for ????? > as.complex(NA_real_) to give
    >> >>>> NA_complex_ instead of ????? > complex(r=NA_real_,
    >> >>>> i=0), to be consistent with ????? > help("as.complex")
    >> >>>> and as.complex(NA) and as.complex(NA_integer_).
    >> >>>>
    >> >>>> ????? > Was any consideration given to the alternative?
    >> >>>> ????? > That is, to changing as.complex(NA) and
    >> >>>> as.complex(NA_integer_) to ????? > give
    >> >>>> complex(r=NA_real_, i=0), consistent with ????? >
    >> >>>> as.complex(NA_real_), then amending help("as.complex")
    >> >>>> ????? > accordingly?
    >> >>>>
    >> >>>> Hmm, as, from R-core, mostly I was involved, I admit to
    >> >>>> say "no", to my knowledge the (above) alternative
    >> >>>> wasn't considered.
    >> >>>>
    >> >>>> ??? > The principle that ??? >
    >> >>>> Im(as.complex(<real=(double|integer|logical)>)) should
    >> >>>> be zero ??? > is quite fundamental, in my view, hence
    >> >>>> the "new" behaviour ??? > seems to really violate the
    >> >>>> principle of least surprise ...
    >> >>>>
    >> >>>> of course "least surprise"? is somewhat subjective.
    >> >>>> Still, I clearly agree that the above would be one
    >> >>>> desirable property.
    >> >>>>
    >> >>>> I think that any solution will lead to *some* surprise
    >> >>>> for some cases, I think primarily because there are
    >> >>>> *many* different values z? for which? is.na(z)? is
    >> >>>> true,? and in any case NA_complex_? is only of the
    >> >>>> many.
    >> >>>>
    >> >>>> I also agree with Mikael that we should reconsider the
    >> >>>> issue that was raised by Davis Vaughan here ("on
    >> >>>> R-devel") last April.
    >> >>>>
    >> >>>> ????? > Another (but maybe weaker) argument is that
    >> >>>> ????? > double->complex coercions happen more often
    >> >>>> than ????? > logical->complex and integer->complex
    >> >>>> ones. Changing the ????? > behaviour of the more
    >> >>>> frequently performed coercion is ????? > more likely to
    >> >>>> affect code "out there".
    >> >>>>
    >> >>>> ????? > Yet another argument is that one expects
    >> >>>>
    >> >>>> ????? >????? identical(as.complex(NA_real_), NA_real_ +
    >> >>>> (0+0i))
    >> >>>>
    >> >>>> ????? > to be TRUE, i.e., that coercing from double to
    >> >>>> complex is ????? > equivalent to adding a complex
    >> >>>> zero.? The new behaviour ????? > makes the above FALSE,
    >> >>>> since NA_real_ + (0+0i) gives ????? >
    >> >>>> complex(r=NA_real_, i=0).
    >> >>>>
    >> >>>> No!? --- To my own surprise (!) --- in current R-devel
    >> >>>> the above is TRUE, and ??????? NA_real_ + (0+0i)? , the
    >> >>>> same as ??????? NA_real_ + 0i????? , really gives
    >> >>>> complex(r=NA, i=NA) :
    >> >>>>
    >> >>>> Using showC() from ?complex
    >> >>>>
    >> >>>> ??? showC <- function(z) noquote(sprintf("(R = %g, I =
    >> >>>> %g)", Re(z), Im(z)))
    >> >>>>
    >> >>>> we see (in R-devel) quite consistently
    >> >>>>
    >> >>>>> showC(NA_real_ + 0i)
    >> >>>> [1] (R = NA, I = NA)
    >> >>>>> showC(NA?????? + 0i)? # NA is 'logical'
    >> >>>> [1] (R = NA, I = NA) where as in R 4.3.1 and
    >> >>>> "R-patched" -- *in*consistently
    >> >>>>
    >> >>>>> showC(NA_real_ + 0i)
    >> >>>> [1] (R = NA, I = 0)
    >> >>>>> showC(NA + 0i)
    >> >>>> [1] (R = NA, I = NA) .... and honestly, I do not see
    >> >>>> *where* (and when) we changed the underlying code (in
    >> >>>> arithmetic.c !?)? in R-devel to *also* produce
    >> >>>> NA_complex_? in such complex *arithmetic*
    >> >>>>
    >> >>>>
    >> >>>> ????? > Having said that, one might also (but more
    >> >>>> naively) expect
    >> >>>>
    >> >>>> ????? >
    >> >>>> identical(as.complex(as.double(NA_complex_)),
    >> >>>> NA_complex_)
    >> >>>>
    >> >>>> ????? > to be TRUE.
    >> >>>>
    >> >>>> as in current R-devel
    >> >>>>
    >> >>>> ????? > Under my proposal it continues to be FALSE.
    >> >>>>
    >> >>>> as in "R-release"
    >> >>>>
    >> >>>> ????? > Well, I'd prefer if it gave FALSE with a
    >> >>>> warning ????? > "imaginary parts discarded in
    >> >>>> coercion", but it seems that ????? >
    >> >>>> as.double(complex(r=a, i=b)) never warns when either of
    >> >>>> ????? > 'a' and 'b' is NA_real_ or NaN, even where
    >> >>>> "information" ????? > {nonzero 'b'} is clearly lost ...
    >> >>>>
    >> >>>> The question of *warning* here is related indeed, but I
    >> >>>> think we should try to look at it only *secondary* to
    >> >>>> your first proposal.
    >> >>>>
    >> >>>> ????? > Whatever decision is made about
    >> >>>> as.complex(NA_real_), ????? > maybe these points should
    >> >>>> be weighed before it becomes part of ????? > R-release
    >> >>>> ...
    >> >>>>
    >> >>>> ????? > Mikael
    >> >>>>
    >> >>>> Indeed.
    >> >>>>
    >> >>>> Can we please get other opinions / ideas here?
    >> >>>>
    >> >>>> Thank you in advance for your thoughts!  Martin
    >> >>>>
    >> >>>> ---
    >> >>>>
    >> >>>> PS:
    >> >>>>
    >> >>>> ?? Our *print()*ing? of complex NA's ("NA" here meaning
    >> >>>> NA or NaN) ?? is also unsatisfactory, e.g. in the case
    >> >>>> where all entries of a ?? vector are NA in the sense of
    >> >>>> is.na(.), but their ?? Re() and Im() are not all NA:
    >> >>>> ??? showC <- function(z) noquote(sprintf("(R = %g, I =
    >> >>>> %g)", Re(z), Im(z))) ??? z <- complex(, c(11, NA, NA),
    >> >>>> c(NA, 99, NA)) ??? z ??? showC(z)
    >> >>>>
    >> >>>> gives
    >> >>>>
    >> >>>> ??? > z ??? [1] NA NA NA ??? > showC(z) ??? [1] (R =
    >> >>>> 11, I = NA) (R = NA, I = 99) (R = NA, I = NA)
    >> >>>>
    >> >>>> but that (printing of complex) *is* another issue, in
    >> >>>> which we have the re-opened bugzilla PR#16752
    >> >>>> ==>https://bugs.r-project.org/show_bug.cgi?id=16752
    >> >>>>
    >> >>>> on which we also worked during the R Sprint in Warwick
    >> >>>> three weeks ago, and where I want to commit changes in
    >> >>>> any case {but think we should change even a bit more
    >> >>>> than we got to during the Sprint}.
    >> >>>>
    >> >>>> ______________________________________________
    >> >>>>R-devel at r-project.org ? mailing list
    >> >>>>https://stat.ethz.ch/mailman/listinfo/r-devel
    >> >>>
    >> >>
    >> > --
    >> > Herv? Pag?s
    >> 
    >> > Bioconductor Core Teamhpages.on.github at gmail.com
    >> 
    >> 
    >> 
    > -- 
    > Herv? Pag?s

    > Bioconductor Core Team
    > hpages.on.github at gmail.com

    > [[alternative HTML version deleted]]


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Mon Sep 25 17:14:58 2023
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Mon, 25 Sep 2023 17:14:58 +0200
Subject: [Rd] is.atomic(NULL) will become FALSE
Message-ID: <25873.41970.991849.206363@stat.math.ethz.ch>

Some of you may remember posts and/or e-mails about this topic
many months ago..

"Atomic vector" is a very well defined term for somewhat advanced
R users.  E.g., from 'The R Language Manual' (in the development
version; i.e. made from latest R source code):
   https://cran.r-project.org/doc/manuals/r-devel/R-lang.html 

Here
  https://cran.r-project.org/doc/manuals/r-devel/R-lang.html#Vector-objects
says

------------------------------------------------------------------------------
2.1.1 Vectors

Vectors can be thought of as contiguous cells containing
data. Cells are accessed through indexing operations such as
x[5]. More details are given in Indexing.

R has six basic (?atomic?) vector types: logical, integer, real,
complex, character (in C aka ?string?) and raw. The modes and
storage modes for the different vector types are listed in the
following table.

    typeof      mode      storage.mode

    logical     logical   logical
    integer     numeric   integer
    double      numeric   double
    complex     complex   complex
    character   character character
    raw         raw       raw

Single numbers, such as 4.2, and strings, such as "four point
two" are still vectors, of length 1; there are no more basic
types. Vectors with length zero are possible (and useful).

A single element of a character vector is often referred to as a
character string or short string.
------------------------------------------------------------------------------

The "Writing R Extensions" Manual, even considerably more
important notably to package writers, mentions atomic vectors
also, e.g., here (again the 6 "R storage mode"s for "basic"
vectors in the  .C() interface

  https://cran.r-project.org/doc/manuals/r-devel/R-exts.html#Interface-functions-_002eC-and-_002eFortran

and then for the more advanced, talking about the C API and
mentioning some of the C-level atomic vector functions, 
first  "for all the atomic vector types" and then notably
IsVectorAtomic() under "some convenience functions"

  https://cran.r-project.org/doc/manuals/r-devel/R-exts.html#Some-convenience-functions

Similarly, Hadley Wickham's book,  "Advanced R", has treated the
topic of  "atomic vectors", currently in
  https://adv-r.hadley.nz/vectors-chap.html#atomic-vectors

-------------------------------------

For historical reasons, R being created (~1993 ff) to be mostly S-compatible,
the  is.atomic() function has also been made compatible
which meant that it gave not only  TRUE  for the 6 atomic vector
types but also for NULL.
At the times, much of the S code (and originally quite a bit
more of the R code than now) treated   NULL
to mean "any vector of size 0"  and probably for that reason, it
was used as preliminary / expressive shortcut of  numeric(0),
logical(0), etc, it was convenient if   is.atomic(NULL)  gave TRUE
the same as it did for all numeric(), integer(), logical(),
.... atomic vectors.

But many time has passed and we had contemplated for many months
if not several years that we'd try to make  is.atomic()  behave
according to the language definition of an atomic-vector.

For this reason, we plan that next year's release of R will have

  > is.atomic(NULL)
  [1] FALSE

instead of  TRUE  as now and historically.

Some package maintainers have been alerted, some as early as 19
months ago  (Feb. 2022), and others a few hours ago that the
above will happen.

Our current plan means it will happen already within the next
few days *if* you are working with the very latest development
versions of R, often called  "R-devel" (as this mailing list).

This will also mean that package maintainers who check their
packages with R-devel  (Automated CRAN jobs do this on an
almost daily schedule; Bioconductor will do this very soon if
they have not already started, and the same could happen if you
use R Hub, CI tools or docker versions of "R-devel".

In all cases where code starts working differently than
previously you could replace

	    is.atomic(<foo>)
by	   (is.atomic(<foo>) || is.null(<foo>))

which will have the R code working equivalently in older and
very new / future versions of R.

Often times however such a change is unnecessary (and even
"wrong" in principle) namely  whenever "you" (or the person who
wrote the code you are working with) really *meant* to check for
atomic vectors which indeed do *not* include NULL.

In other cases, it may be more readable and also better code to
replace code of the form

  if( is.atomic(<foo>) ) {
     .... deal with both NULL and truly-atomic cases ...
     .... deal with both NULL and truly-atomic cases ...
     .... deal with both NULL and truly-atomic cases ...
  }     

by

  if( is.null(<foo>) ) {
     .... deal with NULL case ....
  } else if( is.atomic(<foo>) ) {
     .... deal with truly-atomic case ...
     .... deal with truly-atomic case ...
  }
  
Again, such re-writing makes sense and may improve the quality
and even efficiency of your code, already for current versions
of R and will "automatically" continue to work correctly in
future versions of R where  is.atomic(NULL)  will no longer be
true.

We hope this will help programming safe-ness *and*
make learning and teaching of R more consistent.

Enjoy using R!
Martin

--
Martin Maechler
ETH Zurich  and  R Core team


From hp@ge@@on@g|thub @end|ng |rom gm@||@com  Mon Sep 25 19:08:17 2023
From: hp@ge@@on@g|thub @end|ng |rom gm@||@com (=?UTF-8?B?SGVydsOpIFBhZ8Oocw==?=)
Date: Mon, 25 Sep 2023 10:08:17 -0700
Subject: [Rd] Recent changes to as.complex(NA_real_)
In-Reply-To: <25873.37805.824439.995259@stat.math.ethz.ch>
References: <1cf40db9-ea70-483a-a547-270da6926935@gmail.com>
 <25869.28353.595831.807741@stat.math.ethz.ch>
 <4abe101e-55be-8800-31ee-e0414c89740e@gmail.com>
 <57f77592-26ad-43dd-8e36-f9989c915308@gmail.com>
 <6bcad145-a1f9-32a5-c9e8-97041ed2eab7@gmail.com>
 <25870.60261.24330.115632@stat.math.ethz.ch>
 <52c848e6-80d5-6514-6926-95804ec7562f@gmail.com>
 <25873.37805.824439.995259@stat.math.ethz.ch>
Message-ID: <44e5152a-e1ce-e7ae-9016-8fb71f5d4260@gmail.com>


On 9/25/23 07:05, Martin Maechler wrote:
>>>>>> Herv? Pag?s
>>>>>>      on Sat, 23 Sep 2023 16:52:21 -0700 writes:
>      > Hi Martin,
>      > On 9/23/23 06:43, Martin Maechler wrote:
>      >>>>>>> Herv? Pag?s
>      >>>>>>> on Fri, 22 Sep 2023 16:55:05 -0700 writes:
>      >> > The problem is that you have things that are
>      >> > **semantically** different but look exactly the same:
>      >>
>      >> > They look the same:
>      >>
>      >> >> x
>      >> > [1] NA
>      >> >> y
>      >> > [1] NA
>      >> >> z
>      >> > [1] NA
>      >>
>      >> >> is.na(x)
>      >> > [1] TRUE
>      >> >> is.na(y)
>      >> > [1] TRUE
>      >> >> is.na(z)
>      >> > [1] TRUE
>      >>
>      >> >> str(x)
>      >> >  ?cplx NA
>      >> >> str(y)
>      >> >  ?num NA
>      >> >> str(z)
>      >> >  ?cplx NA
>      >>
>      >> > but they are semantically different e.g.
>      >>
>      >> >> Re(x)
>      >> > [1] NA
>      >> >> Re(y)
>      >> > [1] -0.5? # surprise!
>      >>
>      >> >> Im(x)? # surprise!
>      >> > [1] 2
>      >> >> Im(z)
>      >> > [1] NA
>      >>
>      >> > so any expression involving Re() or Im() will produce
>      >> > different results on input that look the same on the
>      >> > surface.
>      >>
>      >> > You can address this either by normalizing the internal
>      >> > representation of complex NA to always be complex(r=NaN,
>      >> > i=NA_real_), like for NA_complex_, or by allowing the
>      >> > infinite variations that are currently allowed and at the
>      >> > same time making sure that both Re() and Im()? always
>      >> > return NA_real_ on a complex NA.
>      >>
>      >> > My point is that the behavior of complex NA should be
>      >> > predictable. Right now it's not. Once it's predictable
>      >> > (with Re() and Im() both returning NA_real_ regardless of
>      >> > internal representation), then it no longer matters what
>      >> > kind of complex NA is returned by as.complex(NA_real_),
>      >> > because they are no onger distinguishable.
>      >>
>      >> > H.
>      >>
>      >> > On 9/22/23 13:43, Duncan Murdoch wrote:
>      >> >> Since the result of is.na(x) is the same on each of
>      >> >> those, I don't see a problem.? As long as that is
>      >> >> consistent, I don't see a problem. You shouldn't be using
>      >> >> any other test for NA-ness.? You should never be
>      >> >> expecting identical() to treat different types as the
>      >> >> same (e.g.  identical(NA, NA_real_) is FALSE, as it
>      >> >> should be).? If you are using a different test, that's
>      >> >> user error.
>      >> >>
>      >> >> Duncan Murdoch
>      >> >>
>      >> >> On 22/09/2023 2:41 p.m., Herv? Pag?s wrote:
>      >> >>> We could also question the value of having an infinite
>      >> >>> number of NA representations in the complex space. For
>      >> >>> example all these complex values are displayed the same
>      >> >>> way (as NA), are considered NAs by is.na(), but are not
>      >> >>> identical or semantically equivalent (from an Re() or
>      >> >>> Im() point of view):
>      >> >>>
>      >> >>> ? ??? NA_real_ + 0i
>      >> >>>
>      >> >>> ? ??? complex(r=NA_real_, i=Inf)
>      >> >>>
>      >> >>> ? ??? complex(r=2, i=NA_real_)
>      >> >>>
>      >> >>> ? ??? complex(r=NaN, i=NA_real_)
>      >> >>>
>      >> >>> In other words, using a single representation for
>      >> >>> complex NA (i.e.  complex(r=NA_real_, i=NA_real_)) would
>      >> >>> avoid a lot of unnecessary complications and surprises.
>      >> >>>
>      >> >>> Once you do that, whether as.complex(NA_real_) should
>      >> >>> return complex(r=NA_real_, i=0) or complex(r=NA_real_,
>      >> >>> i=NA_real_) becomes a moot point.
>      >> >>>
>      >> >>> Best,
>      >> >>>
>      >> >>> H.
>      >>
>      >> Thank you, Herv?.
>      >> Your proposition is yet another one,
>      >> to declare that all complex NA's should be treated as identical
>      >> (almost/fully?) everywhere.
>      >>
>      >> This would be a possibility, but I think a drastic one.
>      >>
>      >> I think there are too many cases, where I want to keep the
>      >> information of the real part independent of the values of the
>      >> imaginary part (e.g. think of the Riemann hypothesis), and
>      >> typically vice versa.
>      > Use NaN for that, not NA.
>
> Aa..h, *that* is your point.
>
> Well, I was on exactly this line till a few years ago.
>
> However, very *sadly* to me, note how    example(complex)
> nowadays ends :
>
> ##----------------------------------------------------------------------------
>   showC <- function(z) noquote(sprintf("(R = %g, I = %g)", Re(z), Im(z)))
>   
>   ## The exact result of this *depends* on the platform, compiler, math-library:
>   (NpNA <- NaN + NA_complex_) ; str(NpNA) # *behaves* as 'cplx NA' ..
>   stopifnot(is.na(NpNA), is.na(NA_complex_), is.na(Re(NA_complex_)), is.na(Im(NA_complex_)))
>   showC(NpNA)# but does not always show '(R = NaN, I = NA)'
>   ## and this is not TRUE everywhere:
>   identical(NpNA, NA_complex_)
>   showC(NA_complex_) # always == (R = NA, I = NA)
> ##----------------------------------------------------------------------------
>
> Unfortunately --- notably by the appearance of the new (M1, M1 pro, M2, ...)
> processors, but not only ---
> I (and others, but the real experts) have wrongly assumed  that
> NA {which on the C-level is *one* of the many possible internal NaN's}
> would be preserved in computations, as they are on the R level
> -- well, typically, and as long as we've used intel-compatible
> chips and gcc-compilers.
>
> But modern speed optimizations (also seen in accelerated
> Blas/Lapack ..) have noticed that no official C standard
> requires such preservations (i.e., in our case of NA, *the* special NaN),
> and -- for speed reasons -- now on these accelerated platforms,
> R-level NA's "suddenly" turn into R-level NaN's  (all are NaN on
> the C level but "with different payload") from quite "trivial" computations.
>
> Consequently, the strict distinction between  NA  and  NaN
> even when they are so important for us statisticians / careful data analysts,
> nowadays will tend to have to be dismissed eventually.

I see. Thanks for pointing that out.

This would actually be an argument in favor of preserving the current 
as.complex(NA_real_) behavior. If the difference between NA and NaN is 
fading away then there's no reason to change as.complex(NA_real_) to 
make it behave radically differently from as.complex(NaN).

>
> ... and as I have mentioned also mentioned earlier in this thread,
> I believe we should also print the complex values of z
> fulfilling is.na(z)  by their  Re & Im, i.e., e.g.
>
>   NA+iNA  (or NaN+iNA or NA+iNaN or NaN+iNaN
>   NA+0i,  NaN+1i,  3+iNaN, 4+iNA  etc

oops, I should have paid more attention to your first post in this 
thread, sorry for that.

And yes, I totally agree with improving the printing of complexes when 
Re and/or Im is NA

Best,

H.

>
> but note that the exact printing itself should *not* become the topic of this
> thread unless by mentioning that I strongly believe the print()ing
> of complex vectors in R should change anway *and* for that reason,
> the printing / "looks the same as" / ...  should not be strong
> reasons in my view for deciding how  *coercion*,
> notably  as.complex(.)   should work.
>
> Martin
>
>
>      >> With your proposal, for a (potentially large) vector of complex numbers,
>      >> after
>      >> Re(z)  <-  1/2
>      >>
>      >> I could no longer rely on   Re(z) == 1/2,
>      >> because it would be wrong for those z where (the imaginary part/ the number)
>      >> was NA/NaN.
>
>      > My proposal is to do this only if the Re and/or Im parts are NAs, not if
>      > they are NaNs.
>
>      > BTW the difference between how NAs and NaNs are treated in complex
>      > vectors is another issue that adds to the confusion:
>
>      > ? > complex(r=NA, i=2)
>      > ? [1] NA
>      > ? > complex(r=NaN, i=2)
>      > ? [1] NaN+2i
>
>      > Not displaying the real + imaginary parts in the NA case kind of
>      > suggests that somehow they are gone i.e. that Re(z) and Im(z) are both NA.
>
>      > Note that my proposal is not to change the display but to change Re()
>      > and Im() to make them consistent with the display.
>
>      > In your Re(z) <- 1/2 example (which seems to be theoretical only because
>      > I don't see `Re<-` in base R), any NA in 'z' would be replaced with
>      > complex(r=NaN, i=1/2), so you could rely on Re(z) == 1/2.
>
>      >> Also, in a similar case, a
>      >>
>      >> Im(z) <- NA
>      >>
>      >> would have to "destroy" all real parts  Re(z);
>      >> not really typically in memory, but effectively for the user,  Re(z)
>      >> would be all NA/NaN.
>      > Yes, setting a value to NA destroys it beyond repair in the sense that
>      > there's no way you can retrieve any original parts of it. I'm fine with
>      > that. I'm not fine with an NA being used to store hidden information.
>      >>
>      >> And I think there are quite a few other situations
>      >> where looking at Re() and Im() separately makes a lot of sense.
>      > Still doable if the Re or Im parts contain NaNs.
>      >>
>      >> Spencer also made a remark in this direction.
>      >>
>      >> All in all I'd be very reluctant to move in this direction;
>      >> but yes, I'm just one person ... let's continue musing and
>      >> considering !
>
>      > I understand the reluctance since this would not be a light move, but
>      > thanks for considering.
>
>      > Best,
>
>      > H.
>
>      >>
>      >> Martin
>      >>
>      >> >>> On 9/22/23 03:38, Martin Maechler wrote:
>      >> >>>>>>>>> Mikael Jagan ????? on Thu, 21 Sep 2023 00:47:39
>      >> >>>>>>>>> -0400 writes:
>      >> >>>> ????? > Revisiting this thread from April:
>      >> >>>>
>      >> >>>> >https://stat.ethz.ch/pipermail/r-devel/2023-April/082545.html
>      >> >>>>
>      >> >>>> ????? > where the decision (not yet backported) was
>      >> >>>> made for ????? > as.complex(NA_real_) to give
>      >> >>>> NA_complex_ instead of ????? > complex(r=NA_real_,
>      >> >>>> i=0), to be consistent with ????? > help("as.complex")
>      >> >>>> and as.complex(NA) and as.complex(NA_integer_).
>      >> >>>>
>      >> >>>> ????? > Was any consideration given to the alternative?
>      >> >>>> ????? > That is, to changing as.complex(NA) and
>      >> >>>> as.complex(NA_integer_) to ????? > give
>      >> >>>> complex(r=NA_real_, i=0), consistent with ????? >
>      >> >>>> as.complex(NA_real_), then amending help("as.complex")
>      >> >>>> ????? > accordingly?
>      >> >>>>
>      >> >>>> Hmm, as, from R-core, mostly I was involved, I admit to
>      >> >>>> say "no", to my knowledge the (above) alternative
>      >> >>>> wasn't considered.
>      >> >>>>
>      >> >>>> ??? > The principle that ??? >
>      >> >>>> Im(as.complex(<real=(double|integer|logical)>)) should
>      >> >>>> be zero ??? > is quite fundamental, in my view, hence
>      >> >>>> the "new" behaviour ??? > seems to really violate the
>      >> >>>> principle of least surprise ...
>      >> >>>>
>      >> >>>> of course "least surprise"? is somewhat subjective.
>      >> >>>> Still, I clearly agree that the above would be one
>      >> >>>> desirable property.
>      >> >>>>
>      >> >>>> I think that any solution will lead to *some* surprise
>      >> >>>> for some cases, I think primarily because there are
>      >> >>>> *many* different values z? for which? is.na(z)? is
>      >> >>>> true,? and in any case NA_complex_? is only of the
>      >> >>>> many.
>      >> >>>>
>      >> >>>> I also agree with Mikael that we should reconsider the
>      >> >>>> issue that was raised by Davis Vaughan here ("on
>      >> >>>> R-devel") last April.
>      >> >>>>
>      >> >>>> ????? > Another (but maybe weaker) argument is that
>      >> >>>> ????? > double->complex coercions happen more often
>      >> >>>> than ????? > logical->complex and integer->complex
>      >> >>>> ones. Changing the ????? > behaviour of the more
>      >> >>>> frequently performed coercion is ????? > more likely to
>      >> >>>> affect code "out there".
>      >> >>>>
>      >> >>>> ????? > Yet another argument is that one expects
>      >> >>>>
>      >> >>>> ????? >????? identical(as.complex(NA_real_), NA_real_ +
>      >> >>>> (0+0i))
>      >> >>>>
>      >> >>>> ????? > to be TRUE, i.e., that coercing from double to
>      >> >>>> complex is ????? > equivalent to adding a complex
>      >> >>>> zero.? The new behaviour ????? > makes the above FALSE,
>      >> >>>> since NA_real_ + (0+0i) gives ????? >
>      >> >>>> complex(r=NA_real_, i=0).
>      >> >>>>
>      >> >>>> No!? --- To my own surprise (!) --- in current R-devel
>      >> >>>> the above is TRUE, and ??????? NA_real_ + (0+0i)? , the
>      >> >>>> same as ??????? NA_real_ + 0i????? , really gives
>      >> >>>> complex(r=NA, i=NA) :
>      >> >>>>
>      >> >>>> Using showC() from ?complex
>      >> >>>>
>      >> >>>> ??? showC <- function(z) noquote(sprintf("(R = %g, I =
>      >> >>>> %g)", Re(z), Im(z)))
>      >> >>>>
>      >> >>>> we see (in R-devel) quite consistently
>      >> >>>>
>      >> >>>>> showC(NA_real_ + 0i)
>      >> >>>> [1] (R = NA, I = NA)
>      >> >>>>> showC(NA?????? + 0i)? # NA is 'logical'
>      >> >>>> [1] (R = NA, I = NA) where as in R 4.3.1 and
>      >> >>>> "R-patched" -- *in*consistently
>      >> >>>>
>      >> >>>>> showC(NA_real_ + 0i)
>      >> >>>> [1] (R = NA, I = 0)
>      >> >>>>> showC(NA + 0i)
>      >> >>>> [1] (R = NA, I = NA) .... and honestly, I do not see
>      >> >>>> *where* (and when) we changed the underlying code (in
>      >> >>>> arithmetic.c !?)? in R-devel to *also* produce
>      >> >>>> NA_complex_? in such complex *arithmetic*
>      >> >>>>
>      >> >>>>
>      >> >>>> ????? > Having said that, one might also (but more
>      >> >>>> naively) expect
>      >> >>>>
>      >> >>>> ????? >
>      >> >>>> identical(as.complex(as.double(NA_complex_)),
>      >> >>>> NA_complex_)
>      >> >>>>
>      >> >>>> ????? > to be TRUE.
>      >> >>>>
>      >> >>>> as in current R-devel
>      >> >>>>
>      >> >>>> ????? > Under my proposal it continues to be FALSE.
>      >> >>>>
>      >> >>>> as in "R-release"
>      >> >>>>
>      >> >>>> ????? > Well, I'd prefer if it gave FALSE with a
>      >> >>>> warning ????? > "imaginary parts discarded in
>      >> >>>> coercion", but it seems that ????? >
>      >> >>>> as.double(complex(r=a, i=b)) never warns when either of
>      >> >>>> ????? > 'a' and 'b' is NA_real_ or NaN, even where
>      >> >>>> "information" ????? > {nonzero 'b'} is clearly lost ...
>      >> >>>>
>      >> >>>> The question of *warning* here is related indeed, but I
>      >> >>>> think we should try to look at it only *secondary* to
>      >> >>>> your first proposal.
>      >> >>>>
>      >> >>>> ????? > Whatever decision is made about
>      >> >>>> as.complex(NA_real_), ????? > maybe these points should
>      >> >>>> be weighed before it becomes part of ????? > R-release
>      >> >>>> ...
>      >> >>>>
>      >> >>>> ????? > Mikael
>      >> >>>>
>      >> >>>> Indeed.
>      >> >>>>
>      >> >>>> Can we please get other opinions / ideas here?
>      >> >>>>
>      >> >>>> Thank you in advance for your thoughts!  Martin
>      >> >>>>
>      >> >>>> ---
>      >> >>>>
>      >> >>>> PS:
>      >> >>>>
>      >> >>>> ?? Our *print()*ing? of complex NA's ("NA" here meaning
>      >> >>>> NA or NaN) ?? is also unsatisfactory, e.g. in the case
>      >> >>>> where all entries of a ?? vector are NA in the sense of
>      >> >>>> is.na(.), but their ?? Re() and Im() are not all NA:
>      >> >>>> ??? showC <- function(z) noquote(sprintf("(R = %g, I =
>      >> >>>> %g)", Re(z), Im(z))) ??? z <- complex(, c(11, NA, NA),
>      >> >>>> c(NA, 99, NA)) ??? z ??? showC(z)
>      >> >>>>
>      >> >>>> gives
>      >> >>>>
>      >> >>>> ??? > z ??? [1] NA NA NA ??? > showC(z) ??? [1] (R =
>      >> >>>> 11, I = NA) (R = NA, I = 99) (R = NA, I = NA)
>      >> >>>>
>      >> >>>> but that (printing of complex) *is* another issue, in
>      >> >>>> which we have the re-opened bugzilla PR#16752
>      >> >>>> ==>https://bugs.r-project.org/show_bug.cgi?id=16752
>      >> >>>>
>      >> >>>> on which we also worked during the R Sprint in Warwick
>      >> >>>> three weeks ago, and where I want to commit changes in
>      >> >>>> any case {but think we should change even a bit more
>      >> >>>> than we got to during the Sprint}.
>      >> >>>>
>      >> >>>> ______________________________________________
>      >> >>>>R-devel at r-project.org  ? mailing list
>      >> >>>>https://stat.ethz.ch/mailman/listinfo/r-devel
>      >> >>>
>      >> >>
>      >> > --
>      >> > Herv? Pag?s
>      >>
>      >> > Bioconductor CoreTeamhpages.on.github at gmail.com
>      >>
>      >>
>      >>
>      > --
>      > Herv? Pag?s
>
>      > Bioconductor Core Team
>      >hpages.on.github at gmail.com
>
>      > [[alternative HTML version deleted]]
>
-- 
Herv? Pag?s

Bioconductor Core Team
hpages.on.github at gmail.com

	[[alternative HTML version deleted]]


From murdoch@dunc@n @end|ng |rom gm@||@com  Mon Sep 25 21:06:15 2023
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Mon, 25 Sep 2023 15:06:15 -0400
Subject: [Rd] Tight bounding box around text in graphics?
Message-ID: <2e926b05-ac9d-4884-8daf-a95d4b879837@gmail.com>

I've mentioned in previous messages that I'm trying to redo rgl text.

Part of what I need is to measure the size of strings in pixels when 
they are drawn by base graphics.

It appears that

   strwidth(texts, "user", cex = cex, font = font, family = family)

gives accurate measurements of the width in user coordinates.  I've got 
those set up to match pixels, so I'm fine here.

However, the equivalent call for strheight() only measures height above 
the baseline according to the docs, and indeed the number is smaller 
than the size of what's displayed.  Descenders (e.g. the tail of "y") 
aren't counted.

Is there a way to measure how far a character might descend?  Is it 
valid to assume it won't descend more than a line height below the top 
of the char?

I have a partial solution -- textshaping::shape_text gives a "height" 
value that includes lots of space below the character, and a 
"top_border" value that measures from the top of the textbox to the 
baseline.  So I think `height - top_border` would give me what I'm 
asking for.  But this only works with graphics devices in the ragg 
package.  Is there a general solution?

Duncan Murdoch


From p@u| @end|ng |rom @t@t@@uck|@nd@@c@nz  Mon Sep 25 21:53:40 2023
From: p@u| @end|ng |rom @t@t@@uck|@nd@@c@nz (Paul Murrell)
Date: Tue, 26 Sep 2023 08:53:40 +1300
Subject: [Rd] Tight bounding box around text in graphics?
In-Reply-To: <2e926b05-ac9d-4884-8daf-a95d4b879837@gmail.com>
References: <2e926b05-ac9d-4884-8daf-a95d4b879837@gmail.com>
Message-ID: <0ca43560-f654-d7ef-360a-24fc1d849e2e@stat.auckland.ac.nz>

Hi

strheight(), which is based GEStrHeight(), is pretty crude, not only 
ignoring descenders, but also only considering the ascent of the overall 
font (capital "M").

There is a GEStrMetric(), which returns character-specific ascent and 
descent, but that is only currently exposed via grid::stringAscent() and 
grid::stringDescent().  There is also grid::stringHeight(), which is as 
unsubtle as strheight().

For example, these are all the same (just font ascent) ...

 > strheight("y", "in")
[1] 0.1248031
 > strheight("x", "in")
[1] 0.1248031
 > strheight("M", "in")
[1] 0.1248031

... and these are all the same ...

 > convertHeight(stringHeight("y"), "in")
[1] 0.124803149606299inches
 > convertHeight(stringHeight("x"), "in")
[1] 0.124803149606299inches
 > convertHeight(stringAscent("M"), "in")
[1] 0.124803149606299inches

... but these have more detail ...

 > convertHeight(stringAscent("y"), "in")
[1] 0.0936023622047244inches
 > convertHeight(stringDescent("y"), "in")
[1] 0.0416010498687664inches
 > convertHeight(stringAscent("x"), "in")
[1] 0.0936023622047244inches
 > convertHeight(stringDescent("x"), "in")
[1] 0inches
 > convertHeight(stringHeight("M"), "in")
[1] 0.124803149606299inches
 > convertHeight(stringDescent("M"), "in")
[1] 0inches

In theory, it should not be difficult to add a graphics::strascent() and 
graphics::strdescent() if that would help.

Paul

On 26/09/23 08:06, Duncan Murdoch wrote:
> I've mentioned in previous messages that I'm trying to redo rgl text.
> 
> Part of what I need is to measure the size of strings in pixels when
> they are drawn by base graphics.
> 
> It appears that
> 
> strwidth(texts, "user", cex = cex, font = font, family = family)
> 
> gives accurate measurements of the width in user coordinates. I've got
> those set up to match pixels, so I'm fine here.
> 
> However, the equivalent call for strheight() only measures height above
> the baseline according to the docs, and indeed the number is smaller
> than the size of what's displayed. Descenders (e.g. the tail of "y")
> aren't counted.
> 
> Is there a way to measure how far a character might descend? Is it
> valid to assume it won't descend more than a line height below the top
> of the char?
> 
> I have a partial solution -- textshaping::shape_text gives a "height"
> value that includes lots of space below the character, and a
> "top_border" value that measures from the top of the textbox to the
> baseline. So I think `height - top_border` would give me what I'm
> asking for. But this only works with graphics devices in the ragg
> package. Is there a general solution?
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel 
> <https://stat.ethz.ch/mailman/listinfo/r-devel>

-- 
Dr Paul Murrell
Te Kura Tatauranga | Department of Statistics
Waipapa Taumata Rau | The University of Auckland
Private Bag 92019, Auckland 1142, New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
www.stat.auckland.ac.nz/~paul/


From p@u| @end|ng |rom @t@t@@uck|@nd@@c@nz  Tue Sep 26 01:39:57 2023
From: p@u| @end|ng |rom @t@t@@uck|@nd@@c@nz (Paul Murrell)
Date: Tue, 26 Sep 2023 12:39:57 +1300
Subject: [Rd] proposal: 'dev.capabilities()' can also query Unicode
 capabilities of current graphics device
In-Reply-To: <20230923234347.27eb5def@parabola>
References: <CAMigB8EaoRNacP0Z4QUMXjegnks1jCOrx2eN5Z-EK63wtqsLmA@mail.gmail.com>
 <25866.52214.850601.243688@stat.math.ethz.ch>
 <20230923234347.27eb5def@parabola>
Message-ID: <c18a6c45-ee8d-a55f-6a96-f539a5154f15@stat.auckland.ac.nz>

Hi

Yes, you can set up your own font and TeX installations are a good 
source of Type 1 fonts.  Here is an example (paths obviously specific to 
my [Ubuntu 20.04] OS and TeX installation) ...


cmlgc <- Type1Font("cmlgc",
 
rep("/usr/share/texlive/texmf-dist/fonts/afm/public/cm-lgc/fcmr6z.afm", 4),
                    encoding="Cyrillic")
pdfFonts(cmlgc=cmlgc)

x <- '\u410\u411\u412'
pdf("cmlgc.pdf", family="cmlgc", encoding="Cyrillic")
plot(1:10, main = x)
dev.off()

embedFonts("cmlgc.pdf", out="cmlgc-embed.pdf",
 
fontpaths="/usr/share/texlive/texmf-dist/fonts/type1/public/cm-lgc/")


Final result attached.

Thanks for the patch for the unrelated memory problem;  I will take a 
look at that.

Paul

On 24/09/23 09:43, Ivan Krylov wrote:
> On Wed, 20 Sep 2023 12:39:50 +0200
> Martin Maechler <maechler at stat.math.ethz.ch> wrote:
> 
>  > The problem is that some pdf *viewers*,
>  > notably `evince` on Fedora Linux, for several years now,
>  > do *not* show *some* of the UTF-8 glyphs because they do not use
>  > the correct fonts
> 
> One more problem that makes it nontrivial to use Unicode with pdf() is
> the graphics device not knowing some of the font metrics:
> 
> x <- '\u410\u411\u412'
> pdf()
> plot(1:10, main = x)
> # Warning messages:
> # 1: In title(...) : font width unknown for character 0xb0
> # 2: In title(...) : font width unknown for character 0xe4
> # 3: In title(...) : font width unknown for character 0xfc
> # 4: In title(...) : font width unknown for character 0x7f
> dev.off()
> 
> In the resulting PDF file, the three letters are visible, at least in
> Evince 3.38.2, but they are all positioned in the same space.
> 
> I understand that this is strictly speaking not pdf()'s fault
> (grDevices contains the font metrics for all standard Adobe fonts and a
> few more), but I'm not sure what to do as a user. Should I call
> pdfFonts(...), declaring a font with all symbols I need? Where does one
> even get Type-1 Cyrillic Helvetica (or any other font) with separate
> font metrics files for use with pdf()?
> 
> Actually, the wrong number of sometimes random character codes reminds
> me of stack garbage. In src/library/grDevices/src/devPS.c, function
> static double PostScriptStringWidth, there's this bit of code:
> 
> if(!strIsASCII((char *) str) &&
> /*
> * Every fifth font is a symbol font:
> * see postscriptFonts()
> */
> (face % 5) != 0) {
> R_CheckStack2(strlen((char *)str)+1);
> char buff[strlen((char *)str)+1];
> /* Output string cannot be longer */
> mbcsToSbcs((char *)str, buff, encoding, enc);
> str1 = (unsigned char *)buff;
> }
> 
> Later the characters in str1 are iterated over in order to calculate
> the total width of the string. I didn't notice this myself until I saw
> in the debugger that after a few iterations of the loop, the contents
> of str1 are completely different from the result of mbcsToSbcs((char
> *)str, buff, encoding, enc), and went to investigate. Only after the
> debugger told me that there's no variable called "buff" I realised that
> the VLA pointed to by str1 no longer exists.
> 
> --- src/library/grDevices/src/devPS.c (revision 85214)
> +++ src/library/grDevices/src/devPS.c (working copy)
> @@ -721,6 +721,8 @@
> unsigned char p1, p2;
> 
> int status;
> + /* May be about to allocate */
> + void *alloc = vmaxget();
> if(!metrics && (face % 5) != 0) {
> /* This is the CID font case, and should only happen for
> non-symbol fonts. So we assume monospaced with multipliers.
> @@ -755,9 +757,8 @@
> * Every fifth font is a symbol font:
> * see postscriptFonts()
> */
> - (face % 5) != 0) {
> - R_CheckStack2(strlen((char *)str)+1);
> - char buff[strlen((char *)str)+1];
> + (face % 5) != 0 && metrics) {
> + char *buff = R_alloc(strlen((char *)str)+1, 1);
> /* Output string cannot be longer */
> mbcsToSbcs((char *)str, buff, encoding, enc);
> str1 = (unsigned char *)buff;
> @@ -792,6 +793,7 @@
> }
> }
> }
> + vmaxset(alloc);
> return 0.001 * sum;
> }
> 
> 
> 
> After this patch, I'm consistently getting the right character codes in
> the warnings, but I still don't know how to set up the font metrics.
> 
> -- 
> Best regards,
> Ivan
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel 
> <https://stat.ethz.ch/mailman/listinfo/r-devel>

-- 
Dr Paul Murrell
Te Kura Tatauranga | Department of Statistics
Waipapa Taumata Rau | The University of Auckland
Private Bag 92019, Auckland 1142, New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
www.stat.auckland.ac.nz/~paul/

-------------- next part --------------
A non-text attachment was scrubbed...
Name: cmlgc-embed.pdf
Type: application/pdf
Size: 10746 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20230926/c4365ddc/attachment.pdf>

