From pgilbert902 at gmail.com  Thu Sep  1 02:45:17 2016
From: pgilbert902 at gmail.com (Paul Gilbert)
Date: Wed, 31 Aug 2016 20:45:17 -0400
Subject: [Rd] A bug in the R Mersenne Twister (RNG) code?
In-Reply-To: <1955d948-06dd-1c0c-6de1-32b0805152e6@gmail.com>
References: <61f698e6-c432-0dfe-630b-d6d45a78dc05@gmail.com>
	<1955d948-06dd-1c0c-6de1-32b0805152e6@gmail.com>
Message-ID: <5c849045-478c-5fa6-b436-8e707a16f98a@gmail.com>



On 08/30/2016 06:29 PM, Duncan Murdoch wrote:
> I don't see evidence of a bug.  There have been several versions of the
> MT; we may be using a different version than you are.  Ours is the
> 1999/10/28 version; the web page you cite uses one from 2002.
>
> Perhaps the newer version fixes some problems, and then it would be
> worth considering a change.  But changing the default RNG definitely
> introduces problems in reproducibility,

Well "problems in reproducibility" is a bit vague. Results would always 
be reproducible by specifying kind="Mersenne-Twister" or kind="Buggy
Kinderman-Ramage" for older results, so there is no problem reproducing 
results. The only problem is that users expecting to reproduce results 
twenty years later will need to know what random generator they used. 
(BTW, they may also need to record information about the normal or other 
generator, as well as the seed.) Of course, these changes are recorded 
pretty well for R, so the history of "default" can always be found.

I think it is a mistake to encourage users into thinking they do not 
need to keep track of some information if they want reproducibility. 
Perhaps the default should be changed more often in order to encourage 
better user habits.

More seriously, I think "default" should continue to be something that 
is currently considered to be good. So, if there really is a known 
problem, then I think "default" should be changed.

(And, no I did not get burned by the R 1.7.0 change in the default 
generator. I got burned by a much earlier, unadvertised, and more subtle 
change in the Splus generator.)

Paul Gilbert

so it's not obvious that we
> would do it.
>
> Duncan Murdoch
>
>
> On 30/08/2016 5:45 PM, Mark Roberts wrote:
>> Whomever,
>>
>> I recently sent the "bug report" below toR-core at r-project.org and have
>> just been asked to instead submit it to you.
>>
>> Although I am basically not an R user, I have installed version 3.3.1
>> and am also the author of a statistics program written in Visual Basic
>> that contains a component which correctly implements the Mersenne
>> Twister (MT) algorithm.  I believe that it is not possible to generate
>> the correct stream of pseudorandom numbers using the MT default random
>> number generator in R, and am not the first person to notice this.  Here
>> is a posted 2013 entry
>> (www.r-bloggers.com/reproducibility-and-randomness/) on an R website
>> that asserts that the SAS computer program implementation of the MT
>> algorithm produces different numbers than R does when using the same
>> starting seed number.  The author of this post didn?t get anyone to
>> respond to his query about the reason for this SAS vs. R discrepancy.
>>
>> There are two ways of initializing the original MT computer program
>> (written in C) so that an identical stream of numbers can be repeatedly
>> generated:  1) with a particular integer seed number, and 2) with a
>> particular array of integers.   In the 'compilation and usage' section
>> of this webpage (https://github.com/cslarsen/mersenne-twister) there is
>> a listing of the first 200 random numbers the MT algorithm should
>> produce for seed number = 1.  The inventors of the Mersenne Twister
>> random number generator provided two different sets of the first 1000
>> numbers produced by a correctly coded 32-bit implementation of the MT
>> algorithm when initializing it with a particular array of integers at:
>> www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/MT2002/CODES/mt19937ar.out.
>> [There is a link to this output at:
>> www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/MT2002/emt19937ar.html.]
>>
>> My statistics program obtains exactly those 200 numbers from the first
>> site mentioned in the previous paragraph and also obtains those same
>> numbers from the second website (though I didn't check all 2000 values).
>>    Assuming that the MT code within R uses the 32-bit MT algorithm, I
>> suspect that the current version of R can't do that.  If you (i.e.,
>> anyone who might knowledgeably respond to this report) is able to
>> duplicate those reference test-values, then please send me the R code to
>> initialize the MT code within R to successfully do that, and I apologize
>> for having wasted your time. If you (collectively) can't do that, then R
>> is very likely using incorrectly implemented MT code.  And if this
>> latter possibility is true, it seems to me that this is something that
>> should be fixed.
>>
>> Mark Roberts, Ph.D.
>>
>>     [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From gmbecker at ucdavis.edu  Thu Sep  1 17:34:31 2016
From: gmbecker at ucdavis.edu (Gabriel Becker)
Date: Thu, 1 Sep 2016 08:34:31 -0700
Subject: [Rd] A bug in the R Mersenne Twister (RNG) code?
In-Reply-To: <5c849045-478c-5fa6-b436-8e707a16f98a@gmail.com>
References: <61f698e6-c432-0dfe-630b-d6d45a78dc05@gmail.com>
	<1955d948-06dd-1c0c-6de1-32b0805152e6@gmail.com>
	<5c849045-478c-5fa6-b436-8e707a16f98a@gmail.com>
Message-ID: <CADwqtCOib2VoeC=ufPt+jFk68GvzuSceJ7kJ+rrXGae0emzKXw@mail.gmail.com>

I wonder how useful a (set of?) "time machine" functions which look up
/infer things like this based on a date would be. Could ease the pain of
changes generally, though not remove it completely.

~G

On Wed, Aug 31, 2016 at 5:45 PM, Paul Gilbert <pgilbert902 at gmail.com> wrote:

>
>
> On 08/30/2016 06:29 PM, Duncan Murdoch wrote:
>
>> I don't see evidence of a bug.  There have been several versions of the
>> MT; we may be using a different version than you are.  Ours is the
>> 1999/10/28 version; the web page you cite uses one from 2002.
>>
>> Perhaps the newer version fixes some problems, and then it would be
>> worth considering a change.  But changing the default RNG definitely
>> introduces problems in reproducibility,
>>
>
> Well "problems in reproducibility" is a bit vague. Results would always be
> reproducible by specifying kind="Mersenne-Twister" or kind="Buggy
> Kinderman-Ramage" for older results, so there is no problem reproducing
> results. The only problem is that users expecting to reproduce results
> twenty years later will need to know what random generator they used. (BTW,
> they may also need to record information about the normal or other
> generator, as well as the seed.) Of course, these changes are recorded
> pretty well for R, so the history of "default" can always be found.
>
> I think it is a mistake to encourage users into thinking they do not need
> to keep track of some information if they want reproducibility. Perhaps the
> default should be changed more often in order to encourage better user
> habits.
>
> More seriously, I think "default" should continue to be something that is
> currently considered to be good. So, if there really is a known problem,
> then I think "default" should be changed.
>
> (And, no I did not get burned by the R 1.7.0 change in the default
> generator. I got burned by a much earlier, unadvertised, and more subtle
> change in the Splus generator.)
>
> Paul Gilbert
>
>
> so it's not obvious that we
>
>> would do it.
>>
>> Duncan Murdoch
>>
>>
>> On 30/08/2016 5:45 PM, Mark Roberts wrote:
>>
>>> Whomever,
>>>
>>> I recently sent the "bug report" below toR-core at r-project.org and have
>>> just been asked to instead submit it to you.
>>>
>>> Although I am basically not an R user, I have installed version 3.3.1
>>> and am also the author of a statistics program written in Visual Basic
>>> that contains a component which correctly implements the Mersenne
>>> Twister (MT) algorithm.  I believe that it is not possible to generate
>>> the correct stream of pseudorandom numbers using the MT default random
>>> number generator in R, and am not the first person to notice this.  Here
>>> is a posted 2013 entry
>>> (www.r-bloggers.com/reproducibility-and-randomness/) on an R website
>>> that asserts that the SAS computer program implementation of the MT
>>> algorithm produces different numbers than R does when using the same
>>> starting seed number.  The author of this post didn?t get anyone to
>>> respond to his query about the reason for this SAS vs. R discrepancy.
>>>
>>> There are two ways of initializing the original MT computer program
>>> (written in C) so that an identical stream of numbers can be repeatedly
>>> generated:  1) with a particular integer seed number, and 2) with a
>>> particular array of integers.   In the 'compilation and usage' section
>>> of this webpage (https://github.com/cslarsen/mersenne-twister) there is
>>> a listing of the first 200 random numbers the MT algorithm should
>>> produce for seed number = 1.  The inventors of the Mersenne Twister
>>> random number generator provided two different sets of the first 1000
>>> numbers produced by a correctly coded 32-bit implementation of the MT
>>> algorithm when initializing it with a particular array of integers at:
>>> www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/MT2002/CODES/mt19937ar.out.
>>> [There is a link to this output at:
>>> www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/MT2002/emt19937ar.html.]
>>>
>>> My statistics program obtains exactly those 200 numbers from the first
>>> site mentioned in the previous paragraph and also obtains those same
>>> numbers from the second website (though I didn't check all 2000 values).
>>>    Assuming that the MT code within R uses the 32-bit MT algorithm, I
>>> suspect that the current version of R can't do that.  If you (i.e.,
>>> anyone who might knowledgeably respond to this report) is able to
>>> duplicate those reference test-values, then please send me the R code to
>>> initialize the MT code within R to successfully do that, and I apologize
>>> for having wasted your time. If you (collectively) can't do that, then R
>>> is very likely using incorrectly implemented MT code.  And if this
>>> latter possibility is true, it seems to me that this is something that
>>> should be fixed.
>>>
>>> Mark Roberts, Ph.D.
>>>
>>>     [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>
>>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>



-- 
Gabriel Becker, PhD
Associate Scientist (Bioinformatics)
Genentech Research

	[[alternative HTML version deleted]]


From maechler at stat.math.ethz.ch  Fri Sep  2 13:56:36 2016
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 2 Sep 2016 13:56:36 +0200
Subject: [Rd] withAutoprint({ .... }) ?
In-Reply-To: <CAPPM_gSzW37A9xQn1qrTEqd6jWx-mE-cOq2i7zEqU08THqJn7A@mail.gmail.com>
References: <ce4bc223-12d1-2618-aa43-cd12c11396eb@gmail.com>
	<CAPPM_gSzW37A9xQn1qrTEqd6jWx-mE-cOq2i7zEqU08THqJn7A@mail.gmail.com>
Message-ID: <20160902115636.4C6BE1A75DF@lynne.stat.math.ethz.ch>

On R-help, with subject
   '[R] source() does not include added code'

>>>>> Joshua Ulrich <josh.m.ulrich at gmail.com>
>>>>>     on Wed, 31 Aug 2016 10:35:01 -0500 writes:

    > I have quantstrat installed and it works fine for me.  If you're
    > asking why the output of t(tradeStats('macross')) isn't being printed,
    > that's because of what's described in the first paragraph in the
    > *Details* section of help("source"):

    > Note that running code via ?source? differs in a few respects from
    > entering it at the R command line.  Since expressions are not
    > executed at the top level, auto-printing is not done.  So you will
    > need to include explicit ?print? calls for things you want to be
    > printed (and remember that this includes plotting by ?lattice?,
    > FAQ Q7.22).



    > So you need:

    > print(t(tradeStats('macross')))

    > if you want the output printed to the console.

indeed, and "of course"" ;-)

As my subject indicates, this is another case, where it would be
very convenient to have a function

   withAutoprint()

so the OP could have (hopefully) have used
   withAutoprint(source(..))
though that would have been equivalent to the already nicely existing

   source(.., print.eval = TRUE)

which works via the  withVisible(.) utility that returns for each
'expression' if it would auto print or not, and then does print (or
not) accordingly.

My own use cases for such a withAutoprint({...})
are demos and examples, sometimes even package tests which I want to print:

Assume I have a nice demo / example on a help page/ ...

foo(..)
(z <- bar(..))
summary(z)
....

where I carefully do print parts (and don't others),
and suddenly I find I want to run that part of the demo /
example / test only in some circumstances, e.g., only when
interactive, but not in BATCH, or only if it is me, the package maintainer,

if( identical(Sys.getenv("USER"), "maechler") ) {
  foo(..)
  (z <- bar(..))
  summary(z)
  ....
}

Now all the auto-printing is gone, and

1) I have to find out which of these function calls do autoprint and wrap
   a print(..) around these, and

2) the result is quite ugly (for an example on a help page etc.)

What I would like in a future R, is to be able to simply wrap the "{
.. } above with an 'withAutoprint(.) :

if( identical(Sys.getenv("USER"), "maechler") ) withAutoprint({
  foo(..)
  (z <- bar(..))
  summary(z)
  ....
})

Conceptually such a function could be written similar to source() with an R
level for loop, treating each expression separately, calling eval(.) etc.
That may cost too much performnace, ... still to have it would be better than
not having the possibility.

----

If you read so far, you'd probably agree that such a function
could be a nice asset in R,
notably if it was possible to do this on the fast C level of R's main
REPL.

Have any of you looked into how this could be provided in R ?
If you know the source a little, you will remember that there's
the global variable  R_Visible  which is crucial here.
The problem with that is that it *is* global, and only available
as that; that the auto-printing "concept" is so linked to "toplevel context"
and that is not easy, and AFAIK not so much centralized in one place in the
source. Consequently, all kind of (very) low level functions manipulate R_Visible
temporarily.... and so a C level implementation of  withAutoprint() may
need considerable more changes than just setting R_Visible to TRUE in one
place. 

Have any efforts / experiments already happened towards providing such
functionality ?


From murdoch.duncan at gmail.com  Fri Sep  2 14:38:20 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Fri, 2 Sep 2016 08:38:20 -0400
Subject: [Rd] withAutoprint({ .... }) ?
In-Reply-To: <20160902115636.4C6BE1A75DF@lynne.stat.math.ethz.ch>
References: <ce4bc223-12d1-2618-aa43-cd12c11396eb@gmail.com>
	<CAPPM_gSzW37A9xQn1qrTEqd6jWx-mE-cOq2i7zEqU08THqJn7A@mail.gmail.com>
	<20160902115636.4C6BE1A75DF@lynne.stat.math.ethz.ch>
Message-ID: <35c7a86f-8198-c7ec-8174-c86b6b1b91d3@gmail.com>

On 02/09/2016 7:56 AM, Martin Maechler wrote:
> On R-help, with subject
>    '[R] source() does not include added code'
>
>>>>>> Joshua Ulrich <josh.m.ulrich at gmail.com>
>>>>>>     on Wed, 31 Aug 2016 10:35:01 -0500 writes:
>
>     > I have quantstrat installed and it works fine for me.  If you're
>     > asking why the output of t(tradeStats('macross')) isn't being printed,
>     > that's because of what's described in the first paragraph in the
>     > *Details* section of help("source"):
>
>     > Note that running code via ?source? differs in a few respects from
>     > entering it at the R command line.  Since expressions are not
>     > executed at the top level, auto-printing is not done.  So you will
>     > need to include explicit ?print? calls for things you want to be
>     > printed (and remember that this includes plotting by ?lattice?,
>     > FAQ Q7.22).
>
>
>
>     > So you need:
>
>     > print(t(tradeStats('macross')))
>
>     > if you want the output printed to the console.
>
> indeed, and "of course"" ;-)
>
> As my subject indicates, this is another case, where it would be
> very convenient to have a function
>
>    withAutoprint()
>
> so the OP could have (hopefully) have used
>    withAutoprint(source(..))
> though that would have been equivalent to the already nicely existing
>
>    source(.., print.eval = TRUE)
>
> which works via the  withVisible(.) utility that returns for each
> 'expression' if it would auto print or not, and then does print (or
> not) accordingly.
>
> My own use cases for such a withAutoprint({...})
> are demos and examples, sometimes even package tests which I want to print:
>
> Assume I have a nice demo / example on a help page/ ...
>
> foo(..)
> (z <- bar(..))
> summary(z)
> ....
>
> where I carefully do print parts (and don't others),
> and suddenly I find I want to run that part of the demo /
> example / test only in some circumstances, e.g., only when
> interactive, but not in BATCH, or only if it is me, the package maintainer,
>
> if( identical(Sys.getenv("USER"), "maechler") ) {
>   foo(..)
>   (z <- bar(..))
>   summary(z)
>   ....
> }
>
> Now all the auto-printing is gone, and
>
> 1) I have to find out which of these function calls do autoprint and wrap
>    a print(..) around these, and
>
> 2) the result is quite ugly (for an example on a help page etc.)
>
> What I would like in a future R, is to be able to simply wrap the "{
> .. } above with an 'withAutoprint(.) :
>
> if( identical(Sys.getenv("USER"), "maechler") ) withAutoprint({
>   foo(..)
>   (z <- bar(..))
>   summary(z)
>   ....
> })
>
> Conceptually such a function could be written similar to source() with an R
> level for loop, treating each expression separately, calling eval(.) etc.
> That may cost too much performnace, ... still to have it would be better than
> not having the possibility.
>
> ----
>
> If you read so far, you'd probably agree that such a function
> could be a nice asset in R,
> notably if it was possible to do this on the fast C level of R's main
> REPL.
>
> Have any of you looked into how this could be provided in R ?
> If you know the source a little, you will remember that there's
> the global variable  R_Visible  which is crucial here.
> The problem with that is that it *is* global, and only available
> as that; that the auto-printing "concept" is so linked to "toplevel context"
> and that is not easy, and AFAIK not so much centralized in one place in the
> source. Consequently, all kind of (very) low level functions manipulate R_Visible
> temporarily.... and so a C level implementation of  withAutoprint() may
> need considerable more changes than just setting R_Visible to TRUE in one
> place.
>
> Have any efforts / experiments already happened towards providing such
> functionality ?

I don't think the performance cost would matter.  If you're printing 
something, you're already slow.  So doing this at the R level would make 
most sense to me --- that's how Sweave and source and knitr do it, so it 
can't be that bad.

Duncan Murdoch


From thomas.petzoldt at tu-dresden.de  Fri Sep  2 14:54:47 2016
From: thomas.petzoldt at tu-dresden.de (Thomas Petzoldt)
Date: Fri, 2 Sep 2016 14:54:47 +0200
Subject: [Rd] CRAN packages maintained by you
In-Reply-To: <678326B1-FEDC-4F37-8AA3-D483C111AE16@illinois.edu>
References: <0342d85781d846a6b16013f00907ab18@CHIHT4.ad.uillinois.edu>
	<678326B1-FEDC-4F37-8AA3-D483C111AE16@illinois.edu>
Message-ID: <39bb7adc-58a6-61c8-9355-a7a20c1cee4e@tu-dresden.de>

Hi,

I have the same problem and, at a first look, the issues reported by the 
CRAN checks seemed easy to fix. However, after checking it again locally 
and on http://win-builder.r-project.org it appeared that GCC 4.9.3 
(Windows, Rtools 3.4), same also on win-builder reports even more 
issues, especially legacy Fortran (mainly Roger's #2 and #3), but also

"warning: ISO C forbids conversion of object pointer to function pointer 
type"

The latter results from using pointers returned by R_ExternalPtrAddr() 
for calling user-defined functions in DLLs, cf. the following thread 
from the very beginning: 
https://stat.ethz.ch/pipermail/r-devel/2004-September/030792.html

What is now expected to do?

1. Is it really the intention to start a complete rewrite of all legacy 
Fortran code?

2. Is there now a better way for calling user functions than 
R_ExternalPtrAddr()?


Many thanks for clarification,

Thomas


Am 28.08.2016 um 23:48 schrieb Roger Koenker:
> Hi Kurt,
>
> I have started to look into this, and I need some guidance about how to
> prioritize my repairs.  There are basically 4 categories of warnings from
> gfortran?s pedantic critique of my packages:
>
> 	1.  Some errant tab characters it doesn?t like,
> 	2.  Too many or too few continue statements
> 	3.  Horrible (and obsolescent) arithmetic and computed gotos
> 	4.  undeclared doubles and dubious conversions
>
> The last category seems relatively easy to fix and is potentially
> important, but the others seem more difficult to fix and altogether
> less important.  The goto issues are all in code that has been written
> long ago by others and imported, e.g. Peyton and Ng?s cholesky.f.
> I?m very reluctant to mess with any of those gotos.  The fact that
> they were declared obsolete long ago doesn?t mean that gfortran
> has any intention of not supporting these constructs in the future,
> does it?
>
> Before devoting more time and energy, which is in short supply
> lately, I like to hear what others are thinking/doing about all this,
> so I?ll copy this to r-devel.
>
> All the best,
> Roger
>
> url:    www.econ.uiuc.edu/~roger            Roger Koenker
> email    rkoenker at uiuc.edu            Department of Economics
> vox:     217-333-4558                University of Illinois
> fax:       217-244-6678                Urbana, IL 61801
>
>
>> On Aug 28, 2016, at 2:36 AM, Kurt Hornik <Kurt.Hornik at wu.ac.at> wrote:
>>
>>
>> Dear maintainers,
>>
>> This concerns the CRAN packages
>
> ----
>>
>> Using gfortran with options -Wall -pedantic to compile your package
>> Fortran code finds important problems, see your package check pages for
>> more information.
>>
>> Can you please fix these problems as quickly as possible?
>>
>> Best
>> -k
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


-- 
Dr. Thomas Petzoldt
Technische Universitaet Dresden
Faculty of Environmental Sciences
Institute of Hydrobiology
01062 Dresden, Germany

Tel.: +49 351 463 34954
Fax:  +49 351 463 37108
E-Mail: thomas.petzoldt at tu-dresden.de
http://tu-dresden.de/Members/thomas.petzoldt

-- limnology and ecological modelling --


From edd at debian.org  Fri Sep  2 16:02:18 2016
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 2 Sep 2016 09:02:18 -0500
Subject: [Rd] CRAN packages maintained by you
In-Reply-To: <39bb7adc-58a6-61c8-9355-a7a20c1cee4e@tu-dresden.de>
References: <0342d85781d846a6b16013f00907ab18@CHIHT4.ad.uillinois.edu>
	<678326B1-FEDC-4F37-8AA3-D483C111AE16@illinois.edu>
	<39bb7adc-58a6-61c8-9355-a7a20c1cee4e@tu-dresden.de>
Message-ID: <22473.34410.209790.211498@max.nulle.part>


On 2 September 2016 at 14:54, Thomas Petzoldt wrote:
| Hi,
| 
| I have the same problem and, at a first look, the issues reported by the 
| CRAN checks seemed easy to fix. However, after checking it again locally 
| and on http://win-builder.r-project.org it appeared that GCC 4.9.3 
| (Windows, Rtools 3.4), same also on win-builder reports even more 
| issues, especially legacy Fortran (mainly Roger's #2 and #3), but also
| 
| "warning: ISO C forbids conversion of object pointer to function pointer 
| type"
| 
| The latter results from using pointers returned by R_ExternalPtrAddr() 
| for calling user-defined functions in DLLs, cf. the following thread 
| from the very beginning: 
| https://stat.ethz.ch/pipermail/r-devel/2004-September/030792.html
| 
| What is now expected to do?
| 
| 1. Is it really the intention to start a complete rewrite of all legacy 
| Fortran code?
| 
| 2. Is there now a better way for calling user functions than 
| R_ExternalPtrAddr()?

See this commit (where I apologize for referring to GitHub as the
non-canonical source, but it presents things in pretty enough manner) by
Brian Ripley just a few days ago:

  https://github.com/wch/r-source/commit/a528a69b98d3e763c39cfabf9b4a9e398651177c

So R 3.4.0 will have R_MakeExternalPtrFn() and R_ExternalPtrAddrFn().

(Hat tip to Duncan's very useful NEWS summary robot-blog which I read daily).

Dirk
 
 
| Many thanks for clarification,
| 
| Thomas
| 
| 
| Am 28.08.2016 um 23:48 schrieb Roger Koenker:
| > Hi Kurt,
| >
| > I have started to look into this, and I need some guidance about how to
| > prioritize my repairs.  There are basically 4 categories of warnings from
| > gfortran?s pedantic critique of my packages:
| >
| > 	1.  Some errant tab characters it doesn?t like,
| > 	2.  Too many or too few continue statements
| > 	3.  Horrible (and obsolescent) arithmetic and computed gotos
| > 	4.  undeclared doubles and dubious conversions
| >
| > The last category seems relatively easy to fix and is potentially
| > important, but the others seem more difficult to fix and altogether
| > less important.  The goto issues are all in code that has been written
| > long ago by others and imported, e.g. Peyton and Ng?s cholesky.f.
| > I?m very reluctant to mess with any of those gotos.  The fact that
| > they were declared obsolete long ago doesn?t mean that gfortran
| > has any intention of not supporting these constructs in the future,
| > does it?
| >
| > Before devoting more time and energy, which is in short supply
| > lately, I like to hear what others are thinking/doing about all this,
| > so I?ll copy this to r-devel.
| >
| > All the best,
| > Roger
| >
| > url:    www.econ.uiuc.edu/~roger            Roger Koenker
| > email    rkoenker at uiuc.edu            Department of Economics
| > vox:     217-333-4558                University of Illinois
| > fax:       217-244-6678                Urbana, IL 61801
| >
| >
| >> On Aug 28, 2016, at 2:36 AM, Kurt Hornik <Kurt.Hornik at wu.ac.at> wrote:
| >>
| >>
| >> Dear maintainers,
| >>
| >> This concerns the CRAN packages
| >
| > ----
| >>
| >> Using gfortran with options -Wall -pedantic to compile your package
| >> Fortran code finds important problems, see your package check pages for
| >> more information.
| >>
| >> Can you please fix these problems as quickly as possible?
| >>
| >> Best
| >> -k
| >
| > ______________________________________________
| > R-devel at r-project.org mailing list
| > https://stat.ethz.ch/mailman/listinfo/r-devel
| >
| 
| 
| -- 
| Dr. Thomas Petzoldt
| Technische Universitaet Dresden
| Faculty of Environmental Sciences
| Institute of Hydrobiology
| 01062 Dresden, Germany
| 
| Tel.: +49 351 463 34954
| Fax:  +49 351 463 37108
| E-Mail: thomas.petzoldt at tu-dresden.de
| http://tu-dresden.de/Members/thomas.petzoldt
| 
| -- limnology and ecological modelling --
| 
| ______________________________________________
| R-devel at r-project.org mailing list
| https://stat.ethz.ch/mailman/listinfo/r-devel
-- 
http://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From kirill.mueller at ivt.baug.ethz.ch  Fri Sep  2 16:48:23 2016
From: kirill.mueller at ivt.baug.ethz.ch (=?UTF-8?Q?Kirill_M=c3=bcller?=)
Date: Fri, 2 Sep 2016 16:48:23 +0200
Subject: [Rd] withAutoprint({ .... }) ?
In-Reply-To: <35c7a86f-8198-c7ec-8174-c86b6b1b91d3@gmail.com>
References: <ce4bc223-12d1-2618-aa43-cd12c11396eb@gmail.com>
	<CAPPM_gSzW37A9xQn1qrTEqd6jWx-mE-cOq2i7zEqU08THqJn7A@mail.gmail.com>
	<20160902115636.4C6BE1A75DF@lynne.stat.math.ethz.ch>
	<35c7a86f-8198-c7ec-8174-c86b6b1b91d3@gmail.com>
Message-ID: <76d861c7-a25f-399d-5ca0-066ef6df00a3@ivt.baug.ethz.ch>

On 02.09.2016 14:38, Duncan Murdoch wrote:
> On 02/09/2016 7:56 AM, Martin Maechler wrote:
>> On R-help, with subject
>>    '[R] source() does not include added code'
>>
>>>>>>> Joshua Ulrich <josh.m.ulrich at gmail.com>
>>>>>>>     on Wed, 31 Aug 2016 10:35:01 -0500 writes:
>>
>>     > I have quantstrat installed and it works fine for me. If you're
>>     > asking why the output of t(tradeStats('macross')) isn't being 
>> printed,
>>     > that's because of what's described in the first paragraph in the
>>     > *Details* section of help("source"):
>>
>>     > Note that running code via ?source? differs in a few respects from
>>     > entering it at the R command line.  Since expressions are not
>>     > executed at the top level, auto-printing is not done. So you will
>>     > need to include explicit ?print? calls for things you want to be
>>     > printed (and remember that this includes plotting by ?lattice?,
>>     > FAQ Q7.22).
>>
>>
>>
>>     > So you need:
>>
>>     > print(t(tradeStats('macross')))
>>
>>     > if you want the output printed to the console.
>>
>> indeed, and "of course"" ;-)
>>
>> As my subject indicates, this is another case, where it would be
>> very convenient to have a function
>>
>>    withAutoprint()
>>
>> so the OP could have (hopefully) have used
>>    withAutoprint(source(..))
>> though that would have been equivalent to the already nicely existing
>>
>>    source(.., print.eval = TRUE)
>>
>> which works via the  withVisible(.) utility that returns for each
>> 'expression' if it would auto print or not, and then does print (or
>> not) accordingly.
>>
>> My own use cases for such a withAutoprint({...})
>> are demos and examples, sometimes even package tests which I want to 
>> print:
>>
>> Assume I have a nice demo / example on a help page/ ...
>>
>> foo(..)
>> (z <- bar(..))
>> summary(z)
>> ....
>>
>> where I carefully do print parts (and don't others),
>> and suddenly I find I want to run that part of the demo /
>> example / test only in some circumstances, e.g., only when
>> interactive, but not in BATCH, or only if it is me, the package 
>> maintainer,
>>
>> if( identical(Sys.getenv("USER"), "maechler") ) {
>>   foo(..)
>>   (z <- bar(..))
>>   summary(z)
>>   ....
>> }
>>
>> Now all the auto-printing is gone, and
>>
>> 1) I have to find out which of these function calls do autoprint and 
>> wrap
>>    a print(..) around these, and
>>
>> 2) the result is quite ugly (for an example on a help page etc.)
>>
>> What I would like in a future R, is to be able to simply wrap the "{
>> .. } above with an 'withAutoprint(.) :
>>
>> if( identical(Sys.getenv("USER"), "maechler") ) withAutoprint({
>>   foo(..)
>>   (z <- bar(..))
>>   summary(z)
>>   ....
>> })
>>
>> Conceptually such a function could be written similar to source() 
>> with an R
>> level for loop, treating each expression separately, calling eval(.) 
>> etc.
>> That may cost too much performnace, ... still to have it would be 
>> better than
>> not having the possibility.
>>
>> ----
>>
>> If you read so far, you'd probably agree that such a function
>> could be a nice asset in R,
>> notably if it was possible to do this on the fast C level of R's main
>> REPL.
>>
>> Have any of you looked into how this could be provided in R ?
>> If you know the source a little, you will remember that there's
>> the global variable  R_Visible  which is crucial here.
>> The problem with that is that it *is* global, and only available
>> as that; that the auto-printing "concept" is so linked to "toplevel 
>> context"
>> and that is not easy, and AFAIK not so much centralized in one place 
>> in the
>> source. Consequently, all kind of (very) low level functions 
>> manipulate R_Visible
>> temporarily.... and so a C level implementation of withAutoprint() may
>> need considerable more changes than just setting R_Visible to TRUE in 
>> one
>> place.
>>
>> Have any efforts / experiments already happened towards providing such
>> functionality ?
>
> I don't think the performance cost would matter.  If you're printing 
> something, you're already slow.  So doing this at the R level would 
> make most sense to me --- that's how Sweave and source and knitr do 
> it, so it can't be that bad.
>
> Duncan Murdoch
>
A C-level implementation would bring the benefit of a lean traceback() 
in case of an error. I suspect eval() could be enhanced to auto-print.

By the same token it would be extremely helpful to have a C-level 
implementation of local() which wouldn't litter the stack trace.


-Kirill


From thomas.petzoldt at tu-dresden.de  Fri Sep  2 17:13:56 2016
From: thomas.petzoldt at tu-dresden.de (Thomas Petzoldt)
Date: Fri, 2 Sep 2016 17:13:56 +0200
Subject: [Rd] CRAN packages maintained by you
In-Reply-To: <22473.34410.209790.211498@max.nulle.part>
References: <0342d85781d846a6b16013f00907ab18@CHIHT4.ad.uillinois.edu>
	<678326B1-FEDC-4F37-8AA3-D483C111AE16@illinois.edu>
	<39bb7adc-58a6-61c8-9355-a7a20c1cee4e@tu-dresden.de>
	<22473.34410.209790.211498@max.nulle.part>
Message-ID: <ff35e292-b4cd-a283-6fbc-de8e7ebf92ae@tu-dresden.de>

Am 02.09.2016 um 16:02 schrieb Dirk Eddelbuettel:
>
> On 2 September 2016 at 14:54, Thomas Petzoldt wrote:
> | Hi,
> |
> | I have the same problem and, at a first look, the issues reported by the
> | CRAN checks seemed easy to fix. However, after checking it again locally
> | and on http://win-builder.r-project.org it appeared that GCC 4.9.3
> | (Windows, Rtools 3.4), same also on win-builder reports even more
> | issues, especially legacy Fortran (mainly Roger's #2 and #3), but also
> |
> | "warning: ISO C forbids conversion of object pointer to function pointer
> | type"
> |
> | The latter results from using pointers returned by R_ExternalPtrAddr()
> | for calling user-defined functions in DLLs, cf. the following thread
> | from the very beginning:
> | https://stat.ethz.ch/pipermail/r-devel/2004-September/030792.html
> |
> | What is now expected to do?
> |
> | 1. Is it really the intention to start a complete rewrite of all legacy
> | Fortran code?
> |
> | 2. Is there now a better way for calling user functions than
> | R_ExternalPtrAddr()?
>
> See this commit (where I apologize for referring to GitHub as the
> non-canonical source, but it presents things in pretty enough manner) by
> Brian Ripley just a few days ago:
>
>   https://github.com/wch/r-source/commit/a528a69b98d3e763c39cfabf9b4a9e398651177c
>
> So R 3.4.0 will have R_MakeExternalPtrFn() and R_ExternalPtrAddrFn().

Thank you very much for this hint, sounds very promising! I was indeed 
looking for something like this in the R docs+sources, but didn't expect 
that it is that hot. Now I found it now also in the canonical svn 
sources ;)

I am little bit concerned, how fast this should be forced by CRAN 
because of back-compatibility, and if compiler derivatives are worth the 
effort for this ...

Remains issue #1 with "Obsolescent features" of legacy Fortran. While 
updating my Fedora test system, it seems that there are many other 
packages around that use this sort of old-style, and well tested (!!!) 
Fortran ...

Thomas

[...]

> | Am 28.08.2016 um 23:48 schrieb Roger Koenker:
> | > Hi Kurt,
> | >
> | > I have started to look into this, and I need some guidance about how to
> | > prioritize my repairs.  There are basically 4 categories of warnings from
> | > gfortran?s pedantic critique of my packages:
> | >
> | > 	1.  Some errant tab characters it doesn?t like,
> | > 	2.  Too many or too few continue statements
> | > 	3.  Horrible (and obsolescent) arithmetic and computed gotos
> | > 	4.  undeclared doubles and dubious conversions
> | >
> | > The last category seems relatively easy to fix and is potentially
> | > important, but the others seem more difficult to fix and altogether
> | > less important.  The goto issues are all in code that has been written
> | > long ago by others and imported, e.g. Peyton and Ng?s cholesky.f.
> | > I?m very reluctant to mess with any of those gotos.  The fact that
> | > they were declared obsolete long ago doesn?t mean that gfortran
> | > has any intention of not supporting these constructs in the future,
> | > does it?
> | >
> | > Before devoting more time and energy, which is in short supply
> | > lately, I like to hear what others are thinking/doing about all this,
> | > so I?ll copy this to r-devel.
> | >
> | > All the best,
> | > Roger
> | >
> | > url:    www.econ.uiuc.edu/~roger            Roger Koenker
> | > email    rkoenker at uiuc.edu            Department of Economics
> | > vox:     217-333-4558                University of Illinois
> | > fax:       217-244-6678                Urbana, IL 61801
> | >
> | >
> | >> On Aug 28, 2016, at 2:36 AM, Kurt Hornik <Kurt.Hornik at wu.ac.at> wrote:
> | >>
> | >>
> | >> Dear maintainers,
> | >>
> | >> This concerns the CRAN packages
> | >
> | > ----
> | >>
> | >> Using gfortran with options -Wall -pedantic to compile your package
> | >> Fortran code finds important problems, see your package check pages for
> | >> more information.
> | >>
> | >> Can you please fix these problems as quickly as possible?
> | >>
> | >> Best
> | >> -k
> | >
> | > ______________________________________________
> | > R-devel at r-project.org mailing list
> | > https://stat.ethz.ch/mailman/listinfo/r-devel
> | >


-- 
Thomas Petzoldt
thomas.petzoldt at tu-dresden.de
http://tu-dresden.de/Members/thomas.petzoldt


From wdunlap at tibco.com  Fri Sep  2 17:33:47 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Fri, 2 Sep 2016 08:33:47 -0700
Subject: [Rd] withAutoprint({ .... }) ?
In-Reply-To: <20160902115636.4C6BE1A75DF@lynne.stat.math.ethz.ch>
References: <ce4bc223-12d1-2618-aa43-cd12c11396eb@gmail.com>
	<CAPPM_gSzW37A9xQn1qrTEqd6jWx-mE-cOq2i7zEqU08THqJn7A@mail.gmail.com>
	<20160902115636.4C6BE1A75DF@lynne.stat.math.ethz.ch>
Message-ID: <CAF8bMcaOVsUsdSUN0GE+1COsb4hYBRaD7qdFcadY--PXB_bJaQ@mail.gmail.com>

Re withAutoprint(), Splus's source() function could take a expression
(literal or not) in place of a file name or text so it could support
withAutoprint-like functionality in its GUI.  E.g.,

> source(auto.print=TRUE, exprs.literal= { x <- 3:7 ; sum(x) ; y <- log(x)
; x - 100}, prompt="--> ")
--> x <- 3:7
--> sum(x)
[1] 25
--> y <- log(x)
--> x - 100
[1] -97 -96 -95 -94 -93

or

> expr <- quote({ x <- 3:7 ; sum(x) ; y <- log(x) ; x - 100})
> source(auto.print=TRUE, exprs = expr, prompt="--> ")
--> x <- 3:7
--> sum(x)
[1] 25
--> y <- log(x)
--> x - 100
[1] -97 -96 -95 -94 -93

It was easy to implement, since exprs's default value is parse(file) or
parse(text=text), which source is calculating anyway.


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Fri, Sep 2, 2016 at 4:56 AM, Martin Maechler <maechler at stat.math.ethz.ch>
wrote:

> On R-help, with subject
>    '[R] source() does not include added code'
>
> >>>>> Joshua Ulrich <josh.m.ulrich at gmail.com>
> >>>>>     on Wed, 31 Aug 2016 10:35:01 -0500 writes:
>
>     > I have quantstrat installed and it works fine for me.  If you're
>     > asking why the output of t(tradeStats('macross')) isn't being
> printed,
>     > that's because of what's described in the first paragraph in the
>     > *Details* section of help("source"):
>
>     > Note that running code via ?source? differs in a few respects from
>     > entering it at the R command line.  Since expressions are not
>     > executed at the top level, auto-printing is not done.  So you will
>     > need to include explicit ?print? calls for things you want to be
>     > printed (and remember that this includes plotting by ?lattice?,
>     > FAQ Q7.22).
>
>
>
>     > So you need:
>
>     > print(t(tradeStats('macross')))
>
>     > if you want the output printed to the console.
>
> indeed, and "of course"" ;-)
>
> As my subject indicates, this is another case, where it would be
> very convenient to have a function
>
>    withAutoprint()
>
> so the OP could have (hopefully) have used
>    withAutoprint(source(..))
> though that would have been equivalent to the already nicely existing
>
>    source(.., print.eval = TRUE)
>
> which works via the  withVisible(.) utility that returns for each
> 'expression' if it would auto print or not, and then does print (or
> not) accordingly.
>
> My own use cases for such a withAutoprint({...})
> are demos and examples, sometimes even package tests which I want to print:
>
> Assume I have a nice demo / example on a help page/ ...
>
> foo(..)
> (z <- bar(..))
> summary(z)
> ....
>
> where I carefully do print parts (and don't others),
> and suddenly I find I want to run that part of the demo /
> example / test only in some circumstances, e.g., only when
> interactive, but not in BATCH, or only if it is me, the package maintainer,
>
> if( identical(Sys.getenv("USER"), "maechler") ) {
>   foo(..)
>   (z <- bar(..))
>   summary(z)
>   ....
> }
>
> Now all the auto-printing is gone, and
>
> 1) I have to find out which of these function calls do autoprint and wrap
>    a print(..) around these, and
>
> 2) the result is quite ugly (for an example on a help page etc.)
>
> What I would like in a future R, is to be able to simply wrap the "{
> .. } above with an 'withAutoprint(.) :
>
> if( identical(Sys.getenv("USER"), "maechler") ) withAutoprint({
>   foo(..)
>   (z <- bar(..))
>   summary(z)
>   ....
> })
>
> Conceptually such a function could be written similar to source() with an R
> level for loop, treating each expression separately, calling eval(.) etc.
> That may cost too much performnace, ... still to have it would be better
> than
> not having the possibility.
>
> ----
>
> If you read so far, you'd probably agree that such a function
> could be a nice asset in R,
> notably if it was possible to do this on the fast C level of R's main
> REPL.
>
> Have any of you looked into how this could be provided in R ?
> If you know the source a little, you will remember that there's
> the global variable  R_Visible  which is crucial here.
> The problem with that is that it *is* global, and only available
> as that; that the auto-printing "concept" is so linked to "toplevel
> context"
> and that is not easy, and AFAIK not so much centralized in one place in the
> source. Consequently, all kind of (very) low level functions manipulate
> R_Visible
> temporarily.... and so a C level implementation of  withAutoprint() may
> need considerable more changes than just setting R_Visible to TRUE in one
> place.
>
> Have any efforts / experiments already happened towards providing such
> functionality ?
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

	[[alternative HTML version deleted]]


From suharto_anggono at yahoo.com  Fri Sep  2 18:10:00 2016
From: suharto_anggono at yahoo.com (Suharto Anggono Suharto Anggono)
Date: Fri, 2 Sep 2016 16:10:00 +0000 (UTC)
Subject: [Rd] Coercion of 'exclude' in function 'factor' (was 'droplevels'
 inappropriate change)
References: <1077826208.388023.1472832600456.ref@mail.yahoo.com>
Message-ID: <1077826208.388023.1472832600456@mail.yahoo.com>

I am basically fine with the change.

How about using just the following?
    if(!is.character(exclude))
        exclude <- as.vector(exclude, typeof(x)) # may result in NA
    x <- as.character(x)

It looks simpler and is, more or less, equivalent.

In factor.Rd, in description of argument 'exclude', "(when \code{x} is a \code{factor} already)" can be removed.


A larger change that, I think, is reasonable is entirely removing the code
    exclude <- as.vector(exclude, typeof(x)) # may result in NA

The explicit coercion of 'exclude' is not necessary. Function 'factor' works without it.

The coercion of 'exclude' may leads to a surprise because it "may result in NA". Example from https://stat.ethz.ch/pipermail/r-help/2005-April/069053.html :
factor(as.integer(c(1,2,3,3,NA)), exclude=NaN)
excludes NA.

As a bonus, without the coercion of 'exclude', 'exclude' can be a factor if 'x' is a factor. This part of an example in https://stat.ethz.ch/pipermail/r-help/2011-April/276274.html works.
cc <- c("x","y","NA")
ff <- factor(cc)
factor(ff,exclude=ff[3])

However, the coercion of 'exclude' has been in function 'factor' in R "forever".
--------------------------------------------
On Wed, 31/8/16, Martin Maechler <maechler at stat.math.ethz.ch> wrote:

 Subject: Re: [Rd] 'droplevels' inappropriate change

 Cc: "Martin Maechler" <maechler at stat.math.ethz.ch>
 Date: Wednesday, 31 August, 2016, 2:51 PM
 
>>>>> Martin Maechler <maechler at stat.math.ethz.ch>
>>>>>     on Sat, 27 Aug 2016 18:55:37 +0200 writes:

>>>>> Suharto Anggono Suharto Anggono via R-devel <r-devel at r-project.org>
>>>>>     on Sat, 27 Aug 2016 03:17:32 +0000 writes:

    >> In R devel r71157, 'droplevels' documentation, in "Arguments" section, says this about argument 'exclude'.
    >> passed to factor(); factor levels which should be excluded from the result even if present.  Note that this was implicitly NA in R <= 3.3.1 which did drop NA levels even when present in x, contrary to the documentation.  The current default is compatible with x[ , drop=FALSE].

    >> The part
    >> x[ , drop=FALSE]
    >> should be
    >> x[ , drop=TRUE]

[[elided Yahoo spam]]
    > a "typo" by me. .. fixed now.

    >> Saying that 'exclude' is factor levels is not quite true for NA element. NA may be not an original level, but NA in 'exclude' affects the result.

    >> For a factor 'x', factor(x, exclude = exclude) doesn't really work for excluding in general. See, for example, https://stat.ethz.ch/pipermail/r-help/2005-September/079336.html .
    >> factor(factor(c("a","b","c")), exclude="c")

    >> However, this excludes "2":
    >> factor(factor(2:3), exclude=2)

    >> Rather unexpectedly, this excludes NA:
    >> factor(factor(c("a",NA), exclude=NULL), exclude="c")

    >> For a factor 'x', factor(x, exclude = exclude) can only exclude integer-like or NA levels. An explanation is in https://stat.ethz.ch/pipermail/r-help/2011-April/276274.html .

    > Well, Peter Dalgaard (in that R-devel e-mail, a bit more than 5
    > years ago) is confirming the problem there,  and suggesting (as
    > you, right?) that actually   `factor()` is not behaving
    > correctly here.

    > And your persistence is finally getting close to convince me
    > that it is not just droplevels(), but  factor() itself which
    > needs care here.

    > Interestingly, the following patch *does* pass 'make check-all'
    > (after small change in tests/reg-tests-1b.R which is ok),
    > and leads to behavior which is much closer to the documentation,
    > notably for your two examples above would give what one would
    > expect.

    > (( If the R-Hub would support experiments with branches of R-devel 
    > from R-core members,  I could just create such a branch and R Hub
    > would run 'R CMD check <pkg>'  for thousands of CRAN packages
    > and provide a web page with the *differences* in the package
    > check results ... so we could see ... ))

    > I do agree that we should strongly consider such a change.

as nobody has commented, I've been liberal and have taken these
no comments as consent.

Hence I have committed

------------------------------------------------------------------------
r71178 | maechler | 2016-08-31 09:45:40 +0200 (Wed, 31 Aug 2016) | 1 line
Changed paths:
   M /trunk/doc/NEWS.Rd
   M /trunk/src/library/base/R/factor.R
   M /trunk/src/library/base/man/factor.Rd
   M /trunk/tests/reg-tests-1b.R
   M /trunk/tests/reg-tests-1c.R

factor(x, exclude) more "rational" when x or exclude are character
------------------------------------------------------------------------

which apart from documentation, examples, and regression tests
is just the patch below.

Martin Maechler
ETH Zurich



    > --- factor.R    (revision 71157)
    > +++ factor.R    (working copy)
    > @@ -28,8 +28,12 @@
    > levels <- unique(y[ind])
    > }
    > force(ordered) # check if original x is an ordered factor
    > -    exclude <- as.vector(exclude, typeof(x)) # may result in NA
    > -    x <- as.character(x)
    > +    if(!is.character(x)) {
    > +    if(!is.character(exclude))
    > +        exclude <- as.vector(exclude, typeof(x)) # may result in NA
    > +    x <- as.character(x)
    > +    } else
    > +    exclude <- as.vector(exclude, typeof(x)) # may result in NA
    > ## levels could be a long vectors, but match will not handle that.
    > levels <- levels[is.na(match(levels, exclude))]
    > f <- match(x, levels)
 Delete Reply Reply All Forward Apply


From luke-tierney at uiowa.edu  Fri Sep  2 19:56:41 2016
From: luke-tierney at uiowa.edu (luke-tierney at uiowa.edu)
Date: Fri, 2 Sep 2016 12:56:41 -0500
Subject: [Rd] withAutoprint({ .... }) ?
In-Reply-To: <76d861c7-a25f-399d-5ca0-066ef6df00a3@ivt.baug.ethz.ch>
References: <ce4bc223-12d1-2618-aa43-cd12c11396eb@gmail.com>
	<CAPPM_gSzW37A9xQn1qrTEqd6jWx-mE-cOq2i7zEqU08THqJn7A@mail.gmail.com>
	<20160902115636.4C6BE1A75DF@lynne.stat.math.ethz.ch>
	<35c7a86f-8198-c7ec-8174-c86b6b1b91d3@gmail.com>
	<76d861c7-a25f-399d-5ca0-066ef6df00a3@ivt.baug.ethz.ch>
Message-ID: <alpine.DEB.2.20.1609021255110.30700@luke-Latitude>

On Fri, 2 Sep 2016, Kirill M?ller wrote:

> On 02.09.2016 14:38, Duncan Murdoch wrote:
>> On 02/09/2016 7:56 AM, Martin Maechler wrote:
>>> On R-help, with subject
>>>    '[R] source() does not include added code'
>>>
>>>>>>>> Joshua Ulrich <josh.m.ulrich at gmail.com>
>>>>>>>>     on Wed, 31 Aug 2016 10:35:01 -0500 writes:
>>>
>>>     > I have quantstrat installed and it works fine for me. If you're
>>>     > asking why the output of t(tradeStats('macross')) isn't being 
>>> printed,
>>>     > that's because of what's described in the first paragraph in the
>>>     > *Details* section of help("source"):
>>>
>>>     > Note that running code via ?source? differs in a few respects from
>>>     > entering it at the R command line.  Since expressions are not
>>>     > executed at the top level, auto-printing is not done. So you will
>>>     > need to include explicit ?print? calls for things you want to be
>>>     > printed (and remember that this includes plotting by ?lattice?,
>>>     > FAQ Q7.22).
>>>
>>>
>>>
>>>     > So you need:
>>>
>>>     > print(t(tradeStats('macross')))
>>>
>>>     > if you want the output printed to the console.
>>>
>>> indeed, and "of course"" ;-)
>>>
>>> As my subject indicates, this is another case, where it would be
>>> very convenient to have a function
>>>
>>>    withAutoprint()
>>>
>>> so the OP could have (hopefully) have used
>>>    withAutoprint(source(..))
>>> though that would have been equivalent to the already nicely existing
>>>
>>>    source(.., print.eval = TRUE)
>>>
>>> which works via the  withVisible(.) utility that returns for each
>>> 'expression' if it would auto print or not, and then does print (or
>>> not) accordingly.
>>>
>>> My own use cases for such a withAutoprint({...})
>>> are demos and examples, sometimes even package tests which I want to 
>>> print:
>>>
>>> Assume I have a nice demo / example on a help page/ ...
>>>
>>> foo(..)
>>> (z <- bar(..))
>>> summary(z)
>>> ....
>>>
>>> where I carefully do print parts (and don't others),
>>> and suddenly I find I want to run that part of the demo /
>>> example / test only in some circumstances, e.g., only when
>>> interactive, but not in BATCH, or only if it is me, the package 
>>> maintainer,
>>>
>>> if( identical(Sys.getenv("USER"), "maechler") ) {
>>>   foo(..)
>>>   (z <- bar(..))
>>>   summary(z)
>>>   ....
>>> }
>>>
>>> Now all the auto-printing is gone, and
>>>
>>> 1) I have to find out which of these function calls do autoprint and 
>>> wrap
>>>    a print(..) around these, and
>>>
>>> 2) the result is quite ugly (for an example on a help page etc.)
>>>
>>> What I would like in a future R, is to be able to simply wrap the "{
>>> .. } above with an 'withAutoprint(.) :
>>>
>>> if( identical(Sys.getenv("USER"), "maechler") ) withAutoprint({
>>>   foo(..)
>>>   (z <- bar(..))
>>>   summary(z)
>>>   ....
>>> })
>>>
>>> Conceptually such a function could be written similar to source() 
>>> with an R
>>> level for loop, treating each expression separately, calling eval(.) 
>>> etc.
>>> That may cost too much performnace, ... still to have it would be 
>>> better than
>>> not having the possibility.
>>>
>>> ----
>>>
>>> If you read so far, you'd probably agree that such a function
>>> could be a nice asset in R,
>>> notably if it was possible to do this on the fast C level of R's main
>>> REPL.
>>>
>>> Have any of you looked into how this could be provided in R ?
>>> If you know the source a little, you will remember that there's
>>> the global variable  R_Visible  which is crucial here.
>>> The problem with that is that it *is* global, and only available
>>> as that; that the auto-printing "concept" is so linked to "toplevel 
>>> context"
>>> and that is not easy, and AFAIK not so much centralized in one place 
>>> in the
>>> source. Consequently, all kind of (very) low level functions 
>>> manipulate R_Visible
>>> temporarily.... and so a C level implementation of withAutoprint() may
>>> need considerable more changes than just setting R_Visible to TRUE in 
>>> one
>>> place.
>>>
>>> Have any efforts / experiments already happened towards providing such
>>> functionality ?
>>
>> I don't think the performance cost would matter.  If you're printing 
>> something, you're already slow.  So doing this at the R level would 
>> make most sense to me --- that's how Sweave and source and knitr do 
>> it, so it can't be that bad.
>>
>> Duncan Murdoch
>>
> A C-level implementation would bring the benefit of a lean traceback() 
> in case of an error. I suspect eval() could be enhanced to auto-print.
>
> By the same token it would be extremely helpful to have a C-level 
> implementation of local() which wouldn't litter the stack trace.

local() within a byte compiled function already produces a less
cluttered traceback, though perhaps not ideal. Moving the interpreted
version closer to the compiled one is in the works.

Best,

luke

>
>
> -Kirill
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
Luke Tierney
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu

From henrik.bengtsson at gmail.com  Fri Sep  2 23:15:58 2016
From: henrik.bengtsson at gmail.com (Henrik Bengtsson)
Date: Fri, 2 Sep 2016 14:15:58 -0700
Subject: [Rd] WISH: Export utils:::findMatches()
Message-ID: <CAFDcVCSm9prdrRpbs2Z9tg2Sea7dZrCo+seGNpr4QK1UYcCVSQ@mail.gmail.com>

WISH:

I'd like make a plea for utils:::findMatches() to be exported such
that anyone can do:

.DollarNames.MyClass <- function(x, pattern="") {
   utils:::findMatches(pattern, names(x))
}

The utils:::findMatches() is agile to the "fuzzy" options, cf.
.DollarNames.  It also doesn't erase what's been typed if there is not
match (see below).


ALTERNATIVE:
I'm aware of that utils:::.DollarNames.default almost do the same
thing, but, unfortunately, that does not work for all data type, e.g.
environment-based objects.  An alternative to utils:::findMatches() is
to use:

.DollarNames.MyClass <- function(x, pattern="") {
   grep(pattern, names(x), value=TRUE)
}

but that has the additional downside to return an empty string "" if
there's no match, which in turn erases whatever the user typed before
hitting TAB.


Thanks

Henrik


From profjcnash at gmail.com  Sat Sep  3 16:26:21 2016
From: profjcnash at gmail.com (ProfJCNash)
Date: Sat, 3 Sep 2016 10:26:21 -0400
Subject: [Rd] Fortran issues. Was CRAN packages maintained by you
In-Reply-To: <mailman.17.1472896804.25538.r-devel@r-project.org>
References: <mailman.17.1472896804.25538.r-devel@r-project.org>
Message-ID: <3ba6a322-9c5d-eea5-3665-c2184d6d871b@gmail.com>


If there is going to be a review of Fortran sources, then there's quite
a bit of checking and testing work as well. As someone who actually
worked with some of the NPL and Argonne and other people, and
occasionally contributed some code, I'm willing to try to help out with
this. However, I will wait to be asked about specific routines.

Note that Yihui Xie and I added a Fortran engine to knitr while at the
UseR!2014. One of my motivations for this was to allow for documentation
of the Fortran code before those of us with greying/missing hair
evaporated. So far not much usage I believe, but this may be a good use
of that possibility.

Best, JN


From mons.magnusson at gmail.com  Mon Sep  5 02:13:15 2016
From: mons.magnusson at gmail.com (=?UTF-8?Q?M=C3=A5ns_Magnusson?=)
Date: Sun, 4 Sep 2016 20:13:15 -0400
Subject: [Rd] Defragmentation of memory
Message-ID: <CAFQDYB35Y45r+MRPQhT+PXwGXSagV19nJohFo5BGpSeyj3SkcQ@mail.gmail.com>

Dear all developers,

I'm working with a lot of textual data in R and need to handle this batch
by batch. The problem is that I read in batches of 10 000 documents and do
some calculations that results in objects that consume quite some memory
(calculate unigrams, 2-grams and 3-grams). In every iteration a new objects
(~ 500 mB) is created (and I can't control the size, so a new object needs
to be created each iteration). The speed of this computations is decreasing
every iteration (first iteration 7 sec, after 30 iterations 20-30 minutes
per iteration).

I (think) I localized the problem to R:s memory handling and that my
approach is fragmenting the memory. If I do this batch handling in Bash and
starting up a new R session for each batch it takes ~ 7 sec per batch, so
it is nothing with the individual batches. The garbage collector do not
seem to handle this (potential) fragmentation.

Can the reason of the poor performance after a couple of iterations be that
I'm fragmenting the memory? If so, is there a solution that can used to
handle this within R, such as defragmentation or restarting R from within R?

With kind regards
M?ns Magnusson

PhD Student, Statistics, Link?ping University.

	[[alternative HTML version deleted]]


From J.Gorecki at wit.edu.pl  Mon Sep  5 01:52:18 2016
From: J.Gorecki at wit.edu.pl (=?UTF-8?B?SmFuIEfDs3JlY2tp?=)
Date: Mon, 5 Sep 2016 00:52:18 +0100
Subject: [Rd] new function to tools/utils package: dependencies based on
 DESCRIPTION file
In-Reply-To: <CAOQ5NyccwgSaL9VD0aWDZovm7gq0FT2OCoQh_3RfeBBUm2kqGA@mail.gmail.com>
References: <CABE2sp6e_kyX4vhR-uLAqpwNZxDyG-FD+AMBHE0uXp=c+UPQhw@mail.gmail.com>
	<CAJuCY5zfVudo7gyCi3NY1X012WgLyL7Tbkt7QeHy6zf0ZrQ2CA@mail.gmail.com>
	<CABE2sp7SrJtPDg-yZwdBDuNmSDoXv936i2biZzxUUaVEeQLa+g@mail.gmail.com>
	<CAO1zAVY9uAi3qEFUSBVDGdn50ijuhBDmJGHRL7cRNjaU01s6kA@mail.gmail.com>
	<CAO1zAVa69_0kh1ioXuqwq=R2t=5SeOHEZn0h7UJqjcKup45=xA@mail.gmail.com>
	<CAO1zAVZ9Crr370SsRM9y17JJPsLBcV_GT7P4LT7Cv5Zh2xy48Q@mail.gmail.com>
	<CABE2sp4XfD8oSOdMdKEmxT=BFh05ZEPtKif7h+KdACLEWYuLDw@mail.gmail.com>
	<CAOQ5NyccwgSaL9VD0aWDZovm7gq0FT2OCoQh_3RfeBBUm2kqGA@mail.gmail.com>
Message-ID: <CABE2sp6UNMT7jzMUmFqvGm39A+gnwxMU5PfK7_jnf0PwcVerAA@mail.gmail.com>

Is there any better mailing list for utils related discussion?
Jan

On 16 June 2016 at 14:00, Michael Lawrence <lawrence.michael at gene.com>
wrote:

> I agree that the utils package needs some improvements related to
> this, and hope to make them eventually. This type of feedback is very
> helpful.
>
> Thanks,
> Michael
>
>
>
> On Thu, Jun 16, 2016 at 1:42 AM, Jan G?recki <J.Gorecki at wit.edu.pl> wrote:
> > Dear Joris,
> >
> > So it does looks like the proposed function makes a lot sense then,
> isn't it?
> >
> > Cheers,
> > Jan
> >
> > On 16 June 2016 at 08:37, Joris Meys <jorismeys at gmail.com> wrote:
> >> Dear Jan,
> >>
> >> It is unavoidable to have OS and R dependencies for devtools. The
> building
> >> process for packages is both OS and R dependent, so devtools has to be
> too
> >> according to my understanding.
> >>
> >> Cheers
> >> Joris
> >>
> >> On 14 Jun 2016 18:56, "Jan G?recki" <J.Gorecki at wit.edu.pl> wrote:
> >>
> >> Hi Thierry,
> >>
> >> I'm perfectly aware of it. Any idea when devtools would be shipped as
> >> a base R package, or at least recommended package? To actually answer
> >> the problem described in my email.
> >> I have range of useful functions available tools/utils packages which
> >> are shipped together with R. They doesn't require any OS dependencies
> >> or R dependencies, unlike devtools which requires both. Installing
> >> unnecessary OS dependencies and R dependencies just for such a simple
> >> wrapper doesn't seem to be an elegant way to address it, therefore my
> >> proposal to include that simple function in tools, or utils package.
> >>
> >> Regards,
> >> Jan Gorecki
> >>
> >> On 14 June 2016 at 16:17, Thierry Onkelinx <thierry.onkelinx at inbo.be>
> wrote:
> >>> Dear Jan,
> >>>
> >>> Similar functionality is available in devtools::dev_package_deps()
> >>>
> >>> Best regards,
> >>>
> >>> ir. Thierry Onkelinx
> >>> Instituut voor natuur- en bosonderzoek / Research Institute for Nature
> and
> >>> Forest
> >>> team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
> >>> Kliniekstraat 25
> >>> 1070 Anderlecht
> >>> Belgium
> >>>
> >>> To call in the statistician after the experiment is done may be no more
> >>> than
> >>> asking him to perform a post-mortem examination: he may be able to say
> >>> what
> >>> the experiment died of. ~ Sir Ronald Aylmer Fisher
> >>> The plural of anecdote is not data. ~ Roger Brinner
> >>> The combination of some data and an aching desire for an answer does
> not
> >>> ensure that a reasonable answer can be extracted from a given body of
> >>> data.
> >>> ~ John Tukey
> >>>
> >>> 2016-06-14 16:54 GMT+02:00 Jan G?recki <J.Gorecki at wit.edu.pl>:
> >>>>
> >>>> Hi all,
> >>>>
> >>>> Packages tools and utils have a lot of useful stuff for R developers.
> >>>> I find one task still not as straightforward as it could. Simply to
> >>>> extract dependencies of a package from DESCRIPTION file (before it is
> >>>> even installed to library). This would be valuable in automation of CI
> >>>> setup in a more meta-data driven way.
> >>>> The simple function below, I know it is short and simple, but having
> >>>> it to be defined in each CI workflow is a pain, it could be already
> >>>> available in tools or utils namespace.
> >>>>
> >>>> package.dependencies.dcf <- function(file = "DESCRIPTION", which =
> >>>> c("Depends","Imports","LinkingTo")) {
> >>>>     stopifnot(file.exists(file), is.character(which))
> >>>>     which_all <- c("Depends", "Imports", "LinkingTo", "Suggests",
> >>>> "Enhances")
> >>>>     if (identical(which, "all"))
> >>>>         which <- which_all
> >>>>     else if (identical(which, "most"))
> >>>>         which <- c("Depends", "Imports", "LinkingTo", "Suggests")
> >>>>     stopifnot(which %in% which_all)
> >>>>     dcf <- read.dcf(file, which)
> >>>>     # parse fields
> >>>>     raw.deps <- unlist(strsplit(dcf[!is.na(dcf)], ",", fixed = TRUE))
> >>>>     # strip stated dependency version
> >>>>     deps <- trimws(sapply(strsplit(trimws(raw.deps), "(", fixed =
> >>>> TRUE), `[[`, 1L))
> >>>>     # exclude base R pkgs
> >>>>     base.pkgs <- c("R", rownames(installed.packages(priority =
> "base")))
> >>>>     setdiff(deps, base.pkgs)
> >>>> }
> >>>>
> >>>> This allows to easily install all package dependencies just based on
> >>>> DESCRIPTION file, so simplify that in custom CI workflows to:
> >>>>
> >>>> if (length(pkgs<-package.dependencies.dcf(which="all")))
> >>>> install.packages(pkgs)
> >>>>
> >>>> And would not require to install custom packages or shell scripts.
> >>>>
> >>>> Regards,
> >>>> Jan Gorecki
> >>>>
> >>>> ______________________________________________
> >>>> R-devel at r-project.org mailing list
> >>>> https://stat.ethz.ch/mailman/listinfo/r-devel
> >>>
> >>>
> >>
> >> ______________________________________________
> >> R-devel at r-project.org mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-devel
> >
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From lgong at uw.edu  Mon Sep  5 06:40:04 2016
From: lgong at uw.edu (Lixin Gong)
Date: Sun, 4 Sep 2016 21:40:04 -0700
Subject: [Rd] How to print UTF-8 encoded strings from a C routine to R's
	output?
Message-ID: <CAM0+Z0x8mJGuZxjqW9hbmQ6-oVaHUDRKid=rkFjwegOiRhPYgg@mail.gmail.com>

Dear R experts,

It seems that Rprintf has to be used to print from a C routine to guarantee
to write to R?s output according to
https://cran.r-project.org/doc/manuals/r-release/R-exts.html#Printing.

However if a string is UTF-8 encoded, non-ASCII characters (e.g., the
infinity symbol http://www.fileformat.info/info/unicode/char/221e/index.htm)
are misprinted.
Is this an unsupported feature or is there a workaround for this limitation?

Thanks!

Michael

	[[alternative HTML version deleted]]


From murdoch.duncan at gmail.com  Mon Sep  5 12:31:51 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Mon, 5 Sep 2016 06:31:51 -0400
Subject: [Rd] How to print UTF-8 encoded strings from a C routine to R's
 output?
In-Reply-To: <CAM0+Z0x8mJGuZxjqW9hbmQ6-oVaHUDRKid=rkFjwegOiRhPYgg@mail.gmail.com>
References: <CAM0+Z0x8mJGuZxjqW9hbmQ6-oVaHUDRKid=rkFjwegOiRhPYgg@mail.gmail.com>
Message-ID: <9270bb05-6418-633a-03ef-1a1fe68cf75b@gmail.com>

On 05/09/2016 12:40 AM, Lixin Gong wrote:
> Dear R experts,
>
> It seems that Rprintf has to be used to print from a C routine to guarantee
> to write to R?s output according to
> https://cran.r-project.org/doc/manuals/r-release/R-exts.html#Printing.
>
> However if a string is UTF-8 encoded, non-ASCII characters (e.g., the
> infinity symbol http://www.fileformat.info/info/unicode/char/221e/index.htm)
> are misprinted.
> Is this an unsupported feature or is there a workaround for this limitation?

If you are working in a UTF-8 locale (as on most Unix-like systems), you 
should be fine.  If not (as is normal on Windows), you'll need to 
translate the string to the local encoding.  The Writing R Extensions 
manual section 6.11 tells you how to do the re-encoding.

Duncan Murdoch


From luke-tierney at uiowa.edu  Mon Sep  5 17:26:19 2016
From: luke-tierney at uiowa.edu (luke-tierney at uiowa.edu)
Date: Mon, 5 Sep 2016 10:26:19 -0500
Subject: [Rd] Defragmentation of memory
In-Reply-To: <CAFQDYB35Y45r+MRPQhT+PXwGXSagV19nJohFo5BGpSeyj3SkcQ@mail.gmail.com>
References: <CAFQDYB35Y45r+MRPQhT+PXwGXSagV19nJohFo5BGpSeyj3SkcQ@mail.gmail.com>
Message-ID: <alpine.DEB.2.20.1609051022220.4326@luke-Latitude>

On Mon, 5 Sep 2016, M?ns Magnusson wrote:

> Dear all developers,
>
> I'm working with a lot of textual data in R and need to handle this batch
> by batch. The problem is that I read in batches of 10 000 documents and do
> some calculations that results in objects that consume quite some memory
> (calculate unigrams, 2-grams and 3-grams). In every iteration a new objects
> (~ 500 mB) is created (and I can't control the size, so a new object needs
> to be created each iteration). The speed of this computations is decreasing
> every iteration (first iteration 7 sec, after 30 iterations 20-30 minutes
> per iteration).
>
> I (think) I localized the problem to R:s memory handling and that my
> approach is fragmenting the memory. If I do this batch handling in Bash and
> starting up a new R session for each batch it takes ~ 7 sec per batch, so
> it is nothing with the individual batches. The garbage collector do not
> seem to handle this (potential) fragmentation.
>
> Can the reason of the poor performance after a couple of iterations be that
> I'm fragmenting the memory? If so, is there a solution that can used to
> handle this within R, such as defragmentation or restarting R from within R?

Highly unlikely. Fragmentation is rarely an issue on a 64-bit OS and
the symptoms would be different.

To get help with what is actually happening please post a minimal
reproducible example, and please not in html.

Best,

luke

>
> With kind regards
> M?ns Magnusson
>
> PhD Student, Statistics, Link?ping University.
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
Luke Tierney
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu

From lgong at uw.edu  Tue Sep  6 03:05:49 2016
From: lgong at uw.edu (Lixin Gong)
Date: Mon, 5 Sep 2016 18:05:49 -0700
Subject: [Rd] How to print UTF-8 encoded strings from a C routine to R's
	output?
In-Reply-To: <9270bb05-6418-633a-03ef-1a1fe68cf75b@gmail.com>
References: <CAM0+Z0x8mJGuZxjqW9hbmQ6-oVaHUDRKid=rkFjwegOiRhPYgg@mail.gmail.com>
	<9270bb05-6418-633a-03ef-1a1fe68cf75b@gmail.com>
Message-ID: <CAM0+Z0wytEOoTj4ksO4Of+_ajL7p9X_dmrWmxnAgmrOMpFOm6Q@mail.gmail.com>

Hi Duncan,

Thanks a lot for your quick reply pointing out the Re-encoding section that
I missed!

Before trying out R's C-level interface to the iconv's encoding conversion
capabilities,
I did some quick tests with Encoding() and iconv() on Windows with Rgui and
Rterm.
After Encoding(), non-ASCII characters are fine with Rgui but still wrong
with Rterm.
After iconv(), non-ASCII characters are still misprinted no matter if it is
Rgui or Rterm.

Here is the code that I used:

(neg_inf_utf8_hex <- as.raw(c(0x2d, 0xe2, 0x88, 0x9e)))
(neg_inf_utf8 <- rawToChar(neg_inf_utf8_hex))
Encoding(neg_inf_utf8)

Encoding(neg_inf_utf8) <- "UTF-8"
Encoding(neg_inf_utf8)
neg_inf_utf8

charToRaw(neg_inf_utf8)
iconv(neg_inf_utf8, from = "UTF-8", to = "", toRaw = FALSE)
iconv(neg_inf_utf8, from = "UTF-8", to = "", toRaw = TRUE)

Here is what I got with Rgui:

> (neg_inf_utf8_hex <- as.raw(c(0x2d, 0xe2, 0x88, 0x9e)))
[1] 2d e2 88 9e
> (neg_inf_utf8 <- rawToChar(neg_inf_utf8_hex))
[1] "-???"
> Encoding(neg_inf_utf8)
[1] "unknown"
>
> Encoding(neg_inf_utf8) <- "UTF-8"
> Encoding(neg_inf_utf8)
[1] "UTF-8"
> neg_inf_utf8
[1] "-?"
>
> charToRaw(neg_inf_utf8)
[1] 2d e2 88 9e
> iconv(neg_inf_utf8, from = "UTF-8", to = "", toRaw = FALSE)
[1] "-8"
> iconv(neg_inf_utf8, from = "UTF-8", to = "", toRaw = TRUE)
[[1]]
[1] 2d 38
>

Here is what I got with Rterm:

> (neg_inf_utf8_hex <- as.raw(c(0x2d, 0xe2, 0x88, 0x9e)))
[1] 2d e2 88 9e
> (neg_inf_utf8 <- rawToChar(neg_inf_utf8_hex))
[1] "-?^z"
> Encoding(neg_inf_utf8)
[1] "unknown"
>
> Encoding(neg_inf_utf8) <- "UTF-8"
> Encoding(neg_inf_utf8)
[1] "UTF-8"
> neg_inf_utf8
[1] "-8"
>
> charToRaw(neg_inf_utf8)
[1] 2d e2 88 9e
> iconv(neg_inf_utf8, from = "UTF-8", to = "", toRaw = FALSE)
[1] "-8"
> iconv(neg_inf_utf8, from = "UTF-8", to = "", toRaw = TRUE)
[[1]]
[1] 2d 38
>

Here is the sessionInfo:

> sessionInfo()
R version 3.3.1 (2016-06-21)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 10 x64 (build 14393)
locale:
[1] LC_COLLATE=English_United States.1252
[2] LC_CTYPE=English_United States.1252
[3] LC_MONETARY=English_United States.1252
[4] LC_NUMERIC=C
[5] LC_TIME=English_United States.1252
attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base
>

Am I missing something obvious?  Thanks a lot for your help and your time!

Michael

On Mon, Sep 5, 2016 at 3:31 AM, Duncan Murdoch <murdoch.duncan at gmail.com>
wrote:

> On 05/09/2016 12:40 AM, Lixin Gong wrote:
>
>> Dear R experts,
>>
>> It seems that Rprintf has to be used to print from a C routine to
>> guarantee
>> to write to R?s output according to
>> https://cran.r-project.org/doc/manuals/r-release/R-exts.html#Printing.
>>
>> However if a string is UTF-8 encoded, non-ASCII characters (e.g., the
>> infinity symbol http://www.fileformat.info/inf
>> o/unicode/char/221e/index.htm)
>> are misprinted.
>> Is this an unsupported feature or is there a workaround for this
>> limitation?
>>
>
> If you are working in a UTF-8 locale (as on most Unix-like systems), you
> should be fine.  If not (as is normal on Windows), you'll need to translate
> the string to the local encoding.  The Writing R Extensions manual section
> 6.11 tells you how to do the re-encoding.
>
> Duncan Murdoch
>
>

	[[alternative HTML version deleted]]


From alexandre.courtiol at gmail.com  Tue Sep  6 01:48:34 2016
From: alexandre.courtiol at gmail.com (Alexandre Courtiol)
Date: Tue, 6 Sep 2016 01:48:34 +0200
Subject: [Rd] mget call can trigger C stack usage error
Message-ID: <CAERMt4cE8maMs7q9=SB2pDHgV56oikBKphxowjGd8L+gX_V22w@mail.gmail.com>

Hi all, not sure if you will call this a bug or something else but the
following silly call trigger a low level error:

foo <- list(x=1)
class(foo) <- "new"
print.new <- function(x, ...) print(mget(names(formals())))
foo

> Error: C stack usage  7969412 is too close to the limit



-- 
Alexandre Courtiol

http://sites.google.com/site/alexandrecourtiol/home

*"Science is the belief in the ignorance of experts"*, R. Feynman

	[[alternative HTML version deleted]]


From maechler at stat.math.ethz.ch  Tue Sep  6 08:49:06 2016
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 6 Sep 2016 08:49:06 +0200
Subject: [Rd] A bug in the R Mersenne Twister (RNG) code?
In-Reply-To: <CADwqtCOib2VoeC=ufPt+jFk68GvzuSceJ7kJ+rrXGae0emzKXw@mail.gmail.com>
References: <61f698e6-c432-0dfe-630b-d6d45a78dc05@gmail.com>
	<1955d948-06dd-1c0c-6de1-32b0805152e6@gmail.com>
	<5c849045-478c-5fa6-b436-8e707a16f98a@gmail.com>
	<CADwqtCOib2VoeC=ufPt+jFk68GvzuSceJ7kJ+rrXGae0emzKXw@mail.gmail.com>
Message-ID: <22478.26338.215630.824699@stat.math.ethz.ch>

>>>>> Gabriel Becker <gmbecker at ucdavis.edu>
>>>>>     on Thu, 1 Sep 2016 08:34:31 -0700 writes:

    > I wonder how useful a (set of?) "time machine" functions
    > which look up /infer things like this based on a date
    > would be. Could ease the pain of changes generally, though
    > not remove it completely.

Such a set (possibly of size one) may be quite useful, notably
if it got an intuitive interface.
I'd recommend to partly follow options() here, i.e., the
  oc <- compatibilityR("2000-02-29")

would set random number generators (and other changeable
defaults) to those that were in effect when R 1.0.0 was released,
*and* a later call

  compatibilityR (oc)  # reset to previous state

would do what the comment says.


    > On Wed, Aug 31, 2016 at 5:45 PM, Paul Gilbert
    > <pgilbert902 at gmail.com> wrote:

    >> 
    >> 
    >> On 08/30/2016 06:29 PM, Duncan Murdoch wrote:
    >> 
    >>> I don't see evidence of a bug.  There have been several
    >>> versions of the MT; we may be using a different version
    >>> than you are.  Ours is the 1999/10/28 version; the web
    >>> page you cite uses one from 2002.
    >>> 
    >>> Perhaps the newer version fixes some problems, and then
    >>> it would be worth considering a change.  But changing
    >>> the default RNG definitely introduces problems in
    >>> reproducibility,
    >>> 
    >> 
    >> Well "problems in reproducibility" is a bit
    >> vague. Results would always be reproducible by specifying
    >> kind="Mersenne-Twister" or kind="Buggy Kinderman-Ramage"
    >> for older results, so there is no problem reproducing
    >> results. The only problem is that users expecting to
    >> reproduce results twenty years later will need to know
    >> what random generator they used. (BTW, they may also need
    >> to record information about the normal or other
    >> generator, as well as the seed.) Of course, these changes
    >> are recorded pretty well for R, so the history of
    >> "default" can always be found.
    >> 
    >> I think it is a mistake to encourage users into thinking
    >> they do not need to keep track of some information if
    >> they want reproducibility. Perhaps the default should be
    >> changed more often in order to encourage better user
    >> habits.
    >> 
    >> More seriously, I think "default" should continue to be
    >> something that is currently considered to be good. So, if
    >> there really is a known problem, then I think "default"
    >> should be changed.
    >> 
    >> (And, no I did not get burned by the R 1.7.0 change in
    >> the default generator. I got burned by a much earlier,
    >> unadvertised, and more subtle change in the Splus
    >> generator.)
    >> 
    >> Paul Gilbert
    >> 
    >> 
    >> so it's not obvious that we
    >> 
    >>> would do it.
    >>> 
    >>> Duncan Murdoch
    >>> 
    >>> 
    >>> On 30/08/2016 5:45 PM, Mark Roberts wrote:
    >>> 
    >>>> Whomever,
    >>>> 
    >>>> I recently sent the "bug report" below
    >>>> toR-core at r-project.org and have just been asked to
    >>>> instead submit it to you.
    >>>> 
    >>>> Although I am basically not an R user, I have installed
    >>>> version 3.3.1 and am also the author of a statistics
    >>>> program written in Visual Basic that contains a
    >>>> component which correctly implements the Mersenne
    >>>> Twister (MT) algorithm.  I believe that it is not
    >>>> possible to generate the correct stream of pseudorandom
    >>>> numbers using the MT default random number generator in
    >>>> R, and am not the first person to notice this.  Here is
    >>>> a posted 2013 entry
    >>>> (www.r-bloggers.com/reproducibility-and-randomness/) on
    >>>> an R website that asserts that the SAS computer program
    >>>> implementation of the MT algorithm produces different
    >>>> numbers than R does when using the same starting seed
    >>>> number.  The author of this post didn?t get anyone to
    >>>> respond to his query about the reason for this SAS
    >>>> vs. R discrepancy.
    >>>> 
    >>>> There are two ways of initializing the original MT
    >>>> computer program (written in C) so that an identical
    >>>> stream of numbers can be repeatedly generated: 1) with
    >>>> a particular integer seed number, and 2) with a
    >>>> particular array of integers.  In the 'compilation and
    >>>> usage' section of this webpage
    >>>> (https://github.com/cslarsen/mersenne-twister) there is
    >>>> a listing of the first 200 random numbers the MT
    >>>> algorithm should produce for seed number = 1.  The
    >>>> inventors of the Mersenne Twister random number
    >>>> generator provided two different sets of the first 1000
    >>>> numbers produced by a correctly coded 32-bit
    >>>> implementation of the MT algorithm when initializing it
    >>>> with a particular array of integers at:
    >>>> www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/MT2002/CODES/mt19937ar.out.
    >>>> [There is a link to this output at:
    >>>> www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/MT2002/emt19937ar.html.]
    >>>> 
    >>>> My statistics program obtains exactly those 200 numbers
    >>>> from the first site mentioned in the previous paragraph
    >>>> and also obtains those same numbers from the second
    >>>> website (though I didn't check all 2000 values).
    >>>> Assuming that the MT code within R uses the 32-bit MT
    >>>> algorithm, I suspect that the current version of R
    >>>> can't do that.  If you (i.e., anyone who might
    >>>> knowledgeably respond to this report) is able to
    >>>> duplicate those reference test-values, then please send
    >>>> me the R code to initialize the MT code within R to
    >>>> successfully do that, and I apologize for having wasted
    >>>> your time. If you (collectively) can't do that, then R
    >>>> is very likely using incorrectly implemented MT code.
    >>>> And if this latter possibility is true, it seems to me
    >>>> that this is something that should be fixed.
    >>>> 
    >>>> Mark Roberts, Ph.D.
    >>>> 
    >>>> [[alternative HTML version deleted]]
    >>>> 
    >>>> ______________________________________________
    >>>> R-devel at r-project.org mailing list
    >>>> https://stat.ethz.ch/mailman/listinfo/r-devel
    >>>> 
    >>>> 
    >>> ______________________________________________
    >>> R-devel at r-project.org mailing list
    >>> https://stat.ethz.ch/mailman/listinfo/r-devel
    >>> 
    >> 
    >> ______________________________________________
    >> R-devel at r-project.org mailing list
    >> https://stat.ethz.ch/mailman/listinfo/r-devel
    >> 



    > -- 
    > Gabriel Becker, PhD Associate Scientist (Bioinformatics)
    > Genentech Research

    > 	[[alternative HTML version deleted]]

    > ______________________________________________
    > R-devel at r-project.org mailing list
    > https://stat.ethz.ch/mailman/listinfo/r-devel


From maechler at stat.math.ethz.ch  Tue Sep  6 09:21:08 2016
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 6 Sep 2016 09:21:08 +0200
Subject: [Rd] A bug in the R Mersenne Twister (RNG) code?
In-Reply-To: <22470.63487.360801.506998@max.nulle.part>
References: <61f698e6-c432-0dfe-630b-d6d45a78dc05@gmail.com>
	<1955d948-06dd-1c0c-6de1-32b0805152e6@gmail.com>
	<22470.63487.360801.506998@max.nulle.part>
Message-ID: <22478.28260.448589.442458@stat.math.ethz.ch>

>>>>> Dirk Eddelbuettel <edd at debian.org>
>>>>>     on Wed, 31 Aug 2016 10:30:07 -0500 writes:

    > On 30 August 2016 at 18:29, Duncan Murdoch wrote: 
    > I don't see evidence of a bug.  There have been several
    > versions of the  MT; we may be using a different version
    > than you are.  Ours is the  1999/10/28 version; the web
    > page you cite uses one from 2002.
    >  
    >  Perhaps the newer version fixes some problems, and then
    > it would be  worth considering a change.  But changing
    > the default RNG definitely  introduces problems in
    > reproducibility, so it's not obvious that we would do it.

    > Yep. FWIW the GNU GSL adopted the 2002 version a while ago
    > too. Quoting from
    > https://www.gnu.org/software/gsl/manual/html_node/Random-number-generator-algorithms.html

    > Generator: gsl_rng_mt19937

    >    The MT19937 generator of Makoto Matsumoto and Takuji
    > Nishimura is a variant of the twisted generalized feedback
    > shift-register algorithm, and is known as the ?Mersenne
    > Twister? generator. It has a Mersenne prime period of
    > 2^19937 - 1 (about 10^6000) and is equi-distributed in 623
    > dimensions. It has passed the DIEHARD statistical
    > tests. It uses 624 words of state per generator and is
    > comparable in speed to the other generators. The original
    > generator used a default seed of 4357 and choosing s equal
    > to zero in gsl_rng_set reproduces this. Later versions
    > switched to 5489 as the default seed, you can choose this
    > explicitly via gsl_rng_set instead if you require it.

    >    For more information see,

    >       Makoto Matsumoto and Takuji Nishimura, ?Mersenne
    > Twister: A 623-dimensionally equidistributed uniform
    > pseudorandom number generator?. ACM Transactions on
    > Modeling and Computer Simulation, Vol. 8, No. 1
    > (Jan. 1998), Pages 3?30 The generator gsl_rng_mt19937 uses
    > the second revision of the seeding procedure published by
    > the two authors above in 2002. The original seeding
    > procedures could cause spurious artifacts for some seed
    > values. They are still available through the alternative
    > generators gsl_rng_mt19937_1999 and gsl_rng_mt19937_1998.

    > Note the last sentence here.

    > This is all somewhat technical code, so when I noticed the
    > above I could never figure what exactly R was doing in its
    > implementation.  But "innocent until proven guilty" -- a
    > sufficient number of people ought to have looked at this
    > -- so I saw no need to pursue this further.

    > Dirk

This interesting thread went to sleep a bit early.

Let me summarize my view (another R-core after Duncan's) on this:

a) R's reference documentation, easily accessed with one of
     ?Random, ?RNG, ?.Random.seed  (and more)
  clearly indicates the reference for our  "Mersenne-Twister" (MT)
  as the  ' Matsumoto, M. and Nishimura, T. (1998) ... ' publication.
  and we are providing that.
  As Duncan said, there is no bug.

b) The extra information by Mark and notably Dirk indicates that
   "the litterature" showed that the 1998 version of MT has rare
   problems and the GSL, notably uses the 2002 version of MT.

c) I think it would be nice if we could provide that as well as
   another RNGkind().  I for one would be willing to integrate a
   well written patch proposal (what others call PR for "pull
   request", and I'd also call "code donation") into the R sources.

   Well-written for me would include to re-use / share code with
   the 1998 version as much as possible.

d) If we had both kinds, say "Mersenne-Twister" and "Mersenne-Twister_2002",
   we (the R community, not R core necessarily) could conduct
   experiments about the differences... and can contempltate the
   pros and cons of a potential switch of default.

e) A bit tangential to this:  Nowadays, there also exist faster
   gamma variate RNGs .. giving different random values - for
   rgamma() but also several other derived RNGs, notably
   rchisq(), and even more in other CRAN packages.

   With code donation there, we could introduce a new argument
   	     'gamma.kind'
   to the
		set.seed(see, kind = NULL, normal.kind = NULL)
		RNGkind (     kind = NULL, normal.kind = NULL)
   functions.
   (There, currently I'd guess we would not change the default).

---

Note that the above includes my guess that  R-core would not take
action unless we get patch proposals.
(But then, I'm happy if my guess is wrong here ...)

Martin Maechler,
ETH Zurich


From jorismeys at gmail.com  Tue Sep  6 15:25:28 2016
From: jorismeys at gmail.com (Joris Meys)
Date: Tue, 6 Sep 2016 15:25:28 +0200
Subject: [Rd] The use of match.fun
Message-ID: <CAO1zAVYQg+Pa2uAF4OVLGSUk7EMFTMbjmpTpmzEnt8X8mHV1Qw@mail.gmail.com>

Dear gurus,

I was utterly surprised to learn that one of my examples illustrating the
need of match.fun() doesn't give me the expected result.

center <- function(x,FUN) FUN(x)
center(1:10, mean)
mean <- 4
center(1:10, mean)

Used to give me the error message "could not find function FUN". Now it
just works, even though I didn't expect it to. I believe this is at least
partially linked to a change in how R finds functions.

Now I'm not sure any more whether match.fun() actually has any use any
longer, and if so, in which cases it prevents things going wrong.

I've tried to find an example where this went wrong, but couldn't find one.
Any pointer to what happened here is greatly appreciated. I've checked the
NEWS, but I'm not smart enough to find the relevant bits and piece it
together.

Thank you in advance
Cheers
Joris

-- 
Joris Meys
Statistical consultant

Ghent University
Faculty of Bioscience Engineering
Department of Mathematical Modelling, Statistics and Bio-Informatics

tel :  +32 (0)9 264 61 79
Joris.Meys at Ugent.be
-------------------------------
Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php

	[[alternative HTML version deleted]]


From dusa.adrian at unibuc.ro  Tue Sep  6 15:35:10 2016
From: dusa.adrian at unibuc.ro (=?UTF-8?B?QWRyaWFuIER1yJlh?=)
Date: Tue, 6 Sep 2016 16:35:10 +0300
Subject: [Rd] The use of match.fun
In-Reply-To: <CAO1zAVYQg+Pa2uAF4OVLGSUk7EMFTMbjmpTpmzEnt8X8mHV1Qw@mail.gmail.com>
References: <CAO1zAVYQg+Pa2uAF4OVLGSUk7EMFTMbjmpTpmzEnt8X8mHV1Qw@mail.gmail.com>
Message-ID: <CAJ=0CtD=J+O5xoe+d5HzVJXb1p55Q2+kOYYwsUMsBXiGSOwS4g@mail.gmail.com>

I am not able to replicate this:

> center <- function(x,FUN) FUN(x)
> center(1:10, mean)
[1] 5.5
> mean <- 4
> center(1:10, mean)
Error in center(1:10, mean) : could not find function "FUN"

Using a fresh install of version 3.3.1 under MacOS, and tested before with
3.3.0 with the same result.


On Tue, Sep 6, 2016 at 4:25 PM, Joris Meys <jorismeys at gmail.com> wrote:

> Dear gurus,
>
> I was utterly surprised to learn that one of my examples illustrating the
> need of match.fun() doesn't give me the expected result.
>
> center <- function(x,FUN) FUN(x)
> center(1:10, mean)
> mean <- 4
> center(1:10, mean)
>
> Used to give me the error message "could not find function FUN". Now it
> just works, even though I didn't expect it to. I believe this is at least
> partially linked to a change in how R finds functions.
>
> Now I'm not sure any more whether match.fun() actually has any use any
> longer, and if so, in which cases it prevents things going wrong.
>
> I've tried to find an example where this went wrong, but couldn't find one.
> Any pointer to what happened here is greatly appreciated. I've checked the
> NEWS, but I'm not smart enough to find the relevant bits and piece it
> together.
>
> Thank you in advance
> Cheers
> Joris
>
> --
> Joris Meys
> Statistical consultant
>
> Ghent University
> Faculty of Bioscience Engineering
> Department of Mathematical Modelling, Statistics and Bio-Informatics
>
> tel :  +32 (0)9 264 61 79
> Joris.Meys at Ugent.be
> -------------------------------
> Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>



-- 
Adrian Dusa
University of Bucharest
Romanian Social Data Archive
Soseaua Panduri nr.90
050663 Bucharest sector 5
Romania

	[[alternative HTML version deleted]]


From jorismeys at gmail.com  Tue Sep  6 15:42:45 2016
From: jorismeys at gmail.com (Joris Meys)
Date: Tue, 6 Sep 2016 15:42:45 +0200
Subject: [Rd] The use of match.fun
In-Reply-To: <CAJ=0CtD=J+O5xoe+d5HzVJXb1p55Q2+kOYYwsUMsBXiGSOwS4g@mail.gmail.com>
References: <CAO1zAVYQg+Pa2uAF4OVLGSUk7EMFTMbjmpTpmzEnt8X8mHV1Qw@mail.gmail.com>
	<CAJ=0CtD=J+O5xoe+d5HzVJXb1p55Q2+kOYYwsUMsBXiGSOwS4g@mail.gmail.com>
Message-ID: <CAO1zAVaCAg31nr_Dj0+itxs32U_BFGxOt1L12+D1ndVRmv_fvA@mail.gmail.com>

Sorry, I am being a daft idiot again.  Turns out that somehow I had an
object FUN in my global environment, which explains why I didn't get the
result I expected.

So please ignore my question and burn my mail on a stake.
Cheers
Joris

On Tue, Sep 6, 2016 at 3:35 PM, Adrian Du?a <dusa.adrian at unibuc.ro> wrote:

> I am not able to replicate this:
>
> > center <- function(x,FUN) FUN(x)
> > center(1:10, mean)
> [1] 5.5
> > mean <- 4
> > center(1:10, mean)
> Error in center(1:10, mean) : could not find function "FUN"
>
> Using a fresh install of version 3.3.1 under MacOS, and tested before with
> 3.3.0 with the same result.
>
>
> On Tue, Sep 6, 2016 at 4:25 PM, Joris Meys <jorismeys at gmail.com> wrote:
>
>> Dear gurus,
>>
>> I was utterly surprised to learn that one of my examples illustrating the
>> need of match.fun() doesn't give me the expected result.
>>
>> center <- function(x,FUN) FUN(x)
>> center(1:10, mean)
>> mean <- 4
>> center(1:10, mean)
>>
>> Used to give me the error message "could not find function FUN". Now it
>> just works, even though I didn't expect it to. I believe this is at least
>> partially linked to a change in how R finds functions.
>>
>> Now I'm not sure any more whether match.fun() actually has any use any
>> longer, and if so, in which cases it prevents things going wrong.
>>
>> I've tried to find an example where this went wrong, but couldn't find
>> one.
>> Any pointer to what happened here is greatly appreciated. I've checked the
>> NEWS, but I'm not smart enough to find the relevant bits and piece it
>> together.
>>
>> Thank you in advance
>> Cheers
>> Joris
>>
>> --
>> Joris Meys
>> Statistical consultant
>>
>> Ghent University
>> Faculty of Bioscience Engineering
>> Department of Mathematical Modelling, Statistics and Bio-Informatics
>>
>> tel :  +32 (0)9 264 61 79
>> Joris.Meys at Ugent.be
>> -------------------------------
>> Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>
>
>
> --
> Adrian Dusa
> University of Bucharest
> Romanian Social Data Archive
> Soseaua Panduri nr.90
> 050663 Bucharest sector 5
> Romania
>



-- 
Joris Meys
Statistical consultant

Ghent University
Faculty of Bioscience Engineering
Department of Mathematical Modelling, Statistics and Bio-Informatics

tel :  +32 (0)9 264 61 79
Joris.Meys at Ugent.be
-------------------------------
Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php

	[[alternative HTML version deleted]]


From maechler at stat.math.ethz.ch  Tue Sep  6 22:26:31 2016
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 6 Sep 2016 22:26:31 +0200
Subject: [Rd] R (development) changes in arith, logic,
	relop with (0-extent) arrays
In-Reply-To: <22477.32999.558832.601073@stat.math.ethz.ch>
References: <22469.52585.935914.336374@aragorn.wu.ac.at>
	<22470.37902.198447.925212@stat.math.ethz.ch>
	<22470.38100.993865.828495@aragorn.wu.ac.at>
	<22473.36759.791968.814130@stat.math.ethz.ch>
	<22473.38325.834026.658339@stat.math.ethz.ch>
	<22474.35836.489860.992765@aragorn.wu.ac.at>
	<CAPRP4-eKdb1AKGt73P-+khNXFAWm_4q2PZr69Q7BAjWcXKpSzA@mail.gmail.com>
	<CAPRP4-e4fL=j2xUVMjO-kdmeTfvF=+=9AKrU9+981Xvhwc2MPA@mail.gmail.com>
	<22477.32999.558832.601073@stat.math.ethz.ch>
Message-ID: <22479.9847.747220.924128@stat.math.ethz.ch>

Yesterday, changes to R's development version were committed, relating
to arithmetic, logic ('&' and '|') and
comparison/relational ('<', '==') binary operators
which in NEWS are described as

  SIGNIFICANT USER-VISIBLE CHANGES:

         [.............]

         ? Arithmetic, logic (?&?, ?|?) and comparison (aka
           ?relational?, e.g., ?<?, ?==?) operations with arrays now
           behave consistently, notably for arrays of length zero.

           Arithmetic between length-1 arrays and longer non-arrays had
           silently dropped the array attributes and recycled.  This
           now gives a warning and will signal an error in the future,
           as it has always for logic and comparison operations in
           these cases (e.g., compare ?matrix(1,1) + 2:3? and
           ?matrix(1,1) < 2:3?).

As the above "visually suggests" one could think of the changes
falling mainly two groups,
  1) <0-extent array>  (op)     <non-array>
  2) <1-extent array>  (arith)  <non-array of length != 1>

These changes are partly non-back compatible and may break
existing code.  We believe that the internal consistency gained
from the changes is worth the few places with problems.

We expect some package maintainers (10-20, or even more?) need
to adapt their code.

Case '2)' above mainly results in a new warning, e.g.,

   > matrix(1,1) + 1:2
   [1] 2 3
   Warning message:
   In matrix(1, 1) + 1:2 :
     dropping dim() of array of length one.  Will become ERROR
   > 

whereas '1)' gives errors in cases the result silently was a
vector of length zero, or also keeps array (dim & dimnames) in
cases these were silently dropped.

The following is a "heavily" commented  R script showing (all ?)
the important cases with changes :

----------------------------------------------------------------------------

(m <- cbind(a=1[0], b=2[0]))
Lm <- m; storage.mode(Lm) <- "logical"
Im <- m; storage.mode(Im) <- "integer"

## 1. -------------------------
try( m & NULL ) # in R <= 3.3.x :
## Error in m & NULL :
##  operations are possible only for numeric, logical or complex types
##
## gives 'Lm' in R >= 3.4.0

## 2. -------------------------
 m + 2:3 ## gave numeric(0), now remains matrix identical to  m
Im + 2:3 ## gave integer(0), now remains matrix identical to Im (integer)

 m > 1      ## gave logical(0), now remains matrix identical to Lm (logical)
 m > 0.1[0] ##  ditto
 m > NULL   ##  ditto

## 3. -------------------------
mm <- m[,c(1:2,2:1,2)]
try( m == mm ) ## now gives error   "non-conformable arrays",
## but gave logical(0) in R <= 3.3.x

## 4. -------------------------
str( Im + NULL)  ## gave "num", now gives "int"

## 5. -------------------------
## special case for arithmetic w/ length-1 array
(m1 <- matrix(1,1,1, dimnames=list("Ro","col")))
(m2 <- matrix(1,2,1, dimnames=list(c("A","B"),"col")))

m1 + 1:2  # ->  2:3  but now with warning to  "become ERROR"
tools::assertError(m1 & 1:2)# ERR: dims [product 1] do not match the length of object [2]
tools::assertError(m1 < 1:2)# ERR:                  (ditto)
##
## non-0-length arrays combined with {NULL or double() or ...} *fail*

### Length-1 arrays:  Arithmetic with |vectors| > 1  treated array as scalar
m1 + NULL # gave  numeric(0) in R <= 3.3.x --- still, *but* w/ warning to "be ERROR"
try(m1 > NULL)    # gave  logical(0) in R <= 3.3.x --- an *error* now in R >= 3.4.0
tools::assertError(m1 & NULL)    # gave and gives error
tools::assertError(m1 | double())# ditto
## m2 was slightly different:
tools::assertError(m2 + NULL)
tools::assertError(m2 & NULL)
try(m2 == NULL) ## was logical(0) in R <= 3.3.x; now error as above!

----------------------------------------------------------------------------


Note that in R's own  'nls'  sources, there was one case of
situation '2)' above, i.e. a  1x1-matrix was used as a "scalar".

In such cases, you should explicitly coerce it to a vector,
either ("self-explainingly") by  as.vector(.), or as I did in
the nls case  by  c(.) :  The latter is much less
self-explaining, but nicer to read in mathematical formulae, and
currently also more efficient because it is a .Primitive.

Please use R-devel with your code, and let us know if you see
effects that seem adverse.

In some case where R-devel now gives an error but did not
previously, we could contemplate giving another  "warning
.... 'to become ERROR'" if there was too much breakage,  though
I don't expect that.


For the R Core Team,

Martin Maechler,
ETH Zurich


From maechler at stat.math.ethz.ch  Wed Sep  7 11:49:11 2016
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 7 Sep 2016 11:49:11 +0200
Subject: [Rd] R (development) changes in arith, logic,
	relop with (0-extent) arrays
In-Reply-To: <22479.9847.747220.924128@stat.math.ethz.ch>
References: <22469.52585.935914.336374@aragorn.wu.ac.at>
	<22470.37902.198447.925212@stat.math.ethz.ch>
	<22470.38100.993865.828495@aragorn.wu.ac.at>
	<22473.36759.791968.814130@stat.math.ethz.ch>
	<22473.38325.834026.658339@stat.math.ethz.ch>
	<22474.35836.489860.992765@aragorn.wu.ac.at>
	<CAPRP4-eKdb1AKGt73P-+khNXFAWm_4q2PZr69Q7BAjWcXKpSzA@mail.gmail.com>
	<CAPRP4-e4fL=j2xUVMjO-kdmeTfvF=+=9AKrU9+981Xvhwc2MPA@mail.gmail.com>
	<22477.32999.558832.601073@stat.math.ethz.ch>
	<22479.9847.747220.924128@stat.math.ethz.ch>
Message-ID: <22479.58007.358121.626142@stat.math.ethz.ch>

>>>>> Martin Maechler <maechler at stat.math.ethz.ch>
>>>>>     on Tue, 6 Sep 2016 22:26:31 +0200 writes:

    > Yesterday, changes to R's development version were committed, relating
    > to arithmetic, logic ('&' and '|') and
    > comparison/relational ('<', '==') binary operators
    > which in NEWS are described as

    > SIGNIFICANT USER-VISIBLE CHANGES:

    > [.............]

    > ? Arithmetic, logic (?&?, ?|?) and comparison (aka
    > ?relational?, e.g., ?<?, ?==?) operations with arrays now
    > behave consistently, notably for arrays of length zero.

    > Arithmetic between length-1 arrays and longer non-arrays had
    > silently dropped the array attributes and recycled.  This
    > now gives a warning and will signal an error in the future,
    > as it has always for logic and comparison operations in
    > these cases (e.g., compare ?matrix(1,1) + 2:3? and
    > ?matrix(1,1) < 2:3?).

    > As the above "visually suggests" one could think of the changes
    > falling mainly two groups,
    > 1) <0-extent array>  (op)     <non-array>
    > 2) <1-extent array>  (arith)  <non-array of length != 1>

    > These changes are partly non-back compatible and may break
    > existing code.  We believe that the internal consistency gained
    > from the changes is worth the few places with problems.

    > We expect some package maintainers (10-20, or even more?) need
    > to adapt their code.

    > Case '2)' above mainly results in a new warning, e.g.,

    >> matrix(1,1) + 1:2
    > [1] 2 3
    > Warning message:
    > In matrix(1, 1) + 1:2 :
    > dropping dim() of array of length one.  Will become ERROR
    >> 

    > whereas '1)' gives errors in cases the result silently was a
    > vector of length zero, or also keeps array (dim & dimnames) in
    > cases these were silently dropped.

    > The following is a "heavily" commented  R script showing (all ?)
    > the important cases with changes :

    > ----------------------------------------------------------------------------

    > (m <- cbind(a=1[0], b=2[0]))
    > Lm <- m; storage.mode(Lm) <- "logical"
    > Im <- m; storage.mode(Im) <- "integer"

    > ## 1. -------------------------
    > try( m & NULL ) # in R <= 3.3.x :
    > ## Error in m & NULL :
    > ##  operations are possible only for numeric, logical or complex types
    > ##
    > ## gives 'Lm' in R >= 3.4.0

    > ## 2. -------------------------
    > m + 2:3 ## gave numeric(0), now remains matrix identical to  m
    > Im + 2:3 ## gave integer(0), now remains matrix identical to Im (integer)

    > m > 1      ## gave logical(0), now remains matrix identical to Lm (logical)
    > m > 0.1[0] ##  ditto
    > m > NULL   ##  ditto

    > ## 3. -------------------------
    > mm <- m[,c(1:2,2:1,2)]
    > try( m == mm ) ## now gives error   "non-conformable arrays",
    > ## but gave logical(0) in R <= 3.3.x

    > ## 4. -------------------------
    > str( Im + NULL)  ## gave "num", now gives "int"

    > ## 5. -------------------------
    > ## special case for arithmetic w/ length-1 array
    > (m1 <- matrix(1,1,1, dimnames=list("Ro","col")))
    > (m2 <- matrix(1,2,1, dimnames=list(c("A","B"),"col")))

    > m1 + 1:2  # ->  2:3  but now with warning to  "become ERROR"
    > tools::assertError(m1 & 1:2)# ERR: dims [product 1] do not match the length of object [2]
    > tools::assertError(m1 < 1:2)# ERR:                  (ditto)
    > ##
    > ## non-0-length arrays combined with {NULL or double() or ...} *fail*

    > ### Length-1 arrays:  Arithmetic with |vectors| > 1  treated array as scalar
    > m1 + NULL # gave  numeric(0) in R <= 3.3.x --- still, *but* w/ warning to "be ERROR"
    > try(m1 > NULL)    # gave  logical(0) in R <= 3.3.x --- an *error* now in R >= 3.4.0
    > tools::assertError(m1 & NULL)    # gave and gives error
    > tools::assertError(m1 | double())# ditto
    > ## m2 was slightly different:
    > tools::assertError(m2 + NULL)
    > tools::assertError(m2 & NULL)
    > try(m2 == NULL) ## was logical(0) in R <= 3.3.x; now error as above!

    > ----------------------------------------------------------------------------


    > Note that in R's own  'nls'  sources, there was one case of
    > situation '2)' above, i.e. a  1x1-matrix was used as a "scalar".

    > In such cases, you should explicitly coerce it to a vector,
    > either ("self-explainingly") by  as.vector(.), or as I did in
    > the nls case  by  c(.) :  The latter is much less
    > self-explaining, but nicer to read in mathematical formulae, and
    > currently also more efficient because it is a .Primitive.

    > Please use R-devel with your code, and let us know if you see
    > effects that seem adverse.

I've been slightly surprised (or even "frustrated") by the empty
reaction on our R-devel list to this post.

I would have expected some critique, may be even some praise,
... in any case some sign people are "thinking along" (as we say
in German).

In the mean time, I've actually thought along the one case which
is last above:  The <op>  (binary operation) between a
non-0-length array and a 0-length vector (and NULL which should
be treated like a 0-length vector):

R <= 3.3.1  *is* quite inconsistent with these:


and my proposal above (implemented in R-devel, since Sep.5) would give an
error for all these, but instead, R really could be more lenient here:
A 0-length result is ok, and it should *not* inherit the array
(dim, dimnames), since the array is not of length 0. So instead
of the above [for the very last part only!!], we would aim for
the following. These *all* give an error in current R-devel,
with the exception of 'm1 + NULL' which "only" gives a "bad
warning" :

------------------------

m1 <- matrix(1,1)
m2 <- matrix(1,2)

m1 + NULL #    numeric(0) in R <= 3.3.x ---> OK ?!
m1 > NULL #    logical(0) in R <= 3.3.x ---> OK ?!
try(m1 & NULL)    # ERROR in R <= 3.3.x ---> change to logical(0)  ?!
try(m1 | double())# ERROR in R <= 3.3.x ---> change to logical(0)  ?!
## m2 slightly different:
try(m2 + NULL)  # ERROR in R <= 3.3.x ---> change to double(0)  ?!
try(m2 & NULL)  # ERROR in R <= 3.3.x ---> change to logical(0)  ?!
m2 == NULL # logical(0) in R <= 3.3.x ---> OK ?!

------------------------

This would be slightly more back-compatible than the currently
implemented proposal. Everything else I said remains true, and
I'm pretty sure most changes needed in packages would remain to be done.

Opinions ?



    > In some case where R-devel now gives an error but did not
    > previously, we could contemplate giving another  "warning
    > .... 'to become ERROR'" if there was too much breakage,  though
    > I don't expect that.


    > For the R Core Team,

    > Martin Maechler,
    > ETH Zurich


From therneau at mayo.edu  Wed Sep  7 14:22:01 2016
From: therneau at mayo.edu (Therneau, Terry M., Ph.D.)
Date: Wed, 07 Sep 2016 07:22:01 -0500
Subject: [Rd] R (development) changes in arith,
	logic relop with 0-extent arrays
In-Reply-To: <mailman.23.1473242406.7424.r-devel@r-project.org>
References: <mailman.23.1473242406.7424.r-devel@r-project.org>
Message-ID: <083a37$4a03lo@ironport10.mayo.edu>



On 09/07/2016 05:00 AM, r-devel-request at r-project.org wrote:
> I've been slightly surprised (or even "frustrated") by the empty
> reaction on our R-devel list to this post.
>
> I would have expected some critique, may be even some praise,
> ... in any case some sign people are "thinking along" (as we say
> in German).

Have patience Martin.  I read the news in digest form once a day and just saw the 
announcement and this follow-up a few minutes ago.  I haven't had cause to download a new 
copy of R-devel in the last month so the new Sept 5 version hasn't impacted me.

The survival package had one line of code that will need modification, which isn't so bad. 
  It involves the scaled Shoenfeld residuals, for which the formula is r V + 1 beta'; r= 
matrix of residuals with nvar columns and #deaths rows, V=vcov(fit) = variance matrix, and 
then add beta[j] to each column j.  When the number of covariates is 1 the first term of 
this collapsed to a vector *  1x1 matrix, mostly because of how the code was written, 
which was itself the result of evolution over time.   Bottom line is that I don't find 
either the prior behavior or the new one problematic.

Terry T.


From cosi1 at tlen.pl  Wed Sep  7 17:27:41 2016
From: cosi1 at tlen.pl (=?UTF-8?Q?Pawe=C5=82_Pi=C4=85tkowski?=)
Date: Wed, 07 Sep 2016 17:27:41 +0200
Subject: [Rd] =?utf-8?q?Building_R_under_Linux_-_library_dependencies?=
Message-ID: <62ff6ef3.6e95b7a5.57d031ed.7e18d@tlen.pl>

Hello and apologies if this doesn't belong here.

I'm trying to build a "portable" version of R - "portable" means that it could be easily moved to another location or machine simply by copying it. However, I encountered a problem when running it elsewhere: it seems that versions of dynamic libraries used by R are fixed and set at the build time; when that instance of R is run on a system with a different version of certain library (e.g. libicuuc.so.52 instead of libicuuc.so.48), it can't find it and quits.

> bin/exec/R: error while loading shared libraries: libicuuc.so.48: cannot open shared object file: No such file or directory

Is there a way to overcome this problem? Precompiled versions of R can be installed on various system configurations, so I guess that there should be a way to compile it in a version-agnostic manner.

Best regards,
Pawel


From edd at debian.org  Wed Sep  7 19:30:11 2016
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 7 Sep 2016 12:30:11 -0500
Subject: [Rd] Building R under Linux - library dependencies
In-Reply-To: <62ff6ef3.6e95b7a5.57d031ed.7e18d@tlen.pl>
References: <62ff6ef3.6e95b7a5.57d031ed.7e18d@tlen.pl>
Message-ID: <22480.20131.781988.142009@max.nulle.part>


On 7 September 2016 at 17:27, Pawe? Pi?tkowski wrote:
| Hello and apologies if this doesn't belong here.
| 
| I'm trying to build a "portable" version of R - "portable" means that it could be easily moved to another location or machine simply by copying it. However, I encountered a problem when running it elsewhere: it seems that versions of dynamic libraries used by R are fixed and set at the build time; when that instance of R is run on a system with a different version of certain library (e.g. libicuuc.so.52 instead of libicuuc.so.48), it can't find it and quits.
| 
| > bin/exec/R: error while loading shared libraries: libicuuc.so.48: cannot open shared object file: No such file or directory
| 
| Is there a way to overcome this problem? Precompiled versions of R can be installed on various system configurations, so I guess that there should be a way to compile it in a version-agnostic manner.

Yes, for example by

  -- using a Docker container which is portable across OSs (!!) and versions

  -- relying on package management which is what every Linux distro does

Otherwise you are trying to reinvent a systems-level wheel in application
space. I suspect that won't end well.

Dirk


PS For the latter point, our .deb based R package currently shows this:

Package: r-base-core
Source: r-base
Priority: optional
Section: gnu-r
Installed-Size: 33845
Maintainer: Dirk Eddelbuettel <edd at debian.org>
Architecture: amd64
Version: 3.3.1-1.xenial.0
Recommends: r-recommended, r-base-dev, r-doc-html
Replaces: r-base (<= 1.4.1-1), r-base-latex (<= 2.9.2-4), r-cran-rcompgen (<= 0.1-17-1), r-gnome (<= 2.3.1), r-recommended (<< 1.9.0)
Suggests: ess, r-doc-info | r-doc-pdf, r-mathlib, r-base-html
Provides: r-api-3, r-base-latex, r-cran-rcompgen, r-gnome
Depends: zip, unzip, libpaper-utils, xdg-utils, libblas3 | libblas.so.3, libbz2-1.0, libc6 (>= 2.23), libcairo2 (>= 1.6.0), libcurl3 (>= 7.28.0), libglib2.0-0 (>= 2.12.0), libgomp1 (>= 4.9), libjpeg8 (>= 8c), liblapack3 | liblapack.so.3, liblzma5 (>= 5.1.1alpha+20120614), libpango-1.0-0 (>= 1.14.0), libpangocairo-1.0-0 (>= 1.14.0), libpcre3, libpng12-0 (>= 1.2.13-4), libreadline6 (>= 6.0), libtcl8.6 (>= 8.6.0), libtiff5 (>= 4.0.3), libtk8.6 (>= 8.6.0), libx11-6, libxt6, zlib1g (>= 1:1.1.4), ucf (>= 3.0), ca-certificates
Conflicts: r-base-latex, r-cran-rcompgen, r-gnome
Filename: pool/main/r/r-base/r-base-core_3.3.1-1.xenial.0_amd64.deb
Size: 20939808
MD5sum: a983ccafe969cc4d8a631036478ac1c2
SHA1: 1fd9991b2577bf18074cd0a7e8017a98d4efef13
SHA256: 9a0cc3d5edf6b628d854a075731f30b460ea9ff465327693eb2d92b59ac01901
Description-en: GNU R core of statistical computation and graphics system
[...]

Note the detailed and fine-grained breakdown of library dependencies.


-- 
http://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From cosi1 at tlen.pl  Wed Sep  7 20:50:25 2016
From: cosi1 at tlen.pl (=?UTF-8?Q?Pawe=C5=82_Pi=C4=85tkowski?=)
Date: Wed, 07 Sep 2016 20:50:25 +0200
Subject: [Rd] =?utf-8?q?Building_R_under_Linux_-_library_dependencies?=
In-Reply-To: <22480.20131.781988.142009@max.nulle.part>
References: <62ff6ef3.6e95b7a5.57d031ed.7e18d@tlen.pl>
	<22480.20131.781988.142009@max.nulle.part>
Message-ID: <3092aed4.f96460.57d06171.56abc@tlen.pl>

> | Is there a way to overcome this problem? Precompiled versions of R can be installed on various system configurations, so I guess that there should be a way to compile it in a version-agnostic manner.
> 
> Yes, for example by
> 
>   -- using a Docker container which is portable across OSs (!!) and versions

Docker R containers are north of 250 MB. I have checked experimentally that you can trim R down to 16 MB (!) and you'll still be able to execute it (though with warnings). That *is* quite a difference, especially when deploying small applications.

>   -- relying on package management which is what every Linux distro does
> 
> (...)
> 
> PS For the latter point, our .deb based R package currently shows this:
> 
> (...)
> 
> Depends: zip, unzip, libpaper-utils, xdg-utils, libblas3 | libblas.so.3, libbz2-1.0, libc6 (>= 2.23), libcairo2 (>= 1.6.0), libcurl3 (>= 7.28.0), libglib2.0-0 (>= 2.12.0), libgomp1 (>= 4.9), libjpeg8 (>= 8c), liblapack3 | liblapack.so.3, liblzma5 (>= 5.1.1alpha+20120614), libpango-1.0-0 (>= 1.14.0), libpangocairo-1.0-0 (>= 1.14.0), libpcre3, libpng12-0 (>= 1.2.13-4), libreadline6 (>= 6.0), libtcl8.6 (>= 8.6.0), libtiff5 (>= 4.0.3), libtk8.6 (>= 8.6.0), libx11-6, libxt6, zlib1g (>= 1:1.1.4), ucf (>= 3.0), ca-certificates

Sure, package dependencies would be great as well - at least you'd be sure that users of, say, Debian-based distros will be able to run this portable R, as long as they've installed the required libraries. But notice that in your example package versions equal *or greater* than listed are required - so if someone has upgraded their system, they still will be able to run that R. With a version built from source you need *exactly* the same version as on the machine where R was compiled. Hence my question: how come the precompiled distribution of R has "less strict" library requirements than manually compiled versions?

Best,
Pawel


From elw at stderr.org  Wed Sep  7 21:09:25 2016
From: elw at stderr.org (elijah wright)
Date: Wed, 7 Sep 2016 14:09:25 -0500
Subject: [Rd] Building R under Linux - library dependencies
In-Reply-To: <3092aed4.f96460.57d06171.56abc@tlen.pl>
References: <62ff6ef3.6e95b7a5.57d031ed.7e18d@tlen.pl>
	<22480.20131.781988.142009@max.nulle.part>
	<3092aed4.f96460.57d06171.56abc@tlen.pl>
Message-ID: <CANQ3A2PnKHqoueQDfHbmhOiXahFLOzZ=MShroNboYsFODpNb+Q@mail.gmail.com>

On Wed, Sep 7, 2016 at 1:50 PM, Pawe? Pi?tkowski <cosi1 at tlen.pl> wrote:

> > | Is there a way to overcome this problem? Precompiled versions of R can
> be installed on various system configurations, so I guess that there should
> be a way to compile it in a version-agnostic manner.
> >
> > Yes, for example by
> >
> >   -- using a Docker container which is portable across OSs (!!) and
> versions
>
> Docker R containers are north of 250 MB. I have checked experimentally
> that you can trim R down to 16 MB (!) and you'll still be able to execute
> it (though with warnings). That *is* quite a difference, especially when
> deploying small applications.



... I would guesstimate the libraries required to run R with any useful set
of libraries is quite a bit bigger than the cited 16M .......



> >   -- relying on package management which is what every Linux distro does
> >
> > (...)
> >
> > PS For the latter point, our .deb based R package currently shows this:
> >
> > (...)
> >
> > Depends: zip, unzip, libpaper-utils, xdg-utils, libblas3 | libblas.so.3,
> libbz2-1.0, libc6 (>= 2.23), libcairo2 (>= 1.6.0), libcurl3 (>= 7.28.0),
> libglib2.0-0 (>= 2.12.0), libgomp1 (>= 4.9), libjpeg8 (>= 8c), liblapack3 |
> liblapack.so.3, liblzma5 (>= 5.1.1alpha+20120614), libpango-1.0-0 (>=
> 1.14.0), libpangocairo-1.0-0 (>= 1.14.0), libpcre3, libpng12-0 (>=
> 1.2.13-4), libreadline6 (>= 6.0), libtcl8.6 (>= 8.6.0), libtiff5 (>=
> 4.0.3), libtk8.6 (>= 8.6.0), libx11-6, libxt6, zlib1g (>= 1:1.1.4), ucf (>=
> 3.0), ca-certificates
>
> Sure, package dependencies would be great as well - at least you'd be sure
> that users of, say, Debian-based distros will be able to run this portable
> R, as long as they've installed the required libraries. But notice that in
> your example package versions equal *or greater* than listed are required -
> so if someone has upgraded their system, they still will be able to run
> that R. With a version built from source you need *exactly* the same
> version as on the machine where R was compiled. Hence my question: how come
> the precompiled distribution of R has "less strict" library requirements
> than manually compiled versions?
>

Package managers don't usually cite 'less than' versions for packages -
because how do you assert a version that won't work when it hasn't been
released yet?

You could go on a tear and build statically linked versions of
R-with-everything-you-need, and maybe avoid the library madness... but this
is sort of a fool's errand and a huge consumer of time.  OS vendors and
compiler developers have stopped doing things that way for reasons.... it's
much simpler to reduce duplication and make everything work - while
allowing for patching out security issues - when you are *just slightly*
more flexible.

ABI compatibility and library versioning are, I think, fairly well
understood....

Doing this stuff with a container is very much the easiest route, if you
actually want it to be completely portable.  You're certainly welcome to
start with an Alpine Linux base and add R on top and then start paring...
but I start to not understand the point, somewhere in there....  it's a lot
of time spent on something that doesn't seem that beneficial when you've
got (even fairly reasonably modern) hardware that can deal with a tiny bit
of extra bloat.  SD cards and USB sticks are pretty cheap everywhere, now,
aren't they?

I could say, maybe, putting time into this as some kind of retrocomputing
project... but probably not otherwise.

best,

--e

	[[alternative HTML version deleted]]


From cosi1 at tlen.pl  Wed Sep  7 21:46:43 2016
From: cosi1 at tlen.pl (=?UTF-8?Q?Pawe=C5=82_Pi=C4=85tkowski?=)
Date: Wed, 07 Sep 2016 21:46:43 +0200
Subject: [Rd] =?utf-8?q?Building_R_under_Linux_-_library_dependencies?=
In-Reply-To: <CANQ3A2PnKHqoueQDfHbmhOiXahFLOzZ=MShroNboYsFODpNb+Q@mail.gmail.com>
References: <62ff6ef3.6e95b7a5.57d031ed.7e18d@tlen.pl>
	<22480.20131.781988.142009@max.nulle.part>
	<3092aed4.f96460.57d06171.56abc@tlen.pl>
	<CANQ3A2PnKHqoueQDfHbmhOiXahFLOzZ=MShroNboYsFODpNb+Q@mail.gmail.com>
Message-ID: <21e8cb63.199b12c3.57d06ea3.25baf@tlen.pl>

> > Docker R containers are north of 250 MB. I have checked experimentally that you can trim R down to 16 MB (!) and you'll still be able to execute it (though with warnings). That *is* quite a difference, especially when deploying small applications.
?
> ... I would guesstimate the libraries required to run R with any useful set of libraries is quite a bit bigger than the cited 16M .......?
?
Maybe. The minimal usable subset is about 37 MB, add a few custom libraries, code of your application etc... But it's *still* much less than 250 MB.
?
> > Sure, package dependencies would be great as well - at least you'd be sure that users of, say, Debian-based distros will be able to run this portable R, as long as they've installed the required libraries. But notice that in your example package versions equal *or greater* than listed are required - so if someone has upgraded their system, they still will be able to run that R. With a version built from source you need *exactly* the same version as on the machine where R was compiled. Hence my question: how come the precompiled distribution of R has "less strict" library requirements than manually compiled versions?
?
> Package managers don't usually cite 'less than' versions for packages - because how do you assert a version that won't work when it hasn't been released yet?

I meant that manually built versions of R (at least those compiled by me) are fixed at a certain version of dynamic libraries - the same as installed on the machine R was compiled on. You can't run this compiled R on an upgraded configuration.
?
> You could go on a tear and build statically linked versions of R-with-everything-you-need, and maybe avoid the library madness... but this is sort of a fool's errand and a huge consumer of time.? OS vendors and compiler developers have stopped doing things that way for reasons.... it's much simpler to reduce duplication and make everything work - while allowing for patching out security issues - when you are *just slightly* more flexible.

Why link the libraries statically? Most Linux distributions make symlinks to dynamically linked libraries - so you have for example libicuuc.so that links to libicuuc.so.XX (where XX is the version number). Why not rely on these generic names?
?
> Doing this stuff with a container is very much the easiest route, if you actually want it to be completely portable.? You're certainly welcome to start with an Alpine Linux base and add R on top and then start paring... but I start to not understand the point, somewhere in there.... ?it's a lot of time spent on something that doesn't seem that beneficial when you've got (even fairly reasonably modern) hardware that can deal with a tiny bit of extra bloat.? SD cards and USB sticks are pretty cheap everywhere, now, aren't they?
> 
> I could say, maybe, putting time into this as some kind of retrocomputing project... but probably not otherwise.

Potential users who would have to download 250 megabytes beg to differ ;-)

Best,
-p-


From edd at debian.org  Wed Sep  7 21:50:35 2016
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 7 Sep 2016 14:50:35 -0500
Subject: [Rd] Building R under Linux - library dependencies
In-Reply-To: <3092aed4.f96460.57d06171.56abc@tlen.pl>
References: <62ff6ef3.6e95b7a5.57d031ed.7e18d@tlen.pl>
	<22480.20131.781988.142009@max.nulle.part>
	<3092aed4.f96460.57d06171.56abc@tlen.pl>
Message-ID: <22480.28555.417649.426895@max.nulle.part>


On 7 September 2016 at 20:50, Pawe? Pi?tkowski wrote:
| > | Is there a way to overcome this problem? Precompiled versions of R can be installed on various system configurations, so I guess that there should be a way to compile it in a version-agnostic manner.
| > 
| > Yes, for example by
| > 
| >   -- using a Docker container which is portable across OSs (!!) and versions
| 
| Docker R containers are north of 250 MB. I have checked experimentally that you can trim R down to 16 MB (!) and you'll still be able to execute it (though with warnings). That *is* quite a difference, especially when deploying small applications.

You are not enumerating your trade-offs very well. There are natural
conflicts. What is you really want?

- Being able to pre-build and distribute?  We have done that since the last
5C1990s with .deb packages.

- Being able to install with minimal size?  Have you queried your users?  I
note that among the Docker containers for R (in the "Rocker" project Carl and
I run) the _larger_ ones containing RStudio plus optionally "lots from
hadley" plus optionally lots of rOpenSci tend to me _more_ popular (for ease
of installation of the aggregate).

And while share the overall sentiment a little bit, you have to realize that
it is 2016 with the corresponding bandwith and storage:

  edd at max:~$ du -csh /usr/local/lib/R/site-library/
  1.5G    /usr/local/lib/R/site-library/
  1.5G    total
  edd at max:~$

And that it _outside_ of R itself, or the (numerous) other shared libraries.

| >   -- relying on package management which is what every Linux distro does
| > 
| > (...)
| > 
| > PS For the latter point, our .deb based R package currently shows this:
| > 
| > (...)
| > 
| > Depends: zip, unzip, libpaper-utils, xdg-utils, libblas3 | libblas.so.3, libbz2-1.0, libc6 (>= 2.23), libcairo2 (>= 1.6.0), libcurl3 (>= 7.28.0), libglib2.0-0 (>= 2.12.0), libgomp1 (>= 4.9), libjpeg8 (>= 8c), liblapack3 | liblapack.so.3, liblzma5 (>= 5.1.1alpha+20120614), libpango-1.0-0 (>= 1.14.0), libpangocairo-1.0-0 (>= 1.14.0), libpcre3, libpng12-0 (>= 1.2.13-4), libreadline6 (>= 6.0), libtcl8.6 (>= 8.6.0), libtiff5 (>= 4.0.3), libtk8.6 (>= 8.6.0), libx11-6, libxt6, zlib1g (>= 1:1.1.4), ucf (>= 3.0), ca-certificates
| 
| Sure, package dependencies would be great as well - at least you'd be sure that users of, say, Debian-based distros will be able to run this portable R, as long as they've installed the required libraries. But notice that in your example package versions equal *or greater* than listed are required - so if someone has upgraded their system, they still will be able to run that R. With a version built from source you need *exactly* the same version as on the machine where R was compiled. Hence my question: how come the precompiled distribution of R has "less strict" library requirements than manually compiled versions?

This is not the list for internals of how Linux packaging works, but if you
took the question to debian-user or debian-devel you would like get a pretty
qualified answer.  That dependency resolution system has been refined for
well over 20 years, so don't expect one sentence answers.

Good luck,  Dirk

-- 
http://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From spencer.graves at prodsyse.com  Wed Sep  7 22:41:16 2016
From: spencer.graves at prodsyse.com (Spencer Graves)
Date: Wed, 7 Sep 2016 15:41:16 -0500
Subject: [Rd] Fwd: Re: RSiteSearch, sos, rdocumentation.org, ...?
In-Reply-To: <20160907201522.GA12554@psych.upenn.edu>
References: <20160907201522.GA12554@psych.upenn.edu>
Message-ID: <20518b79-b272-b848-a736-a9acf225626b@prodsyse.com>

Hello, All:


       Jonathan Baron is "giving up" maintaining the RSiteSearch database.


       This breaks three things:  (1) The R Site Search web service that 
Baron has maintained.  (2) The RSiteSearch function in the utils 
package.  (3) The sos package, for which I'm the maintainer and lead 
author.


       Might someone else be willing to take these over?


       For me, the "findFn" capability with "writeFindFn2xls" is the 
fastest literature search for anything statistical.  However, I don't 
have the resources to take over the management of Baron's R Site Search 
database.


       He's provided a great service for the R community for many 
years.  I hope we can find a way to keep the system maintained. Failing 
that, I could use help in adapting the sos package to another database.


       Thanks,
       Spencer Graves


-------- Forwarded Message --------
Subject: 	Re: RSiteSearch, sos, rdocumentation.org, ...?
Date: 	Wed, 7 Sep 2016 16:15:22 -0400
From: 	Jonathan Baron <baron at psych.upenn.edu>
To: 	Spencer Graves <spencer.graves at prodsyse.com>
CC: 	Jonathan Baron <baron at psych.upenn.edu>, chris.is.fun at gmail.com, 
info at datacamp.com <info at datacamp.com>, Sundar Dorai-Raj 
<sdorairaj at gmail.com>, webmaster at www.r-project-org



R site search has stopped working. The indexing scrip, mknmz, failed
to complete. It has been producing more and more errors and warnings,
since it has not been updated for 5 yeaers.

I am giving up on this site. I have too many other things to do aside
from find bugs in programs written in languages I don't know (Perl),
or set up an alternative search engine.

Please inform anyone else who needs to be informed.

I cannot find the email of the www.r-project.org webmaster, so I'm
taking a stab. There are several links to this site in those pages.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron
Editor: Judgment and Decision Making (http://journal.sjdm.org)


	[[alternative HTML version deleted]]


From maechler at stat.math.ethz.ch  Wed Sep  7 22:49:31 2016
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 7 Sep 2016 22:49:31 +0200
Subject: [Rd] R (development) changes in arith, logic,
	relop with (0-extent) arrays
In-Reply-To: <22479.58007.358121.626142@stat.math.ethz.ch>
References: <22469.52585.935914.336374@aragorn.wu.ac.at>
	<22470.37902.198447.925212@stat.math.ethz.ch>
	<22470.38100.993865.828495@aragorn.wu.ac.at>
	<22473.36759.791968.814130@stat.math.ethz.ch>
	<22473.38325.834026.658339@stat.math.ethz.ch>
	<22474.35836.489860.992765@aragorn.wu.ac.at>
	<CAPRP4-eKdb1AKGt73P-+khNXFAWm_4q2PZr69Q7BAjWcXKpSzA@mail.gmail.com>
	<CAPRP4-e4fL=j2xUVMjO-kdmeTfvF=+=9AKrU9+981Xvhwc2MPA@mail.gmail.com>
	<22477.32999.558832.601073@stat.math.ethz.ch>
	<22479.9847.747220.924128@stat.math.ethz.ch>
	<22479.58007.358121.626142@stat.math.ethz.ch>
Message-ID: <22480.32091.962256.884510@stat.math.ethz.ch>

>>>>> Martin Maechler <maechler at stat.math.ethz.ch>
>>>>>     on Wed, 7 Sep 2016 11:49:11 +0200 writes:

>>>>> Martin Maechler <maechler at stat.math.ethz.ch>
>>>>>     on Tue, 6 Sep 2016 22:26:31 +0200 writes:

    >> Yesterday, changes to R's development version were committed, relating
    >> to arithmetic, logic ('&' and '|') and
    >> comparison/relational ('<', '==') binary operators
    >> which in NEWS are described as

    >> SIGNIFICANT USER-VISIBLE CHANGES:

    >> [.............]

    >> ? Arithmetic, logic (?&?, ?|?) and comparison (aka
    >> ?relational?, e.g., ?<?, ?==?) operations with arrays now
    >> behave consistently, notably for arrays of length zero.

    >> Arithmetic between length-1 arrays and longer non-arrays had
    >> silently dropped the array attributes and recycled.  This
    >> now gives a warning and will signal an error in the future,
    >> as it has always for logic and comparison operations in
    >> these cases (e.g., compare ?matrix(1,1) + 2:3? and
    >> ?matrix(1,1) < 2:3?).

    >> As the above "visually suggests" one could think of the changes
    >> falling mainly two groups,
    >> 1) <0-extent array>  (op)     <non-array>
    >> 2) <1-extent array>  (arith)  <non-array of length != 1>

    >> These changes are partly non-back compatible and may break
    >> existing code.  We believe that the internal consistency gained
    >> from the changes is worth the few places with problems.

    >> We expect some package maintainers (10-20, or even more?) need
    >> to adapt their code.

    >> Case '2)' above mainly results in a new warning, e.g.,

    >>> matrix(1,1) + 1:2
    >> [1] 2 3
    >> Warning message:
    >> In matrix(1, 1) + 1:2 :
    >> dropping dim() of array of length one.  Will become ERROR
    >>> 

    >> whereas '1)' gives errors in cases the result silently was a
    >> vector of length zero, or also keeps array (dim & dimnames) in
    >> cases these were silently dropped.

    >> The following is a "heavily" commented  R script showing (all ?)
    >> the important cases with changes :

    >> ----------------------------------------------------------------------------

    >> (m <- cbind(a=1[0], b=2[0]))
    >> Lm <- m; storage.mode(Lm) <- "logical"
    >> Im <- m; storage.mode(Im) <- "integer"

    >> ## 1. -------------------------
    >> try( m & NULL ) # in R <= 3.3.x :
    >> ## Error in m & NULL :
    >> ##  operations are possible only for numeric, logical or complex types
    >> ##
    >> ## gives 'Lm' in R >= 3.4.0

    >> ## 2. -------------------------
    >> m + 2:3 ## gave numeric(0), now remains matrix identical to  m
    >> Im + 2:3 ## gave integer(0), now remains matrix identical to Im (integer)

    >> m > 1      ## gave logical(0), now remains matrix identical to Lm (logical)
    >> m > 0.1[0] ##  ditto
    >> m > NULL   ##  ditto

    >> ## 3. -------------------------
    >> mm <- m[,c(1:2,2:1,2)]
    >> try( m == mm ) ## now gives error   "non-conformable arrays",
    >> ## but gave logical(0) in R <= 3.3.x

    >> ## 4. -------------------------
    >> str( Im + NULL)  ## gave "num", now gives "int"

    >> ## 5. -------------------------
    >> ## special case for arithmetic w/ length-1 array
    >> (m1 <- matrix(1,1,1, dimnames=list("Ro","col")))
    >> (m2 <- matrix(1,2,1, dimnames=list(c("A","B"),"col")))

    >> m1 + 1:2  # ->  2:3  but now with warning to  "become ERROR"
    >> tools::assertError(m1 & 1:2)# ERR: dims [product 1] do not match the length of object [2]
    >> tools::assertError(m1 < 1:2)# ERR:                  (ditto)
    >> ##
    >> ## non-0-length arrays combined with {NULL or double() or ...} *fail*

    >> ### Length-1 arrays:  Arithmetic with |vectors| > 1  treated array as scalar
    >> m1 + NULL # gave  numeric(0) in R <= 3.3.x --- still, *but* w/ warning to "be ERROR"
    >> try(m1 > NULL)    # gave  logical(0) in R <= 3.3.x --- an *error* now in R >= 3.4.0
    >> tools::assertError(m1 & NULL)    # gave and gives error
    >> tools::assertError(m1 | double())# ditto
    >> ## m2 was slightly different:
    >> tools::assertError(m2 + NULL)
    >> tools::assertError(m2 & NULL)
    >> try(m2 == NULL) ## was logical(0) in R <= 3.3.x; now error as above!

    >> ----------------------------------------------------------------------------


    >> Note that in R's own  'nls'  sources, there was one case of
    >> situation '2)' above, i.e. a  1x1-matrix was used as a "scalar".

    >> In such cases, you should explicitly coerce it to a vector,
    >> either ("self-explainingly") by  as.vector(.), or as I did in
    >> the nls case  by  c(.) :  The latter is much less
    >> self-explaining, but nicer to read in mathematical formulae, and
    >> currently also more efficient because it is a .Primitive.

    >> Please use R-devel with your code, and let us know if you see
    >> effects that seem adverse.

    > I've been slightly surprised (or even "frustrated") by the empty
    > reaction on our R-devel list to this post.

    > I would have expected some critique, may be even some praise,
    > ... in any case some sign people are "thinking along" (as we say
    > in German).

    > In the mean time, I've actually thought along the one case which
    > is last above:  The <op>  (binary operation) between a
    > non-0-length array and a 0-length vector (and NULL which should
    > be treated like a 0-length vector):

    > R <= 3.3.1  *is* quite inconsistent with these:


    > and my proposal above (implemented in R-devel, since Sep.5) would give an
    > error for all these, but instead, R really could be more lenient here:
    > A 0-length result is ok, and it should *not* inherit the array
    > (dim, dimnames), since the array is not of length 0. So instead
    > of the above [for the very last part only!!], we would aim for
    > the following. These *all* give an error in current R-devel,
    > with the exception of 'm1 + NULL' which "only" gives a "bad
    > warning" :

    > ------------------------

    > m1 <- matrix(1,1)
    > m2 <- matrix(1,2)

    > m1 + NULL #    numeric(0) in R <= 3.3.x ---> OK ?!
    > m1 > NULL #    logical(0) in R <= 3.3.x ---> OK ?!
    > try(m1 & NULL)    # ERROR in R <= 3.3.x ---> change to logical(0)  ?!
    > try(m1 | double())# ERROR in R <= 3.3.x ---> change to logical(0)  ?!
    > ## m2 slightly different:
    > try(m2 + NULL)  # ERROR in R <= 3.3.x ---> change to double(0)  ?!
    > try(m2 & NULL)  # ERROR in R <= 3.3.x ---> change to logical(0)  ?!
    > m2 == NULL # logical(0) in R <= 3.3.x ---> OK ?!

    > ------------------------

    > This would be slightly more back-compatible than the currently
    > implemented proposal. Everything else I said remains true, and
    > I'm pretty sure most changes needed in packages would remain to be done.

    > Opinions ?

I now have updated 'R-devel' so it *does* implement the above
small amendment to the original proposal.

As a consequence, to *cumulative* changes are slightly more back
compatible.

If you are interested in this topic .. or if your CRAN package
checks show recent problems on the 'CRAN checks' web page,
make sure you get an R-devel version with svn rev. 71222  or
newer.

Martin


From baron at psych.upenn.edu  Wed Sep  7 22:53:21 2016
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Wed, 7 Sep 2016 16:53:21 -0400
Subject: [Rd] Fwd: Re: RSiteSearch, sos, rdocumentation.org, ...?
In-Reply-To: <20518b79-b272-b848-a736-a9acf225626b@prodsyse.com>
References: <20160907201522.GA12554@psych.upenn.edu>
	<20518b79-b272-b848-a736-a9acf225626b@prodsyse.com>
Message-ID: <20160907205321.GB24417@psych.upenn.edu>

Spencer,

Thanks for the quick reply.

I am open to someone who knows Perl getting an account on my site and
trying to get it working. It will probably involve fixing more than
one thing, as mknmz depends on some perl modules that also generate
errors.

My main contribution is figuring out how to extract the html help
files and vignettes only, with some help from R developers and Fedora
maintainers. Here is the trick, for someone who wants to do it:

m0 <- rownames(installed.packages())
m1 <- m0[which(m0 %in% needed.packages)]
source("http://bioconductor.org/biocLite.R")
update.packages(oldPkgs=m1,repos=biocinstallRepos())
update.packages(dependencies=FALSE,INSTALL_opts=c("--no-configure","--no-test-load","--no-R","--no-clean-on-error","--no-libs","--no-data","--no-demo","--no-exec","--html"),repos=biocinstallRepos(),ask=F)
m3 <- new.packages()
install.packages(m3,dependencies=FALSE,INSTALL_opts=c("--no-configure","--no-test-load","--no-R","--no-clean-on-error","--no-libs","--no-data","--no-demo","--no-exec","--html"),repos=biocinstallRepos())

Note 1: The first 4 lines are designed to deal with a list of the
packages that you actually use. These can be eliminated if you don't
use R on the same machine. The last 3 lines are all you need.

Note 2: This works on Fedora, but I think that the Fedora maintainers
of R have set some defaults that are helpful.

Jon

On 09/07/16 15:41, Spencer Graves wrote:
>Hello, All:
>
>
>       Jonathan Baron is "giving up" maintaining the RSiteSearch database.
>
>
>       This breaks three things:  (1) The R Site Search web service that 
>Baron has maintained.  (2) The RSiteSearch function in the utils 
>package.  (3) The sos package, for which I'm the maintainer and lead 
>author.
>
>
>       Might someone else be willing to take these over?
>
>
>       For me, the "findFn" capability with "writeFindFn2xls" is the 
>fastest literature search for anything statistical.  However, I don't 
>have the resources to take over the management of Baron's R Site Search 
>database.
>
>
>       He's provided a great service for the R community for many 
>years.  I hope we can find a way to keep the system maintained. Failing 
>that, I could use help in adapting the sos package to another database.
>
>
>       Thanks,
>       Spencer Graves
>
>
>-------- Forwarded Message --------
>Subject: 	Re: RSiteSearch, sos, rdocumentation.org, ...?
>Date: 	Wed, 7 Sep 2016 16:15:22 -0400
>From: 	Jonathan Baron <baron at psych.upenn.edu>
>To: 	Spencer Graves <spencer.graves at prodsyse.com>
>CC: 	Jonathan Baron <baron at psych.upenn.edu>, chris.is.fun at gmail.com, 
>info at datacamp.com <info at datacamp.com>, Sundar Dorai-Raj 
><sdorairaj at gmail.com>, webmaster at www.r-project-org
>
>
>
>R site search has stopped working. The indexing scrip, mknmz, failed
>to complete. It has been producing more and more errors and warnings,
>since it has not been updated for 5 yeaers.
>
>I am giving up on this site. I have too many other things to do aside
>from find bugs in programs written in languages I don't know (Perl),
>or set up an alternative search engine.
>
>Please inform anyone else who needs to be informed.
>
>I cannot find the email of the www.r-project.org webmaster, so I'm
>taking a stab. There are several links to this site in those pages.
>
>Jon
>-- 
>Jonathan Baron, Professor of Psychology, University of Pennsylvania
>Home page: http://www.sas.upenn.edu/~baron
>Editor: Judgment and Decision Making (http://journal.sjdm.org)
>

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron
Editor: Judgment and Decision Making (http://journal.sjdm.org)


From hankin.robin at gmail.com  Thu Sep  8 00:05:21 2016
From: hankin.robin at gmail.com (robin hankin)
Date: Thu, 8 Sep 2016 10:05:21 +1200
Subject: [Rd] R (development) changes in arith, logic,
 relop with (0-extent) arrays
In-Reply-To: <22479.58007.358121.626142@stat.math.ethz.ch>
References: <22469.52585.935914.336374@aragorn.wu.ac.at>
	<22470.37902.198447.925212@stat.math.ethz.ch>
	<22470.38100.993865.828495@aragorn.wu.ac.at>
	<22473.36759.791968.814130@stat.math.ethz.ch>
	<22473.38325.834026.658339@stat.math.ethz.ch>
	<22474.35836.489860.992765@aragorn.wu.ac.at>
	<CAPRP4-eKdb1AKGt73P-+khNXFAWm_4q2PZr69Q7BAjWcXKpSzA@mail.gmail.com>
	<CAPRP4-e4fL=j2xUVMjO-kdmeTfvF=+=9AKrU9+981Xvhwc2MPA@mail.gmail.com>
	<22477.32999.558832.601073@stat.math.ethz.ch>
	<22479.9847.747220.924128@stat.math.ethz.ch>
	<22479.58007.358121.626142@stat.math.ethz.ch>
Message-ID: <CAHHjBM6yvrgwOFiFAtM+w8BGmdwn114Z6gfyjMEsLQDLV8mA5A@mail.gmail.com>

Martin

I'd like to make a comment; I think that R's behaviour on 'edge' cases like
this is an important thing and it's great that you are working on it.

I make heavy use of zero-extent arrays, chiefly because the dimnames are an
efficient and logical way to keep track of certain types of information.

If I have, for example,

 a <- array(0,c(2,0,2))
 dimnames(a) <- list(name=c('Mike','Kevin'),NULL,item=c("hat","scarf"))


Then in R-3.3.1, 70800 I get

> a>0
logical(0)
>

But in 71219 I get

> a>0
, , item = hat


name
  Mike
  Kevin

, , item = scarf


name
  Mike
  Kevin

(which is an empty logical array that holds the names of the people and
their clothes). I find the behaviour of 71219 very much preferable because
there is no reason to discard the information in the dimnames.


Best wishes

Robin




On Wed, Sep 7, 2016 at 9:49 PM, Martin Maechler <maechler at stat.math.ethz.ch>
wrote:

> >>>>> Martin Maechler <maechler at stat.math.ethz.ch>
> >>>>>     on Tue, 6 Sep 2016 22:26:31 +0200 writes:
>
>     > Yesterday, changes to R's development version were committed,
> relating
>     > to arithmetic, logic ('&' and '|') and
>     > comparison/relational ('<', '==') binary operators
>     > which in NEWS are described as
>
>     > SIGNIFICANT USER-VISIBLE CHANGES:
>
>     > [.............]
>
>     > ? Arithmetic, logic (?&?, ?|?) and comparison (aka
>     > ?relational?, e.g., ?<?, ?==?) operations with arrays now
>     > behave consistently, notably for arrays of length zero.
>
>     > Arithmetic between length-1 arrays and longer non-arrays had
>     > silently dropped the array attributes and recycled.  This
>     > now gives a warning and will signal an error in the future,
>     > as it has always for logic and comparison operations in
>     > these cases (e.g., compare ?matrix(1,1) + 2:3? and
>     > ?matrix(1,1) < 2:3?).
>
>     > As the above "visually suggests" one could think of the changes
>     > falling mainly two groups,
>     > 1) <0-extent array>  (op)     <non-array>
>     > 2) <1-extent array>  (arith)  <non-array of length != 1>
>
>     > These changes are partly non-back compatible and may break
>     > existing code.  We believe that the internal consistency gained
>     > from the changes is worth the few places with problems.
>
>     > We expect some package maintainers (10-20, or even more?) need
>     > to adapt their code.
>
>     > Case '2)' above mainly results in a new warning, e.g.,
>
>     >> matrix(1,1) + 1:2
>     > [1] 2 3
>     > Warning message:
>     > In matrix(1, 1) + 1:2 :
>     > dropping dim() of array of length one.  Will become ERROR
>     >>
>
>     > whereas '1)' gives errors in cases the result silently was a
>     > vector of length zero, or also keeps array (dim & dimnames) in
>     > cases these were silently dropped.
>
>     > The following is a "heavily" commented  R script showing (all ?)
>     > the important cases with changes :
>
>     > ------------------------------------------------------------
> ----------------
>
>     > (m <- cbind(a=1[0], b=2[0]))
>     > Lm <- m; storage.mode(Lm) <- "logical"
>     > Im <- m; storage.mode(Im) <- "integer"
>
>     > ## 1. -------------------------
>     > try( m & NULL ) # in R <= 3.3.x :
>     > ## Error in m & NULL :
>     > ##  operations are possible only for numeric, logical or complex
> types
>     > ##
>     > ## gives 'Lm' in R >= 3.4.0
>
>     > ## 2. -------------------------
>     > m + 2:3 ## gave numeric(0), now remains matrix identical to  m
>     > Im + 2:3 ## gave integer(0), now remains matrix identical to Im
> (integer)
>
>     > m > 1      ## gave logical(0), now remains matrix identical to Lm
> (logical)
>     > m > 0.1[0] ##  ditto
>     > m > NULL   ##  ditto
>
>     > ## 3. -------------------------
>     > mm <- m[,c(1:2,2:1,2)]
>     > try( m == mm ) ## now gives error   "non-conformable arrays",
>     > ## but gave logical(0) in R <= 3.3.x
>
>     > ## 4. -------------------------
>     > str( Im + NULL)  ## gave "num", now gives "int"
>
>     > ## 5. -------------------------
>     > ## special case for arithmetic w/ length-1 array
>     > (m1 <- matrix(1,1,1, dimnames=list("Ro","col")))
>     > (m2 <- matrix(1,2,1, dimnames=list(c("A","B"),"col")))
>
>     > m1 + 1:2  # ->  2:3  but now with warning to  "become ERROR"
>     > tools::assertError(m1 & 1:2)# ERR: dims [product 1] do not match the
> length of object [2]
>     > tools::assertError(m1 < 1:2)# ERR:                  (ditto)
>     > ##
>     > ## non-0-length arrays combined with {NULL or double() or ...} *fail*
>
>     > ### Length-1 arrays:  Arithmetic with |vectors| > 1  treated array
> as scalar
>     > m1 + NULL # gave  numeric(0) in R <= 3.3.x --- still, *but* w/
> warning to "be ERROR"
>     > try(m1 > NULL)    # gave  logical(0) in R <= 3.3.x --- an *error*
> now in R >= 3.4.0
>     > tools::assertError(m1 & NULL)    # gave and gives error
>     > tools::assertError(m1 | double())# ditto
>     > ## m2 was slightly different:
>     > tools::assertError(m2 + NULL)
>     > tools::assertError(m2 & NULL)
>     > try(m2 == NULL) ## was logical(0) in R <= 3.3.x; now error as above!
>
>     > ------------------------------------------------------------
> ----------------
>
>
>     > Note that in R's own  'nls'  sources, there was one case of
>     > situation '2)' above, i.e. a  1x1-matrix was used as a "scalar".
>
>     > In such cases, you should explicitly coerce it to a vector,
>     > either ("self-explainingly") by  as.vector(.), or as I did in
>     > the nls case  by  c(.) :  The latter is much less
>     > self-explaining, but nicer to read in mathematical formulae, and
>     > currently also more efficient because it is a .Primitive.
>
>     > Please use R-devel with your code, and let us know if you see
>     > effects that seem adverse.
>
> I've been slightly surprised (or even "frustrated") by the empty
> reaction on our R-devel list to this post.
>
> I would have expected some critique, may be even some praise,
> ... in any case some sign people are "thinking along" (as we say
> in German).
>
> In the mean time, I've actually thought along the one case which
> is last above:  The <op>  (binary operation) between a
> non-0-length array and a 0-length vector (and NULL which should
> be treated like a 0-length vector):
>
> R <= 3.3.1  *is* quite inconsistent with these:
>
>
> and my proposal above (implemented in R-devel, since Sep.5) would give an
> error for all these, but instead, R really could be more lenient here:
> A 0-length result is ok, and it should *not* inherit the array
> (dim, dimnames), since the array is not of length 0. So instead
> of the above [for the very last part only!!], we would aim for
> the following. These *all* give an error in current R-devel,
> with the exception of 'm1 + NULL' which "only" gives a "bad
> warning" :
>
> ------------------------
>
> m1 <- matrix(1,1)
> m2 <- matrix(1,2)
>
> m1 + NULL #    numeric(0) in R <= 3.3.x ---> OK ?!
> m1 > NULL #    logical(0) in R <= 3.3.x ---> OK ?!
> try(m1 & NULL)    # ERROR in R <= 3.3.x ---> change to logical(0)  ?!
> try(m1 | double())# ERROR in R <= 3.3.x ---> change to logical(0)  ?!
> ## m2 slightly different:
> try(m2 + NULL)  # ERROR in R <= 3.3.x ---> change to double(0)  ?!
> try(m2 & NULL)  # ERROR in R <= 3.3.x ---> change to logical(0)  ?!
> m2 == NULL # logical(0) in R <= 3.3.x ---> OK ?!
>
> ------------------------
>
> This would be slightly more back-compatible than the currently
> implemented proposal. Everything else I said remains true, and
> I'm pretty sure most changes needed in packages would remain to be done.
>
> Opinions ?
>
>
>
>     > In some case where R-devel now gives an error but did not
>     > previously, we could contemplate giving another  "warning
>     > .... 'to become ERROR'" if there was too much breakage,  though
>     > I don't expect that.
>
>
>     > For the R Core Team,
>
>     > Martin Maechler,
>     > ETH Zurich
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>



-- 
Robin Hankin
Neutral theorist
hankin.robin at gmail.com

	[[alternative HTML version deleted]]


From baron at psych.upenn.edu  Thu Sep  8 04:06:57 2016
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Wed, 7 Sep 2016 22:06:57 -0400
Subject: [Rd] Fwd: Re: RSiteSearch, sos, rdocumentation.org, ...?
In-Reply-To: <20160907205321.GB24417@psych.upenn.edu>
References: <20160907201522.GA12554@psych.upenn.edu>
	<20518b79-b272-b848-a736-a9acf225626b@prodsyse.com>
	<20160907205321.GB24417@psych.upenn.edu>
Message-ID: <20160908020657.GA27642@psych.upenn.edu>

Don't do anything yet. I may have found the problem by accident.

I tried to use the computer from something else, and it was being
drastically slowed down by some leftover processes, which turned out
to be xlhtml. That is something that converts Excel files. Apparently,
some excel files got into the libraries, and they were causing the
indexing to hang completely.

I am now running everything again, starting from scratch, and it might
work. (I'm doing it wrong, but it is 3/4 done. I will do it right
tomorrow, if it works overnight.)

Jon

On 09/07/16 16:53, Jonathan Baron wrote:
>Spencer,
>
>Thanks for the quick reply.
>
>I am open to someone who knows Perl getting an account on my site and
>trying to get it working. It will probably involve fixing more than
>one thing, as mknmz depends on some perl modules that also generate
>errors.
>
>My main contribution is figuring out how to extract the html help
>files and vignettes only, with some help from R developers and Fedora
>maintainers. Here is the trick, for someone who wants to do it:
>
>m0 <- rownames(installed.packages())
>m1 <- m0[which(m0 %in% needed.packages)]
>source("http://bioconductor.org/biocLite.R")
>update.packages(oldPkgs=m1,repos=biocinstallRepos())
>update.packages(dependencies=FALSE,INSTALL_opts=c("--no-configure","--no-test-load","
>--no-R","--no-clean-on-error","--no-libs","--no-data","--no-demo","--no-exec","--html
>"),repos=biocinstallRepos(),ask=F)
>m3 <- new.packages()
>install.packages(m3,dependencies=FALSE,INSTALL_opts=c("--no-configure","--no-test-loa
>d","--no-R","--no-clean-on-error","--no-libs","--no-data","--no-demo","--no-exec","--
>html"),repos=biocinstallRepos())
>
>Note 1: The first 4 lines are designed to deal with a list of the
>packages that you actually use. These can be eliminated if you don't
>use R on the same machine. The last 3 lines are all you need.
>
>Note 2: This works on Fedora, but I think that the Fedora maintainers
>of R have set some defaults that are helpful.
>
>Jon
>
>On 09/07/16 15:41, Spencer Graves wrote:
>>Hello, All:
>>
>>
>>       Jonathan Baron is "giving up" maintaining the RSiteSearch database.
>>
>>
>>       This breaks three things:  (1) The R Site Search web service that 
>>Baron has maintained.  (2) The RSiteSearch function in the utils 
>>package.  (3) The sos package, for which I'm the maintainer and lead 
>>author.
>>
>>
>>       Might someone else be willing to take these over?
>>
>>
>>       For me, the "findFn" capability with "writeFindFn2xls" is the 
>>fastest literature search for anything statistical.  However, I don't 
>>have the resources to take over the management of Baron's R Site Search 
>>database.
>>
>>
>>       He's provided a great service for the R community for many 
>>years.  I hope we can find a way to keep the system maintained. Failing 
>>that, I could use help in adapting the sos package to another database.
>>
>>
>>       Thanks,
>>       Spencer Graves
>>
>>
>>-------- Forwarded Message --------
>>Subject: 	Re: RSiteSearch, sos, rdocumentation.org, ...?
>>Date: 	Wed, 7 Sep 2016 16:15:22 -0400
>>From: 	Jonathan Baron <baron at psych.upenn.edu>
>>To: 	Spencer Graves <spencer.graves at prodsyse.com>
>>CC: 	Jonathan Baron <baron at psych.upenn.edu>, chris.is.fun at gmail.com, 
>>info at datacamp.com <info at datacamp.com>, Sundar Dorai-Raj 
>><sdorairaj at gmail.com>, webmaster at www.r-project-org
>>
>>
>>
>>R site search has stopped working. The indexing scrip, mknmz, failed
>>to complete. It has been producing more and more errors and warnings,
>>since it has not been updated for 5 yeaers.
>>
>>I am giving up on this site. I have too many other things to do aside
>>from find bugs in programs written in languages I don't know (Perl),
>>or set up an alternative search engine.
>>
>>Please inform anyone else who needs to be informed.
>>
>>I cannot find the email of the www.r-project.org webmaster, so I'm
>>taking a stab. There are several links to this site in those pages.
>>
>>Jon
>>-- 
>>Jonathan Baron, Professor of Psychology, University of Pennsylvania
>>Home page: http://www.sas.upenn.edu/~baron
>>Editor: Judgment and Decision Making (http://journal.sjdm.org)
>>
>
>-- 
>Jonathan Baron, Professor of Psychology, University of Pennsylvania
>Home page: http://www.sas.upenn.edu/~baron
>Editor: Judgment and Decision Making (http://journal.sjdm.org)

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron
Editor: Judgment and Decision Making (http://journal.sjdm.org)


From ironholds at gmail.com  Thu Sep  8 03:59:00 2016
From: ironholds at gmail.com (Oliver Keyes)
Date: Wed, 7 Sep 2016 18:59:00 -0700
Subject: [Rd] R (development) changes in arith, logic,
 relop with (0-extent) arrays
In-Reply-To: <CAHHjBM6yvrgwOFiFAtM+w8BGmdwn114Z6gfyjMEsLQDLV8mA5A@mail.gmail.com>
References: <22469.52585.935914.336374@aragorn.wu.ac.at>
	<22470.37902.198447.925212@stat.math.ethz.ch>
	<22470.38100.993865.828495@aragorn.wu.ac.at>
	<22473.36759.791968.814130@stat.math.ethz.ch>
	<22473.38325.834026.658339@stat.math.ethz.ch>
	<22474.35836.489860.992765@aragorn.wu.ac.at>
	<CAPRP4-eKdb1AKGt73P-+khNXFAWm_4q2PZr69Q7BAjWcXKpSzA@mail.gmail.com>
	<CAPRP4-e4fL=j2xUVMjO-kdmeTfvF=+=9AKrU9+981Xvhwc2MPA@mail.gmail.com>
	<22477.32999.558832.601073@stat.math.ethz.ch>
	<22479.9847.747220.924128@stat.math.ethz.ch>
	<22479.58007.358121.626142@stat.math.ethz.ch>
	<CAHHjBM6yvrgwOFiFAtM+w8BGmdwn114Z6gfyjMEsLQDLV8mA5A@mail.gmail.com>
Message-ID: <CADRwj9-XVd+g7aKYNh3EF8tXK=fCgygCWgk=KOssXQhcZC55Zw@mail.gmail.com>

+1. Very grateful; more consistency is always great :)

On Wednesday, 7 September 2016, robin hankin <hankin.robin at gmail.com> wrote:

> Martin
>
> I'd like to make a comment; I think that R's behaviour on 'edge' cases like
> this is an important thing and it's great that you are working on it.
>
> I make heavy use of zero-extent arrays, chiefly because the dimnames are an
> efficient and logical way to keep track of certain types of information.
>
> If I have, for example,
>
>  a <- array(0,c(2,0,2))
>  dimnames(a) <- list(name=c('Mike','Kevin'),NULL,item=c("hat","scarf"))
>
>
> Then in R-3.3.1, 70800 I get
>
> > a>0
> logical(0)
> >
>
> But in 71219 I get
>
> > a>0
> , , item = hat
>
>
> name
>   Mike
>   Kevin
>
> , , item = scarf
>
>
> name
>   Mike
>   Kevin
>
> (which is an empty logical array that holds the names of the people and
> their clothes). I find the behaviour of 71219 very much preferable because
> there is no reason to discard the information in the dimnames.
>
>
> Best wishes
>
> Robin
>
>
>
>
> On Wed, Sep 7, 2016 at 9:49 PM, Martin Maechler <
> maechler at stat.math.ethz.ch <javascript:;>>
> wrote:
>
> > >>>>> Martin Maechler <maechler at stat.math.ethz.ch <javascript:;>>
> > >>>>>     on Tue, 6 Sep 2016 22:26:31 +0200 writes:
> >
> >     > Yesterday, changes to R's development version were committed,
> > relating
> >     > to arithmetic, logic ('&' and '|') and
> >     > comparison/relational ('<', '==') binary operators
> >     > which in NEWS are described as
> >
> >     > SIGNIFICANT USER-VISIBLE CHANGES:
> >
> >     > [.............]
> >
> >     > ? Arithmetic, logic (?&?, ?|?) and comparison (aka
> >     > ?relational?, e.g., ?<?, ?==?) operations with arrays now
> >     > behave consistently, notably for arrays of length zero.
> >
> >     > Arithmetic between length-1 arrays and longer non-arrays had
> >     > silently dropped the array attributes and recycled.  This
> >     > now gives a warning and will signal an error in the future,
> >     > as it has always for logic and comparison operations in
> >     > these cases (e.g., compare ?matrix(1,1) + 2:3? and
> >     > ?matrix(1,1) < 2:3?).
> >
> >     > As the above "visually suggests" one could think of the changes
> >     > falling mainly two groups,
> >     > 1) <0-extent array>  (op)     <non-array>
> >     > 2) <1-extent array>  (arith)  <non-array of length != 1>
> >
> >     > These changes are partly non-back compatible and may break
> >     > existing code.  We believe that the internal consistency gained
> >     > from the changes is worth the few places with problems.
> >
> >     > We expect some package maintainers (10-20, or even more?) need
> >     > to adapt their code.
> >
> >     > Case '2)' above mainly results in a new warning, e.g.,
> >
> >     >> matrix(1,1) + 1:2
> >     > [1] 2 3
> >     > Warning message:
> >     > In matrix(1, 1) + 1:2 :
> >     > dropping dim() of array of length one.  Will become ERROR
> >     >>
> >
> >     > whereas '1)' gives errors in cases the result silently was a
> >     > vector of length zero, or also keeps array (dim & dimnames) in
> >     > cases these were silently dropped.
> >
> >     > The following is a "heavily" commented  R script showing (all ?)
> >     > the important cases with changes :
> >
> >     > ------------------------------------------------------------
> > ----------------
> >
> >     > (m <- cbind(a=1[0], b=2[0]))
> >     > Lm <- m; storage.mode(Lm) <- "logical"
> >     > Im <- m; storage.mode(Im) <- "integer"
> >
> >     > ## 1. -------------------------
> >     > try( m & NULL ) # in R <= 3.3.x :
> >     > ## Error in m & NULL :
> >     > ##  operations are possible only for numeric, logical or complex
> > types
> >     > ##
> >     > ## gives 'Lm' in R >= 3.4.0
> >
> >     > ## 2. -------------------------
> >     > m + 2:3 ## gave numeric(0), now remains matrix identical to  m
> >     > Im + 2:3 ## gave integer(0), now remains matrix identical to Im
> > (integer)
> >
> >     > m > 1      ## gave logical(0), now remains matrix identical to Lm
> > (logical)
> >     > m > 0.1[0] ##  ditto
> >     > m > NULL   ##  ditto
> >
> >     > ## 3. -------------------------
> >     > mm <- m[,c(1:2,2:1,2)]
> >     > try( m == mm ) ## now gives error   "non-conformable arrays",
> >     > ## but gave logical(0) in R <= 3.3.x
> >
> >     > ## 4. -------------------------
> >     > str( Im + NULL)  ## gave "num", now gives "int"
> >
> >     > ## 5. -------------------------
> >     > ## special case for arithmetic w/ length-1 array
> >     > (m1 <- matrix(1,1,1, dimnames=list("Ro","col")))
> >     > (m2 <- matrix(1,2,1, dimnames=list(c("A","B"),"col")))
> >
> >     > m1 + 1:2  # ->  2:3  but now with warning to  "become ERROR"
> >     > tools::assertError(m1 & 1:2)# ERR: dims [product 1] do not match
> the
> > length of object [2]
> >     > tools::assertError(m1 < 1:2)# ERR:                  (ditto)
> >     > ##
> >     > ## non-0-length arrays combined with {NULL or double() or ...}
> *fail*
> >
> >     > ### Length-1 arrays:  Arithmetic with |vectors| > 1  treated array
> > as scalar
> >     > m1 + NULL # gave  numeric(0) in R <= 3.3.x --- still, *but* w/
> > warning to "be ERROR"
> >     > try(m1 > NULL)    # gave  logical(0) in R <= 3.3.x --- an *error*
> > now in R >= 3.4.0
> >     > tools::assertError(m1 & NULL)    # gave and gives error
> >     > tools::assertError(m1 | double())# ditto
> >     > ## m2 was slightly different:
> >     > tools::assertError(m2 + NULL)
> >     > tools::assertError(m2 & NULL)
> >     > try(m2 == NULL) ## was logical(0) in R <= 3.3.x; now error as
> above!
> >
> >     > ------------------------------------------------------------
> > ----------------
> >
> >
> >     > Note that in R's own  'nls'  sources, there was one case of
> >     > situation '2)' above, i.e. a  1x1-matrix was used as a "scalar".
> >
> >     > In such cases, you should explicitly coerce it to a vector,
> >     > either ("self-explainingly") by  as.vector(.), or as I did in
> >     > the nls case  by  c(.) :  The latter is much less
> >     > self-explaining, but nicer to read in mathematical formulae, and
> >     > currently also more efficient because it is a .Primitive.
> >
> >     > Please use R-devel with your code, and let us know if you see
> >     > effects that seem adverse.
> >
> > I've been slightly surprised (or even "frustrated") by the empty
> > reaction on our R-devel list to this post.
> >
> > I would have expected some critique, may be even some praise,
> > ... in any case some sign people are "thinking along" (as we say
> > in German).
> >
> > In the mean time, I've actually thought along the one case which
> > is last above:  The <op>  (binary operation) between a
> > non-0-length array and a 0-length vector (and NULL which should
> > be treated like a 0-length vector):
> >
> > R <= 3.3.1  *is* quite inconsistent with these:
> >
> >
> > and my proposal above (implemented in R-devel, since Sep.5) would give an
> > error for all these, but instead, R really could be more lenient here:
> > A 0-length result is ok, and it should *not* inherit the array
> > (dim, dimnames), since the array is not of length 0. So instead
> > of the above [for the very last part only!!], we would aim for
> > the following. These *all* give an error in current R-devel,
> > with the exception of 'm1 + NULL' which "only" gives a "bad
> > warning" :
> >
> > ------------------------
> >
> > m1 <- matrix(1,1)
> > m2 <- matrix(1,2)
> >
> > m1 + NULL #    numeric(0) in R <= 3.3.x ---> OK ?!
> > m1 > NULL #    logical(0) in R <= 3.3.x ---> OK ?!
> > try(m1 & NULL)    # ERROR in R <= 3.3.x ---> change to logical(0)  ?!
> > try(m1 | double())# ERROR in R <= 3.3.x ---> change to logical(0)  ?!
> > ## m2 slightly different:
> > try(m2 + NULL)  # ERROR in R <= 3.3.x ---> change to double(0)  ?!
> > try(m2 & NULL)  # ERROR in R <= 3.3.x ---> change to logical(0)  ?!
> > m2 == NULL # logical(0) in R <= 3.3.x ---> OK ?!
> >
> > ------------------------
> >
> > This would be slightly more back-compatible than the currently
> > implemented proposal. Everything else I said remains true, and
> > I'm pretty sure most changes needed in packages would remain to be done.
> >
> > Opinions ?
> >
> >
> >
> >     > In some case where R-devel now gives an error but did not
> >     > previously, we could contemplate giving another  "warning
> >     > .... 'to become ERROR'" if there was too much breakage,  though
> >     > I don't expect that.
> >
> >
> >     > For the R Core Team,
> >
> >     > Martin Maechler,
> >     > ETH Zurich
> >
> > ______________________________________________
> > R-devel at r-project.org <javascript:;> mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
> >
>
>
>
> --
> Robin Hankin
> Neutral theorist
> hankin.robin at gmail.com <javascript:;>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org <javascript:;> mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

	[[alternative HTML version deleted]]


From cosi1 at tlen.pl  Thu Sep  8 09:35:56 2016
From: cosi1 at tlen.pl (=?UTF-8?Q?Pawe=C5=82_Pi=C4=85tkowski?=)
Date: Thu, 08 Sep 2016 09:35:56 +0200
Subject: [Rd] =?utf-8?q?Building_R_under_Linux_-_library_dependencies?=
In-Reply-To: <22480.28555.417649.426895@max.nulle.part>
References: <62ff6ef3.6e95b7a5.57d031ed.7e18d@tlen.pl>
	<22480.20131.781988.142009@max.nulle.part>
	<3092aed4.f96460.57d06171.56abc@tlen.pl>
	<22480.28555.417649.426895@max.nulle.part>
Message-ID: <2a27d83f.6dfdcc53.57d114dc.3f0c8@tlen.pl>

> You are not enumerating your trade-offs very well. There are natural
> conflicts. What is you really want?
> 
> - Being able to pre-build and distribute?  We have done that since the last
> 5C1990s with .deb packages.
> 
> - Being able to install with minimal size?  Have you queried your users?  I
> note that among the Docker containers for R (in the "Rocker" project Carl and
> I run) the _larger_ ones containing RStudio plus optionally "lots from
> hadley" plus optionally lots of rOpenSci tend to me _more_ popular (for ease
> of installation of the aggregate).
> 
> And while share the overall sentiment a little bit, you have to realize that
> it is 2016 with the corresponding bandwith and storage:
> 
>   edd at max:~$ du -csh /usr/local/lib/R/site-library/
>   1.5G    /usr/local/lib/R/site-library/
>   1.5G    total
>   edd at max:~$
> 
> And that it _outside_ of R itself, or the (numerous) other shared libraries.

OK, to be honest, it was rather a proof-of-concept than a specific idea. Other interpreted and VM-based languages have robust app deployment systems with smaller footprint, so I thought that it would be nice to have something similar in R.
Maybe you are right and neither R developers, nor users actually need it.

Thanks for the discussion,
-p-


From baron at psych.upenn.edu  Thu Sep  8 12:01:00 2016
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Thu, 8 Sep 2016 06:01:00 -0400
Subject: [Rd] Fwd: Re: RSiteSearch, sos, rdocumentation.org, ...?
In-Reply-To: <20160908020657.GA27642@psych.upenn.edu>
References: <20160907201522.GA12554@psych.upenn.edu>
	<20518b79-b272-b848-a736-a9acf225626b@prodsyse.com>
	<20160907205321.GB24417@psych.upenn.edu>
	<20160908020657.GA27642@psych.upenn.edu>
Message-ID: <20160908100100.GA30732@psych.upenn.edu>

OK.  It is sort of fixed and sort of works.

We'll keep it for now, but this is not going to work forever. When
namazu fails completely I will not have the time to install a new
search engine.

One option is to use google. For a site like this, I think they will
want some money, but I'm not sure, and I do not have the time to deal
with it.

We have over 10,000 packages now. I wonder if searching all help files
is really helpful anymore.

Jon

On 09/07/16 22:06, Jonathan Baron wrote:
>Don't do anything yet. I may have found the problem by accident.
>
>I tried to use the computer from something else, and it was being
>drastically slowed down by some leftover processes, which turned out
>to be xlhtml. That is something that converts Excel files. Apparently,
>some excel files got into the libraries, and they were causing the
>indexing to hang completely.
>
>I am now running everything again, starting from scratch, and it might
>work. (I'm doing it wrong, but it is 3/4 done. I will do it right
>tomorrow, if it works overnight.)
>
>Jon
>
>On 09/07/16 16:53, Jonathan Baron wrote:
>>Spencer,
>>
>>Thanks for the quick reply.
>>
>>I am open to someone who knows Perl getting an account on my site and
>>trying to get it working. It will probably involve fixing more than
>>one thing, as mknmz depends on some perl modules that also generate
>>errors.
>>
>>My main contribution is figuring out how to extract the html help
>>files and vignettes only, with some help from R developers and Fedora
>>maintainers. Here is the trick, for someone who wants to do it:
>>
>>m0 <- rownames(installed.packages())
>>m1 <- m0[which(m0 %in% needed.packages)]
>>source("http://bioconductor.org/biocLite.R")
>>update.packages(oldPkgs=m1,repos=biocinstallRepos())
>>update.packages(dependencies=FALSE,INSTALL_opts=c("--no-configure","--no-test-load",
>"
>>--no-R","--no-clean-on-error","--no-libs","--no-data","--no-demo","--no-exec","--htm
>l
>>"),repos=biocinstallRepos(),ask=F)
>>m3 <- new.packages()
>>install.packages(m3,dependencies=FALSE,INSTALL_opts=c("--no-configure","--no-test-lo
>a
>>d","--no-R","--no-clean-on-error","--no-libs","--no-data","--no-demo","--no-exec","-
>-
>>html"),repos=biocinstallRepos())
>>
>>Note 1: The first 4 lines are designed to deal with a list of the
>>packages that you actually use. These can be eliminated if you don't
>>use R on the same machine. The last 3 lines are all you need.
>>
>>Note 2: This works on Fedora, but I think that the Fedora maintainers
>>of R have set some defaults that are helpful.
>>
>>Jon
>>
>>On 09/07/16 15:41, Spencer Graves wrote:
>>>Hello, All:
>>>
>>>
>>>       Jonathan Baron is "giving up" maintaining the RSiteSearch database.
>>>
>>>
>>>       This breaks three things:  (1) The R Site Search web service that 
>>>Baron has maintained.  (2) The RSiteSearch function in the utils 
>>>package.  (3) The sos package, for which I'm the maintainer and lead 
>>>author.
>>>
>>>
>>>       Might someone else be willing to take these over?
>>>
>>>
>>>       For me, the "findFn" capability with "writeFindFn2xls" is the 
>>>fastest literature search for anything statistical.  However, I don't 
>>>have the resources to take over the management of Baron's R Site Search 
>>>database.
>>>
>>>
>>>       He's provided a great service for the R community for many 
>>>years.  I hope we can find a way to keep the system maintained. Failing 
>>>that, I could use help in adapting the sos package to another database.
>>>
>>>
>>>       Thanks,
>>>       Spencer Graves
>>>
>>>
>>>-------- Forwarded Message --------
>>>Subject: 	Re: RSiteSearch, sos, rdocumentation.org, ...?
>>>Date: 	Wed, 7 Sep 2016 16:15:22 -0400
>>>From: 	Jonathan Baron <baron at psych.upenn.edu>
>>>To: 	Spencer Graves <spencer.graves at prodsyse.com>
>>>CC: 	Jonathan Baron <baron at psych.upenn.edu>, chris.is.fun at gmail.com, 
>>>info at datacamp.com <info at datacamp.com>, Sundar Dorai-Raj 
>>><sdorairaj at gmail.com>, webmaster at www.r-project-org
>>>
>>>
>>>
>>>R site search has stopped working. The indexing scrip, mknmz, failed
>>>to complete. It has been producing more and more errors and warnings,
>>>since it has not been updated for 5 yeaers.
>>>
>>>I am giving up on this site. I have too many other things to do aside
>>>from find bugs in programs written in languages I don't know (Perl),
>>>or set up an alternative search engine.
>>>
>>>Please inform anyone else who needs to be informed.
>>>
>>>I cannot find the email of the www.r-project.org webmaster, so I'm
>>>taking a stab. There are several links to this site in those pages.
>>>
>>>Jon
>>>-- 
>>>Jonathan Baron, Professor of Psychology, University of Pennsylvania
>>>Home page: http://www.sas.upenn.edu/~baron
>>>Editor: Judgment and Decision Making (http://journal.sjdm.org)
>>>
>>
>>-- 
>>Jonathan Baron, Professor of Psychology, University of Pennsylvania
>>Home page: http://www.sas.upenn.edu/~baron
>>Editor: Judgment and Decision Making (http://journal.sjdm.org)
>
>-- 
>Jonathan Baron, Professor of Psychology, University of Pennsylvania
>Home page: http://www.sas.upenn.edu/~baron
>Editor: Judgment and Decision Making (http://journal.sjdm.org)

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron
Editor: Judgment and Decision Making (http://journal.sjdm.org)


From maechler at stat.math.ethz.ch  Thu Sep  8 12:49:55 2016
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 8 Sep 2016 12:49:55 +0200
Subject: [Rd] R (development) changes in arith, logic,
 relop with (0-extent) arrays
In-Reply-To: <CAHHjBM6yvrgwOFiFAtM+w8BGmdwn114Z6gfyjMEsLQDLV8mA5A@mail.gmail.com>
References: <22469.52585.935914.336374@aragorn.wu.ac.at>
	<22470.37902.198447.925212@stat.math.ethz.ch>
	<22470.38100.993865.828495@aragorn.wu.ac.at>
	<22473.36759.791968.814130@stat.math.ethz.ch>
	<22473.38325.834026.658339@stat.math.ethz.ch>
	<22474.35836.489860.992765@aragorn.wu.ac.at>
	<CAPRP4-eKdb1AKGt73P-+khNXFAWm_4q2PZr69Q7BAjWcXKpSzA@mail.gmail.com>
	<CAPRP4-e4fL=j2xUVMjO-kdmeTfvF=+=9AKrU9+981Xvhwc2MPA@mail.gmail.com>
	<22477.32999.558832.601073@stat.math.ethz.ch>
	<22479.9847.747220.924128@stat.math.ethz.ch>
	<22479.58007.358121.626142@stat.math.ethz.ch>
	<CAHHjBM6yvrgwOFiFAtM+w8BGmdwn114Z6gfyjMEsLQDLV8mA5A@mail.gmail.com>
Message-ID: <22481.16979.861309.264630@stat.math.ethz.ch>

>>>>> robin hankin <hankin.robin at gmail.com>
>>>>>     on Thu, 8 Sep 2016 10:05:21 +1200 writes:

    > Martin I'd like to make a comment; I think that R's
    > behaviour on 'edge' cases like this is an important thing
    > and it's great that you are working on it.

    > I make heavy use of zero-extent arrays, chiefly because
    > the dimnames are an efficient and logical way to keep
    > track of certain types of information.

    > If I have, for example,

    > a <- array(0,c(2,0,2))
    > dimnames(a) <- list(name=c('Mike','Kevin'),NULL,item=c("hat","scarf"))


    > Then in R-3.3.1, 70800 I get

    a> 0
    > logical(0)
    >> 

    > But in 71219 I get

    a> 0
    > , , item = hat


    > name
    > Mike
    > Kevin

    > , , item = scarf


    > name
    > Mike
    > Kevin

    > (which is an empty logical array that holds the names of the people and
    > their clothes). I find the behaviour of 71219 very much preferable because
    > there is no reason to discard the information in the dimnames.

Thanks a lot, Robin, (and Oliver) !

Yes, the above is such a case where the new behavior makes much sense.
And this behavior remains identical after the 71222 amendment.

Martin

    > Best wishes
    > Robin




    > On Wed, Sep 7, 2016 at 9:49 PM, Martin Maechler <maechler at stat.math.ethz.ch>
    > wrote:

    >> >>>>> Martin Maechler <maechler at stat.math.ethz.ch>
    >> >>>>>     on Tue, 6 Sep 2016 22:26:31 +0200 writes:
    >> 
    >> > Yesterday, changes to R's development version were committed,
    >> relating
    >> > to arithmetic, logic ('&' and '|') and
    >> > comparison/relational ('<', '==') binary operators
    >> > which in NEWS are described as
    >> 
    >> > SIGNIFICANT USER-VISIBLE CHANGES:
    >> 
    >> > [.............]
    >> 
    >> > ? Arithmetic, logic (?&?, ?|?) and comparison (aka
    >> > ?relational?, e.g., ?<?, ?==?) operations with arrays now
    >> > behave consistently, notably for arrays of length zero.
    >> 
    >> > Arithmetic between length-1 arrays and longer non-arrays had
    >> > silently dropped the array attributes and recycled.  This
    >> > now gives a warning and will signal an error in the future,
    >> > as it has always for logic and comparison operations in
    >> > these cases (e.g., compare ?matrix(1,1) + 2:3? and
    >> > ?matrix(1,1) < 2:3?).
    >> 
    >> > As the above "visually suggests" one could think of the changes
    >> > falling mainly two groups,
    >> > 1) <0-extent array>  (op)     <non-array>
    >> > 2) <1-extent array>  (arith)  <non-array of length != 1>
    >> 
    >> > These changes are partly non-back compatible and may break
    >> > existing code.  We believe that the internal consistency gained
    >> > from the changes is worth the few places with problems.
    >> 
    >> > We expect some package maintainers (10-20, or even more?) need
    >> > to adapt their code.
    >> 
    >> > Case '2)' above mainly results in a new warning, e.g.,
    >> 
    >> >> matrix(1,1) + 1:2
    >> > [1] 2 3
    >> > Warning message:
    >> > In matrix(1, 1) + 1:2 :
    >> > dropping dim() of array of length one.  Will become ERROR
    >> >>
    >> 
    >> > whereas '1)' gives errors in cases the result silently was a
    >> > vector of length zero, or also keeps array (dim & dimnames) in
    >> > cases these were silently dropped.
    >> 
    >> > The following is a "heavily" commented  R script showing (all ?)
    >> > the important cases with changes :
    >> 
    >> > ------------------------------------------------------------
    >> ----------------
    >> 
    >> > (m <- cbind(a=1[0], b=2[0]))
    >> > Lm <- m; storage.mode(Lm) <- "logical"
    >> > Im <- m; storage.mode(Im) <- "integer"
    >> 
    >> > ## 1. -------------------------
    >> > try( m & NULL ) # in R <= 3.3.x :
    >> > ## Error in m & NULL :
    >> > ##  operations are possible only for numeric, logical or complex
    >> types
    >> > ##
    >> > ## gives 'Lm' in R >= 3.4.0
    >> 
    >> > ## 2. -------------------------
    >> > m + 2:3 ## gave numeric(0), now remains matrix identical to  m
    >> > Im + 2:3 ## gave integer(0), now remains matrix identical to Im
    >> (integer)
    >> 
    >> > m > 1      ## gave logical(0), now remains matrix identical to Lm
    >> (logical)
    >> > m > 0.1[0] ##  ditto
    >> > m > NULL   ##  ditto
    >> 
    >> > ## 3. -------------------------
    >> > mm <- m[,c(1:2,2:1,2)]
    >> > try( m == mm ) ## now gives error   "non-conformable arrays",
    >> > ## but gave logical(0) in R <= 3.3.x
    >> 
    >> > ## 4. -------------------------
    >> > str( Im + NULL)  ## gave "num", now gives "int"
    >> 
    >> > ## 5. -------------------------
    >> > ## special case for arithmetic w/ length-1 array
    >> > (m1 <- matrix(1,1,1, dimnames=list("Ro","col")))
    >> > (m2 <- matrix(1,2,1, dimnames=list(c("A","B"),"col")))
    >> 
    >> > m1 + 1:2  # ->  2:3  but now with warning to  "become ERROR"
    >> > tools::assertError(m1 & 1:2)# ERR: dims [product 1] do not match the
    >> length of object [2]
    >> > tools::assertError(m1 < 1:2)# ERR:                  (ditto)
    >> > ##
    >> > ## non-0-length arrays combined with {NULL or double() or ...} *fail*
    >> 
    >> > ### Length-1 arrays:  Arithmetic with |vectors| > 1  treated array
    >> as scalar
    >> > m1 + NULL # gave  numeric(0) in R <= 3.3.x --- still, *but* w/
    >> warning to "be ERROR"
    >> > try(m1 > NULL)    # gave  logical(0) in R <= 3.3.x --- an *error*
    >> now in R >= 3.4.0
    >> > tools::assertError(m1 & NULL)    # gave and gives error
    >> > tools::assertError(m1 | double())# ditto
    >> > ## m2 was slightly different:
    >> > tools::assertError(m2 + NULL)
    >> > tools::assertError(m2 & NULL)
    >> > try(m2 == NULL) ## was logical(0) in R <= 3.3.x; now error as above!
    >> 
    >> > ------------------------------------------------------------
    >> ----------------
    >> 
    >> 
    >> > Note that in R's own  'nls'  sources, there was one case of
    >> > situation '2)' above, i.e. a  1x1-matrix was used as a "scalar".
    >> 
    >> > In such cases, you should explicitly coerce it to a vector,
    >> > either ("self-explainingly") by  as.vector(.), or as I did in
    >> > the nls case  by  c(.) :  The latter is much less
    >> > self-explaining, but nicer to read in mathematical formulae, and
    >> > currently also more efficient because it is a .Primitive.
    >> 
    >> > Please use R-devel with your code, and let us know if you see
    >> > effects that seem adverse.
    >> 
    >> I've been slightly surprised (or even "frustrated") by the empty
    >> reaction on our R-devel list to this post.
    >> 
    >> I would have expected some critique, may be even some praise,
    >> ... in any case some sign people are "thinking along" (as we say
    >> in German).
    >> 
    >> In the mean time, I've actually thought along the one case which
    >> is last above:  The <op>  (binary operation) between a
    >> non-0-length array and a 0-length vector (and NULL which should
    >> be treated like a 0-length vector):
    >> 
    >> R <= 3.3.1  *is* quite inconsistent with these:
    >> 
    >> 
    >> and my proposal above (implemented in R-devel, since Sep.5) would give an
    >> error for all these, but instead, R really could be more lenient here:
    >> A 0-length result is ok, and it should *not* inherit the array
    >> (dim, dimnames), since the array is not of length 0. So instead
    >> of the above [for the very last part only!!], we would aim for
    >> the following. These *all* give an error in current R-devel,
    >> with the exception of 'm1 + NULL' which "only" gives a "bad
    >> warning" :
    >> 
    >> ------------------------
    >> 
    >> m1 <- matrix(1,1)
    >> m2 <- matrix(1,2)
    >> 
    >> m1 + NULL #    numeric(0) in R <= 3.3.x ---> OK ?!
    >> m1 > NULL #    logical(0) in R <= 3.3.x ---> OK ?!
    >> try(m1 & NULL)    # ERROR in R <= 3.3.x ---> change to logical(0)  ?!
    >> try(m1 | double())# ERROR in R <= 3.3.x ---> change to logical(0)  ?!
    >> ## m2 slightly different:
    >> try(m2 + NULL)  # ERROR in R <= 3.3.x ---> change to double(0)  ?!
    >> try(m2 & NULL)  # ERROR in R <= 3.3.x ---> change to logical(0)  ?!
    >> m2 == NULL # logical(0) in R <= 3.3.x ---> OK ?!
    >> 
    >> ------------------------
    >> 
    >> This would be slightly more back-compatible than the currently
    >> implemented proposal. Everything else I said remains true, and
    >> I'm pretty sure most changes needed in packages would remain to be done.
    >> 
    >> Opinions ?
    >> 
    >> 
    >> 
    >> > In some case where R-devel now gives an error but did not
    >> > previously, we could contemplate giving another  "warning
    >> > .... 'to become ERROR'" if there was too much breakage,  though
    >> > I don't expect that.
    >> 
    >> 
    >> > For the R Core Team,
    >> 
    >> > Martin Maechler,
    >> > ETH Zurich
    >> 
    >> ______________________________________________
    >> R-devel at r-project.org mailing list
    >> https://stat.ethz.ch/mailman/listinfo/r-devel
    >> 



    > -- 
    > Robin Hankin
    > Neutral theorist
    > hankin.robin at gmail.com

    > [[alternative HTML version deleted]]


From lists at dewey.myzen.co.uk  Thu Sep  8 13:00:54 2016
From: lists at dewey.myzen.co.uk (Michael Dewey)
Date: Thu, 8 Sep 2016 12:00:54 +0100
Subject: [Rd] Fwd: Re: RSiteSearch, sos, rdocumentation.org, ...?
In-Reply-To: <20160908100100.GA30732@psych.upenn.edu>
References: <20160907201522.GA12554@psych.upenn.edu>
	<20518b79-b272-b848-a736-a9acf225626b@prodsyse.com>
	<20160907205321.GB24417@psych.upenn.edu>
	<20160908020657.GA27642@psych.upenn.edu>
	<20160908100100.GA30732@psych.upenn.edu>
Message-ID: <dd10d6ec-6af9-55f0-d7a7-b8c615cd2f49@dewey.myzen.co.uk>

I have mixed feelings about this. I used to find the sos package very 
useful when I first started using it but as the number of packages has 
grown I now find it gives me a huge list which takes a lot of time to 
digest. This may of course reflect my rudimentary search term selection 
skills.

Michael

On 08/09/2016 11:01, Jonathan Baron wrote:
> OK.  It is sort of fixed and sort of works.
>
> We'll keep it for now, but this is not going to work forever. When
> namazu fails completely I will not have the time to install a new
> search engine.
>
> One option is to use google. For a site like this, I think they will
> want some money, but I'm not sure, and I do not have the time to deal
> with it.
>
> We have over 10,000 packages now. I wonder if searching all help files
> is really helpful anymore.
>
> Jon
>
> On 09/07/16 22:06, Jonathan Baron wrote:
>> Don't do anything yet. I may have found the problem by accident.
>>
>> I tried to use the computer from something else, and it was being
>> drastically slowed down by some leftover processes, which turned out
>> to be xlhtml. That is something that converts Excel files. Apparently,
>> some excel files got into the libraries, and they were causing the
>> indexing to hang completely.
>>
>> I am now running everything again, starting from scratch, and it might
>> work. (I'm doing it wrong, but it is 3/4 done. I will do it right
>> tomorrow, if it works overnight.)
>>
>> Jon
>>
>> On 09/07/16 16:53, Jonathan Baron wrote:
>>> Spencer,
>>>
>>> Thanks for the quick reply.
>>>
>>> I am open to someone who knows Perl getting an account on my site and
>>> trying to get it working. It will probably involve fixing more than
>>> one thing, as mknmz depends on some perl modules that also generate
>>> errors.
>>>
>>> My main contribution is figuring out how to extract the html help
>>> files and vignettes only, with some help from R developers and Fedora
>>> maintainers. Here is the trick, for someone who wants to do it:
>>>
>>> m0 <- rownames(installed.packages())
>>> m1 <- m0[which(m0 %in% needed.packages)]
>>> source("http://bioconductor.org/biocLite.R")
>>> update.packages(oldPkgs=m1,repos=biocinstallRepos())
>>> update.packages(dependencies=FALSE,INSTALL_opts=c("--no-configure","--no-test-load",
>>>
>> "
>>> --no-R","--no-clean-on-error","--no-libs","--no-data","--no-demo","--no-exec","--htm
>>>
>> l
>>> "),repos=biocinstallRepos(),ask=F)
>>> m3 <- new.packages()
>>> install.packages(m3,dependencies=FALSE,INSTALL_opts=c("--no-configure","--no-test-lo
>>>
>> a
>>> d","--no-R","--no-clean-on-error","--no-libs","--no-data","--no-demo","--no-exec","-
>>>
>> -
>>> html"),repos=biocinstallRepos())
>>>
>>> Note 1: The first 4 lines are designed to deal with a list of the
>>> packages that you actually use. These can be eliminated if you don't
>>> use R on the same machine. The last 3 lines are all you need.
>>>
>>> Note 2: This works on Fedora, but I think that the Fedora maintainers
>>> of R have set some defaults that are helpful.
>>>
>>> Jon
>>>
>>> On 09/07/16 15:41, Spencer Graves wrote:
>>>> Hello, All:
>>>>
>>>>
>>>>       Jonathan Baron is "giving up" maintaining the RSiteSearch
>>>> database.
>>>>
>>>>
>>>>       This breaks three things:  (1) The R Site Search web service
>>>> that Baron has maintained.  (2) The RSiteSearch function in the
>>>> utils package.  (3) The sos package, for which I'm the maintainer
>>>> and lead author.
>>>>
>>>>
>>>>       Might someone else be willing to take these over?
>>>>
>>>>
>>>>       For me, the "findFn" capability with "writeFindFn2xls" is the
>>>> fastest literature search for anything statistical.  However, I
>>>> don't have the resources to take over the management of Baron's R
>>>> Site Search database.
>>>>
>>>>
>>>>       He's provided a great service for the R community for many
>>>> years.  I hope we can find a way to keep the system maintained.
>>>> Failing that, I could use help in adapting the sos package to
>>>> another database.
>>>>
>>>>
>>>>       Thanks,
>>>>       Spencer Graves
>>>>
>>>>
>>>> -------- Forwarded Message --------
>>>> Subject:     Re: RSiteSearch, sos, rdocumentation.org, ...?
>>>> Date:     Wed, 7 Sep 2016 16:15:22 -0400
>>>> From:     Jonathan Baron <baron at psych.upenn.edu>
>>>> To:     Spencer Graves <spencer.graves at prodsyse.com>
>>>> CC:     Jonathan Baron <baron at psych.upenn.edu>,
>>>> chris.is.fun at gmail.com, info at datacamp.com <info at datacamp.com>,
>>>> Sundar Dorai-Raj <sdorairaj at gmail.com>, webmaster at www.r-project-org
>>>>
>>>>
>>>>
>>>> R site search has stopped working. The indexing scrip, mknmz, failed
>>>> to complete. It has been producing more and more errors and warnings,
>>>> since it has not been updated for 5 yeaers.
>>>>
>>>> I am giving up on this site. I have too many other things to do aside
>>>> from find bugs in programs written in languages I don't know (Perl),
>>>> or set up an alternative search engine.
>>>>
>>>> Please inform anyone else who needs to be informed.
>>>>
>>>> I cannot find the email of the www.r-project.org webmaster, so I'm
>>>> taking a stab. There are several links to this site in those pages.
>>>>
>>>> Jon
>>>> --
>>>> Jonathan Baron, Professor of Psychology, University of Pennsylvania
>>>> Home page: http://www.sas.upenn.edu/~baron
>>>> Editor: Judgment and Decision Making (http://journal.sjdm.org)
>>>>
>>>
>>> --
>>> Jonathan Baron, Professor of Psychology, University of Pennsylvania
>>> Home page: http://www.sas.upenn.edu/~baron
>>> Editor: Judgment and Decision Making (http://journal.sjdm.org)
>>
>> --
>> Jonathan Baron, Professor of Psychology, University of Pennsylvania
>> Home page: http://www.sas.upenn.edu/~baron
>> Editor: Judgment and Decision Making (http://journal.sjdm.org)
>

-- 
Michael
http://www.dewey.myzen.co.uk/home.html


From edd at debian.org  Thu Sep  8 13:36:32 2016
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 8 Sep 2016 06:36:32 -0500
Subject: [Rd] Fwd: Re: RSiteSearch, sos, rdocumentation.org, ...?
In-Reply-To: <20160908100100.GA30732@psych.upenn.edu>
References: <20160907201522.GA12554@psych.upenn.edu>
	<20518b79-b272-b848-a736-a9acf225626b@prodsyse.com>
	<20160907205321.GB24417@psych.upenn.edu>
	<20160908020657.GA27642@psych.upenn.edu>
	<20160908100100.GA30732@psych.upenn.edu>
Message-ID: <22481.19776.517445.200878@max.nulle.part>


On 8 September 2016 at 06:01, Jonathan Baron wrote:
| We have over 10,000 packages now. I wonder if searching all help files
| is really helpful anymore.

Yes it is. I go to http://rdocumentation.org a lot for quick look-ups.

So thanks to Datacamp for running that.  

Dirk

-- 
http://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From spencer.graves at prodsyse.com  Thu Sep  8 15:23:57 2016
From: spencer.graves at prodsyse.com (Spencer Graves)
Date: Thu, 8 Sep 2016 08:23:57 -0500
Subject: [Rd] Fwd: Re: RSiteSearch, sos, rdocumentation.org, ...?
In-Reply-To: <CAO1zAVY9Angw-UW2Cgtxs=HAZM=y=qG_QNiSdsw30ENi12ShUg@mail.gmail.com>
References: <20160907201522.GA12554@psych.upenn.edu>
	<20518b79-b272-b848-a736-a9acf225626b@prodsyse.com>
	<20160907205321.GB24417@psych.upenn.edu>
	<20160908020657.GA27642@psych.upenn.edu>
	<CAO1zAVY9Angw-UW2Cgtxs=HAZM=y=qG_QNiSdsw30ENi12ShUg@mail.gmail.com>
Message-ID: <b2996b76-7ace-39b0-19e2-972e7f7ca703@prodsyse.com>



On 9/8/2016 3:30 AM, Joris Meys wrote:
>
> Hi Jonathan,
>
> I have neither the resources nor the skills to take over, but whatever 
> happens I want to thank you for all the work. Too often people forget 
> that all these nice tools keep working due to the devotion of people 
> like you.
>
> So thank you!
>


       I concur.  People all over the world live better today, because R 
made it easier for others to solve problems -- and Jon made a 
substantive contribution to that.  Spencer


> Cheers
> Joris
>
>
> On 8 Sep 2016 04:08, "Jonathan Baron" <baron at psych.upenn.edu 
> <mailto:baron at psych.upenn.edu>> wrote:
>
>     Don't do anything yet. I may have found the problem by accident.
>
>     I tried to use the computer from something else, and it was being
>     drastically slowed down by some leftover processes, which turned out
>     to be xlhtml. That is something that converts Excel files. Apparently,
>     some excel files got into the libraries, and they were causing the
>     indexing to hang completely.
>
>     I am now running everything again, starting from scratch, and it might
>     work. (I'm doing it wrong, but it is 3/4 done. I will do it right
>     tomorrow, if it works overnight.)
>
>     Jon
>
>     On 09/07/16 16:53, Jonathan Baron wrote:
>
>         Spencer,
>
>         Thanks for the quick reply.
>
>         I am open to someone who knows Perl getting an account on my
>         site and
>         trying to get it working. It will probably involve fixing more
>         than
>         one thing, as mknmz depends on some perl modules that also
>         generate
>         errors.
>
>         My main contribution is figuring out how to extract the html help
>         files and vignettes only, with some help from R developers and
>         Fedora
>         maintainers. Here is the trick, for someone who wants to do it:
>
>         m0 <- rownames(installed.packages())
>         m1 <- m0[which(m0 %in% needed.packages)]
>         source("http://bioconductor.org/biocLite.R
>         <http://bioconductor.org/biocLite.R>")
>         update.packages(oldPkgs=m1,repos=biocinstallRepos())
>         update.packages(dependencies=FALSE,INSTALL_opts=c("--no-configure","--no-test-load","
>         --no-R","--no-clean-on-error","--no-libs","--no-data","--no-demo","--no-exec","--html
>         "),repos=biocinstallRepos(),ask=F)
>         m3 <- new.packages()
>         install.packages(m3,dependencies=FALSE,INSTALL_opts=c("--no-configure","--no-test-loa
>         d","--no-R","--no-clean-on-error","--no-libs","--no-data","--no-demo","--no-exec","--
>         html"),repos=biocinstallRepos())
>
>         Note 1: The first 4 lines are designed to deal with a list of the
>         packages that you actually use. These can be eliminated if you
>         don't
>         use R on the same machine. The last 3 lines are all you need.
>
>         Note 2: This works on Fedora, but I think that the Fedora
>         maintainers
>         of R have set some defaults that are helpful.
>
>         Jon
>
>         On 09/07/16 15:41, Spencer Graves wrote:
>
>             Hello, All:
>
>
>                   Jonathan Baron is "giving up" maintaining the
>             RSiteSearch database.
>
>
>                   This breaks three things:  (1) The R Site Search web
>             service that Baron has maintained.  (2) The RSiteSearch
>             function in the utils package.  (3) The sos package, for
>             which I'm the maintainer and lead author.
>
>
>                   Might someone else be willing to take these over?
>
>
>                   For me, the "findFn" capability with
>             "writeFindFn2xls" is the fastest literature search for
>             anything statistical.  However, I don't have the resources
>             to take over the management of Baron's R Site Search database.
>
>
>                   He's provided a great service for the R community
>             for many years.  I hope we can find a way to keep the
>             system maintained. Failing that, I could use help in
>             adapting the sos package to another database.
>
>
>                   Thanks,
>                   Spencer Graves
>
>
>             -------- Forwarded Message --------
>             Subject:        Re: RSiteSearch, sos, rdocumentation.org
>             <http://rdocumentation.org>, ...?
>             Date:   Wed, 7 Sep 2016 16:15:22 -0400
>             From:   Jonathan Baron <baron at psych.upenn.edu
>             <mailto:baron at psych.upenn.edu>>
>             To:     Spencer Graves <spencer.graves at prodsyse.com
>             <mailto:spencer.graves at prodsyse.com>>
>             CC:     Jonathan Baron <baron at psych.upenn.edu
>             <mailto:baron at psych.upenn.edu>>, chris.is.fun at gmail.com
>             <mailto:chris.is.fun at gmail.com>, info at datacamp.com
>             <mailto:info at datacamp.com> <info at datacamp.com
>             <mailto:info at datacamp.com>>, Sundar Dorai-Raj
>             <sdorairaj at gmail.com <mailto:sdorairaj at gmail.com>>,
>             webmaster at www.r-project-org
>
>
>
>             R site search has stopped working. The indexing scrip,
>             mknmz, failed
>             to complete. It has been producing more and more errors
>             and warnings,
>             since it has not been updated for 5 yeaers.
>
>             I am giving up on this site. I have too many other things
>             to do aside
>             from find bugs in programs written in languages I don't
>             know (Perl),
>             or set up an alternative search engine.
>
>             Please inform anyone else who needs to be informed.
>
>             I cannot find the email of the www.r-project.org
>             <http://www.r-project.org> webmaster, so I'm
>             taking a stab. There are several links to this site in
>             those pages.
>
>             Jon
>             -- 
>             Jonathan Baron, Professor of Psychology, University of
>             Pennsylvania
>             Home page: http://www.sas.upenn.edu/~baron
>             <http://www.sas.upenn.edu/%7Ebaron>
>             Editor: Judgment and Decision Making (http://journal.sjdm.org)
>
>
>         -- 
>         Jonathan Baron, Professor of Psychology, University of
>         Pennsylvania
>         Home page: http://www.sas.upenn.edu/~baron
>         <http://www.sas.upenn.edu/%7Ebaron>
>         Editor: Judgment and Decision Making (http://journal.sjdm.org)
>
>
>     -- 
>     Jonathan Baron, Professor of Psychology, University of Pennsylvania
>     Home page: http://www.sas.upenn.edu/~baron
>     <http://www.sas.upenn.edu/%7Ebaron>
>     Editor: Judgment and Decision Making (http://journal.sjdm.org)
>
>     ______________________________________________
>     R-devel at r-project.org <mailto:R-devel at r-project.org> mailing list
>     https://stat.ethz.ch/mailman/listinfo/r-devel
>     <https://stat.ethz.ch/mailman/listinfo/r-devel>
>


	[[alternative HTML version deleted]]


From spencer.graves at prodsyse.com  Thu Sep  8 15:32:16 2016
From: spencer.graves at prodsyse.com (Spencer Graves)
Date: Thu, 8 Sep 2016 08:32:16 -0500
Subject: [Rd] Fwd: Re: RSiteSearch, sos, rdocumentation.org, ...?
In-Reply-To: <20160908100100.GA30732@psych.upenn.edu>
References: <20160907201522.GA12554@psych.upenn.edu>
	<20518b79-b272-b848-a736-a9acf225626b@prodsyse.com>
	<20160907205321.GB24417@psych.upenn.edu>
	<20160908020657.GA27642@psych.upenn.edu>
	<20160908100100.GA30732@psych.upenn.edu>
Message-ID: <b7e7da04-0154-3dcb-fda9-be1d0dc13929@prodsyse.com>



On 9/8/2016 5:01 AM, Jonathan Baron wrote:
> OK.  It is sort of fixed and sort of works.
>
> We'll keep it for now, but this is not going to work forever. When
> namazu fails completely I will not have the time to install a new
> search engine.
>
> One option is to use google. For a site like this, I think they will
> want some money, but I'm not sure, and I do not have the time to deal
> with it.
>
> We have over 10,000 packages now. I wonder if searching all help files
> is really helpful anymore.


       The fastest way I know to do a literature search for anything 
statistical uses the sos package as follows:


             1.  docPages <- findFn('search string') or findFn('{search 
string}')


             2.  installPackages(docPages) # this installs packages to 
enable a more complete package summary


             3.  writeFindFn2xls(docPages) # this creates an Excel file 
with 3 sheets:  a package summary, the findFn table, and the call.


             4.  Then I open the Excel file, and review the package 
summary sheet.  I prioritize my search from there based on the number 
and strength of matches, how close it sounds to what I want, the date of 
the last update, whether it has a vignette, and the authors and 
maintainers.


       There may be a better way to do this using Google or something 
else.  I'd be pleased if someone else could enlighten me.  I admit to 
being biased:  I'm the lead author and maintainer of "sos". However, I 
don't want to perpetuate a tool that has outlived its usefulness, and 
I'm too blind to see that!


       Spencer

>
> Jon
>
> On 09/07/16 22:06, Jonathan Baron wrote:
>> Don't do anything yet. I may have found the problem by accident.
>>
>> I tried to use the computer from something else, and it was being
>> drastically slowed down by some leftover processes, which turned out
>> to be xlhtml. That is something that converts Excel files. Apparently,
>> some excel files got into the libraries, and they were causing the
>> indexing to hang completely.
>>
>> I am now running everything again, starting from scratch, and it might
>> work. (I'm doing it wrong, but it is 3/4 done. I will do it right
>> tomorrow, if it works overnight.)
>>
>> Jon
>>
>> On 09/07/16 16:53, Jonathan Baron wrote:
>>> Spencer,
>>>
>>> Thanks for the quick reply.
>>>
>>> I am open to someone who knows Perl getting an account on my site and
>>> trying to get it working. It will probably involve fixing more than
>>> one thing, as mknmz depends on some perl modules that also generate
>>> errors.
>>>
>>> My main contribution is figuring out how to extract the html help
>>> files and vignettes only, with some help from R developers and Fedora
>>> maintainers. Here is the trick, for someone who wants to do it:
>>>
>>> m0 <- rownames(installed.packages())
>>> m1 <- m0[which(m0 %in% needed.packages)]
>>> source("http://bioconductor.org/biocLite.R")
>>> update.packages(oldPkgs=m1,repos=biocinstallRepos())
>>> update.packages(dependencies=FALSE,INSTALL_opts=c("--no-configure","--no-test-load", 
>>>
>> "
>>> --no-R","--no-clean-on-error","--no-libs","--no-data","--no-demo","--no-exec","--htm 
>>>
>> l
>>> "),repos=biocinstallRepos(),ask=F)
>>> m3 <- new.packages()
>>> install.packages(m3,dependencies=FALSE,INSTALL_opts=c("--no-configure","--no-test-lo 
>>>
>> a
>>> d","--no-R","--no-clean-on-error","--no-libs","--no-data","--no-demo","--no-exec","- 
>>>
>> -
>>> html"),repos=biocinstallRepos())
>>>
>>> Note 1: The first 4 lines are designed to deal with a list of the
>>> packages that you actually use. These can be eliminated if you don't
>>> use R on the same machine. The last 3 lines are all you need.
>>>
>>> Note 2: This works on Fedora, but I think that the Fedora maintainers
>>> of R have set some defaults that are helpful.
>>>
>>> Jon
>>>
>>> On 09/07/16 15:41, Spencer Graves wrote:
>>>> Hello, All:
>>>>
>>>>
>>>>       Jonathan Baron is "giving up" maintaining the RSiteSearch 
>>>> database.
>>>>
>>>>
>>>>       This breaks three things:  (1) The R Site Search web service 
>>>> that Baron has maintained.  (2) The RSiteSearch function in the 
>>>> utils package.  (3) The sos package, for which I'm the maintainer 
>>>> and lead author.
>>>>
>>>>
>>>>       Might someone else be willing to take these over?
>>>>
>>>>
>>>>       For me, the "findFn" capability with "writeFindFn2xls" is the 
>>>> fastest literature search for anything statistical. However, I 
>>>> don't have the resources to take over the management of Baron's R 
>>>> Site Search database.
>>>>
>>>>
>>>>       He's provided a great service for the R community for many 
>>>> years.  I hope we can find a way to keep the system maintained. 
>>>> Failing that, I could use help in adapting the sos package to 
>>>> another database.
>>>>
>>>>
>>>>       Thanks,
>>>>       Spencer Graves
>>>>
>>>>
>>>> -------- Forwarded Message --------
>>>> Subject:     Re: RSiteSearch, sos, rdocumentation.org, ...?
>>>> Date:     Wed, 7 Sep 2016 16:15:22 -0400
>>>> From:     Jonathan Baron <baron at psych.upenn.edu>
>>>> To:     Spencer Graves <spencer.graves at prodsyse.com>
>>>> CC:     Jonathan Baron <baron at psych.upenn.edu>, 
>>>> chris.is.fun at gmail.com, info at datacamp.com <info at datacamp.com>, 
>>>> Sundar Dorai-Raj <sdorairaj at gmail.com>, webmaster at www.r-project-org
>>>>
>>>>
>>>>
>>>> R site search has stopped working. The indexing scrip, mknmz, failed
>>>> to complete. It has been producing more and more errors and warnings,
>>>> since it has not been updated for 5 yeaers.
>>>>
>>>> I am giving up on this site. I have too many other things to do aside
>>>> from find bugs in programs written in languages I don't know (Perl),
>>>> or set up an alternative search engine.
>>>>
>>>> Please inform anyone else who needs to be informed.
>>>>
>>>> I cannot find the email of the www.r-project.org webmaster, so I'm
>>>> taking a stab. There are several links to this site in those pages.
>>>>
>>>> Jon
>>>> -- 
>>>> Jonathan Baron, Professor of Psychology, University of Pennsylvania
>>>> Home page: http://www.sas.upenn.edu/~baron
>>>> Editor: Judgment and Decision Making (http://journal.sjdm.org)
>>>>
>>>
>>> -- 
>>> Jonathan Baron, Professor of Psychology, University of Pennsylvania
>>> Home page: http://www.sas.upenn.edu/~baron
>>> Editor: Judgment and Decision Making (http://journal.sjdm.org)
>>
>> -- 
>> Jonathan Baron, Professor of Psychology, University of Pennsylvania
>> Home page: http://www.sas.upenn.edu/~baron
>> Editor: Judgment and Decision Making (http://journal.sjdm.org)
>


From baron at psych.upenn.edu  Thu Sep  8 16:18:46 2016
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Thu, 8 Sep 2016 10:18:46 -0400
Subject: [Rd] Fwd: Re: RSiteSearch, sos, rdocumentation.org, ...?
In-Reply-To: <b7e7da04-0154-3dcb-fda9-be1d0dc13929@prodsyse.com>
	<22481.19776.517445.200878@max.nulle.part>
References: <20160907205321.GB24417@psych.upenn.edu>
	<20160908020657.GA27642@psych.upenn.edu>
	<20160908100100.GA30732@psych.upenn.edu>
	<b7e7da04-0154-3dcb-fda9-be1d0dc13929@prodsyse.com>
	<20160907201522.GA12554@psych.upenn.edu>
	<20518b79-b272-b848-a736-a9acf225626b@prodsyse.com>
	<20160907205321.GB24417@psych.upenn.edu>
	<20160908020657.GA27642@psych.upenn.edu>
	<20160908100100.GA30732@psych.upenn.edu>
	<22481.19776.517445.200878@max.nulle.part>
Message-ID: <20160908141846.GA6944@psych.upenn.edu>

I looked at rdocumentation.org. At first I thought it was a superior
replacement for namazu, but after I tried a few things I decided that
it wasn't. I could not find any documentation about how to search, and
the various things I tried seemed to yield very strange responses,
e.g., a search for "Hayes mediation bootstrap" gave me mostly
functions that had nothing to do with the search except for the word
"bootstrap".

So I managed to fix the major Perl module errors (one of which was
quite bothersome although not fatal ... yet). And I figured out a new
way to create the indices that namazu uses; the new way is more
selective. And things seem to work now. Aside from the problems I just
fixed, this is not hard to maintain, so I will continue.

It also seems that someone IS sort of maintaining namazu,
sporadically. There is a Fedora rpm for it. That was how I found out
how to fix the Perl module.

But I did end up spending a few hours on this on a day when I am
behind writing action letters, etc. etc. And ultimately I cannot do
this forever and would love it if someone else took it over, or at
least helped, with an account on my server.

Jon

On 09/08/16 06:36, Dirk Eddelbuettel wrote:
>
>On 8 September 2016 at 06:01, Jonathan Baron wrote:
>| We have over 10,000 packages now. I wonder if searching all help files
>| is really helpful anymore.
>
>Yes it is. I go to http://rdocumentation.org a lot for quick look-ups.
>
>So thanks to Datacamp for running that.  
>
>Dirk
>
>-- 
>http://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron
Editor: Judgment and Decision Making (http://journal.sjdm.org)


From baron at psych.upenn.edu  Thu Sep  8 16:31:23 2016
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Thu, 8 Sep 2016 10:31:23 -0400
Subject: [Rd] Fwd: Re: RSiteSearch, sos, rdocumentation.org, ...?
In-Reply-To: <CAESjWns3e8_1tHjDSkxGwkoTUfo36r6fv4HzeVj7VXuMH63NAw@mail.gmail.com>
References: <20160907201522.GA12554@psych.upenn.edu>
	<20518b79-b272-b848-a736-a9acf225626b@prodsyse.com>
	<20160907205321.GB24417@psych.upenn.edu>
	<20160908020657.GA27642@psych.upenn.edu>
	<20160908100100.GA30732@psych.upenn.edu>
	<b7e7da04-0154-3dcb-fda9-be1d0dc13929@prodsyse.com>
	<CAESjWns3e8_1tHjDSkxGwkoTUfo36r6fv4HzeVj7VXuMH63NAw@mail.gmail.com>
Message-ID: <20160908143123.GA19099@psych.upenn.edu>

On 09/08/16 07:09, John Merrill wrote:
>Given Google's commitment to R, I don't think that they'd be at all averse
>to supporting a custom search box on the package page. It might well be a
>good thing for "someone" to examine the API for setting up such a page and
>to investigate how to mark the main CRAN page as searchable.

The main CRAN page is not ideal. We need to be able to search the help
files. My site has only the html help files for each package (except
the ones I use, which are fully installed), so someone should
re-create that. The CRAN page has a "Reference manual" in pdf for
every package, but the individual functions are not separated.

But, yes, Google would work, even for my page. And the sos package
would have to be modified for that. As I said, I'm not going to do
this. But I would welcome it.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron
Editor: Judgment and Decision Making (http://journal.sjdm.org)


From edd at debian.org  Thu Sep  8 16:50:58 2016
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 8 Sep 2016 09:50:58 -0500
Subject: [Rd] Fwd: Re: RSiteSearch, sos, rdocumentation.org, ...?
In-Reply-To: <20160908141846.GA6944@psych.upenn.edu>
References: <20160907205321.GB24417@psych.upenn.edu>
	<20160908020657.GA27642@psych.upenn.edu>
	<20160908100100.GA30732@psych.upenn.edu>
	<b7e7da04-0154-3dcb-fda9-be1d0dc13929@prodsyse.com>
	<20160907201522.GA12554@psych.upenn.edu>
	<20518b79-b272-b848-a736-a9acf225626b@prodsyse.com>
	<22481.19776.517445.200878@max.nulle.part>
	<20160908141846.GA6944@psych.upenn.edu>
Message-ID: <22481.31442.251636.513174@max.nulle.part>


Jonathan,

FWIW I mentored a Google Summer of Code student (who was more than highly
self-sufficient and needed next to no help, apart from some small R packaging
tricks) as part of the Xapian project in order to write RXapian:

   https://github.com/amandaJayanetti/RXapian

which is an R interface to the Xapian index engine.

I don't know much about these indice generators, but Xapian [1] appears to be
free, open-source, current, maintained, powerful, and used.  From what I
gather you are still betting on an older (and as I seem to recall,
deprecated) technology. There may be more teers ahead.

The other tip would be to get in touch with Gabor who as part of r-hub has
indices for just about anything, and 9as he his a generation younger than
Spencer, you or me) also provides current (ie JSON over REST) interfaces.

Dirk

[1] https://xapian.org/

-- 
http://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From kevin.r.coombes at gmail.com  Thu Sep  8 16:51:22 2016
From: kevin.r.coombes at gmail.com (Kevin Coombes)
Date: Thu, 8 Sep 2016 10:51:22 -0400
Subject: [Rd] Fwd: Re: RSiteSearch, sos, rdocumentation.org, ...?
In-Reply-To: <20160908143123.GA19099@psych.upenn.edu>
References: <20160907201522.GA12554@psych.upenn.edu>
	<20518b79-b272-b848-a736-a9acf225626b@prodsyse.com>
	<20160907205321.GB24417@psych.upenn.edu>
	<20160908020657.GA27642@psych.upenn.edu>
	<20160908100100.GA30732@psych.upenn.edu>
	<b7e7da04-0154-3dcb-fda9-be1d0dc13929@prodsyse.com>
	<CAESjWns3e8_1tHjDSkxGwkoTUfo36r6fv4HzeVj7VXuMH63NAw@mail.gmail.com>
	<20160908143123.GA19099@psych.upenn.edu>
Message-ID: <bdf1c5eb-17f7-d2df-8171-7899679c5d57@gmail.com>

Would it make sense to recreate the "searchable R help pages" by feeding 
them all into elasticsearch, which will automatically index them and 
also provides an extensive (HTTP+JSON-based) API to perform complex 
searches?

On 9/8/2016 10:31 AM, Jonathan Baron wrote:
> On 09/08/16 07:09, John Merrill wrote:
>> Given Google's commitment to R, I don't think that they'd be at all 
>> averse
>> to supporting a custom search box on the package page. It might well 
>> be a
>> good thing for "someone" to examine the API for setting up such a 
>> page and
>> to investigate how to mark the main CRAN page as searchable.
>
> The main CRAN page is not ideal. We need to be able to search the help
> files. My site has only the html help files for each package (except
> the ones I use, which are fully installed), so someone should
> re-create that. The CRAN page has a "Reference manual" in pdf for
> every package, but the individual functions are not separated.
>
> But, yes, Google would work, even for my page. And the sos package
> would have to be modified for that. As I said, I'm not going to do
> this. But I would welcome it.
>
> Jon


---
This email has been checked for viruses by Avast antivirus software.
https://www.avast.com/antivirus


From john.merrill at gmail.com  Thu Sep  8 17:05:04 2016
From: john.merrill at gmail.com (John Merrill)
Date: Thu, 8 Sep 2016 08:05:04 -0700
Subject: [Rd] Fwd: Re: RSiteSearch, sos, rdocumentation.org, ...?
In-Reply-To: <bdf1c5eb-17f7-d2df-8171-7899679c5d57@gmail.com>
References: <20160907201522.GA12554@psych.upenn.edu>
	<20518b79-b272-b848-a736-a9acf225626b@prodsyse.com>
	<20160907205321.GB24417@psych.upenn.edu>
	<20160908020657.GA27642@psych.upenn.edu>
	<20160908100100.GA30732@psych.upenn.edu>
	<b7e7da04-0154-3dcb-fda9-be1d0dc13929@prodsyse.com>
	<CAESjWns3e8_1tHjDSkxGwkoTUfo36r6fv4HzeVj7VXuMH63NAw@mail.gmail.com>
	<20160908143123.GA19099@psych.upenn.edu>
	<bdf1c5eb-17f7-d2df-8171-7899679c5d57@gmail.com>
Message-ID: <CAESjWnumJV6nQ0ZujsgK2V7QoiVQ1=6XxmmEdiJ4azZZwW9-uQ@mail.gmail.com>

That would work, although it would entail standing up a server to front the
elasticsearch module.  The strikes me a huge investment of time which
would, in addition, recreate the current key man risk.

On Thu, Sep 8, 2016 at 7:51 AM, Kevin Coombes <kevin.r.coombes at gmail.com>
wrote:

> Would it make sense to recreate the "searchable R help pages" by feeding
> them all into elasticsearch, which will automatically index them and also
> provides an extensive (HTTP+JSON-based) API to perform complex searches?
>
> On 9/8/2016 10:31 AM, Jonathan Baron wrote:
>
>> On 09/08/16 07:09, John Merrill wrote:
>>
>>> Given Google's commitment to R, I don't think that they'd be at all
>>> averse
>>> to supporting a custom search box on the package page. It might well be a
>>> good thing for "someone" to examine the API for setting up such a page
>>> and
>>> to investigate how to mark the main CRAN page as searchable.
>>>
>>
>> The main CRAN page is not ideal. We need to be able to search the help
>> files. My site has only the html help files for each package (except
>> the ones I use, which are fully installed), so someone should
>> re-create that. The CRAN page has a "Reference manual" in pdf for
>> every package, but the individual functions are not separated.
>>
>> But, yes, Google would work, even for my page. And the sos package
>> would have to be modified for that. As I said, I'm not going to do
>> this. But I would welcome it.
>>
>> Jon
>>
>
>
> ---
> This email has been checked for viruses by Avast antivirus software.
> https://www.avast.com/antivirus
>
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From gmbecker at ucdavis.edu  Thu Sep  8 17:43:09 2016
From: gmbecker at ucdavis.edu (Gabriel Becker)
Date: Thu, 8 Sep 2016 08:43:09 -0700
Subject: [Rd] R (development) changes in arith, logic,
 relop with (0-extent) arrays
In-Reply-To: <22481.16979.861309.264630@stat.math.ethz.ch>
References: <22469.52585.935914.336374@aragorn.wu.ac.at>
	<22470.37902.198447.925212@stat.math.ethz.ch>
	<22470.38100.993865.828495@aragorn.wu.ac.at>
	<22473.36759.791968.814130@stat.math.ethz.ch>
	<22473.38325.834026.658339@stat.math.ethz.ch>
	<22474.35836.489860.992765@aragorn.wu.ac.at>
	<CAPRP4-eKdb1AKGt73P-+khNXFAWm_4q2PZr69Q7BAjWcXKpSzA@mail.gmail.com>
	<CAPRP4-e4fL=j2xUVMjO-kdmeTfvF=+=9AKrU9+981Xvhwc2MPA@mail.gmail.com>
	<22477.32999.558832.601073@stat.math.ethz.ch>
	<22479.9847.747220.924128@stat.math.ethz.ch>
	<22479.58007.358121.626142@stat.math.ethz.ch>
	<CAHHjBM6yvrgwOFiFAtM+w8BGmdwn114Z6gfyjMEsLQDLV8mA5A@mail.gmail.com>
	<22481.16979.861309.264630@stat.math.ethz.ch>
Message-ID: <CADwqtCN2PnTFwsrV-K3=hXt64r5eHYNt5dg0OZZdEcDtPOtJuQ@mail.gmail.com>

Martin,

Like Robin and Oliver I think this type of edge-case consistency is
important and that it's fantastic that R-core - and you personally - are
willing to tackle some of these "gotcha" behaviors. "Little" stuff like
this really does combine to go a long way to making R better and better.

I do wonder a  bit about the

x = 1:2

y = NULL

x < y

case.

Returning a logical of length 0 is more backwards compatible, but is it
ever what the author actually intended? I have trouble thinking of a case
where that less-than didn't carry an implicit assumption that y was
non-NULL.  I can say that in my own code, I've never hit that behavior in a
case that wasn't an error.

My vote (unless someone else points out a compelling use for the behavior)
is for the to throw an error. As a developer, I'd rather things like this
break so the bug in my logic is visible, rather than  propagating as the
0-length logical is &'ed or |'ed with other logical vectors, or used to
subset, or (in the case it should be length 1) passed to if() (if throws an
error now, but the rest would silently "work").

Best,
~G

On Thu, Sep 8, 2016 at 3:49 AM, Martin Maechler <maechler at stat.math.ethz.ch>
wrote:

> >>>>> robin hankin <hankin.robin at gmail.com>
> >>>>>     on Thu, 8 Sep 2016 10:05:21 +1200 writes:
>
>     > Martin I'd like to make a comment; I think that R's
>     > behaviour on 'edge' cases like this is an important thing
>     > and it's great that you are working on it.
>
>     > I make heavy use of zero-extent arrays, chiefly because
>     > the dimnames are an efficient and logical way to keep
>     > track of certain types of information.
>
>     > If I have, for example,
>
>     > a <- array(0,c(2,0,2))
>     > dimnames(a) <- list(name=c('Mike','Kevin'),
> NULL,item=c("hat","scarf"))
>
>
>     > Then in R-3.3.1, 70800 I get
>
>     a> 0
>     > logical(0)
>     >>
>
>     > But in 71219 I get
>
>     a> 0
>     > , , item = hat
>
>
>     > name
>     > Mike
>     > Kevin
>
>     > , , item = scarf
>
>
>     > name
>     > Mike
>     > Kevin
>
>     > (which is an empty logical array that holds the names of the people
> and
>     > their clothes). I find the behaviour of 71219 very much preferable
> because
>     > there is no reason to discard the information in the dimnames.
>
> Thanks a lot, Robin, (and Oliver) !
>
> Yes, the above is such a case where the new behavior makes much sense.
> And this behavior remains identical after the 71222 amendment.
>
> Martin
>
>     > Best wishes
>     > Robin
>
>
>
>
>     > On Wed, Sep 7, 2016 at 9:49 PM, Martin Maechler <
> maechler at stat.math.ethz.ch>
>     > wrote:
>
>     >> >>>>> Martin Maechler <maechler at stat.math.ethz.ch>
>     >> >>>>>     on Tue, 6 Sep 2016 22:26:31 +0200 writes:
>     >>
>     >> > Yesterday, changes to R's development version were committed,
>     >> relating
>     >> > to arithmetic, logic ('&' and '|') and
>     >> > comparison/relational ('<', '==') binary operators
>     >> > which in NEWS are described as
>     >>
>     >> > SIGNIFICANT USER-VISIBLE CHANGES:
>     >>
>     >> > [.............]
>     >>
>     >> > ? Arithmetic, logic (?&?, ?|?) and comparison (aka
>     >> > ?relational?, e.g., ?<?, ?==?) operations with arrays now
>     >> > behave consistently, notably for arrays of length zero.
>     >>
>     >> > Arithmetic between length-1 arrays and longer non-arrays had
>     >> > silently dropped the array attributes and recycled.  This
>     >> > now gives a warning and will signal an error in the future,
>     >> > as it has always for logic and comparison operations in
>     >> > these cases (e.g., compare ?matrix(1,1) + 2:3? and
>     >> > ?matrix(1,1) < 2:3?).
>     >>
>     >> > As the above "visually suggests" one could think of the changes
>     >> > falling mainly two groups,
>     >> > 1) <0-extent array>  (op)     <non-array>
>     >> > 2) <1-extent array>  (arith)  <non-array of length != 1>
>     >>
>     >> > These changes are partly non-back compatible and may break
>     >> > existing code.  We believe that the internal consistency gained
>     >> > from the changes is worth the few places with problems.
>     >>
>     >> > We expect some package maintainers (10-20, or even more?) need
>     >> > to adapt their code.
>     >>
>     >> > Case '2)' above mainly results in a new warning, e.g.,
>     >>
>     >> >> matrix(1,1) + 1:2
>     >> > [1] 2 3
>     >> > Warning message:
>     >> > In matrix(1, 1) + 1:2 :
>     >> > dropping dim() of array of length one.  Will become ERROR
>     >> >>
>     >>
>     >> > whereas '1)' gives errors in cases the result silently was a
>     >> > vector of length zero, or also keeps array (dim & dimnames) in
>     >> > cases these were silently dropped.
>     >>
>     >> > The following is a "heavily" commented  R script showing (all ?)
>     >> > the important cases with changes :
>     >>
>     >> > ------------------------------------------------------------
>     >> ----------------
>     >>
>     >> > (m <- cbind(a=1[0], b=2[0]))
>     >> > Lm <- m; storage.mode(Lm) <- "logical"
>     >> > Im <- m; storage.mode(Im) <- "integer"
>     >>
>     >> > ## 1. -------------------------
>     >> > try( m & NULL ) # in R <= 3.3.x :
>     >> > ## Error in m & NULL :
>     >> > ##  operations are possible only for numeric, logical or complex
>     >> types
>     >> > ##
>     >> > ## gives 'Lm' in R >= 3.4.0
>     >>
>     >> > ## 2. -------------------------
>     >> > m + 2:3 ## gave numeric(0), now remains matrix identical to  m
>     >> > Im + 2:3 ## gave integer(0), now remains matrix identical to Im
>     >> (integer)
>     >>
>     >> > m > 1      ## gave logical(0), now remains matrix identical to Lm
>     >> (logical)
>     >> > m > 0.1[0] ##  ditto
>     >> > m > NULL   ##  ditto
>     >>
>     >> > ## 3. -------------------------
>     >> > mm <- m[,c(1:2,2:1,2)]
>     >> > try( m == mm ) ## now gives error   "non-conformable arrays",
>     >> > ## but gave logical(0) in R <= 3.3.x
>     >>
>     >> > ## 4. -------------------------
>     >> > str( Im + NULL)  ## gave "num", now gives "int"
>     >>
>     >> > ## 5. -------------------------
>     >> > ## special case for arithmetic w/ length-1 array
>     >> > (m1 <- matrix(1,1,1, dimnames=list("Ro","col")))
>     >> > (m2 <- matrix(1,2,1, dimnames=list(c("A","B"),"col")))
>     >>
>     >> > m1 + 1:2  # ->  2:3  but now with warning to  "become ERROR"
>     >> > tools::assertError(m1 & 1:2)# ERR: dims [product 1] do not match
> the
>     >> length of object [2]
>     >> > tools::assertError(m1 < 1:2)# ERR:                  (ditto)
>     >> > ##
>     >> > ## non-0-length arrays combined with {NULL or double() or ...}
> *fail*
>     >>
>     >> > ### Length-1 arrays:  Arithmetic with |vectors| > 1  treated array
>     >> as scalar
>     >> > m1 + NULL # gave  numeric(0) in R <= 3.3.x --- still, *but* w/
>     >> warning to "be ERROR"
>     >> > try(m1 > NULL)    # gave  logical(0) in R <= 3.3.x --- an *error*
>     >> now in R >= 3.4.0
>     >> > tools::assertError(m1 & NULL)    # gave and gives error
>     >> > tools::assertError(m1 | double())# ditto
>     >> > ## m2 was slightly different:
>     >> > tools::assertError(m2 + NULL)
>     >> > tools::assertError(m2 & NULL)
>     >> > try(m2 == NULL) ## was logical(0) in R <= 3.3.x; now error as
> above!
>     >>
>     >> > ------------------------------------------------------------
>     >> ----------------
>     >>
>     >>
>     >> > Note that in R's own  'nls'  sources, there was one case of
>     >> > situation '2)' above, i.e. a  1x1-matrix was used as a "scalar".
>     >>
>     >> > In such cases, you should explicitly coerce it to a vector,
>     >> > either ("self-explainingly") by  as.vector(.), or as I did in
>     >> > the nls case  by  c(.) :  The latter is much less
>     >> > self-explaining, but nicer to read in mathematical formulae, and
>     >> > currently also more efficient because it is a .Primitive.
>     >>
>     >> > Please use R-devel with your code, and let us know if you see
>     >> > effects that seem adverse.
>     >>
>     >> I've been slightly surprised (or even "frustrated") by the empty
>     >> reaction on our R-devel list to this post.
>     >>
>     >> I would have expected some critique, may be even some praise,
>     >> ... in any case some sign people are "thinking along" (as we say
>     >> in German).
>     >>
>     >> In the mean time, I've actually thought along the one case which
>     >> is last above:  The <op>  (binary operation) between a
>     >> non-0-length array and a 0-length vector (and NULL which should
>     >> be treated like a 0-length vector):
>     >>
>     >> R <= 3.3.1  *is* quite inconsistent with these:
>     >>
>     >>
>     >> and my proposal above (implemented in R-devel, since Sep.5) would
> give an
>     >> error for all these, but instead, R really could be more lenient
> here:
>     >> A 0-length result is ok, and it should *not* inherit the array
>     >> (dim, dimnames), since the array is not of length 0. So instead
>     >> of the above [for the very last part only!!], we would aim for
>     >> the following. These *all* give an error in current R-devel,
>     >> with the exception of 'm1 + NULL' which "only" gives a "bad
>     >> warning" :
>     >>
>     >> ------------------------
>     >>
>     >> m1 <- matrix(1,1)
>     >> m2 <- matrix(1,2)
>     >>
>     >> m1 + NULL #    numeric(0) in R <= 3.3.x ---> OK ?!
>     >> m1 > NULL #    logical(0) in R <= 3.3.x ---> OK ?!
>     >> try(m1 & NULL)    # ERROR in R <= 3.3.x ---> change to logical(0)
> ?!
>     >> try(m1 | double())# ERROR in R <= 3.3.x ---> change to logical(0)
> ?!
>     >> ## m2 slightly different:
>     >> try(m2 + NULL)  # ERROR in R <= 3.3.x ---> change to double(0)  ?!
>     >> try(m2 & NULL)  # ERROR in R <= 3.3.x ---> change to logical(0)  ?!
>     >> m2 == NULL # logical(0) in R <= 3.3.x ---> OK ?!
>     >>
>     >> ------------------------
>     >>
>     >> This would be slightly more back-compatible than the currently
>     >> implemented proposal. Everything else I said remains true, and
>     >> I'm pretty sure most changes needed in packages would remain to be
> done.
>     >>
>     >> Opinions ?
>     >>
>     >>
>     >>
>     >> > In some case where R-devel now gives an error but did not
>     >> > previously, we could contemplate giving another  "warning
>     >> > .... 'to become ERROR'" if there was too much breakage,  though
>     >> > I don't expect that.
>     >>
>     >>
>     >> > For the R Core Team,
>     >>
>     >> > Martin Maechler,
>     >> > ETH Zurich
>     >>
>     >> ______________________________________________
>     >> R-devel at r-project.org mailing list
>     >> https://stat.ethz.ch/mailman/listinfo/r-devel
>     >>
>
>
>
>     > --
>     > Robin Hankin
>     > Neutral theorist
>     > hankin.robin at gmail.com
>
>     > [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>



-- 
Gabriel Becker, PhD
Associate Scientist (Bioinformatics)
Genentech Research

	[[alternative HTML version deleted]]


From gmbecker at ucdavis.edu  Thu Sep  8 18:50:22 2016
From: gmbecker at ucdavis.edu (Gabriel Becker)
Date: Thu, 8 Sep 2016 09:50:22 -0700
Subject: [Rd] mget call can trigger C stack usage error
In-Reply-To: <CAERMt4cE8maMs7q9=SB2pDHgV56oikBKphxowjGd8L+gX_V22w@mail.gmail.com>
References: <CAERMt4cE8maMs7q9=SB2pDHgV56oikBKphxowjGd8L+gX_V22w@mail.gmail.com>
Message-ID: <CADwqtCM2g-CfH0cBcS-zNdB4jGZWQrJB-WY5RD5=z8jjyqbXFg@mail.gmail.com>

Alexandre,

AFAICS, this code actually causes infinite recursion, and here's why:


   1. formals grabs returns the formals of the function identified by
   sys.function(sys.parent()) this ends up being print.new, whose first
   argument is x
   2. mget looks for the symbol x in envir = as.environment(-1L) which ends
   up being the evaluation frame for print.new [1]
   3. x in that environment resolves the the object you are trying to print
   4. print() is called on that object, and dispatches to print.new() ...



[1]

> debug(mget)

> foo

*<snip>*

[1] "envir"      "ifnotfound" "inherits"   "mode"       "x"

Browse[2]> *envir*

*<environment: 0x7fa39bf1b278>*

Browse[2]> sys.frames()

[[1]]

<environment: 0x7fa39bf1b550>


*[[2]]*

*<environment: 0x7fa39bf1b278>*


*<snip>*


Browse[2]> sys.calls()

[[1]]

function (x, ...)

UseMethod("print")(x)


*[[2]]*

*print.new(x)*


*<snip>*


Browse[2]> class(envir$x)

[1] *"new"*

Hope that helps.
~G

On Mon, Sep 5, 2016 at 4:48 PM, Alexandre Courtiol <
alexandre.courtiol at gmail.com> wrote:

> Hi all, not sure if you will call this a bug or something else but the
> following silly call trigger a low level error:
>
> foo <- list(x=1)
> class(foo) <- "new"
> print.new <- function(x, ...) print(mget(names(formals())))
> foo
>
> > Error: C stack usage  7969412 is too close to the limit
>
>
>
> --
> Alexandre Courtiol
>
> http://sites.google.com/site/alexandrecourtiol/home
>
> *"Science is the belief in the ignorance of experts"*, R. Feynman
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>



-- 
Gabriel Becker, PhD
Associate Scientist (Bioinformatics)
Genentech Research

	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Thu Sep  8 19:05:33 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Thu, 8 Sep 2016 10:05:33 -0700
Subject: [Rd] R (development) changes in arith, logic,
 relop with (0-extent) arrays
In-Reply-To: <CADwqtCN2PnTFwsrV-K3=hXt64r5eHYNt5dg0OZZdEcDtPOtJuQ@mail.gmail.com>
References: <22469.52585.935914.336374@aragorn.wu.ac.at>
	<22470.37902.198447.925212@stat.math.ethz.ch>
	<22470.38100.993865.828495@aragorn.wu.ac.at>
	<22473.36759.791968.814130@stat.math.ethz.ch>
	<22473.38325.834026.658339@stat.math.ethz.ch>
	<22474.35836.489860.992765@aragorn.wu.ac.at>
	<CAPRP4-eKdb1AKGt73P-+khNXFAWm_4q2PZr69Q7BAjWcXKpSzA@mail.gmail.com>
	<CAPRP4-e4fL=j2xUVMjO-kdmeTfvF=+=9AKrU9+981Xvhwc2MPA@mail.gmail.com>
	<22477.32999.558832.601073@stat.math.ethz.ch>
	<22479.9847.747220.924128@stat.math.ethz.ch>
	<22479.58007.358121.626142@stat.math.ethz.ch>
	<CAHHjBM6yvrgwOFiFAtM+w8BGmdwn114Z6gfyjMEsLQDLV8mA5A@mail.gmail.com>
	<22481.16979.861309.264630@stat.math.ethz.ch>
	<CADwqtCN2PnTFwsrV-K3=hXt64r5eHYNt5dg0OZZdEcDtPOtJuQ@mail.gmail.com>
Message-ID: <CAF8bMcYBMduLcp_aQoTh74MBBKB9QnTDQG9J_dZyTrFkGd0azw@mail.gmail.com>

Shouldn't binary operators (arithmetic and logical) should throw an error
when one operand is NULL (or other type that doesn't make sense)?  This is
a different case than a zero-length operand of a legitimate type.  E.g.,
     any(x < 0)
should return FALSE if x is number-like and length(x)==0 but give an error
if x is NULL.

I.e., I think the type check should be done before the length check.


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Thu, Sep 8, 2016 at 8:43 AM, Gabriel Becker <gmbecker at ucdavis.edu> wrote:

> Martin,
>
> Like Robin and Oliver I think this type of edge-case consistency is
> important and that it's fantastic that R-core - and you personally - are
> willing to tackle some of these "gotcha" behaviors. "Little" stuff like
> this really does combine to go a long way to making R better and better.
>
> I do wonder a  bit about the
>
> x = 1:2
>
> y = NULL
>
> x < y
>
> case.
>
> Returning a logical of length 0 is more backwards compatible, but is it
> ever what the author actually intended? I have trouble thinking of a case
> where that less-than didn't carry an implicit assumption that y was
> non-NULL.  I can say that in my own code, I've never hit that behavior in a
> case that wasn't an error.
>
> My vote (unless someone else points out a compelling use for the behavior)
> is for the to throw an error. As a developer, I'd rather things like this
> break so the bug in my logic is visible, rather than  propagating as the
> 0-length logical is &'ed or |'ed with other logical vectors, or used to
> subset, or (in the case it should be length 1) passed to if() (if throws an
> error now, but the rest would silently "work").
>
> Best,
> ~G
>
> On Thu, Sep 8, 2016 at 3:49 AM, Martin Maechler <
> maechler at stat.math.ethz.ch>
> wrote:
>
> > >>>>> robin hankin <hankin.robin at gmail.com>
> > >>>>>     on Thu, 8 Sep 2016 10:05:21 +1200 writes:
> >
> >     > Martin I'd like to make a comment; I think that R's
> >     > behaviour on 'edge' cases like this is an important thing
> >     > and it's great that you are working on it.
> >
> >     > I make heavy use of zero-extent arrays, chiefly because
> >     > the dimnames are an efficient and logical way to keep
> >     > track of certain types of information.
> >
> >     > If I have, for example,
> >
> >     > a <- array(0,c(2,0,2))
> >     > dimnames(a) <- list(name=c('Mike','Kevin'),
> > NULL,item=c("hat","scarf"))
> >
> >
> >     > Then in R-3.3.1, 70800 I get
> >
> >     a> 0
> >     > logical(0)
> >     >>
> >
> >     > But in 71219 I get
> >
> >     a> 0
> >     > , , item = hat
> >
> >
> >     > name
> >     > Mike
> >     > Kevin
> >
> >     > , , item = scarf
> >
> >
> >     > name
> >     > Mike
> >     > Kevin
> >
> >     > (which is an empty logical array that holds the names of the people
> > and
> >     > their clothes). I find the behaviour of 71219 very much preferable
> > because
> >     > there is no reason to discard the information in the dimnames.
> >
> > Thanks a lot, Robin, (and Oliver) !
> >
> > Yes, the above is such a case where the new behavior makes much sense.
> > And this behavior remains identical after the 71222 amendment.
> >
> > Martin
> >
> >     > Best wishes
> >     > Robin
> >
> >
> >
> >
> >     > On Wed, Sep 7, 2016 at 9:49 PM, Martin Maechler <
> > maechler at stat.math.ethz.ch>
> >     > wrote:
> >
> >     >> >>>>> Martin Maechler <maechler at stat.math.ethz.ch>
> >     >> >>>>>     on Tue, 6 Sep 2016 22:26:31 +0200 writes:
> >     >>
> >     >> > Yesterday, changes to R's development version were committed,
> >     >> relating
> >     >> > to arithmetic, logic ('&' and '|') and
> >     >> > comparison/relational ('<', '==') binary operators
> >     >> > which in NEWS are described as
> >     >>
> >     >> > SIGNIFICANT USER-VISIBLE CHANGES:
> >     >>
> >     >> > [.............]
> >     >>
> >     >> > ? Arithmetic, logic (?&?, ?|?) and comparison (aka
> >     >> > ?relational?, e.g., ?<?, ?==?) operations with arrays now
> >     >> > behave consistently, notably for arrays of length zero.
> >     >>
> >     >> > Arithmetic between length-1 arrays and longer non-arrays had
> >     >> > silently dropped the array attributes and recycled.  This
> >     >> > now gives a warning and will signal an error in the future,
> >     >> > as it has always for logic and comparison operations in
> >     >> > these cases (e.g., compare ?matrix(1,1) + 2:3? and
> >     >> > ?matrix(1,1) < 2:3?).
> >     >>
> >     >> > As the above "visually suggests" one could think of the changes
> >     >> > falling mainly two groups,
> >     >> > 1) <0-extent array>  (op)     <non-array>
> >     >> > 2) <1-extent array>  (arith)  <non-array of length != 1>
> >     >>
> >     >> > These changes are partly non-back compatible and may break
> >     >> > existing code.  We believe that the internal consistency gained
> >     >> > from the changes is worth the few places with problems.
> >     >>
> >     >> > We expect some package maintainers (10-20, or even more?) need
> >     >> > to adapt their code.
> >     >>
> >     >> > Case '2)' above mainly results in a new warning, e.g.,
> >     >>
> >     >> >> matrix(1,1) + 1:2
> >     >> > [1] 2 3
> >     >> > Warning message:
> >     >> > In matrix(1, 1) + 1:2 :
> >     >> > dropping dim() of array of length one.  Will become ERROR
> >     >> >>
> >     >>
> >     >> > whereas '1)' gives errors in cases the result silently was a
> >     >> > vector of length zero, or also keeps array (dim & dimnames) in
> >     >> > cases these were silently dropped.
> >     >>
> >     >> > The following is a "heavily" commented  R script showing (all ?)
> >     >> > the important cases with changes :
> >     >>
> >     >> > ------------------------------------------------------------
> >     >> ----------------
> >     >>
> >     >> > (m <- cbind(a=1[0], b=2[0]))
> >     >> > Lm <- m; storage.mode(Lm) <- "logical"
> >     >> > Im <- m; storage.mode(Im) <- "integer"
> >     >>
> >     >> > ## 1. -------------------------
> >     >> > try( m & NULL ) # in R <= 3.3.x :
> >     >> > ## Error in m & NULL :
> >     >> > ##  operations are possible only for numeric, logical or complex
> >     >> types
> >     >> > ##
> >     >> > ## gives 'Lm' in R >= 3.4.0
> >     >>
> >     >> > ## 2. -------------------------
> >     >> > m + 2:3 ## gave numeric(0), now remains matrix identical to  m
> >     >> > Im + 2:3 ## gave integer(0), now remains matrix identical to Im
> >     >> (integer)
> >     >>
> >     >> > m > 1      ## gave logical(0), now remains matrix identical to
> Lm
> >     >> (logical)
> >     >> > m > 0.1[0] ##  ditto
> >     >> > m > NULL   ##  ditto
> >     >>
> >     >> > ## 3. -------------------------
> >     >> > mm <- m[,c(1:2,2:1,2)]
> >     >> > try( m == mm ) ## now gives error   "non-conformable arrays",
> >     >> > ## but gave logical(0) in R <= 3.3.x
> >     >>
> >     >> > ## 4. -------------------------
> >     >> > str( Im + NULL)  ## gave "num", now gives "int"
> >     >>
> >     >> > ## 5. -------------------------
> >     >> > ## special case for arithmetic w/ length-1 array
> >     >> > (m1 <- matrix(1,1,1, dimnames=list("Ro","col")))
> >     >> > (m2 <- matrix(1,2,1, dimnames=list(c("A","B"),"col")))
> >     >>
> >     >> > m1 + 1:2  # ->  2:3  but now with warning to  "become ERROR"
> >     >> > tools::assertError(m1 & 1:2)# ERR: dims [product 1] do not match
> > the
> >     >> length of object [2]
> >     >> > tools::assertError(m1 < 1:2)# ERR:                  (ditto)
> >     >> > ##
> >     >> > ## non-0-length arrays combined with {NULL or double() or ...}
> > *fail*
> >     >>
> >     >> > ### Length-1 arrays:  Arithmetic with |vectors| > 1  treated
> array
> >     >> as scalar
> >     >> > m1 + NULL # gave  numeric(0) in R <= 3.3.x --- still, *but* w/
> >     >> warning to "be ERROR"
> >     >> > try(m1 > NULL)    # gave  logical(0) in R <= 3.3.x --- an
> *error*
> >     >> now in R >= 3.4.0
> >     >> > tools::assertError(m1 & NULL)    # gave and gives error
> >     >> > tools::assertError(m1 | double())# ditto
> >     >> > ## m2 was slightly different:
> >     >> > tools::assertError(m2 + NULL)
> >     >> > tools::assertError(m2 & NULL)
> >     >> > try(m2 == NULL) ## was logical(0) in R <= 3.3.x; now error as
> > above!
> >     >>
> >     >> > ------------------------------------------------------------
> >     >> ----------------
> >     >>
> >     >>
> >     >> > Note that in R's own  'nls'  sources, there was one case of
> >     >> > situation '2)' above, i.e. a  1x1-matrix was used as a "scalar".
> >     >>
> >     >> > In such cases, you should explicitly coerce it to a vector,
> >     >> > either ("self-explainingly") by  as.vector(.), or as I did in
> >     >> > the nls case  by  c(.) :  The latter is much less
> >     >> > self-explaining, but nicer to read in mathematical formulae, and
> >     >> > currently also more efficient because it is a .Primitive.
> >     >>
> >     >> > Please use R-devel with your code, and let us know if you see
> >     >> > effects that seem adverse.
> >     >>
> >     >> I've been slightly surprised (or even "frustrated") by the empty
> >     >> reaction on our R-devel list to this post.
> >     >>
> >     >> I would have expected some critique, may be even some praise,
> >     >> ... in any case some sign people are "thinking along" (as we say
> >     >> in German).
> >     >>
> >     >> In the mean time, I've actually thought along the one case which
> >     >> is last above:  The <op>  (binary operation) between a
> >     >> non-0-length array and a 0-length vector (and NULL which should
> >     >> be treated like a 0-length vector):
> >     >>
> >     >> R <= 3.3.1  *is* quite inconsistent with these:
> >     >>
> >     >>
> >     >> and my proposal above (implemented in R-devel, since Sep.5) would
> > give an
> >     >> error for all these, but instead, R really could be more lenient
> > here:
> >     >> A 0-length result is ok, and it should *not* inherit the array
> >     >> (dim, dimnames), since the array is not of length 0. So instead
> >     >> of the above [for the very last part only!!], we would aim for
> >     >> the following. These *all* give an error in current R-devel,
> >     >> with the exception of 'm1 + NULL' which "only" gives a "bad
> >     >> warning" :
> >     >>
> >     >> ------------------------
> >     >>
> >     >> m1 <- matrix(1,1)
> >     >> m2 <- matrix(1,2)
> >     >>
> >     >> m1 + NULL #    numeric(0) in R <= 3.3.x ---> OK ?!
> >     >> m1 > NULL #    logical(0) in R <= 3.3.x ---> OK ?!
> >     >> try(m1 & NULL)    # ERROR in R <= 3.3.x ---> change to logical(0)
> > ?!
> >     >> try(m1 | double())# ERROR in R <= 3.3.x ---> change to logical(0)
> > ?!
> >     >> ## m2 slightly different:
> >     >> try(m2 + NULL)  # ERROR in R <= 3.3.x ---> change to double(0)  ?!
> >     >> try(m2 & NULL)  # ERROR in R <= 3.3.x ---> change to logical(0)
> ?!
> >     >> m2 == NULL # logical(0) in R <= 3.3.x ---> OK ?!
> >     >>
> >     >> ------------------------
> >     >>
> >     >> This would be slightly more back-compatible than the currently
> >     >> implemented proposal. Everything else I said remains true, and
> >     >> I'm pretty sure most changes needed in packages would remain to be
> > done.
> >     >>
> >     >> Opinions ?
> >     >>
> >     >>
> >     >>
> >     >> > In some case where R-devel now gives an error but did not
> >     >> > previously, we could contemplate giving another  "warning
> >     >> > .... 'to become ERROR'" if there was too much breakage,  though
> >     >> > I don't expect that.
> >     >>
> >     >>
> >     >> > For the R Core Team,
> >     >>
> >     >> > Martin Maechler,
> >     >> > ETH Zurich
> >     >>
> >     >> ______________________________________________
> >     >> R-devel at r-project.org mailing list
> >     >> https://stat.ethz.ch/mailman/listinfo/r-devel
> >     >>
> >
> >
> >
> >     > --
> >     > Robin Hankin
> >     > Neutral theorist
> >     > hankin.robin at gmail.com
> >
> >     > [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
> >
>
>
>
> --
> Gabriel Becker, PhD
> Associate Scientist (Bioinformatics)
> Genentech Research
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From gmbecker at ucdavis.edu  Thu Sep  8 19:22:17 2016
From: gmbecker at ucdavis.edu (Gabriel Becker)
Date: Thu, 8 Sep 2016 10:22:17 -0700
Subject: [Rd] R (development) changes in arith, logic,
 relop with (0-extent) arrays
In-Reply-To: <CAF8bMcYBMduLcp_aQoTh74MBBKB9QnTDQG9J_dZyTrFkGd0azw@mail.gmail.com>
References: <22469.52585.935914.336374@aragorn.wu.ac.at>
	<22470.37902.198447.925212@stat.math.ethz.ch>
	<22470.38100.993865.828495@aragorn.wu.ac.at>
	<22473.36759.791968.814130@stat.math.ethz.ch>
	<22473.38325.834026.658339@stat.math.ethz.ch>
	<22474.35836.489860.992765@aragorn.wu.ac.at>
	<CAPRP4-eKdb1AKGt73P-+khNXFAWm_4q2PZr69Q7BAjWcXKpSzA@mail.gmail.com>
	<CAPRP4-e4fL=j2xUVMjO-kdmeTfvF=+=9AKrU9+981Xvhwc2MPA@mail.gmail.com>
	<22477.32999.558832.601073@stat.math.ethz.ch>
	<22479.9847.747220.924128@stat.math.ethz.ch>
	<22479.58007.358121.626142@stat.math.ethz.ch>
	<CAHHjBM6yvrgwOFiFAtM+w8BGmdwn114Z6gfyjMEsLQDLV8mA5A@mail.gmail.com>
	<22481.16979.861309.264630@stat.math.ethz.ch>
	<CADwqtCN2PnTFwsrV-K3=hXt64r5eHYNt5dg0OZZdEcDtPOtJuQ@mail.gmail.com>
	<CAF8bMcYBMduLcp_aQoTh74MBBKB9QnTDQG9J_dZyTrFkGd0azw@mail.gmail.com>
Message-ID: <CADwqtCNXQpg6twfLv1LRscnMa=RND543Kgm1Xn-2APjgsOJWFA@mail.gmail.com>

On Thu, Sep 8, 2016 at 10:05 AM, William Dunlap <wdunlap at tibco.com> wrote:

> Shouldn't binary operators (arithmetic and logical) should throw an error
> when one operand is NULL (or other type that doesn't make sense)?  This is
> a different case than a zero-length operand of a legitimate type.  E.g.,
>      any(x < 0)
> should return FALSE if x is number-like and length(x)==0 but give an error
> if x is NULL.
>
Bill,

That is a good point. I can see the argument for this in the case that the
non-zero length is 1. I'm not sure which is better though. If we switch
any() to all(), things get murky.

Mathematically, all(x<0) is TRUE if x is length 0 (as are all(x==0), and
all(x>0)), but the likelihood of this being a thought-bug on the author's
part is exceedingly high, imho. So the desirable behavior seems to depend
on the angle we look at it from.

My personal opinion is that x < y with length(x)==0 should fail if length(y)
> 1, at least, and I'd be for it being an error even if y is length 1,
though I do acknowledge this is more likely (though still quite unlikely
imho) to be the intended behavior.

~G

>
> I.e., I think the type check should be done before the length check.
>
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
> On Thu, Sep 8, 2016 at 8:43 AM, Gabriel Becker <gmbecker at ucdavis.edu>
> wrote:
>
>> Martin,
>>
>> Like Robin and Oliver I think this type of edge-case consistency is
>> important and that it's fantastic that R-core - and you personally - are
>> willing to tackle some of these "gotcha" behaviors. "Little" stuff like
>> this really does combine to go a long way to making R better and better.
>>
>> I do wonder a  bit about the
>>
>> x = 1:2
>>
>> y = NULL
>>
>> x < y
>>
>> case.
>>
>> Returning a logical of length 0 is more backwards compatible, but is it
>> ever what the author actually intended? I have trouble thinking of a case
>> where that less-than didn't carry an implicit assumption that y was
>> non-NULL.  I can say that in my own code, I've never hit that behavior in
>> a
>> case that wasn't an error.
>>
>> My vote (unless someone else points out a compelling use for the behavior)
>> is for the to throw an error. As a developer, I'd rather things like this
>> break so the bug in my logic is visible, rather than  propagating as the
>> 0-length logical is &'ed or |'ed with other logical vectors, or used to
>> subset, or (in the case it should be length 1) passed to if() (if throws
>> an
>> error now, but the rest would silently "work").
>>
>> Best,
>> ~G
>>
>> On Thu, Sep 8, 2016 at 3:49 AM, Martin Maechler <
>> maechler at stat.math.ethz.ch>
>> wrote:
>>
>> > >>>>> robin hankin <hankin.robin at gmail.com>
>> > >>>>>     on Thu, 8 Sep 2016 10:05:21 +1200 writes:
>> >
>> >     > Martin I'd like to make a comment; I think that R's
>> >     > behaviour on 'edge' cases like this is an important thing
>> >     > and it's great that you are working on it.
>> >
>> >     > I make heavy use of zero-extent arrays, chiefly because
>> >     > the dimnames are an efficient and logical way to keep
>> >     > track of certain types of information.
>> >
>> >     > If I have, for example,
>> >
>> >     > a <- array(0,c(2,0,2))
>> >     > dimnames(a) <- list(name=c('Mike','Kevin'),
>> > NULL,item=c("hat","scarf"))
>> >
>> >
>> >     > Then in R-3.3.1, 70800 I get
>> >
>> >     a> 0
>> >     > logical(0)
>> >     >>
>> >
>> >     > But in 71219 I get
>> >
>> >     a> 0
>> >     > , , item = hat
>> >
>> >
>> >     > name
>> >     > Mike
>> >     > Kevin
>> >
>> >     > , , item = scarf
>> >
>> >
>> >     > name
>> >     > Mike
>> >     > Kevin
>> >
>> >     > (which is an empty logical array that holds the names of the
>> people
>> > and
>> >     > their clothes). I find the behaviour of 71219 very much preferable
>> > because
>> >     > there is no reason to discard the information in the dimnames.
>> >
>> > Thanks a lot, Robin, (and Oliver) !
>> >
>> > Yes, the above is such a case where the new behavior makes much sense.
>> > And this behavior remains identical after the 71222 amendment.
>> >
>> > Martin
>> >
>> >     > Best wishes
>> >     > Robin
>> >
>> >
>> >
>> >
>> >     > On Wed, Sep 7, 2016 at 9:49 PM, Martin Maechler <
>> > maechler at stat.math.ethz.ch>
>> >     > wrote:
>> >
>> >     >> >>>>> Martin Maechler <maechler at stat.math.ethz.ch>
>> >     >> >>>>>     on Tue, 6 Sep 2016 22:26:31 +0200 writes:
>> >     >>
>> >     >> > Yesterday, changes to R's development version were committed,
>> >     >> relating
>> >     >> > to arithmetic, logic ('&' and '|') and
>> >     >> > comparison/relational ('<', '==') binary operators
>> >     >> > which in NEWS are described as
>> >     >>
>> >     >> > SIGNIFICANT USER-VISIBLE CHANGES:
>> >     >>
>> >     >> > [.............]
>> >     >>
>> >     >> > ? Arithmetic, logic (?&?, ?|?) and comparison (aka
>> >     >> > ?relational?, e.g., ?<?, ?==?) operations with arrays now
>> >     >> > behave consistently, notably for arrays of length zero.
>> >     >>
>> >     >> > Arithmetic between length-1 arrays and longer non-arrays had
>> >     >> > silently dropped the array attributes and recycled.  This
>> >     >> > now gives a warning and will signal an error in the future,
>> >     >> > as it has always for logic and comparison operations in
>> >     >> > these cases (e.g., compare ?matrix(1,1) + 2:3? and
>> >     >> > ?matrix(1,1) < 2:3?).
>> >     >>
>> >     >> > As the above "visually suggests" one could think of the changes
>> >     >> > falling mainly two groups,
>> >     >> > 1) <0-extent array>  (op)     <non-array>
>> >     >> > 2) <1-extent array>  (arith)  <non-array of length != 1>
>> >     >>
>> >     >> > These changes are partly non-back compatible and may break
>> >     >> > existing code.  We believe that the internal consistency gained
>> >     >> > from the changes is worth the few places with problems.
>> >     >>
>> >     >> > We expect some package maintainers (10-20, or even more?) need
>> >     >> > to adapt their code.
>> >     >>
>> >     >> > Case '2)' above mainly results in a new warning, e.g.,
>> >     >>
>> >     >> >> matrix(1,1) + 1:2
>> >     >> > [1] 2 3
>> >     >> > Warning message:
>> >     >> > In matrix(1, 1) + 1:2 :
>> >     >> > dropping dim() of array of length one.  Will become ERROR
>> >     >> >>
>> >     >>
>> >     >> > whereas '1)' gives errors in cases the result silently was a
>> >     >> > vector of length zero, or also keeps array (dim & dimnames) in
>> >     >> > cases these were silently dropped.
>> >     >>
>> >     >> > The following is a "heavily" commented  R script showing (all
>> ?)
>> >     >> > the important cases with changes :
>> >     >>
>> >     >> > ------------------------------------------------------------
>> >     >> ----------------
>> >     >>
>> >     >> > (m <- cbind(a=1[0], b=2[0]))
>> >     >> > Lm <- m; storage.mode(Lm) <- "logical"
>> >     >> > Im <- m; storage.mode(Im) <- "integer"
>> >     >>
>> >     >> > ## 1. -------------------------
>> >     >> > try( m & NULL ) # in R <= 3.3.x :
>> >     >> > ## Error in m & NULL :
>> >     >> > ##  operations are possible only for numeric, logical or
>> complex
>> >     >> types
>> >     >> > ##
>> >     >> > ## gives 'Lm' in R >= 3.4.0
>> >     >>
>> >     >> > ## 2. -------------------------
>> >     >> > m + 2:3 ## gave numeric(0), now remains matrix identical to  m
>> >     >> > Im + 2:3 ## gave integer(0), now remains matrix identical to Im
>> >     >> (integer)
>> >     >>
>> >     >> > m > 1      ## gave logical(0), now remains matrix identical to
>> Lm
>> >     >> (logical)
>> >     >> > m > 0.1[0] ##  ditto
>> >     >> > m > NULL   ##  ditto
>> >     >>
>> >     >> > ## 3. -------------------------
>> >     >> > mm <- m[,c(1:2,2:1,2)]
>> >     >> > try( m == mm ) ## now gives error   "non-conformable arrays",
>> >     >> > ## but gave logical(0) in R <= 3.3.x
>> >     >>
>> >     >> > ## 4. -------------------------
>> >     >> > str( Im + NULL)  ## gave "num", now gives "int"
>> >     >>
>> >     >> > ## 5. -------------------------
>> >     >> > ## special case for arithmetic w/ length-1 array
>> >     >> > (m1 <- matrix(1,1,1, dimnames=list("Ro","col")))
>> >     >> > (m2 <- matrix(1,2,1, dimnames=list(c("A","B"),"col")))
>> >     >>
>> >     >> > m1 + 1:2  # ->  2:3  but now with warning to  "become ERROR"
>> >     >> > tools::assertError(m1 & 1:2)# ERR: dims [product 1] do not
>> match
>> > the
>> >     >> length of object [2]
>> >     >> > tools::assertError(m1 < 1:2)# ERR:                  (ditto)
>> >     >> > ##
>> >     >> > ## non-0-length arrays combined with {NULL or double() or ...}
>> > *fail*
>> >     >>
>> >     >> > ### Length-1 arrays:  Arithmetic with |vectors| > 1  treated
>> array
>> >     >> as scalar
>> >     >> > m1 + NULL # gave  numeric(0) in R <= 3.3.x --- still, *but* w/
>> >     >> warning to "be ERROR"
>> >     >> > try(m1 > NULL)    # gave  logical(0) in R <= 3.3.x --- an
>> *error*
>> >     >> now in R >= 3.4.0
>> >     >> > tools::assertError(m1 & NULL)    # gave and gives error
>> >     >> > tools::assertError(m1 | double())# ditto
>> >     >> > ## m2 was slightly different:
>> >     >> > tools::assertError(m2 + NULL)
>> >     >> > tools::assertError(m2 & NULL)
>> >     >> > try(m2 == NULL) ## was logical(0) in R <= 3.3.x; now error as
>> > above!
>> >     >>
>> >     >> > ------------------------------------------------------------
>> >     >> ----------------
>> >     >>
>> >     >>
>> >     >> > Note that in R's own  'nls'  sources, there was one case of
>> >     >> > situation '2)' above, i.e. a  1x1-matrix was used as a
>> "scalar".
>> >     >>
>> >     >> > In such cases, you should explicitly coerce it to a vector,
>> >     >> > either ("self-explainingly") by  as.vector(.), or as I did in
>> >     >> > the nls case  by  c(.) :  The latter is much less
>> >     >> > self-explaining, but nicer to read in mathematical formulae,
>> and
>> >     >> > currently also more efficient because it is a .Primitive.
>> >     >>
>> >     >> > Please use R-devel with your code, and let us know if you see
>> >     >> > effects that seem adverse.
>> >     >>
>> >     >> I've been slightly surprised (or even "frustrated") by the empty
>> >     >> reaction on our R-devel list to this post.
>> >     >>
>> >     >> I would have expected some critique, may be even some praise,
>> >     >> ... in any case some sign people are "thinking along" (as we say
>> >     >> in German).
>> >     >>
>> >     >> In the mean time, I've actually thought along the one case which
>> >     >> is last above:  The <op>  (binary operation) between a
>> >     >> non-0-length array and a 0-length vector (and NULL which should
>> >     >> be treated like a 0-length vector):
>> >     >>
>> >     >> R <= 3.3.1  *is* quite inconsistent with these:
>> >     >>
>> >     >>
>> >     >> and my proposal above (implemented in R-devel, since Sep.5) would
>> > give an
>> >     >> error for all these, but instead, R really could be more lenient
>> > here:
>> >     >> A 0-length result is ok, and it should *not* inherit the array
>> >     >> (dim, dimnames), since the array is not of length 0. So instead
>> >     >> of the above [for the very last part only!!], we would aim for
>> >     >> the following. These *all* give an error in current R-devel,
>> >     >> with the exception of 'm1 + NULL' which "only" gives a "bad
>> >     >> warning" :
>> >     >>
>> >     >> ------------------------
>> >     >>
>> >     >> m1 <- matrix(1,1)
>> >     >> m2 <- matrix(1,2)
>> >     >>
>> >     >> m1 + NULL #    numeric(0) in R <= 3.3.x ---> OK ?!
>> >     >> m1 > NULL #    logical(0) in R <= 3.3.x ---> OK ?!
>> >     >> try(m1 & NULL)    # ERROR in R <= 3.3.x ---> change to logical(0)
>> > ?!
>> >     >> try(m1 | double())# ERROR in R <= 3.3.x ---> change to logical(0)
>> > ?!
>> >     >> ## m2 slightly different:
>> >     >> try(m2 + NULL)  # ERROR in R <= 3.3.x ---> change to double(0)
>> ?!
>> >     >> try(m2 & NULL)  # ERROR in R <= 3.3.x ---> change to logical(0)
>> ?!
>> >     >> m2 == NULL # logical(0) in R <= 3.3.x ---> OK ?!
>> >     >>
>> >     >> ------------------------
>> >     >>
>> >     >> This would be slightly more back-compatible than the currently
>> >     >> implemented proposal. Everything else I said remains true, and
>> >     >> I'm pretty sure most changes needed in packages would remain to
>> be
>> > done.
>> >     >>
>> >     >> Opinions ?
>> >     >>
>> >     >>
>> >     >>
>> >     >> > In some case where R-devel now gives an error but did not
>> >     >> > previously, we could contemplate giving another  "warning
>> >     >> > .... 'to become ERROR'" if there was too much breakage,  though
>> >     >> > I don't expect that.
>> >     >>
>> >     >>
>> >     >> > For the R Core Team,
>> >     >>
>> >     >> > Martin Maechler,
>> >     >> > ETH Zurich
>> >     >>
>> >     >> ______________________________________________
>> >     >> R-devel at r-project.org mailing list
>> >     >> https://stat.ethz.ch/mailman/listinfo/r-devel
>> >     >>
>> >
>> >
>> >
>> >     > --
>> >     > Robin Hankin
>> >     > Neutral theorist
>> >     > hankin.robin at gmail.com
>> >
>> >     > [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > R-devel at r-project.org mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-devel
>> >
>>
>>
>>
>> --
>> Gabriel Becker, PhD
>> Associate Scientist (Bioinformatics)
>> Genentech Research
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>
>


-- 
Gabriel Becker, PhD
Associate Scientist (Bioinformatics)
Genentech Research

	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Thu Sep  8 19:45:07 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Thu, 8 Sep 2016 10:45:07 -0700
Subject: [Rd] R (development) changes in arith, logic,
 relop with (0-extent) arrays
In-Reply-To: <CADwqtCNXQpg6twfLv1LRscnMa=RND543Kgm1Xn-2APjgsOJWFA@mail.gmail.com>
References: <22469.52585.935914.336374@aragorn.wu.ac.at>
	<22470.37902.198447.925212@stat.math.ethz.ch>
	<22470.38100.993865.828495@aragorn.wu.ac.at>
	<22473.36759.791968.814130@stat.math.ethz.ch>
	<22473.38325.834026.658339@stat.math.ethz.ch>
	<22474.35836.489860.992765@aragorn.wu.ac.at>
	<CAPRP4-eKdb1AKGt73P-+khNXFAWm_4q2PZr69Q7BAjWcXKpSzA@mail.gmail.com>
	<CAPRP4-e4fL=j2xUVMjO-kdmeTfvF=+=9AKrU9+981Xvhwc2MPA@mail.gmail.com>
	<22477.32999.558832.601073@stat.math.ethz.ch>
	<22479.9847.747220.924128@stat.math.ethz.ch>
	<22479.58007.358121.626142@stat.math.ethz.ch>
	<CAHHjBM6yvrgwOFiFAtM+w8BGmdwn114Z6gfyjMEsLQDLV8mA5A@mail.gmail.com>
	<22481.16979.861309.264630@stat.math.ethz.ch>
	<CADwqtCN2PnTFwsrV-K3=hXt64r5eHYNt5dg0OZZdEcDtPOtJuQ@mail.gmail.com>
	<CAF8bMcYBMduLcp_aQoTh74MBBKB9QnTDQG9J_dZyTrFkGd0azw@mail.gmail.com>
	<CADwqtCNXQpg6twfLv1LRscnMa=RND543Kgm1Xn-2APjgsOJWFA@mail.gmail.com>
Message-ID: <CAF8bMcY8d5Jduf3TtT39+vdD0sAz1fs8hqQ9x0Td9GF=mNGF+Q@mail.gmail.com>

Prior to the mid-1990s, S did "length-0 OP length-n -> rep(NA, n)" and it
was changed
to "length-0 OP length-n -> length-0" to avoid lots of problems like
any(x<0) being NA
when length(x)==0.  Yes, people could code defensively by putting lots of
if(length(x)==0)...
in their code, but that is tedious and error-prone and creates really ugly
code.

Is your suggestion to leave the length-0 OP length-1 case as it is but make
length-0 OP length-two-or-higher an error or warning (akin to the length-2
OP length-3 case)?

By the way, the all(numeric(0)<0) is TRUE, as is all(numeric()>0), by de
Morgan's rule, but that is not really relevant here.


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Thu, Sep 8, 2016 at 10:22 AM, Gabriel Becker <gmbecker at ucdavis.edu>
wrote:

>
>
> On Thu, Sep 8, 2016 at 10:05 AM, William Dunlap <wdunlap at tibco.com> wrote:
>
>> Shouldn't binary operators (arithmetic and logical) should throw an error
>> when one operand is NULL (or other type that doesn't make sense)?  This is
>> a different case than a zero-length operand of a legitimate type.  E.g.,
>>      any(x < 0)
>> should return FALSE if x is number-like and length(x)==0 but give an
>> error if x is NULL.
>>
> Bill,
>
> That is a good point. I can see the argument for this in the case that the
> non-zero length is 1. I'm not sure which is better though. If we switch
> any() to all(), things get murky.
>
> Mathematically, all(x<0) is TRUE if x is length 0 (as are all(x==0), and
> all(x>0)), but the likelihood of this being a thought-bug on the author's
> part is exceedingly high, imho. So the desirable behavior seems to depend
> on the angle we look at it from.
>
> My personal opinion is that x < y with length(x)==0 should fail if length(y)
> > 1, at least, and I'd be for it being an error even if y is length 1,
> though I do acknowledge this is more likely (though still quite unlikely
> imho) to be the intended behavior.
>
> ~G
>
>>
>> I.e., I think the type check should be done before the length check.
>>
>>
>> Bill Dunlap
>> TIBCO Software
>> wdunlap tibco.com
>>
>> On Thu, Sep 8, 2016 at 8:43 AM, Gabriel Becker <gmbecker at ucdavis.edu>
>> wrote:
>>
>>> Martin,
>>>
>>> Like Robin and Oliver I think this type of edge-case consistency is
>>> important and that it's fantastic that R-core - and you personally - are
>>> willing to tackle some of these "gotcha" behaviors. "Little" stuff like
>>> this really does combine to go a long way to making R better and better.
>>>
>>> I do wonder a  bit about the
>>>
>>> x = 1:2
>>>
>>> y = NULL
>>>
>>> x < y
>>>
>>> case.
>>>
>>> Returning a logical of length 0 is more backwards compatible, but is it
>>> ever what the author actually intended? I have trouble thinking of a case
>>> where that less-than didn't carry an implicit assumption that y was
>>> non-NULL.  I can say that in my own code, I've never hit that behavior
>>> in a
>>> case that wasn't an error.
>>>
>>> My vote (unless someone else points out a compelling use for the
>>> behavior)
>>> is for the to throw an error. As a developer, I'd rather things like this
>>> break so the bug in my logic is visible, rather than  propagating as the
>>> 0-length logical is &'ed or |'ed with other logical vectors, or used to
>>> subset, or (in the case it should be length 1) passed to if() (if throws
>>> an
>>> error now, but the rest would silently "work").
>>>
>>> Best,
>>> ~G
>>>
>>> On Thu, Sep 8, 2016 at 3:49 AM, Martin Maechler <
>>> maechler at stat.math.ethz.ch>
>>> wrote:
>>>
>>> > >>>>> robin hankin <hankin.robin at gmail.com>
>>> > >>>>>     on Thu, 8 Sep 2016 10:05:21 +1200 writes:
>>> >
>>> >     > Martin I'd like to make a comment; I think that R's
>>> >     > behaviour on 'edge' cases like this is an important thing
>>> >     > and it's great that you are working on it.
>>> >
>>> >     > I make heavy use of zero-extent arrays, chiefly because
>>> >     > the dimnames are an efficient and logical way to keep
>>> >     > track of certain types of information.
>>> >
>>> >     > If I have, for example,
>>> >
>>> >     > a <- array(0,c(2,0,2))
>>> >     > dimnames(a) <- list(name=c('Mike','Kevin'),
>>> > NULL,item=c("hat","scarf"))
>>> >
>>> >
>>> >     > Then in R-3.3.1, 70800 I get
>>> >
>>> >     a> 0
>>> >     > logical(0)
>>> >     >>
>>> >
>>> >     > But in 71219 I get
>>> >
>>> >     a> 0
>>> >     > , , item = hat
>>> >
>>> >
>>> >     > name
>>> >     > Mike
>>> >     > Kevin
>>> >
>>> >     > , , item = scarf
>>> >
>>> >
>>> >     > name
>>> >     > Mike
>>> >     > Kevin
>>> >
>>> >     > (which is an empty logical array that holds the names of the
>>> people
>>> > and
>>> >     > their clothes). I find the behaviour of 71219 very much
>>> preferable
>>> > because
>>> >     > there is no reason to discard the information in the dimnames.
>>> >
>>> > Thanks a lot, Robin, (and Oliver) !
>>> >
>>> > Yes, the above is such a case where the new behavior makes much sense.
>>> > And this behavior remains identical after the 71222 amendment.
>>> >
>>> > Martin
>>> >
>>> >     > Best wishes
>>> >     > Robin
>>> >
>>> >
>>> >
>>> >
>>> >     > On Wed, Sep 7, 2016 at 9:49 PM, Martin Maechler <
>>> > maechler at stat.math.ethz.ch>
>>> >     > wrote:
>>> >
>>> >     >> >>>>> Martin Maechler <maechler at stat.math.ethz.ch>
>>> >     >> >>>>>     on Tue, 6 Sep 2016 22:26:31 +0200 writes:
>>> >     >>
>>> >     >> > Yesterday, changes to R's development version were committed,
>>> >     >> relating
>>> >     >> > to arithmetic, logic ('&' and '|') and
>>> >     >> > comparison/relational ('<', '==') binary operators
>>> >     >> > which in NEWS are described as
>>> >     >>
>>> >     >> > SIGNIFICANT USER-VISIBLE CHANGES:
>>> >     >>
>>> >     >> > [.............]
>>> >     >>
>>> >     >> > ? Arithmetic, logic (?&?, ?|?) and comparison (aka
>>> >     >> > ?relational?, e.g., ?<?, ?==?) operations with arrays now
>>> >     >> > behave consistently, notably for arrays of length zero.
>>> >     >>
>>> >     >> > Arithmetic between length-1 arrays and longer non-arrays had
>>> >     >> > silently dropped the array attributes and recycled.  This
>>> >     >> > now gives a warning and will signal an error in the future,
>>> >     >> > as it has always for logic and comparison operations in
>>> >     >> > these cases (e.g., compare ?matrix(1,1) + 2:3? and
>>> >     >> > ?matrix(1,1) < 2:3?).
>>> >     >>
>>> >     >> > As the above "visually suggests" one could think of the
>>> changes
>>> >     >> > falling mainly two groups,
>>> >     >> > 1) <0-extent array>  (op)     <non-array>
>>> >     >> > 2) <1-extent array>  (arith)  <non-array of length != 1>
>>> >     >>
>>> >     >> > These changes are partly non-back compatible and may break
>>> >     >> > existing code.  We believe that the internal consistency
>>> gained
>>> >     >> > from the changes is worth the few places with problems.
>>> >     >>
>>> >     >> > We expect some package maintainers (10-20, or even more?) need
>>> >     >> > to adapt their code.
>>> >     >>
>>> >     >> > Case '2)' above mainly results in a new warning, e.g.,
>>> >     >>
>>> >     >> >> matrix(1,1) + 1:2
>>> >     >> > [1] 2 3
>>> >     >> > Warning message:
>>> >     >> > In matrix(1, 1) + 1:2 :
>>> >     >> > dropping dim() of array of length one.  Will become ERROR
>>> >     >> >>
>>> >     >>
>>> >     >> > whereas '1)' gives errors in cases the result silently was a
>>> >     >> > vector of length zero, or also keeps array (dim & dimnames) in
>>> >     >> > cases these were silently dropped.
>>> >     >>
>>> >     >> > The following is a "heavily" commented  R script showing (all
>>> ?)
>>> >     >> > the important cases with changes :
>>> >     >>
>>> >     >> > ------------------------------------------------------------
>>> >     >> ----------------
>>> >     >>
>>> >     >> > (m <- cbind(a=1[0], b=2[0]))
>>> >     >> > Lm <- m; storage.mode(Lm) <- "logical"
>>> >     >> > Im <- m; storage.mode(Im) <- "integer"
>>> >     >>
>>> >     >> > ## 1. -------------------------
>>> >     >> > try( m & NULL ) # in R <= 3.3.x :
>>> >     >> > ## Error in m & NULL :
>>> >     >> > ##  operations are possible only for numeric, logical or
>>> complex
>>> >     >> types
>>> >     >> > ##
>>> >     >> > ## gives 'Lm' in R >= 3.4.0
>>> >     >>
>>> >     >> > ## 2. -------------------------
>>> >     >> > m + 2:3 ## gave numeric(0), now remains matrix identical to  m
>>> >     >> > Im + 2:3 ## gave integer(0), now remains matrix identical to
>>> Im
>>> >     >> (integer)
>>> >     >>
>>> >     >> > m > 1      ## gave logical(0), now remains matrix identical
>>> to Lm
>>> >     >> (logical)
>>> >     >> > m > 0.1[0] ##  ditto
>>> >     >> > m > NULL   ##  ditto
>>> >     >>
>>> >     >> > ## 3. -------------------------
>>> >     >> > mm <- m[,c(1:2,2:1,2)]
>>> >     >> > try( m == mm ) ## now gives error   "non-conformable arrays",
>>> >     >> > ## but gave logical(0) in R <= 3.3.x
>>> >     >>
>>> >     >> > ## 4. -------------------------
>>> >     >> > str( Im + NULL)  ## gave "num", now gives "int"
>>> >     >>
>>> >     >> > ## 5. -------------------------
>>> >     >> > ## special case for arithmetic w/ length-1 array
>>> >     >> > (m1 <- matrix(1,1,1, dimnames=list("Ro","col")))
>>> >     >> > (m2 <- matrix(1,2,1, dimnames=list(c("A","B"),"col")))
>>> >     >>
>>> >     >> > m1 + 1:2  # ->  2:3  but now with warning to  "become ERROR"
>>> >     >> > tools::assertError(m1 & 1:2)# ERR: dims [product 1] do not
>>> match
>>> > the
>>> >     >> length of object [2]
>>> >     >> > tools::assertError(m1 < 1:2)# ERR:                  (ditto)
>>> >     >> > ##
>>> >     >> > ## non-0-length arrays combined with {NULL or double() or ...}
>>> > *fail*
>>> >     >>
>>> >     >> > ### Length-1 arrays:  Arithmetic with |vectors| > 1  treated
>>> array
>>> >     >> as scalar
>>> >     >> > m1 + NULL # gave  numeric(0) in R <= 3.3.x --- still, *but* w/
>>> >     >> warning to "be ERROR"
>>> >     >> > try(m1 > NULL)    # gave  logical(0) in R <= 3.3.x --- an
>>> *error*
>>> >     >> now in R >= 3.4.0
>>> >     >> > tools::assertError(m1 & NULL)    # gave and gives error
>>> >     >> > tools::assertError(m1 | double())# ditto
>>> >     >> > ## m2 was slightly different:
>>> >     >> > tools::assertError(m2 + NULL)
>>> >     >> > tools::assertError(m2 & NULL)
>>> >     >> > try(m2 == NULL) ## was logical(0) in R <= 3.3.x; now error as
>>> > above!
>>> >     >>
>>> >     >> > ------------------------------------------------------------
>>> >     >> ----------------
>>> >     >>
>>> >     >>
>>> >     >> > Note that in R's own  'nls'  sources, there was one case of
>>> >     >> > situation '2)' above, i.e. a  1x1-matrix was used as a
>>> "scalar".
>>> >     >>
>>> >     >> > In such cases, you should explicitly coerce it to a vector,
>>> >     >> > either ("self-explainingly") by  as.vector(.), or as I did in
>>> >     >> > the nls case  by  c(.) :  The latter is much less
>>> >     >> > self-explaining, but nicer to read in mathematical formulae,
>>> and
>>> >     >> > currently also more efficient because it is a .Primitive.
>>> >     >>
>>> >     >> > Please use R-devel with your code, and let us know if you see
>>> >     >> > effects that seem adverse.
>>> >     >>
>>> >     >> I've been slightly surprised (or even "frustrated") by the empty
>>> >     >> reaction on our R-devel list to this post.
>>> >     >>
>>> >     >> I would have expected some critique, may be even some praise,
>>> >     >> ... in any case some sign people are "thinking along" (as we say
>>> >     >> in German).
>>> >     >>
>>> >     >> In the mean time, I've actually thought along the one case which
>>> >     >> is last above:  The <op>  (binary operation) between a
>>> >     >> non-0-length array and a 0-length vector (and NULL which should
>>> >     >> be treated like a 0-length vector):
>>> >     >>
>>> >     >> R <= 3.3.1  *is* quite inconsistent with these:
>>> >     >>
>>> >     >>
>>> >     >> and my proposal above (implemented in R-devel, since Sep.5)
>>> would
>>> > give an
>>> >     >> error for all these, but instead, R really could be more lenient
>>> > here:
>>> >     >> A 0-length result is ok, and it should *not* inherit the array
>>> >     >> (dim, dimnames), since the array is not of length 0. So instead
>>> >     >> of the above [for the very last part only!!], we would aim for
>>> >     >> the following. These *all* give an error in current R-devel,
>>> >     >> with the exception of 'm1 + NULL' which "only" gives a "bad
>>> >     >> warning" :
>>> >     >>
>>> >     >> ------------------------
>>> >     >>
>>> >     >> m1 <- matrix(1,1)
>>> >     >> m2 <- matrix(1,2)
>>> >     >>
>>> >     >> m1 + NULL #    numeric(0) in R <= 3.3.x ---> OK ?!
>>> >     >> m1 > NULL #    logical(0) in R <= 3.3.x ---> OK ?!
>>> >     >> try(m1 & NULL)    # ERROR in R <= 3.3.x ---> change to
>>> logical(0)
>>> > ?!
>>> >     >> try(m1 | double())# ERROR in R <= 3.3.x ---> change to
>>> logical(0)
>>> > ?!
>>> >     >> ## m2 slightly different:
>>> >     >> try(m2 + NULL)  # ERROR in R <= 3.3.x ---> change to double(0)
>>> ?!
>>> >     >> try(m2 & NULL)  # ERROR in R <= 3.3.x ---> change to
>>> logical(0)  ?!
>>> >     >> m2 == NULL # logical(0) in R <= 3.3.x ---> OK ?!
>>> >     >>
>>> >     >> ------------------------
>>> >     >>
>>> >     >> This would be slightly more back-compatible than the currently
>>> >     >> implemented proposal. Everything else I said remains true, and
>>> >     >> I'm pretty sure most changes needed in packages would remain to
>>> be
>>> > done.
>>> >     >>
>>> >     >> Opinions ?
>>> >     >>
>>> >     >>
>>> >     >>
>>> >     >> > In some case where R-devel now gives an error but did not
>>> >     >> > previously, we could contemplate giving another  "warning
>>> >     >> > .... 'to become ERROR'" if there was too much breakage,
>>> though
>>> >     >> > I don't expect that.
>>> >     >>
>>> >     >>
>>> >     >> > For the R Core Team,
>>> >     >>
>>> >     >> > Martin Maechler,
>>> >     >> > ETH Zurich
>>> >     >>
>>> >     >> ______________________________________________
>>> >     >> R-devel at r-project.org mailing list
>>> >     >> https://stat.ethz.ch/mailman/listinfo/r-devel
>>> >     >>
>>> >
>>> >
>>> >
>>> >     > --
>>> >     > Robin Hankin
>>> >     > Neutral theorist
>>> >     > hankin.robin at gmail.com
>>> >
>>> >     > [[alternative HTML version deleted]]
>>> >
>>> > ______________________________________________
>>> > R-devel at r-project.org mailing list
>>> > https://stat.ethz.ch/mailman/listinfo/r-devel
>>> >
>>>
>>>
>>>
>>> --
>>> Gabriel Becker, PhD
>>> Associate Scientist (Bioinformatics)
>>> Genentech Research
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>
>>
>>
>
>
> --
> Gabriel Becker, PhD
> Associate Scientist (Bioinformatics)
> Genentech Research
>

	[[alternative HTML version deleted]]


From pgilbert902 at gmail.com  Thu Sep  8 20:00:58 2016
From: pgilbert902 at gmail.com (Paul Gilbert)
Date: Thu, 8 Sep 2016 14:00:58 -0400
Subject: [Rd] R (development) changes in arith, logic,
 relop with (0-extent) arrays
In-Reply-To: <CADwqtCNXQpg6twfLv1LRscnMa=RND543Kgm1Xn-2APjgsOJWFA@mail.gmail.com>
References: <22469.52585.935914.336374@aragorn.wu.ac.at>
	<22470.37902.198447.925212@stat.math.ethz.ch>
	<22470.38100.993865.828495@aragorn.wu.ac.at>
	<22473.36759.791968.814130@stat.math.ethz.ch>
	<22473.38325.834026.658339@stat.math.ethz.ch>
	<22474.35836.489860.992765@aragorn.wu.ac.at>
	<CAPRP4-eKdb1AKGt73P-+khNXFAWm_4q2PZr69Q7BAjWcXKpSzA@mail.gmail.com>
	<CAPRP4-e4fL=j2xUVMjO-kdmeTfvF=+=9AKrU9+981Xvhwc2MPA@mail.gmail.com>
	<22477.32999.558832.601073@stat.math.ethz.ch>
	<22479.9847.747220.924128@stat.math.ethz.ch>
	<22479.58007.358121.626142@stat.math.ethz.ch>
	<CAHHjBM6yvrgwOFiFAtM+w8BGmdwn114Z6gfyjMEsLQDLV8mA5A@mail.gmail.com>
	<22481.16979.861309.264630@stat.math.ethz.ch>
	<CADwqtCN2PnTFwsrV-K3=hXt64r5eHYNt5dg0OZZdEcDtPOtJuQ@mail.gmail.com>
	<CAF8bMcYBMduLcp_aQoTh74MBBKB9QnTDQG9J_dZyTrFkGd0azw@mail.gmail.com>
	<CADwqtCNXQpg6twfLv1LRscnMa=RND543Kgm1Xn-2APjgsOJWFA@mail.gmail.com>
Message-ID: <4704b7c7-797c-a57b-6fc8-27e02920ff71@gmail.com>



On 09/08/2016 01:22 PM, Gabriel Becker wrote:
> On Thu, Sep 8, 2016 at 10:05 AM, William Dunlap <wdunlap at tibco.com> wrote:
>
>> Shouldn't binary operators (arithmetic and logical) should throw an error
>> when one operand is NULL (or other type that doesn't make sense)?  This is
>> a different case than a zero-length operand of a legitimate type.  E.g.,
>>      any(x < 0)
>> should return FALSE if x is number-like and length(x)==0 but give an error
>> if x is NULL.
>>
> Bill,
>
> That is a good point. I can see the argument for this in the case that the
> non-zero length is 1. I'm not sure which is better though. If we switch
> any() to all(), things get murky.
>
> Mathematically, all(x<0) is TRUE if x is length 0 (as are all(x==0), and
> all(x>0)), but the likelihood of this being a thought-bug on the author's
> part is exceedingly high, imho.

I suspect there may be more R users than you think that understand and 
use vacuously true in code. I don't really like the idea of turning a 
perfectly good and properly documented mathematical test into an error 
in order to protect against a possible "thought-bug".

Paul

So the desirable behavior seems to depend
> on the angle we look at it from.
>
> My personal opinion is that x < y with length(x)==0 should fail if length(y)
>> 1, at least, and I'd be for it being an error even if y is length 1,
> though I do acknowledge this is more likely (though still quite unlikely
> imho) to be the intended behavior.
>
> ~G
>
>>
>> I.e., I think the type check should be done before the length check.
>>
>>
>> Bill Dunlap
>> TIBCO Software
>> wdunlap tibco.com
>>
>> On Thu, Sep 8, 2016 at 8:43 AM, Gabriel Becker <gmbecker at ucdavis.edu>
>> wrote:
>>
>>> Martin,
>>>
>>> Like Robin and Oliver I think this type of edge-case consistency is
>>> important and that it's fantastic that R-core - and you personally - are
>>> willing to tackle some of these "gotcha" behaviors. "Little" stuff like
>>> this really does combine to go a long way to making R better and better.
>>>
>>> I do wonder a  bit about the
>>>
>>> x = 1:2
>>>
>>> y = NULL
>>>
>>> x < y
>>>
>>> case.
>>>
>>> Returning a logical of length 0 is more backwards compatible, but is it
>>> ever what the author actually intended? I have trouble thinking of a case
>>> where that less-than didn't carry an implicit assumption that y was
>>> non-NULL.  I can say that in my own code, I've never hit that behavior in
>>> a
>>> case that wasn't an error.
>>>
>>> My vote (unless someone else points out a compelling use for the behavior)
>>> is for the to throw an error. As a developer, I'd rather things like this
>>> break so the bug in my logic is visible, rather than  propagating as the
>>> 0-length logical is &'ed or |'ed with other logical vectors, or used to
>>> subset, or (in the case it should be length 1) passed to if() (if throws
>>> an
>>> error now, but the rest would silently "work").
>>>
>>> Best,
>>> ~G
>>>
>>> On Thu, Sep 8, 2016 at 3:49 AM, Martin Maechler <
>>> maechler at stat.math.ethz.ch>
>>> wrote:
>>>
>>>>>>>>> robin hankin <hankin.robin at gmail.com>
>>>>>>>>>     on Thu, 8 Sep 2016 10:05:21 +1200 writes:
>>>>
>>>>     > Martin I'd like to make a comment; I think that R's
>>>>     > behaviour on 'edge' cases like this is an important thing
>>>>     > and it's great that you are working on it.
>>>>
>>>>     > I make heavy use of zero-extent arrays, chiefly because
>>>>     > the dimnames are an efficient and logical way to keep
>>>>     > track of certain types of information.
>>>>
>>>>     > If I have, for example,
>>>>
>>>>     > a <- array(0,c(2,0,2))
>>>>     > dimnames(a) <- list(name=c('Mike','Kevin'),
>>>> NULL,item=c("hat","scarf"))
>>>>
>>>>
>>>>     > Then in R-3.3.1, 70800 I get
>>>>
>>>>     a> 0
>>>>     > logical(0)
>>>>     >>
>>>>
>>>>     > But in 71219 I get
>>>>
>>>>     a> 0
>>>>     > , , item = hat
>>>>
>>>>
>>>>     > name
>>>>     > Mike
>>>>     > Kevin
>>>>
>>>>     > , , item = scarf
>>>>
>>>>
>>>>     > name
>>>>     > Mike
>>>>     > Kevin
>>>>
>>>>     > (which is an empty logical array that holds the names of the
>>> people
>>>> and
>>>>     > their clothes). I find the behaviour of 71219 very much preferable
>>>> because
>>>>     > there is no reason to discard the information in the dimnames.
>>>>
>>>> Thanks a lot, Robin, (and Oliver) !
>>>>
>>>> Yes, the above is such a case where the new behavior makes much sense.
>>>> And this behavior remains identical after the 71222 amendment.
>>>>
>>>> Martin
>>>>
>>>>     > Best wishes
>>>>     > Robin
>>>>
>>>>
>>>>
>>>>
>>>>     > On Wed, Sep 7, 2016 at 9:49 PM, Martin Maechler <
>>>> maechler at stat.math.ethz.ch>
>>>>     > wrote:
>>>>
>>>>     >> >>>>> Martin Maechler <maechler at stat.math.ethz.ch>
>>>>     >> >>>>>     on Tue, 6 Sep 2016 22:26:31 +0200 writes:
>>>>     >>
>>>>     >> > Yesterday, changes to R's development version were committed,
>>>>     >> relating
>>>>     >> > to arithmetic, logic ('&' and '|') and
>>>>     >> > comparison/relational ('<', '==') binary operators
>>>>     >> > which in NEWS are described as
>>>>     >>
>>>>     >> > SIGNIFICANT USER-VISIBLE CHANGES:
>>>>     >>
>>>>     >> > [.............]
>>>>     >>
>>>>     >> > ? Arithmetic, logic (?&?, ?|?) and comparison (aka
>>>>     >> > ?relational?, e.g., ?<?, ?==?) operations with arrays now
>>>>     >> > behave consistently, notably for arrays of length zero.
>>>>     >>
>>>>     >> > Arithmetic between length-1 arrays and longer non-arrays had
>>>>     >> > silently dropped the array attributes and recycled.  This
>>>>     >> > now gives a warning and will signal an error in the future,
>>>>     >> > as it has always for logic and comparison operations in
>>>>     >> > these cases (e.g., compare ?matrix(1,1) + 2:3? and
>>>>     >> > ?matrix(1,1) < 2:3?).
>>>>     >>
>>>>     >> > As the above "visually suggests" one could think of the changes
>>>>     >> > falling mainly two groups,
>>>>     >> > 1) <0-extent array>  (op)     <non-array>
>>>>     >> > 2) <1-extent array>  (arith)  <non-array of length != 1>
>>>>     >>
>>>>     >> > These changes are partly non-back compatible and may break
>>>>     >> > existing code.  We believe that the internal consistency gained
>>>>     >> > from the changes is worth the few places with problems.
>>>>     >>
>>>>     >> > We expect some package maintainers (10-20, or even more?) need
>>>>     >> > to adapt their code.
>>>>     >>
>>>>     >> > Case '2)' above mainly results in a new warning, e.g.,
>>>>     >>
>>>>     >> >> matrix(1,1) + 1:2
>>>>     >> > [1] 2 3
>>>>     >> > Warning message:
>>>>     >> > In matrix(1, 1) + 1:2 :
>>>>     >> > dropping dim() of array of length one.  Will become ERROR
>>>>     >> >>
>>>>     >>
>>>>     >> > whereas '1)' gives errors in cases the result silently was a
>>>>     >> > vector of length zero, or also keeps array (dim & dimnames) in
>>>>     >> > cases these were silently dropped.
>>>>     >>
>>>>     >> > The following is a "heavily" commented  R script showing (all
>>> ?)
>>>>     >> > the important cases with changes :
>>>>     >>
>>>>     >> > ------------------------------------------------------------
>>>>     >> ----------------
>>>>     >>
>>>>     >> > (m <- cbind(a=1[0], b=2[0]))
>>>>     >> > Lm <- m; storage.mode(Lm) <- "logical"
>>>>     >> > Im <- m; storage.mode(Im) <- "integer"
>>>>     >>
>>>>     >> > ## 1. -------------------------
>>>>     >> > try( m & NULL ) # in R <= 3.3.x :
>>>>     >> > ## Error in m & NULL :
>>>>     >> > ##  operations are possible only for numeric, logical or
>>> complex
>>>>     >> types
>>>>     >> > ##
>>>>     >> > ## gives 'Lm' in R >= 3.4.0
>>>>     >>
>>>>     >> > ## 2. -------------------------
>>>>     >> > m + 2:3 ## gave numeric(0), now remains matrix identical to  m
>>>>     >> > Im + 2:3 ## gave integer(0), now remains matrix identical to Im
>>>>     >> (integer)
>>>>     >>
>>>>     >> > m > 1      ## gave logical(0), now remains matrix identical to
>>> Lm
>>>>     >> (logical)
>>>>     >> > m > 0.1[0] ##  ditto
>>>>     >> > m > NULL   ##  ditto
>>>>     >>
>>>>     >> > ## 3. -------------------------
>>>>     >> > mm <- m[,c(1:2,2:1,2)]
>>>>     >> > try( m == mm ) ## now gives error   "non-conformable arrays",
>>>>     >> > ## but gave logical(0) in R <= 3.3.x
>>>>     >>
>>>>     >> > ## 4. -------------------------
>>>>     >> > str( Im + NULL)  ## gave "num", now gives "int"
>>>>     >>
>>>>     >> > ## 5. -------------------------
>>>>     >> > ## special case for arithmetic w/ length-1 array
>>>>     >> > (m1 <- matrix(1,1,1, dimnames=list("Ro","col")))
>>>>     >> > (m2 <- matrix(1,2,1, dimnames=list(c("A","B"),"col")))
>>>>     >>
>>>>     >> > m1 + 1:2  # ->  2:3  but now with warning to  "become ERROR"
>>>>     >> > tools::assertError(m1 & 1:2)# ERR: dims [product 1] do not
>>> match
>>>> the
>>>>     >> length of object [2]
>>>>     >> > tools::assertError(m1 < 1:2)# ERR:                  (ditto)
>>>>     >> > ##
>>>>     >> > ## non-0-length arrays combined with {NULL or double() or ...}
>>>> *fail*
>>>>     >>
>>>>     >> > ### Length-1 arrays:  Arithmetic with |vectors| > 1  treated
>>> array
>>>>     >> as scalar
>>>>     >> > m1 + NULL # gave  numeric(0) in R <= 3.3.x --- still, *but* w/
>>>>     >> warning to "be ERROR"
>>>>     >> > try(m1 > NULL)    # gave  logical(0) in R <= 3.3.x --- an
>>> *error*
>>>>     >> now in R >= 3.4.0
>>>>     >> > tools::assertError(m1 & NULL)    # gave and gives error
>>>>     >> > tools::assertError(m1 | double())# ditto
>>>>     >> > ## m2 was slightly different:
>>>>     >> > tools::assertError(m2 + NULL)
>>>>     >> > tools::assertError(m2 & NULL)
>>>>     >> > try(m2 == NULL) ## was logical(0) in R <= 3.3.x; now error as
>>>> above!
>>>>     >>
>>>>     >> > ------------------------------------------------------------
>>>>     >> ----------------
>>>>     >>
>>>>     >>
>>>>     >> > Note that in R's own  'nls'  sources, there was one case of
>>>>     >> > situation '2)' above, i.e. a  1x1-matrix was used as a
>>> "scalar".
>>>>     >>
>>>>     >> > In such cases, you should explicitly coerce it to a vector,
>>>>     >> > either ("self-explainingly") by  as.vector(.), or as I did in
>>>>     >> > the nls case  by  c(.) :  The latter is much less
>>>>     >> > self-explaining, but nicer to read in mathematical formulae,
>>> and
>>>>     >> > currently also more efficient because it is a .Primitive.
>>>>     >>
>>>>     >> > Please use R-devel with your code, and let us know if you see
>>>>     >> > effects that seem adverse.
>>>>     >>
>>>>     >> I've been slightly surprised (or even "frustrated") by the empty
>>>>     >> reaction on our R-devel list to this post.
>>>>     >>
>>>>     >> I would have expected some critique, may be even some praise,
>>>>     >> ... in any case some sign people are "thinking along" (as we say
>>>>     >> in German).
>>>>     >>
>>>>     >> In the mean time, I've actually thought along the one case which
>>>>     >> is last above:  The <op>  (binary operation) between a
>>>>     >> non-0-length array and a 0-length vector (and NULL which should
>>>>     >> be treated like a 0-length vector):
>>>>     >>
>>>>     >> R <= 3.3.1  *is* quite inconsistent with these:
>>>>     >>
>>>>     >>
>>>>     >> and my proposal above (implemented in R-devel, since Sep.5) would
>>>> give an
>>>>     >> error for all these, but instead, R really could be more lenient
>>>> here:
>>>>     >> A 0-length result is ok, and it should *not* inherit the array
>>>>     >> (dim, dimnames), since the array is not of length 0. So instead
>>>>     >> of the above [for the very last part only!!], we would aim for
>>>>     >> the following. These *all* give an error in current R-devel,
>>>>     >> with the exception of 'm1 + NULL' which "only" gives a "bad
>>>>     >> warning" :
>>>>     >>
>>>>     >> ------------------------
>>>>     >>
>>>>     >> m1 <- matrix(1,1)
>>>>     >> m2 <- matrix(1,2)
>>>>     >>
>>>>     >> m1 + NULL #    numeric(0) in R <= 3.3.x ---> OK ?!
>>>>     >> m1 > NULL #    logical(0) in R <= 3.3.x ---> OK ?!
>>>>     >> try(m1 & NULL)    # ERROR in R <= 3.3.x ---> change to logical(0)
>>>> ?!
>>>>     >> try(m1 | double())# ERROR in R <= 3.3.x ---> change to logical(0)
>>>> ?!
>>>>     >> ## m2 slightly different:
>>>>     >> try(m2 + NULL)  # ERROR in R <= 3.3.x ---> change to double(0)
>>> ?!
>>>>     >> try(m2 & NULL)  # ERROR in R <= 3.3.x ---> change to logical(0)
>>> ?!
>>>>     >> m2 == NULL # logical(0) in R <= 3.3.x ---> OK ?!
>>>>     >>
>>>>     >> ------------------------
>>>>     >>
>>>>     >> This would be slightly more back-compatible than the currently
>>>>     >> implemented proposal. Everything else I said remains true, and
>>>>     >> I'm pretty sure most changes needed in packages would remain to
>>> be
>>>> done.
>>>>     >>
>>>>     >> Opinions ?
>>>>     >>
>>>>     >>
>>>>     >>
>>>>     >> > In some case where R-devel now gives an error but did not
>>>>     >> > previously, we could contemplate giving another  "warning
>>>>     >> > .... 'to become ERROR'" if there was too much breakage,  though
>>>>     >> > I don't expect that.
>>>>     >>
>>>>     >>
>>>>     >> > For the R Core Team,
>>>>     >>
>>>>     >> > Martin Maechler,
>>>>     >> > ETH Zurich
>>>>     >>
>>>>     >> ______________________________________________
>>>>     >> R-devel at r-project.org mailing list
>>>>     >> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>     >>
>>>>
>>>>
>>>>
>>>>     > --
>>>>     > Robin Hankin
>>>>     > Neutral theorist
>>>>     > hankin.robin at gmail.com
>>>>
>>>>     > [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-devel at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>
>>>
>>>
>>>
>>> --
>>> Gabriel Becker, PhD
>>> Associate Scientist (Bioinformatics)
>>> Genentech Research
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>
>>
>>
>
>


From hankin.robin at gmail.com  Thu Sep  8 23:06:11 2016
From: hankin.robin at gmail.com (robin hankin)
Date: Fri, 9 Sep 2016 09:06:11 +1200
Subject: [Rd] R (development) changes in arith, logic,
 relop with (0-extent) arrays
In-Reply-To: <4704b7c7-797c-a57b-6fc8-27e02920ff71@gmail.com>
References: <22469.52585.935914.336374@aragorn.wu.ac.at>
	<22470.37902.198447.925212@stat.math.ethz.ch>
	<22470.38100.993865.828495@aragorn.wu.ac.at>
	<22473.36759.791968.814130@stat.math.ethz.ch>
	<22473.38325.834026.658339@stat.math.ethz.ch>
	<22474.35836.489860.992765@aragorn.wu.ac.at>
	<CAPRP4-eKdb1AKGt73P-+khNXFAWm_4q2PZr69Q7BAjWcXKpSzA@mail.gmail.com>
	<CAPRP4-e4fL=j2xUVMjO-kdmeTfvF=+=9AKrU9+981Xvhwc2MPA@mail.gmail.com>
	<22477.32999.558832.601073@stat.math.ethz.ch>
	<22479.9847.747220.924128@stat.math.ethz.ch>
	<22479.58007.358121.626142@stat.math.ethz.ch>
	<CAHHjBM6yvrgwOFiFAtM+w8BGmdwn114Z6gfyjMEsLQDLV8mA5A@mail.gmail.com>
	<22481.16979.861309.264630@stat.math.ethz.ch>
	<CADwqtCN2PnTFwsrV-K3=hXt64r5eHYNt5dg0OZZdEcDtPOtJuQ@mail.gmail.com>
	<CAF8bMcYBMduLcp_aQoTh74MBBKB9QnTDQG9J_dZyTrFkGd0azw@mail.gmail.com>
	<CADwqtCNXQpg6twfLv1LRscnMa=RND543Kgm1Xn-2APjgsOJWFA@mail.gmail.com>
	<4704b7c7-797c-a57b-6fc8-27e02920ff71@gmail.com>
Message-ID: <CAHHjBM446huVPXRFZQMDbAugdS090FPjnkTEWYy1bcwCHoJnaw@mail.gmail.com>

Could we take a cue from min() and max()?

> x <- 1:10
> min(x[x>7])
[1] 8
> min(x[x>11])
[1] Inf
Warning message:
In min(x[x > 11]) : no non-missing arguments to min; returning Inf
>

As ?min says, this is implemented to preserve transitivity, and this
makes a lot of sense.
I think the issuing of a warning here is a good compromise; I can
always turn off warnings if I want.

I find this behaviour of min() and max() to be annoying in the *right*
way: it annoys me precisely when I need to be
annoyed, that is, when I haven't thought through the consequences of
sending zero-length arguments.


On Fri, Sep 9, 2016 at 6:00 AM, Paul Gilbert <pgilbert902 at gmail.com> wrote:
>
>
> On 09/08/2016 01:22 PM, Gabriel Becker wrote:
>>
>> On Thu, Sep 8, 2016 at 10:05 AM, William Dunlap <wdunlap at tibco.com> wrote:
>>
>>> Shouldn't binary operators (arithmetic and logical) should throw an error
>>> when one operand is NULL (or other type that doesn't make sense)?  This
>>> is
>>> a different case than a zero-length operand of a legitimate type.  E.g.,
>>>      any(x < 0)
>>> should return FALSE if x is number-like and length(x)==0 but give an
>>> error
>>> if x is NULL.
>>>
>> Bill,
>>
>> That is a good point. I can see the argument for this in the case that the
>> non-zero length is 1. I'm not sure which is better though. If we switch
>> any() to all(), things get murky.
>>
>> Mathematically, all(x<0) is TRUE if x is length 0 (as are all(x==0), and
>> all(x>0)), but the likelihood of this being a thought-bug on the author's
>> part is exceedingly high, imho.
>
>
> I suspect there may be more R users than you think that understand and use
> vacuously true in code. I don't really like the idea of turning a perfectly
> good and properly documented mathematical test into an error in order to
> protect against a possible "thought-bug".
>
> Paul
>
>
> So the desirable behavior seems to depend
>>
>> on the angle we look at it from.
>>
>> My personal opinion is that x < y with length(x)==0 should fail if
>> length(y)
>>>
>>> 1, at least, and I'd be for it being an error even if y is length 1,
>>
>> though I do acknowledge this is more likely (though still quite unlikely
>> imho) to be the intended behavior.
>>
>> ~G
>>
>>>
>>> I.e., I think the type check should be done before the length check.
>>>
>>>
>>> Bill Dunlap
>>> TIBCO Software
>>> wdunlap tibco.com
>>>
>>> On Thu, Sep 8, 2016 at 8:43 AM, Gabriel Becker <gmbecker at ucdavis.edu>
>>> wrote:
>>>
>>>> Martin,
>>>>
>>>> Like Robin and Oliver I think this type of edge-case consistency is
>>>> important and that it's fantastic that R-core - and you personally - are
>>>> willing to tackle some of these "gotcha" behaviors. "Little" stuff like
>>>> this really does combine to go a long way to making R better and better.
>>>>
>>>> I do wonder a  bit about the
>>>>
>>>> x = 1:2
>>>>
>>>> y = NULL
>>>>
>>>> x < y
>>>>
>>>> case.
>>>>
>>>> Returning a logical of length 0 is more backwards compatible, but is it
>>>> ever what the author actually intended? I have trouble thinking of a
>>>> case
>>>> where that less-than didn't carry an implicit assumption that y was
>>>> non-NULL.  I can say that in my own code, I've never hit that behavior
>>>> in
>>>> a
>>>> case that wasn't an error.
>>>>
>>>> My vote (unless someone else points out a compelling use for the
>>>> behavior)
>>>> is for the to throw an error. As a developer, I'd rather things like
>>>> this
>>>> break so the bug in my logic is visible, rather than  propagating as the
>>>> 0-length logical is &'ed or |'ed with other logical vectors, or used to
>>>> subset, or (in the case it should be length 1) passed to if() (if throws
>>>> an
>>>> error now, but the rest would silently "work").
>>>>
>>>> Best,
>>>> ~G
>>>>
>>>> On Thu, Sep 8, 2016 at 3:49 AM, Martin Maechler <
>>>> maechler at stat.math.ethz.ch>
>>>> wrote:
>>>>
>>>>>>>>>> robin hankin <hankin.robin at gmail.com>
>>>>>>>>>>     on Thu, 8 Sep 2016 10:05:21 +1200 writes:
>>>>>
>>>>>
>>>>>     > Martin I'd like to make a comment; I think that R's
>>>>>     > behaviour on 'edge' cases like this is an important thing
>>>>>     > and it's great that you are working on it.
>>>>>
>>>>>     > I make heavy use of zero-extent arrays, chiefly because
>>>>>     > the dimnames are an efficient and logical way to keep
>>>>>     > track of certain types of information.
>>>>>
>>>>>     > If I have, for example,
>>>>>
>>>>>     > a <- array(0,c(2,0,2))
>>>>>     > dimnames(a) <- list(name=c('Mike','Kevin'),
>>>>> NULL,item=c("hat","scarf"))
>>>>>
>>>>>
>>>>>     > Then in R-3.3.1, 70800 I get
>>>>>
>>>>>     a> 0
>>>>>     > logical(0)
>>>>>     >>
>>>>>
>>>>>     > But in 71219 I get
>>>>>
>>>>>     a> 0
>>>>>     > , , item = hat
>>>>>
>>>>>
>>>>>     > name
>>>>>     > Mike
>>>>>     > Kevin
>>>>>
>>>>>     > , , item = scarf
>>>>>
>>>>>
>>>>>     > name
>>>>>     > Mike
>>>>>     > Kevin
>>>>>
>>>>>     > (which is an empty logical array that holds the names of the
>>>>
>>>> people
>>>>>
>>>>> and
>>>>>     > their clothes). I find the behaviour of 71219 very much
>>>>> preferable
>>>>> because
>>>>>     > there is no reason to discard the information in the dimnames.
>>>>>
>>>>> Thanks a lot, Robin, (and Oliver) !
>>>>>
>>>>> Yes, the above is such a case where the new behavior makes much sense.
>>>>> And this behavior remains identical after the 71222 amendment.
>>>>>
>>>>> Martin
>>>>>
>>>>>     > Best wishes
>>>>>     > Robin
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>     > On Wed, Sep 7, 2016 at 9:49 PM, Martin Maechler <
>>>>> maechler at stat.math.ethz.ch>
>>>>>     > wrote:
>>>>>
>>>>>     >> >>>>> Martin Maechler <maechler at stat.math.ethz.ch>
>>>>>     >> >>>>>     on Tue, 6 Sep 2016 22:26:31 +0200 writes:
>>>>>     >>
>>>>>     >> > Yesterday, changes to R's development version were committed,
>>>>>     >> relating
>>>>>     >> > to arithmetic, logic ('&' and '|') and
>>>>>     >> > comparison/relational ('<', '==') binary operators
>>>>>     >> > which in NEWS are described as
>>>>>     >>
>>>>>     >> > SIGNIFICANT USER-VISIBLE CHANGES:
>>>>>     >>
>>>>>     >> > [.............]
>>>>>     >>
>>>>>     >> > ? Arithmetic, logic (?&?, ?|?) and comparison (aka
>>>>>     >> > ?relational?, e.g., ?<?, ?==?) operations with arrays now
>>>>>     >> > behave consistently, notably for arrays of length zero.
>>>>>     >>
>>>>>     >> > Arithmetic between length-1 arrays and longer non-arrays had
>>>>>     >> > silently dropped the array attributes and recycled.  This
>>>>>     >> > now gives a warning and will signal an error in the future,
>>>>>     >> > as it has always for logic and comparison operations in
>>>>>     >> > these cases (e.g., compare ?matrix(1,1) + 2:3? and
>>>>>     >> > ?matrix(1,1) < 2:3?).
>>>>>     >>
>>>>>     >> > As the above "visually suggests" one could think of the
>>>>> changes
>>>>>     >> > falling mainly two groups,
>>>>>     >> > 1) <0-extent array>  (op)     <non-array>
>>>>>     >> > 2) <1-extent array>  (arith)  <non-array of length != 1>
>>>>>     >>
>>>>>     >> > These changes are partly non-back compatible and may break
>>>>>     >> > existing code.  We believe that the internal consistency
>>>>> gained
>>>>>     >> > from the changes is worth the few places with problems.
>>>>>     >>
>>>>>     >> > We expect some package maintainers (10-20, or even more?) need
>>>>>     >> > to adapt their code.
>>>>>     >>
>>>>>     >> > Case '2)' above mainly results in a new warning, e.g.,
>>>>>     >>
>>>>>     >> >> matrix(1,1) + 1:2
>>>>>     >> > [1] 2 3
>>>>>     >> > Warning message:
>>>>>     >> > In matrix(1, 1) + 1:2 :
>>>>>     >> > dropping dim() of array of length one.  Will become ERROR
>>>>>     >> >>
>>>>>     >>
>>>>>     >> > whereas '1)' gives errors in cases the result silently was a
>>>>>     >> > vector of length zero, or also keeps array (dim & dimnames) in
>>>>>     >> > cases these were silently dropped.
>>>>>     >>
>>>>>     >> > The following is a "heavily" commented  R script showing (all
>>>>
>>>> ?)
>>>>>
>>>>>     >> > the important cases with changes :
>>>>>     >>
>>>>>     >> > ------------------------------------------------------------
>>>>>     >> ----------------
>>>>>     >>
>>>>>     >> > (m <- cbind(a=1[0], b=2[0]))
>>>>>     >> > Lm <- m; storage.mode(Lm) <- "logical"
>>>>>     >> > Im <- m; storage.mode(Im) <- "integer"
>>>>>     >>
>>>>>     >> > ## 1. -------------------------
>>>>>     >> > try( m & NULL ) # in R <= 3.3.x :
>>>>>     >> > ## Error in m & NULL :
>>>>>     >> > ##  operations are possible only for numeric, logical or
>>>>
>>>> complex
>>>>>
>>>>>     >> types
>>>>>     >> > ##
>>>>>     >> > ## gives 'Lm' in R >= 3.4.0
>>>>>     >>
>>>>>     >> > ## 2. -------------------------
>>>>>     >> > m + 2:3 ## gave numeric(0), now remains matrix identical to  m
>>>>>     >> > Im + 2:3 ## gave integer(0), now remains matrix identical to
>>>>> Im
>>>>>     >> (integer)
>>>>>     >>
>>>>>     >> > m > 1      ## gave logical(0), now remains matrix identical to
>>>>
>>>> Lm
>>>>>
>>>>>     >> (logical)
>>>>>     >> > m > 0.1[0] ##  ditto
>>>>>     >> > m > NULL   ##  ditto
>>>>>     >>
>>>>>     >> > ## 3. -------------------------
>>>>>     >> > mm <- m[,c(1:2,2:1,2)]
>>>>>     >> > try( m == mm ) ## now gives error   "non-conformable arrays",
>>>>>     >> > ## but gave logical(0) in R <= 3.3.x
>>>>>     >>
>>>>>     >> > ## 4. -------------------------
>>>>>     >> > str( Im + NULL)  ## gave "num", now gives "int"
>>>>>     >>
>>>>>     >> > ## 5. -------------------------
>>>>>     >> > ## special case for arithmetic w/ length-1 array
>>>>>     >> > (m1 <- matrix(1,1,1, dimnames=list("Ro","col")))
>>>>>     >> > (m2 <- matrix(1,2,1, dimnames=list(c("A","B"),"col")))
>>>>>     >>
>>>>>     >> > m1 + 1:2  # ->  2:3  but now with warning to  "become ERROR"
>>>>>     >> > tools::assertError(m1 & 1:2)# ERR: dims [product 1] do not
>>>>
>>>> match
>>>>>
>>>>> the
>>>>>     >> length of object [2]
>>>>>     >> > tools::assertError(m1 < 1:2)# ERR:                  (ditto)
>>>>>     >> > ##
>>>>>     >> > ## non-0-length arrays combined with {NULL or double() or ...}
>>>>> *fail*
>>>>>     >>
>>>>>     >> > ### Length-1 arrays:  Arithmetic with |vectors| > 1  treated
>>>>
>>>> array
>>>>>
>>>>>     >> as scalar
>>>>>     >> > m1 + NULL # gave  numeric(0) in R <= 3.3.x --- still, *but* w/
>>>>>     >> warning to "be ERROR"
>>>>>     >> > try(m1 > NULL)    # gave  logical(0) in R <= 3.3.x --- an
>>>>
>>>> *error*
>>>>>
>>>>>     >> now in R >= 3.4.0
>>>>>     >> > tools::assertError(m1 & NULL)    # gave and gives error
>>>>>     >> > tools::assertError(m1 | double())# ditto
>>>>>     >> > ## m2 was slightly different:
>>>>>     >> > tools::assertError(m2 + NULL)
>>>>>     >> > tools::assertError(m2 & NULL)
>>>>>     >> > try(m2 == NULL) ## was logical(0) in R <= 3.3.x; now error as
>>>>> above!
>>>>>     >>
>>>>>     >> > ------------------------------------------------------------
>>>>>     >> ----------------
>>>>>     >>
>>>>>     >>
>>>>>     >> > Note that in R's own  'nls'  sources, there was one case of
>>>>>     >> > situation '2)' above, i.e. a  1x1-matrix was used as a
>>>>
>>>> "scalar".
>>>>>
>>>>>     >>
>>>>>     >> > In such cases, you should explicitly coerce it to a vector,
>>>>>     >> > either ("self-explainingly") by  as.vector(.), or as I did in
>>>>>     >> > the nls case  by  c(.) :  The latter is much less
>>>>>     >> > self-explaining, but nicer to read in mathematical formulae,
>>>>
>>>> and
>>>>>
>>>>>     >> > currently also more efficient because it is a .Primitive.
>>>>>     >>
>>>>>     >> > Please use R-devel with your code, and let us know if you see
>>>>>     >> > effects that seem adverse.
>>>>>     >>
>>>>>     >> I've been slightly surprised (or even "frustrated") by the empty
>>>>>     >> reaction on our R-devel list to this post.
>>>>>     >>
>>>>>     >> I would have expected some critique, may be even some praise,
>>>>>     >> ... in any case some sign people are "thinking along" (as we say
>>>>>     >> in German).
>>>>>     >>
>>>>>     >> In the mean time, I've actually thought along the one case which
>>>>>     >> is last above:  The <op>  (binary operation) between a
>>>>>     >> non-0-length array and a 0-length vector (and NULL which should
>>>>>     >> be treated like a 0-length vector):
>>>>>     >>
>>>>>     >> R <= 3.3.1  *is* quite inconsistent with these:
>>>>>     >>
>>>>>     >>
>>>>>     >> and my proposal above (implemented in R-devel, since Sep.5)
>>>>> would
>>>>> give an
>>>>>     >> error for all these, but instead, R really could be more lenient
>>>>> here:
>>>>>     >> A 0-length result is ok, and it should *not* inherit the array
>>>>>     >> (dim, dimnames), since the array is not of length 0. So instead
>>>>>     >> of the above [for the very last part only!!], we would aim for
>>>>>     >> the following. These *all* give an error in current R-devel,
>>>>>     >> with the exception of 'm1 + NULL' which "only" gives a "bad
>>>>>     >> warning" :
>>>>>     >>
>>>>>     >> ------------------------
>>>>>     >>
>>>>>     >> m1 <- matrix(1,1)
>>>>>     >> m2 <- matrix(1,2)
>>>>>     >>
>>>>>     >> m1 + NULL #    numeric(0) in R <= 3.3.x ---> OK ?!
>>>>>     >> m1 > NULL #    logical(0) in R <= 3.3.x ---> OK ?!
>>>>>     >> try(m1 & NULL)    # ERROR in R <= 3.3.x ---> change to
>>>>> logical(0)
>>>>> ?!
>>>>>     >> try(m1 | double())# ERROR in R <= 3.3.x ---> change to
>>>>> logical(0)
>>>>> ?!
>>>>>     >> ## m2 slightly different:
>>>>>     >> try(m2 + NULL)  # ERROR in R <= 3.3.x ---> change to double(0)
>>>>
>>>> ?!
>>>>>
>>>>>     >> try(m2 & NULL)  # ERROR in R <= 3.3.x ---> change to logical(0)
>>>>
>>>> ?!
>>>>>
>>>>>     >> m2 == NULL # logical(0) in R <= 3.3.x ---> OK ?!
>>>>>     >>
>>>>>     >> ------------------------
>>>>>     >>
>>>>>     >> This would be slightly more back-compatible than the currently
>>>>>     >> implemented proposal. Everything else I said remains true, and
>>>>>     >> I'm pretty sure most changes needed in packages would remain to
>>>>
>>>> be
>>>>>
>>>>> done.
>>>>>     >>
>>>>>     >> Opinions ?
>>>>>     >>
>>>>>     >>
>>>>>     >>
>>>>>     >> > In some case where R-devel now gives an error but did not
>>>>>     >> > previously, we could contemplate giving another  "warning
>>>>>     >> > .... 'to become ERROR'" if there was too much breakage,
>>>>> though
>>>>>     >> > I don't expect that.
>>>>>     >>
>>>>>     >>
>>>>>     >> > For the R Core Team,
>>>>>     >>
>>>>>     >> > Martin Maechler,
>>>>>     >> > ETH Zurich
>>>>>     >>
>>>>>     >> ______________________________________________
>>>>>     >> R-devel at r-project.org mailing list
>>>>>     >> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>>     >>
>>>>>
>>>>>
>>>>>
>>>>>     > --
>>>>>     > Robin Hankin
>>>>>     > Neutral theorist
>>>>>     > hankin.robin at gmail.com
>>>>>
>>>>>     > [[alternative HTML version deleted]]
>>>>>
>>>>> ______________________________________________
>>>>> R-devel at r-project.org mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>>
>>>>
>>>>
>>>>
>>>> --
>>>> Gabriel Becker, PhD
>>>> Associate Scientist (Bioinformatics)
>>>> Genentech Research
>>>>
>>>>         [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-devel at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>
>>>
>>>
>>
>>
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel



-- 
Robin Hankin
Neutral theorist
hankin.robin at gmail.com


From radford at cs.toronto.edu  Thu Sep  8 23:11:18 2016
From: radford at cs.toronto.edu (Radford Neal)
Date: Thu, 8 Sep 2016 17:11:18 -0400
Subject: [Rd] R (development) changes in arith, logic,
 relop with (0-extent) arrays
In-Reply-To: <mailman.27.1473328804.17880.r-devel@r-project.org>
References: <mailman.27.1473328804.17880.r-devel@r-project.org>
Message-ID: <20160908211118.GA17008@mail.cs.toronto.edu>

Regarding Martin Maechler's proposal:

   Arithmetic between length-1 arrays and longer non-arrays had
   silently dropped the array attributes and recycled.  This now gives
   a warning and will signal an error in the future, as it has always
   for logic and comparison operations

For example, matrix(1,1,1) + (1:2) would give a warning/error.

I think this might be a mistake.

The potential benefits of this would be detection of some programming
errors, and increased consistency.  The downsides are breaking
existing working programs, and decreased consistency.

Regarding consistency, the overall R philosophy is that attaching an
attribute to a vector doesn't change what you can do with it, or what
the result is, except that the result (often) gets the attributes
carried forward.  By this logic, adding a 'dim' attribute shouldn't
stop you from doing arithmetic (or comparisons) that you otherwise
could.

But maybe 'dim' attributes are special?  Well, they are in some
circumstances, and have to be when they are intended to change the
behaviour, such as when a matrix is used as an index with [.

But in many cases at present, 'dim' attributes DON'T stop you from
treating the object as a plain vector - for example, one is allowed 
to do matrix(1:4,2,2)[3], and a<-numeric(10); a[2:5]<-matrix(1,2,2).

So it may make more sense to move towards consistency in the
permissive direction, rather than the restrictive direction.  That
would mean allowing matrix(1,1,1)<(1:2), and maybe also things
like matrix(1,2,2)+(1:8).

Obviously, a change that removes error conditions is much less likely
to produce backwards-compatibility problems than a change that gives
errors for previously-allowed operations.

And I think there would be some significant problems. In addition to
the 10-20+ packages that Martin expects to break, there could be quite
a bit of user code that would no longer work - scripts for analysing
data sets that used to work, but now don't with the latest version.

There are reasons to expect such problems.  Some operations such as
vector dot products using %*% produce results that are always scalar,
but are formed as 1x1 matrices.  One can anticipate that many people
have not been getting rid of the 'dim' attribute in such cases, when
doing so hasn't been necessary in the past.

Regarding the 0-length vector issue, I agree with other posters that
after a<-numeric(0), is has to be allowable to write a<1.  To not
allow this would be highly destructive of code reliability.  And for
similar reason, after a<-c(), a<1 needs to be allowed, which means
NULL<1 should be allowed (giving logical(0)), since c() is NULL.

   Radford Neal


From maechler at stat.math.ethz.ch  Fri Sep  9 08:51:46 2016
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 9 Sep 2016 08:51:46 +0200
Subject: [Rd] R (development) changes in arith, logic,
 relop with (0-extent) arrays
In-Reply-To: <CAF8bMcY8d5Jduf3TtT39+vdD0sAz1fs8hqQ9x0Td9GF=mNGF+Q@mail.gmail.com>
References: <22469.52585.935914.336374@aragorn.wu.ac.at>
	<22470.37902.198447.925212@stat.math.ethz.ch>
	<22470.38100.993865.828495@aragorn.wu.ac.at>
	<22473.36759.791968.814130@stat.math.ethz.ch>
	<22473.38325.834026.658339@stat.math.ethz.ch>
	<22474.35836.489860.992765@aragorn.wu.ac.at>
	<CAPRP4-eKdb1AKGt73P-+khNXFAWm_4q2PZr69Q7BAjWcXKpSzA@mail.gmail.com>
	<CAPRP4-e4fL=j2xUVMjO-kdmeTfvF=+=9AKrU9+981Xvhwc2MPA@mail.gmail.com>
	<22477.32999.558832.601073@stat.math.ethz.ch>
	<22479.9847.747220.924128@stat.math.ethz.ch>
	<22479.58007.358121.626142@stat.math.ethz.ch>
	<CAHHjBM6yvrgwOFiFAtM+w8BGmdwn114Z6gfyjMEsLQDLV8mA5A@mail.gmail.com>
	<22481.16979.861309.264630@stat.math.ethz.ch>
	<CADwqtCN2PnTFwsrV-K3=hXt64r5eHYNt5dg0OZZdEcDtPOtJuQ@mail.gmail.com>
	<CAF8bMcYBMduLcp_aQoTh74MBBKB9QnTDQG9J_dZyTrFkGd0azw@mail.gmail.com>
	<CADwqtCNXQpg6twfLv1LRscnMa=RND543Kgm1Xn-2APjgsOJWFA@mail.gmail.com>
	<CAF8bMcY8d5Jduf3TtT39+vdD0sAz1fs8hqQ9x0Td9GF=mNGF+Q@mail.gmail.com>
Message-ID: <22482.23554.507321.387197@stat.math.ethz.ch>

Thank you, Gabe and Bill,

for taking up the discussion.

>>>>> William Dunlap <wdunlap at tibco.com>
>>>>>     on Thu, 8 Sep 2016 10:45:07 -0700 writes:

    > Prior to the mid-1990s, S did "length-0 OP length-n -> rep(NA, n)" and it
    > was changed
    > to "length-0 OP length-n -> length-0" to avoid lots of problems like
    > any(x<0) being NA
    > when length(x)==0.  Yes, people could code defensively by putting lots of
    > if(length(x)==0)...
    > in their code, but that is tedious and error-prone and creates really ugly
    > code.

Yes, so actually, basically

     length-0 OP <anything>  -> length-0

Now the case of NULL that Bill mentioned.
I agree that NULL  is not at all the same thing as  double(0) or logical(0),
*but* there have been quite a few cases, where NULL is the
result of operations where "for consistency"  double(0) / logical(0) should have
been.... and there are the users who use NULL as the equivalent
of those, e.g., by initializing a (to be grown, yes, very inefficient!)
vector with NULL instead of with say double(0).

For these reasons, many operations that expect a "number-like"
(includes logical) atomic vector have treated NULL as such...
*and* parts of the {arith/logic/relop} OPs have done so already
in R "forever".
I still would argue that for these OPs, treating NULL as  logical(0) {which
then may be promoted by the usual rules} is good thing.


    > Is your suggestion to leave the length-0 OP length-1 case as it is but make
    > length-0 OP length-two-or-higher an error or warning (akin to the length-2
    > OP length-3 case)?

That's exactly what one thing the current changes eliminated:
arithmetic (only; not logic, or relop) did treat the length-1
case (for arrays!) different from the length-GE-2 case.  And I strongly
believe that this is very wrong and counter to the predominant
recycling rules in (S and) R.


    > By the way, the all(numeric(0)<0) is TRUE, as is all(numeric()>0), by de
    > Morgan's rule, but that is not really relevant here.



    > Bill Dunlap
    > TIBCO Software
    > wdunlap tibco.com

    > On Thu, Sep 8, 2016 at 10:22 AM, Gabriel Becker <gmbecker at ucdavis.edu>
    > wrote:

    >> 
    >> 
    >> On Thu, Sep 8, 2016 at 10:05 AM, William Dunlap <wdunlap at tibco.com> wrote:
    >> 
    >>> Shouldn't binary operators (arithmetic and logical) should throw an error
    >>> when one operand is NULL (or other type that doesn't make sense)?  This is
    >>> a different case than a zero-length operand of a legitimate type.  E.g.,
    >>> any(x < 0)
    >>> should return FALSE if x is number-like and length(x)==0 but give an
    >>> error if x is NULL.
    >>> 
    >> Bill,
    >> 
    >> That is a good point. I can see the argument for this in the case that the
    >> non-zero length is 1. I'm not sure which is better though. If we switch
    >> any() to all(), things get murky.
    >> 
    >> Mathematically, all(x<0) is TRUE if x is length 0 (as are all(x==0), and
    >> all(x>0)), but the likelihood of this being a thought-bug on the author's
    >> part is exceedingly high, imho. So the desirable behavior seems to depend
    >> on the angle we look at it from.
    >> 
    >> My personal opinion is that x < y with length(x)==0 should fail if length(y)
    >> > 1, at least, and I'd be for it being an error even if y is length 1,
    >> though I do acknowledge this is more likely (though still quite unlikely
    >> imho) to be the intended behavior.
    >> 
    >> ~G
    >> 
    >>> 
    >>> I.e., I think the type check should be done before the length check.
    >>> 
    >>> 
    >>> Bill Dunlap
    >>> TIBCO Software
    >>> wdunlap tibco.com
    >>> 
    >>> On Thu, Sep 8, 2016 at 8:43 AM, Gabriel Becker <gmbecker at ucdavis.edu>
    >>> wrote:
    >>> 
    >>>> Martin,
    >>>> 
    >>>> Like Robin and Oliver I think this type of edge-case consistency is
    >>>> important and that it's fantastic that R-core - and you personally - are
    >>>> willing to tackle some of these "gotcha" behaviors. "Little" stuff like
    >>>> this really does combine to go a long way to making R better and better.
    >>>> 
    >>>> I do wonder a  bit about the
    >>>> 
    >>>> x = 1:2
    >>>> 
    >>>> y = NULL
    >>>> 
    >>>> x < y
    >>>> 
    >>>> case.
    >>>> 
    >>>> Returning a logical of length 0 is more backwards compatible, but is it
    >>>> ever what the author actually intended? I have trouble thinking of a case
    >>>> where that less-than didn't carry an implicit assumption that y was
    >>>> non-NULL.  I can say that in my own code, I've never hit that behavior
    >>>> in a
    >>>> case that wasn't an error.
    >>>> 
    >>>> My vote (unless someone else points out a compelling use for the
    >>>> behavior)
    >>>> is for the to throw an error. As a developer, I'd rather things like this
    >>>> break so the bug in my logic is visible, rather than  propagating as the
    >>>> 0-length logical is &'ed or |'ed with other logical vectors, or used to
    >>>> subset, or (in the case it should be length 1) passed to if() (if throws
    >>>> an
    >>>> error now, but the rest would silently "work").
    >>>> 
    >>>> Best,
    >>>> ~G
    >>>> 
    >>>> On Thu, Sep 8, 2016 at 3:49 AM, Martin Maechler <
    >>>> maechler at stat.math.ethz.ch>
    >>>> wrote:
    >>>> 
    >>>> > >>>>> robin hankin <hankin.robin at gmail.com>
    >>>> > >>>>>     on Thu, 8 Sep 2016 10:05:21 +1200 writes:
    >>>> >
    >>>> >     > Martin I'd like to make a comment; I think that R's
    >>>> >     > behaviour on 'edge' cases like this is an important thing
    >>>> >     > and it's great that you are working on it.
    >>>> >
    >>>> >     > I make heavy use of zero-extent arrays, chiefly because
    >>>> >     > the dimnames are an efficient and logical way to keep
    >>>> >     > track of certain types of information.
    >>>> >
    >>>> >     > If I have, for example,
    >>>> >
    >>>> >     > a <- array(0,c(2,0,2))
    >>>> >     > dimnames(a) <- list(name=c('Mike','Kevin'),
    >>>> > NULL,item=c("hat","scarf"))
    >>>> >
    >>>> >
    >>>> >     > Then in R-3.3.1, 70800 I get
    >>>> >
    >>>> >     a> 0
    >>>> >     > logical(0)
    >>>> >     >>
    >>>> >
    >>>> >     > But in 71219 I get
    >>>> >
    >>>> >     a> 0
    >>>> >     > , , item = hat
    >>>> >
    >>>> >
    >>>> >     > name
    >>>> >     > Mike
    >>>> >     > Kevin
    >>>> >
    >>>> >     > , , item = scarf
    >>>> >
    >>>> >
    >>>> >     > name
    >>>> >     > Mike
    >>>> >     > Kevin
    >>>> >
    >>>> >     > (which is an empty logical array that holds the names of the
    >>>> people
    >>>> > and
    >>>> >     > their clothes). I find the behaviour of 71219 very much
    >>>> preferable
    >>>> > because
    >>>> >     > there is no reason to discard the information in the dimnames.
    >>>> >
    >>>> > Thanks a lot, Robin, (and Oliver) !
    >>>> >
    >>>> > Yes, the above is such a case where the new behavior makes much sense.
    >>>> > And this behavior remains identical after the 71222 amendment.
    >>>> >
    >>>> > Martin
    >>>> >
    >>>> >     > Best wishes
    >>>> >     > Robin
    >>>> >
    >>>> >
    >>>> >
    >>>> >
    >>>> >     > On Wed, Sep 7, 2016 at 9:49 PM, Martin Maechler <
    >>>> > maechler at stat.math.ethz.ch>
    >>>> >     > wrote:
    >>>> >
    >>>> >     >> >>>>> Martin Maechler <maechler at stat.math.ethz.ch>
    >>>> >     >> >>>>>     on Tue, 6 Sep 2016 22:26:31 +0200 writes:
    >>>> >     >>
    >>>> >     >> > Yesterday, changes to R's development version were committed,
    >>>> >     >> relating
    >>>> >     >> > to arithmetic, logic ('&' and '|') and
    >>>> >     >> > comparison/relational ('<', '==') binary operators
    >>>> >     >> > which in NEWS are described as
    >>>> >     >>
    >>>> >     >> > SIGNIFICANT USER-VISIBLE CHANGES:
    >>>> >     >>
    >>>> >     >> > [.............]
    >>>> >     >>
    >>>> >     >> > ? Arithmetic, logic (?&?, ?|?) and comparison (aka
    >>>> >     >> > ?relational?, e.g., ?<?, ?==?) operations with arrays now
    >>>> >     >> > behave consistently, notably for arrays of length zero.
    >>>> >     >>
    >>>> >     >> > Arithmetic between length-1 arrays and longer non-arrays had
    >>>> >     >> > silently dropped the array attributes and recycled.  This
    >>>> >     >> > now gives a warning and will signal an error in the future,
    >>>> >     >> > as it has always for logic and comparison operations in
    >>>> >     >> > these cases (e.g., compare ?matrix(1,1) + 2:3? and
    >>>> >     >> > ?matrix(1,1) < 2:3?).
    >>>> >     >>
    >>>> >     >> > As the above "visually suggests" one could think of the
    >>>> changes
    >>>> >     >> > falling mainly two groups,
    >>>> >     >> > 1) <0-extent array>  (op)     <non-array>
    >>>> >     >> > 2) <1-extent array>  (arith)  <non-array of length != 1>
    >>>> >     >>
    >>>> >     >> > These changes are partly non-back compatible and may break
    >>>> >     >> > existing code.  We believe that the internal consistency
    >>>> gained
    >>>> >     >> > from the changes is worth the few places with problems.
    >>>> >     >>
    >>>> >     >> > We expect some package maintainers (10-20, or even more?) need
    >>>> >     >> > to adapt their code.
    >>>> >     >>
    >>>> >     >> > Case '2)' above mainly results in a new warning, e.g.,
    >>>> >     >>
    >>>> >     >> >> matrix(1,1) + 1:2
    >>>> >     >> > [1] 2 3
    >>>> >     >> > Warning message:
    >>>> >     >> > In matrix(1, 1) + 1:2 :
    >>>> >     >> > dropping dim() of array of length one.  Will become ERROR
    >>>> >     >> >>
    >>>> >     >>
    >>>> >     >> > whereas '1)' gives errors in cases the result silently was a
    >>>> >     >> > vector of length zero, or also keeps array (dim & dimnames) in
    >>>> >     >> > cases these were silently dropped.
    >>>> >     >>
    >>>> >     >> > The following is a "heavily" commented  R script showing (all
    >>>> ?)
    >>>> >     >> > the important cases with changes :
    >>>> >     >>
    >>>> >     >> > ------------------------------------------------------------
    >>>> >     >> ----------------
    >>>> >     >>
    >>>> >     >> > (m <- cbind(a=1[0], b=2[0]))
    >>>> >     >> > Lm <- m; storage.mode(Lm) <- "logical"
    >>>> >     >> > Im <- m; storage.mode(Im) <- "integer"
    >>>> >     >>
    >>>> >     >> > ## 1. -------------------------
    >>>> >     >> > try( m & NULL ) # in R <= 3.3.x :
    >>>> >     >> > ## Error in m & NULL :
    >>>> >     >> > ##  operations are possible only for numeric, logical or
    >>>> complex
    >>>> >     >> types
    >>>> >     >> > ##
    >>>> >     >> > ## gives 'Lm' in R >= 3.4.0
    >>>> >     >>
    >>>> >     >> > ## 2. -------------------------
    >>>> >     >> > m + 2:3 ## gave numeric(0), now remains matrix identical to  m
    >>>> >     >> > Im + 2:3 ## gave integer(0), now remains matrix identical to
    >>>> Im
    >>>> >     >> (integer)
    >>>> >     >>
    >>>> >     >> > m > 1      ## gave logical(0), now remains matrix identical
    >>>> to Lm
    >>>> >     >> (logical)
    >>>> >     >> > m > 0.1[0] ##  ditto
    >>>> >     >> > m > NULL   ##  ditto
    >>>> >     >>
    >>>> >     >> > ## 3. -------------------------
    >>>> >     >> > mm <- m[,c(1:2,2:1,2)]
    >>>> >     >> > try( m == mm ) ## now gives error   "non-conformable arrays",
    >>>> >     >> > ## but gave logical(0) in R <= 3.3.x
    >>>> >     >>
    >>>> >     >> > ## 4. -------------------------
    >>>> >     >> > str( Im + NULL)  ## gave "num", now gives "int"
    >>>> >     >>
    >>>> >     >> > ## 5. -------------------------
    >>>> >     >> > ## special case for arithmetic w/ length-1 array
    >>>> >     >> > (m1 <- matrix(1,1,1, dimnames=list("Ro","col")))
    >>>> >     >> > (m2 <- matrix(1,2,1, dimnames=list(c("A","B"),"col")))
    >>>> >     >>
    >>>> >     >> > m1 + 1:2  # ->  2:3  but now with warning to  "become ERROR"
    >>>> >     >> > tools::assertError(m1 & 1:2)# ERR: dims [product 1] do not
    >>>> match
    >>>> > the
    >>>> >     >> length of object [2]
    >>>> >     >> > tools::assertError(m1 < 1:2)# ERR:                  (ditto)
    >>>> >     >> > ##
    >>>> >     >> > ## non-0-length arrays combined with {NULL or double() or ...}
    >>>> > *fail*
    >>>> >     >>
    >>>> >     >> > ### Length-1 arrays:  Arithmetic with |vectors| > 1  treated
    >>>> array
    >>>> >     >> as scalar
    >>>> >     >> > m1 + NULL # gave  numeric(0) in R <= 3.3.x --- still, *but* w/
    >>>> >     >> warning to "be ERROR"
    >>>> >     >> > try(m1 > NULL)    # gave  logical(0) in R <= 3.3.x --- an
    >>>> *error*
    >>>> >     >> now in R >= 3.4.0
    >>>> >     >> > tools::assertError(m1 & NULL)    # gave and gives error
    >>>> >     >> > tools::assertError(m1 | double())# ditto
    >>>> >     >> > ## m2 was slightly different:
    >>>> >     >> > tools::assertError(m2 + NULL)
    >>>> >     >> > tools::assertError(m2 & NULL)
    >>>> >     >> > try(m2 == NULL) ## was logical(0) in R <= 3.3.x; now error as
    >>>> > above!
    >>>> >     >>
    >>>> >     >> > ------------------------------------------------------------
    >>>> >     >> ----------------
    >>>> >     >>
    >>>> >     >>
    >>>> >     >> > Note that in R's own  'nls'  sources, there was one case of
    >>>> >     >> > situation '2)' above, i.e. a  1x1-matrix was used as a
    >>>> "scalar".
    >>>> >     >>
    >>>> >     >> > In such cases, you should explicitly coerce it to a vector,
    >>>> >     >> > either ("self-explainingly") by  as.vector(.), or as I did in
    >>>> >     >> > the nls case  by  c(.) :  The latter is much less
    >>>> >     >> > self-explaining, but nicer to read in mathematical formulae,
    >>>> and
    >>>> >     >> > currently also more efficient because it is a .Primitive.
    >>>> >     >>
    >>>> >     >> > Please use R-devel with your code, and let us know if you see
    >>>> >     >> > effects that seem adverse.
    >>>> >     >>
    >>>> >     >> I've been slightly surprised (or even "frustrated") by the empty
    >>>> >     >> reaction on our R-devel list to this post.
    >>>> >     >>
    >>>> >     >> I would have expected some critique, may be even some praise,
    >>>> >     >> ... in any case some sign people are "thinking along" (as we say
    >>>> >     >> in German).
    >>>> >     >>
    >>>> >     >> In the mean time, I've actually thought along the one case which
    >>>> >     >> is last above:  The <op>  (binary operation) between a
    >>>> >     >> non-0-length array and a 0-length vector (and NULL which should
    >>>> >     >> be treated like a 0-length vector):
    >>>> >     >>
    >>>> >     >> R <= 3.3.1  *is* quite inconsistent with these:
    >>>> >     >>
    >>>> >     >>
    >>>> >     >> and my proposal above (implemented in R-devel, since Sep.5)
    >>>> would
    >>>> > give an
    >>>> >     >> error for all these, but instead, R really could be more lenient
    >>>> > here:
    >>>> >     >> A 0-length result is ok, and it should *not* inherit the array
    >>>> >     >> (dim, dimnames), since the array is not of length 0. So instead
    >>>> >     >> of the above [for the very last part only!!], we would aim for
    >>>> >     >> the following. These *all* give an error in current R-devel,
    >>>> >     >> with the exception of 'm1 + NULL' which "only" gives a "bad
    >>>> >     >> warning" :
    >>>> >     >>
    >>>> >     >> ------------------------
    >>>> >     >>
    >>>> >     >> m1 <- matrix(1,1)
    >>>> >     >> m2 <- matrix(1,2)
    >>>> >     >>
    >>>> >     >> m1 + NULL #    numeric(0) in R <= 3.3.x ---> OK ?!
    >>>> >     >> m1 > NULL #    logical(0) in R <= 3.3.x ---> OK ?!
    >>>> >     >> try(m1 & NULL)    # ERROR in R <= 3.3.x ---> change to
    >>>> logical(0)
    >>>> > ?!
    >>>> >     >> try(m1 | double())# ERROR in R <= 3.3.x ---> change to
    >>>> logical(0)
    >>>> > ?!
    >>>> >     >> ## m2 slightly different:
    >>>> >     >> try(m2 + NULL)  # ERROR in R <= 3.3.x ---> change to double(0)
    >>>> ?!
    >>>> >     >> try(m2 & NULL)  # ERROR in R <= 3.3.x ---> change to
    >>>> logical(0)  ?!
    >>>> >     >> m2 == NULL # logical(0) in R <= 3.3.x ---> OK ?!
    >>>> >     >>
    >>>> >     >> ------------------------
    >>>> >     >>
    >>>> >     >> This would be slightly more back-compatible than the currently
    >>>> >     >> implemented proposal. Everything else I said remains true, and
    >>>> >     >> I'm pretty sure most changes needed in packages would remain to
    >>>> be
    >>>> > done.
    >>>> >     >>
    >>>> >     >> Opinions ?
    >>>> >     >>
    >>>> >     >>
    >>>> >     >>
    >>>> >     >> > In some case where R-devel now gives an error but did not
    >>>> >     >> > previously, we could contemplate giving another  "warning
    >>>> >     >> > .... 'to become ERROR'" if there was too much breakage,
    >>>> though
    >>>> >     >> > I don't expect that.
    >>>> >     >>
    >>>> >     >>
    >>>> >     >> > For the R Core Team,
    >>>> >     >>
    >>>> >     >> > Martin Maechler,
    >>>> >     >> > ETH Zurich
    >>>> >     >>
    >>>> >     >> ______________________________________________
    >>>> >     >> R-devel at r-project.org mailing list
    >>>> >     >> https://stat.ethz.ch/mailman/listinfo/r-devel
    >>>> >     >>
    >>>> >
    >>>> >
    >>>> >
    >>>> >     > --
    >>>> >     > Robin Hankin
    >>>> >     > Neutral theorist
    >>>> >     > hankin.robin at gmail.com
    >>>> >
    >>>> >     > [[alternative HTML version deleted]]
    >>>> >
    >>>> > ______________________________________________
    >>>> > R-devel at r-project.org mailing list
    >>>> > https://stat.ethz.ch/mailman/listinfo/r-devel
    >>>> >
    >>>> 
    >>>> 
    >>>> 
    >>>> --
    >>>> Gabriel Becker, PhD
    >>>> Associate Scientist (Bioinformatics)
    >>>> Genentech Research
    >>>> 
    >>>> [[alternative HTML version deleted]]
    >>>> 
    >>>> ______________________________________________
    >>>> R-devel at r-project.org mailing list
    >>>> https://stat.ethz.ch/mailman/listinfo/r-devel
    >>>> 
    >>> 
    >>> 
    >> 
    >> 
    >> --
    >> Gabriel Becker, PhD
    >> Associate Scientist (Bioinformatics)
    >> Genentech Research
    >> 

    > [[alternative HTML version deleted]]


From maechler at stat.math.ethz.ch  Fri Sep  9 10:35:04 2016
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 9 Sep 2016 10:35:04 +0200
Subject: [Rd] R (development) changes in arith, logic,
 relop with (0-extent) arrays
In-Reply-To: <20160908211118.GA17008@mail.cs.toronto.edu>
References: <mailman.27.1473328804.17880.r-devel@r-project.org>
	<20160908211118.GA17008@mail.cs.toronto.edu>
Message-ID: <22482.29752.663855.365479@stat.math.ethz.ch>

>>>>> Radford Neal <radford at cs.toronto.edu>
>>>>>     on Thu, 8 Sep 2016 17:11:18 -0400 writes:

    > Regarding Martin Maechler's proposal:
    > Arithmetic between length-1 arrays and longer non-arrays had
    > silently dropped the array attributes and recycled.  This now gives
    > a warning and will signal an error in the future, as it has always
    > for logic and comparison operations

    > For example, matrix(1,1,1) + (1:2) would give a warning/error.

    > I think this might be a mistake.

    > The potential benefits of this would be detection of some programming
    > errors, and increased consistency.  The downsides are breaking
    > existing working programs, and decreased consistency.

    > Regarding consistency, the overall R philosophy is that attaching an
    > attribute to a vector doesn't change what you can do with it, or what
    > the result is, except that the result (often) gets the attributes
    > carried forward.  By this logic, adding a 'dim' attribute shouldn't
    > stop you from doing arithmetic (or comparisons) that you otherwise
    > could.

Thank you, Radford, for joining in.
The above is a good line of reasoning.

    > But maybe 'dim' attributes are special?  Well, they are in some
    > circumstances, and have to be when they are intended to change the
    > behaviour, such as when a matrix is used as an index with [.

indeed.

    > But in many cases at present, 'dim' attributes DON'T stop you from
    > treating the object as a plain vector - for example, one is allowed 
    > to do matrix(1:4,2,2)[3], and a<-numeric(10); a[2:5]<-matrix(1,2,2).

agreed.

    > So it may make more sense to move towards consistency in the
    > permissive direction, rather than the restrictive direction.

    > That would mean allowing matrix(1,1,1) < (1:2), and maybe also things
    > like matrix(1,2,2)+(1:8).

That is an interesting idea.  Yes, in my view that would
definitely also have to allow the latter, by the above argument
of not treating the dim/dimnames attributes special.  For
non-arrays length-1 is not treated much special apart from the
fact that length-1 can always be recycled (without warning).


    > Obviously, a change that removes error conditions is much less likely
    > to produce backwards-compatibility problems than a change that gives
    > errors for previously-allowed operations.

Of course that is true... and that has also been the reason for
my amendment

    > And I think there would be some significant problems. In addition to
    > the 10-20+ packages that Martin expects to break, there could be quite
    > a bit of user code that would no longer work - scripts for analysing
    > data sets that used to work, but now don't with the latest version.

That's not true (at least for the cases above): They would give
a strong warning, "strong" because it is

   > matrix(1,1) + 1:2
   [1] 2 3
   Warning message:
   In matrix(1, 1) + 1:2 :
     dropping dim() of array of length one.  Will become ERROR
   > 

*and* the  logic and relop versions of this, e.g.,
   matrix(TRUE,1) | c(TRUE,FALSE) ;  matrix(1,1) > 1:2,
have always been an  error; so nothing would break there.


    > There are reasons to expect such problems.  Some operations such as
    > vector dot products using %*% produce results that are always scalar,
    > but are formed as 1x1 matrices.

Of course; that *was* the reason the very special treatment for arithmetic
length-1 arrays had been introduced.  It is convenient.

However, *some* of the conveniences in S (and hence R) functions
have been dangerous {and much more used, hence close to
impossible to abolish, e.g., sample(x) when x  is numeric of length 1,
and several others, you'll find in the "R Inferno"}, or at least
quirky for *programming* with R (as opposed to pure interactive use).

    > One can anticipate that many people
    > have not been getting rid of the 'dim' attribute in such cases, when
    > doing so hasn't been necessary in the past.

If it remains at 10-20 CRAN packages (out of 9000), each with
just very few instances, that would indicate I think not so wide
spread use.
Note that they only did not have to get rid of the dim() in the
length-1 case (and only for arithmetic): as soon as they had
another dimension, they would have got an error.

Still, I agree about the validity of your line of thought, and
that in order to get consistency we also could go into the
direction of being more permissive rather than restrictive.

I'm interested to hear other opinions notably as in recent years,
some famous R teachers have typically critized R are as being
not strict enough ...

    > Regarding the 0-length vector issue, I agree with other posters that
    > after a<-numeric(0), is has to be allowable to write a<1.  To not
    > allow this would be highly destructive of code reliability.  And for
    > similar reason, after a<-c(), a<1 needs to be allowed, which means
    > NULL<1 should be allowed (giving logical(0)), since c() is
    > NULL.

Yes, indeed, treating NULL the same as a length-0 atomic
vector here is also correct in my view, and maybe the fact you
mention that c() is NULL  does help to convince others.

Martin


From ramonfallon at gmail.com  Fri Sep  9 13:21:48 2016
From: ramonfallon at gmail.com (=?UTF-8?Q?Ram=C3=B3n_Fallon?=)
Date: Fri, 9 Sep 2016 12:21:48 +0100
Subject: [Rd] forgive possible repost: alternate bzip2 library
Message-ID: <CA+WKpEvm9LaErgWygwmSm3i5g9AxK=XUQHknp66rh6292o_u6w@mail.gmail.com>

Hi,

I am resubmitting a question, mainly because I suspect I may have
inadvertently cancelled it, while it was awaiting moderator approval.

It's about manually compiling R-3.3.1 and using, not the standard system's
(ver 1.0.5), but an alternate a bzip2 (v1.0.6) which is located in a
non-standard location.

I usually lean on pkg-config to deal with issues like this. I've create an
appropriate bzip2.pc file for bzip2 for this purpose.

I've been focusing on the configure script, which at 50K lines is quite
hefty and my initial question was whether the R compile configure script
used pkg-config for locating headers and libraries.

My feeling now is that the answer is "sometimes". In locating
cairographics, it says it does, but for bzip2 I think it uses autoconf's
dubious magic. Don't know if anybody can confirm this.

I could modify the configure so that it looks for bzip2 via pkg-config but
manual modification of this script is probably not proper practice, as well
as being very time consuming.

Any guidance appreaicted. Many thanks.

	[[alternative HTML version deleted]]


From edd at debian.org  Fri Sep  9 16:07:08 2016
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 9 Sep 2016 09:07:08 -0500
Subject: [Rd] forgive possible repost: alternate bzip2 library
In-Reply-To: <CA+WKpEvm9LaErgWygwmSm3i5g9AxK=XUQHknp66rh6292o_u6w@mail.gmail.com>
References: <CA+WKpEvm9LaErgWygwmSm3i5g9AxK=XUQHknp66rh6292o_u6w@mail.gmail.com>
Message-ID: <22482.49676.236572.603468@max.nulle.part>


On 9 September 2016 at 12:21, Ram?n Fallon wrote:
| I am resubmitting a question, mainly because I suspect I may have
| inadvertently cancelled it, while it was awaiting moderator approval.
| 
| It's about manually compiling R-3.3.1 and using, not the standard system's
| (ver 1.0.5), but an alternate a bzip2 (v1.0.6) which is located in a
| non-standard location.
| 
| I usually lean on pkg-config to deal with issues like this. I've create an
| appropriate bzip2.pc file for bzip2 for this purpose.
| 
| I've been focusing on the configure script, which at 50K lines is quite
| hefty and my initial question was whether the R compile configure script
| used pkg-config for locating headers and libraries.

You want configure.ac, the source, not configure, the 'compiled' output.
 
| My feeling now is that the answer is "sometimes". In locating
| cairographics, it says it does, but for bzip2 I think it uses autoconf's
| dubious magic. Don't know if anybody can confirm this.
| 
| I could modify the configure so that it looks for bzip2 via pkg-config but
| manual modification of this script is probably not proper practice, as well
| as being very time consuming.

As a (local) one-off, you could. You could continue to locally patch configure.

In general, pkg-config is good (and I just filed a request for another
library to support it).  Maybe here, you could work on a patch to
configure.ac to _first_ use pkg-config for bzip2, and, if no result was
found, fall back on the current behaviour.  It's tricky.  R has a very mature
and very well tested build system, so for all changes you want to make
/really/ sure you are not compromising any existing behaviour.

Dirk


-- 
http://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From radford at cs.toronto.edu  Fri Sep  9 16:29:14 2016
From: radford at cs.toronto.edu (Radford Neal)
Date: Fri, 9 Sep 2016 10:29:14 -0400
Subject: [Rd] R (development) changes in arith, logic,
 relop with (0-extent) arrays
In-Reply-To: <22482.29752.663855.365479@stat.math.ethz.ch>
References: <mailman.27.1473328804.17880.r-devel@r-project.org>
	<20160908211118.GA17008@mail.cs.toronto.edu>
	<22482.29752.663855.365479@stat.math.ethz.ch>
Message-ID: <20160909142914.GA32628@cs.toronto.edu>

>     Radford Nea:
>     > So it may make more sense to move towards consistency in the
>     > permissive direction, rather than the restrictive direction.
> 
>     > That would mean allowing matrix(1,1,1) < (1:2), and maybe also things
>     > like matrix(1,2,2)+(1:8).
> 
> Martin Maechler:
> That is an interesting idea.  Yes, in my view that would
> definitely also have to allow the latter, by the above argument
> of not treating the dim/dimnames attributes special.  For
> non-arrays length-1 is not treated much special apart from the
> fact that length-1 can always be recycled (without warning).

I think one could argue for allowing matrix(1,1,1)+(1:8) but not
matrix(1,2,2)+(1:8).  Length-1 vectors certainly are special in some
circumstances, being R's only way of representing a scalar.  For
instance, if (c(T,F)) gives a warning.

This really goes back to what I think may have been a basic mistake in
the design of S, in deciding that everything is a vector, then halfway
modifying this with dim attributes, but it's too late to totally undo
that (though allowing a 0-length dim attribute to explicitly mark a
length-1 vector as a scalar might help).

>     > And I think there would be some significant problems. In addition to
>     > the 10-20+ packages that Martin expects to break, there could be quite
>     > a bit of user code that would no longer work - scripts for analysing
>     > data sets that used to work, but now don't with the latest version.
> 
> That's not true (at least for the cases above): They would give
> a strong warning

But isn't the intent to make it an error later?  So I assume we're
debating making it an error, not just a warning.  (Though I'm
generally opposed to such warnings anyway, unless they could somehow
be restricted to come up only for interactive uses, not from deep in a
program the user didn't write, making them totally mysterious...)

> *and* the  logic and relop versions of this, e.g.,
>    matrix(TRUE,1) | c(TRUE,FALSE) ;  matrix(1,1) > 1:2,
> have always been an  error; so nothing would break there.

Yes, that wouldn't change the behaviour of old code, but if we're
aiming for consistencey, it might make sense to get rid of that error,
allowing code like sum(a%*%b<c(10,20,30)) with a and b being vectors,
rather than forcing the programmer to write sum(c(a%*%b)<c(10,20,30)).

> Of course; that *was* the reason the very special treatment for arithmetic
> length-1 arrays had been introduced.  It is convenient.
> 
> However, *some* of the conveniences in S (and hence R) functions
> have been dangerous {and much more used, hence close to
> impossible to abolish, e.g., sample(x) when x  is numeric of length 1,

There's a difference between these two.  Giving an error when using a
1x1 matrix as a scalar may detect some programming bugs, but not
giving an error doesn't introduce a bug.  Whereas sample(2:n) behaving
differently when n is 2 than when n is greater than 2 is itself a bug,
that the programmer has to consciously avoid by being aware of the quirk.

   Radford Neal


From ramonfallon at gmail.com  Fri Sep  9 17:49:37 2016
From: ramonfallon at gmail.com (=?UTF-8?Q?Ram=C3=B3n_Fallon?=)
Date: Fri, 9 Sep 2016 16:49:37 +0100
Subject: [Rd] forgive possible repost: alternate bzip2 library
In-Reply-To: <22482.49676.236572.603468@max.nulle.part>
References: <CA+WKpEvm9LaErgWygwmSm3i5g9AxK=XUQHknp66rh6292o_u6w@mail.gmail.com>
	<22482.49676.236572.603468@max.nulle.part>
Message-ID: <CA+WKpEuQn1yjMgB2-gNWx=PoHYqo=swcxUgqcbVM0=sKgcmEPQ@mail.gmail.com>

Dirk, thanks for your reply ... yes configure.ac, definitely. OK, mature
build system ... that's a warning indeed, though of course some dependency
libs already are treated like this, bitmaps/jpg for example: configure.ac

## Bitmap headers and libraries.
if test -n "${PKGCONF}"; then
R_BITMAPS2
else
R_BITMAPS
fi

(although the test here is existence of pkg-config, not if it finds
libraries). R_BITMAPS must be a set of macros of some sort. Will go out
hunting them now, unless anybody mentions where/how they live.

Cheers!


On Fri, Sep 9, 2016 at 3:07 PM, Dirk Eddelbuettel <edd at debian.org> wrote:

>
> On 9 September 2016 at 12:21, Ram?n Fallon wrote:
> | I am resubmitting a question, mainly because I suspect I may have
> | inadvertently cancelled it, while it was awaiting moderator approval.
> |
> | It's about manually compiling R-3.3.1 and using, not the standard
> system's
> | (ver 1.0.5), but an alternate a bzip2 (v1.0.6) which is located in a
> | non-standard location.
> |
> | I usually lean on pkg-config to deal with issues like this. I've create
> an
> | appropriate bzip2.pc file for bzip2 for this purpose.
> |
> | I've been focusing on the configure script, which at 50K lines is quite
> | hefty and my initial question was whether the R compile configure script
> | used pkg-config for locating headers and libraries.
>
> You want configure.ac, the source, not configure, the 'compiled' output.
>
> | My feeling now is that the answer is "sometimes". In locating
> | cairographics, it says it does, but for bzip2 I think it uses autoconf's
> | dubious magic. Don't know if anybody can confirm this.
> |
> | I could modify the configure so that it looks for bzip2 via pkg-config
> but
> | manual modification of this script is probably not proper practice, as
> well
> | as being very time consuming.
>
> As a (local) one-off, you could. You could continue to locally patch
> configure.
>
> In general, pkg-config is good (and I just filed a request for another
> library to support it).  Maybe here, you could work on a patch to
> configure.ac to _first_ use pkg-config for bzip2, and, if no result was
> found, fall back on the current behaviour.  It's tricky.  R has a very
> mature
> and very well tested build system, so for all changes you want to make
> /really/ sure you are not compromising any existing behaviour.
>
> Dirk
>
>
> --
> http://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org
>

	[[alternative HTML version deleted]]


From suharto_anggono at yahoo.com  Fri Sep  9 18:52:01 2016
From: suharto_anggono at yahoo.com (Suharto Anggono Suharto Anggono)
Date: Fri, 9 Sep 2016 16:52:01 +0000 (UTC)
Subject: [Rd] R-intro: function 'stderr' and 'sd'
References: <1979148586.1732979.1473439921401.ref@mail.yahoo.com>
Message-ID: <1979148586.1732979.1473439921401@mail.yahoo.com>

In "An Introduction to R" Version 3.3.1, in "4.2 The function tapply() and ragged arrays", after
stderr <- function(x) sqrt(var(x)/length(x))  ,
there is a note in brackets:
Writing functions will be considered later in [Writing your own functions], and in this case was unnecessary as R also has a builtin function sd().

The part "in this case was unnecessary as R also has a builtin function sd()" is misleading. The builtin function sd() doesn't calculate standard error of the mean. It calculates standard deviation. The function 'stderr' can use 'sd':
function(x) sd(x)/sqrt(length(x))


From hwborchers at gmail.com  Fri Sep  9 19:24:32 2016
From: hwborchers at gmail.com (Hans W Borchers)
Date: Fri, 9 Sep 2016 19:24:32 +0200
Subject: [Rd] Different results for tan(pi/2) and tanpi(1/2)
Message-ID: <CAML4n3NUKBMtXSt=naEktP4udKruenhG9N-Kdyx=sjpcx-BXvA@mail.gmail.com>

As the subject line says, we get different results for tan(pi/2) and
tanpi(1/2), though this should not be the case:

    > tan(pi/2)
    [1] 1.633124e+16

    > tanpi(1/2)
    [1] NaN
    Warning message:
    In tanpi(1/2) : NaNs produced

By redefining tanpi with sinpi and cospi, we can get closer:

    > tanpi <- function(x) sinpi(x) / cospi(x)

    > tanpi(c(0, 1/2, 1, 3/2, 2))
    [1]    0  Inf    0 -Inf    0

Hans Werner


From john.archie.mckown at gmail.com  Fri Sep  9 19:51:03 2016
From: john.archie.mckown at gmail.com (John McKown)
Date: Fri, 9 Sep 2016 12:51:03 -0500
Subject: [Rd] Different results for tan(pi/2) and tanpi(1/2)
In-Reply-To: <CAML4n3NUKBMtXSt=naEktP4udKruenhG9N-Kdyx=sjpcx-BXvA@mail.gmail.com>
References: <CAML4n3NUKBMtXSt=naEktP4udKruenhG9N-Kdyx=sjpcx-BXvA@mail.gmail.com>
Message-ID: <CAAJSdjjiQnHnPn4qO8goWm-5uvhz9iM7vCBSn2pfs-dU8+9pjg@mail.gmail.com>

On Fri, Sep 9, 2016 at 12:24 PM, Hans W Borchers <hwborchers at gmail.com>
wrote:

> As the subject line says, we get different results for tan(pi/2) and
> tanpi(1/2), though this should not be the case:
>
>     > tan(pi/2)
>     [1] 1.633124e+16
>
>     > tanpi(1/2)
>     [1] NaN
>     Warning message:
>     In tanpi(1/2) : NaNs produced
>
> By redefining tanpi with sinpi and cospi, we can get closer:
>
>     > tanpi <- function(x) sinpi(x) / cospi(x)
>
>     > tanpi(c(0, 1/2, 1, 3/2, 2))
>     [1]    0  Inf    0 -Inf    0
>
> Hans Werner
>
>
>
?When I do a ?tanpi, I see the following:

     ?tanpi(0.5)? is ?NaN?.  Similarly for other inputs with fractional
     part ?0.5?.
?


?I don't know why this is, but apparently the function is working as
documented. Whether that is correct or not is not for me to say.?



-- 
Unix: Some say the learning curve is steep, but you only have to climb it
once. -- Karl Lehenbauer
Unicode: http://xkcd.com/1726/

Maranatha! <><
John McKown

	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Fri Sep  9 20:44:27 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Fri, 9 Sep 2016 11:44:27 -0700
Subject: [Rd] Different results for tan(pi/2) and tanpi(1/2)
In-Reply-To: <CAML4n3NUKBMtXSt=naEktP4udKruenhG9N-Kdyx=sjpcx-BXvA@mail.gmail.com>
References: <CAML4n3NUKBMtXSt=naEktP4udKruenhG9N-Kdyx=sjpcx-BXvA@mail.gmail.com>
Message-ID: <CAF8bMcbA8aToZg1KezJh7WYOYh-S9rByxbsd6m_AvMJugNkn-g@mail.gmail.com>

It should be the case that tan(pi*x) != tanpi(x) in many cases - that is
why it was added.  The limits from below and below of the real function
tan(pi*x) as x approaches 1/2 are different, +Inf and -Inf, so the limit is
not well defined.   Hence the computer function tanpi(1/2) ought to return
Not-a-Number.

Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Fri, Sep 9, 2016 at 10:24 AM, Hans W Borchers <hwborchers at gmail.com>
wrote:

> As the subject line says, we get different results for tan(pi/2) and
> tanpi(1/2), though this should not be the case:
>
>     > tan(pi/2)
>     [1] 1.633124e+16
>
>     > tanpi(1/2)
>     [1] NaN
>     Warning message:
>     In tanpi(1/2) : NaNs produced
>
> By redefining tanpi with sinpi and cospi, we can get closer:
>
>     > tanpi <- function(x) sinpi(x) / cospi(x)
>
>     > tanpi(c(0, 1/2, 1, 3/2, 2))
>     [1]    0  Inf    0 -Inf    0
>
> Hans Werner
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From hwborchers at gmail.com  Fri Sep  9 20:55:43 2016
From: hwborchers at gmail.com (Hans W Borchers)
Date: Fri, 9 Sep 2016 20:55:43 +0200
Subject: [Rd] Different results for tan(pi/2) and tanpi(1/2)
In-Reply-To: <CAF8bMcbA8aToZg1KezJh7WYOYh-S9rByxbsd6m_AvMJugNkn-g@mail.gmail.com>
References: <CAML4n3NUKBMtXSt=naEktP4udKruenhG9N-Kdyx=sjpcx-BXvA@mail.gmail.com>
	<CAF8bMcbA8aToZg1KezJh7WYOYh-S9rByxbsd6m_AvMJugNkn-g@mail.gmail.com>
Message-ID: <CAML4n3P6duAC75ewwEtxaqsX6scygbstOjvn98AC7RUa0Mth=g@mail.gmail.com>

The same argument would hold for tan(pi/2).
I don't say the result 'NaN' is wrong,
but I thought,
tan(pi*x) and tanpi(x) should give the same result.

Hans Werner


On Fri, Sep 9, 2016 at 8:44 PM, William Dunlap <wdunlap at tibco.com> wrote:
> It should be the case that tan(pi*x) != tanpi(x) in many cases - that is why
> it was added.  The limits from below and below of the real function
> tan(pi*x) as x approaches 1/2 are different, +Inf and -Inf, so the limit is
> not well defined.   Hence the computer function tanpi(1/2) ought to return
> Not-a-Number.
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
> On Fri, Sep 9, 2016 at 10:24 AM, Hans W Borchers <hwborchers at gmail.com>
> wrote:
>>
>> As the subject line says, we get different results for tan(pi/2) and
>> tanpi(1/2), though this should not be the case:
>>
>>     > tan(pi/2)
>>     [1] 1.633124e+16
>>
>>     > tanpi(1/2)
>>     [1] NaN
>>     Warning message:
>>     In tanpi(1/2) : NaNs produced
>>
>> By redefining tanpi with sinpi and cospi, we can get closer:
>>
>>     > tanpi <- function(x) sinpi(x) / cospi(x)
>>
>>     > tanpi(c(0, 1/2, 1, 3/2, 2))
>>     [1]    0  Inf    0 -Inf    0
>>
>> Hans Werner
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>
>


From wdunlap at tibco.com  Fri Sep  9 21:05:44 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Fri, 9 Sep 2016 12:05:44 -0700
Subject: [Rd] Different results for tan(pi/2) and tanpi(1/2)
In-Reply-To: <CAML4n3P6duAC75ewwEtxaqsX6scygbstOjvn98AC7RUa0Mth=g@mail.gmail.com>
References: <CAML4n3NUKBMtXSt=naEktP4udKruenhG9N-Kdyx=sjpcx-BXvA@mail.gmail.com>
	<CAF8bMcbA8aToZg1KezJh7WYOYh-S9rByxbsd6m_AvMJugNkn-g@mail.gmail.com>
	<CAML4n3P6duAC75ewwEtxaqsX6scygbstOjvn98AC7RUa0Mth=g@mail.gmail.com>
Message-ID: <CAF8bMcYuUpLZ7hhwM81AVSMG6Dq-kOiJfOvGEW9mc2rL9P9wTQ@mail.gmail.com>

tanpi(x) should be more accurate than tan(pi*x), especially near multiples
of pi/2.

Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Fri, Sep 9, 2016 at 11:55 AM, Hans W Borchers <hwborchers at gmail.com>
wrote:

> The same argument would hold for tan(pi/2).
> I don't say the result 'NaN' is wrong,
> but I thought,
> tan(pi*x) and tanpi(x) should give the same result.
>
> Hans Werner
>
>
> On Fri, Sep 9, 2016 at 8:44 PM, William Dunlap <wdunlap at tibco.com> wrote:
> > It should be the case that tan(pi*x) != tanpi(x) in many cases - that is
> why
> > it was added.  The limits from below and below of the real function
> > tan(pi*x) as x approaches 1/2 are different, +Inf and -Inf, so the limit
> is
> > not well defined.   Hence the computer function tanpi(1/2) ought to
> return
> > Not-a-Number.
> >
> > Bill Dunlap
> > TIBCO Software
> > wdunlap tibco.com
> >
> > On Fri, Sep 9, 2016 at 10:24 AM, Hans W Borchers <hwborchers at gmail.com>
> > wrote:
> >>
> >> As the subject line says, we get different results for tan(pi/2) and
> >> tanpi(1/2), though this should not be the case:
> >>
> >>     > tan(pi/2)
> >>     [1] 1.633124e+16
> >>
> >>     > tanpi(1/2)
> >>     [1] NaN
> >>     Warning message:
> >>     In tanpi(1/2) : NaNs produced
> >>
> >> By redefining tanpi with sinpi and cospi, we can get closer:
> >>
> >>     > tanpi <- function(x) sinpi(x) / cospi(x)
> >>
> >>     > tanpi(c(0, 1/2, 1, 3/2, 2))
> >>     [1]    0  Inf    0 -Inf    0
> >>
> >> Hans Werner
> >>
> >> ______________________________________________
> >> R-devel at r-project.org mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-devel
> >
> >
>

	[[alternative HTML version deleted]]


From pgilbert902 at gmail.com  Fri Sep  9 21:11:30 2016
From: pgilbert902 at gmail.com (Paul Gilbert)
Date: Fri, 9 Sep 2016 15:11:30 -0400
Subject: [Rd] R (development) changes in arith, logic,
 relop with (0-extent) arrays
In-Reply-To: <CAHHjBM446huVPXRFZQMDbAugdS090FPjnkTEWYy1bcwCHoJnaw@mail.gmail.com>
References: <22469.52585.935914.336374@aragorn.wu.ac.at>
	<22470.38100.993865.828495@aragorn.wu.ac.at>
	<22473.36759.791968.814130@stat.math.ethz.ch>
	<22473.38325.834026.658339@stat.math.ethz.ch>
	<22474.35836.489860.992765@aragorn.wu.ac.at>
	<CAPRP4-eKdb1AKGt73P-+khNXFAWm_4q2PZr69Q7BAjWcXKpSzA@mail.gmail.com>
	<CAPRP4-e4fL=j2xUVMjO-kdmeTfvF=+=9AKrU9+981Xvhwc2MPA@mail.gmail.com>
	<22477.32999.558832.601073@stat.math.ethz.ch>
	<22479.9847.747220.924128@stat.math.ethz.ch>
	<22479.58007.358121.626142@stat.math.ethz.ch>
	<CAHHjBM6yvrgwOFiFAtM+w8BGmdwn114Z6gfyjMEsLQDLV8mA5A@mail.gmail.com>
	<22481.16979.861309.264630@stat.math.ethz.ch>
	<CADwqtCN2PnTFwsrV-K3=hXt64r5eHYNt5dg0OZZdEcDtPOtJuQ@mail.gmail.com>
	<CAF8bMcYBMduLcp_aQoTh74MBBKB9QnTDQG9J_dZyTrFkGd0azw@mail.gmail.com>
	<CADwqtCNXQpg6twfLv1LRscnMa=RND543Kgm1Xn-2APjgsOJWFA@mail.gmail.com>
	<4704b7c7-797c-a57b-6fc8-27e02920ff71@gmail.com>
	<CAHHjBM446huVPXRFZQMDbAugdS090FPjnkTEWYy1bcwCHoJnaw@mail.gmail.com>
Message-ID: <5603eadb-d116-8064-7cc1-e291b112398f@gmail.com>



On 09/08/2016 05:06 PM, robin hankin wrote:
> Could we take a cue from min() and max()?
>
>> x <- 1:10
>> min(x[x>7])
> [1] 8
>> min(x[x>11])
> [1] Inf
> Warning message:
> In min(x[x > 11]) : no non-missing arguments to min; returning Inf
>>
>
> As ?min says, this is implemented to preserve transitivity, and this
> makes a lot of sense.
> I think the issuing of a warning here is a good compromise; I can
> always turn off warnings if I want.

I fear you are thinking of this as an end user, rather than as a package 
developer. Warnings are for end users, when they do something they 
possibly should be warned about. A package really should not generate 
warnings unless they are for end user consumption. In package 
development I treat warnings the same way I treat errors: build fails, 
program around it. So what you call a compromise is no compromise at all 
as far as I am concerned.

But perhaps there is a use for an end user version, maybe All() or ALL() 
that issues an error or warning. There are a lot of functions and 
operators in R that could warn about mistakes that a user may be making.

Paul

>
> I find this behaviour of min() and max() to be annoying in the *right*
> way: it annoys me precisely when I need to be
> annoyed, that is, when I haven't thought through the consequences of
> sending zero-length arguments.
>
>
> On Fri, Sep 9, 2016 at 6:00 AM, Paul Gilbert <pgilbert902 at gmail.com> wrote:
>>
>>
>> On 09/08/2016 01:22 PM, Gabriel Becker wrote:
>>>
>>> On Thu, Sep 8, 2016 at 10:05 AM, William Dunlap <wdunlap at tibco.com> wrote:
>>>
>>>> Shouldn't binary operators (arithmetic and logical) should throw an error
>>>> when one operand is NULL (or other type that doesn't make sense)?  This
>>>> is
>>>> a different case than a zero-length operand of a legitimate type.  E.g.,
>>>>      any(x < 0)
>>>> should return FALSE if x is number-like and length(x)==0 but give an
>>>> error
>>>> if x is NULL.
>>>>
>>> Bill,
>>>
>>> That is a good point. I can see the argument for this in the case that the
>>> non-zero length is 1. I'm not sure which is better though. If we switch
>>> any() to all(), things get murky.
>>>
>>> Mathematically, all(x<0) is TRUE if x is length 0 (as are all(x==0), and
>>> all(x>0)), but the likelihood of this being a thought-bug on the author's
>>> part is exceedingly high, imho.
>>
>>
>> I suspect there may be more R users than you think that understand and use
>> vacuously true in code. I don't really like the idea of turning a perfectly
>> good and properly documented mathematical test into an error in order to
>> protect against a possible "thought-bug".
>>
>> Paul
>>
>>
>> So the desirable behavior seems to depend
>>>
>>> on the angle we look at it from.
>>>
>>> My personal opinion is that x < y with length(x)==0 should fail if
>>> length(y)
>>>>
>>>> 1, at least, and I'd be for it being an error even if y is length 1,
>>>
>>> though I do acknowledge this is more likely (though still quite unlikely
>>> imho) to be the intended behavior.
>>>
>>> ~G
>>>
>>>>
>>>> I.e., I think the type check should be done before the length check.
>>>>
>>>>
>>>> Bill Dunlap
>>>> TIBCO Software
>>>> wdunlap tibco.com
>>>>
>>>> On Thu, Sep 8, 2016 at 8:43 AM, Gabriel Becker <gmbecker at ucdavis.edu>
>>>> wrote:
>>>>
>>>>> Martin,
>>>>>
>>>>> Like Robin and Oliver I think this type of edge-case consistency is
>>>>> important and that it's fantastic that R-core - and you personally - are
>>>>> willing to tackle some of these "gotcha" behaviors. "Little" stuff like
>>>>> this really does combine to go a long way to making R better and better.
>>>>>
>>>>> I do wonder a  bit about the
>>>>>
>>>>> x = 1:2
>>>>>
>>>>> y = NULL
>>>>>
>>>>> x < y
>>>>>
>>>>> case.
>>>>>
>>>>> Returning a logical of length 0 is more backwards compatible, but is it
>>>>> ever what the author actually intended? I have trouble thinking of a
>>>>> case
>>>>> where that less-than didn't carry an implicit assumption that y was
>>>>> non-NULL.  I can say that in my own code, I've never hit that behavior
>>>>> in
>>>>> a
>>>>> case that wasn't an error.
>>>>>
>>>>> My vote (unless someone else points out a compelling use for the
>>>>> behavior)
>>>>> is for the to throw an error. As a developer, I'd rather things like
>>>>> this
>>>>> break so the bug in my logic is visible, rather than  propagating as the
>>>>> 0-length logical is &'ed or |'ed with other logical vectors, or used to
>>>>> subset, or (in the case it should be length 1) passed to if() (if throws
>>>>> an
>>>>> error now, but the rest would silently "work").
>>>>>
>>>>> Best,
>>>>> ~G
>>>>>
>>>>> On Thu, Sep 8, 2016 at 3:49 AM, Martin Maechler <
>>>>> maechler at stat.math.ethz.ch>
>>>>> wrote:
>>>>>
>>>>>>>>>>> robin hankin <hankin.robin at gmail.com>
>>>>>>>>>>>     on Thu, 8 Sep 2016 10:05:21 +1200 writes:
>>>>>>
>>>>>>
>>>>>>     > Martin I'd like to make a comment; I think that R's
>>>>>>     > behaviour on 'edge' cases like this is an important thing
>>>>>>     > and it's great that you are working on it.
>>>>>>
>>>>>>     > I make heavy use of zero-extent arrays, chiefly because
>>>>>>     > the dimnames are an efficient and logical way to keep
>>>>>>     > track of certain types of information.
>>>>>>
>>>>>>     > If I have, for example,
>>>>>>
>>>>>>     > a <- array(0,c(2,0,2))
>>>>>>     > dimnames(a) <- list(name=c('Mike','Kevin'),
>>>>>> NULL,item=c("hat","scarf"))
>>>>>>
>>>>>>
>>>>>>     > Then in R-3.3.1, 70800 I get
>>>>>>
>>>>>>     a> 0
>>>>>>     > logical(0)
>>>>>>     >>
>>>>>>
>>>>>>     > But in 71219 I get
>>>>>>
>>>>>>     a> 0
>>>>>>     > , , item = hat
>>>>>>
>>>>>>
>>>>>>     > name
>>>>>>     > Mike
>>>>>>     > Kevin
>>>>>>
>>>>>>     > , , item = scarf
>>>>>>
>>>>>>
>>>>>>     > name
>>>>>>     > Mike
>>>>>>     > Kevin
>>>>>>
>>>>>>     > (which is an empty logical array that holds the names of the
>>>>>
>>>>> people
>>>>>>
>>>>>> and
>>>>>>     > their clothes). I find the behaviour of 71219 very much
>>>>>> preferable
>>>>>> because
>>>>>>     > there is no reason to discard the information in the dimnames.
>>>>>>
>>>>>> Thanks a lot, Robin, (and Oliver) !
>>>>>>
>>>>>> Yes, the above is such a case where the new behavior makes much sense.
>>>>>> And this behavior remains identical after the 71222 amendment.
>>>>>>
>>>>>> Martin
>>>>>>
>>>>>>     > Best wishes
>>>>>>     > Robin
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>     > On Wed, Sep 7, 2016 at 9:49 PM, Martin Maechler <
>>>>>> maechler at stat.math.ethz.ch>
>>>>>>     > wrote:
>>>>>>
>>>>>>     >> >>>>> Martin Maechler <maechler at stat.math.ethz.ch>
>>>>>>     >> >>>>>     on Tue, 6 Sep 2016 22:26:31 +0200 writes:
>>>>>>     >>
>>>>>>     >> > Yesterday, changes to R's development version were committed,
>>>>>>     >> relating
>>>>>>     >> > to arithmetic, logic ('&' and '|') and
>>>>>>     >> > comparison/relational ('<', '==') binary operators
>>>>>>     >> > which in NEWS are described as
>>>>>>     >>
>>>>>>     >> > SIGNIFICANT USER-VISIBLE CHANGES:
>>>>>>     >>
>>>>>>     >> > [.............]
>>>>>>     >>
>>>>>>     >> > ? Arithmetic, logic (?&?, ?|?) and comparison (aka
>>>>>>     >> > ?relational?, e.g., ?<?, ?==?) operations with arrays now
>>>>>>     >> > behave consistently, notably for arrays of length zero.
>>>>>>     >>
>>>>>>     >> > Arithmetic between length-1 arrays and longer non-arrays had
>>>>>>     >> > silently dropped the array attributes and recycled.  This
>>>>>>     >> > now gives a warning and will signal an error in the future,
>>>>>>     >> > as it has always for logic and comparison operations in
>>>>>>     >> > these cases (e.g., compare ?matrix(1,1) + 2:3? and
>>>>>>     >> > ?matrix(1,1) < 2:3?).
>>>>>>     >>
>>>>>>     >> > As the above "visually suggests" one could think of the
>>>>>> changes
>>>>>>     >> > falling mainly two groups,
>>>>>>     >> > 1) <0-extent array>  (op)     <non-array>
>>>>>>     >> > 2) <1-extent array>  (arith)  <non-array of length != 1>
>>>>>>     >>
>>>>>>     >> > These changes are partly non-back compatible and may break
>>>>>>     >> > existing code.  We believe that the internal consistency
>>>>>> gained
>>>>>>     >> > from the changes is worth the few places with problems.
>>>>>>     >>
>>>>>>     >> > We expect some package maintainers (10-20, or even more?) need
>>>>>>     >> > to adapt their code.
>>>>>>     >>
>>>>>>     >> > Case '2)' above mainly results in a new warning, e.g.,
>>>>>>     >>
>>>>>>     >> >> matrix(1,1) + 1:2
>>>>>>     >> > [1] 2 3
>>>>>>     >> > Warning message:
>>>>>>     >> > In matrix(1, 1) + 1:2 :
>>>>>>     >> > dropping dim() of array of length one.  Will become ERROR
>>>>>>     >> >>
>>>>>>     >>
>>>>>>     >> > whereas '1)' gives errors in cases the result silently was a
>>>>>>     >> > vector of length zero, or also keeps array (dim & dimnames) in
>>>>>>     >> > cases these were silently dropped.
>>>>>>     >>
>>>>>>     >> > The following is a "heavily" commented  R script showing (all
>>>>>
>>>>> ?)
>>>>>>
>>>>>>     >> > the important cases with changes :
>>>>>>     >>
>>>>>>     >> > ------------------------------------------------------------
>>>>>>     >> ----------------
>>>>>>     >>
>>>>>>     >> > (m <- cbind(a=1[0], b=2[0]))
>>>>>>     >> > Lm <- m; storage.mode(Lm) <- "logical"
>>>>>>     >> > Im <- m; storage.mode(Im) <- "integer"
>>>>>>     >>
>>>>>>     >> > ## 1. -------------------------
>>>>>>     >> > try( m & NULL ) # in R <= 3.3.x :
>>>>>>     >> > ## Error in m & NULL :
>>>>>>     >> > ##  operations are possible only for numeric, logical or
>>>>>
>>>>> complex
>>>>>>
>>>>>>     >> types
>>>>>>     >> > ##
>>>>>>     >> > ## gives 'Lm' in R >= 3.4.0
>>>>>>     >>
>>>>>>     >> > ## 2. -------------------------
>>>>>>     >> > m + 2:3 ## gave numeric(0), now remains matrix identical to  m
>>>>>>     >> > Im + 2:3 ## gave integer(0), now remains matrix identical to
>>>>>> Im
>>>>>>     >> (integer)
>>>>>>     >>
>>>>>>     >> > m > 1      ## gave logical(0), now remains matrix identical to
>>>>>
>>>>> Lm
>>>>>>
>>>>>>     >> (logical)
>>>>>>     >> > m > 0.1[0] ##  ditto
>>>>>>     >> > m > NULL   ##  ditto
>>>>>>     >>
>>>>>>     >> > ## 3. -------------------------
>>>>>>     >> > mm <- m[,c(1:2,2:1,2)]
>>>>>>     >> > try( m == mm ) ## now gives error   "non-conformable arrays",
>>>>>>     >> > ## but gave logical(0) in R <= 3.3.x
>>>>>>     >>
>>>>>>     >> > ## 4. -------------------------
>>>>>>     >> > str( Im + NULL)  ## gave "num", now gives "int"
>>>>>>     >>
>>>>>>     >> > ## 5. -------------------------
>>>>>>     >> > ## special case for arithmetic w/ length-1 array
>>>>>>     >> > (m1 <- matrix(1,1,1, dimnames=list("Ro","col")))
>>>>>>     >> > (m2 <- matrix(1,2,1, dimnames=list(c("A","B"),"col")))
>>>>>>     >>
>>>>>>     >> > m1 + 1:2  # ->  2:3  but now with warning to  "become ERROR"
>>>>>>     >> > tools::assertError(m1 & 1:2)# ERR: dims [product 1] do not
>>>>>
>>>>> match
>>>>>>
>>>>>> the
>>>>>>     >> length of object [2]
>>>>>>     >> > tools::assertError(m1 < 1:2)# ERR:                  (ditto)
>>>>>>     >> > ##
>>>>>>     >> > ## non-0-length arrays combined with {NULL or double() or ...}
>>>>>> *fail*
>>>>>>     >>
>>>>>>     >> > ### Length-1 arrays:  Arithmetic with |vectors| > 1  treated
>>>>>
>>>>> array
>>>>>>
>>>>>>     >> as scalar
>>>>>>     >> > m1 + NULL # gave  numeric(0) in R <= 3.3.x --- still, *but* w/
>>>>>>     >> warning to "be ERROR"
>>>>>>     >> > try(m1 > NULL)    # gave  logical(0) in R <= 3.3.x --- an
>>>>>
>>>>> *error*
>>>>>>
>>>>>>     >> now in R >= 3.4.0
>>>>>>     >> > tools::assertError(m1 & NULL)    # gave and gives error
>>>>>>     >> > tools::assertError(m1 | double())# ditto
>>>>>>     >> > ## m2 was slightly different:
>>>>>>     >> > tools::assertError(m2 + NULL)
>>>>>>     >> > tools::assertError(m2 & NULL)
>>>>>>     >> > try(m2 == NULL) ## was logical(0) in R <= 3.3.x; now error as
>>>>>> above!
>>>>>>     >>
>>>>>>     >> > ------------------------------------------------------------
>>>>>>     >> ----------------
>>>>>>     >>
>>>>>>     >>
>>>>>>     >> > Note that in R's own  'nls'  sources, there was one case of
>>>>>>     >> > situation '2)' above, i.e. a  1x1-matrix was used as a
>>>>>
>>>>> "scalar".
>>>>>>
>>>>>>     >>
>>>>>>     >> > In such cases, you should explicitly coerce it to a vector,
>>>>>>     >> > either ("self-explainingly") by  as.vector(.), or as I did in
>>>>>>     >> > the nls case  by  c(.) :  The latter is much less
>>>>>>     >> > self-explaining, but nicer to read in mathematical formulae,
>>>>>
>>>>> and
>>>>>>
>>>>>>     >> > currently also more efficient because it is a .Primitive.
>>>>>>     >>
>>>>>>     >> > Please use R-devel with your code, and let us know if you see
>>>>>>     >> > effects that seem adverse.
>>>>>>     >>
>>>>>>     >> I've been slightly surprised (or even "frustrated") by the empty
>>>>>>     >> reaction on our R-devel list to this post.
>>>>>>     >>
>>>>>>     >> I would have expected some critique, may be even some praise,
>>>>>>     >> ... in any case some sign people are "thinking along" (as we say
>>>>>>     >> in German).
>>>>>>     >>
>>>>>>     >> In the mean time, I've actually thought along the one case which
>>>>>>     >> is last above:  The <op>  (binary operation) between a
>>>>>>     >> non-0-length array and a 0-length vector (and NULL which should
>>>>>>     >> be treated like a 0-length vector):
>>>>>>     >>
>>>>>>     >> R <= 3.3.1  *is* quite inconsistent with these:
>>>>>>     >>
>>>>>>     >>
>>>>>>     >> and my proposal above (implemented in R-devel, since Sep.5)
>>>>>> would
>>>>>> give an
>>>>>>     >> error for all these, but instead, R really could be more lenient
>>>>>> here:
>>>>>>     >> A 0-length result is ok, and it should *not* inherit the array
>>>>>>     >> (dim, dimnames), since the array is not of length 0. So instead
>>>>>>     >> of the above [for the very last part only!!], we would aim for
>>>>>>     >> the following. These *all* give an error in current R-devel,
>>>>>>     >> with the exception of 'm1 + NULL' which "only" gives a "bad
>>>>>>     >> warning" :
>>>>>>     >>
>>>>>>     >> ------------------------
>>>>>>     >>
>>>>>>     >> m1 <- matrix(1,1)
>>>>>>     >> m2 <- matrix(1,2)
>>>>>>     >>
>>>>>>     >> m1 + NULL #    numeric(0) in R <= 3.3.x ---> OK ?!
>>>>>>     >> m1 > NULL #    logical(0) in R <= 3.3.x ---> OK ?!
>>>>>>     >> try(m1 & NULL)    # ERROR in R <= 3.3.x ---> change to
>>>>>> logical(0)
>>>>>> ?!
>>>>>>     >> try(m1 | double())# ERROR in R <= 3.3.x ---> change to
>>>>>> logical(0)
>>>>>> ?!
>>>>>>     >> ## m2 slightly different:
>>>>>>     >> try(m2 + NULL)  # ERROR in R <= 3.3.x ---> change to double(0)
>>>>>
>>>>> ?!
>>>>>>
>>>>>>     >> try(m2 & NULL)  # ERROR in R <= 3.3.x ---> change to logical(0)
>>>>>
>>>>> ?!
>>>>>>
>>>>>>     >> m2 == NULL # logical(0) in R <= 3.3.x ---> OK ?!
>>>>>>     >>
>>>>>>     >> ------------------------
>>>>>>     >>
>>>>>>     >> This would be slightly more back-compatible than the currently
>>>>>>     >> implemented proposal. Everything else I said remains true, and
>>>>>>     >> I'm pretty sure most changes needed in packages would remain to
>>>>>
>>>>> be
>>>>>>
>>>>>> done.
>>>>>>     >>
>>>>>>     >> Opinions ?
>>>>>>     >>
>>>>>>     >>
>>>>>>     >>
>>>>>>     >> > In some case where R-devel now gives an error but did not
>>>>>>     >> > previously, we could contemplate giving another  "warning
>>>>>>     >> > .... 'to become ERROR'" if there was too much breakage,
>>>>>> though
>>>>>>     >> > I don't expect that.
>>>>>>     >>
>>>>>>     >>
>>>>>>     >> > For the R Core Team,
>>>>>>     >>
>>>>>>     >> > Martin Maechler,
>>>>>>     >> > ETH Zurich
>>>>>>     >>
>>>>>>     >> ______________________________________________
>>>>>>     >> R-devel at r-project.org mailing list
>>>>>>     >> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>>>     >>
>>>>>>
>>>>>>
>>>>>>
>>>>>>     > --
>>>>>>     > Robin Hankin
>>>>>>     > Neutral theorist
>>>>>>     > hankin.robin at gmail.com
>>>>>>
>>>>>>     > [[alternative HTML version deleted]]
>>>>>>
>>>>>> ______________________________________________
>>>>>> R-devel at r-project.org mailing list
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>>>
>>>>>
>>>>>
>>>>>
>>>>> --
>>>>> Gabriel Becker, PhD
>>>>> Associate Scientist (Bioinformatics)
>>>>> Genentech Research
>>>>>
>>>>>         [[alternative HTML version deleted]]
>>>>>
>>>>> ______________________________________________
>>>>> R-devel at r-project.org mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>>
>>>>
>>>>
>>>
>>>
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>
>
>


From gmbecker at ucdavis.edu  Fri Sep  9 21:15:08 2016
From: gmbecker at ucdavis.edu (Gabriel Becker)
Date: Fri, 9 Sep 2016 12:15:08 -0700
Subject: [Rd] R (development) changes in arith, logic,
 relop with (0-extent) arrays
In-Reply-To: <22482.23554.507321.387197@stat.math.ethz.ch>
References: <22469.52585.935914.336374@aragorn.wu.ac.at>
	<22470.37902.198447.925212@stat.math.ethz.ch>
	<22470.38100.993865.828495@aragorn.wu.ac.at>
	<22473.36759.791968.814130@stat.math.ethz.ch>
	<22473.38325.834026.658339@stat.math.ethz.ch>
	<22474.35836.489860.992765@aragorn.wu.ac.at>
	<CAPRP4-eKdb1AKGt73P-+khNXFAWm_4q2PZr69Q7BAjWcXKpSzA@mail.gmail.com>
	<CAPRP4-e4fL=j2xUVMjO-kdmeTfvF=+=9AKrU9+981Xvhwc2MPA@mail.gmail.com>
	<22477.32999.558832.601073@stat.math.ethz.ch>
	<22479.9847.747220.924128@stat.math.ethz.ch>
	<22479.58007.358121.626142@stat.math.ethz.ch>
	<CAHHjBM6yvrgwOFiFAtM+w8BGmdwn114Z6gfyjMEsLQDLV8mA5A@mail.gmail.com>
	<22481.16979.861309.264630@stat.math.ethz.ch>
	<CADwqtCN2PnTFwsrV-K3=hXt64r5eHYNt5dg0OZZdEcDtPOtJuQ@mail.gmail.com>
	<CAF8bMcYBMduLcp_aQoTh74MBBKB9QnTDQG9J_dZyTrFkGd0azw@mail.gmail.com>
	<CADwqtCNXQpg6twfLv1LRscnMa=RND543Kgm1Xn-2APjgsOJWFA@mail.gmail.com>
	<CAF8bMcY8d5Jduf3TtT39+vdD0sAz1fs8hqQ9x0Td9GF=mNGF+Q@mail.gmail.com>
	<22482.23554.507321.387197@stat.math.ethz.ch>
Message-ID: <CADwqtCN5cihoFnoqa0kFbFYmJS-j50kXhaFpNvk0LU_j7TWp7A@mail.gmail.com>

Martin et al.,

I seem to be in the minority here, so I won't belabor the point too much,
but one last response inline:

On Thu, Sep 8, 2016 at 11:51 PM, Martin Maechler <maechler at stat.math.ethz.ch
> wrote:

> Thank you, Gabe and Bill,
>
> for taking up the discussion.
>
> >>>>> William Dunlap <wdunlap at tibco.com>
> >>>>>     on Thu, 8 Sep 2016 10:45:07 -0700 writes:
>
>     > Prior to the mid-1990s, S did "length-0 OP length-n -> rep(NA, n)"
> and it
>     > was changed
>     > to "length-0 OP length-n -> length-0" to avoid lots of problems like
>     > any(x<0) being NA
>     > when length(x)==0.  Yes, people could code defensively by putting
> lots of
>     > if(length(x)==0)...
>     > in their code, but that is tedious and error-prone and creates
> really ugly
>     > code.
>
> Yes, so actually, basically
>
>      length-0 OP <anything>  -> length-0
>
> Now the case of NULL that Bill mentioned.
> I agree that NULL  is not at all the same thing as  double(0) or
> logical(0),
> *but* there have been quite a few cases, where NULL is the
> result of operations where "for consistency"  double(0) / logical(0)
> should have
> been.... and there are the users who use NULL as the equivalent
> of those, e.g., by initializing a (to be grown, yes, very inefficient!)
> vector with NULL instead of with say double(0).
>
> For these reasons, many operations that expect a "number-like"
> (includes logical) atomic vector have treated NULL as such...
> *and* parts of the {arith/logic/relop} OPs have done so already
> in R "forever".
> I still would argue that for these OPs, treating NULL as  logical(0) {which
> then may be promoted by the usual rules} is good thing.
>
>
>     > Is your suggestion to leave the length-0 OP length-1 case as it is
> but make
>     > length-0 OP length-two-or-higher an error or warning (akin to the
> length-2
>     > OP length-3 case)?
>
> That's exactly what one thing the current changes eliminated:
> arithmetic (only; not logic, or relop) did treat the length-1
> case (for arrays!) different from the length-GE-2 case.  And I strongly
> believe that this is very wrong and counter to the predominant
> recycling rules in (S and) R.
>

In my view, the recycling rules apply first and foremost to pairs of
vectors of lengths n,m >=1. And they can be semantically explained in that
case very easily: "the shorter, non-zero-length vector is rep'ed out to be
the length of the longer vector and then (generally) an element wise
operation takes place". The zero-length behavior already does not adhere to
this definition, as it would be impossible to do in the case of a
zero-length vector and a nonzero-length vector.

So the zero-length recycling behavior is already special-cased as I
understand it. In light of that, it seems that it would be allowable to
have different behavior based on the length of the other vector.
Furthermore, while I acknowledge the usefulness of the

x = numeric()

x <  5


case (i.e., the other vector is length 1), I can't come up with any use of,
e.g.,

y  = numeric()
y < 3:5


That I can make any sense of other than as a violation of implicit
assumptions by the coder about the length of y.

Thus, I still think that should at *least* warn, preferably (imho) give an
error.

Best,
~G


-- 
Gabriel Becker, PhD
Associate Scientist (Bioinformatics)
Genentech Research

	[[alternative HTML version deleted]]


From 538280 at gmail.com  Fri Sep  9 21:58:31 2016
From: 538280 at gmail.com (Greg Snow)
Date: Fri, 9 Sep 2016 13:58:31 -0600
Subject: [Rd] Different results for tan(pi/2) and tanpi(1/2)
In-Reply-To: <CAML4n3P6duAC75ewwEtxaqsX6scygbstOjvn98AC7RUa0Mth=g@mail.gmail.com>
References: <CAML4n3NUKBMtXSt=naEktP4udKruenhG9N-Kdyx=sjpcx-BXvA@mail.gmail.com>
	<CAF8bMcbA8aToZg1KezJh7WYOYh-S9rByxbsd6m_AvMJugNkn-g@mail.gmail.com>
	<CAML4n3P6duAC75ewwEtxaqsX6scygbstOjvn98AC7RUa0Mth=g@mail.gmail.com>
Message-ID: <CAFEqCdxVz0q-i7peOXTke3x1uDeBEpFftT9FgY+hx7z9ArTJ5Q@mail.gmail.com>

If pi were stored and computed to infinite precision then yes we would
expect tan(pi/2) to be NaN, but computers in general and R
specifically don't store to infinite precision (some packages allow
arbitrary (but still finite) precision) and irrational numbers cannot
be stored exactly.  So you take the value of the built in variable pi,
which is close to the theoretical value, but not exactly equal, divide
it by 2 which could reduce the precision, then pass that number (which
is not equal to the actual irrational value where tan has a
discontinuity) to tan and tan returns its best estimate.

Using finite precision approximations to irrational and other numbers
that cannot be stored exactly can have all types of problems at and
near certain values, that is why there are many specific functions for
calculating in those regions.





On Fri, Sep 9, 2016 at 12:55 PM, Hans W Borchers <hwborchers at gmail.com> wrote:
> The same argument would hold for tan(pi/2).
> I don't say the result 'NaN' is wrong,
> but I thought,
> tan(pi*x) and tanpi(x) should give the same result.
>
> Hans Werner
>
>
> On Fri, Sep 9, 2016 at 8:44 PM, William Dunlap <wdunlap at tibco.com> wrote:
>> It should be the case that tan(pi*x) != tanpi(x) in many cases - that is why
>> it was added.  The limits from below and below of the real function
>> tan(pi*x) as x approaches 1/2 are different, +Inf and -Inf, so the limit is
>> not well defined.   Hence the computer function tanpi(1/2) ought to return
>> Not-a-Number.
>>
>> Bill Dunlap
>> TIBCO Software
>> wdunlap tibco.com
>>
>> On Fri, Sep 9, 2016 at 10:24 AM, Hans W Borchers <hwborchers at gmail.com>
>> wrote:
>>>
>>> As the subject line says, we get different results for tan(pi/2) and
>>> tanpi(1/2), though this should not be the case:
>>>
>>>     > tan(pi/2)
>>>     [1] 1.633124e+16
>>>
>>>     > tanpi(1/2)
>>>     [1] NaN
>>>     Warning message:
>>>     In tanpi(1/2) : NaNs produced
>>>
>>> By redefining tanpi with sinpi and cospi, we can get closer:
>>>
>>>     > tanpi <- function(x) sinpi(x) / cospi(x)
>>>
>>>     > tanpi(c(0, 1/2, 1, 3/2, 2))
>>>     [1]    0  Inf    0 -Inf    0
>>>
>>> Hans Werner
>>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>>
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel



-- 
Gregory (Greg) L. Snow Ph.D.
538280 at gmail.com


From wdunlap at tibco.com  Fri Sep  9 22:12:58 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Fri, 9 Sep 2016 13:12:58 -0700
Subject: [Rd] Different results for tan(pi/2) and tanpi(1/2)
In-Reply-To: <CAFEqCdxVz0q-i7peOXTke3x1uDeBEpFftT9FgY+hx7z9ArTJ5Q@mail.gmail.com>
References: <CAML4n3NUKBMtXSt=naEktP4udKruenhG9N-Kdyx=sjpcx-BXvA@mail.gmail.com>
	<CAF8bMcbA8aToZg1KezJh7WYOYh-S9rByxbsd6m_AvMJugNkn-g@mail.gmail.com>
	<CAML4n3P6duAC75ewwEtxaqsX6scygbstOjvn98AC7RUa0Mth=g@mail.gmail.com>
	<CAFEqCdxVz0q-i7peOXTke3x1uDeBEpFftT9FgY+hx7z9ArTJ5Q@mail.gmail.com>
Message-ID: <CAF8bMcZoSCDB_5ah0bR1nU90bzD3AFZaxy1fE-H1r2naVyZtow@mail.gmail.com>

Other examples of functions like this are log1p(x), which is log(1+x)
accurate for small x, and expm1(x), which is exp(x)-1 accurate for small
x.  E.g.,
  > log1p( 1e-20 )
  [1] 1e-20
  > log( 1 + 1e-20 )
  [1] 0
log itself cannot be accurate here because the problem is that 1 == 1 +
1e-20 in double precision arithmetic (52 binary digits of precision).



Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Fri, Sep 9, 2016 at 12:58 PM, Greg Snow <538280 at gmail.com> wrote:

> If pi were stored and computed to infinite precision then yes we would
> expect tan(pi/2) to be NaN, but computers in general and R
> specifically don't store to infinite precision (some packages allow
> arbitrary (but still finite) precision) and irrational numbers cannot
> be stored exactly.  So you take the value of the built in variable pi,
> which is close to the theoretical value, but not exactly equal, divide
> it by 2 which could reduce the precision, then pass that number (which
> is not equal to the actual irrational value where tan has a
> discontinuity) to tan and tan returns its best estimate.
>
> Using finite precision approximations to irrational and other numbers
> that cannot be stored exactly can have all types of problems at and
> near certain values, that is why there are many specific functions for
> calculating in those regions.
>
>
>
>
>
> On Fri, Sep 9, 2016 at 12:55 PM, Hans W Borchers <hwborchers at gmail.com>
> wrote:
> > The same argument would hold for tan(pi/2).
> > I don't say the result 'NaN' is wrong,
> > but I thought,
> > tan(pi*x) and tanpi(x) should give the same result.
> >
> > Hans Werner
> >
> >
> > On Fri, Sep 9, 2016 at 8:44 PM, William Dunlap <wdunlap at tibco.com>
> wrote:
> >> It should be the case that tan(pi*x) != tanpi(x) in many cases - that
> is why
> >> it was added.  The limits from below and below of the real function
> >> tan(pi*x) as x approaches 1/2 are different, +Inf and -Inf, so the
> limit is
> >> not well defined.   Hence the computer function tanpi(1/2) ought to
> return
> >> Not-a-Number.
> >>
> >> Bill Dunlap
> >> TIBCO Software
> >> wdunlap tibco.com
> >>
> >> On Fri, Sep 9, 2016 at 10:24 AM, Hans W Borchers <hwborchers at gmail.com>
> >> wrote:
> >>>
> >>> As the subject line says, we get different results for tan(pi/2) and
> >>> tanpi(1/2), though this should not be the case:
> >>>
> >>>     > tan(pi/2)
> >>>     [1] 1.633124e+16
> >>>
> >>>     > tanpi(1/2)
> >>>     [1] NaN
> >>>     Warning message:
> >>>     In tanpi(1/2) : NaNs produced
> >>>
> >>> By redefining tanpi with sinpi and cospi, we can get closer:
> >>>
> >>>     > tanpi <- function(x) sinpi(x) / cospi(x)
> >>>
> >>>     > tanpi(c(0, 1/2, 1, 3/2, 2))
> >>>     [1]    0  Inf    0 -Inf    0
> >>>
> >>> Hans Werner
> >>>
> >>> ______________________________________________
> >>> R-devel at r-project.org mailing list
> >>> https://stat.ethz.ch/mailman/listinfo/r-devel
> >>
> >>
> >
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
>
>
>
> --
> Gregory (Greg) L. Snow Ph.D.
> 538280 at gmail.com
>

	[[alternative HTML version deleted]]


From suharto_anggono at yahoo.com  Sat Sep 10 04:36:54 2016
From: suharto_anggono at yahoo.com (Suharto Anggono Suharto Anggono)
Date: Sat, 10 Sep 2016 02:36:54 +0000 (UTC)
Subject: [Rd] table(exclude = NULL) always includes NA
References: <485115119.1895185.1473475014687.ref@mail.yahoo.com>
Message-ID: <485115119.1895185.1473475014687@mail.yahoo.com>

Looking at the code of function 'table' in R devel r71227, I see that the part "remove NA level if it was added only for excluded in factor(a, exclude=.)" is not quite right.

In
		is.na(a) <- match(a0, c(exclude,NA), nomatch=0L)   ,
I think that what is intended is
                a[a0 %in% c(exclude,NA)] <- NA  .
So, it should be
		is.na(a) <- match(a0, c(exclude,NA), nomatch=0L) > 0L
or
		is.na(a) <- as.logical(match(a0, c(exclude,NA), nomatch=0L))  .
The parallel code
		is.na(a) <- match(a0,   exclude,     nomatch=0L)
is to be treated similarly.

Example that gives wrong result in R devel r71225:
table(3:1, exclude = 1)
table(3:1, exclude = 1, useNA = "always")
--------------------------------------------
On Tue, 16/8/16, Martin Maechler <maechler at stat.math.ethz.ch> wrote:

 Subject: Re: [Rd] table(exclude = NULL) always includes NA

 Cc: "Martin Maechler" <maechler at stat.math.ethz.ch>
 Date: Tuesday, 16 August, 2016, 5:42 PM

>>>>> Martin Maechler <maechler at stat.math.ethz.ch>
>>>>>     on Mon, 15 Aug 2016 12:35:41 +0200 writes:

>>>>> Martin Maechler <maechler at stat.math.ethz.ch>
>>>>>     on Mon, 15 Aug 2016 11:07:43 +0200 writes:


>>>>>     on Sun, 14 Aug 2016 03:42:08 +0000 writes:

    >>> useNA <- if (missing(useNA) && !missing(exclude) && !(NA %in% exclude)) "ifany"
    >>> An example where it change 'table' result for non-factor input, from https://stat.ethz.ch/pipermail/r-help/2005-April/069053.html :

    >>> x <- c(1,2,3,3,NA)
    >>> table(as.integer(x), exclude=NaN)

    >>> I bring the example up, in case that the change in result is not intended.

    >> Thanks a lot, Suharto.

    >> To me, the example is convincing that the change (I commited
    >> Friday), svn rev 71087 & 71088,   are a clear improvement:

    >> (As you surely know, but not all the other readers:)
    >> Before the change, the above example gave *different* results
    >> for  'x'  and  'as.integer(x)', the integer case *not* counting the NAs,
    >> whereas with the change in effect, they are the same:

    >>> x <- as.integer(dx <- c(1,2,3,3,NA))
    >>> table(x, exclude=NaN); table(dx, exclude=NaN)
    >> x
    >> 1    2    3 <NA> 
    >> 1    1    2    1 
    >> dx
    >> 1    2    3 <NA> 
    >> 1    1    2    1 
    >>> 

    >> --
    >> But the change has affected 6-8 (of the 8000+) CRAN packages
    >> which I am investigating now and probably will be in contact with the
    >> package maintainers after that.

    > There has been another bug in table(), since the time  'useNA'
    > was introduced, which gives (in released R, R-patched, or R-devel):

    >> table(1:3, exclude = 1, useNA = "ifany")

    > 2    3 <NA> 
    > 1    1    1 
    >> 

    > and that bug now (in R-devel, after my changes) triggers in
    > cases it did not previously, notably in

    > table(1:3, exclude = 1)

    > which now does set 'useNA = "ifany"' and so gives the same silly
    > result as the one above.

    > The reason for this bug is that   addNA(..)  is called (in all R
    > versions mentioned) in this case, but it should not.

    > I'm currently testing yet another amendment..

which was not sufficient... so I had to do *much* more work.

The result is code which functions -- I hope -- uniformly better
than the current code, but unfortunately, code that is much longer.

After all I came to the conclusion that using addNA() was not
good enough [I did not yet consider *changing* addNA() itself,
even though the only place we use it in R's own packages is
inside table()] and so for now have code in table() that does
the equivalent of addNA() *but* does remember if addNA() did add
an NA level or not.
I also have extended the regression tests considerably,
*and*  example(table)  now reverts to give identical output to
R 3.3.1 (which it did no longer in R-devel (r 71088)).

I'm still investigating the CRAN package fallout (from the above
change 4 days ago) but plan to commit my (unfortunately
somewhat extensive) changes.

Also, I think this will become the first in this year's R-devel

SIGNIFICANT USER-VISIBLE CHANGES:

  ? ?table()? has been amended to be more internally consistent
    and become back compatible to R <= 2.7.2 again.
    Consequently, ?table(1:2, exclude=NULL)? no longer contains
    a zero count for ?<NA>?, but ?useNA = "always"? continues to
    do so.


--
Martin


From Andrew.Redd at hsc.utah.edu  Sat Sep 10 01:02:21 2016
From: Andrew.Redd at hsc.utah.edu (Andrew Redd)
Date: Fri, 9 Sep 2016 23:02:21 +0000
Subject: [Rd] Announcing the R Documentation Task Force
Message-ID: <2c85a36a-9160-af19-6d80-e0161c073b47@hsc.utah.edu>

cross-posting announcement to R-Announce, R-devel and R-package-devel.

The R Consortium recently announced 
(https://www.r-consortium.org/news/blogs/2016/08/r-consortium-funds-three-projects-july) 
support of the R Documentation Task Force.  The task force aims to 
design and implement the next generation documentation system for R.  We 
aim to take the best from the many attempts to improve documentation and 
unify them into a system that will be more standard, flexible and 
powerful.  We have the support and participation of R Core Developers.  
The full proposal is available on the announcement website given above.

If you have expertise in documentation systems or documentation of 
objects or interest in building a documentation system we invite you to 
participate.  We will be considering documentation in all common forms 
such as function and class documentation, but also areas such as data 
frames, non-standard objects, packages, and how vignettes fit into the 
documentation framework.  If you have opinions or concerns that you 
would like to make sure are addressed please join.

To join send an email to Andrew dot Redd at hsc dot utah dot edu, 
expressing your interests, skills or expertise as it relates to R 
documentation.  Also email if you have ideas or concerns but do not wish 
to play and active role.

Thank you,
Andrew Redd

From maechler at stat.math.ethz.ch  Sat Sep 10 17:27:24 2016
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 10 Sep 2016 17:27:24 +0200
Subject: [Rd] c(<Matrix>, <Matrix>) / help(dotsMethods) etc
In-Reply-To: <FDA7E69C-303D-414A-96A3-55008150BC7A@illinois.edu>
References: <FDA7E69C-303D-414A-96A3-55008150BC7A@illinois.edu>
Message-ID: <22484.9820.875895.113534@stat.math.ethz.ch>

I have been asked  (by Roger; thank you for the good question,
       	    	    and I hope it's fine to answer to the public) :
       
    > with Pi a sparse matrix and x,y, and ones
    > compatible n-vectors ? when I do:

    >> c(t(x) %*% Pi %*% ones, t(ones) %*% Pi %*% y )
    > [[1]] 1 x 1 Matrix of class "dgeMatrix"
    > [,1] [1,]
    > 0.1338527
    > [[2]] 1 x 1 Matrix of class "dgeMatrix"
     [,1] [1,]
    > 0.7810341

    > I get a list whereas if Pi is an ordinary matrix I get a
    > vector.  Is this intentional?

Well, no.  But it has been "unavoidable" in the sense that it had not
been possible to provide S4 methods for '...' in the "remote"
past, when  Matrix was created.

Later ... also quite a few years ago, John Chambers had added
that possibility, with still some limitation (all '...' must be
of the same class), and also plans to remove some of the
limitations, see   ?dotsMethods  in R.

I honestly have forgotten the history of my trying to provide 'c'
methods for our "Matrix" objects after the  'dotsMethods'
possibility had emerged,  but I know I tried and had not seen a
way to succeed "satisfactorily",
but maybe I now think I maybe should try again.
I currently think this needs changes to R before it can be done
satisfactorily, and this is the main reason why this is a public
answer to R-devel at ..., but I'm happy if I'am wrong.

The real challenge here is that I think that if it  should "work",
it should work so in all cases, e.g., also for

    c(NA, 3:2, Matrix(2:1), matrix(10:11))

and that's not so easy, e.g., the following class and method
definitions do *not* achieve the desired result:

## "mMatrix" is already hidden in Matrix pkg:
setClassUnion("mMatrix", members = c("matrix", "Matrix"))
setClassUnion("numMatrixLike", members =
                c("logical", "integer","numeric", "mMatrix"))

c.Matrix <- function(...) unlist(lapply(list(...), as.vector))
## NB: Must use   signature  '(x, ..., recursive = FALSE)' :
setMethod("c", "Matrix", function(x, ..., recursive) c.Matrix(x,
...))
## The above is not sufficient for
##    c(NA, 3:2, <Matrix>, <matrix>) :
setMethod("c", "numMatrixLike", function(x, ..., recursive)
   c.Matrix(x, ...))

## but the above does not really help:

> c(Diagonal(3), NA, Matrix(10:11))   ## works fine,
 [1]  1  0  0  0  1  0  0  0  1 NA 10 11

> c(NA, Diagonal(3)) ## R's lowlevel c() already decided to use list():
[[1]]
 [1] NA

[[2]]
     [,1] [,2] [,3]
     [1,]    1    .    .
     [2,]    .    1    .
     [3,]    .    .    1

>
----------------------------------------------

BTW, I (and the package users) suffer from exactly the same
problem with the "MPFR" (multi precision numbers) provided by my
package Rmpfr:

> require(Rmpfr)
> c(mpfr(3,100), 1/mpfr(7, 80)) ## works fine
2 'mpfr' numbers of precision  80 .. 100  bits
[1]                            3 0.14285714285714285714285708

> c(pi, 1/mpfr(7, 80)) ## "fails" even worse than in 'Matrix' case
[[1]]
[1] 3.141593

[[2]]
'mpfr1' 0.14285714285714285714285708

> 


Yes, it would be very nice  if  c(.)  could be used to
concatenate quite arbitrary  R objects into one long atomic
vector, but I don't see how to achieve this easily.

The fact, that  c()  just builds a list of its arguments if it
("thinks" it) cannot dispatch to a method, is a good strategy,
but I'd hope it should be possible to have c() try to do better
(and hence work for this case, and
without a noticable performance penalty.

Suggestions are very welcome.
Martin


From jmc at r-project.org  Sat Sep 10 18:16:38 2016
From: jmc at r-project.org (John Chambers)
Date: Sat, 10 Sep 2016 09:16:38 -0700
Subject: [Rd] c(<Matrix>, <Matrix>) / help(dotsMethods) etc
In-Reply-To: <22484.9820.875895.113534@stat.math.ethz.ch>
References: <FDA7E69C-303D-414A-96A3-55008150BC7A@illinois.edu>
	<22484.9820.875895.113534@stat.math.ethz.ch>
Message-ID: <9B81AE92-C400-4FBB-9C8E-A774F22402CD@r-project.org>

(Brief reply, I'm traveling but as per below, this is on my radar right now so wanted to comment.)

Two points regarding "dotsMethods".

1.  To clarify the limitation.  It's not that all the arguments have to be of the same class, but there must be one class that they belong to or subclass.  (So, as in the example in the documentation, the method could be defined for a class union or other virtual class that all the actual arguments inherit from.)

2.  The current documentation is a mess.  In common with lots of other very old documentation.  I'm in the process of rewriting a large chunk of the documentation including that for dotsMethods.  Sometime in the next few weeks, I hope to have it coherent enough to commit.

So far, I'm not trying to change any significant aspects of the code, including for "..." methods, which seem to do roughly what was intended.

John


On Sep 10, 2016, at 8:27 AM, Martin Maechler <maechler at stat.math.ethz.ch> wrote:

> I have been asked  (by Roger; thank you for the good question,
>       	    	    and I hope it's fine to answer to the public) :
> 
>> with Pi a sparse matrix and x,y, and ones
>> compatible n-vectors ? when I do:
> 
>>> c(t(x) %*% Pi %*% ones, t(ones) %*% Pi %*% y )
>> [[1]] 1 x 1 Matrix of class "dgeMatrix"
>> [,1] [1,]
>> 0.1338527
>> [[2]] 1 x 1 Matrix of class "dgeMatrix"
>     [,1] [1,]
>> 0.7810341
> 
>> I get a list whereas if Pi is an ordinary matrix I get a
>> vector.  Is this intentional?
> 
> Well, no.  But it has been "unavoidable" in the sense that it had not
> been possible to provide S4 methods for '...' in the "remote"
> past, when  Matrix was created.
> 
> Later ... also quite a few years ago, John Chambers had added
> that possibility, with still some limitation (all '...' must be
> of the same class), and also plans to remove some of the
> limitations, see   ?dotsMethods  in R.
> 
> I honestly have forgotten the history of my trying to provide 'c'
> methods for our "Matrix" objects after the  'dotsMethods'
> possibility had emerged,  but I know I tried and had not seen a
> way to succeed "satisfactorily",
> but maybe I now think I maybe should try again.
> I currently think this needs changes to R before it can be done
> satisfactorily, and this is the main reason why this is a public
> answer to R-devel at ..., but I'm happy if I'am wrong.
> 
> The real challenge here is that I think that if it  should "work",
> it should work so in all cases, e.g., also for
> 
>    c(NA, 3:2, Matrix(2:1), matrix(10:11))
> 
> and that's not so easy, e.g., the following class and method
> definitions do *not* achieve the desired result:
> 
> ## "mMatrix" is already hidden in Matrix pkg:
> setClassUnion("mMatrix", members = c("matrix", "Matrix"))
> setClassUnion("numMatrixLike", members =
>                c("logical", "integer","numeric", "mMatrix"))
> 
> c.Matrix <- function(...) unlist(lapply(list(...), as.vector))
> ## NB: Must use   signature  '(x, ..., recursive = FALSE)' :
> setMethod("c", "Matrix", function(x, ..., recursive) c.Matrix(x,
> ...))
> ## The above is not sufficient for
> ##    c(NA, 3:2, <Matrix>, <matrix>) :
> setMethod("c", "numMatrixLike", function(x, ..., recursive)
>   c.Matrix(x, ...))
> 
> ## but the above does not really help:
> 
>> c(Diagonal(3), NA, Matrix(10:11))   ## works fine,
> [1]  1  0  0  0  1  0  0  0  1 NA 10 11
> 
>> c(NA, Diagonal(3)) ## R's lowlevel c() already decided to use list():
> [[1]]
> [1] NA
> 
> [[2]]
>     [,1] [,2] [,3]
>     [1,]    1    .    .
>     [2,]    .    1    .
>     [3,]    .    .    1
> 
>> 
> ----------------------------------------------
> 
> BTW, I (and the package users) suffer from exactly the same
> problem with the "MPFR" (multi precision numbers) provided by my
> package Rmpfr:
> 
>> require(Rmpfr)
>> c(mpfr(3,100), 1/mpfr(7, 80)) ## works fine
> 2 'mpfr' numbers of precision  80 .. 100  bits
> [1]                            3 0.14285714285714285714285708
> 
>> c(pi, 1/mpfr(7, 80)) ## "fails" even worse than in 'Matrix' case
> [[1]]
> [1] 3.141593
> 
> [[2]]
> 'mpfr1' 0.14285714285714285714285708
> 
>> 
> 
> 
> Yes, it would be very nice  if  c(.)  could be used to
> concatenate quite arbitrary  R objects into one long atomic
> vector, but I don't see how to achieve this easily.
> 
> The fact, that  c()  just builds a list of its arguments if it
> ("thinks" it) cannot dispatch to a method, is a good strategy,
> but I'd hope it should be possible to have c() try to do better
> (and hence work for this case, and
> without a noticable performance penalty.
> 
> Suggestions are very welcome.
> Martin
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From maechler at stat.math.ethz.ch  Sat Sep 10 19:49:03 2016
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 10 Sep 2016 19:49:03 +0200
Subject: [Rd] table(exclude = NULL) always includes NA
In-Reply-To: <485115119.1895185.1473475014687@mail.yahoo.com>
References: <485115119.1895185.1473475014687.ref@mail.yahoo.com>
	<485115119.1895185.1473475014687@mail.yahoo.com>
Message-ID: <22484.18319.99371.446681@stat.math.ethz.ch>

>>>>> Suharto Anggono Suharto Anggono <suharto_anggono at yahoo.com>
>>>>>     on Sat, 10 Sep 2016 02:36:54 +0000 writes:

    > Looking at the code of function 'table' in R devel r71227, I see that the part "remove NA level if it was added only for excluded in factor(a, exclude=.)" is not quite right.
    > In
    > is.na(a) <- match(a0, c(exclude,NA), nomatch=0L)   ,
    > I think that what is intended is
    > a[a0 %in% c(exclude,NA)] <- NA  .
yes.
    > So, it should be
    >   is.na(a) <- match(a0, c(exclude,NA), nomatch=0L) > 0L
    > or
    >   is.na(a) <- as.logical(match(a0, c(exclude,NA), nomatch=0L))  .
    > The parallel code
    >    is.na(a) <- match(a0,   exclude,     nomatch=0L)
    > is to be treated similarly.

indeed.  I may have been  very wrongly thinking that `is.na<-`
coerced its value to logical... or otherwise not thinking at all ;-)


    > Example that gives wrong result in R devel r71225:
    > table(3:1, exclude = 1)
    > table(3:1, exclude = 1, useNA = "always")
    > --------------------------------------------

Thanks a lot, Suharto.   You are entirely correct.

I'm amazed that  table(*, exclude = *)  seems so rarely used / tested,
that this has gone undetected for almost four weeks.
It is fixed now with svn r71230.

Martin


From lawrence.michael at gene.com  Sat Sep 10 20:16:05 2016
From: lawrence.michael at gene.com (Michael Lawrence)
Date: Sat, 10 Sep 2016 11:16:05 -0700
Subject: [Rd] c(<Matrix>, <Matrix>) / help(dotsMethods) etc
In-Reply-To: <22484.9820.875895.113534@stat.math.ethz.ch>
References: <FDA7E69C-303D-414A-96A3-55008150BC7A@illinois.edu>
	<22484.9820.875895.113534@stat.math.ethz.ch>
Message-ID: <CAOQ5NydF5hj98cweYx7XDN6vQQOYAvEnY7bRf=-V-c_x7YTziw@mail.gmail.com>

One option would be to use the same strategy that we use for cbind()
and rbind(), i.e., if dispatch fails, call a binary generic, c2(),
recursively. Could do the same for pmin() and pmax().

Michael

On Sat, Sep 10, 2016 at 8:27 AM, Martin Maechler
<maechler at stat.math.ethz.ch> wrote:
> I have been asked  (by Roger; thank you for the good question,
>                     and I hope it's fine to answer to the public) :
>
>     > with Pi a sparse matrix and x,y, and ones
>     > compatible n-vectors ? when I do:
>
>     >> c(t(x) %*% Pi %*% ones, t(ones) %*% Pi %*% y )
>     > [[1]] 1 x 1 Matrix of class "dgeMatrix"
>     > [,1] [1,]
>     > 0.1338527
>     > [[2]] 1 x 1 Matrix of class "dgeMatrix"
>      [,1] [1,]
>     > 0.7810341
>
>     > I get a list whereas if Pi is an ordinary matrix I get a
>     > vector.  Is this intentional?
>
> Well, no.  But it has been "unavoidable" in the sense that it had not
> been possible to provide S4 methods for '...' in the "remote"
> past, when  Matrix was created.
>
> Later ... also quite a few years ago, John Chambers had added
> that possibility, with still some limitation (all '...' must be
> of the same class), and also plans to remove some of the
> limitations, see   ?dotsMethods  in R.
>
> I honestly have forgotten the history of my trying to provide 'c'
> methods for our "Matrix" objects after the  'dotsMethods'
> possibility had emerged,  but I know I tried and had not seen a
> way to succeed "satisfactorily",
> but maybe I now think I maybe should try again.
> I currently think this needs changes to R before it can be done
> satisfactorily, and this is the main reason why this is a public
> answer to R-devel at ..., but I'm happy if I'am wrong.
>
> The real challenge here is that I think that if it  should "work",
> it should work so in all cases, e.g., also for
>
>     c(NA, 3:2, Matrix(2:1), matrix(10:11))
>
> and that's not so easy, e.g., the following class and method
> definitions do *not* achieve the desired result:
>
> ## "mMatrix" is already hidden in Matrix pkg:
> setClassUnion("mMatrix", members = c("matrix", "Matrix"))
> setClassUnion("numMatrixLike", members =
>                 c("logical", "integer","numeric", "mMatrix"))
>
> c.Matrix <- function(...) unlist(lapply(list(...), as.vector))
> ## NB: Must use   signature  '(x, ..., recursive = FALSE)' :
> setMethod("c", "Matrix", function(x, ..., recursive) c.Matrix(x,
> ...))
> ## The above is not sufficient for
> ##    c(NA, 3:2, <Matrix>, <matrix>) :
> setMethod("c", "numMatrixLike", function(x, ..., recursive)
>    c.Matrix(x, ...))
>
> ## but the above does not really help:
>
>> c(Diagonal(3), NA, Matrix(10:11))   ## works fine,
>  [1]  1  0  0  0  1  0  0  0  1 NA 10 11
>
>> c(NA, Diagonal(3)) ## R's lowlevel c() already decided to use list():
> [[1]]
>  [1] NA
>
> [[2]]
>      [,1] [,2] [,3]
>      [1,]    1    .    .
>      [2,]    .    1    .
>      [3,]    .    .    1
>
>>
> ----------------------------------------------
>
> BTW, I (and the package users) suffer from exactly the same
> problem with the "MPFR" (multi precision numbers) provided by my
> package Rmpfr:
>
>> require(Rmpfr)
>> c(mpfr(3,100), 1/mpfr(7, 80)) ## works fine
> 2 'mpfr' numbers of precision  80 .. 100  bits
> [1]                            3 0.14285714285714285714285708
>
>> c(pi, 1/mpfr(7, 80)) ## "fails" even worse than in 'Matrix' case
> [[1]]
> [1] 3.141593
>
> [[2]]
> 'mpfr1' 0.14285714285714285714285708
>
>>
>
>
> Yes, it would be very nice  if  c(.)  could be used to
> concatenate quite arbitrary  R objects into one long atomic
> vector, but I don't see how to achieve this easily.
>
> The fact, that  c()  just builds a list of its arguments if it
> ("thinks" it) cannot dispatch to a method, is a good strategy,
> but I'd hope it should be possible to have c() try to do better
> (and hence work for this case, and
> without a noticable performance penalty.
>
> Suggestions are very welcome.
> Martin
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From maechler at stat.math.ethz.ch  Sat Sep 10 21:49:37 2016
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 10 Sep 2016 21:49:37 +0200
Subject: [Rd] c(<Matrix>, <Matrix>) / help(dotsMethods) etc
In-Reply-To: <9B81AE92-C400-4FBB-9C8E-A774F22402CD@r-project.org>
References: <FDA7E69C-303D-414A-96A3-55008150BC7A@illinois.edu>
	<22484.9820.875895.113534@stat.math.ethz.ch>
	<9B81AE92-C400-4FBB-9C8E-A774F22402CD@r-project.org>
Message-ID: <22484.25553.279808.392030@stat.math.ethz.ch>

>>>>> John Chambers <jmc at r-project.org>
>>>>>     on Sat, 10 Sep 2016 09:16:38 -0700 writes:

    > (Brief reply, I'm traveling but as per below, this is on my radar right now so wanted to comment.)
    > Two points regarding "dotsMethods".

    > 1.  To clarify the limitation.  It's not that all the arguments have to be of the same class, but there must be one class that they belong to or subclass.  (So, as in the example in the documentation, the method could be defined for a class union or other virtual class that all the actual arguments inherit from.)

Thank you for the clarification.
I knew that the limitation "the same class" has not been a big
one, that's why I did use a class union in my example (below).
I thought there were other limitations.. never mind.

    > 2.  The current documentation is a mess.  In common with lots of other very old documentation.  I'm in the process of rewriting a large chunk of the documentation including that for dotsMethods.  Sometime in the next few weeks, I hope to have it coherent enough to commit.

That's great!

    > So far, I'm not trying to change any significant aspects of the code, including for "..." methods, which seem to do roughly what was intended.

Yes, I'm sorry if I sounded like saying something different.

That I think this [getting c() to work for a collection objects,
some S4] needs changes in R is because it seems that do_c()
fails to dispatch here, and hence the problem was with our C
function that has carried the comment 

| * To call this an ugly hack would be to insult all existing ugly hacks
| * at large in the world.
    
and I don't think I would be able to correctly patch that
infamous function (in src/main/eval.c) ...

Martin

    > John


    > On Sep 10, 2016, at 8:27 AM, Martin Maechler <maechler at stat.math.ethz.ch> wrote:

    >> I have been asked  (by Roger; thank you for the good question,
    >> and I hope it's fine to answer to the public) :
    >> 
    >>> with Pi a sparse matrix and x,y, and ones
    >>> compatible n-vectors ? when I do:
    >> 
    >>>> c(t(x) %*% Pi %*% ones, t(ones) %*% Pi %*% y )
    >>> [[1]] 1 x 1 Matrix of class "dgeMatrix"
    >>> [,1] [1,]
    >>> 0.1338527
    >>> [[2]] 1 x 1 Matrix of class "dgeMatrix"
    >> [,1] [1,]
    >>> 0.7810341
    >> 
    >>> I get a list whereas if Pi is an ordinary matrix I get a
    >>> vector.  Is this intentional?
    >> 
    >> Well, no.  But it has been "unavoidable" in the sense that it had not
    >> been possible to provide S4 methods for '...' in the "remote"
    >> past, when  Matrix was created.
    >> 
    >> Later ... also quite a few years ago, John Chambers had added
    >> that possibility, with still some limitation (all '...' must be
    >> of the same class), and also plans to remove some of the
    >> limitations, see   ?dotsMethods  in R.
    >> 
    >> I honestly have forgotten the history of my trying to provide 'c'
    >> methods for our "Matrix" objects after the  'dotsMethods'
    >> possibility had emerged,  but I know I tried and had not seen a
    >> way to succeed "satisfactorily",
    >> but maybe I now think I maybe should try again.
    >> I currently think this needs changes to R before it can be done
    >> satisfactorily, and this is the main reason why this is a public
    >> answer to R-devel at ..., but I'm happy if I'am wrong.
    >> 
    >> The real challenge here is that I think that if it  should "work",
    >> it should work so in all cases, e.g., also for
    >> 
    >> c(NA, 3:2, Matrix(2:1), matrix(10:11))
    >> 
    >> and that's not so easy, e.g., the following class and method
    >> definitions do *not* achieve the desired result:
    >> 
    >> ## "mMatrix" is already hidden in Matrix pkg:
    >> setClassUnion("mMatrix", members = c("matrix", "Matrix"))
    >> setClassUnion("numMatrixLike", members =
    >> c("logical", "integer","numeric", "mMatrix"))
    >> 
    >> c.Matrix <- function(...) unlist(lapply(list(...), as.vector))
    >> ## NB: Must use   signature  '(x, ..., recursive = FALSE)' :
    >> setMethod("c", "Matrix", function(x, ..., recursive) c.Matrix(x,
    >> ...))
    >> ## The above is not sufficient for
    >> ##    c(NA, 3:2, <Matrix>, <matrix>) :
    >> setMethod("c", "numMatrixLike", function(x, ..., recursive)
    >> c.Matrix(x, ...))
    >> 
    >> ## but the above does not really help:
    >> 
    >>> c(Diagonal(3), NA, Matrix(10:11))   ## works fine,
    >> [1]  1  0  0  0  1  0  0  0  1 NA 10 11
    >> 
    >>> c(NA, Diagonal(3)) ## R's lowlevel c() already decided to use list():
    >> [[1]]
    >> [1] NA
    >> 
    >> [[2]]
    >> [,1] [,2] [,3]
    >> [1,]    1    .    .
    >> [2,]    .    1    .
    >> [3,]    .    .    1
    >> 
    >>> 
    >> ----------------------------------------------
    >> 
    >> BTW, I (and the package users) suffer from exactly the same
    >> problem with the "MPFR" (multi precision numbers) provided by my
    >> package Rmpfr:
    >> 
    >>> require(Rmpfr)
    >>> c(mpfr(3,100), 1/mpfr(7, 80)) ## works fine
    >> 2 'mpfr' numbers of precision  80 .. 100  bits
    >> [1]                            3 0.14285714285714285714285708
    >> 
    >>> c(pi, 1/mpfr(7, 80)) ## "fails" even worse than in 'Matrix' case
    >> [[1]]
    >> [1] 3.141593
    >> 
    >> [[2]]
    >> 'mpfr1' 0.14285714285714285714285708
    >> 
    >>> 
    >> 
    >> 
    >> Yes, it would be very nice  if  c(.)  could be used to
    >> concatenate quite arbitrary  R objects into one long atomic
    >> vector, but I don't see how to achieve this easily.
    >> 
    >> The fact, that  c()  just builds a list of its arguments if it
    >> ("thinks" it) cannot dispatch to a method, is a good strategy,
    >> but I'd hope it should be possible to have c() try to do better
    >> (and hence work for this case, and
    >> without a noticable performance penalty.
    >> 
    >> Suggestions are very welcome.
    >> Martin
    >> 
    >> ______________________________________________
    >> R-devel at r-project.org mailing list
    >> https://stat.ethz.ch/mailman/listinfo/r-devel


From maechler at stat.math.ethz.ch  Mon Sep 12 17:21:14 2016
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 12 Sep 2016 17:21:14 +0200
Subject: [Rd] R (development) changes in arith, logic,
 relop with (0-extent) arrays
In-Reply-To: <20160909142914.GA32628@cs.toronto.edu>
References: <mailman.27.1473328804.17880.r-devel@r-project.org>
	<20160908211118.GA17008@mail.cs.toronto.edu>
	<22482.29752.663855.365479@stat.math.ethz.ch>
	<20160909142914.GA32628@cs.toronto.edu>
Message-ID: <22486.51178.55616.173156@stat.math.ethz.ch>

>>>>> Radford Neal <radford at cs.toronto.edu>
>>>>>     on Fri, 9 Sep 2016 10:29:14 -0400 writes:

    >> Radford Nea:
    >> > So it may make more sense to move towards consistency in the
    >> > permissive direction, rather than the restrictive direction.
    >> 
    >> > That would mean allowing matrix(1,1,1) < (1:2), and maybe also things
    >> > like matrix(1,2,2)+(1:8).
    >> 
    >> Martin Maechler:
    >> That is an interesting idea.  Yes, in my view that would
    >> definitely also have to allow the latter, by the above argument
    >> of not treating the dim/dimnames attributes special.  For
    >> non-arrays length-1 is not treated much special apart from the
    >> fact that length-1 can always be recycled (without warning).

    > I think one could argue for allowing matrix(1,1,1)+(1:8) but not
    > matrix(1,2,2)+(1:8).  Length-1 vectors certainly are special in some
    > circumstances, being R's only way of representing a scalar.  For
    > instance, if (c(T,F)) gives a warning.

well, the if(.)  situation is very special and does not weigh
much for me, here.

    > This really goes back to what I think may have been a basic mistake in
    > the design of S, in deciding that everything is a vector, then halfway
    > modifying this with dim attributes, but it's too late to totally undo
    > that (though allowing a 0-length dim attribute to explicitly mark a
    > length-1 vector as a scalar might help).

(yes; I think there are also other ideas of adding true small
 scalars to R... I am not familiar with those, and in any case
 that should be a completely different thread and not be
 discussed in this one)

    >> > And I think there would be some significant problems. In addition to
    >> > the 10-20+ packages that Martin expects to break, there could be quite
    >> > a bit of user code that would no longer work - scripts for analysing
    >> > data sets that used to work, but now don't with the latest version.
    >> 
    >> That's not true (at least for the cases above): They would give
    >> a strong warning

    > But isn't the intent to make it an error later?  So I assume we're
    > debating making it an error, not just a warning.  

Yes, that's correct.
But if we have a longish deprecation period (i.e. where there's
only a warning) all important code should have been adapted
before it turns to an error 
 (( unless for those people who are careless enough to "graciously"
    use something like suppressWarnings(...) in too many places )).

    > (Though I'm
    > generally opposed to such warnings anyway, unless they could somehow
    > be restricted to come up only for interactive uses, not from deep in a
    > program the user didn't write, making them totally mysterious...)

    >> *and* the  logic and relop versions of this, e.g.,
    >> matrix(TRUE,1) | c(TRUE,FALSE) ;  matrix(1,1) > 1:2,
    >> have always been an  error; so nothing would break there.

    > Yes, that wouldn't change the behaviour of old code, but if we're
    > aiming for consistency, it might make sense to get rid of that error,
    > allowing code like sum(a%*%b<c(10,20,30)) with a and b being vectors,
    > rather than forcing the programmer to write sum(c(a%*%b)<c(10,20,30)).

Yes, that would be another way for consistency... leading to
less problems in existing code.  As said earlier, getting
consistency by becoming "more lenient" instead of "more restrictive" 
is a good option in my view.

We would however have this somewhat special  length-1-array
exception in how arrays behave in binary OPs, and both the underlying C
code and the full documentation being/becoming slightly more complicated
rather than simpler,

OTOH we would remain back compatible (*) to S or at least S-plus
(as far as I know) and all earlier versions of R, here,
and that is valuable, too, I agree.

Nobody else has commented yet on this sub-thread ... not even
privately to me.  If that status does not change quite a bit,
I don't see enough incentive for changing (the current R-devel code).

Martin

--
(*) "back-compatible" in the sense that old code which "worked"
    would continue to work the same
    (but some old code that gave an error would no longer do so)



    >> Of course; that *was* the reason the very special treatment for arithmetic
    >> length-1 arrays had been introduced.  It is convenient.
    >> 
    >> However, *some* of the conveniences in S (and hence R) functions
    >> have been dangerous {and much more used, hence close to
    >> impossible to abolish, e.g., sample(x) when x  is numeric of length 1,

    > There's a difference between these two.  Giving an error when using a
    > 1x1 matrix as a scalar may detect some programming bugs, but not
    > giving an error doesn't introduce a bug.  Whereas sample(2:n) behaving
    > differently when n is 2 than when n is greater than 2 is itself a bug,
    > that the programmer has to consciously avoid by being aware of the quirk.

    > Radford Neal


From radford at cs.toronto.edu  Mon Sep 12 18:51:42 2016
From: radford at cs.toronto.edu (Radford Neal)
Date: Mon, 12 Sep 2016 12:51:42 -0400
Subject: [Rd] R (development) changes in arith, logic,
 relop with (0-extent) arrays
In-Reply-To: <22486.51178.55616.173156@stat.math.ethz.ch>
References: <mailman.27.1473328804.17880.r-devel@r-project.org>
	<20160908211118.GA17008@mail.cs.toronto.edu>
	<22482.29752.663855.365479@stat.math.ethz.ch>
	<20160909142914.GA32628@cs.toronto.edu>
	<22486.51178.55616.173156@stat.math.ethz.ch>
Message-ID: <20160912165142.GA20825@cs.toronto.edu>

>     > But isn't the intent to make it an error later?  So I assume we're
>     > debating making it an error, not just a warning.  
> 
> Yes, that's correct.
> But if we have a longish deprecation period (i.e. where there's
> only a warning) all important code should have been adapted
> before it turns to an error 

That might be true for continuously-used code.  But for old scripts
for analysing some dataset that someone decides to run again five
years from now, they will be reduced to trying to guess which version
of R they ran under originally, or if they now want to use newer
features, to doing a binary search for the most recent version of R
for which they still work, or of course going through the script
trying to find the problem.

This wouldn't be a disaster, but I'm not seeing the magnitude of
benefit that would justify imposing this burden on users.  A language
specification shouldn't really be changing all the time for no
particular reason.

   Radford Neal


From Andrew.Redd at hsc.utah.edu  Mon Sep 12 19:18:20 2016
From: Andrew.Redd at hsc.utah.edu (Andrew Redd)
Date: Mon, 12 Sep 2016 17:18:20 +0000
Subject: [Rd] Announcing the R Documentation Task Force
In-Reply-To: <2c85a36a-9160-af19-6d80-e0161c073b47@hsc.utah.edu>
References: <2c85a36a-9160-af19-6d80-e0161c073b47@hsc.utah.edu>
Message-ID: <CAK_CNyb=b8bNpKeVuWsoBCo6OtU5YemCu_PbCqyPTyd16SO8Og@mail.gmail.com>

To clarify, the developers, Duncan Murdoch, Michael Lawrence, and Martin
Maechler, who are R Core Developers are participating in the project, but
this should not be construed to be an endorsement from the R Core
Developers as a whole, and the work of the R Documentation Task Force is
not guaranteed to be included in R Core.  My apologies to any who may have
misinterpreted the intentions of the announcement.

Sincerely,
Andrew Redd

On Fri, Sep 9, 2016 at 8:46 PM Andrew Redd <Andrew.Redd at hsc.utah.edu> wrote:

> cross-posting announcement to R-Announce, R-devel and R-package-devel.
>
> The R Consortium recently announced
> (
> https://www.r-consortium.org/news/blogs/2016/08/r-consortium-funds-three-projects-july
> )
> support of the R Documentation Task Force.  The task force aims to
> design and implement the next generation documentation system for R.  We
> aim to take the best from the many attempts to improve documentation and
> unify them into a system that will be more standard, flexible and
> powerful.  We have the support and participation of R Core Developers.
> The full proposal is available on the announcement website given above.
>
> If you have expertise in documentation systems or documentation of
> objects or interest in building a documentation system we invite you to
> participate.  We will be considering documentation in all common forms
> such as function and class documentation, but also areas such as data
> frames, non-standard objects, packages, and how vignettes fit into the
> documentation framework.  If you have opinions or concerns that you
> would like to make sure are addressed please join.
>
> To join send an email to Andrew dot Redd at hsc dot utah dot edu,
> expressing your interests, skills or expertise as it relates to R
> documentation.  Also email if you have ideas or concerns but do not wish
> to play and active role.
>
> Thank you,
> Andrew Redd
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From gmbecker at ucdavis.edu  Mon Sep 12 20:45:27 2016
From: gmbecker at ucdavis.edu (Gabriel Becker)
Date: Mon, 12 Sep 2016 11:45:27 -0700
Subject: [Rd] R (development) changes in arith, logic,
 relop with (0-extent) arrays
In-Reply-To: <20160912165142.GA20825@cs.toronto.edu>
References: <mailman.27.1473328804.17880.r-devel@r-project.org>
	<20160908211118.GA17008@mail.cs.toronto.edu>
	<22482.29752.663855.365479@stat.math.ethz.ch>
	<20160909142914.GA32628@cs.toronto.edu>
	<22486.51178.55616.173156@stat.math.ethz.ch>
	<20160912165142.GA20825@cs.toronto.edu>
Message-ID: <CADwqtCNoX3wd6r1B3bT5zCHDFqwoKYztMZbHjhj5fOcc84sY8g@mail.gmail.com>

On Mon, Sep 12, 2016 at 9:51 AM, Radford Neal <radford at cs.toronto.edu>
wrote:

> >     > But isn't the intent to make it an error later?  So I assume we're
> >     > debating making it an error, not just a warning.
> >
> > Yes, that's correct.
> > But if we have a longish deprecation period (i.e. where there's
> > only a warning) all important code should have been adapted
> > before it turns to an error
>
> That might be true for continuously-used code.  But for old scripts
> for analysing some dataset that someone decides to run again five
> years from now, they will be reduced to trying to guess which version
> of R they ran under originally,


In the general case this is true of any packages they use anyway, though.
And if you go back far enough old package versions can't even be installed
for current versions of R.

Results are always contingent on the software, including versions, used to
generate them imho. There are ways to try to address this, but they can be
painful and generally require extra work the author of any 5 year old
script you have in your hands now likely didn't do.


> or if they now want to use newer
> features, to doing a binary search for the most recent version of R
> for which they still work, or of course going through the script
> trying to find the problem.
>

Now you're talking not about running a script, though, but of maintaining
and developing it. I know from personal experience how painful developing
on top of legacy code can be, but nonetheless sometimes It needs to be
maintained and updated.

If you really want absolute safety, that is (more or less) possible, but it
means staying in an old version of R with old package versions. I don't
think the expectation of being guaranteed to be able to use new features
and unmodified legacy code at the same time is reasonable. It's great what
that does happen, but it's a bonus imho, not a requirement.

Best,
~G




>
> This wouldn't be a disaster, but I'm not seeing the magnitude of
> benefit that would justify imposing this burden on users.  A language
> specification shouldn't really be changing all the time for no
> particular reason.
>
>    Radford Neal
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>



-- 
Gabriel Becker, PhD
Associate Scientist (Bioinformatics)
Genentech Research

	[[alternative HTML version deleted]]


From hpages at fredhutch.org  Mon Sep 12 21:23:00 2016
From: hpages at fredhutch.org (=?UTF-8?B?SGVydsOpIFBhZ8Oocw==?=)
Date: Mon, 12 Sep 2016 12:23:00 -0700
Subject: [Rd] R (development) changes in arith, logic,
 relop with (0-extent) arrays
In-Reply-To: <22486.51178.55616.173156@stat.math.ethz.ch>
References: <mailman.27.1473328804.17880.r-devel@r-project.org>
	<20160908211118.GA17008@mail.cs.toronto.edu>
	<22482.29752.663855.365479@stat.math.ethz.ch>
	<20160909142914.GA32628@cs.toronto.edu>
	<22486.51178.55616.173156@stat.math.ethz.ch>
Message-ID: <d8114f83-77d7-fbef-cc20-fccf222350db@fredhutch.org>

Hi,

On 09/12/2016 08:21 AM, Martin Maechler wrote:
>>>>>> Radford Neal <radford at cs.toronto.edu>
>>>>>>     on Fri, 9 Sep 2016 10:29:14 -0400 writes:
>
>     >> Radford Nea:
>     >> > So it may make more sense to move towards consistency in the
>     >> > permissive direction, rather than the restrictive direction.
>     >>
>     >> > That would mean allowing matrix(1,1,1) < (1:2), and maybe also things
>     >> > like matrix(1,2,2)+(1:8).
>     >>
>     >> Martin Maechler:
>     >> That is an interesting idea.  Yes, in my view that would
>     >> definitely also have to allow the latter, by the above argument
>     >> of not treating the dim/dimnames attributes special.  For
>     >> non-arrays length-1 is not treated much special apart from the
>     >> fact that length-1 can always be recycled (without warning).
>
>     > I think one could argue for allowing matrix(1,1,1)+(1:8) but not
>     > matrix(1,2,2)+(1:8).  Length-1 vectors certainly are special in some
>     > circumstances, being R's only way of representing a scalar.  For
>     > instance, if (c(T,F)) gives a warning.
>
> well, the if(.)  situation is very special and does not weigh
> much for me, here.
>
>     > This really goes back to what I think may have been a basic mistake in
>     > the design of S, in deciding that everything is a vector, then halfway
>     > modifying this with dim attributes, but it's too late to totally undo
>     > that (though allowing a 0-length dim attribute to explicitly mark a
>     > length-1 vector as a scalar might help).
>
> (yes; I think there are also other ideas of adding true small
>  scalars to R... I am not familiar with those, and in any case
>  that should be a completely different thread and not be
>  discussed in this one)
>
>     >> > And I think there would be some significant problems. In addition to
>     >> > the 10-20+ packages that Martin expects to break, there could be quite
>     >> > a bit of user code that would no longer work - scripts for analysing
>     >> > data sets that used to work, but now don't with the latest version.
>     >>
>     >> That's not true (at least for the cases above): They would give
>     >> a strong warning
>
>     > But isn't the intent to make it an error later?  So I assume we're
>     > debating making it an error, not just a warning.
>
> Yes, that's correct.
> But if we have a longish deprecation period (i.e. where there's
> only a warning) all important code should have been adapted
> before it turns to an error
>  (( unless for those people who are careless enough to "graciously"
>     use something like suppressWarnings(...) in too many places )).

Any reason for not using the formal deprecation mechanism (i.e.
.Deprecated()) for this? The advantage being that 'R CMD check'
reports deprecation warnings and not ordinary warnings, IIRC. It
won't help (and won't hurt either) in the case of packages using
suppressWarnings(...) but maybe suppressWarnings() shouldn't suppress
deprecation warnings, at least be default.

Cheers,
H.

>
>     > (Though I'm
>     > generally opposed to such warnings anyway, unless they could somehow
>     > be restricted to come up only for interactive uses, not from deep in a
>     > program the user didn't write, making them totally mysterious...)
>
>     >> *and* the  logic and relop versions of this, e.g.,
>     >> matrix(TRUE,1) | c(TRUE,FALSE) ;  matrix(1,1) > 1:2,
>     >> have always been an  error; so nothing would break there.
>
>     > Yes, that wouldn't change the behaviour of old code, but if we're
>     > aiming for consistency, it might make sense to get rid of that error,
>     > allowing code like sum(a%*%b<c(10,20,30)) with a and b being vectors,
>     > rather than forcing the programmer to write sum(c(a%*%b)<c(10,20,30)).
>
> Yes, that would be another way for consistency... leading to
> less problems in existing code.  As said earlier, getting
> consistency by becoming "more lenient" instead of "more restrictive"
> is a good option in my view.
>
> We would however have this somewhat special  length-1-array
> exception in how arrays behave in binary OPs, and both the underlying C
> code and the full documentation being/becoming slightly more complicated
> rather than simpler,
>
> OTOH we would remain back compatible (*) to S or at least S-plus
> (as far as I know) and all earlier versions of R, here,
> and that is valuable, too, I agree.
>
> Nobody else has commented yet on this sub-thread ... not even
> privately to me.  If that status does not change quite a bit,
> I don't see enough incentive for changing (the current R-devel code).
>
> Martin
>
> --
> (*) "back-compatible" in the sense that old code which "worked"
>     would continue to work the same
>     (but some old code that gave an error would no longer do so)
>
>
>
>     >> Of course; that *was* the reason the very special treatment for arithmetic
>     >> length-1 arrays had been introduced.  It is convenient.
>     >>
>     >> However, *some* of the conveniences in S (and hence R) functions
>     >> have been dangerous {and much more used, hence close to
>     >> impossible to abolish, e.g., sample(x) when x  is numeric of length 1,
>
>     > There's a difference between these two.  Giving an error when using a
>     > 1x1 matrix as a scalar may detect some programming bugs, but not
>     > giving an error doesn't introduce a bug.  Whereas sample(2:n) behaving
>     > differently when n is 2 than when n is greater than 2 is itself a bug,
>     > that the programmer has to consciously avoid by being aware of the quirk.
>
>     > Radford Neal
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fredhutch.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From maechler at stat.math.ethz.ch  Tue Sep 13 17:49:51 2016
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 13 Sep 2016 17:49:51 +0200
Subject: [Rd] c(<Matrix>, <Matrix>) / help(dotsMethods) etc
In-Reply-To: <22484.25553.279808.392030@stat.math.ethz.ch>
References: <FDA7E69C-303D-414A-96A3-55008150BC7A@illinois.edu>
	<22484.9820.875895.113534@stat.math.ethz.ch>
	<9B81AE92-C400-4FBB-9C8E-A774F22402CD@r-project.org>
	<22484.25553.279808.392030@stat.math.ethz.ch>
Message-ID: <22488.8223.272404.952405@stat.math.ethz.ch>

>>>>> Martin Maechler <maechler at stat.math.ethz.ch>
>>>>>     on Sat, 10 Sep 2016 21:49:37 +0200 writes:

>>>>> John Chambers <jmc at r-project.org>
>>>>>     on Sat, 10 Sep 2016 09:16:38 -0700 writes:

    >> (Brief reply, I'm traveling but as per below, this is on my radar right now so wanted to comment.)
    >> Two points regarding "dotsMethods".

    >> 1.  To clarify the limitation.  It's not that all the arguments have to be of the same class, but there must be one class that they belong to or subclass.  (So, as in the example in the documentation, the method could be defined for a class union or other virtual class that all the actual arguments inherit from.)

    > Thank you for the clarification.
    > I knew that the limitation "the same class" has not been a big
    > one, that's why I did use a class union in my example (below).
    > I thought there were other limitations.. never mind.

    >> 2.  The current documentation is a mess.  In common with lots of other very old documentation.  I'm in the process of rewriting a large chunk of the documentation including that for dotsMethods.  Sometime in the next few weeks, I hope to have it coherent enough to commit.

    > That's great!

    >> So far, I'm not trying to change any significant aspects of the code, including for "..." methods, which seem to do roughly what was intended.

    > Yes, I'm sorry if I sounded like saying something different.

    > That I think this [getting c() to work for a collection objects,
    > some S4] needs changes in R is because it seems that do_c()
    > fails to dispatch here, and hence the problem was with our C
    > function that has carried the comment 

    > | * To call this an ugly hack would be to insult all existing ugly hacks
    > | * at large in the world.
    
    > and I don't think I would be able to correctly patch that
    > infamous function (in src/main/eval.c) ...

    > Martin

On the other hand,
with the following patch

--- bind.c	(revision 71239)
+++ bind.c	(working copy)
@@ -732,7 +732,8 @@
 
     /* Attempt method dispatch. */
 
-    if (DispatchOrEval(call, op, "c", args, env, &ans, 1, 1))
+    if (DispatchAnyOrEval(call, op, "c", args, env, &ans, 1, 1))
+	//      ^^^ "Any" => all args are eval()ed and checked => correct multi-arg dispatch
 	return(ans);
     PROTECT(ans);
     SEXP res = do_c_dflt(call, op, ans, env);

the problem is basically solved.

Yes it does cost a tiny bit: according to minimal example testing (and
microbenchmark):

d4 <- diag(4); microbenchmark(c(), c(1), c(2,3), c(d4,3:1), times=2^12)

it costs 10-20 nanoseconds per call .. and possibly slightly
more after attaching a version of 'Matrix' with new 'c' methods,
where all versions of

	   c(..., <Matrix>, ...)

would work.
OTOH, it seems very natural to me to allow proper dispatch once
a
   setMethod("c", "numMatrixLike", function(x, ..., recursive) { .......})
or even a
   setMethod("c", "ANY", function(x, ..., recursive) { .......})

method is defined.




    >> On Sep 10, 2016, at 8:27 AM, Martin Maechler <maechler at stat.math.ethz.ch> wrote:

    >>> I have been asked  (by Roger; thank you for the good question,
    >>> and I hope it's fine to answer to the public) :
    >>> 
    >>>> with Pi a sparse matrix and x,y, and ones
    >>>> compatible n-vectors ? when I do:
    >>> 
    >>>>> c(t(x) %*% Pi %*% ones, t(ones) %*% Pi %*% y )
    >>>> [[1]] 1 x 1 Matrix of class "dgeMatrix"
    >>>> [,1] [1,]
    >>>> 0.1338527
    >>>> [[2]] 1 x 1 Matrix of class "dgeMatrix"
    >>> [,1] [1,]
    >>>> 0.7810341
    >>> 
    >>>> I get a list whereas if Pi is an ordinary matrix I get a
    >>>> vector.  Is this intentional?
    >>> 
    >>> Well, no.  But it has been "unavoidable" in the sense that it had not
    >>> been possible to provide S4 methods for '...' in the "remote"
    >>> past, when  Matrix was created.
    >>> 
    >>> Later ... also quite a few years ago, John Chambers had added
    >>> that possibility, with still some limitation (all '...' must be
    >>> of the same class), and also plans to remove some of the
    >>> limitations, see   ?dotsMethods  in R.
    >>> 
    >>> I honestly have forgotten the history of my trying to provide 'c'
    >>> methods for our "Matrix" objects after the  'dotsMethods'
    >>> possibility had emerged,  but I know I tried and had not seen a
    >>> way to succeed "satisfactorily",
    >>> but maybe I now think I maybe should try again.
    >>> I currently think this needs changes to R before it can be done
    >>> satisfactorily, and this is the main reason why this is a public
    >>> answer to R-devel at ..., but I'm happy if I'am wrong.
    >>> 
    >>> The real challenge here is that I think that if it  should "work",
    >>> it should work so in all cases, e.g., also for
    >>> 
    >>> c(NA, 3:2, Matrix(2:1), matrix(10:11))
    >>> 
    >>> and that's not so easy, e.g., the following class and method
    >>> definitions do *not* achieve the desired result:
    >>> 
    >>> ## "mMatrix" is already hidden in Matrix pkg:
    >>> setClassUnion("mMatrix", members = c("matrix", "Matrix"))
    >>> setClassUnion("numMatrixLike", members =
    >>> c("logical", "integer","numeric", "mMatrix"))
    >>> 
    >>> c.Matrix <- function(...) unlist(lapply(list(...), as.vector))
    >>> ## NB: Must use   signature  '(x, ..., recursive = FALSE)' :
    >>> setMethod("c", "Matrix", function(x, ..., recursive) c.Matrix(x,
    >>> ...))
    >>> ## The above is not sufficient for
    >>> ##    c(NA, 3:2, <Matrix>, <matrix>) :
    >>> setMethod("c", "numMatrixLike", function(x, ..., recursive)
    >>> c.Matrix(x, ...))
    >>> 
    >>> ## but the above does not really help:
    >>> 
    >>>> c(Diagonal(3), NA, Matrix(10:11))   ## works fine,
    >>> [1]  1  0  0  0  1  0  0  0  1 NA 10 11
    >>> 
    >>>> c(NA, Diagonal(3)) ## R's lowlevel c() already decided to use list():
    >>> [[1]]
    >>> [1] NA
    >>> 
    >>> [[2]]
    >>> [,1] [,2] [,3]
    >>> [1,]    1    .    .
    >>> [2,]    .    1    .
    >>> [3,]    .    .    1
    >>> 
    >>>> 
    >>> ----------------------------------------------
    >>> 
    >>> BTW, I (and the package users) suffer from exactly the same
    >>> problem with the "MPFR" (multi precision numbers) provided by my
    >>> package Rmpfr:
    >>> 
    >>>> require(Rmpfr)
    >>>> c(mpfr(3,100), 1/mpfr(7, 80)) ## works fine
    >>> 2 'mpfr' numbers of precision  80 .. 100  bits
    >>> [1]                            3 0.14285714285714285714285708
    >>> 
    >>>> c(pi, 1/mpfr(7, 80)) ## "fails" even worse than in 'Matrix' case
    >>> [[1]]
    >>> [1] 3.141593
    >>> 
    >>> [[2]]
    >>> 'mpfr1' 0.14285714285714285714285708
    >>> 
    >>>> 
    >>> 
    >>> 
    >>> Yes, it would be very nice  if  c(.)  could be used to
    >>> concatenate quite arbitrary  R objects into one long atomic
    >>> vector, but I don't see how to achieve this easily.
    >>> 
    >>> The fact, that  c()  just builds a list of its arguments if it
    >>> ("thinks" it) cannot dispatch to a method, is a good strategy,
    >>> but I'd hope it should be possible to have c() try to do better
    >>> (and hence work for this case, and
    >>> without a noticable performance penalty.
    >>> 
    >>> Suggestions are very welcome.
    >>> Martin
    >>> 
    >>> ______________________________________________
    >>> R-devel at r-project.org mailing list
    >>> https://stat.ethz.ch/mailman/listinfo/r-devel


From maechler at stat.math.ethz.ch  Tue Sep 13 17:55:28 2016
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 13 Sep 2016 17:55:28 +0200
Subject: [Rd] R-intro: function 'stderr' and 'sd'
In-Reply-To: <1979148586.1732979.1473439921401@mail.yahoo.com>
References: <1979148586.1732979.1473439921401.ref@mail.yahoo.com>
	<1979148586.1732979.1473439921401@mail.yahoo.com>
Message-ID: <22488.8560.466832.88971@stat.math.ethz.ch>

>>>>> Suharto Anggono Suharto Anggono via R-devel <r-devel at r-project.org>
>>>>>     on Fri, 9 Sep 2016 16:52:01 +0000 writes:

    > In "An Introduction to R" Version 3.3.1, in "4.2 The function tapply() and ragged arrays", after
    > stderr <- function(x) sqrt(var(x)/length(x))  ,
    > there is a note in brackets:
    > Writing functions will be considered later in [Writing your own functions], and in this case was unnecessary as R also has a builtin function sd().

    > The part "in this case was unnecessary as R also has a builtin function sd()" is misleading. The builtin function sd() doesn't calculate standard error of the mean. It calculates standard deviation. The function 'stderr' can use 'sd':
    > function(x) sd(x)/sqrt(length(x))

You are right; thank you Suharto.
It now says

(Writing functions will be considered later in @ref{Writing your own
functions}.  Note that @R{}'s a builtin function @code{sd()} is something different.)


From wdunlap at tibco.com  Tue Sep 13 18:06:00 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Tue, 13 Sep 2016 09:06:00 -0700
Subject: [Rd] R-intro: function 'stderr' and 'sd'
In-Reply-To: <22488.8560.466832.88971@stat.math.ethz.ch>
References: <1979148586.1732979.1473439921401.ref@mail.yahoo.com>
	<1979148586.1732979.1473439921401@mail.yahoo.com>
	<22488.8560.466832.88971@stat.math.ethz.ch>
Message-ID: <CAF8bMcaF3bCCOYv1GVkMD0WF0j03dD0cwE4PFg5oPX9D3SsZ+Q@mail.gmail.com>

While you are editing that, you might change its name from 'stderr'
to standardError (or standard_error, etc.) so as not to conflict with
base::stderr().


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Tue, Sep 13, 2016 at 8:55 AM, Martin Maechler <maechler at stat.math.ethz.ch
> wrote:

> >>>>> Suharto Anggono Suharto Anggono via R-devel <r-devel at r-project.org>
> >>>>>     on Fri, 9 Sep 2016 16:52:01 +0000 writes:
>
>     > In "An Introduction to R" Version 3.3.1, in "4.2 The function
> tapply() and ragged arrays", after
>     > stderr <- function(x) sqrt(var(x)/length(x))  ,
>     > there is a note in brackets:
>     > Writing functions will be considered later in [Writing your own
> functions], and in this case was unnecessary as R also has a builtin
> function sd().
>
>     > The part "in this case was unnecessary as R also has a builtin
> function sd()" is misleading. The builtin function sd() doesn't calculate
> standard error of the mean. It calculates standard deviation. The function
> 'stderr' can use 'sd':
>     > function(x) sd(x)/sqrt(length(x))
>
> You are right; thank you Suharto.
> It now says
>
> (Writing functions will be considered later in @ref{Writing your own
> functions}.  Note that @R{}'s a builtin function @code{sd()} is something
> different.)
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From maechler at stat.math.ethz.ch  Tue Sep 13 18:33:35 2016
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 13 Sep 2016 18:33:35 +0200
Subject: [Rd] Coercion of 'exclude' in function 'factor' (was
 'droplevels' inappropriate change)
In-Reply-To: <1077826208.388023.1472832600456@mail.yahoo.com>
References: <1077826208.388023.1472832600456.ref@mail.yahoo.com>
	<1077826208.388023.1472832600456@mail.yahoo.com>
Message-ID: <22488.10847.162472.776657@stat.math.ethz.ch>

>>>>> Suharto Anggono Suharto Anggono via R-devel <r-devel at r-project.org>
>>>>>     on Fri, 2 Sep 2016 16:10:00 +0000 writes:

    > I am basically fine with the change.
    > How about using just the following?
    > if(!is.character(exclude))
    >   exclude <- as.vector(exclude, typeof(x)) # may result in NA
    > x <- as.character(x)

    > It looks simpler and is, more or less, equivalent.

yes, but the current code should be slightly faster..

    > In factor.Rd, in description of argument 'exclude', "(when \code{x} is a \code{factor} already)" can be removed.


    > A larger change that, I think, is reasonable is entirely removing the code
    > exclude <- as.vector(exclude, typeof(x)) # may result in NA

    > The explicit coercion of 'exclude' is not necessary. 
    > Function 'factor' works without it.

    > The coercion of 'exclude' may lead to a surprise because it "may result in NA". 
    > Example from https://stat.ethz.ch/pipermail/r-help/2005-April/069053.html :
    >    factor(as.integer(c(1,2,3,3,NA)), exclude=NaN)
    > excludes NA.

    > As a bonus, without the coercion of 'exclude', 'exclude' can be a factor if 'x' is a factor. This part of an example in https://stat.ethz.ch/pipermail/r-help/2011-April/276274.html works.
    > cc <- c("x","y","NA")
    > ff <- factor(cc)
    > factor(ff,exclude=ff[3])

Yes, two good reasons for a change.  factor() would finally
behave according to the documentation which has been mentioning
that 'exclude' could be a factor: ((Until my R-devel changes of a
few weeks ago, i.e. at least in all recent released versions of R)),
the help page for factor has said

    || If 'exclude' is used it should either be a factor with the same
    || level set as 'x' or a set of codes for the levels to be excluded.

  > However, the coercion of 'exclude' has been in function 'factor' in R "forever".

Indeed: On March 6, 1998, svn rev. 845, when the R source files got a
'.R' appended, and quite a long time before  R 1.0.0,
the factor function was as short as (but using an .Internal(.) !)

function (x, levels = sort(unique(x), na.last = TRUE), labels, exclude = NA, 
	ordered = FALSE) 
{
	if (length(x) == 0) 
		return(character(0))
	exclude <- as.vector(exclude, typeof(x))
	levels <- levels[is.na(match(levels, exclude))]
	x <- .Internal(factor(match(x, levels), length(levels), 
		ordered))
	if (missing(labels)) 
		levels(x) <- levels
	else levels(x) <- labels
	x
}

and already contained that line.

Nevertheless, simplifying factor() by removing that line (or those
2 lines, now!) seems to only have advantages....

I'm not yet committing to anything, but currently would strongly
consider it .. though *after* the
	   '<length-1-array>  OP  <non-array>'
issue has settled a bit.

Martin

    > --------------------------------------------
    > On Wed, 31/8/16, Martin Maechler <maechler at stat.math.ethz.ch> wrote:

    > Subject: Re: [Rd] 'droplevels' inappropriate change

    > Cc: "Martin Maechler" <maechler at stat.math.ethz.ch>
    > Date: Wednesday, 31 August, 2016, 2:51 PM
 
>>>>> Martin Maechler <maechler at stat.math.ethz.ch>
>>>>>     on Sat, 27 Aug 2016 18:55:37 +0200 writes:

>>>>> Suharto Anggono Suharto Anggono via R-devel <r-devel at r-project.org>
>>>>>     on Sat, 27 Aug 2016 03:17:32 +0000 writes:

    >>> In R devel r71157, 'droplevels' documentation, in "Arguments" section, says this about argument 'exclude'.
    >>> passed to factor(); factor levels which should be excluded from the result even if present.  Note that this was implicitly NA in R <= 3.3.1 which did drop NA levels even when present in x, contrary to the documentation.  The current default is compatible with x[ , drop=FALSE].

    >>> The part
    >>> x[ , drop=FALSE]
    >>> should be
    >>> x[ , drop=TRUE]

    > [[elided Yahoo spam]]
    >> a "typo" by me. .. fixed now.

    >>> Saying that 'exclude' is factor levels is not quite true for NA element. NA may be not an original level, but NA in 'exclude' affects the result.

    >>> For a factor 'x', factor(x, exclude = exclude) doesn't really work for excluding in general. See, for example, https://stat.ethz.ch/pipermail/r-help/2005-September/079336.html .
    >>> factor(factor(c("a","b","c")), exclude="c")

    >>> However, this excludes "2":
    >>> factor(factor(2:3), exclude=2)

    >>> Rather unexpectedly, this excludes NA:
    >>> factor(factor(c("a",NA), exclude=NULL), exclude="c")

    >>> For a factor 'x', factor(x, exclude = exclude) can only exclude integer-like or NA levels. An explanation is in https://stat.ethz.ch/pipermail/r-help/2011-April/276274.html .

    >> Well, Peter Dalgaard (in that R-devel e-mail, a bit more than 5
    >> years ago) is confirming the problem there,  and suggesting (as
    >> you, right?) that actually   `factor()` is not behaving
    >> correctly here.

    >> And your persistence is finally getting close to convince me
    >> that it is not just droplevels(), but  factor() itself which
    >> needs care here.

    >> Interestingly, the following patch *does* pass 'make check-all'
    >> (after small change in tests/reg-tests-1b.R which is ok),
    >> and leads to behavior which is much closer to the documentation,
    >> notably for your two examples above would give what one would
    >> expect.

    >> (( If the R-Hub would support experiments with branches of R-devel 
    >> from R-core members,  I could just create such a branch and R Hub
    >> would run 'R CMD check <pkg>'  for thousands of CRAN packages
    >> and provide a web page with the *differences* in the package
    >> check results ... so we could see ... ))

    >> I do agree that we should strongly consider such a change.

    > as nobody has commented, I've been liberal and have taken these
    > no comments as consent.

    > Hence I have committed

    > ------------------------------------------------------------------------
    > r71178 | maechler | 2016-08-31 09:45:40 +0200 (Wed, 31 Aug 2016) | 1 line
    > Changed paths:
    > M /trunk/doc/NEWS.Rd
    > M /trunk/src/library/base/R/factor.R
    > M /trunk/src/library/base/man/factor.Rd
    > M /trunk/tests/reg-tests-1b.R
    > M /trunk/tests/reg-tests-1c.R

    > factor(x, exclude) more "rational" when x or exclude are character
    > ------------------------------------------------------------------------

    > which apart from documentation, examples, and regression tests
    > is just the patch below.

    > Martin Maechler
    > ETH Zurich



    >> --- factor.R    (revision 71157)
    >> +++ factor.R    (working copy)
    >> @@ -28,8 +28,12 @@
    >> levels <- unique(y[ind])
    >> }
    >> force(ordered) # check if original x is an ordered factor
    >> -    exclude <- as.vector(exclude, typeof(x)) # may result in NA
    >> -    x <- as.character(x)
    >> +    if(!is.character(x)) {
    >> +    if(!is.character(exclude))
    >> +        exclude <- as.vector(exclude, typeof(x)) # may result in NA
    >> +    x <- as.character(x)
    >> +    } else
    >> +    exclude <- as.vector(exclude, typeof(x)) # may result in NA
    >> ## levels could be a long vectors, but match will not handle that.
    >> levels <- levels[is.na(match(levels, exclude))]
    >> f <- match(x, levels)
    > Delete Reply Reply All Forward Apply

    > ______________________________________________
    > R-devel at r-project.org mailing list
    > https://stat.ethz.ch/mailman/listinfo/r-devel


From maechler at stat.math.ethz.ch  Tue Sep 13 18:36:39 2016
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 13 Sep 2016 18:36:39 +0200
Subject: [Rd] R-intro: function 'stderr' and 'sd'
In-Reply-To: <CAF8bMcaF3bCCOYv1GVkMD0WF0j03dD0cwE4PFg5oPX9D3SsZ+Q@mail.gmail.com>
References: <1979148586.1732979.1473439921401.ref@mail.yahoo.com>
	<1979148586.1732979.1473439921401@mail.yahoo.com>
	<22488.8560.466832.88971@stat.math.ethz.ch>
	<CAF8bMcaF3bCCOYv1GVkMD0WF0j03dD0cwE4PFg5oPX9D3SsZ+Q@mail.gmail.com>
Message-ID: <22488.11031.90673.123974@stat.math.ethz.ch>

>>>>> William Dunlap <wdunlap at tibco.com>
>>>>>     on Tue, 13 Sep 2016 09:06:00 -0700 writes:

    > While you are editing that, you might change its name from 'stderr'
    > to standardError (or standard_error, etc.) so as not to conflict with
    > base::stderr().

oh yes.. blush! ..
Martin

    > Bill Dunlap
    > TIBCO Software
    > wdunlap tibco.com

    > On Tue, Sep 13, 2016 at 8:55 AM, Martin Maechler <maechler at stat.math.ethz.ch
    >> wrote:

    >> >>>>> Suharto Anggono Suharto Anggono via R-devel <r-devel at r-project.org>
    >> >>>>>     on Fri, 9 Sep 2016 16:52:01 +0000 writes:
    >> 
    >> > In "An Introduction to R" Version 3.3.1, in "4.2 The function
    >> tapply() and ragged arrays", after
    >> > stderr <- function(x) sqrt(var(x)/length(x))  ,
    >> > there is a note in brackets:
    >> > Writing functions will be considered later in [Writing your own
    >> functions], and in this case was unnecessary as R also has a builtin
    >> function sd().
    >> 
    >> > The part "in this case was unnecessary as R also has a builtin
    >> function sd()" is misleading. The builtin function sd() doesn't calculate
    >> standard error of the mean. It calculates standard deviation. The function
    >> 'stderr' can use 'sd':
    >> > function(x) sd(x)/sqrt(length(x))
    >> 
    >> You are right; thank you Suharto.
    >> It now says
    >> 
    >> (Writing functions will be considered later in @ref{Writing your own
    >> functions}.  Note that @R{}'s a builtin function @code{sd()} is something
    >> different.)
    >> 
    >> ______________________________________________
    >> R-devel at r-project.org mailing list
    >> https://stat.ethz.ch/mailman/listinfo/r-devel
    >> 

    > [[alternative HTML version deleted]]


From suharto_anggono at yahoo.com  Wed Sep 14 17:15:35 2016
From: suharto_anggono at yahoo.com (Suharto Anggono Suharto Anggono)
Date: Wed, 14 Sep 2016 15:15:35 +0000 (UTC)
Subject: [Rd] R-intro: length of 'ifelse' result
References: <1598036366.967396.1473866135477.ref@mail.yahoo.com>
Message-ID: <1598036366.967396.1473866135477@mail.yahoo.com>

In "An Introduction to R" Version 3.3.1, in "9.2.1 Conditional execution: if statements", the last paragraph is the following:
There is a vectorized version of the if/else construct, the ifelse function. This has the form ifelse(condition, a, b) and returns a vector of the length of its longest argument, with elements a[i] if condition[i] is true, otherwise b[i].


In fact, ifelse(condition, a, b) returns a vector of the length of 'condition', even if 'a' or 'b' is longer.


From murdoch.duncan at gmail.com  Wed Sep 14 19:09:37 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Wed, 14 Sep 2016 13:09:37 -0400
Subject: [Rd] R-intro: length of 'ifelse' result
In-Reply-To: <1598036366.967396.1473866135477@mail.yahoo.com>
References: <1598036366.967396.1473866135477.ref@mail.yahoo.com>
	<1598036366.967396.1473866135477@mail.yahoo.com>
Message-ID: <d9a668d0-218c-de9f-ed3c-e8338ae09819@gmail.com>

On 14/09/2016 11:15 AM, Suharto Anggono Suharto Anggono via R-devel wrote:
> In "An Introduction to R" Version 3.3.1, in "9.2.1 Conditional execution: if statements", the last paragraph is the following:
> There is a vectorized version of the if/else construct, the ifelse function. This has the form ifelse(condition, a, b) and returns a vector of the length of its longest argument, with elements a[i] if condition[i] is true, otherwise b[i].
>
>
> In fact, ifelse(condition, a, b) returns a vector of the length of 'condition', even if 'a' or 'b' is longer.

Thanks, I'll fix that.

Duncan Murdoch


From jeroen.ooms at stat.ucla.edu  Thu Sep 15 14:29:36 2016
From: jeroen.ooms at stat.ucla.edu (Jeroen Ooms)
Date: Thu, 15 Sep 2016 14:29:36 +0200
Subject: [Rd] Finalizer execution order question
Message-ID: <CABFfbXubYGPHXc0WAC2P8GWQv1THGHL+N+D2R6f8SPV5Bn2oAw@mail.gmail.com>

Given an externalptr object 'pool' which protects an R object 'prot':

  # SEXP prot = (a dynamically updated list with handles)
  SEXP pool =  R_MakeExternalPtr(p, R_NilValue, prot);
  R_RegisterCFinalizerEx(pool, fin_pool, TRUE);

WRE explains that 'prot' remains in existence as long as 'pool' is
around. Does this also mean 'prot' still exists when the finalizer of
'pool' gets executed?

Long story: I am running into an issue with the next generation of the
curl package which uses dynamic pools of (many) http request handles.
Both the pool and the handles are externalptr objects with finalizers
that clean up after themselves.

When the pool goes out of scope, its finalizer has to loop over
pending handles to release them. However, to my surprise, the
finalizers of the handles get executed *before* the finalizer of
'pool'. Therefore the handles have destroyed themselves before I can
properly take hem out of the pool. This is exactly what I was trying
to prevent by putting the handles in a 'prot' list.

I wrote a simple package [1] that illustrates this issue. To run the example:

  devtools::install_github("jeroenooms/test")
  library(test)
  pool <- make_pool()
  show_handles(pool)
  rm(pool)
  gc()

What this example is supposed to illustrate is that even though 'prot'
gets protected from GC while the externalptr is around, the finalizers
in 'prot' have already executed when the externalptr gets finalized.

What is even stranger (to me) is that the SEXPs in 'prot' still seem
in tact during the finalizer of externalptr (ASAN is not giving
use-after-free warnings either). It's just that their finalizers have
already executed. So that leaves the pool finalizer in the odd
position of seeing only the post-mortem SEXP contents of the handles.
This does provide a workaround in my case, but can we rely on this?

So my question: is this expected behavior? What would be an
alternative approach to ensure that handles stay protected until the
pool has been cleaned and finalized (rather than running all
finalizers in the order they were registered)?


[1] https://github.com/jeroenooms/test


From luke-tierney at uiowa.edu  Thu Sep 15 16:18:12 2016
From: luke-tierney at uiowa.edu (luke-tierney at uiowa.edu)
Date: Thu, 15 Sep 2016 09:18:12 -0500
Subject: [Rd] Finalizer execution order question
In-Reply-To: <CABFfbXubYGPHXc0WAC2P8GWQv1THGHL+N+D2R6f8SPV5Bn2oAw@mail.gmail.com>
References: <CABFfbXubYGPHXc0WAC2P8GWQv1THGHL+N+D2R6f8SPV5Bn2oAw@mail.gmail.com>
Message-ID: <alpine.DEB.2.20.1609150909380.2394@luke-Latitude>

On Thu, 15 Sep 2016, Jeroen Ooms wrote:

> Given an externalptr object 'pool' which protects an R object 'prot':
>
>  # SEXP prot = (a dynamically updated list with handles)
>  SEXP pool =  R_MakeExternalPtr(p, R_NilValue, prot);
>  R_RegisterCFinalizerEx(pool, fin_pool, TRUE);
>
> WRE explains that 'prot' remains in existence as long as 'pool' is
> around. Does this also mean 'prot' still exists when the finalizer of
> 'pool' gets executed?

prot is still reachable and valid. If it also has a finalizer on it
then that finalizer might have been run before the one on prot -- you
should not assume anything about the order in which finalizers are run.

> Long story: I am running into an issue with the next generation of the
> curl package which uses dynamic pools of (many) http request handles.
> Both the pool and the handles are externalptr objects with finalizers
> that clean up after themselves.
>
> When the pool goes out of scope, its finalizer has to loop over
> pending handles to release them. However, to my surprise, the
> finalizers of the handles get executed *before* the finalizer of
> 'pool'. Therefore the handles have destroyed themselves before I can
> properly take hem out of the pool. This is exactly what I was trying
> to prevent by putting the handles in a 'prot' list.

If your handles are always in a pool, then put a finalizer on the pool
but not on the handles?

Lists of weak references might also be useful to consider for this
sort of thing. I've used those to make sure finalizers are run before
a package DLL is inloaded (since running them later would not be a
good idea).

Best,

luke

>
> I wrote a simple package [1] that illustrates this issue. To run the example:
>
>  devtools::install_github("jeroenooms/test")
>  library(test)
>  pool <- make_pool()
>  show_handles(pool)
>  rm(pool)
>  gc()
>
> What this example is supposed to illustrate is that even though 'prot'
> gets protected from GC while the externalptr is around, the finalizers
> in 'prot' have already executed when the externalptr gets finalized.
>
> What is even stranger (to me) is that the SEXPs in 'prot' still seem
> in tact during the finalizer of externalptr (ASAN is not giving
> use-after-free warnings either). It's just that their finalizers have
> already executed. So that leaves the pool finalizer in the odd
> position of seeing only the post-mortem SEXP contents of the handles.
> This does provide a workaround in my case, but can we rely on this?
>
> So my question: is this expected behavior? What would be an
> alternative approach to ensure that handles stay protected until the
> pool has been cleaned and finalized (rather than running all
> finalizers in the order they were registered)?
>
>
> [1] https://github.com/jeroenooms/test
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

-- 
Luke Tierney
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu


From otoomet at gmail.com  Thu Sep 15 20:15:11 2016
From: otoomet at gmail.com (Ott Toomet)
Date: Thu, 15 Sep 2016 11:15:11 -0700
Subject: [Rd] row names of 'rowsum()'
Message-ID: <CAMMJQ0YFrrqFvOa2j3F=Kc3pTJkefQRq-zBT=AcNm9CMJiBT6A@mail.gmail.com>

'rowsum()' seems to add row names to the resulting matrix, corresponding to
the respective 'group' values.  This is very handy, but it is not
documented.  Should the documentation mention it so it could be relied upon
as part of API?

Cheers,
Ott

-- 
Ott Toomet

Visiting Researcher
School of Information
Mary Gates Hall, Suite 310
University of Washington
Seattle, WA 98195

	[[alternative HTML version deleted]]


From winstonchang1 at gmail.com  Thu Sep 15 21:15:31 2016
From: winstonchang1 at gmail.com (Winston Chang)
Date: Thu, 15 Sep 2016 14:15:31 -0500
Subject: [Rd] Time zone issues when compiling R
Message-ID: <CAFOpNVHzsFL2dTOic8_VULoc_xPwSqA=ZE5TX2wghxkrgc_iYA@mail.gmail.com>

I've been trying to build R 3.3.1 inside of a Nix environment on a
Ubuntu 16.04 machine. It builds, but then it fails a regression test
related to time zones, and I hope that someone could help me debug the
problem.

The failing test is in tests/reg-tests-rc.R
(https://github.com/wch/r-source/blob/c3fe9cd4/tests/reg-tests-1c.R#L1577-L1587):

## format.POSIXlt() of Jan.1 if  1941 or '42 is involved:
tJan1 <- function(n1, n2)
    strptime(paste0(n1:n2,"/01/01"), "%Y/%m/%d", tz="CET")
wDSTJan1 <- function(n1, n2)
    which("CEST" == sub(".* ", '', format(tJan1(n1,n2), usetz=TRUE)))
(w8 <- wDSTJan1(1801, 2300))
(w9 <- wDSTJan1(1901, 2300))
stopifnot(identical(w8, 141:142),# exactly 1941:1942 had CEST on Jan.1
          identical(w9,  41: 42))
## for R-devel Jan.2016 to Mar.14 -- *AND* for R 3.2.4 -- the above gave
## integer(0)  and  c(41:42, 99:100, ..., 389:390)  respectively


The resulting output is:
> (w8 <- wDSTJan1(1801, 2300))
integer(0)
> (w9 <- wDSTJan1(1901, 2300))
integer(0)
> stopifnot(identical(w8, 141:142),# exactly 1941:1942 had CEST on Jan.1
+           identical(w9,  41: 42))
Error: identical(w8, 141:142) is not TRUE
Execution halted


In this build of R, I get the following:
> strptime(paste0(1940:1945,"/01/01"), "%Y/%m/%d", tz="CET")
[1] "1940-01-01 CET" "1941-01-01 CET" "1942-01-01 CET" "1943-01-01 CET"
[5] "1944-01-01 CET" "1945-01-01 CET"


However, when I run the same code in R 3.3.1 installed via the .deb
packages on cran.r-project.org, I get a different result: years 1941
and 1942 have the CEST time zone. This is what the tests above were
expecting.
>  strptime(paste0(1940:1945,"/01/01"), "%Y/%m/%d", tz="CET")
[1] "1940-01-01 CET"  "1941-01-01 CEST" "1942-01-01 CEST" "1943-01-01 CET"
[5] "1944-01-01 CET"  "1945-01-01 CET"


I'm not sure where to start looking to fix this problem, and I'd
appreciate any pointers.


For reference, the test was introduced in:
  https://github.com/wch/r-source/commit/55cdf88dv

And that commit's message says that it fixed a bug introduced by:
    https://github.com/wch/r-source/commit/2e36b365

-Winston


From alexis.sarda at gmail.com  Fri Sep 16 12:41:14 2016
From: alexis.sarda at gmail.com (Alexis Sarda)
Date: Fri, 16 Sep 2016 12:41:14 +0200
Subject: [Rd] Numerical accuracy of matrix multiplication
Message-ID: <CAA4naPDPXh2Q+KUMQhBJs4cecOC7uetbxihz_fxxk92JZNPLKg@mail.gmail.com>

Hello,

while testing the crossprod() function under Linux, I noticed the following:

set.seed(883)
x <- rnorm(100)
x %*% x - sum(x^2) # equal to 1.421085e-14

Is this difference normal? It seems to be rather large for double precision.

Regards,
Alexis.

	[[alternative HTML version deleted]]


From pdalgd at gmail.com  Fri Sep 16 13:33:11 2016
From: pdalgd at gmail.com (peter dalgaard)
Date: Fri, 16 Sep 2016 13:33:11 +0200
Subject: [Rd] Numerical accuracy of matrix multiplication
In-Reply-To: <CAA4naPDPXh2Q+KUMQhBJs4cecOC7uetbxihz_fxxk92JZNPLKg@mail.gmail.com>
References: <CAA4naPDPXh2Q+KUMQhBJs4cecOC7uetbxihz_fxxk92JZNPLKg@mail.gmail.com>
Message-ID: <FAAEBE2A-0C85-40B1-8305-27452CF7A00C@gmail.com>


On 16 Sep 2016, at 12:41 , Alexis Sarda <alexis.sarda at gmail.com> wrote:

> Hello,
> 
> while testing the crossprod() function under Linux, I noticed the following:
> 
> set.seed(883)
> x <- rnorm(100)
> x %*% x - sum(x^2) # equal to 1.421085e-14
> 
> Is this difference normal? It seems to be rather large for double precision.
> 

It's less than .Machine$double.eps, relative (!) to x  %*% x ~= 100.

-pd

> Regards,
> Alexis.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From r.bodewits at home.nl  Fri Sep 16 18:01:49 2016
From: r.bodewits at home.nl (Richard Bodewits)
Date: Fri, 16 Sep 2016 18:01:49 +0200
Subject: [Rd] getGraphicsEvent() questions, minor feature/tweak request,
 and patch(es).
Message-ID: <d9c233c6-c048-37b7-cb31-f39ae0499b1f@home.nl>

Hey all, new R user and first timer here.

I've been using getGraphicsEvent() on an X11-Xlib device in a kind of
interactive loop, and would like to be able to stop it from printing
newlines in the console when I don't actually want to print a prompt.
Even an empty "" value still causes a newline right now.

To not break any code that depends on this behavior, I figured getting
C_getGraphicsEvent to accept an NA value would be the way to go. It
strikes me as highly unlikely that there's any code out there built on
expecting getGraphicsEvent() to error out when getting a 'prompt = NA'
parameter, so this seems like a safe change to make.

After rooting about in the R source for a bit I've found a way to
implement this change myself in src/main/gevents.c, and I've included a
patch as attachment. What I don't know is if there's a better or at
least more conventional way to be checking for the NA value.

Coercing with asChar() and only comparing against R_NaString causes NULL
values, functions, expression objects and potentially others to become
valid prompt values as well, so I'm using TYPEOF() to restrict accepted
values further. I've found manually entered NA values are interpreted as
being of LGLSXP, so that's the only type I'm accepting NA values for. Is
there a better way? There's an isna() in library/utils/src/io.c, but
that module doesn't seem to be available to the linker in the current
build script for gevents.c's module, which would make it a more invasive
patch to use. I'm also not certain it'd work the way I'd want it to,
from looking at its internals.

As far as I can tell, the prompt value is only used in two places;
do_getGraphicsEvent() in src/main/gevents.c, and GA_eventHelper() in
library/grDevices/src/devWindows.c. The latter has a sanity check for
string-ness already, so making NA a possibility should be safe there, as
well.

That apparent lack of use elsewhere leads me to a question about the
help file for getGraphicsEvent(). In it, the claim is made that 'prompt'
is used to display a prompt text on the device. This might be true in MS
Windows, but for X11 the prompt value seems to be solely used for
Rprintf() to the console. Is this discrepency a bug or missing feature
in the X11 implementation, an incorrect wording in the help, or an
interpretation error on my side? Whatever it is, I've also included a
minor patch for the help file to at least indicate support for 'prompt =
NA'.

Apologies if I'm doing this all wrong, or if I'm missing obvious reasons
why these patches are unacceptable. As I said, first timer here and new
to R in general. ;-)


- Richard Bodewits
-------------- next part --------------
Index: src/main/gevents.c
===================================================================
--- src/main/gevents.c	(revision 71269)
+++ src/main/gevents.c	(working copy)
@@ -135,7 +135,7 @@
     checkArity(op, args);
 
     prompt = CAR(args);
-    if (!isString(prompt) || !length(prompt)) error(_("invalid prompt"));
+    if ((!isString(prompt) || !length(prompt)) && (TYPEOF(prompt) != LGLSXP || asChar(prompt) != R_NaString)) error(_("invalid prompt"));
 
     /* NB:  cleanup of event handlers must be done by driver in onExit handler */
 
@@ -159,8 +159,10 @@
 	if (!count)
 	    error(_("no graphics event handlers set"));
 
-	Rprintf("%s\n", CHAR(asChar(prompt)));
-	R_FlushConsole();
+    if (TYPEOF(prompt) != LGLSXP || asChar(prompt) != R_NaString) {
+        Rprintf("%s\n", CHAR(asChar(prompt)));
+        R_FlushConsole();
+    }
 
 	/* Poll them */
 	while (result == R_NilValue) {
-------------- next part --------------
Index: src/library/grDevices/man/getGraphicsEvent.Rd
===================================================================
--- src/library/grDevices/man/getGraphicsEvent.Rd	(revision 71269)
+++ src/library/grDevices/man/getGraphicsEvent.Rd	(working copy)
@@ -24,12 +24,12 @@
 
 }
 \arguments{
-  \item{prompt}{prompt to be displayed to the user in the graphics window}
+  \item{prompt}{prompt to be displayed to the user in the graphics window, or NA for no prompt}
   \item{onMouseDown}{a function to respond to mouse clicks}
   \item{onMouseMove}{a function to respond to mouse movement}
   \item{onMouseUp}{a function to respond to mouse button releases}
   \item{onKeybd}{a function to respond to key presses}
-  \item{consolePrompt}{prompt to be displayed to the user in the console}
+  \item{consolePrompt}{prompt to be displayed to the user in the console, or NA for no prompt}
   \item{which}{which graphics device does the call apply to?}
   \item{...}{items including handlers to be placed in the event environment}
   \item{env}{an environment to be used as the event environment}

From r.bodewits at home.nl  Sat Sep 17 17:29:34 2016
From: r.bodewits at home.nl (Richard Bodewits)
Date: Sat, 17 Sep 2016 17:29:34 +0200
Subject: [Rd] Handlers in setGraphicsEventHandlers() can recursively call
 getGraphicsEvent(). Intended behavior?
Message-ID: <4c06aff8-8215-c5a1-e367-e1653ee58206@home.nl>

Hey all.

As in general it's a bad idea to allow an event handler to generate an
event, and as comments in the code seem to suggest this isn't the
intention either, I was wondering about recursion in getGraphicsEvent().
In main/gevents.c, both doMouseEvent() and doKeybd() have the following
line;

    dd->gettingEvent = FALSE; /* avoid recursive calls */

And they reset it to TRUE before returning. The effective result of this
is that the event handlers on the R side are allowed to call
getGraphicsEvent(), and recurse until they eventually would run out of
stack space. Though R does catch and handle that cleanly with the error;

    Error: evaluation nested too deeply: infinite recursion /
options(expressions=)?

A quick scan of the SVN logs suggests this code has been untouched since
its introduction in 2004, so I'm left to wonder if this is intended
behavior. It stands out as a bit of a sore thumb due to the generic
check for recursion in do_getGraphicsEvent() in the same file, which
will error out with error(_("recursive use of 'getGraphicsEvent' not
supported")) if dd->gettingEvent is already set to TRUE. Which would
suggest recursively calling it is very much not intended to be possible.

To me, setting gettingEvent to FALSE seems like an easy mistake to make
if you temporarily interpret gettingEvent to mean that event(s) are
allowed to still come in. But the actual interpretation in
do_getGraphicsEvents() is the opposite, as it's interpreted as an
indicator of whether or not an event is currently being processed.

I've removed the gettingEvent altering lines from both doMouseEvent()
and doKeybd() to no ill effect, and doing so disabled the ability to
call getGraphicsEvent() from inside one of the assigned handlers as
expected. But after 12 (!) years, it's conceivable that people have come
to depend on this behavior in existing scripts. Is this something that
should be left alone to minimize disruption? Or should this be fixed (if
it is indeed unintended) for the sake of protecting people from infinite
recursions?

I've included a small patch as attachment that removes the offending lines.


- Richard Bodewits
-------------- next part --------------
Index: src/main/gevents.c
===================================================================
--- src/main/gevents.c	(revision 71293)
+++ src/main/gevents.c	(working copy)
@@ -211,8 +211,6 @@
     int i;
     SEXP handler, bvec, sx, sy, temp, result;
 
-    dd->gettingEvent = FALSE; /* avoid recursive calls */
-
     PROTECT(handler = findVar(install(mouseHandlers[event]), dd->eventEnv));
     if (TYPEOF(handler) == PROMSXP) {
 	handler = eval(handler, dd->eventEnv);
@@ -242,7 +240,7 @@
 	R_FlushConsole();
     }
     UNPROTECT(1); /* handler */
-    dd->gettingEvent = TRUE;
+
     return;
 }
 
@@ -257,8 +255,6 @@
 {
     SEXP handler, skey, temp, result;
 
-    dd->gettingEvent = FALSE; /* avoid recursive calls */
-
     PROTECT(handler = findVar(install(keybdHandler), dd->eventEnv));
     if (TYPEOF(handler) == PROMSXP) {
 	handler = eval(handler, dd->eventEnv);
@@ -277,6 +273,6 @@
 	R_FlushConsole();
     }
     UNPROTECT(1); /* handler */
-    dd->gettingEvent = TRUE;
+
     return;
 }

From r.bodewits at home.nl  Sun Sep 18 14:12:37 2016
From: r.bodewits at home.nl (Richard Bodewits)
Date: Sun, 18 Sep 2016 14:12:37 +0200
Subject: [Rd] getGraphicsEvent() questions, minor feature/tweak request,
 and patch(es).
In-Reply-To: <d9c233c6-c048-37b7-cb31-f39ae0499b1f@home.nl>
References: <d9c233c6-c048-37b7-cb31-f39ae0499b1f@home.nl>
Message-ID: <1b8d557e-3e21-0950-d3de-ab7ee5e3d064@home.nl>

Attached a minor revision of the previous patch, to avoid NA_character_
prompt values from being printed as NA.


- Richard Bodewits



On 09/16/2016 06:01 PM, Richard Bodewits wrote:
> Hey all, new R user and first timer here.
> 
> I've been using getGraphicsEvent() on an X11-Xlib device in a kind of
> interactive loop, and would like to be able to stop it from printing
> newlines in the console when I don't actually want to print a prompt.
> Even an empty "" value still causes a newline right now.
> 
> To not break any code that depends on this behavior, I figured getting
> C_getGraphicsEvent to accept an NA value would be the way to go. It
> strikes me as highly unlikely that there's any code out there built on
> expecting getGraphicsEvent() to error out when getting a 'prompt = NA'
> parameter, so this seems like a safe change to make.
> 
> After rooting about in the R source for a bit I've found a way to
> implement this change myself in src/main/gevents.c, and I've included a
> patch as attachment. What I don't know is if there's a better or at
> least more conventional way to be checking for the NA value.
> 
> Coercing with asChar() and only comparing against R_NaString causes NULL
> values, functions, expression objects and potentially others to become
> valid prompt values as well, so I'm using TYPEOF() to restrict accepted
> values further. I've found manually entered NA values are interpreted as
> being of LGLSXP, so that's the only type I'm accepting NA values for. Is
> there a better way? There's an isna() in library/utils/src/io.c, but
> that module doesn't seem to be available to the linker in the current
> build script for gevents.c's module, which would make it a more invasive
> patch to use. I'm also not certain it'd work the way I'd want it to,
> from looking at its internals.
> 
> As far as I can tell, the prompt value is only used in two places;
> do_getGraphicsEvent() in src/main/gevents.c, and GA_eventHelper() in
> library/grDevices/src/devWindows.c. The latter has a sanity check for
> string-ness already, so making NA a possibility should be safe there, as
> well.
> 
> That apparent lack of use elsewhere leads me to a question about the
> help file for getGraphicsEvent(). In it, the claim is made that 'prompt'
> is used to display a prompt text on the device. This might be true in MS
> Windows, but for X11 the prompt value seems to be solely used for
> Rprintf() to the console. Is this discrepency a bug or missing feature
> in the X11 implementation, an incorrect wording in the help, or an
> interpretation error on my side? Whatever it is, I've also included a
> minor patch for the help file to at least indicate support for 'prompt =
> NA'.
> 
> Apologies if I'm doing this all wrong, or if I'm missing obvious reasons
> why these patches are unacceptable. As I said, first timer here and new
> to R in general. ;-)
> 
> 
> - Richard Bodewits
> 
> 
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 
-------------- next part --------------
Index: src/main/gevents.c
===================================================================
--- src/main/gevents.c	(revision 71298)
+++ src/main/gevents.c	(working copy)
@@ -135,7 +135,7 @@
     checkArity(op, args);
 
     prompt = CAR(args);
-    if (!isString(prompt) || !length(prompt)) error(_("invalid prompt"));
+    if ((!isString(prompt) || !length(prompt)) && (TYPEOF(prompt) != LGLSXP || asChar(prompt) != R_NaString)) error(_("invalid prompt"));
 
     /* NB:  cleanup of event handlers must be done by driver in onExit handler */
 
@@ -159,8 +159,10 @@
 	if (!count)
 	    error(_("no graphics event handlers set"));
 
-	Rprintf("%s\n", CHAR(asChar(prompt)));
-	R_FlushConsole();
+    if (asChar(prompt) != R_NaString) {
+        Rprintf("%s\n", CHAR(asChar(prompt)));
+        R_FlushConsole();
+    }
 
 	/* Poll them */
 	while (result == R_NilValue) {

From r.bodewits at home.nl  Sun Sep 18 14:56:57 2016
From: r.bodewits at home.nl (Richard Bodewits)
Date: Sun, 18 Sep 2016 14:56:57 +0200
Subject: [Rd] getGraphicsEvent() and setTimeLimit() bug and compatibility
	patch.
Message-ID: <83109014-eb3c-e641-cf36-84832d48c568@home.nl>

Hey all.

Setting a time limit with setTimeLimit(), and then using
getGraphicsEvent(), will cause graphics event handling for the current
device to break on timeout, until the device is destroyed and recreated.

The problem lies in do_getGraphicsEvent() checking the value of
dd->gettingEvent and concluding it's being called recursively
(ironically this same test fails to detect actual recursion). The
gettingEvent value is not reset on error conditions, so the device
becomes unusable for the remainder of its life.

As far as I've been able to tell, the values set with setTimeLimit() are
checked in R_ProcessEvents(), which is defined separately in
gnuwin32/system.c and unix/sys-unix.c. If a time limit is exceeded, it
error()s out. That in turn percolates up and through jump_to_top_ex(),
as defined in main/errors.c, which eventually calls GEonExit() in
main/engine.c, a function meant to reset some state on graphics devices
and which from the look of it and its comments was added to fix a
similar bug with recordGraphics().

I've added a single line to GEonExit(), to also reset gettingEvent back
to FALSE, which seems to have no side effects, and serves to make
getGraphicsEvent() timeout-friendly. It errors out as expected, and can
then be reused immediately. The patch is attached to this mail.

One problem with this, is recursion. My previous patch fixes the problem
of do_getGraphicsEvent() being called recursively from its own handlers,
but without that patch it becomes possible that R_ProcessEvents()
triggers a timeout while we're in a recursive call of
do_getGraphicsEvent(). Resetting gettingEvent would then potentially
affect all parent do_getGraphicsEvent() calls in the recursion stack.

Currently do_getGraphicsEvent() does not actually check the value of
gettingEvent after it would have triggered a recursion, but this seems
like a landmine for future changes to step on. To support setTimeLimit()
properly without applying the recursion fix, would require some sort of
stack-aware alternative to gettingEvent's current single boolean
implementation.

So short version; this patch builds on my do_getGraphicsEvent()
recursion patch, and will fix getGraphicsEvent() choking on
setTimeLimit() timing out.


- Richard Bodewits
-------------- next part --------------
Index: src/main/engine.c
===================================================================
--- src/main/engine.c	(revision 71293)
+++ src/main/engine.c	(working copy)
@@ -3043,6 +3043,7 @@
 	    gd = GEgetDevice(devNum);
 	    gd->recordGraphics = TRUE;
 	    dd = gd->dev;
+	    dd->gettingEvent = FALSE; // Added to allow setTimeLimit() to be used with getGraphicsEvent().
 	    if (dd->onExit) dd->onExit(dd);
 	    devNum = nextDevice(devNum);
 	}

From S.Ellison at LGCGroup.com  Mon Sep 19 17:41:43 2016
From: S.Ellison at LGCGroup.com (S Ellison)
Date: Mon, 19 Sep 2016 16:41:43 +0100
Subject: [Rd] row names of 'rowsum()'
In-Reply-To: <CAMMJQ0YFrrqFvOa2j3F=Kc3pTJkefQRq-zBT=AcNm9CMJiBT6A@mail.gmail.com>
References: <CAMMJQ0YFrrqFvOa2j3F=Kc3pTJkefQRq-zBT=AcNm9CMJiBT6A@mail.gmail.com>
Message-ID: <1A8C1289955EF649A09086A153E2672403FE677130@GBTEDVPEXCMB04.corp.lgc-group.com>

> 'rowsum()' seems to add row names to the resulting matrix, corresponding to
> the respective 'group' values.  This is very handy, but it is not documented.
> Should the documentation mention it so it could be relied upon as part of API?

If you're referring to base::rowSums, the 'value' section of the help page says 
" A numeric or complex array of suitable size, or a vector if the
  result is one-dimensional.  For the first four functions the
  'dimnames' (or 'names' for a vector result) are taken from the
  original array. "

S Ellison


*******************************************************************
This email and any attachments are confidential. Any use...{{dropped:8}}


From otoomet at gmail.com  Mon Sep 19 18:05:13 2016
From: otoomet at gmail.com (Ott Toomet)
Date: Mon, 19 Sep 2016 09:05:13 -0700
Subject: [Rd] row names of 'rowsum()'
In-Reply-To: <1A8C1289955EF649A09086A153E2672403FE677130@GBTEDVPEXCMB04.corp.lgc-group.com>
References: <CAMMJQ0YFrrqFvOa2j3F=Kc3pTJkefQRq-zBT=AcNm9CMJiBT6A@mail.gmail.com>
	<1A8C1289955EF649A09086A153E2672403FE677130@GBTEDVPEXCMB04.corp.lgc-group.com>
Message-ID: <CAMMJQ0YyoPsf3Aa1zdaJ1H-dNsJHgcA6chWWFc=Gtf78sbCRRg@mail.gmail.com>

I am referring to base::rowsum(), not rowSums().  For some reason I cannot
access it's help on my computer but the online documentation (R-devel
version) states:

Value
A matrix or data frame containing the sums. There will be one row per
unique value of group

Period.  Above, the argument 'reorder' is mentioned which orders rows to
'agree with tapply'.

Cheers,
Ott

On Mon, Sep 19, 2016 at 8:41 AM, S Ellison <S.Ellison at lgcgroup.com> wrote:

> > 'rowsum()' seems to add row names to the resulting matrix, corresponding
> to
> > the respective 'group' values.  This is very handy, but it is not
> documented.
> > Should the documentation mention it so it could be relied upon as part
> of API?
>
> If you're referring to base::rowSums, the 'value' section of the help page
> says
> " A numeric or complex array of suitable size, or a vector if the
>   result is one-dimensional.  For the first four functions the
>   'dimnames' (or 'names' for a vector result) are taken from the
>   original array. "
>
> S Ellison
>
>
> *******************************************************************
> This email and any attachments are confidential. Any u...{{dropped:23}}


From hastie at stanford.edu  Mon Sep 19 22:13:29 2016
From: hastie at stanford.edu (Trevor John Hastie)
Date: Mon, 19 Sep 2016 20:13:29 +0000
Subject: [Rd] Subsetting issue in model.frame with na.omit
Message-ID: <D4DBD9FF-9C9D-4393-AB8B-492FB441E8E5@stanford.edu>

Running R version 3.3.1 (2016-06-21) Bug in Your Hair

I have discovered an issue with model.frame() with regard to its
implementation of the na.action argument. This impacts the gam package.

We are expecting the last thing to happen in model.frame() is that it
	runs na.action on the frame it has produced.  In the example
	below, we use "na.action=na.omit", which calls for subsetting
	out rows of the frame.  However, when it does this, it does
	not see that there is a [.smooth method for the two columns,
	which are of S3 class "smooth". So it does do the subsetting,
	but does not use the subset methods. In my example, this is
	evidenced by the attribute element $NAs of (each) of
	the components still being present.

When instead, I use "na.action=na.pass" in the call to model.frame,
and then filter the resulting frame through na.omit(), it does the right thing.
The $NAs component has disappeared, which is what should have
happened here.

set.seed(101)
n=30
x=matrix(runif(n*2),n,2)
x[sample(1:20,6,replace=FALSE)]=NA
dx=data.frame(x)
library(gam)
###Compare
m=model.frame(~s(X1,df=4)+s(X2,df=4),data=dx,na.action=na.omit)
attributes(m[[1]])
###with
m=model.frame(~s(X1,df=4)+s(X2,df=4),data=dx,na.action=na.pass)
m=na.omit(m)
attributes(m[[1]])

------------------------------------------------------------------------------
  Trevor Hastie                                   hastie at stanford.edu  
  Professor, Department of Statistics, Stanford University
  Phone: (650) 725-2231                 Fax: (650) 725-8977  
  URL: http://www.stanford.edu/~hastie  
   address: room 104, Department of Statistics, Sequoia Hall
           390 Serra Mall, Stanford University, CA 94305-4065  
 ------------------------------------------------------------------------------


From maechler at stat.math.ethz.ch  Tue Sep 20 17:27:03 2016
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 20 Sep 2016 17:27:03 +0200
Subject: [Rd] Numerical accuracy of matrix multiplication
In-Reply-To: <FAAEBE2A-0C85-40B1-8305-27452CF7A00C@gmail.com>
References: <CAA4naPDPXh2Q+KUMQhBJs4cecOC7uetbxihz_fxxk92JZNPLKg@mail.gmail.com>
	<FAAEBE2A-0C85-40B1-8305-27452CF7A00C@gmail.com>
Message-ID: <22497.21831.450319.430114@stat.math.ethz.ch>

>>>>> peter dalgaard <pdalgd at gmail.com>
>>>>>     on Fri, 16 Sep 2016 13:33:11 +0200 writes:

    > On 16 Sep 2016, at 12:41 , Alexis Sarda <alexis.sarda at gmail.com> wrote:

    >> Hello,
    >> 
    >> while testing the crossprod() function under Linux, I noticed the following:
    >> 
    >> set.seed(883)
    >> x <- rnorm(100)
    >> x %*% x - sum(x^2) # equal to 1.421085e-14
    >> 
    >> Is this difference normal? It seems to be rather large for double precision.
    >> 

    > It's less than .Machine$double.eps, relative (!) to x  %*% x ~= 100.

indeed!

Still, it gives exactly 0 on my platform(s), where I'm using R's
own version of BLAS / Lapack.

Are you perhaps using an "optimized" BLAS / LAPACK , i.e, one
that is fast but slightly less so accurate ?

Martin Maechler,
ETH Zurich


    > -pd

    >> Regards,
    >> Alexis.
    >> 
    >> [[alternative HTML version deleted]]
    >> 
    >> ______________________________________________
    >> R-devel at r-project.org mailing list
    >> https://stat.ethz.ch/mailman/listinfo/r-devel

    > -- 
    > Peter Dalgaard, Professor,
    > Center for Statistics, Copenhagen Business School
    > Solbjerg Plads 3, 2000 Frederiksberg, Denmark
    > Phone: (+45)38153501
    > Office: A 4.23
    > Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com

    > ______________________________________________
    > R-devel at r-project.org mailing list
    > https://stat.ethz.ch/mailman/listinfo/r-devel


From maechler at stat.math.ethz.ch  Tue Sep 20 17:42:11 2016
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 20 Sep 2016 17:42:11 +0200
Subject: [Rd] Numerical accuracy of matrix multiplication
In-Reply-To: <CAA4naPDv07XZuOUo_LyH50z_0GZV-kgn2LTHhTVk2OTUdP_KEA@mail.gmail.com>
References: <CAA4naPDPXh2Q+KUMQhBJs4cecOC7uetbxihz_fxxk92JZNPLKg@mail.gmail.com>
	<FAAEBE2A-0C85-40B1-8305-27452CF7A00C@gmail.com>
	<22497.21831.450319.430114@stat.math.ethz.ch>
	<CAA4naPDv07XZuOUo_LyH50z_0GZV-kgn2LTHhTVk2OTUdP_KEA@mail.gmail.com>
Message-ID: <22497.22739.270609.943810@stat.math.ethz.ch>

>>>>> Alexis Sarda <alexis.sarda at gmail.com>
>>>>>     on Tue, 20 Sep 2016 17:33:49 +0200 writes:

    > I just realized that I was actually using a different random number
    > generator, could that be a valid reason for the discrepancy?

    > The code should be:

    > RNGkind("L'Ecuyer")
    > set.seed(883)
    > x <- rnorm(100)
    > x %*% x - sum(x^2) # equal to 1.421085e-14


Yes, now I get the same result so my story on  "BLAS / LAPACK"
is not relevant here.

But do note the main point from Peter Dalgaard that this is well
within Machine epsilon precision.

More precisely, here it is really one bit difference in the
least significant bit :

> print(rbind( x%*%x, crossprod(x), sum(x^2)), digits= 19)
                     [,1]
[1,] 103.5096830356289814
[2,] 103.5096830356289814
[3,] 103.5096830356289672
> cbind(sprintf("%a", c(x%*%x, crossprod(x), sum(x^2))))
     [,1]                  
[1,] "0x1.9e09ea598568fp+6"
[2,] "0x1.9e09ea598568fp+6"
[3,] "0x1.9e09ea598568ep+6"
> 


    > Regards,
    > Alexis Sarda.



    > On Tue, Sep 20, 2016 at 5:27 PM, Martin Maechler <maechler at stat.math.ethz.ch
    >> wrote:

    >> >>>>> peter dalgaard <pdalgd at gmail.com>
    >> >>>>>     on Fri, 16 Sep 2016 13:33:11 +0200 writes:
    >> 
    >> > On 16 Sep 2016, at 12:41 , Alexis Sarda <alexis.sarda at gmail.com>
    >> wrote:
    >> 
    >> >> Hello,
    >> >>
    >> >> while testing the crossprod() function under Linux, I noticed the
    >> following:
    >> >>
    >> >> set.seed(883)
    >> >> x <- rnorm(100)
    >> >> x %*% x - sum(x^2) # equal to 1.421085e-14
    >> >>
    >> >> Is this difference normal? It seems to be rather large for double
    >> precision.
    >> >>
    >> 
    >> > It's less than .Machine$double.eps, relative (!) to x  %*% x ~= 100.
    >> 
    >> indeed!
    >> 
    >> Still, it gives exactly 0 on my platform(s), where I'm using R's
    >> own version of BLAS / Lapack.
    >> 
    >> Are you perhaps using an "optimized" BLAS / LAPACK , i.e, one
    >> that is fast but slightly less so accurate ?
    >> 
    >> Martin Maechler,
    >> ETH Zurich
    >> 
    >> 
    >> > -pd
    >> 
    >> >> Regards,
    >> >> Alexis.
    >> >>
    >> >> [[alternative HTML version deleted]]
    >> >>
    >> >> ______________________________________________
    >> >> R-devel at r-project.org mailing list
    >> >> https://stat.ethz.ch/mailman/listinfo/r-devel
    >> 
    >> > --
    >> > Peter Dalgaard, Professor,
    >> > Center for Statistics, Copenhagen Business School
    >> > Solbjerg Plads 3, 2000 Frederiksberg, Denmark
    >> > Phone: (+45)38153501
    >> > Office: A 4.23
    >> > Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
    >> 
    >> > ______________________________________________
    >> > R-devel at r-project.org mailing list
    >> > https://stat.ethz.ch/mailman/listinfo/r-devel
    >> 

    > [[alternative HTML version deleted]]


From alexis.sarda at gmail.com  Tue Sep 20 17:33:49 2016
From: alexis.sarda at gmail.com (Alexis Sarda)
Date: Tue, 20 Sep 2016 17:33:49 +0200
Subject: [Rd] Numerical accuracy of matrix multiplication
In-Reply-To: <22497.21831.450319.430114@stat.math.ethz.ch>
References: <CAA4naPDPXh2Q+KUMQhBJs4cecOC7uetbxihz_fxxk92JZNPLKg@mail.gmail.com>
	<FAAEBE2A-0C85-40B1-8305-27452CF7A00C@gmail.com>
	<22497.21831.450319.430114@stat.math.ethz.ch>
Message-ID: <CAA4naPDv07XZuOUo_LyH50z_0GZV-kgn2LTHhTVk2OTUdP_KEA@mail.gmail.com>

I just realized that I was actually using a different random number
generator, could that be a valid reason for the discrepancy?

The code should be:

RNGkind("L'Ecuyer")
set.seed(883)
x <- rnorm(100)
x %*% x - sum(x^2) # equal to 1.421085e-14


Regards,
Alexis Sarda.



On Tue, Sep 20, 2016 at 5:27 PM, Martin Maechler <maechler at stat.math.ethz.ch
> wrote:

> >>>>> peter dalgaard <pdalgd at gmail.com>
> >>>>>     on Fri, 16 Sep 2016 13:33:11 +0200 writes:
>
>     > On 16 Sep 2016, at 12:41 , Alexis Sarda <alexis.sarda at gmail.com>
> wrote:
>
>     >> Hello,
>     >>
>     >> while testing the crossprod() function under Linux, I noticed the
> following:
>     >>
>     >> set.seed(883)
>     >> x <- rnorm(100)
>     >> x %*% x - sum(x^2) # equal to 1.421085e-14
>     >>
>     >> Is this difference normal? It seems to be rather large for double
> precision.
>     >>
>
>     > It's less than .Machine$double.eps, relative (!) to x  %*% x ~= 100.
>
> indeed!
>
> Still, it gives exactly 0 on my platform(s), where I'm using R's
> own version of BLAS / Lapack.
>
> Are you perhaps using an "optimized" BLAS / LAPACK , i.e, one
> that is fast but slightly less so accurate ?
>
> Martin Maechler,
> ETH Zurich
>
>
>     > -pd
>
>     >> Regards,
>     >> Alexis.
>     >>
>     >> [[alternative HTML version deleted]]
>     >>
>     >> ______________________________________________
>     >> R-devel at r-project.org mailing list
>     >> https://stat.ethz.ch/mailman/listinfo/r-devel
>
>     > --
>     > Peter Dalgaard, Professor,
>     > Center for Statistics, Copenhagen Business School
>     > Solbjerg Plads 3, 2000 Frederiksberg, Denmark
>     > Phone: (+45)38153501
>     > Office: A 4.23
>     > Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
>     > ______________________________________________
>     > R-devel at r-project.org mailing list
>     > https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From paul at stat.auckland.ac.nz  Wed Sep 21 03:42:44 2016
From: paul at stat.auckland.ac.nz (Paul Murrell)
Date: Wed, 21 Sep 2016 13:42:44 +1200
Subject: [Rd] Handlers in setGraphicsEventHandlers() can recursively
 call getGraphicsEvent(). Intended behavior?
In-Reply-To: <4c06aff8-8215-c5a1-e367-e1653ee58206@home.nl>
References: <4c06aff8-8215-c5a1-e367-e1653ee58206@home.nl>
Message-ID: <b08014d6-1833-ce3d-c16d-3eb1ffc8c8cf@stat.auckland.ac.nz>

Hi

Is the correct patch to remove the setting of the gettingEvent flag or 
would it be better to flip the TRUE/FALSE setting (set to TRUE before 
handling then reset to FALSE after handling) ?

Also, for this patch and for the other two you sent, one difficulty will 
be with testing the patches.  I have no testing code for this, so would 
need at least a test or two from you (ideally someone would also have 
some regression tests, beyond ?getGraphicsEvent, to ensure continuity of 
previous behaviour).

Paul

On 18/09/2016 3:29 a.m., Richard Bodewits wrote:
> Hey all.
>
> As in general it's a bad idea to allow an event handler to generate an
> event, and as comments in the code seem to suggest this isn't the
> intention either, I was wondering about recursion in getGraphicsEvent().
> In main/gevents.c, both doMouseEvent() and doKeybd() have the following
> line;
>
>     dd->gettingEvent = FALSE; /* avoid recursive calls */
>
> And they reset it to TRUE before returning. The effective result of this
> is that the event handlers on the R side are allowed to call
> getGraphicsEvent(), and recurse until they eventually would run out of
> stack space. Though R does catch and handle that cleanly with the error;
>
>     Error: evaluation nested too deeply: infinite recursion /
> options(expressions=)?
>
> A quick scan of the SVN logs suggests this code has been untouched since
> its introduction in 2004, so I'm left to wonder if this is intended
> behavior. It stands out as a bit of a sore thumb due to the generic
> check for recursion in do_getGraphicsEvent() in the same file, which
> will error out with error(_("recursive use of 'getGraphicsEvent' not
> supported")) if dd->gettingEvent is already set to TRUE. Which would
> suggest recursively calling it is very much not intended to be possible.
>
> To me, setting gettingEvent to FALSE seems like an easy mistake to make
> if you temporarily interpret gettingEvent to mean that event(s) are
> allowed to still come in. But the actual interpretation in
> do_getGraphicsEvents() is the opposite, as it's interpreted as an
> indicator of whether or not an event is currently being processed.
>
> I've removed the gettingEvent altering lines from both doMouseEvent()
> and doKeybd() to no ill effect, and doing so disabled the ability to
> call getGraphicsEvent() from inside one of the assigned handlers as
> expected. But after 12 (!) years, it's conceivable that people have come
> to depend on this behavior in existing scripts. Is this something that
> should be left alone to minimize disruption? Or should this be fixed (if
> it is indeed unintended) for the sake of protecting people from infinite
> recursions?
>
> I've included a small patch as attachment that removes the offending lines.
>
>
> - Richard Bodewits
>
>
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/


From kmillar at google.com  Wed Sep 21 04:18:21 2016
From: kmillar at google.com (Karl Millar)
Date: Tue, 20 Sep 2016 19:18:21 -0700
Subject: [Rd] Undocumented 'use.names' argument to c()
Message-ID: <CABz6aZeFACz08xo8qXpSshDJDfwJpDt5kzLzbVY=gS14UUBEdQ@mail.gmail.com>

'c' has an undocumented 'use.names' argument.  I'm not sure if this is
a documentation or implementation bug.

> c(a = 1)
a
1
> c(a = 1, use.names = F)
[1] 1

Karl


From dwinsemius at comcast.net  Wed Sep 21 08:46:48 2016
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 20 Sep 2016 23:46:48 -0700
Subject: [Rd] Undocumented 'use.names' argument to c()
In-Reply-To: <CABz6aZeFACz08xo8qXpSshDJDfwJpDt5kzLzbVY=gS14UUBEdQ@mail.gmail.com>
References: <CABz6aZeFACz08xo8qXpSshDJDfwJpDt5kzLzbVY=gS14UUBEdQ@mail.gmail.com>
Message-ID: <D28561FA-195C-4342-BB0B-175473288C3A@comcast.net>


> On Sep 20, 2016, at 7:18 PM, Karl Millar via R-devel <r-devel at r-project.org> wrote:
> 
> 'c' has an undocumented 'use.names' argument.  I'm not sure if this is
> a documentation or implementation bug.

It came up on stackoverflow a couple of years ago:

http://stackoverflow.com/questions/24815572/why-does-function-c-accept-an-undocumented-argument/24815653#24815653

At the time it appeared to me to be a documentation lag.


> 
>> c(a = 1)
> a
> 1
>> c(a = 1, use.names = F)
> [1] 1
> 
> Karl
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

David Winsemius
Alameda, CA, USA


From maechler at stat.math.ethz.ch  Wed Sep 21 09:50:33 2016
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 21 Sep 2016 09:50:33 +0200
Subject: [Rd] Undocumented 'use.names' argument to c()
In-Reply-To: <D28561FA-195C-4342-BB0B-175473288C3A@comcast.net>
References: <CABz6aZeFACz08xo8qXpSshDJDfwJpDt5kzLzbVY=gS14UUBEdQ@mail.gmail.com>
	<D28561FA-195C-4342-BB0B-175473288C3A@comcast.net>
Message-ID: <22498.15305.131081.171552@stat.math.ethz.ch>

>>>>> David Winsemius <dwinsemius at comcast.net>
>>>>>     on Tue, 20 Sep 2016 23:46:48 -0700 writes:

    >> On Sep 20, 2016, at 7:18 PM, Karl Millar via R-devel <r-devel at r-project.org> wrote:
    >> 
    >> 'c' has an undocumented 'use.names' argument.  I'm not sure if this is
    >> a documentation or implementation bug.

    > It came up on stackoverflow a couple of years ago:

    > http://stackoverflow.com/questions/24815572/why-does-function-c-accept-an-undocumented-argument/24815653#24815653

    > At the time it appeared to me to be a documentation lag.

Thank you, Karl and David,
yes it is a documentation glitch ... and a bit more:  Experts know that
print()ing of primitive functions is, eehm, "special".

I've committed a change to R-devel ... (with the intent to port
to R-patched).

Martin

    >> 
    >>> c(a = 1)
    >> a
    >> 1
    >>> c(a = 1, use.names = F)
    >> [1] 1
    >> 
    >> Karl


From r.bodewits at home.nl  Wed Sep 21 13:23:44 2016
From: r.bodewits at home.nl (Richard Bodewits)
Date: Wed, 21 Sep 2016 13:23:44 +0200
Subject: [Rd] Handlers in setGraphicsEventHandlers() can recursively
 call getGraphicsEvent(). Intended behavior?
In-Reply-To: <b08014d6-1833-ce3d-c16d-3eb1ffc8c8cf@stat.auckland.ac.nz>
References: <4c06aff8-8215-c5a1-e367-e1653ee58206@home.nl>
	<b08014d6-1833-ce3d-c16d-3eb1ffc8c8cf@stat.auckland.ac.nz>
Message-ID: <005395b9-2749-d7f0-7e8d-664d9aa7aa1f@home.nl>

doKeybd() gets called in CHelpKeyIn() and NHelpKeyIn() in
library/grDevices/src/devWindows.c, where the call is encapsulated in an
'if (dd->gettingEvent)' block. So the only times this code ever calls
doKeybd() is when gettingEvent is in fact set.

Further, it's called in two locations in X11_eventHelper(), in
modules/X11/devX11.c, where the state of gettingEvent seems to be moot
for its handling.

doMouseEvent() in turn is called in HelpMouseClick(), HelpMouseMove(),
and HelpMouseUp() in devWindows.c, where again each call is only made
after checking if gettingEvent is true.

In devX11.c it's called once in X11_eventHelper(), after checking if
gettingEvent is true.

Other than these calls, gettingEvent does not get checked anywhere
outside of main/gevents.c, and nothing called in doKeybd() or
doMouseEvent() depends on its state being toggled either which way.

As far as I've been able to ascertain these lines are completely
superfluous, as well as introducing a recursion issue that they seem to
have been meant to somehow prevent.

As for tests, I can look into cooking something up, though I'm going to
be spread a little thin for the next three months.

After 12 years of this code being in place I doubt I'll be able to cover
every imaginable usecase scenario by myself though.

But thanks for replying. Was starting to think I was screaming into the
void. ;-)


- Richard Bodewits


On 09/21/2016 03:42 AM, Paul Murrell wrote:
> Hi
> 
> Is the correct patch to remove the setting of the gettingEvent flag or
> would it be better to flip the TRUE/FALSE setting (set to TRUE before
> handling then reset to FALSE after handling) ?
> 
> Also, for this patch and for the other two you sent, one difficulty will
> be with testing the patches.  I have no testing code for this, so would
> need at least a test or two from you (ideally someone would also have
> some regression tests, beyond ?getGraphicsEvent, to ensure continuity of
> previous behaviour).
> 
> Paul
> 
> On 18/09/2016 3:29 a.m., Richard Bodewits wrote:
>> Hey all.
>>
>> As in general it's a bad idea to allow an event handler to generate an
>> event, and as comments in the code seem to suggest this isn't the
>> intention either, I was wondering about recursion in getGraphicsEvent().
>> In main/gevents.c, both doMouseEvent() and doKeybd() have the following
>> line;
>>
>>     dd->gettingEvent = FALSE; /* avoid recursive calls */
>>
>> And they reset it to TRUE before returning. The effective result of this
>> is that the event handlers on the R side are allowed to call
>> getGraphicsEvent(), and recurse until they eventually would run out of
>> stack space. Though R does catch and handle that cleanly with the error;
>>
>>     Error: evaluation nested too deeply: infinite recursion /
>> options(expressions=)?
>>
>> A quick scan of the SVN logs suggests this code has been untouched since
>> its introduction in 2004, so I'm left to wonder if this is intended
>> behavior. It stands out as a bit of a sore thumb due to the generic
>> check for recursion in do_getGraphicsEvent() in the same file, which
>> will error out with error(_("recursive use of 'getGraphicsEvent' not
>> supported")) if dd->gettingEvent is already set to TRUE. Which would
>> suggest recursively calling it is very much not intended to be possible.
>>
>> To me, setting gettingEvent to FALSE seems like an easy mistake to make
>> if you temporarily interpret gettingEvent to mean that event(s) are
>> allowed to still come in. But the actual interpretation in
>> do_getGraphicsEvents() is the opposite, as it's interpreted as an
>> indicator of whether or not an event is currently being processed.
>>
>> I've removed the gettingEvent altering lines from both doMouseEvent()
>> and doKeybd() to no ill effect, and doing so disabled the ability to
>> call getGraphicsEvent() from inside one of the assigned handlers as
>> expected. But after 12 (!) years, it's conceivable that people have come
>> to depend on this behavior in existing scripts. Is this something that
>> should be left alone to minimize disruption? Or should this be fixed (if
>> it is indeed unintended) for the sake of protecting people from infinite
>> recursions?
>>
>> I've included a small patch as attachment that removes the offending
>> lines.
>>
>>
>> - Richard Bodewits
>>
>>
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>


From mxkuhn at gmail.com  Wed Sep 21 17:28:46 2016
From: mxkuhn at gmail.com (Max Kuhn)
Date: Wed, 21 Sep 2016 11:28:46 -0400
Subject: [Rd] formal process for orphaning a package
Message-ID: <CAJ9CoW=EcE+Jw1u5UkOy81tY1MJqzVVdVxmAp1p3wEjp2bMeXA@mail.gmail.com>

The CRAN policy page
(https://cran.r-project.org/web/packages/policies.html) implies that
there is a formal procedure for orphaning a package but none is
mentioned in the Extensions manual
(https://cran.r-project.org/doc/manuals/r-devel/R-exts.html).

This page (https://cran.r-project.org/src/contrib/Orphaned/README)
implies that one would simply resubmit the package to CRAN with the
text "ORPHANED" in the `Maintainer` field.

Is this the case?

If this is not documented somewhere, can it be added to the Extensions manual?

Thanks,

Max


From amredd at gmail.com  Wed Sep 21 18:53:09 2016
From: amredd at gmail.com (Andrew Redd)
Date: Wed, 21 Sep 2016 16:53:09 +0000
Subject: [Rd] formal process for orphaning a package
In-Reply-To: <CAJ9CoW=EcE+Jw1u5UkOy81tY1MJqzVVdVxmAp1p3wEjp2bMeXA@mail.gmail.com>
References: <CAJ9CoW=EcE+Jw1u5UkOy81tY1MJqzVVdVxmAp1p3wEjp2bMeXA@mail.gmail.com>
Message-ID: <CAK_CNybVxs32LscopzD4ynPaB-2bo2R4rBGKtQOtE-kUZbxcgg@mail.gmail.com>

The README states clearly that a package is orphaned under any of three
conditions:

   1. The Maintainer requests is.
   2. The maintainer email bounces
   3. The maintainer is unresponsive to requests regarding the package from
   CRAN maintainers

But I think that it is a good idea to include those conditions in the
manuals.

On Wed, Sep 21, 2016 at 9:30 AM Max Kuhn <mxkuhn at gmail.com> wrote:

> The CRAN policy page
> (https://cran.r-project.org/web/packages/policies.html) implies that
> there is a formal procedure for orphaning a package but none is
> mentioned in the Extensions manual
> (https://cran.r-project.org/doc/manuals/r-devel/R-exts.html).
>
> This page (https://cran.r-project.org/src/contrib/Orphaned/README)
> implies that one would simply resubmit the package to CRAN with the
> text "ORPHANED" in the `Maintainer` field.
>
> Is this the case?
>
> If this is not documented somewhere, can it be added to the Extensions
> manual?
>
> Thanks,
>
> Max
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Wed Sep 21 20:06:21 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Wed, 21 Sep 2016 11:06:21 -0700
Subject: [Rd] strcapture enhancement
Message-ID: <CAF8bMcbLMdFkoUAvve7P8Y+MhRbsRE8HsWV8psK2JmOfP=fV=Q@mail.gmail.com>

The new strcapture function in R-devel is handy, capturing
the matches to the parenthesized subpatterns in a regular
expression in the columns of a data.frame, whose column
names and classes are given by the 'proto' argument.  E.g.,

> p1 <- data.frame(Name="", Number=0)
> str(strcapture("([[:alpha:]]*) +([[:digit:]]*)", c("Three 3", "Twenty
20"), proto=p1))
'data.frame':   2 obs. of  2 variables:
 $ Name  : Factor w/ 2 levels "Three","Twenty": 1 2
 $ Number: num  3 20

I think it would be even nicer if it constructed its data.frame
using the check.names=FALSE and stringsAsFactors=FALSE
arguments.  Then the names and types specified in the proto
argument would be respected instead of changing them as
in the following example

> p2 <- data.frame("The Name"="", "The Number"=0, stringsAsFactors=FALSE,
check.names=FALSE)
> str(strcapture("([[:alpha:]]*) +([[:digit:]]*)", c("Three 3", "Twenty
20"), proto=p2))
'data.frame':   2 obs. of  2 variables:
 $ The.Name  : Factor w/ 2 levels "Three","Twenty": 1 2
 $ The.Number: num  3 20


Bill Dunlap
TIBCO Software
wdunlap tibco.com

	[[alternative HTML version deleted]]


From ggrothendieck at gmail.com  Wed Sep 21 20:22:07 2016
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Wed, 21 Sep 2016 14:22:07 -0400
Subject: [Rd] strcapture enhancement
In-Reply-To: <CAF8bMcbLMdFkoUAvve7P8Y+MhRbsRE8HsWV8psK2JmOfP=fV=Q@mail.gmail.com>
References: <CAF8bMcbLMdFkoUAvve7P8Y+MhRbsRE8HsWV8psK2JmOfP=fV=Q@mail.gmail.com>
Message-ID: <CAP01uRm-FJ-+VvQuHNyBbYHEE++vO7k8tMFURcHjwXiq1_7bQg@mail.gmail.com>

Note that read.pattern in gsubfn does accept stringsAsFactors = FALSE,
e.g. using your input lines and pattern:

library(gsubfn)
Lines <- c("Three 3", "Twenty 20")
pat <- "([[:alpha:]]*) +([[:digit:]]*)"

s2 <- read.pattern(text = Lines, pattern = pat, stringsAsFactors = FALSE,
 col.names = c("Name", "Number"))

giving:

> str(s2)
'data.frame':   2 obs. of  2 variables:
 $ Name  : chr  "Three" "Twenty"
 $ Number: int  3 20


On Wed, Sep 21, 2016 at 2:06 PM, William Dunlap via R-devel
<r-devel at r-project.org> wrote:
> The new strcapture function in R-devel is handy, capturing
> the matches to the parenthesized subpatterns in a regular
> expression in the columns of a data.frame, whose column
> names and classes are given by the 'proto' argument.  E.g.,
>
>> p1 <- data.frame(Name="", Number=0)
>> str(strcapture("([[:alpha:]]*) +([[:digit:]]*)", c("Three 3", "Twenty
> 20"), proto=p1))
> 'data.frame':   2 obs. of  2 variables:
>  $ Name  : Factor w/ 2 levels "Three","Twenty": 1 2
>  $ Number: num  3 20
>
> I think it would be even nicer if it constructed its data.frame
> using the check.names=FALSE and stringsAsFactors=FALSE
> arguments.  Then the names and types specified in the proto
> argument would be respected instead of changing them as
> in the following example
>
>> p2 <- data.frame("The Name"="", "The Number"=0, stringsAsFactors=FALSE,
> check.names=FALSE)
>> str(strcapture("([[:alpha:]]*) +([[:digit:]]*)", c("Three 3", "Twenty
> 20"), proto=p2))
> 'data.frame':   2 obs. of  2 variables:
>  $ The.Name  : Factor w/ 2 levels "Three","Twenty": 1 2
>  $ The.Number: num  3 20
>
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel



-- 
Statistics & Software Consulting
GKX Group, GKX Associates Inc.
tel: 1-877-GKX-GROUP
email: ggrothendieck at gmail.com


From lawrence.michael at gene.com  Wed Sep 21 20:43:00 2016
From: lawrence.michael at gene.com (Michael Lawrence)
Date: Wed, 21 Sep 2016 11:43:00 -0700
Subject: [Rd] strcapture enhancement
In-Reply-To: <CAF8bMcbLMdFkoUAvve7P8Y+MhRbsRE8HsWV8psK2JmOfP=fV=Q@mail.gmail.com>
References: <CAF8bMcbLMdFkoUAvve7P8Y+MhRbsRE8HsWV8psK2JmOfP=fV=Q@mail.gmail.com>
Message-ID: <CAOQ5NydA0hYAGBHNPBtbEcTakJfobLhwD+kRDE-eNFPsqTKcFQ@mail.gmail.com>

Thanks for the suggestion. Checked in that change.

Michael

On Wed, Sep 21, 2016 at 11:06 AM, William Dunlap via R-devel
<r-devel at r-project.org> wrote:
> The new strcapture function in R-devel is handy, capturing
> the matches to the parenthesized subpatterns in a regular
> expression in the columns of a data.frame, whose column
> names and classes are given by the 'proto' argument.  E.g.,
>
>> p1 <- data.frame(Name="", Number=0)
>> str(strcapture("([[:alpha:]]*) +([[:digit:]]*)", c("Three 3", "Twenty
> 20"), proto=p1))
> 'data.frame':   2 obs. of  2 variables:
>  $ Name  : Factor w/ 2 levels "Three","Twenty": 1 2
>  $ Number: num  3 20
>
> I think it would be even nicer if it constructed its data.frame
> using the check.names=FALSE and stringsAsFactors=FALSE
> arguments.  Then the names and types specified in the proto
> argument would be respected instead of changing them as
> in the following example
>
>> p2 <- data.frame("The Name"="", "The Number"=0, stringsAsFactors=FALSE,
> check.names=FALSE)
>> str(strcapture("([[:alpha:]]*) +([[:digit:]]*)", c("Three 3", "Twenty
> 20"), proto=p2))
> 'data.frame':   2 obs. of  2 variables:
>  $ The.Name  : Factor w/ 2 levels "Three","Twenty": 1 2
>  $ The.Number: num  3 20
>
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From mxkuhn at gmail.com  Wed Sep 21 20:48:30 2016
From: mxkuhn at gmail.com (Max Kuhn)
Date: Wed, 21 Sep 2016 14:48:30 -0400
Subject: [Rd] formal process for orphaning a package
In-Reply-To: <CAK_CNybVxs32LscopzD4ynPaB-2bo2R4rBGKtQOtE-kUZbxcgg@mail.gmail.com>
References: <CAJ9CoW=EcE+Jw1u5UkOy81tY1MJqzVVdVxmAp1p3wEjp2bMeXA@mail.gmail.com>
	<CAK_CNybVxs32LscopzD4ynPaB-2bo2R4rBGKtQOtE-kUZbxcgg@mail.gmail.com>
Message-ID: <CAJ9CoW=_73nexoZRvA-c0kohak7CBe6rs0xNX3HJctB3WYz7mw@mail.gmail.com>

I agree that the file shows the _reasons_ but that is not the same as
a _process_.

With the high volume of packages that the CRAN maintainers handle, an
explicit procedure beyond "request it" is needed or should at least be
spelled out. We have pushed everything CRAN-related else towards
automation, verification, and formalization; this shouldn't be
different.

On Wed, Sep 21, 2016 at 12:53 PM, Andrew Redd <amredd at gmail.com> wrote:
> The README states clearly that a package is orphaned under any of three
> conditions:
>
> The Maintainer requests is.
> The maintainer email bounces
> The maintainer is unresponsive to requests regarding the package from CRAN
> maintainers
>
> But I think that it is a good idea to include those conditions in the
> manuals.
>
> On Wed, Sep 21, 2016 at 9:30 AM Max Kuhn <mxkuhn at gmail.com> wrote:
>>
>> The CRAN policy page
>> (https://cran.r-project.org/web/packages/policies.html) implies that
>> there is a formal procedure for orphaning a package but none is
>> mentioned in the Extensions manual
>> (https://cran.r-project.org/doc/manuals/r-devel/R-exts.html).
>>
>> This page (https://cran.r-project.org/src/contrib/Orphaned/README)
>> implies that one would simply resubmit the package to CRAN with the
>> text "ORPHANED" in the `Maintainer` field.
>>
>> Is this the case?
>>
>> If this is not documented somewhere, can it be added to the Extensions
>> manual?
>>
>> Thanks,
>>
>> Max
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel


From wdunlap at tibco.com  Wed Sep 21 21:11:32 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Wed, 21 Sep 2016 12:11:32 -0700
Subject: [Rd] error handling in strcapture
Message-ID: <CAF8bMcZrTbM7FFPVoQv1=BpzpVY2-tvLGB+68UaQzLROF2-JDw@mail.gmail.com>

Michael, thanks for looking at my first issue with utils::strcapture.

Another issue is how it deals with lines that don't match the pattern.
Currently it gives an error

> strcapture("(.+) (.+)", c("One 1", "noSpaceInLine", "Three 3"),
proto=list(Name="", Number=0))
Error in strcapture("(.+) (.+)", c("One 1", "noSpaceInLine", "Three 3"),  :
  number of matches does not always match ncol(proto)

First, isn't the 'number of matches' the number of parenthesized
subpatterns in the regular expression?  I thought that if the entire
pattern matches then the subpatterns without matches would be
shown as matches at position 0 with length 0.  Hence either the
pattern is compatible with the prototype or it isn't, it does not depend
on the text input.  E.g.,

> regexec("^(([[:alpha:]]+)|([[:digit:]]+))$", c("Twelve", "12", "Z280"))
[[1]]
[1] 1 1 1 0
attr(,"match.length")
[1] 6 6 6 0
attr(,"useBytes")
[1] TRUE

[[2]]
[1] 1 1 0 1
attr(,"match.length")
[1] 2 2 0 2
attr(,"useBytes")
[1] TRUE

[[3]]
[1] -1
attr(,"match.length")
[1] -1
attr(,"useBytes")
[1] TRUE

Second, an error message like 'some lines were bad' is not very helpful.
Should it put NA's in all the columns of the current output row if the
input line didn't match the pattern and perhaps warn the user that there
were problems?  The user could then look for rows of NA's to see where the
problems were.

Bill Dunlap
TIBCO Software
wdunlap tibco.com

	[[alternative HTML version deleted]]


From paul at stat.auckland.ac.nz  Wed Sep 21 21:59:07 2016
From: paul at stat.auckland.ac.nz (Paul Murrell)
Date: Thu, 22 Sep 2016 07:59:07 +1200
Subject: [Rd] Handlers in setGraphicsEventHandlers() can recursively
 call getGraphicsEvent(). Intended behavior?
In-Reply-To: <005395b9-2749-d7f0-7e8d-664d9aa7aa1f@home.nl>
References: <4c06aff8-8215-c5a1-e367-e1653ee58206@home.nl>
	<b08014d6-1833-ce3d-c16d-3eb1ffc8c8cf@stat.auckland.ac.nz>
	<005395b9-2749-d7f0-7e8d-664d9aa7aa1f@home.nl>
Message-ID: <8b67f33c-633c-9f83-5496-010ebfc17a78@stat.auckland.ac.nz>

Hi

Thanks for the further analysis.  Sounds reasonable to me.

What would be good to get from you is:

- a test that does recursive getGraphicsEvent() calls, that is fixed by 
your patch

- a test that produces the newline prompt from getGraphicsEvent(), that 
is fixed by your other patch

- a test that loses event handling with setTimeLimit(), that is fixed by 
your other other patch.

I have a partial list of packages that are known to make use of 
getGraphicsEvent(), so we can hopefully involve those package authors in 
testing for backward compatibility.

Paul

On 21/09/16 23:23, Richard Bodewits wrote:
> doKeybd() gets called in CHelpKeyIn() and NHelpKeyIn() in
> library/grDevices/src/devWindows.c, where the call is encapsulated in an
> 'if (dd->gettingEvent)' block. So the only times this code ever calls
> doKeybd() is when gettingEvent is in fact set.
>
> Further, it's called in two locations in X11_eventHelper(), in
> modules/X11/devX11.c, where the state of gettingEvent seems to be moot
> for its handling.
>
> doMouseEvent() in turn is called in HelpMouseClick(), HelpMouseMove(),
> and HelpMouseUp() in devWindows.c, where again each call is only made
> after checking if gettingEvent is true.
>
> In devX11.c it's called once in X11_eventHelper(), after checking if
> gettingEvent is true.
>
> Other than these calls, gettingEvent does not get checked anywhere
> outside of main/gevents.c, and nothing called in doKeybd() or
> doMouseEvent() depends on its state being toggled either which way.
>
> As far as I've been able to ascertain these lines are completely
> superfluous, as well as introducing a recursion issue that they seem to
> have been meant to somehow prevent.
>
> As for tests, I can look into cooking something up, though I'm going to
> be spread a little thin for the next three months.
>
> After 12 years of this code being in place I doubt I'll be able to cover
> every imaginable usecase scenario by myself though.
>
> But thanks for replying. Was starting to think I was screaming into the
> void. ;-)
>
>
> - Richard Bodewits
>
>
> On 09/21/2016 03:42 AM, Paul Murrell wrote:
>> Hi
>>
>> Is the correct patch to remove the setting of the gettingEvent flag or
>> would it be better to flip the TRUE/FALSE setting (set to TRUE before
>> handling then reset to FALSE after handling) ?
>>
>> Also, for this patch and for the other two you sent, one difficulty will
>> be with testing the patches.  I have no testing code for this, so would
>> need at least a test or two from you (ideally someone would also have
>> some regression tests, beyond ?getGraphicsEvent, to ensure continuity of
>> previous behaviour).
>>
>> Paul
>>
>> On 18/09/2016 3:29 a.m., Richard Bodewits wrote:
>>> Hey all.
>>>
>>> As in general it's a bad idea to allow an event handler to generate an
>>> event, and as comments in the code seem to suggest this isn't the
>>> intention either, I was wondering about recursion in getGraphicsEvent().
>>> In main/gevents.c, both doMouseEvent() and doKeybd() have the following
>>> line;
>>>
>>>     dd->gettingEvent = FALSE; /* avoid recursive calls */
>>>
>>> And they reset it to TRUE before returning. The effective result of this
>>> is that the event handlers on the R side are allowed to call
>>> getGraphicsEvent(), and recurse until they eventually would run out of
>>> stack space. Though R does catch and handle that cleanly with the error;
>>>
>>>     Error: evaluation nested too deeply: infinite recursion /
>>> options(expressions=)?
>>>
>>> A quick scan of the SVN logs suggests this code has been untouched since
>>> its introduction in 2004, so I'm left to wonder if this is intended
>>> behavior. It stands out as a bit of a sore thumb due to the generic
>>> check for recursion in do_getGraphicsEvent() in the same file, which
>>> will error out with error(_("recursive use of 'getGraphicsEvent' not
>>> supported")) if dd->gettingEvent is already set to TRUE. Which would
>>> suggest recursively calling it is very much not intended to be possible.
>>>
>>> To me, setting gettingEvent to FALSE seems like an easy mistake to make
>>> if you temporarily interpret gettingEvent to mean that event(s) are
>>> allowed to still come in. But the actual interpretation in
>>> do_getGraphicsEvents() is the opposite, as it's interpreted as an
>>> indicator of whether or not an event is currently being processed.
>>>
>>> I've removed the gettingEvent altering lines from both doMouseEvent()
>>> and doKeybd() to no ill effect, and doing so disabled the ability to
>>> call getGraphicsEvent() from inside one of the assigned handlers as
>>> expected. But after 12 (!) years, it's conceivable that people have come
>>> to depend on this behavior in existing scripts. Is this something that
>>> should be left alone to minimize disruption? Or should this be fixed (if
>>> it is indeed unintended) for the sake of protecting people from infinite
>>> recursions?
>>>
>>> I've included a small patch as attachment that removes the offending
>>> lines.
>>>
>>>
>>> - Richard Bodewits
>>>
>>>
>>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>
>>
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/


From lawrence.michael at gene.com  Wed Sep 21 23:10:41 2016
From: lawrence.michael at gene.com (Michael Lawrence)
Date: Wed, 21 Sep 2016 14:10:41 -0700
Subject: [Rd] error handling in strcapture
In-Reply-To: <CAF8bMcZrTbM7FFPVoQv1=BpzpVY2-tvLGB+68UaQzLROF2-JDw@mail.gmail.com>
References: <CAF8bMcZrTbM7FFPVoQv1=BpzpVY2-tvLGB+68UaQzLROF2-JDw@mail.gmail.com>
Message-ID: <CAOQ5NydFbWHyAYasxUvhNjAu3hiWAwtvMKv35sd2TsNbihLqqw@mail.gmail.com>

Hi Bill,

Thanks, another good suggestion. strcapture() now returns NAs for
non-matches. It's nice to have someone kicking the tires on that
function.

Michael

On Wed, Sep 21, 2016 at 12:11 PM, William Dunlap via R-devel
<r-devel at r-project.org> wrote:
> Michael, thanks for looking at my first issue with utils::strcapture.
>
> Another issue is how it deals with lines that don't match the pattern.
> Currently it gives an error
>
>> strcapture("(.+) (.+)", c("One 1", "noSpaceInLine", "Three 3"),
> proto=list(Name="", Number=0))
> Error in strcapture("(.+) (.+)", c("One 1", "noSpaceInLine", "Three 3"),  :
>   number of matches does not always match ncol(proto)
>
> First, isn't the 'number of matches' the number of parenthesized
> subpatterns in the regular expression?  I thought that if the entire
> pattern matches then the subpatterns without matches would be
> shown as matches at position 0 with length 0.  Hence either the
> pattern is compatible with the prototype or it isn't, it does not depend
> on the text input.  E.g.,
>
>> regexec("^(([[:alpha:]]+)|([[:digit:]]+))$", c("Twelve", "12", "Z280"))
> [[1]]
> [1] 1 1 1 0
> attr(,"match.length")
> [1] 6 6 6 0
> attr(,"useBytes")
> [1] TRUE
>
> [[2]]
> [1] 1 1 0 1
> attr(,"match.length")
> [1] 2 2 0 2
> attr(,"useBytes")
> [1] TRUE
>
> [[3]]
> [1] -1
> attr(,"match.length")
> [1] -1
> attr(,"useBytes")
> [1] TRUE
>
> Second, an error message like 'some lines were bad' is not very helpful.
> Should it put NA's in all the columns of the current output row if the
> input line didn't match the pattern and perhaps warn the user that there
> were problems?  The user could then look for rows of NA's to see where the
> problems were.
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From wdunlap at tibco.com  Wed Sep 21 23:21:21 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Wed, 21 Sep 2016 14:21:21 -0700
Subject: [Rd] error handling in strcapture
In-Reply-To: <CAOQ5NydFbWHyAYasxUvhNjAu3hiWAwtvMKv35sd2TsNbihLqqw@mail.gmail.com>
References: <CAF8bMcZrTbM7FFPVoQv1=BpzpVY2-tvLGB+68UaQzLROF2-JDw@mail.gmail.com>
	<CAOQ5NydFbWHyAYasxUvhNjAu3hiWAwtvMKv35sd2TsNbihLqqw@mail.gmail.com>
Message-ID: <CAF8bMca95a+ByPbD9YKgnZm26ZFQO8ju1irj9KVkkXqaP5B74Q@mail.gmail.com>

If there are any matches then strcapture can see if the pattern has the
same number of capture expressions as the prototype has columns and give an
error if not.  That seems appropriate.

If there are no matches, then there is no easy way to see if the prototype
is compatible with the pattern, so should strcapture just assume the best
and fill in the prototype with NA's?

Should there be warnings?  This is kind of like strptime(), which silently
gives NA's when the format does not match the text input.


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Wed, Sep 21, 2016 at 2:10 PM, Michael Lawrence <lawrence.michael at gene.com
> wrote:

> Hi Bill,
>
> Thanks, another good suggestion. strcapture() now returns NAs for
> non-matches. It's nice to have someone kicking the tires on that
> function.
>
> Michael
>
> On Wed, Sep 21, 2016 at 12:11 PM, William Dunlap via R-devel
> <r-devel at r-project.org> wrote:
> > Michael, thanks for looking at my first issue with utils::strcapture.
> >
> > Another issue is how it deals with lines that don't match the pattern.
> > Currently it gives an error
> >
> >> strcapture("(.+) (.+)", c("One 1", "noSpaceInLine", "Three 3"),
> > proto=list(Name="", Number=0))
> > Error in strcapture("(.+) (.+)", c("One 1", "noSpaceInLine", "Three
> 3"),  :
> >   number of matches does not always match ncol(proto)
> >
> > First, isn't the 'number of matches' the number of parenthesized
> > subpatterns in the regular expression?  I thought that if the entire
> > pattern matches then the subpatterns without matches would be
> > shown as matches at position 0 with length 0.  Hence either the
> > pattern is compatible with the prototype or it isn't, it does not depend
> > on the text input.  E.g.,
> >
> >> regexec("^(([[:alpha:]]+)|([[:digit:]]+))$", c("Twelve", "12", "Z280"))
> > [[1]]
> > [1] 1 1 1 0
> > attr(,"match.length")
> > [1] 6 6 6 0
> > attr(,"useBytes")
> > [1] TRUE
> >
> > [[2]]
> > [1] 1 1 0 1
> > attr(,"match.length")
> > [1] 2 2 0 2
> > attr(,"useBytes")
> > [1] TRUE
> >
> > [[3]]
> > [1] -1
> > attr(,"match.length")
> > [1] -1
> > attr(,"useBytes")
> > [1] TRUE
> >
> > Second, an error message like 'some lines were bad' is not very helpful.
> > Should it put NA's in all the columns of the current output row if the
> > input line didn't match the pattern and perhaps warn the user that there
> > were problems?  The user could then look for rows of NA's to see where
> the
> > problems were.
> >
> > Bill Dunlap
> > TIBCO Software
> > wdunlap tibco.com
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From lawrence.michael at gene.com  Wed Sep 21 23:32:45 2016
From: lawrence.michael at gene.com (Michael Lawrence)
Date: Wed, 21 Sep 2016 14:32:45 -0700
Subject: [Rd] error handling in strcapture
In-Reply-To: <CAF8bMca95a+ByPbD9YKgnZm26ZFQO8ju1irj9KVkkXqaP5B74Q@mail.gmail.com>
References: <CAF8bMcZrTbM7FFPVoQv1=BpzpVY2-tvLGB+68UaQzLROF2-JDw@mail.gmail.com>
	<CAOQ5NydFbWHyAYasxUvhNjAu3hiWAwtvMKv35sd2TsNbihLqqw@mail.gmail.com>
	<CAF8bMca95a+ByPbD9YKgnZm26ZFQO8ju1irj9KVkkXqaP5B74Q@mail.gmail.com>
Message-ID: <CAOQ5Nyd3MfPh-Bvmi9yNyxsrMWzJAbBs_zRrKni8pdFB-fkw7w@mail.gmail.com>

The new behavior is that it yields NAs when the pattern does not match
(like strptime) and for empty captures in a matching pattern it yields
the empty string, which is consistent with regmatches().

Michael

On Wed, Sep 21, 2016 at 2:21 PM, William Dunlap <wdunlap at tibco.com> wrote:
> If there are any matches then strcapture can see if the pattern has the same
> number of capture expressions as the prototype has columns and give an
> error if not.  That seems appropriate.
>
> If there are no matches, then there is no easy way to see if the prototype
> is compatible with the pattern, so should strcapture just assume the best
> and fill in the prototype with NA's?
>
> Should there be warnings?  This is kind of like strptime(), which silently
> gives NA's when the format does not match the text input.
>
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
> On Wed, Sep 21, 2016 at 2:10 PM, Michael Lawrence
> <lawrence.michael at gene.com> wrote:
>>
>> Hi Bill,
>>
>> Thanks, another good suggestion. strcapture() now returns NAs for
>> non-matches. It's nice to have someone kicking the tires on that
>> function.
>>
>> Michael
>>
>> On Wed, Sep 21, 2016 at 12:11 PM, William Dunlap via R-devel
>> <r-devel at r-project.org> wrote:
>> > Michael, thanks for looking at my first issue with utils::strcapture.
>> >
>> > Another issue is how it deals with lines that don't match the pattern.
>> > Currently it gives an error
>> >
>> >> strcapture("(.+) (.+)", c("One 1", "noSpaceInLine", "Three 3"),
>> > proto=list(Name="", Number=0))
>> > Error in strcapture("(.+) (.+)", c("One 1", "noSpaceInLine", "Three 3"),
>> > :
>> >   number of matches does not always match ncol(proto)
>> >
>> > First, isn't the 'number of matches' the number of parenthesized
>> > subpatterns in the regular expression?  I thought that if the entire
>> > pattern matches then the subpatterns without matches would be
>> > shown as matches at position 0 with length 0.  Hence either the
>> > pattern is compatible with the prototype or it isn't, it does not depend
>> > on the text input.  E.g.,
>> >
>> >> regexec("^(([[:alpha:]]+)|([[:digit:]]+))$", c("Twelve", "12", "Z280"))
>> > [[1]]
>> > [1] 1 1 1 0
>> > attr(,"match.length")
>> > [1] 6 6 6 0
>> > attr(,"useBytes")
>> > [1] TRUE
>> >
>> > [[2]]
>> > [1] 1 1 0 1
>> > attr(,"match.length")
>> > [1] 2 2 0 2
>> > attr(,"useBytes")
>> > [1] TRUE
>> >
>> > [[3]]
>> > [1] -1
>> > attr(,"match.length")
>> > [1] -1
>> > attr(,"useBytes")
>> > [1] TRUE
>> >
>> > Second, an error message like 'some lines were bad' is not very helpful.
>> > Should it put NA's in all the columns of the current output row if the
>> > input line didn't match the pattern and perhaps warn the user that there
>> > were problems?  The user could then look for rows of NA's to see where
>> > the
>> > problems were.
>> >
>> > Bill Dunlap
>> > TIBCO Software
>> > wdunlap tibco.com
>> >
>> >         [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > R-devel at r-project.org mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-devel
>
>


From mikko.korpela at helsinki.fi  Thu Sep 22 13:43:12 2016
From: mikko.korpela at helsinki.fi (Mikko Korpela)
Date: Thu, 22 Sep 2016 14:43:12 +0300
Subject: [Rd] Typo in ?image
Message-ID: <5f6487d6-ec89-2d05-ccf4-3fc55df45b52@helsinki.fi>

In ?image (src/library/graphics/man/image.Rd), the text fragment "will 
show though" should probably be "will show through".

-- 
Mikko Korpela
Department of Geosciences and Geography
University of Helsinki


From murdoch.duncan at gmail.com  Thu Sep 22 14:01:52 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Thu, 22 Sep 2016 08:01:52 -0400
Subject: [Rd] Typo in ?image
In-Reply-To: <5f6487d6-ec89-2d05-ccf4-3fc55df45b52@helsinki.fi>
References: <5f6487d6-ec89-2d05-ccf4-3fc55df45b52@helsinki.fi>
Message-ID: <e6208fc7-e86e-35e9-56d1-861543fba618@gmail.com>

On 22/09/2016 7:43 AM, Mikko Korpela wrote:
> In ?image (src/library/graphics/man/image.Rd), the text fragment "will
> show though" should probably be "will show through".
>
Thanks, fixed.


Duncan Murdoch


From otoomet at gmail.com  Thu Sep 22 19:41:48 2016
From: otoomet at gmail.com (Ott Toomet)
Date: Thu, 22 Sep 2016 10:41:48 -0700
Subject: [Rd] as.character.factor and S4 object containing factor
Message-ID: <CAMMJQ0bqi8hYzNoLh-H8SCuuGrXqm9dXq9ozepLsf039TpB0Jw@mail.gmail.com>

Do I mess up something or is this a bug?  If I define an S4 object
that contains "factor", all the tests indicate that it is a factor but
as.character.factor() complains of it being a non-factor...

> setClass("Foo", contains="factor")
> a <- new("Foo", factor(1:3))
> a
Object of class "Foo"
[1] 1 2 3
Levels: 1 2 3
> class(a)
[1] "Foo"
attr(,"package")
[1] ".GlobalEnv"
> inherits(a, "factor")
[1] TRUE
> is(a, "factor")
[1] TRUE
> as.character.factor(a)
Error in as.character.factor(a) : attempting to coerce non-factor
> print(a)
Error in as.character.factor(x) : attempting to coerce non-factor
In addition: Warning message:
In print.factor(a) :
  Setting class(x) to NULL;   result will no longer be an S4 object

This means I cannot use ordinary print/summary methods...

platform       x86_64-pc-linux-gnu
arch           x86_64
os             linux-gnu
system         x86_64, linux-gnu
status
major          3
minor          3.1
year           2016
month          06
day            21
svn rev        70800
language       R
version.string R version 3.3.1 (2016-06-21)
nickname       Bug in Your Hair

Cheers,
Ott


-- 
Ott Toomet

Visiting Researcher
School of Information
Mary Gates Hall, Suite 310
University of Washington
Seattle, WA 98195

	[[alternative HTML version deleted]]


From lawrence.michael at gene.com  Thu Sep 22 20:20:04 2016
From: lawrence.michael at gene.com (Michael Lawrence)
Date: Thu, 22 Sep 2016 11:20:04 -0700
Subject: [Rd] as.character.factor and S4 object containing factor
In-Reply-To: <CAMMJQ0bqi8hYzNoLh-H8SCuuGrXqm9dXq9ozepLsf039TpB0Jw@mail.gmail.com>
References: <CAMMJQ0bqi8hYzNoLh-H8SCuuGrXqm9dXq9ozepLsf039TpB0Jw@mail.gmail.com>
Message-ID: <CAOQ5NyfVxGBjSM8WxDBUVpt9qRB5B0NrnCMSH1UrWzEkOvLdvQ@mail.gmail.com>

The issue with as.character.factor() was reported and fixed recently.

https://bugs.r-project.org/bugzilla/show_bug.cgi?id=17141

The warning emitted from print.factor() is interesting. I'm not sure
why we are setting the class to NULL there. Could just create a new
character vector instead. Will look into it later.

Michael

On Thu, Sep 22, 2016 at 10:41 AM, Ott Toomet <otoomet at gmail.com> wrote:
> Do I mess up something or is this a bug?  If I define an S4 object
> that contains "factor", all the tests indicate that it is a factor but
> as.character.factor() complains of it being a non-factor...
>
>> setClass("Foo", contains="factor")
>> a <- new("Foo", factor(1:3))
>> a
> Object of class "Foo"
> [1] 1 2 3
> Levels: 1 2 3
>> class(a)
> [1] "Foo"
> attr(,"package")
> [1] ".GlobalEnv"
>> inherits(a, "factor")
> [1] TRUE
>> is(a, "factor")
> [1] TRUE
>> as.character.factor(a)
> Error in as.character.factor(a) : attempting to coerce non-factor
>> print(a)
> Error in as.character.factor(x) : attempting to coerce non-factor
> In addition: Warning message:
> In print.factor(a) :
>   Setting class(x) to NULL;   result will no longer be an S4 object
>
> This means I cannot use ordinary print/summary methods...
>
> platform       x86_64-pc-linux-gnu
> arch           x86_64
> os             linux-gnu
> system         x86_64, linux-gnu
> status
> major          3
> minor          3.1
> year           2016
> month          06
> day            21
> svn rev        70800
> language       R
> version.string R version 3.3.1 (2016-06-21)
> nickname       Bug in Your Hair
>
> Cheers,
> Ott
>
>
> --
> Ott Toomet
>
> Visiting Researcher
> School of Information
> Mary Gates Hall, Suite 310
> University of Washington
> Seattle, WA 98195
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From suharto_anggono at yahoo.com  Fri Sep 23 18:37:44 2016
From: suharto_anggono at yahoo.com (Suharto Anggono Suharto Anggono)
Date: Fri, 23 Sep 2016 16:37:44 +0000 (UTC)
Subject: [Rd] Undocumented 'use.names' argument to c()
References: <380713756.3661510.1474648664366.ref@mail.yahoo.com>
Message-ID: <380713756.3661510.1474648664366@mail.yahoo.com>

In S-PLUS 3.4 help on 'c' (http://www.uni-muenster.de/ZIV.BennoSueselbeck/s-html/helpfiles/c.html), there is no 'use.names' argument.

Because 'c' is a generic function, I don't think that changing formal arguments is good.

In R devel r71344, 'use.names' is not an argument of functions 'c.Date', 'c.POSIXct' and 'c.difftime'.

Could 'use.names' be documented to be accepted by the default method of 'c', but not listed as a formal argument of 'c'? Or, could the code that handles the argument name 'use.names' be removed?
----------------
>>>>> David Winsemius <dwinsemius at comcast.net>
>>>>>     on Tue, 20 Sep 2016 23:46:48 -0700 writes:

    >> On Sep 20, 2016, at 7:18 PM, Karl Millar via R-devel <r-devel at r-project.org> wrote:
    >> 
    >> 'c' has an undocumented 'use.names' argument.  I'm not sure if this is
    >> a documentation or implementation bug.

    > It came up on stackoverflow a couple of years ago:

    > http://stackoverflow.com/questions/24815572/why-does-function-c-accept-an-undocumented-argument/24815653#24815653

    > At the time it appeared to me to be a documentation lag.

Thank you, Karl and David,
yes it is a documentation glitch ... and a bit more:  Experts know that
print()ing of primitive functions is, eehm, "special".

I've committed a change to R-devel ... (with the intent to port
to R-patched).

Martin

    >> 
    >>> c(a = 1)
    >> a
    >> 1
    >>> c(a = 1, use.names = F)
    >> [1] 1
    >> 
    >> Karl


From wdunlap at tibco.com  Fri Sep 23 19:25:04 2016
From: wdunlap at tibco.com (William Dunlap)
Date: Fri, 23 Sep 2016 10:25:04 -0700
Subject: [Rd] Undocumented 'use.names' argument to c()
In-Reply-To: <380713756.3661510.1474648664366@mail.yahoo.com>
References: <380713756.3661510.1474648664366.ref@mail.yahoo.com>
	<380713756.3661510.1474648664366@mail.yahoo.com>
Message-ID: <CAF8bMcYsxdgoWQCLRiTA83P4sF=Qseghz5+Ug4zVsH4B1hmNqw@mail.gmail.com>

In Splus c() and unlist() called the same C code, but with a different
'sys_index'  code (the last argument to .Internal) and c() did not consider
an argument named 'use.names' special.

> c
function(..., recursive = F)
.Internal(c(..., recursive = recursive), "S_unlist", TRUE, 1)
> unlist
function(data, recursive = T, use.names = T)
.Internal(unlist(data, recursive = recursive, use.names = use.names),
"S_unlist", TRUE, 2)
> c(A=1,B=2,use.names=FALSE)
 A B use.names
 1 2         0

The C code used sys_index==2 to mean 'the last  argument is the 'use.names'
argument, if sys_index==1 only the recursive argument was considered
special.

Sys.funs.c:
 405 S_unlist(vector *ent, vector *arglist, s_evaluator *S_evaluator)
 406 {
 407         int which = sys_index; boolean named, recursive, names;
 ...
 419         args = arglist->value.tree; n = arglist->length;
 ...
 424         names = which==2 ? logical_value(args[--n], ent, S_evaluator)
: (which == 1);

Thus there is no historical reason for giving c() the use.names argument.


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Fri, Sep 23, 2016 at 9:37 AM, Suharto Anggono Suharto Anggono via
R-devel <r-devel at r-project.org> wrote:

> In S-PLUS 3.4 help on 'c' (http://www.uni-muenster.de/
> ZIV.BennoSueselbeck/s-html/helpfiles/c.html), there is no 'use.names'
> argument.
>
> Because 'c' is a generic function, I don't think that changing formal
> arguments is good.
>
> In R devel r71344, 'use.names' is not an argument of functions 'c.Date',
> 'c.POSIXct' and 'c.difftime'.
>
> Could 'use.names' be documented to be accepted by the default method of
> 'c', but not listed as a formal argument of 'c'? Or, could the code that
> handles the argument name 'use.names' be removed?
> ----------------
> >>>>> David Winsemius <dwinsemius at comcast.net>
> >>>>>     on Tue, 20 Sep 2016 23:46:48 -0700 writes:
>
>     >> On Sep 20, 2016, at 7:18 PM, Karl Millar via R-devel <r-devel at
> r-project.org> wrote:
>     >>
>     >> 'c' has an undocumented 'use.names' argument.  I'm not sure if this
> is
>     >> a documentation or implementation bug.
>
>     > It came up on stackoverflow a couple of years ago:
>
>     > http://stackoverflow.com/questions/24815572/why-does-
> function-c-accept-an-undocumented-argument/24815653#24815653
>
>     > At the time it appeared to me to be a documentation lag.
>
> Thank you, Karl and David,
> yes it is a documentation glitch ... and a bit more:  Experts know that
> print()ing of primitive functions is, eehm, "special".
>
> I've committed a change to R-devel ... (with the intent to port
> to R-patched).
>
> Martin
>
>     >>
>     >>> c(a = 1)
>     >> a
>     >> 1
>     >>> c(a = 1, use.names = F)
>     >> [1] 1
>     >>
>     >> Karl
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From henrik.bengtsson at gmail.com  Fri Sep 23 19:54:16 2016
From: henrik.bengtsson at gmail.com (Henrik Bengtsson)
Date: Fri, 23 Sep 2016 10:54:16 -0700
Subject: [Rd] Undocumented 'use.names' argument to c()
In-Reply-To: <CAF8bMcYsxdgoWQCLRiTA83P4sF=Qseghz5+Ug4zVsH4B1hmNqw@mail.gmail.com>
References: <380713756.3661510.1474648664366.ref@mail.yahoo.com>
	<380713756.3661510.1474648664366@mail.yahoo.com>
	<CAF8bMcYsxdgoWQCLRiTA83P4sF=Qseghz5+Ug4zVsH4B1hmNqw@mail.gmail.com>
Message-ID: <CAFDcVCS0CDb0Rj5=XEu_toHcEFywLy4NDhasOtdBvc385y4n0w@mail.gmail.com>

I'd vote for it to stay.  It could of course suprise someone who'd
expect c(list(a=1), b=2, use.names = FALSE) to generate list(a=1, b=2,
use.names=FALSE).   On the upside, is the performance gain from using
use.names=FALSE.  Below benchmarks show that the combining of the
names attributes themselves takes ~20-25 times longer than the
combining of the integers themselves.  Also, at no surprise,
use.names=FALSE avoids some memory allocations.

> options(digits = 2)
>
> a <- b <- c <- d <- 1:1e4
> names(c) <- c
> names(d) <- d
>
> stats <- microbenchmark::microbenchmark(
+   c(a, b, use.names=FALSE),
+   c(c, d, use.names=FALSE),
+   c(a, d, use.names=FALSE),
+   c(a, b, use.names=TRUE),
+   c(a, d, use.names=TRUE),
+   c(c, d, use.names=TRUE),
+   unit = "ms"
+ )
>
> stats
Unit: milliseconds
                       expr   min    lq  mean median    uq   max neval
 c(a, b, use.names = FALSE) 0.031 0.032 0.049  0.034 0.036 1.474   100
 c(c, d, use.names = FALSE) 0.031 0.031 0.035  0.034 0.035 0.064   100
 c(a, d, use.names = FALSE) 0.031 0.031 0.049  0.034 0.035 1.452   100
  c(a, b, use.names = TRUE) 0.031 0.031 0.055  0.034 0.036 2.094   100
  c(a, d, use.names = TRUE) 0.510 0.526 0.588  0.549 0.617 1.998   100
  c(c, d, use.names = TRUE) 0.780 0.815 0.886  0.841 0.944 1.430   100

> profmem::profmem(c(c, d, use.names=FALSE))
Rprofmem memory profiling of:
c(c, d, use.names = FALSE)

Memory allocations:
      bytes      calls
1     80040 <internal>
total 80040

> profmem::profmem(c(c, d, use.names=TRUE))
Rprofmem memory profiling of:
c(c, d, use.names = TRUE)

Memory allocations:
       bytes      calls
1      80040 <internal>
2     160040 <internal>
total 240080

/Henrik

On Fri, Sep 23, 2016 at 10:25 AM, William Dunlap via R-devel
<r-devel at r-project.org> wrote:
> In Splus c() and unlist() called the same C code, but with a different
> 'sys_index'  code (the last argument to .Internal) and c() did not consider
> an argument named 'use.names' special.
>
>> c
> function(..., recursive = F)
> .Internal(c(..., recursive = recursive), "S_unlist", TRUE, 1)
>> unlist
> function(data, recursive = T, use.names = T)
> .Internal(unlist(data, recursive = recursive, use.names = use.names),
> "S_unlist", TRUE, 2)
>> c(A=1,B=2,use.names=FALSE)
>  A B use.names
>  1 2         0
>
> The C code used sys_index==2 to mean 'the last  argument is the 'use.names'
> argument, if sys_index==1 only the recursive argument was considered
> special.
>
> Sys.funs.c:
>  405 S_unlist(vector *ent, vector *arglist, s_evaluator *S_evaluator)
>  406 {
>  407         int which = sys_index; boolean named, recursive, names;
>  ...
>  419         args = arglist->value.tree; n = arglist->length;
>  ...
>  424         names = which==2 ? logical_value(args[--n], ent, S_evaluator)
> : (which == 1);
>
> Thus there is no historical reason for giving c() the use.names argument.
>
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
> On Fri, Sep 23, 2016 at 9:37 AM, Suharto Anggono Suharto Anggono via
> R-devel <r-devel at r-project.org> wrote:
>
>> In S-PLUS 3.4 help on 'c' (http://www.uni-muenster.de/
>> ZIV.BennoSueselbeck/s-html/helpfiles/c.html), there is no 'use.names'
>> argument.
>>
>> Because 'c' is a generic function, I don't think that changing formal
>> arguments is good.
>>
>> In R devel r71344, 'use.names' is not an argument of functions 'c.Date',
>> 'c.POSIXct' and 'c.difftime'.
>>
>> Could 'use.names' be documented to be accepted by the default method of
>> 'c', but not listed as a formal argument of 'c'? Or, could the code that
>> handles the argument name 'use.names' be removed?
>> ----------------
>> >>>>> David Winsemius <dwinsemius at comcast.net>
>> >>>>>     on Tue, 20 Sep 2016 23:46:48 -0700 writes:
>>
>>     >> On Sep 20, 2016, at 7:18 PM, Karl Millar via R-devel <r-devel at
>> r-project.org> wrote:
>>     >>
>>     >> 'c' has an undocumented 'use.names' argument.  I'm not sure if this
>> is
>>     >> a documentation or implementation bug.
>>
>>     > It came up on stackoverflow a couple of years ago:
>>
>>     > http://stackoverflow.com/questions/24815572/why-does-
>> function-c-accept-an-undocumented-argument/24815653#24815653
>>
>>     > At the time it appeared to me to be a documentation lag.
>>
>> Thank you, Karl and David,
>> yes it is a documentation glitch ... and a bit more:  Experts know that
>> print()ing of primitive functions is, eehm, "special".
>>
>> I've committed a change to R-devel ... (with the intent to port
>> to R-patched).
>>
>> Martin
>>
>>     >>
>>     >>> c(a = 1)
>>     >> a
>>     >> 1
>>     >>> c(a = 1, use.names = F)
>>     >> [1] 1
>>     >>
>>     >> Karl
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From kmillar at google.com  Fri Sep 23 20:12:49 2016
From: kmillar at google.com (Karl Millar)
Date: Fri, 23 Sep 2016 11:12:49 -0700
Subject: [Rd] Undocumented 'use.names' argument to c()
In-Reply-To: <CAFDcVCS0CDb0Rj5=XEu_toHcEFywLy4NDhasOtdBvc385y4n0w@mail.gmail.com>
References: <380713756.3661510.1474648664366.ref@mail.yahoo.com>
	<380713756.3661510.1474648664366@mail.yahoo.com>
	<CAF8bMcYsxdgoWQCLRiTA83P4sF=Qseghz5+Ug4zVsH4B1hmNqw@mail.gmail.com>
	<CAFDcVCS0CDb0Rj5=XEu_toHcEFywLy4NDhasOtdBvc385y4n0w@mail.gmail.com>
Message-ID: <CABz6aZeaFsiFpdOGsSWTtfYHRHFk8MUa3Mg85EBGPtdX4OrBrQ@mail.gmail.com>

I'd expect that a lot of the performance overhead could be eliminated
by simply improving the underlying code.  IMHO, we should ignore it in
deciding the API that we want here.

On Fri, Sep 23, 2016 at 10:54 AM, Henrik Bengtsson
<henrik.bengtsson at gmail.com> wrote:
> I'd vote for it to stay.  It could of course suprise someone who'd
> expect c(list(a=1), b=2, use.names = FALSE) to generate list(a=1, b=2,
> use.names=FALSE).   On the upside, is the performance gain from using
> use.names=FALSE.  Below benchmarks show that the combining of the
> names attributes themselves takes ~20-25 times longer than the
> combining of the integers themselves.  Also, at no surprise,
> use.names=FALSE avoids some memory allocations.
>
>> options(digits = 2)
>>
>> a <- b <- c <- d <- 1:1e4
>> names(c) <- c
>> names(d) <- d
>>
>> stats <- microbenchmark::microbenchmark(
> +   c(a, b, use.names=FALSE),
> +   c(c, d, use.names=FALSE),
> +   c(a, d, use.names=FALSE),
> +   c(a, b, use.names=TRUE),
> +   c(a, d, use.names=TRUE),
> +   c(c, d, use.names=TRUE),
> +   unit = "ms"
> + )
>>
>> stats
> Unit: milliseconds
>                        expr   min    lq  mean median    uq   max neval
>  c(a, b, use.names = FALSE) 0.031 0.032 0.049  0.034 0.036 1.474   100
>  c(c, d, use.names = FALSE) 0.031 0.031 0.035  0.034 0.035 0.064   100
>  c(a, d, use.names = FALSE) 0.031 0.031 0.049  0.034 0.035 1.452   100
>   c(a, b, use.names = TRUE) 0.031 0.031 0.055  0.034 0.036 2.094   100
>   c(a, d, use.names = TRUE) 0.510 0.526 0.588  0.549 0.617 1.998   100
>   c(c, d, use.names = TRUE) 0.780 0.815 0.886  0.841 0.944 1.430   100
>
>> profmem::profmem(c(c, d, use.names=FALSE))
> Rprofmem memory profiling of:
> c(c, d, use.names = FALSE)
>
> Memory allocations:
>       bytes      calls
> 1     80040 <internal>
> total 80040
>
>> profmem::profmem(c(c, d, use.names=TRUE))
> Rprofmem memory profiling of:
> c(c, d, use.names = TRUE)
>
> Memory allocations:
>        bytes      calls
> 1      80040 <internal>
> 2     160040 <internal>
> total 240080
>
> /Henrik
>
> On Fri, Sep 23, 2016 at 10:25 AM, William Dunlap via R-devel
> <r-devel at r-project.org> wrote:
>> In Splus c() and unlist() called the same C code, but with a different
>> 'sys_index'  code (the last argument to .Internal) and c() did not consider
>> an argument named 'use.names' special.
>>
>>> c
>> function(..., recursive = F)
>> .Internal(c(..., recursive = recursive), "S_unlist", TRUE, 1)
>>> unlist
>> function(data, recursive = T, use.names = T)
>> .Internal(unlist(data, recursive = recursive, use.names = use.names),
>> "S_unlist", TRUE, 2)
>>> c(A=1,B=2,use.names=FALSE)
>>  A B use.names
>>  1 2         0
>>
>> The C code used sys_index==2 to mean 'the last  argument is the 'use.names'
>> argument, if sys_index==1 only the recursive argument was considered
>> special.
>>
>> Sys.funs.c:
>>  405 S_unlist(vector *ent, vector *arglist, s_evaluator *S_evaluator)
>>  406 {
>>  407         int which = sys_index; boolean named, recursive, names;
>>  ...
>>  419         args = arglist->value.tree; n = arglist->length;
>>  ...
>>  424         names = which==2 ? logical_value(args[--n], ent, S_evaluator)
>> : (which == 1);
>>
>> Thus there is no historical reason for giving c() the use.names argument.
>>
>>
>> Bill Dunlap
>> TIBCO Software
>> wdunlap tibco.com
>>
>> On Fri, Sep 23, 2016 at 9:37 AM, Suharto Anggono Suharto Anggono via
>> R-devel <r-devel at r-project.org> wrote:
>>
>>> In S-PLUS 3.4 help on 'c' (http://www.uni-muenster.de/
>>> ZIV.BennoSueselbeck/s-html/helpfiles/c.html), there is no 'use.names'
>>> argument.
>>>
>>> Because 'c' is a generic function, I don't think that changing formal
>>> arguments is good.
>>>
>>> In R devel r71344, 'use.names' is not an argument of functions 'c.Date',
>>> 'c.POSIXct' and 'c.difftime'.
>>>
>>> Could 'use.names' be documented to be accepted by the default method of
>>> 'c', but not listed as a formal argument of 'c'? Or, could the code that
>>> handles the argument name 'use.names' be removed?
>>> ----------------
>>> >>>>> David Winsemius <dwinsemius at comcast.net>
>>> >>>>>     on Tue, 20 Sep 2016 23:46:48 -0700 writes:
>>>
>>>     >> On Sep 20, 2016, at 7:18 PM, Karl Millar via R-devel <r-devel at
>>> r-project.org> wrote:
>>>     >>
>>>     >> 'c' has an undocumented 'use.names' argument.  I'm not sure if this
>>> is
>>>     >> a documentation or implementation bug.
>>>
>>>     > It came up on stackoverflow a couple of years ago:
>>>
>>>     > http://stackoverflow.com/questions/24815572/why-does-
>>> function-c-accept-an-undocumented-argument/24815653#24815653
>>>
>>>     > At the time it appeared to me to be a documentation lag.
>>>
>>> Thank you, Karl and David,
>>> yes it is a documentation glitch ... and a bit more:  Experts know that
>>> print()ing of primitive functions is, eehm, "special".
>>>
>>> I've committed a change to R-devel ... (with the intent to port
>>> to R-patched).
>>>
>>> Martin
>>>
>>>     >>
>>>     >>> c(a = 1)
>>>     >> a
>>>     >> 1
>>>     >>> c(a = 1, use.names = F)
>>>     >> [1] 1
>>>     >>
>>>     >> Karl
>>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From maechler at stat.math.ethz.ch  Sat Sep 24 15:37:59 2016
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 24 Sep 2016 15:37:59 +0200
Subject: [Rd] withAutoprint({ .... }) ?
In-Reply-To: <CAF8bMcaOVsUsdSUN0GE+1COsb4hYBRaD7qdFcadY--PXB_bJaQ@mail.gmail.com>
References: <ce4bc223-12d1-2618-aa43-cd12c11396eb@gmail.com>
	<CAPPM_gSzW37A9xQn1qrTEqd6jWx-mE-cOq2i7zEqU08THqJn7A@mail.gmail.com>
	<20160902115636.4C6BE1A75DF@lynne.stat.math.ethz.ch>
	<CAF8bMcaOVsUsdSUN0GE+1COsb4hYBRaD7qdFcadY--PXB_bJaQ@mail.gmail.com>
Message-ID: <22502.33207.568185.181074@stat.math.ethz.ch>

>>>>> William Dunlap <wdunlap at tibco.com>
>>>>>     on Fri, 2 Sep 2016 08:33:47 -0700 writes:

    > Re withAutoprint(), Splus's source() function could take a expression
    > (literal or not) in place of a file name or text so it could support
    > withAutoprint-like functionality in its GUI.  E.g.,

    >> source(auto.print=TRUE, exprs.literal= { x <- 3:7 ; sum(x) ; y <- log(x)
    > ; x - 100}, prompt="--> ")
    --> x <- 3:7
    --> sum(x)
    > [1] 25
    --> y <- log(x)
    --> x - 100
    > [1] -97 -96 -95 -94 -93

    > or

    >> expr <- quote({ x <- 3:7 ; sum(x) ; y <- log(x) ; x - 100})
    >> source(auto.print=TRUE, exprs = expr, prompt="--> ")
    --> x <- 3:7
    --> sum(x)
    > [1] 25
    --> y <- log(x)
    --> x - 100
    > [1] -97 -96 -95 -94 -93

    > It was easy to implement, since exprs's default value is parse(file) or
    > parse(text=text), which source is calculating anyway.


    > Bill Dunlap
    > TIBCO Software
    > wdunlap tibco.com

Thank you, Bill  (and the other correspondents); that's indeed a
very good suggestion :

I've come to the conclusion that Duncan and Bill are right:  One
should do this in R (not C) and as Bill hinted, one should use
source().  I first tried to do it separately, just "like source()",
but a considerable part of the source of source()  {:-)} is
about using src attributes instead of deparse() when the former
are present,  and it does make sense to generalize
withAutoprint() to have the same feature, so after all, have it
call source().

I've spent a few hours now trying things and variants, also
found I needed to enhance source()  very slightly also in a few
other details, and now (in my uncommitted version of R-devel), 

  withAutoprint({ x <- 1:12; x-1; (y <- (x-5)^2); z <- y; z - 10 })

produces

> withAutoprint({ x <- 1:12; x-1; (y <- (x-5)^2); z <- y; z - 10 })
> x <- 1:12
> x - 1
 [1]  0  1  2  3  4  5  6  7  8  9 10 11
> (y <- (x - 5)^2)
 [1] 16  9  4  1  0  1  4  9 16 25 36 49
> z <- y
> z - 10
 [1]   6  -1  -6  -9 -10  -9  -6  -1   6  15  26  39
> 

and is equivalent to 

   withAutoprint(expression(x <- 1:12, x-1, (y <- (x-5)^2), z <- y, z - 10 ))

I don't see any way around the "mis-feature" that all "input"
expressions are in the end shown twice in the "output" (the
first time by showing the withAutoprint(...) call itself).

The function *name* is "not bad" but also a bit longish;
maybe there are better ideas?  (not longer, no "_" - I know this
      	    	       	        is a matter of taste only)

Martin

    > On Fri, Sep 2, 2016 at 4:56 AM, Martin Maechler <maechler at stat.math.ethz.ch>
    > wrote:

    >> On R-help, with subject
    >> '[R] source() does not include added code'
    >> 
    >> >>>>> Joshua Ulrich <josh.m.ulrich at gmail.com>
    >> >>>>>     on Wed, 31 Aug 2016 10:35:01 -0500 writes:
    >> 
    >> > I have quantstrat installed and it works fine for me.  If you're
    >> > asking why the output of t(tradeStats('macross')) isn't being
    >> printed,
    >> > that's because of what's described in the first paragraph in the
    >> > *Details* section of help("source"):
    >> 
    >> > Note that running code via ?source? differs in a few respects from
    >> > entering it at the R command line.  Since expressions are not
    >> > executed at the top level, auto-printing is not done.  So you will
    >> > need to include explicit ?print? calls for things you want to be
    >> > printed (and remember that this includes plotting by ?lattice?,
    >> > FAQ Q7.22).
    >> 
    >> 
    >> 
    >> > So you need:
    >> 
    >> > print(t(tradeStats('macross')))
    >> 
    >> > if you want the output printed to the console.
    >> 
    >> indeed, and "of course"" ;-)
    >> 
    >> As my subject indicates, this is another case, where it would be
    >> very convenient to have a function
    >> 
    >> withAutoprint()
    >> 
    >> so the OP could have (hopefully) have used
    >> withAutoprint(source(..))
    >> though that would have been equivalent to the already nicely existing
    >> 
    >> source(.., print.eval = TRUE)
    >> 
    >> which works via the  withVisible(.) utility that returns for each
    >> 'expression' if it would auto print or not, and then does print (or
    >> not) accordingly.
    >> 
    >> My own use cases for such a withAutoprint({...})
    >> are demos and examples, sometimes even package tests which I want to print:
    >> 
    >> Assume I have a nice demo / example on a help page/ ...
    >> 
    >> foo(..)
    >> (z <- bar(..))
    >> summary(z)
    >> ....
    >> 
    >> where I carefully do print parts (and don't others),
    >> and suddenly I find I want to run that part of the demo /
    >> example / test only in some circumstances, e.g., only when
    >> interactive, but not in BATCH, or only if it is me, the package maintainer,
    >> 
    >> if( identical(Sys.getenv("USER"), "maechler") ) {
    >> foo(..)
    >> (z <- bar(..))
    >> summary(z)
    >> ....
    >> }
    >> 
    >> Now all the auto-printing is gone, and
    >> 
    >> 1) I have to find out which of these function calls do autoprint and wrap
    >> a print(..) around these, and
    >> 
    >> 2) the result is quite ugly (for an example on a help page etc.)
    >> 
    >> What I would like in a future R, is to be able to simply wrap the "{
    >> .. } above with an 'withAutoprint(.) :
    >> 
    >> if( identical(Sys.getenv("USER"), "maechler") ) withAutoprint({
    >> foo(..)
    >> (z <- bar(..))
    >> summary(z)
    >> ....
    >> })
    >> 
    >> Conceptually such a function could be written similar to source() with an R
    >> level for loop, treating each expression separately, calling eval(.) etc.
    >> That may cost too much performnace, ... still to have it would be better
    >> than
    >> not having the possibility.
    >> 
    >> ----
    >> 
    >> If you read so far, you'd probably agree that such a function
    >> could be a nice asset in R,
    >> notably if it was possible to do this on the fast C level of R's main
    >> REPL.
    >> 
    >> Have any of you looked into how this could be provided in R ?
    >> If you know the source a little, you will remember that there's
    >> the global variable  R_Visible  which is crucial here.
    >> The problem with that is that it *is* global, and only available
    >> as that; that the auto-printing "concept" is so linked to "toplevel
    >> context"
    >> and that is not easy, and AFAIK not so much centralized in one place in the
    >> source. Consequently, all kind of (very) low level functions manipulate
    >> R_Visible
    >> temporarily.... and so a C level implementation of  withAutoprint() may
    >> need considerable more changes than just setting R_Visible to TRUE in one
    >> place.
    >> 
    >> Have any efforts / experiments already happened towards providing such
    >> functionality ?
    >> 
    >> ______________________________________________
    >> R-devel at r-project.org mailing list
    >> https://stat.ethz.ch/mailman/listinfo/r-devel

    > [[alternative HTML version deleted]]


From maechler at stat.math.ethz.ch  Sat Sep 24 16:12:14 2016
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 24 Sep 2016 16:12:14 +0200
Subject: [Rd] Undocumented 'use.names' argument to c()
In-Reply-To: <CABz6aZeaFsiFpdOGsSWTtfYHRHFk8MUa3Mg85EBGPtdX4OrBrQ@mail.gmail.com>
References: <380713756.3661510.1474648664366.ref@mail.yahoo.com>
	<380713756.3661510.1474648664366@mail.yahoo.com>
	<CAF8bMcYsxdgoWQCLRiTA83P4sF=Qseghz5+Ug4zVsH4B1hmNqw@mail.gmail.com>
	<CAFDcVCS0CDb0Rj5=XEu_toHcEFywLy4NDhasOtdBvc385y4n0w@mail.gmail.com>
	<CABz6aZeaFsiFpdOGsSWTtfYHRHFk8MUa3Mg85EBGPtdX4OrBrQ@mail.gmail.com>
Message-ID: <22502.35262.599928.875184@stat.math.ethz.ch>

>>>>> Karl Millar via R-devel <r-devel at r-project.org>
>>>>>     on Fri, 23 Sep 2016 11:12:49 -0700 writes:

    > I'd expect that a lot of the performance overhead could be eliminated
    > by simply improving the underlying code.  IMHO, we should ignore it in
    > deciding the API that we want here.

I agree partially.  Even if the underlying code can be made
faster, the 'use.names = FALSE' version will still be faster
than the default, notably in some "long" cases.

More further down.

    > On Fri, Sep 23, 2016 at 10:54 AM, Henrik Bengtsson
    > <henrik.bengtsson at gmail.com> wrote:
    >> I'd vote for it to stay.  It could of course suprise someone who'd
    >> expect c(list(a=1), b=2, use.names = FALSE) to generate list(a=1, b=2,
    >> use.names=FALSE).   On the upside, is the performance gain from using
    >> use.names=FALSE.  Below benchmarks show that the combining of the
    >> names attributes themselves takes ~20-25 times longer than the
    >> combining of the integers themselves.  Also, at no surprise,
    >> use.names=FALSE avoids some memory allocations.
    >> 
    >>> options(digits = 2)
    >>> 
    >>> a <- b <- c <- d <- 1:1e4
    >>> names(c) <- c
    >>> names(d) <- d
    >>> 
    >>> stats <- microbenchmark::microbenchmark(
    >> +   c(a, b, use.names=FALSE),
    >> +   c(c, d, use.names=FALSE),
    >> +   c(a, d, use.names=FALSE),
    >> +   c(a, b, use.names=TRUE),
    >> +   c(a, d, use.names=TRUE),
    >> +   c(c, d, use.names=TRUE),
    >> +   unit = "ms"
    >> + )
    >>> 
    >>> stats
    >> Unit: milliseconds
    >> expr   min    lq  mean median    uq   max neval
    >> c(a, b, use.names = FALSE) 0.031 0.032 0.049  0.034 0.036 1.474   100
    >> c(c, d, use.names = FALSE) 0.031 0.031 0.035  0.034 0.035 0.064   100
    >> c(a, d, use.names = FALSE) 0.031 0.031 0.049  0.034 0.035 1.452   100
    >> c(a, b, use.names = TRUE) 0.031 0.031 0.055  0.034 0.036 2.094   100
    >> c(a, d, use.names = TRUE) 0.510 0.526 0.588  0.549 0.617 1.998   100
    >> c(c, d, use.names = TRUE) 0.780 0.815 0.886  0.841 0.944 1.430   100
    >> 
    >>> profmem::profmem(c(c, d, use.names=FALSE))
    >> Rprofmem memory profiling of:
    >> c(c, d, use.names = FALSE)
    >> 
    >> Memory allocations:
    >> bytes      calls
    >> 1     80040 <internal>
    >> total 80040
    >> 
    >>> profmem::profmem(c(c, d, use.names=TRUE))
    >> Rprofmem memory profiling of:
    >> c(c, d, use.names = TRUE)
    >> 
    >> Memory allocations:
    >> bytes      calls
    >> 1      80040 <internal>
    >> 2     160040 <internal>
    >> total 240080
    >> 
    >> /Henrik
    >> 
    >> On Fri, Sep 23, 2016 at 10:25 AM, William Dunlap via R-devel
    >> <r-devel at r-project.org> wrote:
    >>> In Splus c() and unlist() called the same C code, but with a different
    >>> 'sys_index'  code (the last argument to .Internal) and c() did not consider
    >>> an argument named 'use.names' special.

Thank you, Bill, very much, for making the historical context
clear, and giving us the facts, there.

OTOH, it is also true in R, that  c() and unlist() share code
.. quite a bit less though .. but more importantly, the very
original C code of Ross Ihaka (and possibly Robert Gentleman)
had explicitly considered both extra arguments 'recursive' and
'use.names', and not just the first.

The fact that c() has always been a .Primitive function and that
these have no formals()  had contributed to what I think to be a
documentation glitch early on, and when, quite a bit later, we've
added a fake argument list for printing, the then current
documentation was used.

This was the reason for declaring it a documentation "hole"
rather than something we do not want.

(read on)

    >>>> c
    >>> function(..., recursive = F)
    >>> .Internal(c(..., recursive = recursive), "S_unlist", TRUE, 1)
    >>>> unlist
    >>> function(data, recursive = T, use.names = T)
    >>> .Internal(unlist(data, recursive = recursive, use.names = use.names),
    >>> "S_unlist", TRUE, 2)
    >>>> c(A=1,B=2,use.names=FALSE)
    >>> A B use.names
    >>> 1 2         0
    >>> 
    >>> The C code used sys_index==2 to mean 'the last  argument is the 'use.names'
    >>> argument, if sys_index==1 only the recursive argument was considered
    >>> special.
    >>> 
    >>> Sys.funs.c:
    >>> 405 S_unlist(vector *ent, vector *arglist, s_evaluator *S_evaluator)
    >>> 406 {
    >>> 407         int which = sys_index; boolean named, recursive, names;
    >>> ...
    >>> 419         args = arglist->value.tree; n = arglist->length;
    >>> ...
    >>> 424         names = which==2 ? logical_value(args[--n], ent, S_evaluator)
    >>> : (which == 1);
    >>> 
    >>> Thus there is no historical reason for giving c() the use.names argument.
    >>> 
    >>> 
    >>> Bill Dunlap
    >>> TIBCO Software
    >>> wdunlap tibco.com
    >>> 
    >>> On Fri, Sep 23, 2016 at 9:37 AM, Suharto Anggono Suharto Anggono via
    >>> R-devel <r-devel at r-project.org> wrote:
    >>> 
    >>>> In S-PLUS 3.4 help on 'c' (http://www.uni-muenster.de/
    >>>> ZIV.BennoSueselbeck/s-html/helpfiles/c.html), there is no 'use.names'
    >>>> argument.
    >>>> 
    >>>> Because 'c' is a generic function, I don't think that changing formal
    >>>> arguments is good.
    >>>> 
    >>>> In R devel r71344, 'use.names' is not an argument of functions 'c.Date',
    >>>> 'c.POSIXct' and 'c.difftime'.
You are right, Suharto, that methods for c() currently have no
such argument.

But again because c() is primitive and has a '...' at the
beginning, this does not explicitly hurt, currently, does it?

    >>>> Could 'use.names' be documented to be accepted by the default method of
    >>>> 'c', but not listed as a formal argument of 'c'?
    >>>> Or, could the code that handles the argument name
    >>>> 'use.names' be removed? 

In principle, of course both could happen, and if one of these
two was preferable to the current state, I'd tend to the first one:
Consider 'use.names [= FALSE]' just an argument of the default
method for c(),  so existing c() methods would not have a strong need
for updating.

Notably, as the S4 generic for c,
via lines 48-49 of src/library/methods/R/BasicFunsList.R

, "c" = structure(function(x, ..., recursive = FALSE) standardGeneric("c"),
                  signature="x")

has never had 'recursive' as part of the signature..
(and yes, that line 48 does need an update too !!!).

Martin

    >>>> ----------------
    >>>> >>>>> David Winsemius <dwinsemius at comcast.net>
    >>>> >>>>>     on Tue, 20 Sep 2016 23:46:48 -0700 writes:
    >>>> 
    >>>> >> On Sep 20, 2016, at 7:18 PM, Karl Millar via R-devel <r-devel at
    r-project.org> wrote:
    >>>> >>
    >>>> >> 'c' has an undocumented 'use.names' argument.  I'm not sure if this
    >>>> is
    >>>> >> a documentation or implementation bug.
    >>>> 
    >>>> > It came up on stackoverflow a couple of years ago:
    >>>> 
    >>>> > http://stackoverflow.com/questions/24815572/why-does-
    >>>> function-c-accept-an-undocumented-argument/24815653#24815653
    >>>> 
    >>>> > At the time it appeared to me to be a documentation lag.
    >>>> 
    >>>> Thank you, Karl and David,
    >>>> yes it is a documentation glitch ... and a bit more:  Experts know that
    >>>> print()ing of primitive functions is, eehm, "special".
    >>>> 
    >>>> I've committed a change to R-devel ... (with the intent to port
    >>>> to R-patched).
    >>>> 
    >>>> Martin
    >>>> 
    >>>> >>
    >>>> >>> c(a = 1)
    >>>> >> a
    >>>> >> 1
    >>>> >>> c(a = 1, use.names = F)
    >>>> >> [1] 1
    >>>> >>
    >>>> >> Karl
    >>>>


From henrik.bengtsson at gmail.com  Sat Sep 24 20:31:49 2016
From: henrik.bengtsson at gmail.com (Henrik Bengtsson)
Date: Sat, 24 Sep 2016 11:31:49 -0700
Subject: [Rd] withAutoprint({ .... }) ?
In-Reply-To: <22502.33207.568185.181074@stat.math.ethz.ch>
References: <ce4bc223-12d1-2618-aa43-cd12c11396eb@gmail.com>
	<CAPPM_gSzW37A9xQn1qrTEqd6jWx-mE-cOq2i7zEqU08THqJn7A@mail.gmail.com>
	<20160902115636.4C6BE1A75DF@lynne.stat.math.ethz.ch>
	<CAF8bMcaOVsUsdSUN0GE+1COsb4hYBRaD7qdFcadY--PXB_bJaQ@mail.gmail.com>
	<22502.33207.568185.181074@stat.math.ethz.ch>
Message-ID: <CAFDcVCSYsxk5QVqU_6GtWqET=nbqqcMCakhDbx2nDhAhr5C7xg@mail.gmail.com>

Martin, did you post your code for withAutoprint() anywhere?

Building withAutoprint() on top of source() definitely makes sense,
unless, as Bill says, source() itself could provide the same feature.

To differentiate between withAutoprint({ x <- 1 }) and
withAutoprint(expr) where is an expression / language object, one
could have an optional argument `substitute=TRUE`, e.g.

withAutoprint <- function(expr, substitute = TRUE, ...) {
  if (substitute) expr <- substitute(expr)
  [...]
}

Just some thoughts

/Henrik


On Sat, Sep 24, 2016 at 6:37 AM, Martin Maechler
<maechler at stat.math.ethz.ch> wrote:
>>>>>> William Dunlap <wdunlap at tibco.com>
>>>>>>     on Fri, 2 Sep 2016 08:33:47 -0700 writes:
>
>     > Re withAutoprint(), Splus's source() function could take a expression
>     > (literal or not) in place of a file name or text so it could support
>     > withAutoprint-like functionality in its GUI.  E.g.,
>
>     >> source(auto.print=TRUE, exprs.literal= { x <- 3:7 ; sum(x) ; y <- log(x)
>     > ; x - 100}, prompt="--> ")
>     --> x <- 3:7
>     --> sum(x)
>     > [1] 25
>     --> y <- log(x)
>     --> x - 100
>     > [1] -97 -96 -95 -94 -93
>
>     > or
>
>     >> expr <- quote({ x <- 3:7 ; sum(x) ; y <- log(x) ; x - 100})
>     >> source(auto.print=TRUE, exprs = expr, prompt="--> ")
>     --> x <- 3:7
>     --> sum(x)
>     > [1] 25
>     --> y <- log(x)
>     --> x - 100
>     > [1] -97 -96 -95 -94 -93
>
>     > It was easy to implement, since exprs's default value is parse(file) or
>     > parse(text=text), which source is calculating anyway.
>
>
>     > Bill Dunlap
>     > TIBCO Software
>     > wdunlap tibco.com
>
> Thank you, Bill  (and the other correspondents); that's indeed a
> very good suggestion :
>
> I've come to the conclusion that Duncan and Bill are right:  One
> should do this in R (not C) and as Bill hinted, one should use
> source().  I first tried to do it separately, just "like source()",
> but a considerable part of the source of source()  {:-)} is
> about using src attributes instead of deparse() when the former
> are present,  and it does make sense to generalize
> withAutoprint() to have the same feature, so after all, have it
> call source().
>
> I've spent a few hours now trying things and variants, also
> found I needed to enhance source()  very slightly also in a few
> other details, and now (in my uncommitted version of R-devel),
>
>   withAutoprint({ x <- 1:12; x-1; (y <- (x-5)^2); z <- y; z - 10 })
>
> produces
>
>> withAutoprint({ x <- 1:12; x-1; (y <- (x-5)^2); z <- y; z - 10 })
>> x <- 1:12
>> x - 1
>  [1]  0  1  2  3  4  5  6  7  8  9 10 11
>> (y <- (x - 5)^2)
>  [1] 16  9  4  1  0  1  4  9 16 25 36 49
>> z <- y
>> z - 10
>  [1]   6  -1  -6  -9 -10  -9  -6  -1   6  15  26  39
>>
>
> and is equivalent to
>
>    withAutoprint(expression(x <- 1:12, x-1, (y <- (x-5)^2), z <- y, z - 10 ))
>
> I don't see any way around the "mis-feature" that all "input"
> expressions are in the end shown twice in the "output" (the
> first time by showing the withAutoprint(...) call itself).
>
> The function *name* is "not bad" but also a bit longish;
> maybe there are better ideas?  (not longer, no "_" - I know this
>                                 is a matter of taste only)
>
> Martin
>
>     > On Fri, Sep 2, 2016 at 4:56 AM, Martin Maechler <maechler at stat.math.ethz.ch>
>     > wrote:
>
>     >> On R-help, with subject
>     >> '[R] source() does not include added code'
>     >>
>     >> >>>>> Joshua Ulrich <josh.m.ulrich at gmail.com>
>     >> >>>>>     on Wed, 31 Aug 2016 10:35:01 -0500 writes:
>     >>
>     >> > I have quantstrat installed and it works fine for me.  If you're
>     >> > asking why the output of t(tradeStats('macross')) isn't being
>     >> printed,
>     >> > that's because of what's described in the first paragraph in the
>     >> > *Details* section of help("source"):
>     >>
>     >> > Note that running code via ?source? differs in a few respects from
>     >> > entering it at the R command line.  Since expressions are not
>     >> > executed at the top level, auto-printing is not done.  So you will
>     >> > need to include explicit ?print? calls for things you want to be
>     >> > printed (and remember that this includes plotting by ?lattice?,
>     >> > FAQ Q7.22).
>     >>
>     >>
>     >>
>     >> > So you need:
>     >>
>     >> > print(t(tradeStats('macross')))
>     >>
>     >> > if you want the output printed to the console.
>     >>
>     >> indeed, and "of course"" ;-)
>     >>
>     >> As my subject indicates, this is another case, where it would be
>     >> very convenient to have a function
>     >>
>     >> withAutoprint()
>     >>
>     >> so the OP could have (hopefully) have used
>     >> withAutoprint(source(..))
>     >> though that would have been equivalent to the already nicely existing
>     >>
>     >> source(.., print.eval = TRUE)
>     >>
>     >> which works via the  withVisible(.) utility that returns for each
>     >> 'expression' if it would auto print or not, and then does print (or
>     >> not) accordingly.
>     >>
>     >> My own use cases for such a withAutoprint({...})
>     >> are demos and examples, sometimes even package tests which I want to print:
>     >>
>     >> Assume I have a nice demo / example on a help page/ ...
>     >>
>     >> foo(..)
>     >> (z <- bar(..))
>     >> summary(z)
>     >> ....
>     >>
>     >> where I carefully do print parts (and don't others),
>     >> and suddenly I find I want to run that part of the demo /
>     >> example / test only in some circumstances, e.g., only when
>     >> interactive, but not in BATCH, or only if it is me, the package maintainer,
>     >>
>     >> if( identical(Sys.getenv("USER"), "maechler") ) {
>     >> foo(..)
>     >> (z <- bar(..))
>     >> summary(z)
>     >> ....
>     >> }
>     >>
>     >> Now all the auto-printing is gone, and
>     >>
>     >> 1) I have to find out which of these function calls do autoprint and wrap
>     >> a print(..) around these, and
>     >>
>     >> 2) the result is quite ugly (for an example on a help page etc.)
>     >>
>     >> What I would like in a future R, is to be able to simply wrap the "{
>     >> .. } above with an 'withAutoprint(.) :
>     >>
>     >> if( identical(Sys.getenv("USER"), "maechler") ) withAutoprint({
>     >> foo(..)
>     >> (z <- bar(..))
>     >> summary(z)
>     >> ....
>     >> })
>     >>
>     >> Conceptually such a function could be written similar to source() with an R
>     >> level for loop, treating each expression separately, calling eval(.) etc.
>     >> That may cost too much performnace, ... still to have it would be better
>     >> than
>     >> not having the possibility.
>     >>
>     >> ----
>     >>
>     >> If you read so far, you'd probably agree that such a function
>     >> could be a nice asset in R,
>     >> notably if it was possible to do this on the fast C level of R's main
>     >> REPL.
>     >>
>     >> Have any of you looked into how this could be provided in R ?
>     >> If you know the source a little, you will remember that there's
>     >> the global variable  R_Visible  which is crucial here.
>     >> The problem with that is that it *is* global, and only available
>     >> as that; that the auto-printing "concept" is so linked to "toplevel
>     >> context"
>     >> and that is not easy, and AFAIK not so much centralized in one place in the
>     >> source. Consequently, all kind of (very) low level functions manipulate
>     >> R_Visible
>     >> temporarily.... and so a C level implementation of  withAutoprint() may
>     >> need considerable more changes than just setting R_Visible to TRUE in one
>     >> place.
>     >>
>     >> Have any efforts / experiments already happened towards providing such
>     >> functionality ?
>     >>
>     >> ______________________________________________
>     >> R-devel at r-project.org mailing list
>     >> https://stat.ethz.ch/mailman/listinfo/r-devel
>
>     > [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From suharto_anggono at yahoo.com  Sun Sep 25 16:12:10 2016
From: suharto_anggono at yahoo.com (Suharto Anggono Suharto Anggono)
Date: Sun, 25 Sep 2016 14:12:10 +0000 (UTC)
Subject: [Rd] Undocumented 'use.names' argument to c()
References: <1598063985.4335611.1474812730344.ref@mail.yahoo.com>
Message-ID: <1598063985.4335611.1474812730344@mail.yahoo.com>

>From comments in http://stackoverflow.com/questions/24815572/why-does-function-c-accept-an-undocumented-argument/24815653 : The code of c() and unlist() was formerly shared but has been (long time passing) separated. From July 30, 1998, is where do_c got split into do_c and do_unlist.

With the implementation of 'c.Date' in R devel r71350, an argument named 'use.names' is included for concatenation. So, it doesn't follow the documented 'c'. But, 'c.Date' is not explicitly documented in Dates.Rd, that has 'c.Date' as an alias.
--------------------------------------------
On Sat, 24/9/16, Martin Maechler <maechler at stat.math.ethz.ch> wrote:

 Subject: Re: [Rd] Undocumented 'use.names' argument to c()
 To: "Karl Millar" <kmillar at google.com>

 Date: Saturday, 24 September, 2016, 9:12 PM
 
 >>>>> Karl Millar via R-devel <r-devel at r-project.org>
>>>>>     on Fri, 23 Sep 2016 11:12:49 -0700 writes:

    > I'd expect that a lot of the performance overhead could be eliminated
    > by simply improving the underlying code.  IMHO, we should ignore it in
    > deciding the API that we want here.

I agree partially.  Even if the underlying code can be made
faster, the 'use.names = FALSE' version will still be faster
than the default, notably in some "long" cases.

More further down.

    > On Fri, Sep 23, 2016 at 10:54 AM, Henrik Bengtsson
    > <henrik.bengtsson at gmail.com> wrote:
    >> I'd vote for it to stay.  It could of course suprise someone who'd
    >> expect c(list(a=1), b=2, use.names = FALSE) to generate list(a=1, b=2,
    >> use.names=FALSE).   On the upside, is the performance gain from using
    >> use.names=FALSE.  Below benchmarks show that the combining of the
    >> names attributes themselves takes ~20-25 times longer than the
    >> combining of the integers themselves.  Also, at no surprise,
    >> use.names=FALSE avoids some memory allocations.
    >> 
    >>> options(digits = 2)
    >>> 
    >>> a <- b <- c <- d <- 1:1e4
    >>> names(c) <- c
    >>> names(d) <- d
    >>> 
    >>> stats <- microbenchmark::microbenchmark(
    >> +   c(a, b, use.names=FALSE),
    >> +   c(c, d, use.names=FALSE),
    >> +   c(a, d, use.names=FALSE),
    >> +   c(a, b, use.names=TRUE),
    >> +   c(a, d, use.names=TRUE),
    >> +   c(c, d, use.names=TRUE),
    >> +   unit = "ms"
    >> + )
    >>> 
    >>> stats
    >> Unit: milliseconds
    >> expr   min    lq  mean median    uq   max neval
    >> c(a, b, use.names = FALSE) 0.031 0.032 0.049  0.034 0.036 1.474   100
    >> c(c, d, use.names = FALSE) 0.031 0.031 0.035  0.034 0.035 0.064   100
    >> c(a, d, use.names = FALSE) 0.031 0.031 0.049  0.034 0.035 1.452   100
    >> c(a, b, use.names = TRUE) 0.031 0.031 0.055  0.034 0.036 2.094   100
    >> c(a, d, use.names = TRUE) 0.510 0.526 0.588  0.549 0.617 1.998   100
    >> c(c, d, use.names = TRUE) 0.780 0.815 0.886  0.841 0.944 1.430   100
    >> 
    >>> profmem::profmem(c(c, d, use.names=FALSE))
    >> Rprofmem memory profiling of:
    >> c(c, d, use.names = FALSE)
    >> 
    >> Memory allocations:
    >> bytes      calls
    >> 1     80040 <internal>
    >> total 80040
    >> 
    >>> profmem::profmem(c(c, d, use.names=TRUE))
    >> Rprofmem memory profiling of:
    >> c(c, d, use.names = TRUE)
    >> 
    >> Memory allocations:
    >> bytes      calls
    >> 1      80040 <internal>
    >> 2     160040 <internal>
    >> total 240080
    >> 
    >> /Henrik
    >> 
    >> On Fri, Sep 23, 2016 at 10:25 AM, William Dunlap via R-devel
    >> <r-devel at r-project.org> wrote:
    >>> In Splus c() and unlist() called the same C code, but with a different
    >>> 'sys_index'  code (the last argument to .Internal) and c() did not consider
    >>> an argument named 'use.names' special.

Thank you, Bill, very much, for making the historical context
clear, and giving us the facts, there.

OTOH, it is also true in R, that  c() and unlist() share code
.. quite a bit less though .. but more importantly, the very
original C code of Ross Ihaka (and possibly Robert Gentleman)
had explicitly considered both extra arguments 'recursive' and
'use.names', and not just the first.

The fact that c() has always been a .Primitive function and that
these have no formals()  had contributed to what I think to be a
documentation glitch early on, and when, quite a bit later, we've
added a fake argument list for printing, the then current
documentation was used.

This was the reason for declaring it a documentation "hole"
rather than something we do not want.

(read on)

    >>>> c
    >>> function(..., recursive = F)
    >>> .Internal(c(..., recursive = recursive), "S_unlist", TRUE, 1)
    >>>> unlist
    >>> function(data, recursive = T, use.names = T)
    >>> .Internal(unlist(data, recursive = recursive, use.names = use.names),
    >>> "S_unlist", TRUE, 2)
    >>>> c(A=1,B=2,use.names=FALSE)
    >>> A B use.names
    >>> 1 2         0
    >>> 
    >>> The C code used sys_index==2 to mean 'the last  argument is the 'use.names'
    >>> argument, if sys_index==1 only the recursive argument was considered
    >>> special.
    >>> 
    >>> Sys.funs.c:
    >>> 405 S_unlist(vector *ent, vector *arglist, s_evaluator *S_evaluator)
    >>> 406 {
    >>> 407         int which = sys_index; boolean named, recursive, names;
    >>> ...
    >>> 419         args = arglist->value.tree; n = arglist->length;
    >>> ...
    >>> 424         names = which==2 ? logical_value(args[--n], ent, S_evaluator)
    >>> : (which == 1);
    >>> 
    >>> Thus there is no historical reason for giving c() the use.names argument.
    >>> 
    >>> 
    >>> Bill Dunlap
    >>> TIBCO Software
    >>> wdunlap tibco.com
    >>> 
    >>> On Fri, Sep 23, 2016 at 9:37 AM, Suharto Anggono Suharto Anggono via
    >>> R-devel <r-devel at r-project.org> wrote:
    >>> 
    >>>> In S-PLUS 3.4 help on 'c' (http://www.uni-muenster.de/
    >>>> ZIV.BennoSueselbeck/s-html/helpfiles/c.html), there is no 'use.names'
    >>>> argument.
    >>>> 
    >>>> Because 'c' is a generic function, I don't think that changing formal
    >>>> arguments is good.
    >>>> 
    >>>> In R devel r71344, 'use.names' is not an argument of functions 'c.Date',
    >>>> 'c.POSIXct' and 'c.difftime'.
You are right, Suharto, that methods for c() currently have no
such argument.

But again because c() is primitive and has a '...' at the
beginning, this does not explicitly hurt, currently, does it?

    >>>> Could 'use.names' be documented to be accepted by the default method of
    >>>> 'c', but not listed as a formal argument of 'c'?
    >>>> Or, could the code that handles the argument name
    >>>> 'use.names' be removed? 

In principle, of course both could happen, and if one of these
two was preferable to the current state, I'd tend to the first one:
Consider 'use.names [= FALSE]' just an argument of the default
method for c(),  so existing c() methods would not have a strong need
for updating.

Notably, as the S4 generic for c,
via lines 48-49 of src/library/methods/R/BasicFunsList.R

, "c" = structure(function(x, ..., recursive = FALSE) standardGeneric("c"),
                  signature="x")

has never had 'recursive' as part of the signature..
(and yes, that line 48 does need an update too !!!).

Martin


    >>>> ----------------
    >>>> >>>>> David Winsemius <dwinsemius at comcast.net>
    >>>> >>>>>     on Tue, 20 Sep 2016 23:46:48 -0700 writes:
    >>>> 
    >>>> >> On Sep 20, 2016, at 7:18 PM, Karl Millar via R-devel <r-devel at
    r-project.org> wrote:
    >>>> >>
    >>>> >> 'c' has an undocumented 'use.names' argument.  I'm not sure if this
    >>>> is
    >>>> >> a documentation or implementation bug.
    >>>> 
    >>>> > It came up on stackoverflow a couple of years ago:
    >>>> 
    >>>> > http://stackoverflow.com/questions/24815572/why-does-
    >>>> function-c-accept-an-undocumented-argument/24815653#24815653
    >>>> 
    >>>> > At the time it appeared to me to be a documentation lag.
    >>>> 
    >>>> Thank you, Karl and David,
    >>>> yes it is a documentation glitch ... and a bit more:  Experts know that
    >>>> print()ing of primitive functions is, eehm, "special".
    >>>> 
    >>>> I've committed a change to R-devel ... (with the intent to port
    >>>> to R-patched).
    >>>> 
    >>>> Martin
    >>>> 
    >>>> >>
    >>>> >>> c(a = 1)
    >>>> >> a
    >>>> >> 1
    >>>> >>> c(a = 1, use.names = F)
    >>>> >> [1] 1
    >>>> >>
    >>>> >> Karl
    >>>>


From maechler at stat.math.ethz.ch  Sun Sep 25 17:14:04 2016
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sun, 25 Sep 2016 17:14:04 +0200
Subject: [Rd] Undocumented 'use.names' argument to c()
In-Reply-To: <1598063985.4335611.1474812730344@mail.yahoo.com>
References: <1598063985.4335611.1474812730344.ref@mail.yahoo.com>
	<1598063985.4335611.1474812730344@mail.yahoo.com>
Message-ID: <22503.59836.871086.787800@stat.math.ethz.ch>

>>>>> Suharto Anggono Suharto Anggono via R-devel <r-devel at r-project.org>
>>>>>     on Sun, 25 Sep 2016 14:12:10 +0000 writes:

    >> From comments in
    >> http://stackoverflow.com/questions/24815572/why-does-function-c-accept-an-undocumented-argument/24815653
    >> : The code of c() and unlist() was formerly shared but
    >> has been (long time passing) separated. From July 30,
    >> 1998, is where do_c got split into do_c and do_unlist.
    > With the implementation of 'c.Date' in R devel r71350, an
    > argument named 'use.names' is included for
    > concatenation. So, it doesn't follow the documented
    > 'c'. But, 'c.Date' is not explicitly documented in
    > Dates.Rd, that has 'c.Date' as an alias.

I do not see any  c.Date  in R-devel with a 'use.names'; its a
base function, hence not hidden ..

As mentioned before, 'use.names' is used in unlist() in quite a
few places, and such an argument also exists for

    lengths()        	and
    all.equal.list()

and now c() 

    > --------------------------------------------
    > On Sat, 24/9/16, Martin Maechler
    > <maechler at stat.math.ethz.ch> wrote:

    >  Subject: Re: [Rd] Undocumented 'use.names' argument to
    > c() To: "Karl Millar" <kmillar at google.com>

    >  Date: Saturday, 24 September, 2016, 9:12 PM
 
    >>>>>> Karl Millar via R-devel <r-devel at r-project.org>
>>>>> on Fri, 23 Sep 2016 11:12:49 -0700 writes:

    >> I'd expect that a lot of the performance overhead could
    >> be eliminated by simply improving the underlying code.
    >> IMHO, we should ignore it in deciding the API that we
    >> want here.

    > I agree partially.  Even if the underlying code can be
    > made faster, the 'use.names = FALSE' version will still be
    > faster than the default, notably in some "long" cases.

    > More further down.

    >> On Fri, Sep 23, 2016 at 10:54 AM, Henrik Bengtsson
    >> <henrik.bengtsson at gmail.com> wrote:
    >>> I'd vote for it to stay.  It could of course suprise
    >>> someone who'd expect c(list(a=1), b=2, use.names =
    >>> FALSE) to generate list(a=1, b=2, use.names=FALSE).  On
    >>> the upside, is the performance gain from using
    >>> use.names=FALSE.  Below benchmarks show that the
    >>> combining of the names attributes themselves takes
    >>> ~20-25 times longer than the combining of the integers
    >>> themselves.  Also, at no surprise, use.names=FALSE
    >>> avoids some memory allocations.
    >>> 
    >>>> options(digits = 2)
    >>>> 
    >>>> a <- b <- c <- d <- 1:1e4 names(c) <- c names(d) <- d
    >>>> 
    >>>> stats <- microbenchmark::microbenchmark(
    >>> + c(a, b, use.names=FALSE), + c(c, d, use.names=FALSE),
    >>> + c(a, d, use.names=FALSE), + c(a, b, use.names=TRUE), +
    >>> c(a, d, use.names=TRUE), + c(c, d, use.names=TRUE), +
    >>> unit = "ms" + )
    >>>> 
    >>>> stats
    >>> Unit: milliseconds expr min lq mean median uq max neval
    >>> c(a, b, use.names = FALSE) 0.031 0.032 0.049 0.034 0.036
    >>> 1.474 100 c(c, d, use.names = FALSE) 0.031 0.031 0.035
    >>> 0.034 0.035 0.064 100 c(a, d, use.names = FALSE) 0.031
    >>> 0.031 0.049 0.034 0.035 1.452 100 c(a, b, use.names =
    >>> TRUE) 0.031 0.031 0.055 0.034 0.036 2.094 100 c(a, d,
    >>> use.names = TRUE) 0.510 0.526 0.588 0.549 0.617 1.998
    >>> 100 c(c, d, use.names = TRUE) 0.780 0.815 0.886 0.841
    >>> 0.944 1.430 100
    >>> 
    >>>> profmem::profmem(c(c, d, use.names=FALSE))
    >>> Rprofmem memory profiling of: c(c, d, use.names = FALSE)
    >>> 
    >>> Memory allocations: bytes calls 1 80040 <internal> total
    >>> 80040
    >>> 
    >>>> profmem::profmem(c(c, d, use.names=TRUE))
    >>> Rprofmem memory profiling of: c(c, d, use.names = TRUE)
    >>> 
    >>> Memory allocations: bytes calls 1 80040 <internal> 2
    >>> 160040 <internal> total 240080
    >>> 
    >>> /Henrik
    >>> 
    >>> On Fri, Sep 23, 2016 at 10:25 AM, William Dunlap via
    >>> R-devel <r-devel at r-project.org> wrote:
    >>>> In Splus c() and unlist() called the same C code, but
    >>>> with a different 'sys_index' code (the last argument to
    >>>> .Internal) and c() did not consider an argument named
    >>>> 'use.names' special.

    > Thank you, Bill, very much, for making the historical
    > context clear, and giving us the facts, there.

    > OTOH, it is also true in R, that c() and unlist() share
    > code .. quite a bit less though .. but more importantly,
    > the very original C code of Ross Ihaka (and possibly
    > Robert Gentleman) had explicitly considered both extra
    > arguments 'recursive' and 'use.names', and not just the
    > first.

    > The fact that c() has always been a .Primitive function
    > and that these have no formals() had contributed to what I
    > think to be a documentation glitch early on, and when,
    > quite a bit later, we've added a fake argument list for
    > printing, the then current documentation was used.

    > This was the reason for declaring it a documentation
    > "hole" rather than something we do not want.

    > (read on)

    >>>>> c
    >>>> function(..., recursive = F) .Internal(c(..., recursive
    >>>> = recursive), "S_unlist", TRUE, 1)
    >>>>> unlist
    >>>> function(data, recursive = T, use.names = T)
    >>>> .Internal(unlist(data, recursive = recursive, use.names
    >>>> = use.names), "S_unlist", TRUE, 2)
    >>>>> c(A=1,B=2,use.names=FALSE)
    >>>> A B use.names 1 2 0
    >>>> 
    >>>> The C code used sys_index==2 to mean 'the last argument
    >>>> is the 'use.names' argument, if sys_index==1 only the
    >>>> recursive argument was considered special.
    >>>> 
    >>>> Sys.funs.c: 405 S_unlist(vector *ent, vector *arglist,
    >>>> s_evaluator *S_evaluator) 406 { 407 int which =
    >>>> sys_index; boolean named, recursive, names; ...  419
    >>>> args = arglist->value.tree; n = arglist->length; ...
    >>>> 424 names = which==2 ? logical_value(args[--n], ent,
    >>>> S_evaluator) : (which == 1);
    >>>> 
    >>>> Thus there is no historical reason for giving c() the
    >>>> use.names argument.
    >>>> 
    >>>> 
    >>>> Bill Dunlap TIBCO Software wdunlap tibco.com
    >>>> 
    >>>> On Fri, Sep 23, 2016 at 9:37 AM, Suharto Anggono
    >>>> Suharto Anggono via R-devel <r-devel at r-project.org>
    >>>> wrote:
    >>>> 
    >>>>> In S-PLUS 3.4 help on 'c' (http://www.uni-muenster.de/
    >>>>> ZIV.BennoSueselbeck/s-html/helpfiles/c.html), there is
    >>>>> no 'use.names' argument.
    >>>>> 
    >>>>> Because 'c' is a generic function, I don't think that
    >>>>> changing formal arguments is good.
    >>>>> 
    >>>>> In R devel r71344, 'use.names' is not an argument of
    >>>>> functions 'c.Date', 'c.POSIXct' and 'c.difftime'.
    > You are right, Suharto, that methods for c() currently
    > have no such argument.

    > But again because c() is primitive and has a '...' at the
    > beginning, this does not explicitly hurt, currently, does
    > it?

    >>>>> Could 'use.names' be documented to be accepted by the
    >>>>> default method of 'c', but not listed as a formal
    >>>>> argument of 'c'?  Or, could the code that handles the
    >>>>> argument name 'use.names' be removed?

    > In principle, of course both could happen, and if one of
    > these two was preferable to the current state, I'd tend to
    > the first one: Consider 'use.names [= FALSE]' just an
    > argument of the default method for c(), so existing c()
    > methods would not have a strong need for updating.

    > Notably, as the S4 generic for c, via lines 48-49 of
    > src/library/methods/R/BasicFunsList.R

    > , "c" = structure(function(x, ..., recursive = FALSE)
    > standardGeneric("c"), signature="x")

    > has never had 'recursive' as part of the signature..  (and
    > yes, that line 48 does need an update too !!!).

    > Martin


    >>>>> ----------------
    >>>>> >>>>> David Winsemius <dwinsemius at comcast.net>
    >>>>> >>>>> on Tue, 20 Sep 2016 23:46:48 -0700 writes:
    >>>>> 
    >>>>> >> On Sep 20, 2016, at 7:18 PM, Karl Millar via
    >>>>> R-devel <r-devel at
    r-project.org> wrote:
    >>>>> >>
    >>>>> >> 'c' has an undocumented 'use.names' argument.  I'm
    >>>>> not sure if this is >> a documentation or
    >>>>> implementation bug.
    >>>>> 
    >>>>> > It came up on stackoverflow a couple of years ago:
    >>>>> 
    >>>>> >
    >>>>> http://stackoverflow.com/questions/24815572/why-does-
    >>>>> function-c-accept-an-undocumented-argument/24815653#24815653
    >>>>> 
    >>>>> > At the time it appeared to me to be a documentation
    >>>>> lag.
    >>>>> 
    >>>>> Thank you, Karl and David, yes it is a documentation
    >>>>> glitch ... and a bit more: Experts know that
    >>>>> print()ing of primitive functions is, eehm, "special".
    >>>>> 
    >>>>> I've committed a change to R-devel ... (with the
    >>>>> intent to port to R-patched).
    >>>>> 
    >>>>> Martin
    >>>>> 
    >>>>> >>
    >>>>> >>> c(a = 1) >> a >> 1 >>> c(a = 1, use.names = F) >>
    >>>>> [1] 1
    >>>>> >>
    >>>>> >> Karl
    >>>>> 

    > ______________________________________________
    > R-devel at r-project.org mailing list
    > https://stat.ethz.ch/mailman/listinfo/r-devel


From maechler at stat.math.ethz.ch  Sun Sep 25 18:29:21 2016
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sun, 25 Sep 2016 18:29:21 +0200
Subject: [Rd] withAutoprint({ .... }) ?
In-Reply-To: <CAFDcVCSYsxk5QVqU_6GtWqET=nbqqcMCakhDbx2nDhAhr5C7xg@mail.gmail.com>
References: <ce4bc223-12d1-2618-aa43-cd12c11396eb@gmail.com>
	<CAPPM_gSzW37A9xQn1qrTEqd6jWx-mE-cOq2i7zEqU08THqJn7A@mail.gmail.com>
	<20160902115636.4C6BE1A75DF@lynne.stat.math.ethz.ch>
	<CAF8bMcaOVsUsdSUN0GE+1COsb4hYBRaD7qdFcadY--PXB_bJaQ@mail.gmail.com>
	<22502.33207.568185.181074@stat.math.ethz.ch>
	<CAFDcVCSYsxk5QVqU_6GtWqET=nbqqcMCakhDbx2nDhAhr5C7xg@mail.gmail.com>
Message-ID: <22503.64353.207088.597099@stat.math.ethz.ch>

>>>>> Henrik Bengtsson <henrik.bengtsson at gmail.com>
>>>>>     on Sat, 24 Sep 2016 11:31:49 -0700 writes:

    > Martin, did you post your code for withAutoprint() anywhere?
    > Building withAutoprint() on top of source() definitely makes sense,
    > unless, as Bill says, source() itself could provide the same feature.

I was really mainly asking for advice about the function name
.. and got none.

I'm now committing my version (including (somewhat incomplete)
documentation, so you (all) can look at it and try / test it further.

    > To differentiate between withAutoprint({ x <- 1 }) and
    > withAutoprint(expr) where is an expression / language object, one
    > could have an optional argument `substitute=TRUE`, e.g.

    > withAutoprint <- function(expr, substitute = TRUE, ...) {
    >    if (substitute) expr <- substitute(expr)
    >    [...]
    > }

I think my approach is nicer insofar it does not seem to need
such an argument.... I'm sure you'll try to disprove that ;-)

Martin

    > Just some thoughts
    > /Henrik


    > On Sat, Sep 24, 2016 at 6:37 AM, Martin Maechler
    > <maechler at stat.math.ethz.ch> wrote:
    >>>>>>> William Dunlap <wdunlap at tibco.com>
    >>>>>>> on Fri, 2 Sep 2016 08:33:47 -0700 writes:
    >> 
    >> > Re withAutoprint(), Splus's source() function could take a expression
    >> > (literal or not) in place of a file name or text so it could support
    >> > withAutoprint-like functionality in its GUI.  E.g.,
    >> 
    >> >> source(auto.print=TRUE, exprs.literal= { x <- 3:7 ; sum(x) ; y <- log(x)
    >> > ; x - 100}, prompt="--> ")
    --> x <- 3:7
    --> sum(x)
    >> > [1] 25
    --> y <- log(x)
    --> x - 100
    >> > [1] -97 -96 -95 -94 -93
    >> 
    >> > or
    >> 
    >> >> expr <- quote({ x <- 3:7 ; sum(x) ; y <- log(x) ; x - 100})
    >> >> source(auto.print=TRUE, exprs = expr, prompt="--> ")
    --> x <- 3:7
    --> sum(x)
    >> > [1] 25
    --> y <- log(x)
    --> x - 100
    >> > [1] -97 -96 -95 -94 -93
    >> 
    >> > It was easy to implement, since exprs's default value is parse(file) or
    >> > parse(text=text), which source is calculating anyway.
    >> 
    >> 
    >> > Bill Dunlap
    >> > TIBCO Software
    >> > wdunlap tibco.com
    >> 
    >> Thank you, Bill  (and the other correspondents); that's indeed a
    >> very good suggestion :
    >> 
    >> I've come to the conclusion that Duncan and Bill are right:  One
    >> should do this in R (not C) and as Bill hinted, one should use
    >> source().  I first tried to do it separately, just "like source()",
    >> but a considerable part of the source of source()  {:-)} is
    >> about using src attributes instead of deparse() when the former
    >> are present,  and it does make sense to generalize
    >> withAutoprint() to have the same feature, so after all, have it
    >> call source().
    >> 
    >> I've spent a few hours now trying things and variants, also
    >> found I needed to enhance source()  very slightly also in a few
    >> other details, and now (in my uncommitted version of R-devel),
    >> 
    >> withAutoprint({ x <- 1:12; x-1; (y <- (x-5)^2); z <- y; z - 10 })
    >> 
    >> produces
    >> 
    >>> withAutoprint({ x <- 1:12; x-1; (y <- (x-5)^2); z <- y; z - 10 })
    >>> x <- 1:12
    >>> x - 1
    >> [1]  0  1  2  3  4  5  6  7  8  9 10 11
    >>> (y <- (x - 5)^2)
    >> [1] 16  9  4  1  0  1  4  9 16 25 36 49
    >>> z <- y
    >>> z - 10
    >> [1]   6  -1  -6  -9 -10  -9  -6  -1   6  15  26  39
    >>> 
    >> 
    >> and is equivalent to
    >> 
    >> withAutoprint(expression(x <- 1:12, x-1, (y <- (x-5)^2), z <- y, z - 10 ))
    >> 
    >> I don't see any way around the "mis-feature" that all "input"
    >> expressions are in the end shown twice in the "output" (the
    >> first time by showing the withAutoprint(...) call itself).
    >> 
    >> The function *name* is "not bad" but also a bit longish;
    >> maybe there are better ideas?  (not longer, no "_" - I know this
    >> is a matter of taste only)
    >> 
    >> Martin
    >> 
    >> > On Fri, Sep 2, 2016 at 4:56 AM, Martin Maechler <maechler at stat.math.ethz.ch>
    >> > wrote:
    >> 
    >> >> On R-help, with subject
    >> >> '[R] source() does not include added code'
    >> >>
    >> >> >>>>> Joshua Ulrich <josh.m.ulrich at gmail.com>
    >> >> >>>>>     on Wed, 31 Aug 2016 10:35:01 -0500 writes:
    >> >>
    >> >> > I have quantstrat installed and it works fine for me.  If you're
    >> >> > asking why the output of t(tradeStats('macross')) isn't being
    >> >> printed,
    >> >> > that's because of what's described in the first paragraph in the
    >> >> > *Details* section of help("source"):
    >> >>
    >> >> > Note that running code via ?source? differs in a few respects from
    >> >> > entering it at the R command line.  Since expressions are not
    >> >> > executed at the top level, auto-printing is not done.  So you will
    >> >> > need to include explicit ?print? calls for things you want to be
    >> >> > printed (and remember that this includes plotting by ?lattice?,
    >> >> > FAQ Q7.22).
    >> >>
    >> >>
    >> >>
    >> >> > So you need:
    >> >>
    >> >> > print(t(tradeStats('macross')))
    >> >>
    >> >> > if you want the output printed to the console.
    >> >>
    >> >> indeed, and "of course"" ;-)
    >> >>
    >> >> As my subject indicates, this is another case, where it would be
    >> >> very convenient to have a function
    >> >>
    >> >> withAutoprint()
    >> >>
    >> >> so the OP could have (hopefully) have used
    >> >> withAutoprint(source(..))
    >> >> though that would have been equivalent to the already nicely existing
    >> >>
    >> >> source(.., print.eval = TRUE)
    >> >>
    >> >> which works via the  withVisible(.) utility that returns for each
    >> >> 'expression' if it would auto print or not, and then does print (or
    >> >> not) accordingly.
    >> >>
    >> >> My own use cases for such a withAutoprint({...})
    >> >> are demos and examples, sometimes even package tests which I want to print:
    >> >>
    >> >> Assume I have a nice demo / example on a help page/ ...
    >> >>
    >> >> foo(..)
    >> >> (z <- bar(..))
    >> >> summary(z)
    >> >> ....
    >> >>
    >> >> where I carefully do print parts (and don't others),
    >> >> and suddenly I find I want to run that part of the demo /
    >> >> example / test only in some circumstances, e.g., only when
    >> >> interactive, but not in BATCH, or only if it is me, the package maintainer,
    >> >>
    >> >> if( identical(Sys.getenv("USER"), "maechler") ) {
    >> >> foo(..)
    >> >> (z <- bar(..))
    >> >> summary(z)
    >> >> ....
    >> >> }
    >> >>
    >> >> Now all the auto-printing is gone, and
    >> >>
    >> >> 1) I have to find out which of these function calls do autoprint and wrap
    >> >> a print(..) around these, and
    >> >>
    >> >> 2) the result is quite ugly (for an example on a help page etc.)
    >> >>
    >> >> What I would like in a future R, is to be able to simply wrap the "{
    >> >> .. } above with an 'withAutoprint(.) :
    >> >>
    >> >> if( identical(Sys.getenv("USER"), "maechler") ) withAutoprint({
    >> >> foo(..)
    >> >> (z <- bar(..))
    >> >> summary(z)
    >> >> ....
    >> >> })
    >> >>
    >> >> Conceptually such a function could be written similar to source() with an R
    >> >> level for loop, treating each expression separately, calling eval(.) etc.
    >> >> That may cost too much performnace, ... still to have it would be better
    >> >> than
    >> >> not having the possibility.
    >> >>
    >> >> ----
    >> >>
    >> >> If you read so far, you'd probably agree that such a function
    >> >> could be a nice asset in R,
    >> >> notably if it was possible to do this on the fast C level of R's main
    >> >> REPL.
    >> >>
    >> >> Have any of you looked into how this could be provided in R ?
    >> >> If you know the source a little, you will remember that there's
    >> >> the global variable  R_Visible  which is crucial here.
    >> >> The problem with that is that it *is* global, and only available
    >> >> as that; that the auto-printing "concept" is so linked to "toplevel
    >> >> context"
    >> >> and that is not easy, and AFAIK not so much centralized in one place in the
    >> >> source. Consequently, all kind of (very) low level functions manipulate
    >> >> R_Visible
    >> >> temporarily.... and so a C level implementation of  withAutoprint() may
    >> >> need considerable more changes than just setting R_Visible to TRUE in one
    >> >> place.
    >> >>
    >> >> Have any efforts / experiments already happened towards providing such
    >> >> functionality ?
    >> >>
    >> >> ______________________________________________
    >> >> R-devel at r-project.org mailing list
    >> >> https://stat.ethz.ch/mailman/listinfo/r-devel
    >> 
    >> > [[alternative HTML version deleted]]
    >> 
    >> ______________________________________________
    >> R-devel at r-project.org mailing list
    >> https://stat.ethz.ch/mailman/listinfo/r-devel


From henrik.bengtsson at gmail.com  Sun Sep 25 21:38:27 2016
From: henrik.bengtsson at gmail.com (Henrik Bengtsson)
Date: Sun, 25 Sep 2016 12:38:27 -0700
Subject: [Rd] withAutoprint({ .... }) ?
In-Reply-To: <22503.64353.207088.597099@stat.math.ethz.ch>
References: <ce4bc223-12d1-2618-aa43-cd12c11396eb@gmail.com>
	<CAPPM_gSzW37A9xQn1qrTEqd6jWx-mE-cOq2i7zEqU08THqJn7A@mail.gmail.com>
	<20160902115636.4C6BE1A75DF@lynne.stat.math.ethz.ch>
	<CAF8bMcaOVsUsdSUN0GE+1COsb4hYBRaD7qdFcadY--PXB_bJaQ@mail.gmail.com>
	<22502.33207.568185.181074@stat.math.ethz.ch>
	<CAFDcVCSYsxk5QVqU_6GtWqET=nbqqcMCakhDbx2nDhAhr5C7xg@mail.gmail.com>
	<22503.64353.207088.597099@stat.math.ethz.ch>
Message-ID: <CAFDcVCQSyLuza-zEtr6ZvHr0QPdYkjppBCRmge5LNfhRuT0y9w@mail.gmail.com>

On Sun, Sep 25, 2016 at 9:29 AM, Martin Maechler
<maechler at stat.math.ethz.ch> wrote:
>>>>>> Henrik Bengtsson <henrik.bengtsson at gmail.com>
>>>>>>     on Sat, 24 Sep 2016 11:31:49 -0700 writes:
>
>     > Martin, did you post your code for withAutoprint() anywhere?
>     > Building withAutoprint() on top of source() definitely makes sense,
>     > unless, as Bill says, source() itself could provide the same feature.
>
> I was really mainly asking for advice about the function name
> .. and got none.

I missed that part.  I think the name is good.  A shorter alternative
would be withEcho(), but could be a little bit misleading since it
doesn't reflect 'print=TRUE' to source().

>
> I'm now committing my version (including (somewhat incomplete)
> documentation, so you (all) can look at it and try / test it further.
>
>     > To differentiate between withAutoprint({ x <- 1 }) and
>     > withAutoprint(expr) where is an expression / language object, one
>     > could have an optional argument `substitute=TRUE`, e.g.
>
>     > withAutoprint <- function(expr, substitute = TRUE, ...) {
>     >    if (substitute) expr <- substitute(expr)
>     >    [...]
>     > }
>
> I think my approach is nicer insofar it does not seem to need
> such an argument.... I'm sure you'll try to disprove that ;-)

Nah, I like that you've extended source() with the 'exprs' argument.

May I suggest to add:

svn diff src/library/base/R/
Index: src/library/base/R/source.R
===================================================================
--- src/library/base/R/source.R (revision 71357)
+++ src/library/base/R/source.R (working copy)
@@ -198,7 +198,7 @@
      if (!tail) {
     # Deparse.  Must drop "expression(...)"
     dep <- substr(paste(deparse(ei, width.cutoff = width.cutoff,
-                                                control = "showAttributes"),
+  control = c("keepInteger", "showAttributes")),
  collapse = "\n"), 12L, 1e+06L)
     dep <- paste0(prompt.echo,
   gsub("\n", paste0("\n", continue.echo), dep))

such that you get:

> withAutoprint(x <- c(1L, NA_integer_, NA))
> x <- c(1L, NA_integer_, NA)

because without it, you get:

> withAutoprint(x <- c(1L, NA_integer_, NA))
> x <- c(1, NA, NA)

Thanks,

Henrik


>
> Martin
>
>     > Just some thoughts
>     > /Henrik
>
>
>     > On Sat, Sep 24, 2016 at 6:37 AM, Martin Maechler
>     > <maechler at stat.math.ethz.ch> wrote:
>     >>>>>>> William Dunlap <wdunlap at tibco.com>
>     >>>>>>> on Fri, 2 Sep 2016 08:33:47 -0700 writes:
>     >>
>     >> > Re withAutoprint(), Splus's source() function could take a expression
>     >> > (literal or not) in place of a file name or text so it could support
>     >> > withAutoprint-like functionality in its GUI.  E.g.,
>     >>
>     >> >> source(auto.print=TRUE, exprs.literal= { x <- 3:7 ; sum(x) ; y <- log(x)
>     >> > ; x - 100}, prompt="--> ")
>     --> x <- 3:7
>     --> sum(x)
>     >> > [1] 25
>     --> y <- log(x)
>     --> x - 100
>     >> > [1] -97 -96 -95 -94 -93
>     >>
>     >> > or
>     >>
>     >> >> expr <- quote({ x <- 3:7 ; sum(x) ; y <- log(x) ; x - 100})
>     >> >> source(auto.print=TRUE, exprs = expr, prompt="--> ")
>     --> x <- 3:7
>     --> sum(x)
>     >> > [1] 25
>     --> y <- log(x)
>     --> x - 100
>     >> > [1] -97 -96 -95 -94 -93
>     >>
>     >> > It was easy to implement, since exprs's default value is parse(file) or
>     >> > parse(text=text), which source is calculating anyway.
>     >>
>     >>
>     >> > Bill Dunlap
>     >> > TIBCO Software
>     >> > wdunlap tibco.com
>     >>
>     >> Thank you, Bill  (and the other correspondents); that's indeed a
>     >> very good suggestion :
>     >>
>     >> I've come to the conclusion that Duncan and Bill are right:  One
>     >> should do this in R (not C) and as Bill hinted, one should use
>     >> source().  I first tried to do it separately, just "like source()",
>     >> but a considerable part of the source of source()  {:-)} is
>     >> about using src attributes instead of deparse() when the former
>     >> are present,  and it does make sense to generalize
>     >> withAutoprint() to have the same feature, so after all, have it
>     >> call source().
>     >>
>     >> I've spent a few hours now trying things and variants, also
>     >> found I needed to enhance source()  very slightly also in a few
>     >> other details, and now (in my uncommitted version of R-devel),
>     >>
>     >> withAutoprint({ x <- 1:12; x-1; (y <- (x-5)^2); z <- y; z - 10 })
>     >>
>     >> produces
>     >>
>     >>> withAutoprint({ x <- 1:12; x-1; (y <- (x-5)^2); z <- y; z - 10 })
>     >>> x <- 1:12
>     >>> x - 1
>     >> [1]  0  1  2  3  4  5  6  7  8  9 10 11
>     >>> (y <- (x - 5)^2)
>     >> [1] 16  9  4  1  0  1  4  9 16 25 36 49
>     >>> z <- y
>     >>> z - 10
>     >> [1]   6  -1  -6  -9 -10  -9  -6  -1   6  15  26  39
>     >>>
>     >>
>     >> and is equivalent to
>     >>
>     >> withAutoprint(expression(x <- 1:12, x-1, (y <- (x-5)^2), z <- y, z - 10 ))
>     >>
>     >> I don't see any way around the "mis-feature" that all "input"
>     >> expressions are in the end shown twice in the "output" (the
>     >> first time by showing the withAutoprint(...) call itself).
>     >>
>     >> The function *name* is "not bad" but also a bit longish;
>     >> maybe there are better ideas?  (not longer, no "_" - I know this
>     >> is a matter of taste only)
>     >>
>     >> Martin
>     >>
>     >> > On Fri, Sep 2, 2016 at 4:56 AM, Martin Maechler <maechler at stat.math.ethz.ch>
>     >> > wrote:
>     >>
>     >> >> On R-help, with subject
>     >> >> '[R] source() does not include added code'
>     >> >>
>     >> >> >>>>> Joshua Ulrich <josh.m.ulrich at gmail.com>
>     >> >> >>>>>     on Wed, 31 Aug 2016 10:35:01 -0500 writes:
>     >> >>
>     >> >> > I have quantstrat installed and it works fine for me.  If you're
>     >> >> > asking why the output of t(tradeStats('macross')) isn't being
>     >> >> printed,
>     >> >> > that's because of what's described in the first paragraph in the
>     >> >> > *Details* section of help("source"):
>     >> >>
>     >> >> > Note that running code via ?source? differs in a few respects from
>     >> >> > entering it at the R command line.  Since expressions are not
>     >> >> > executed at the top level, auto-printing is not done.  So you will
>     >> >> > need to include explicit ?print? calls for things you want to be
>     >> >> > printed (and remember that this includes plotting by ?lattice?,
>     >> >> > FAQ Q7.22).
>     >> >>
>     >> >>
>     >> >>
>     >> >> > So you need:
>     >> >>
>     >> >> > print(t(tradeStats('macross')))
>     >> >>
>     >> >> > if you want the output printed to the console.
>     >> >>
>     >> >> indeed, and "of course"" ;-)
>     >> >>
>     >> >> As my subject indicates, this is another case, where it would be
>     >> >> very convenient to have a function
>     >> >>
>     >> >> withAutoprint()
>     >> >>
>     >> >> so the OP could have (hopefully) have used
>     >> >> withAutoprint(source(..))
>     >> >> though that would have been equivalent to the already nicely existing
>     >> >>
>     >> >> source(.., print.eval = TRUE)
>     >> >>
>     >> >> which works via the  withVisible(.) utility that returns for each
>     >> >> 'expression' if it would auto print or not, and then does print (or
>     >> >> not) accordingly.
>     >> >>
>     >> >> My own use cases for such a withAutoprint({...})
>     >> >> are demos and examples, sometimes even package tests which I want to print:
>     >> >>
>     >> >> Assume I have a nice demo / example on a help page/ ...
>     >> >>
>     >> >> foo(..)
>     >> >> (z <- bar(..))
>     >> >> summary(z)
>     >> >> ....
>     >> >>
>     >> >> where I carefully do print parts (and don't others),
>     >> >> and suddenly I find I want to run that part of the demo /
>     >> >> example / test only in some circumstances, e.g., only when
>     >> >> interactive, but not in BATCH, or only if it is me, the package maintainer,
>     >> >>
>     >> >> if( identical(Sys.getenv("USER"), "maechler") ) {
>     >> >> foo(..)
>     >> >> (z <- bar(..))
>     >> >> summary(z)
>     >> >> ....
>     >> >> }
>     >> >>
>     >> >> Now all the auto-printing is gone, and
>     >> >>
>     >> >> 1) I have to find out which of these function calls do autoprint and wrap
>     >> >> a print(..) around these, and
>     >> >>
>     >> >> 2) the result is quite ugly (for an example on a help page etc.)
>     >> >>
>     >> >> What I would like in a future R, is to be able to simply wrap the "{
>     >> >> .. } above with an 'withAutoprint(.) :
>     >> >>
>     >> >> if( identical(Sys.getenv("USER"), "maechler") ) withAutoprint({
>     >> >> foo(..)
>     >> >> (z <- bar(..))
>     >> >> summary(z)
>     >> >> ....
>     >> >> })
>     >> >>
>     >> >> Conceptually such a function could be written similar to source() with an R
>     >> >> level for loop, treating each expression separately, calling eval(.) etc.
>     >> >> That may cost too much performnace, ... still to have it would be better
>     >> >> than
>     >> >> not having the possibility.
>     >> >>
>     >> >> ----
>     >> >>
>     >> >> If you read so far, you'd probably agree that such a function
>     >> >> could be a nice asset in R,
>     >> >> notably if it was possible to do this on the fast C level of R's main
>     >> >> REPL.
>     >> >>
>     >> >> Have any of you looked into how this could be provided in R ?
>     >> >> If you know the source a little, you will remember that there's
>     >> >> the global variable  R_Visible  which is crucial here.
>     >> >> The problem with that is that it *is* global, and only available
>     >> >> as that; that the auto-printing "concept" is so linked to "toplevel
>     >> >> context"
>     >> >> and that is not easy, and AFAIK not so much centralized in one place in the
>     >> >> source. Consequently, all kind of (very) low level functions manipulate
>     >> >> R_Visible
>     >> >> temporarily.... and so a C level implementation of  withAutoprint() may
>     >> >> need considerable more changes than just setting R_Visible to TRUE in one
>     >> >> place.
>     >> >>
>     >> >> Have any efforts / experiments already happened towards providing such
>     >> >> functionality ?
>     >> >>
>     >> >> ______________________________________________
>     >> >> R-devel at r-project.org mailing list
>     >> >> https://stat.ethz.ch/mailman/listinfo/r-devel
>     >>
>     >> > [[alternative HTML version deleted]]
>     >>
>     >> ______________________________________________
>     >> R-devel at r-project.org mailing list
>     >> https://stat.ethz.ch/mailman/listinfo/r-devel


From suharto_anggono at yahoo.com  Mon Sep 26 16:51:11 2016
From: suharto_anggono at yahoo.com (Suharto Anggono Suharto Anggono)
Date: Mon, 26 Sep 2016 14:51:11 +0000 (UTC)
Subject: [Rd] Undocumented 'use.names' argument to c()
References: <304919818.4777007.1474901471533.ref@mail.yahoo.com>
Message-ID: <304919818.4777007.1474901471533@mail.yahoo.com>

By "an argument named 'use.names' is included for concatenation", I meant something like this, that someone might try.

> c(as.Date("2016-01-01"), use.names=FALSE)
                use.names 
"2016-01-01" "1970-01-01" 

See, 'use.names' is in the output. That's precisely because 'c.Date' doesn't have 'use.names', so that 'use.names' is absorbed into '...'.
--------------------------------------------
On Sun, 25/9/16, Martin Maechler <maechler at stat.math.ethz.ch> wrote:

 Subject: Re: [Rd] Undocumented 'use.names' argument to c()

 Cc: "R-devel" <R-devel at r-project.org>
 Date: Sunday, 25 September, 2016, 10:14 PM
 
>>>>> Suharto Anggono Suharto Anggono via R-devel <r-devel at r-project.org>
>>>>>     on Sun, 25 Sep 2016 14:12:10 +0000 writes:

    >> From comments in
    >> http://stackoverflow.com/questions/24815572/why-does-function-c-accept-an-undocumented-argument/24815653
    >> : The code of c() and unlist() was formerly shared but
    >> has been (long time passing) separated. From July 30,
    >> 1998, is where do_c got split into do_c and do_unlist.
    > With the implementation of 'c.Date' in R devel r71350, an
    > argument named 'use.names' is included for
    > concatenation. So, it doesn't follow the documented
    > 'c'. But, 'c.Date' is not explicitly documented in
    > Dates.Rd, that has 'c.Date' as an alias.

I do not see any  c.Date  in R-devel with a 'use.names'; its a
base function, hence not hidden ..

As mentioned before, 'use.names' is used in unlist() in quite a
few places, and such an argument also exists for

    lengths()            and
    all.equal.list()

and now c()


From maechler at stat.math.ethz.ch  Mon Sep 26 18:26:25 2016
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 26 Sep 2016 18:26:25 +0200
Subject: [Rd] Undocumented 'use.names' argument to c()
In-Reply-To: <304919818.4777007.1474901471533@mail.yahoo.com>
References: <304919818.4777007.1474901471533.ref@mail.yahoo.com>
	<304919818.4777007.1474901471533@mail.yahoo.com>
Message-ID: <22505.19505.247801.449980@stat.math.ethz.ch>

>>>>> Suharto Anggono Suharto Anggono <suharto_anggono at yahoo.com>
>>>>>     on Mon, 26 Sep 2016 14:51:11 +0000 writes:

    > By "an argument named 'use.names' is included for concatenation", I meant something like this, that someone might try.
    >> c(as.Date("2016-01-01"), use.names=FALSE)
    > use.names 
    > "2016-01-01" "1970-01-01" 

    > See, 'use.names' is in the output. That's precisely because 'c.Date' doesn't have 'use.names', so that 'use.names' is absorbed into '...'.

Yes, of course.
Thank you for the explanation; now I understand what you meant.

Indeed, the situation is not entirely satisfactory:

Ideally, *both* the  'recursive' and 'use.names' arguments of
c() should be considered arguments of only the *default* method of c(),
not the generic.

OTOH, c() being .Primitive() the implementation is in C only,
and (in some sense) of *both* the generic function and the
default method.
The C code clearly treats  'recursive' and 'use.names' "the
same", and has been part of R "forever".

I think that ideally, we should aim for

1) The generic function  c()  only has arguments "..." (or possibly
   ---  because of history of the S4 part ---  "x, ...").

2) The default method has additional arguments
       'recursive = FALSE, use.names = TRUE'
   and other methods of c() can choose if they want to also
   support one or two or none of these extras.

Somewhat related, but in principle independent of '1)'
and '2)' above  -- I think, because of the ".Primitive"-ness of c() --
is the quite how 'c' should print in R.
Currently it prints like what I say should just be the default
method.

Honestly, I'm not sure if it would be straightforward or even
just relatively painless to go to  '1) + 2)' ... may change
r71349 (to the S4 generic definition of "c") had dramatical
effects in "package land" and hence reversion of that (with
r71354) was necessary, for the time being.

Martin


    > --------------------------------------------
    > On Sun, 25/9/16, Martin Maechler <maechler at stat.math.ethz.ch> wrote:

    > Subject: Re: [Rd] Undocumented 'use.names' argument to c()
    > To: "Suharto Anggono Suharto Anggono" <suharto_anggono at yahoo.com>
    > Cc: "R-devel" <R-devel at r-project.org>
    > Date: Sunday, 25 September, 2016, 10:14 PM
 
>>>>> Suharto Anggono Suharto Anggono via R-devel <r-devel at r-project.org>
>>>>>     on Sun, 25 Sep 2016 14:12:10 +0000 writes:

    >>> From comments in
    >>> http://stackoverflow.com/questions/24815572/why-does-function-c-accept-an-undocumented-argument/24815653
    >>> : The code of c() and unlist() was formerly shared but
    >>> has been (long time passing) separated. From July 30,
    >>> 1998, is where do_c got split into do_c and do_unlist.
    >> With the implementation of 'c.Date' in R devel r71350, an
    >> argument named 'use.names' is included for
    >> concatenation. So, it doesn't follow the documented
    >> 'c'. But, 'c.Date' is not explicitly documented in
    >> Dates.Rd, that has 'c.Date' as an alias.

    > I do not see any  c.Date  in R-devel with a 'use.names'; its a
    > base function, hence not hidden ..

    > As mentioned before, 'use.names' is used in unlist() in quite a
    > few places, and such an argument also exists for

    > lengths()            and
    > all.equal.list()

    > and now c()


From ecortens at mtroyal.ca  Mon Sep 26 23:27:02 2016
From: ecortens at mtroyal.ca (Evan Cortens)
Date: Mon, 26 Sep 2016 15:27:02 -0600
Subject: [Rd] Recursive dir.create() on Windows shares
Message-ID: <CABKQe-ZbusW48e0=YFbR562DXFqPaDX7bb6FZFaQpBcF5Hy+pw@mail.gmail.com>

Hi folks,

I've noticed that there's an issue with the recursive creation of
directories that reside on network shares. For example:

>
dir.create('\\\\SERVERNAME\\Empl\\Home1\\active\\e\\ecortens\\thisisatest',
recursive = TRUE)
Warning message:
In
dir.create("\\\\SERVERNAME\\Empl\\Home1\\active\\e\\ecortens\\thisisatest",
 :
  cannot create dir '\\SERVERNAME\Empl', reason 'Permission denied'

The issue is that dir.create() is skipping the server name, but it's not
skipping the share name. So, in the above example, it's trying to create
"Empl", which fails, setting errno to the code for "Permission denied",
instead of EEXIST, the code for "file already exists", because it's not
actually a file, and therefore can't exist as one. (Incidentally, the same
challenge arises with the system call _wstat(), which also will return a
-1, telling you that the share name doesn't exist.)

The solution to this issue, then, is to skip not only the server name, but
the share name as well, which is easily done with one more line of code:

C:\Users\ecortens\Software\R-devel\src>svn diff
Index: main/platform.c
===================================================================
--- main/platform.c     (revision 71366)
+++ main/platform.c     (working copy)
@@ -2159,10 +2159,11 @@
     while (*p == L'\\' && wcslen(dir) > 1 && *(p-1) != L':') *p-- = L'\0';
     if (recursive) {
        p = dir;
-       /* skip leading \\share */
+       /* skip leading \\server\share */
        if (*p == L'\\' && *(p+1) == L'\\') {
            p += 2;
            p = wcschr(p, L'\\');
+           p = wcschr(p+1, L'\\');
        }
        while ((p = wcschr(p+1, L'\\'))) {
            *p = L'\0';

This fixes the issue for me, and I can create directories no problem.
However, I'm a little worried, as the code in platform.c has been this way
since 2008--surely this can't have been a bug since then. Yet I can't find
any indication that the UNC naming convention has changed, and I can't find
any way that will let you test the pathname to see if it's "\\server\share"
or "\\share".

I've also filed this on bugzilla, and have updated it there.  See
https://bugs.r-project.org/bugzilla/show_bug.cgi?id=1715

Thanks for an amazing piece of software!

Best,

Evan

P. S. I'm new to the mailing list, so I apologize in advance if I'm
violating any conventions I'm unaware of.

-- 
Evan Cortens, PhD
Institutional Analyst - Office of Institutional Analysis
Mount Royal University
403-440-6529

	[[alternative HTML version deleted]]


From murdoch.duncan at gmail.com  Mon Sep 26 23:46:19 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Mon, 26 Sep 2016 17:46:19 -0400
Subject: [Rd] Recursive dir.create() on Windows shares
In-Reply-To: <CABKQe-ZbusW48e0=YFbR562DXFqPaDX7bb6FZFaQpBcF5Hy+pw@mail.gmail.com>
References: <CABKQe-ZbusW48e0=YFbR562DXFqPaDX7bb6FZFaQpBcF5Hy+pw@mail.gmail.com>
Message-ID: <95074f7b-8e21-7a98-0637-9973a31dc230@gmail.com>

On 26/09/2016 5:27 PM, Evan Cortens wrote:
> Hi folks,
>
> I've noticed that there's an issue with the recursive creation of
> directories that reside on network shares. For example:
>
>>
> dir.create('\\\\SERVERNAME\\Empl\\Home1\\active\\e\\ecortens\\thisisatest',
> recursive = TRUE)
> Warning message:
> In
> dir.create("\\\\SERVERNAME\\Empl\\Home1\\active\\e\\ecortens\\thisisatest",
>  :
>   cannot create dir '\\SERVERNAME\Empl', reason 'Permission denied'
>
> The issue is that dir.create() is skipping the server name, but it's not
> skipping the share name. So, in the above example, it's trying to create
> "Empl", which fails, setting errno to the code for "Permission denied",
> instead of EEXIST, the code for "file already exists", because it's not
> actually a file, and therefore can't exist as one. (Incidentally, the same
> challenge arises with the system call _wstat(), which also will return a
> -1, telling you that the share name doesn't exist.)
>
> The solution to this issue, then, is to skip not only the server name, but
> the share name as well, which is easily done with one more line of code:
>
> C:\Users\ecortens\Software\R-devel\src>svn diff
> Index: main/platform.c
> ===================================================================
> --- main/platform.c     (revision 71366)
> +++ main/platform.c     (working copy)
> @@ -2159,10 +2159,11 @@
>      while (*p == L'\\' && wcslen(dir) > 1 && *(p-1) != L':') *p-- = L'\0';
>      if (recursive) {
>         p = dir;
> -       /* skip leading \\share */
> +       /* skip leading \\server\share */
>         if (*p == L'\\' && *(p+1) == L'\\') {
>             p += 2;
>             p = wcschr(p, L'\\');
> +           p = wcschr(p+1, L'\\');
>         }
>         while ((p = wcschr(p+1, L'\\'))) {
>             *p = L'\0';
>
> This fixes the issue for me, and I can create directories no problem.
> However, I'm a little worried, as the code in platform.c has been this way
> since 2008--surely this can't have been a bug since then. Yet I can't find
> any indication that the UNC naming convention has changed, and I can't find
> any way that will let you test the pathname to see if it's "\\server\share"
> or "\\share".
>
> I've also filed this on bugzilla, and have updated it there.  See
> https://bugs.r-project.org/bugzilla/show_bug.cgi?id=1715
>
> Thanks for an amazing piece of software!
>
> Best,
>
> Evan
>
> P. S. I'm new to the mailing list, so I apologize in advance if I'm
> violating any conventions I'm unaware of.
>

Presumably someone from Microsoft will respond to this.

Duncan Murdoch


From maechler at stat.math.ethz.ch  Tue Sep 27 11:25:01 2016
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 27 Sep 2016 11:25:01 +0200
Subject: [Rd] withAutoprint({ .... }) ?
In-Reply-To: <CAFDcVCQSyLuza-zEtr6ZvHr0QPdYkjppBCRmge5LNfhRuT0y9w@mail.gmail.com>
References: <ce4bc223-12d1-2618-aa43-cd12c11396eb@gmail.com>
	<CAPPM_gSzW37A9xQn1qrTEqd6jWx-mE-cOq2i7zEqU08THqJn7A@mail.gmail.com>
	<20160902115636.4C6BE1A75DF@lynne.stat.math.ethz.ch>
	<CAF8bMcaOVsUsdSUN0GE+1COsb4hYBRaD7qdFcadY--PXB_bJaQ@mail.gmail.com>
	<22502.33207.568185.181074@stat.math.ethz.ch>
	<CAFDcVCSYsxk5QVqU_6GtWqET=nbqqcMCakhDbx2nDhAhr5C7xg@mail.gmail.com>
	<22503.64353.207088.597099@stat.math.ethz.ch>
	<CAFDcVCQSyLuza-zEtr6ZvHr0QPdYkjppBCRmge5LNfhRuT0y9w@mail.gmail.com>
Message-ID: <22506.15085.94077.528648@stat.math.ethz.ch>

>>>>> Henrik Bengtsson <henrik.bengtsson at gmail.com>
>>>>>     on Sun, 25 Sep 2016 12:38:27 -0700 writes:

    > On Sun, Sep 25, 2016 at 9:29 AM, Martin Maechler
    > <maechler at stat.math.ethz.ch> wrote:
    >>>>>>> Henrik Bengtsson <henrik.bengtsson at gmail.com> on
    >>>>>>> Sat, 24 Sep 2016 11:31:49 -0700 writes:
    >> 
    >> > Martin, did you post your code for withAutoprint()
    >> anywhere?  > Building withAutoprint() on top of source()
    >> definitely makes sense, > unless, as Bill says, source()
    >> itself could provide the same feature.
    >> 
    >> I was really mainly asking for advice about the function
    >> name .. and got none.

    > I missed that part.  I think the name is good.  A shorter
    > alternative would be withEcho(), but could be a little bit
    > misleading since it doesn't reflect 'print=TRUE' to
    > source().

    >> 
    >> I'm now committing my version (including (somewhat incomplete)
    >> documentation, so you (all) can look at it and try / test it further.
    >> 
    >> > To differentiate between withAutoprint({ x <- 1 }) and
    >> > withAutoprint(expr) where is an expression / language object, one
    >> > could have an optional argument `substitute=TRUE`, e.g.
    >> 
    >> > withAutoprint <- function(expr, substitute = TRUE, ...) {
    >> >    if (substitute) expr <- substitute(expr)
    >> >    [...]
    >> > }
    >> 
    >> I think my approach is nicer insofar it does not seem to need
    >> such an argument.... I'm sure you'll try to disprove that ;-)

    > Nah, I like that you've extended source() with the 'exprs' argument.

    > May I suggest to add:

    > svn diff src/library/base/R/
    > Index: src/library/base/R/source.R
    > ===================================================================
    > --- src/library/base/R/source.R (revision 71357)
    > +++ src/library/base/R/source.R (working copy)
    > @@ -198,7 +198,7 @@
    > if (!tail) {
    > # Deparse.  Must drop "expression(...)"
    > dep <- substr(paste(deparse(ei, width.cutoff = width.cutoff,
    > -                                                control = "showAttributes"),
    > +  control = c("keepInteger", "showAttributes")),
    > collapse = "\n"), 12L, 1e+06L)
    > dep <- paste0(prompt.echo,
    > gsub("\n", paste0("\n", continue.echo), dep))

    > such that you get:

    >> withAutoprint(x <- c(1L, NA_integer_, NA))
    >> x <- c(1L, NA_integer_, NA)

    > because without it, you get:

    >> withAutoprint(x <- c(1L, NA_integer_, NA))
    >> x <- c(1, NA, NA)

That's a very good consideration.
However, your change would change the semantics of source(),
not just those of withAutoprint(), and I would not want to do
that ... at least not at the moment. 

What I've done instead, is to make this yet another new
 argument of both source() and withAutoprint(),
called   'deparseCtrl'  and with different defaults (currently)
for the 2 functions. 

Thank you for the feedback!
Martin




    > Thanks,
    > Henrik


    >> 
    >> Martin
    >> 
    >> > Just some thoughts
    >> > /Henrik
    >> 
    >> 
    >> > On Sat, Sep 24, 2016 at 6:37 AM, Martin Maechler
    >> > <maechler at stat.math.ethz.ch> wrote:
    >> >>>>>>> William Dunlap <wdunlap at tibco.com>
    >> >>>>>>> on Fri, 2 Sep 2016 08:33:47 -0700 writes:
    >> >>
    >> >> > Re withAutoprint(), Splus's source() function could take a expression
    >> >> > (literal or not) in place of a file name or text so it could support
    >> >> > withAutoprint-like functionality in its GUI.  E.g.,
    >> >>
    >> >> >> source(auto.print=TRUE, exprs.literal= { x <- 3:7 ; sum(x) ; y <- log(x)
    >> >> > ; x - 100}, prompt="--> ")
    --> x <- 3:7
    --> sum(x)
    >> >> > [1] 25
    --> y <- log(x)
    --> x - 100
    >> >> > [1] -97 -96 -95 -94 -93
    >> >>
    >> >> > or
    >> >>
    >> >> >> expr <- quote({ x <- 3:7 ; sum(x) ; y <- log(x) ; x - 100})
    >> >> >> source(auto.print=TRUE, exprs = expr, prompt="--> ")
    --> x <- 3:7
    --> sum(x)
    >> >> > [1] 25
    --> y <- log(x)
    --> x - 100
    >> >> > [1] -97 -96 -95 -94 -93
    >> >>
    >> >> > It was easy to implement, since exprs's default value is parse(file) or
    >> >> > parse(text=text), which source is calculating anyway.
    >> >>
    >> >>
    >> >> > Bill Dunlap
    >> >> > TIBCO Software
    >> >> > wdunlap tibco.com
    >> >>
    >> >> Thank you, Bill  (and the other correspondents); that's indeed a
    >> >> very good suggestion :
    >> >>
    >> >> I've come to the conclusion that Duncan and Bill are right:  One
    >> >> should do this in R (not C) and as Bill hinted, one should use
    >> >> source().  I first tried to do it separately, just "like source()",
    >> >> but a considerable part of the source of source()  {:-)} is
    >> >> about using src attributes instead of deparse() when the former
    >> >> are present,  and it does make sense to generalize
    >> >> withAutoprint() to have the same feature, so after all, have it
    >> >> call source().
    >> >>
    >> >> I've spent a few hours now trying things and variants, also
    >> >> found I needed to enhance source()  very slightly also in a few
    >> >> other details, and now (in my uncommitted version of R-devel),
    >> >>
    >> >> withAutoprint({ x <- 1:12; x-1; (y <- (x-5)^2); z <- y; z - 10 })
    >> >>
    >> >> produces
    >> >>
    >> >>> withAutoprint({ x <- 1:12; x-1; (y <- (x-5)^2); z <- y; z - 10 })
    >> >>> x <- 1:12
    >> >>> x - 1
    >> >> [1]  0  1  2  3  4  5  6  7  8  9 10 11
    >> >>> (y <- (x - 5)^2)
    >> >> [1] 16  9  4  1  0  1  4  9 16 25 36 49
    >> >>> z <- y
    >> >>> z - 10
    >> >> [1]   6  -1  -6  -9 -10  -9  -6  -1   6  15  26  39
    >> >>>
    >> >>
    >> >> and is equivalent to
    >> >>
    >> >> withAutoprint(expression(x <- 1:12, x-1, (y <- (x-5)^2), z <- y, z - 10 ))
    >> >>
    >> >> I don't see any way around the "mis-feature" that all "input"
    >> >> expressions are in the end shown twice in the "output" (the
    >> >> first time by showing the withAutoprint(...) call itself).
    >> >>
    >> >> The function *name* is "not bad" but also a bit longish;
    >> >> maybe there are better ideas?  (not longer, no "_" - I know this
    >> >> is a matter of taste only)
    >> >>
    >> >> Martin
    >> >>
    >> >> > On Fri, Sep 2, 2016 at 4:56 AM, Martin Maechler <maechler at stat.math.ethz.ch>
    >> >> > wrote:
    >> >>
    >> >> >> On R-help, with subject
    >> >> >> '[R] source() does not include added code'
    >> >> >>
    >> >> >> >>>>> Joshua Ulrich <josh.m.ulrich at gmail.com>
    >> >> >> >>>>>     on Wed, 31 Aug 2016 10:35:01 -0500 writes:
    >> >> >>
    >> >> >> > I have quantstrat installed and it works fine for me.  If you're
    >> >> >> > asking why the output of t(tradeStats('macross')) isn't being
    >> >> >> printed,
    >> >> >> > that's because of what's described in the first paragraph in the
    >> >> >> > *Details* section of help("source"):
    >> >> >>
    >> >> >> > Note that running code via ?source? differs in a few respects from
    >> >> >> > entering it at the R command line.  Since expressions are not
    >> >> >> > executed at the top level, auto-printing is not done.  So you will
    >> >> >> > need to include explicit ?print? calls for things you want to be
    >> >> >> > printed (and remember that this includes plotting by ?lattice?,
    >> >> >> > FAQ Q7.22).
    >> >> >>
    >> >> >>
    >> >> >>
    >> >> >> > So you need:
    >> >> >>
    >> >> >> > print(t(tradeStats('macross')))
    >> >> >>
    >> >> >> > if you want the output printed to the console.
    >> >> >>
    >> >> >> indeed, and "of course"" ;-)
    >> >> >>
    >> >> >> As my subject indicates, this is another case, where it would be
    >> >> >> very convenient to have a function
    >> >> >>
    >> >> >> withAutoprint()
    >> >> >>
    >> >> >> so the OP could have (hopefully) have used
    >> >> >> withAutoprint(source(..))
    >> >> >> though that would have been equivalent to the already nicely existing
    >> >> >>
    >> >> >> source(.., print.eval = TRUE)
    >> >> >>
    >> >> >> which works via the  withVisible(.) utility that returns for each
    >> >> >> 'expression' if it would auto print or not, and then does print (or
    >> >> >> not) accordingly.
    >> >> >>
    >> >> >> My own use cases for such a withAutoprint({...})
    >> >> >> are demos and examples, sometimes even package tests which I want to print:
    >> >> >>
    >> >> >> Assume I have a nice demo / example on a help page/ ...
    >> >> >>
    >> >> >> foo(..)
    >> >> >> (z <- bar(..))
    >> >> >> summary(z)
    >> >> >> ....
    >> >> >>
    >> >> >> where I carefully do print parts (and don't others),
    >> >> >> and suddenly I find I want to run that part of the demo /
    >> >> >> example / test only in some circumstances, e.g., only when
    >> >> >> interactive, but not in BATCH, or only if it is me, the package maintainer,
    >> >> >>
    >> >> >> if( identical(Sys.getenv("USER"), "maechler") ) {
    >> >> >> foo(..)
    >> >> >> (z <- bar(..))
    >> >> >> summary(z)
    >> >> >> ....
    >> >> >> }
    >> >> >>
    >> >> >> Now all the auto-printing is gone, and
    >> >> >>
    >> >> >> 1) I have to find out which of these function calls do autoprint and wrap
    >> >> >> a print(..) around these, and
    >> >> >>
    >> >> >> 2) the result is quite ugly (for an example on a help page etc.)
    >> >> >>
    >> >> >> What I would like in a future R, is to be able to simply wrap the "{
    >> >> >> .. } above with an 'withAutoprint(.) :
    >> >> >>
    >> >> >> if( identical(Sys.getenv("USER"), "maechler") ) withAutoprint({
    >> >> >> foo(..)
    >> >> >> (z <- bar(..))
    >> >> >> summary(z)
    >> >> >> ....
    >> >> >> })
    >> >> >>
    >> >> >> Conceptually such a function could be written similar to source() with an R
    >> >> >> level for loop, treating each expression separately, calling eval(.) etc.
    >> >> >> That may cost too much performnace, ... still to have it would be better
    >> >> >> than
    >> >> >> not having the possibility.
    >> >> >>
    >> >> >> ----
    >> >> >>
    >> >> >> If you read so far, you'd probably agree that such a function
    >> >> >> could be a nice asset in R,
    >> >> >> notably if it was possible to do this on the fast C level of R's main
    >> >> >> REPL.
    >> >> >>
    >> >> >> Have any of you looked into how this could be provided in R ?
    >> >> >> If you know the source a little, you will remember that there's
    >> >> >> the global variable  R_Visible  which is crucial here.
    >> >> >> The problem with that is that it *is* global, and only available
    >> >> >> as that; that the auto-printing "concept" is so linked to "toplevel
    >> >> >> context"
    >> >> >> and that is not easy, and AFAIK not so much centralized in one place in the
    >> >> >> source. Consequently, all kind of (very) low level functions manipulate
    >> >> >> R_Visible
    >> >> >> temporarily.... and so a C level implementation of  withAutoprint() may
    >> >> >> need considerable more changes than just setting R_Visible to TRUE in one
    >> >> >> place.
    >> >> >>
    >> >> >> Have any efforts / experiments already happened towards providing such
    >> >> >> functionality ?
    >> >> >>
    >> >> >> ______________________________________________
    >> >> >> R-devel at r-project.org mailing list
    >> >> >> https://stat.ethz.ch/mailman/listinfo/r-devel
    >> >>
    >> >> > [[alternative HTML version deleted]]
    >> >>
    >> >> ______________________________________________
    >> >> R-devel at r-project.org mailing list
    >> >> https://stat.ethz.ch/mailman/listinfo/r-devel


From edeveaud at pasteur.fr  Tue Sep 27 09:37:12 2016
From: edeveaud at pasteur.fr (Eric Deveaud)
Date: Tue, 27 Sep 2016 09:37:12 +0200
Subject: [Rd] src/Makevars ignored ?
Message-ID: <5abedec3-552a-b141-a9e2-ef74e212b893@pasteur.fr>

     Hello,

I'm tring to install a Rpackage that holds some C//C++ code

as far as I understood the R library generic compilation mechanism, 
compilation of C//C++ sources is controled

1) at system level by the ocntentos RHOME/etc/Makeconf
2) at user level by the content of ~/.R/Makevars
3) at package level by the content of src/Makevars

Problem I have is that src/Makevars is ignored


see following example:

R is compiled and use the following CC and CFLAGS definition

bigmess:epactsR/src > R CMD config CC
gcc -std=gnu99
bigmess:epactsR/src > R CMD config CFLAGS
-Wall -g

so building C sources lead to the following

bigmess:epactsR/src > R CMD SHLIB index.c
gcc -std=gnu99 -I/local/gensoft2/adm/lib64/R/include -DNDEBUG 
-I/usr/local/include    -fpic  -Wall -g  -c index.c -o index.o

normal, it uses defintion from RHOME/etc/Makeconf


when I set upp a ~/.R/Makevars that overwrite CC and CFLAGS definition.

bigmess:epactsR/src > cat ~/.R/Makevars
CC=gcc
CFLAGS=-O3
bigmess:epactsR/src > R CMD SHLIB index.c
gcc -I/local/gensoft2/adm/lib64/R/include -DNDEBUG  -I/usr/local/include 
    -fpic  -O3 -c index.c -o index.o
gcc -std=gnu99 -shared -L/usr/local/lib64 -o index.so index.o


OK CC and CFLAGS are honored and set accordingly to ~/.R/Makevars


but when I try to use src/Makevars, it is ignored

bigmess:epactsR/src > cat ~/.R/Makevars
cat: /home/edeveaud/.R/Makevars: No such file or directory
bigmess:epactsR/src > cat ./Makevars
CC = gcc
CFLAGS=-O3
bigmess:epactsR/src > R CMD SHLIB index.c
gcc -std=gnu99 -I/local/gensoft2/adm/lib64/R/include -DNDEBUG 
-I/usr/local/include    -fpic  -Wall -g  -c index.c -o index.o


is there something I have missed, misunderstood or is there something 
wrong ?


PS I tested the ssame behaviour with various version of R from R/2.15 to 
R/3.3

     best regards

     Eric


From mikko.korpela at helsinki.fi  Tue Sep 27 11:49:09 2016
From: mikko.korpela at helsinki.fi (Mikko Korpela)
Date: Tue, 27 Sep 2016 12:49:09 +0300
Subject: [Rd] library() asks user to accept license of some built-in packages
Message-ID: <1733ce04-9659-157d-fb2b-85fb71cf38f4@helsinki.fi>

When 'getOption("checkPackageLicense")' is 'TRUE' and the user calls 
'library(grid)' for the first time, R asks the user to either accept or 
decline the package license. This should not be necessary as the package 
license is "Part of R ..." with "..." denoting the R version number, and 
R is free and open source.

The unnecessary license question is asked for the built-in packages 
"compiler", "grid" and "parallel".

The source file where the checks happen is 
"src/library/base/R/library.R". I think one solution could be to add 
something like

if(identical(pkgInfo$DESCRIPTION[["Priority"]], "base")) return()

to the beginning of checkLicense(), or add more packages to the 
hard-coded exemption list checked before calling checkLicense().

Also, in find.package(), the shortcut list of standard packages is 
missing "compiler".

-- 
Mikko Korpela
Department of Geosciences and Geography
University of Helsinki


From kirill.mueller at ivt.baug.ethz.ch  Tue Sep 27 13:27:20 2016
From: kirill.mueller at ivt.baug.ethz.ch (=?UTF-8?Q?Kirill_M=c3=bcller?=)
Date: Tue, 27 Sep 2016 13:27:20 +0200
Subject: [Rd] withAutoprint({ .... }) ?
In-Reply-To: <22503.64353.207088.597099@stat.math.ethz.ch>
References: <ce4bc223-12d1-2618-aa43-cd12c11396eb@gmail.com>
	<CAPPM_gSzW37A9xQn1qrTEqd6jWx-mE-cOq2i7zEqU08THqJn7A@mail.gmail.com>
	<20160902115636.4C6BE1A75DF@lynne.stat.math.ethz.ch>
	<CAF8bMcaOVsUsdSUN0GE+1COsb4hYBRaD7qdFcadY--PXB_bJaQ@mail.gmail.com>
	<22502.33207.568185.181074@stat.math.ethz.ch>
	<CAFDcVCSYsxk5QVqU_6GtWqET=nbqqcMCakhDbx2nDhAhr5C7xg@mail.gmail.com>
	<22503.64353.207088.597099@stat.math.ethz.ch>
Message-ID: <9f2453df-715a-5cad-1c8e-161eb28dc6d9@ivt.baug.ethz.ch>

On 25.09.2016 18:29, Martin Maechler wrote:
> I'm now committing my version (including (somewhat incomplete)
> documentation, so you (all) can look at it and try / test it further.
Thanks, that's awesome. Is `withAutoprint()` recursive? How about 
calling the new function in `example()` (instead of `source()` as it is 
now) so that examples are always rendered in auto-print mode? That may 
add some extra output to examples (which can be removed easily), but 
solve the original problem in a painless way.


-Kirill


From edd at debian.org  Tue Sep 27 13:31:24 2016
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue, 27 Sep 2016 06:31:24 -0500
Subject: [Rd] src/Makevars ignored ?
In-Reply-To: <5abedec3-552a-b141-a9e2-ef74e212b893@pasteur.fr>
References: <5abedec3-552a-b141-a9e2-ef74e212b893@pasteur.fr>
Message-ID: <22506.22668.727653.313373@max.nulle.part>


On 27 September 2016 at 09:37, Eric Deveaud wrote:
|      Hello,
| 
| I'm tring to install a Rpackage that holds some C//C++ code
| 
| as far as I understood the R library generic compilation mechanism, 
| compilation of C//C++ sources is controled
| 
| 1) at system level by the ocntentos RHOME/etc/Makeconf
| 2) at user level by the content of ~/.R/Makevars
| 3) at package level by the content of src/Makevars
| 
| Problem I have is that src/Makevars is ignored
| 
| 
| see following example:
| 
| R is compiled and use the following CC and CFLAGS definition
| 
| bigmess:epactsR/src > R CMD config CC
| gcc -std=gnu99
| bigmess:epactsR/src > R CMD config CFLAGS
| -Wall -g
| 
| so building C sources lead to the following
| 
| bigmess:epactsR/src > R CMD SHLIB index.c
| gcc -std=gnu99 -I/local/gensoft2/adm/lib64/R/include -DNDEBUG 
| -I/usr/local/include    -fpic  -Wall -g  -c index.c -o index.o
| 
| normal, it uses defintion from RHOME/etc/Makeconf
| 
| 
| when I set upp a ~/.R/Makevars that overwrite CC and CFLAGS definition.
| 
| bigmess:epactsR/src > cat ~/.R/Makevars
| CC=gcc
| CFLAGS=-O3
| bigmess:epactsR/src > R CMD SHLIB index.c
| gcc -I/local/gensoft2/adm/lib64/R/include -DNDEBUG  -I/usr/local/include 
|     -fpic  -O3 -c index.c -o index.o
| gcc -std=gnu99 -shared -L/usr/local/lib64 -o index.so index.o
| 
| 
| OK CC and CFLAGS are honored and set accordingly to ~/.R/Makevars
| 
| 
| but when I try to use src/Makevars, it is ignored
| 
| bigmess:epactsR/src > cat ~/.R/Makevars
| cat: /home/edeveaud/.R/Makevars: No such file or directory
| bigmess:epactsR/src > cat ./Makevars
| CC = gcc
| CFLAGS=-O3
| bigmess:epactsR/src > R CMD SHLIB index.c
| gcc -std=gnu99 -I/local/gensoft2/adm/lib64/R/include -DNDEBUG 
| -I/usr/local/include    -fpic  -Wall -g  -c index.c -o index.o
| 
| 
| is there something I have missed, misunderstood or is there something 
| wrong ?

You have not demonstrated that src/Makevars is ignored -- as it clearly
isn't, given how thousands of CRAN packages use it.

What you have done is demonstrate that you _cannot change CC and CXX_ in
src/Makevars.  And I think that was known, though maybe not as widely.

Dirk

| PS I tested the ssame behaviour with various version of R from R/2.15 to 
| R/3.3
| 
|      best regards
| 
|      Eric
| 
| ______________________________________________
| R-devel at r-project.org mailing list
| https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
http://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From edeveaud at pasteur.fr  Tue Sep 27 15:23:57 2016
From: edeveaud at pasteur.fr (Eric Deveaud)
Date: Tue, 27 Sep 2016 15:23:57 +0200
Subject: [Rd] src/Makevars ignored ?
In-Reply-To: <22506.22668.727653.313373@max.nulle.part>
References: <5abedec3-552a-b141-a9e2-ef74e212b893@pasteur.fr>
	<22506.22668.727653.313373@max.nulle.part>
Message-ID: <b11e2a62-95bd-0d2f-66ba-2d6177f62bb9@pasteur.fr>

Le 27/09/16 ? 13:31, Dirk Eddelbuettel a ?crit :
>
> On 27 September 2016 at 09:37, Eric Deveaud wrote:
> |      Hello,
> |
> | I'm tring to install a Rpackage that holds some C//C++ code
> |
> | as far as I understood the R library generic compilation mechanism,
> | compilation of C//C++ sources is controled
> |
> | 1) at system level by the ocntentos RHOME/etc/Makeconf
> | 2) at user level by the content of ~/.R/Makevars
> | 3) at package level by the content of src/Makevars
> |
> | Problem I have is that src/Makevars is ignored
> |
> |
> | see following example:
> |
> | R is compiled and use the following CC and CFLAGS definition
> |
> | bigmess:epactsR/src > R CMD config CC
> | gcc -std=gnu99
> | bigmess:epactsR/src > R CMD config CFLAGS
> | -Wall -g
> |
> | so building C sources lead to the following
> |
> | bigmess:epactsR/src > R CMD SHLIB index.c
> | gcc -std=gnu99 -I/local/gensoft2/adm/lib64/R/include -DNDEBUG
> | -I/usr/local/include    -fpic  -Wall -g  -c index.c -o index.o
> |
> | normal, it uses defintion from RHOME/etc/Makeconf
> |
> |
> | when I set upp a ~/.R/Makevars that overwrite CC and CFLAGS definition.
> |
> | bigmess:epactsR/src > cat ~/.R/Makevars
> | CC=gcc
> | CFLAGS=-O3
> | bigmess:epactsR/src > R CMD SHLIB index.c
> | gcc -I/local/gensoft2/adm/lib64/R/include -DNDEBUG  -I/usr/local/include
> |     -fpic  -O3 -c index.c -o index.o
> | gcc -std=gnu99 -shared -L/usr/local/lib64 -o index.so index.o
> |
> |
> | OK CC and CFLAGS are honored and set accordingly to ~/.R/Makevars
> |
> |
> | but when I try to use src/Makevars, it is ignored
> |
> | bigmess:epactsR/src > cat ~/.R/Makevars
> | cat: /home/edeveaud/.R/Makevars: No such file or directory
> | bigmess:epactsR/src > cat ./Makevars
> | CC = gcc
> | CFLAGS=-O3
> | bigmess:epactsR/src > R CMD SHLIB index.c
> | gcc -std=gnu99 -I/local/gensoft2/adm/lib64/R/include -DNDEBUG
> | -I/usr/local/include    -fpic  -Wall -g  -c index.c -o index.o
> |
> |
> | is there something I have missed, misunderstood or is there something
> | wrong ?
>
> You have not demonstrated that src/Makevars is ignored -- as it clearly
> isn't, given how thousands of CRAN packages use it.
>
> What you have done is demonstrate that you _cannot change CC and CXX_ in
> src/Makevars.  And I think that was known, though maybe not as widely.


     Hello Dirk,


so why ~/R/Makevars allows to change CC and not src/Makevars ?

this is pretty confusing.

in unix world mechanism of overwriting system default in user and 
package order is something standadr and consistent given the level

priority is package specific stuff overwrite user configuration that 
overwrite system default

mechanism of Makevars is NOT consistent


furthermore diggng in $RHOME/bin/config one can see the following


if test "${site}" = "yes"; then
: ${R_MAKEVARS_SITE="${R_HOME}/etc${R_ARCH}/Makevars.site"}
   if test -f "${R_MAKEVARS_SITE}"; then
     makefiles="${makefiles} -f ${R_MAKEVARS_SITE}"
   fi
fi
if test "${personal}" = "yes"; then
   if test "${R_OSTYPE}" = "windows"; then
     if test -f "${R_MAKEVARS_USER}"; then
       makefiles="${makefiles} -f ${R_MAKEVARS_USER}"
     elif test ${R_ARCH} = "/x64" -a -f "${HOME}/.R/Makevars.win64"; then
       makefiles="${makefiles} -f ${HOME}/.R/Makevars.win64"
     elif test -f "${HOME}/.R/Makevars.win"; then
       makefiles="${makefiles} -f ${HOME}/.R/Makevars.win"
     elif test -f "${HOME}/.R/Makevars"; then
       makefiles="${makefiles} -f ${HOME}/.R/Makevars"
     fi
   else
     . ${R_HOME}/etc${R_ARCH}/Renviron
     if test -f "${R_MAKEVARS_USER}"; then
       makefiles="${makefiles} -f ${R_MAKEVARS_USER}"
     elif test -f "${HOME}/.R/Makevars-${R_PLATFORM}"; then
       makefiles="${makefiles} -f ${HOME}/.R/Makevars-${R_PLATFORM}"
     elif test -f "${HOME}/.R/Makevars"; then
       makefiles="${makefiles} -f ${HOME}/.R/Makevars"
     fi
   fi
fi

it checks and honours R_MAKEVARS_USER, ~/.R/Makevars, but not src/Makevars
I'm not really familiar with the R policy about code comilation, but I 
consider this a bug but maybee I'm wrong


as a side note I solved the problem of overwritting CC for this 
particular package using MAKEFLAGS="CC=gcc" R CMD INSTALL package


     best regards

     Eric


From kasperdanielhansen at gmail.com  Tue Sep 27 16:17:29 2016
From: kasperdanielhansen at gmail.com (Kasper Daniel Hansen)
Date: Tue, 27 Sep 2016 10:17:29 -0400
Subject: [Rd] src/Makevars ignored ?
In-Reply-To: <b11e2a62-95bd-0d2f-66ba-2d6177f62bb9@pasteur.fr>
References: <5abedec3-552a-b141-a9e2-ef74e212b893@pasteur.fr>
	<22506.22668.727653.313373@max.nulle.part>
	<b11e2a62-95bd-0d2f-66ba-2d6177f62bb9@pasteur.fr>
Message-ID: <CAC2h7usqc7JQxkJG72k-NA4dqT+MR0BB_Qm4GBkROSVHux+6Yw@mail.gmail.com>

As a package author, it is in my opinion irresponsible to override these
system settings (which is why it is also impossible).  You have no idea
what system it is being deployed on, I mean, you don't even know if the
compiler is gcc. If a user wants (say) heavy optimization they will compile
R with optimization.  (For this reason I also don't think users should
modify their ~/.R/Makevars, unless for testing purposes).  The exception is
(in my opinion) if you need to decrease the level of optimization because
you know or suspect the compiler to generate wrong code.

What you should do, is use
  PKG_CFLAGS
as documented in R-exts, 1.2.1 under "Using Makevars".

Best,
Kasper


On Tue, Sep 27, 2016 at 9:23 AM, Eric Deveaud <edeveaud at pasteur.fr> wrote:

> Le 27/09/16 ? 13:31, Dirk Eddelbuettel a ?crit :
>
>
>> On 27 September 2016 at 09:37, Eric Deveaud wrote:
>> |      Hello,
>> |
>> | I'm tring to install a Rpackage that holds some C//C++ code
>> |
>> | as far as I understood the R library generic compilation mechanism,
>> | compilation of C//C++ sources is controled
>> |
>> | 1) at system level by the ocntentos RHOME/etc/Makeconf
>> | 2) at user level by the content of ~/.R/Makevars
>> | 3) at package level by the content of src/Makevars
>> |
>> | Problem I have is that src/Makevars is ignored
>> |
>> |
>> | see following example:
>> |
>> | R is compiled and use the following CC and CFLAGS definition
>> |
>> | bigmess:epactsR/src > R CMD config CC
>> | gcc -std=gnu99
>> | bigmess:epactsR/src > R CMD config CFLAGS
>> | -Wall -g
>> |
>> | so building C sources lead to the following
>> |
>> | bigmess:epactsR/src > R CMD SHLIB index.c
>> | gcc -std=gnu99 -I/local/gensoft2/adm/lib64/R/include -DNDEBUG
>> | -I/usr/local/include    -fpic  -Wall -g  -c index.c -o index.o
>> |
>> | normal, it uses defintion from RHOME/etc/Makeconf
>> |
>> |
>> | when I set upp a ~/.R/Makevars that overwrite CC and CFLAGS definition.
>> |
>> | bigmess:epactsR/src > cat ~/.R/Makevars
>> | CC=gcc
>> | CFLAGS=-O3
>> | bigmess:epactsR/src > R CMD SHLIB index.c
>> | gcc -I/local/gensoft2/adm/lib64/R/include -DNDEBUG
>> -I/usr/local/include
>> |     -fpic  -O3 -c index.c -o index.o
>> | gcc -std=gnu99 -shared -L/usr/local/lib64 -o index.so index.o
>> |
>> |
>> | OK CC and CFLAGS are honored and set accordingly to ~/.R/Makevars
>> |
>> |
>> | but when I try to use src/Makevars, it is ignored
>> |
>> | bigmess:epactsR/src > cat ~/.R/Makevars
>> | cat: /home/edeveaud/.R/Makevars: No such file or directory
>> | bigmess:epactsR/src > cat ./Makevars
>> | CC = gcc
>> | CFLAGS=-O3
>> | bigmess:epactsR/src > R CMD SHLIB index.c
>> | gcc -std=gnu99 -I/local/gensoft2/adm/lib64/R/include -DNDEBUG
>> | -I/usr/local/include    -fpic  -Wall -g  -c index.c -o index.o
>> |
>> |
>> | is there something I have missed, misunderstood or is there something
>> | wrong ?
>>
>> You have not demonstrated that src/Makevars is ignored -- as it clearly
>> isn't, given how thousands of CRAN packages use it.
>>
>> What you have done is demonstrate that you _cannot change CC and CXX_ in
>> src/Makevars.  And I think that was known, though maybe not as widely.
>>
>
>
>     Hello Dirk,
>
>
> so why ~/R/Makevars allows to change CC and not src/Makevars ?
>
> this is pretty confusing.
>
> in unix world mechanism of overwriting system default in user and package
> order is something standadr and consistent given the level
>
> priority is package specific stuff overwrite user configuration that
> overwrite system default
>
> mechanism of Makevars is NOT consistent
>
>
> furthermore diggng in $RHOME/bin/config one can see the following
>
>
> if test "${site}" = "yes"; then
> : ${R_MAKEVARS_SITE="${R_HOME}/etc${R_ARCH}/Makevars.site"}
>   if test -f "${R_MAKEVARS_SITE}"; then
>     makefiles="${makefiles} -f ${R_MAKEVARS_SITE}"
>   fi
> fi
> if test "${personal}" = "yes"; then
>   if test "${R_OSTYPE}" = "windows"; then
>     if test -f "${R_MAKEVARS_USER}"; then
>       makefiles="${makefiles} -f ${R_MAKEVARS_USER}"
>     elif test ${R_ARCH} = "/x64" -a -f "${HOME}/.R/Makevars.win64"; then
>       makefiles="${makefiles} -f ${HOME}/.R/Makevars.win64"
>     elif test -f "${HOME}/.R/Makevars.win"; then
>       makefiles="${makefiles} -f ${HOME}/.R/Makevars.win"
>     elif test -f "${HOME}/.R/Makevars"; then
>       makefiles="${makefiles} -f ${HOME}/.R/Makevars"
>     fi
>   else
>     . ${R_HOME}/etc${R_ARCH}/Renviron
>     if test -f "${R_MAKEVARS_USER}"; then
>       makefiles="${makefiles} -f ${R_MAKEVARS_USER}"
>     elif test -f "${HOME}/.R/Makevars-${R_PLATFORM}"; then
>       makefiles="${makefiles} -f ${HOME}/.R/Makevars-${R_PLATFORM}"
>     elif test -f "${HOME}/.R/Makevars"; then
>       makefiles="${makefiles} -f ${HOME}/.R/Makevars"
>     fi
>   fi
> fi
>
> it checks and honours R_MAKEVARS_USER, ~/.R/Makevars, but not src/Makevars
> I'm not really familiar with the R policy about code comilation, but I
> consider this a bug but maybee I'm wrong
>
>
> as a side note I solved the problem of overwritting CC for this particular
> package using MAKEFLAGS="CC=gcc" R CMD INSTALL package
>
>
>
>     best regards
>
>     Eric
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From edd at debian.org  Tue Sep 27 16:30:09 2016
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue, 27 Sep 2016 09:30:09 -0500
Subject: [Rd] src/Makevars ignored ?
In-Reply-To: <b11e2a62-95bd-0d2f-66ba-2d6177f62bb9@pasteur.fr>
References: <5abedec3-552a-b141-a9e2-ef74e212b893@pasteur.fr>
	<22506.22668.727653.313373@max.nulle.part>
	<b11e2a62-95bd-0d2f-66ba-2d6177f62bb9@pasteur.fr>
Message-ID: <22506.33393.332368.471733@max.nulle.part>


On 27 September 2016 at 15:23, Eric Deveaud wrote:
| so why ~/R/Makevars allows to change CC and not src/Makevars ?
| 
| this is pretty confusing.

It seems weird at first, but makes some sense when you think about it like
this:

  -- src/Makevars is inside a package and cannot / should not alter "system"
     parameters like compiler brand or version as there may only be one
     compiler, the one R was built with

     it does allow however to set compilation flags, language standards,
     include directories etc as needed to build the package

  -- (user- or system-level) Makeconf allow compiler changes, but that is
     orthogonal to the per-package config and stays local to the machine
 
| in unix world mechanism of overwriting system default in user and 
| package order is something standadr and consistent given the level
| 
| priority is package specific stuff overwrite user configuration that 
| overwrite system default
| 
| mechanism of Makevars is NOT consistent

Yes we sometimes wish we could override system Makeconf settings in a
package, but we can't.

| as a side note I solved the problem of overwritting CC for this 
| particular package using MAKEFLAGS="CC=gcc" R CMD INSTALL package

That works and is documented, but is not "portable" to other machines.

Hope this helps,  Dirk

-- 
http://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From edeveaud at pasteur.fr  Tue Sep 27 16:52:21 2016
From: edeveaud at pasteur.fr (Eric Deveaud)
Date: Tue, 27 Sep 2016 16:52:21 +0200
Subject: [Rd] src/Makevars ignored ?
In-Reply-To: <CAC2h7usqc7JQxkJG72k-NA4dqT+MR0BB_Qm4GBkROSVHux+6Yw@mail.gmail.com>
References: <5abedec3-552a-b141-a9e2-ef74e212b893@pasteur.fr>
	<22506.22668.727653.313373@max.nulle.part>
	<b11e2a62-95bd-0d2f-66ba-2d6177f62bb9@pasteur.fr>
	<CAC2h7usqc7JQxkJG72k-NA4dqT+MR0BB_Qm4GBkROSVHux+6Yw@mail.gmail.com>
Message-ID: <2af1ebe3-2a41-7b02-c066-b0571789a5a3@pasteur.fr>

Le 27/09/16 ? 16:17, Kasper Daniel Hansen a ?crit :
> As a package author, it is in my opinion irresponsible to override these
> system settings (which is why it is also impossible).  You have no idea
> what system it is being deployed on,

as the it guy dedicated to install and maintain softs on our cluster I 
have a reasonable knowledge of the systems I work on.

I don't want to distribute any of the piece of code I was asked to made 
available on the cluster.
I just need (and succeded plus functional test succeded) to build the 
Rpackage requested by a specific software.


> I mean, you don't even know if the
> compiler is gcc.If a user wants (say) heavy optimization they will
> compile R with optimization.  (For this reason I also don't think users
> should modify their ~/.R/Makevars, unless for testing purposes).

my question was not in a R package developer context, but in the R user 
that grabs some piece of code and is not abble to compile it because of
1) a developper that mixed C and C++ code which is legit.
2) a silly interaction beetween C and C++ symbol generation because of 
the use, in our case, of CC = gcc -std=gnu99
3) a dev that answwer: "I have no clue, in debian it works" ;-(

anyway I still convinced that if R provides a mechanisn hierachical way 
of variable overwrite pkg / user/ system it _SHOULD_ be consistent at 
all levels

my question was raised because of our install mechanism that (hopefully) 
does not allow
modification of files like ~/.R/Makevars.
I can only "play" with the sources of the software it is working on 
and//or environment variables. so I wanted  to have a temporary way of 
setting CC to be plain gcc without ISO C99 language standard support 
just for this specific R library.


> The exception is (in my opinion) if you need to decrease the level of
> optimization because you know or suspect the compiler to generate wrong
> code.
>
> What you should do, is use
>   PKG_CFLAGS
> as documented in R-exts, 1.2.1 under "Using Makevars".

in the documentation you pointed (and trust me I read it), keyword is 
_set additional_  preprocessor options and//or compiler flags

only way to _remove_ is to overwrite

back to logic.

either Makevars, whatever level, allow to overwrite CC definition
either Makevars, whatever level, disable CC redefinition

but not a mix

	Eric


From edeveaud at pasteur.fr  Tue Sep 27 16:57:35 2016
From: edeveaud at pasteur.fr (Eric Deveaud)
Date: Tue, 27 Sep 2016 16:57:35 +0200
Subject: [Rd] src/Makevars ignored ?
In-Reply-To: <22506.33393.332368.471733@max.nulle.part>
References: <5abedec3-552a-b141-a9e2-ef74e212b893@pasteur.fr>
	<22506.22668.727653.313373@max.nulle.part>
	<b11e2a62-95bd-0d2f-66ba-2d6177f62bb9@pasteur.fr>
	<22506.33393.332368.471733@max.nulle.part>
Message-ID: <3e6766ed-2b10-397c-d9ee-f2d81b87d107@pasteur.fr>

Le 27/09/16 ? 16:30, Dirk Eddelbuettel a ?crit :
>
> On 27 September 2016 at 15:23, Eric Deveaud wrote:
> | so why ~/R/Makevars allows to change CC and not src/Makevars ?
> |
> | this is pretty confusing.
>
> It seems weird at first, but makes some sense when you think about it like
> this:
>
>   -- src/Makevars is inside a package and cannot / should not alter "system"
>      parameters like compiler brand or version as there may only be one
>      compiler, the one R was built with
>
>      it does allow however to set compilation flags, language standards,
>      include directories etc as needed to build the package
>
>   -- (user- or system-level) Makeconf allow compiler changes, but that is
>      orthogonal to the per-package config and stays local to the machine


yep I fully aggree with this.

>
> | in unix world mechanism of overwriting system default in user and
> | package order is something standadr and consistent given the level
> |
> | priority is package specific stuff overwrite user configuration that
> | overwrite system default
> |
> | mechanism of Makevars is NOT consistent
>
> Yes we sometimes wish we could override system Makeconf settings in a
> package, but we can't.
>
> | as a side note I solved the problem of overwritting CC for this
> | particular package using MAKEFLAGS="CC=gcc" R CMD INSTALL package
>
> That works and is documented, but is not "portable" to other machines.

I realize based on Kasper's answer and your's that I was not clear on 
the context I was working.


	Eric


From kasperdanielhansen at gmail.com  Tue Sep 27 17:19:40 2016
From: kasperdanielhansen at gmail.com (Kasper Daniel Hansen)
Date: Tue, 27 Sep 2016 11:19:40 -0400
Subject: [Rd] src/Makevars ignored ?
In-Reply-To: <2af1ebe3-2a41-7b02-c066-b0571789a5a3@pasteur.fr>
References: <5abedec3-552a-b141-a9e2-ef74e212b893@pasteur.fr>
	<22506.22668.727653.313373@max.nulle.part>
	<b11e2a62-95bd-0d2f-66ba-2d6177f62bb9@pasteur.fr>
	<CAC2h7usqc7JQxkJG72k-NA4dqT+MR0BB_Qm4GBkROSVHux+6Yw@mail.gmail.com>
	<2af1ebe3-2a41-7b02-c066-b0571789a5a3@pasteur.fr>
Message-ID: <CAC2h7usTeJ6XEC4ypDkwqzDZhYE1z0KG6MC2DK+aMzutbJpiBQ@mail.gmail.com>

So now we have some important context for your original question.

I understand why you want "symmetry" but because of the reasons Dirk
outlines, I personally think it is a bad idea, ie. I disagree with your
statement "anyway I still convinced that if R provides a mechanisn
hierachical way of variable overwrite pkg / user/ system it _SHOULD_ be
consistent at all levels".

The mechanism for achieving what you want - overriding CC on a local
machine -  is using a ~/.R/Makevars file.  I am not sure that this can be
done in a package specific manner (I don;t do this very often), but you
could create this file ('this file" being ~/.R/Makevars), install the
package and then remove it.  Remember, this is an install-time setting, so
you don't need the users of your system to be involved in this. That way
you could test your proposed fix.  If the fix works, it seems to me like it
should be included in the package source by the package maintainer, perhaps
using a configure script, but that is ultimately something which is up to
the package maintainer.

Best,
Kasper

On Tue, Sep 27, 2016 at 10:52 AM, Eric Deveaud <edeveaud at pasteur.fr> wrote:

> Le 27/09/16 ? 16:17, Kasper Daniel Hansen a ?crit :
>
>> As a package author, it is in my opinion irresponsible to override these
>> system settings (which is why it is also impossible).  You have no idea
>> what system it is being deployed on,
>>
>
> as the it guy dedicated to install and maintain softs on our cluster I
> have a reasonable knowledge of the systems I work on.
>
> I don't want to distribute any of the piece of code I was asked to made
> available on the cluster.
> I just need (and succeded plus functional test succeded) to build the
> Rpackage requested by a specific software.
>
>
> I mean, you don't even know if the
>> compiler is gcc.If a user wants (say) heavy optimization they will
>> compile R with optimization.  (For this reason I also don't think users
>> should modify their ~/.R/Makevars, unless for testing purposes).
>>
>
> my question was not in a R package developer context, but in the R user
> that grabs some piece of code and is not abble to compile it because of
> 1) a developper that mixed C and C++ code which is legit.
> 2) a silly interaction beetween C and C++ symbol generation because of the
> use, in our case, of CC = gcc -std=gnu99
> 3) a dev that answwer: "I have no clue, in debian it works" ;-(
>
> anyway I still convinced that if R provides a mechanisn hierachical way of
> variable overwrite pkg / user/ system it _SHOULD_ be consistent at all
> levels
>
> my question was raised because of our install mechanism that (hopefully)
> does not allow
> modification of files like ~/.R/Makevars.
> I can only "play" with the sources of the software it is working on
> and//or environment variables. so I wanted  to have a temporary way of
> setting CC to be plain gcc without ISO C99 language standard support just
> for this specific R library.
>
>
> The exception is (in my opinion) if you need to decrease the level of
>> optimization because you know or suspect the compiler to generate wrong
>> code.
>>
>> What you should do, is use
>>   PKG_CFLAGS
>> as documented in R-exts, 1.2.1 under "Using Makevars".
>>
>
> in the documentation you pointed (and trust me I read it), keyword is _set
> additional_  preprocessor options and//or compiler flags
>
> only way to _remove_ is to overwrite
>
> back to logic.
>
> either Makevars, whatever level, allow to overwrite CC definition
> either Makevars, whatever level, disable CC redefinition
>
> but not a mix
>
>
>         Eric
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From bob at rud.is  Tue Sep 27 17:47:11 2016
From: bob at rud.is (Bob Rudis)
Date: Tue, 27 Sep 2016 11:47:11 -0400
Subject: [Rd] src/Makevars ignored ?
In-Reply-To: <CAC2h7usTeJ6XEC4ypDkwqzDZhYE1z0KG6MC2DK+aMzutbJpiBQ@mail.gmail.com>
References: <5abedec3-552a-b141-a9e2-ef74e212b893@pasteur.fr>
	<22506.22668.727653.313373@max.nulle.part>
	<b11e2a62-95bd-0d2f-66ba-2d6177f62bb9@pasteur.fr>
	<CAC2h7usqc7JQxkJG72k-NA4dqT+MR0BB_Qm4GBkROSVHux+6Yw@mail.gmail.com>
	<2af1ebe3-2a41-7b02-c066-b0571789a5a3@pasteur.fr>
	<CAC2h7usTeJ6XEC4ypDkwqzDZhYE1z0KG6MC2DK+aMzutbJpiBQ@mail.gmail.com>
Message-ID: <CAA-FpKUsdkcee+5VPz_L7=eD7vkNTcMyqwTxWVbWEXd9OzWW_w@mail.gmail.com>

You're then asking CRAN to violate your "ideal contract" w/r/t compiler
switching inside src/Makevars since CRAN needs to setup and produce
standard, predictable, repeatable builds, including binary generation for
two platforms (much to Dirk's chagrin, there _are_ other operating systems
besides Debian linux ;-)

I wouldn't want CRAN to honor said compiler switching inside src/Makevars
but the way you're describing the perceived contract, you're implying they
should.

I totally understand your posit, but there's a larger universe to consider
than internal package builds. In a sense you really want an R version of
Rule of Acquisition #17 "A contract is a contract is a contract... but only
on my internal systems." and that doesn't seem like a good idea to me when
the larger universe of CRAN and the need for baseline standard package
builds are considered.



On Tue, Sep 27, 2016 at 11:19 AM, Kasper Daniel Hansen <
kasperdanielhansen at gmail.com> wrote:

> So now we have some important context for your original question.
>
> I understand why you want "symmetry" but because of the reasons Dirk
> outlines, I personally think it is a bad idea, ie. I disagree with your
> statement "anyway I still convinced that if R provides a mechanisn
> hierachical way of variable overwrite pkg / user/ system it _SHOULD_ be
> consistent at all levels".
>
> The mechanism for achieving what you want - overriding CC on a local
> machine -  is using a ~/.R/Makevars file.  I am not sure that this can be
> done in a package specific manner (I don;t do this very often), but you
> could create this file ('this file" being ~/.R/Makevars), install the
> package and then remove it.  Remember, this is an install-time setting, so
> you don't need the users of your system to be involved in this. That way
> you could test your proposed fix.  If the fix works, it seems to me like it
> should be included in the package source by the package maintainer, perhaps
> using a configure script, but that is ultimately something which is up to
> the package maintainer.
>
> Best,
> Kasper
>
> On Tue, Sep 27, 2016 at 10:52 AM, Eric Deveaud <edeveaud at pasteur.fr>
> wrote:
>
> > Le 27/09/16 ? 16:17, Kasper Daniel Hansen a ?crit :
> >
> >> As a package author, it is in my opinion irresponsible to override these
> >> system settings (which is why it is also impossible).  You have no idea
> >> what system it is being deployed on,
> >>
> >
> > as the it guy dedicated to install and maintain softs on our cluster I
> > have a reasonable knowledge of the systems I work on.
> >
> > I don't want to distribute any of the piece of code I was asked to made
> > available on the cluster.
> > I just need (and succeded plus functional test succeded) to build the
> > Rpackage requested by a specific software.
> >
> >
> > I mean, you don't even know if the
> >> compiler is gcc.If a user wants (say) heavy optimization they will
> >> compile R with optimization.  (For this reason I also don't think users
> >> should modify their ~/.R/Makevars, unless for testing purposes).
> >>
> >
> > my question was not in a R package developer context, but in the R user
> > that grabs some piece of code and is not abble to compile it because of
> > 1) a developper that mixed C and C++ code which is legit.
> > 2) a silly interaction beetween C and C++ symbol generation because of
> the
> > use, in our case, of CC = gcc -std=gnu99
> > 3) a dev that answwer: "I have no clue, in debian it works" ;-(
> >
> > anyway I still convinced that if R provides a mechanisn hierachical way
> of
> > variable overwrite pkg / user/ system it _SHOULD_ be consistent at all
> > levels
> >
> > my question was raised because of our install mechanism that (hopefully)
> > does not allow
> > modification of files like ~/.R/Makevars.
> > I can only "play" with the sources of the software it is working on
> > and//or environment variables. so I wanted  to have a temporary way of
> > setting CC to be plain gcc without ISO C99 language standard support just
> > for this specific R library.
> >
> >
> > The exception is (in my opinion) if you need to decrease the level of
> >> optimization because you know or suspect the compiler to generate wrong
> >> code.
> >>
> >> What you should do, is use
> >>   PKG_CFLAGS
> >> as documented in R-exts, 1.2.1 under "Using Makevars".
> >>
> >
> > in the documentation you pointed (and trust me I read it), keyword is
> _set
> > additional_  preprocessor options and//or compiler flags
> >
> > only way to _remove_ is to overwrite
> >
> > back to logic.
> >
> > either Makevars, whatever level, allow to overwrite CC definition
> > either Makevars, whatever level, disable CC redefinition
> >
> > but not a mix
> >
> >
> >         Eric
> >
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From ecortens at mtroyal.ca  Tue Sep 27 17:53:23 2016
From: ecortens at mtroyal.ca (Evan Cortens)
Date: Tue, 27 Sep 2016 09:53:23 -0600
Subject: [Rd] Recursive dir.create() on Windows shares
In-Reply-To: <95074f7b-8e21-7a98-0637-9973a31dc230@gmail.com>
References: <CABKQe-ZbusW48e0=YFbR562DXFqPaDX7bb6FZFaQpBcF5Hy+pw@mail.gmail.com>
	<95074f7b-8e21-7a98-0637-9973a31dc230@gmail.com>
Message-ID: <CABKQe-Ys7TKL4taC1MHW-QR4SjOXmGs+5ezNRPjSPOGyE=GfRA@mail.gmail.com>

One more comment on this. In Python, there is a function,
os.path.splitdrive(), that performs path splitting in the same way my patch
does. Here's a quote from the Python docs:

"If the path contains a UNC path, drive will contain the host name and
share, up to but not including the fourth separator. e.g.
splitdrive("//host/computer/dir") returns ("//host/computer", "/dir")" (see
https://docs.python.org/3/library/os.path.html#os.path.splitdrive )

The now-deprecated (as of Python 3.1) os.path.splitunc() treated UNC paths
in a similar way.

So this to say I believe the correct behaviour is to skip the first two
parts of a path beginning with \\ before attempting to create a directory
in a call to dir.create() with recursive = TRUE.

On Mon, Sep 26, 2016 at 3:46 PM, Duncan Murdoch <murdoch.duncan at gmail.com>
wrote:

> On 26/09/2016 5:27 PM, Evan Cortens wrote:
>
>> Hi folks,
>>
>> I've noticed that there's an issue with the recursive creation of
>> directories that reside on network shares. For example:
>>
>>
>>> dir.create('\\\\SERVERNAME\\Empl\\Home1\\active\\e\\ecortens
>> \\thisisatest',
>> recursive = TRUE)
>> Warning message:
>> In
>> dir.create("\\\\SERVERNAME\\Empl\\Home1\\active\\e\\ecortens
>> \\thisisatest",
>>  :
>>   cannot create dir '\\SERVERNAME\Empl', reason 'Permission denied'
>>
>> The issue is that dir.create() is skipping the server name, but it's not
>> skipping the share name. So, in the above example, it's trying to create
>> "Empl", which fails, setting errno to the code for "Permission denied",
>> instead of EEXIST, the code for "file already exists", because it's not
>> actually a file, and therefore can't exist as one. (Incidentally, the same
>> challenge arises with the system call _wstat(), which also will return a
>> -1, telling you that the share name doesn't exist.)
>>
>> The solution to this issue, then, is to skip not only the server name, but
>> the share name as well, which is easily done with one more line of code:
>>
>> C:\Users\ecortens\Software\R-devel\src>svn diff
>> Index: main/platform.c
>> ===================================================================
>> --- main/platform.c     (revision 71366)
>> +++ main/platform.c     (working copy)
>> @@ -2159,10 +2159,11 @@
>>      while (*p == L'\\' && wcslen(dir) > 1 && *(p-1) != L':') *p-- =
>> L'\0';
>>      if (recursive) {
>>         p = dir;
>> -       /* skip leading \\share */
>> +       /* skip leading \\server\share */
>>         if (*p == L'\\' && *(p+1) == L'\\') {
>>             p += 2;
>>             p = wcschr(p, L'\\');
>> +           p = wcschr(p+1, L'\\');
>>         }
>>         while ((p = wcschr(p+1, L'\\'))) {
>>             *p = L'\0';
>>
>> This fixes the issue for me, and I can create directories no problem.
>> However, I'm a little worried, as the code in platform.c has been this way
>> since 2008--surely this can't have been a bug since then. Yet I can't find
>> any indication that the UNC naming convention has changed, and I can't
>> find
>> any way that will let you test the pathname to see if it's
>> "\\server\share"
>> or "\\share".
>>
>> I've also filed this on bugzilla, and have updated it there.  See
>> https://bugs.r-project.org/bugzilla/show_bug.cgi?id=1715
>>
>> Thanks for an amazing piece of software!
>>
>> Best,
>>
>> Evan
>>
>> P. S. I'm new to the mailing list, so I apologize in advance if I'm
>> violating any conventions I'm unaware of.
>>
>>
> Presumably someone from Microsoft will respond to this.
>
> Duncan Murdoch
>
>


-- 
Evan Cortens, PhD
Institutional Analyst - Office of Institutional Analysis
Mount Royal University
403-440-6529

	[[alternative HTML version deleted]]


From Jens.Oehlschlaegel at truecluster.com  Tue Sep 27 21:33:39 2016
From: Jens.Oehlschlaegel at truecluster.com (=?UTF-8?Q?Dr._Jens_Oehlschl=c3=a4gel?=)
Date: Tue, 27 Sep 2016 21:33:39 +0200
Subject: [Rd] problem in levels<- and other inconsistencies
Message-ID: <13dad3b3-7b28-6fc1-75e4-37dff9c93667@truecluster.com>

# A couple of years ago
# I helped making R's character NA handling more consistent
# Today I report an issue with R's factor NA handling
# The core problem is that
# levels(g) <- levels(g)
# can change the levels of g
# more details below
# Kind regards
# Jens Oehlschl?gel

# Say I have an NA element in a vector or list

x <- c("a","b",NA)

# then using split() it gets lost

split(x, x)

# as it is (somewhat) when converting to a default factor

table(as.factor(x))

# for table the workaround is

table(as.factor(x), exclude=NULL)

# but for split we need

f <- factor(x, exclude=NULL)

split(x, f)

# conclusion: we MUST use an NA level

# so far so good

g <- f
levels(g)

# but re-assigning the levels changes them

levels(g) <- levels(g)
levels(g)

# which I consider a severe problem.
# Yes, I read the help page of levels<-
# about removing levels by assigning NAs to them
# but that implies: we MUST NOT use an NA level

# If a language suggests
# that we MUST and we MUST NOT use an NA level
# the language has limited usefulness
# (and a user who depends on the language
# is put into a DOUBLE BIND)
# SUGGESTION: assure the above assignment does not change levels

# trying to apply the levels of f to new data also fails

g <- factor(x, levels=levels(f))
g

# and giving both arguments even stops

h <- factor(x, levels=levels(f), labels=levels(f))

# I do understand that exclude= meaningfully has effect
# if levels= are to be determined automatically, but
# SUGGESTION: with explicit levels= exclude= should be ignored.

# SUGGESTION: give split(x, y, exclude=NA) an exclude= argument,
# which when set to NULL will prevent dropping NA levels
# when coercing y to factor
# (it still remains open what should have priority
# if y is a factor with an NA-level and exclude=NA)

table(f, exclude=NA)

# here existing levels win over exclude=
# which is consistent with my suggestion for factor(, levels=, exclude=)


From hpages at fredhutch.org  Tue Sep 27 23:20:50 2016
From: hpages at fredhutch.org (=?UTF-8?B?SGVydsOpIFBhZ8Oocw==?=)
Date: Tue, 27 Sep 2016 14:20:50 -0700
Subject: [Rd] problem in levels<- and other inconsistencies
In-Reply-To: <13dad3b3-7b28-6fc1-75e4-37dff9c93667@truecluster.com>
References: <13dad3b3-7b28-6fc1-75e4-37dff9c93667@truecluster.com>
Message-ID: <7d167696-87c1-4387-5967-a26de6defd19@fredhutch.org>

Hi,

I totally agree that having foo(x) <- foo(x) behave like a no-op
is a must. This is something I try to be careful about when I design
my own objects and their getters and setters.

Just wanted to mention though that there is notorious violation of
this:

   x <- list(3:-1, NULL)
   x[[2]] <- x[[2]]
   x
   # [[1]]
   # [1]  3  2  1  0 -1

Now of course, not just because there is a precedent means the factor
API shouldn't be improved.

Cheers,
H.


On 09/27/2016 12:33 PM, Dr. Jens Oehlschl?gel wrote:
> # A couple of years ago
> # I helped making R's character NA handling more consistent
> # Today I report an issue with R's factor NA handling
> # The core problem is that
> # levels(g) <- levels(g)
> # can change the levels of g
> # more details below
> # Kind regards
> # Jens Oehlschl?gel
>
> # Say I have an NA element in a vector or list
>
> x <- c("a","b",NA)
>
> # then using split() it gets lost
>
> split(x, x)
>
> # as it is (somewhat) when converting to a default factor
>
> table(as.factor(x))
>
> # for table the workaround is
>
> table(as.factor(x), exclude=NULL)
>
> # but for split we need
>
> f <- factor(x, exclude=NULL)
>
> split(x, f)
>
> # conclusion: we MUST use an NA level
>
> # so far so good
>
> g <- f
> levels(g)
>
> # but re-assigning the levels changes them
>
> levels(g) <- levels(g)
> levels(g)
>
> # which I consider a severe problem.
> # Yes, I read the help page of levels<-
> # about removing levels by assigning NAs to them
> # but that implies: we MUST NOT use an NA level
>
> # If a language suggests
> # that we MUST and we MUST NOT use an NA level
> # the language has limited usefulness
> # (and a user who depends on the language
> # is put into a DOUBLE BIND)
> # SUGGESTION: assure the above assignment does not change levels
>
> # trying to apply the levels of f to new data also fails
>
> g <- factor(x, levels=levels(f))
> g
>
> # and giving both arguments even stops
>
> h <- factor(x, levels=levels(f), labels=levels(f))
>
> # I do understand that exclude= meaningfully has effect
> # if levels= are to be determined automatically, but
> # SUGGESTION: with explicit levels= exclude= should be ignored.
>
> # SUGGESTION: give split(x, y, exclude=NA) an exclude= argument,
> # which when set to NULL will prevent dropping NA levels
> # when coercing y to factor
> # (it still remains open what should have priority
> # if y is a factor with an NA-level and exclude=NA)
>
> table(f, exclude=NA)
>
> # here existing levels win over exclude=
> # which is consistent with my suggestion for factor(, levels=, exclude=)
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fredhutch.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From jens.oehlschlaegel at truecluster.com  Wed Sep 28 10:50:47 2016
From: jens.oehlschlaegel at truecluster.com (jens.oehlschlaegel at truecluster.com)
Date: Wed, 28 Sep 2016 10:50:47 +0200
Subject: [Rd] problem in levels<- and other inconsistencies
In-Reply-To: <7d167696-87c1-4387-5967-a26de6defd19@fredhutch.org>
References: <13dad3b3-7b28-6fc1-75e4-37dff9c93667@truecluster.com>,
	<7d167696-87c1-4387-5967-a26de6defd19@fredhutch.org>
Message-ID: <trinity-5c0f7dd6-abb1-45f3-9c7f-622bb069d18e-1475052647396@3capp-webde-bs13>

Herv?,
?
Good point, but easy to solve:
?
since 

list[i] # always is.list

deleting a list element with

list[i] <- NULL # !is.list(NULL)

does not lead into a contradiction

whereas 

list[[i]] <- NULL 

should do the same as 

list[i] <- list(NULL)

YES, I know that this would be major change, but NO, this is no justification to not fix a mistake in a language. Unless one has given up to fix the language, in which case we all should switch to another one (Julia, ...)

Jens
?
?

Gesendet:?Dienstag, 27. September 2016 um 23:20 Uhr
Von:?"Herv? Pag?s" <hpages at fredhutch.org>
An:?"Dr. Jens Oehlschl?gel" <Jens.Oehlschlaegel at truecluster.com>, r-devel at r-project.org
Betreff:?Re: [Rd] problem in levels<- and other inconsistencies
Hi,

I totally agree that having foo(x) <- foo(x) behave like a no-op
is a must. This is something I try to be careful about when I design
my own objects and their getters and setters.

Just wanted to mention though that there is notorious violation of
this:

x <- list(3:-1, NULL)
x[[2]] <- x[[2]]
x
# [[1]]
# [1] 3 2 1 0 -1

Now of course, not just because there is a precedent means the factor
API shouldn't be improved.

Cheers,
H.


On 09/27/2016 12:33 PM, Dr. Jens Oehlschl?gel wrote:
> # A couple of years ago
> # I helped making R's character NA handling more consistent
> # Today I report an issue with R's factor NA handling
> # The core problem is that
> # levels(g) <- levels(g)
> # can change the levels of g
> # more details below
> # Kind regards
> # Jens Oehlschl?gel
>
> # Say I have an NA element in a vector or list
>
> x <- c("a","b",NA)
>
> # then using split() it gets lost
>
> split(x, x)
>
> # as it is (somewhat) when converting to a default factor
>
> table(as.factor(x))
>
> # for table the workaround is
>
> table(as.factor(x), exclude=NULL)
>
> # but for split we need
>
> f <- factor(x, exclude=NULL)
>
> split(x, f)
>
> # conclusion: we MUST use an NA level
>
> # so far so good
>
> g <- f
> levels(g)
>
> # but re-assigning the levels changes them
>
> levels(g) <- levels(g)
> levels(g)
>
> # which I consider a severe problem.
> # Yes, I read the help page of levels<-
> # about removing levels by assigning NAs to them
> # but that implies: we MUST NOT use an NA level
>
> # If a language suggests
> # that we MUST and we MUST NOT use an NA level
> # the language has limited usefulness
> # (and a user who depends on the language
> # is put into a DOUBLE BIND)
> # SUGGESTION: assure the above assignment does not change levels
>
> # trying to apply the levels of f to new data also fails
>
> g <- factor(x, levels=levels(f))
> g
>
> # and giving both arguments even stops
>
> h <- factor(x, levels=levels(f), labels=levels(f))
>
> # I do understand that exclude= meaningfully has effect
> # if levels= are to be determined automatically, but
> # SUGGESTION: with explicit levels= exclude= should be ignored.
>
> # SUGGESTION: give split(x, y, exclude=NA) an exclude= argument,
> # which when set to NULL will prevent dropping NA levels
> # when coercing y to factor
> # (it still remains open what should have priority
> # if y is a factor with an NA-level and exclude=NA)
>
> table(f, exclude=NA)
>
> # here existing levels win over exclude=
> # which is consistent with my suggestion for factor(, levels=, exclude=)
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

--
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fredhutch.org
Phone: (206) 667-5791
Fax: (206) 667-1319


From maechler at stat.math.ethz.ch  Thu Sep 29 11:32:08 2016
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 29 Sep 2016 11:32:08 +0200
Subject: [Rd] Undocumented 'use.names' argument to c()
In-Reply-To: <22505.19505.247801.449980@stat.math.ethz.ch>
References: <304919818.4777007.1474901471533.ref@mail.yahoo.com>
	<304919818.4777007.1474901471533@mail.yahoo.com>
	<22505.19505.247801.449980@stat.math.ethz.ch>
Message-ID: <22508.57240.757214.581997@stat.math.ethz.ch>

>>>>> Martin Maechler <maechler at stat.math.ethz.ch>
>>>>>     on Mon, 26 Sep 2016 18:26:25 +0200 writes:

>>>>> Suharto Anggono Suharto Anggono <suharto_anggono at yahoo.com>
>>>>>     on Mon, 26 Sep 2016 14:51:11 +0000 writes:

    >> By "an argument named 'use.names' is included for concatenation", I meant something like this, that someone might try.
    >>> c(as.Date("2016-01-01"), use.names=FALSE)
    >> use.names 
    >> "2016-01-01" "1970-01-01" 

    >> See, 'use.names' is in the output. That's precisely because 'c.Date' doesn't have 'use.names', so that 'use.names' is absorbed into '...'.

    > Yes, of course.
    > Thank you for the explanation; now I understand what you meant.

    > Indeed, the situation is not entirely satisfactory:

    > Ideally, *both* the  'recursive' and 'use.names' arguments of
    > c() should be considered arguments of only the *default* method of c(),
    > not the generic.

    > OTOH, c() being .Primitive() the implementation is in C only,
    > and (in some sense) of *both* the generic function and the
    > default method.
    > The C code clearly treats  'recursive' and 'use.names' "the
    > same", and has been part of R "forever".

    > I think that ideally, we should aim for

    > 1) The generic function  c()  only has arguments "..." (or possibly
    > ---  because of history of the S4 part ---  "x, ...").

    > 2) The default method has additional arguments
    > 'recursive = FALSE, use.names = TRUE'
    > and other methods of c() can choose if they want to also
    > support one or two or none of these extras.

    > Somewhat related, but in principle independent of '1)'
    > and '2)' above  -- I think, because of the ".Primitive"-ness of c() --
    > is the quite how 'c' should print in R.
    > Currently it prints like what I say should just be the default
    > method.

    > Honestly, I'm not sure if it would be straightforward or even
    > just relatively painless to go to  '1) + 2)' ... may change
    > r71349 (to the S4 generic definition of "c") had dramatical
    > effects in "package land" and hence reversion of that (with
    > r71354) was necessary, for the time being.

I have just now committed a change to R-devel which on the   ?c
help page gives

| Usage:
| 
|      ## S3 Generic function
|      c(...)
|      
|      ## Default S3 method:
|      c(..., recursive = FALSE, use.names = TRUE)

and in the console, simply

| > c
| function (...)  .Primitive("c")
| > 

and am considering committing a similar change to the place
where S4 generic c() is setup in the 'methods' package.

If this persists,  methods for c(), S3 or S4, will have the
freedom to carry none, one, or both of  'recursive' and
'use.names' arguments.

  > methods(c)
  [1] c.bibentry*       c.Date            c.difftime        c.noquote        
  [5] c.numeric_version c.person*         c.POSIXct         c.POSIXlt        
  [9] c.warnings       

Currently, most existing c() methods have a 'recursive = FALSE'
*and* ignore a 'recursive' specification completely .. and as
Suharto has noted already, of course they do not have a
'use.names' argument yet and so do not ignore it, but treat it
as a regular argument (to be concatenated).

One consequence of this change (the above commit) is that in
principle all c() methods which have more than the '...'
arguments should be documented as "they have surprising
arguments":  They have a 'recursive' argument which is not part
of the generic.

I would say that this "should be documented" is rather a good
thing, because indeed they do silently ignore any 'recursive = foobar()'
and that should be documented, e.g., in current R (and R-devel):

  > c(Sys.Date(), recursive=quote(foobar_nonsense()))
  [1] "2016-09-29"
  > 

which is not well documented, I'd say

Martin



    >> --------------------------------------------
    >> On Sun, 25/9/16, Martin Maechler <maechler at stat.math.ethz.ch> wrote:

    >> Subject: Re: [Rd] Undocumented 'use.names' argument to c()
    >> To: "Suharto Anggono Suharto Anggono" <suharto_anggono at yahoo.com>
    >> Cc: "R-devel" <R-devel at r-project.org>
    >> Date: Sunday, 25 September, 2016, 10:14 PM
 
>>>>> Suharto Anggono Suharto Anggono via R-devel <r-devel at r-project.org>
>>>>>     on Sun, 25 Sep 2016 14:12:10 +0000 writes:

    >>>> From comments in
    >>>> http://stackoverflow.com/questions/24815572/why-does-function-c-accept-an-undocumented-argument/24815653
    >>>> : The code of c() and unlist() was formerly shared but
    >>>> has been (long time passing) separated. From July 30,
    >>>> 1998, is where do_c got split into do_c and do_unlist.
    >>> With the implementation of 'c.Date' in R devel r71350, an
    >>> argument named 'use.names' is included for
    >>> concatenation. So, it doesn't follow the documented
    >>> 'c'. But, 'c.Date' is not explicitly documented in
    >>> Dates.Rd, that has 'c.Date' as an alias.

    >> I do not see any  c.Date  in R-devel with a 'use.names'; its a
    >> base function, hence not hidden ..

    >> As mentioned before, 'use.names' is used in unlist() in quite a
    >> few places, and such an argument also exists for

    >> lengths()            and
    >> all.equal.list()

    >> and now c()


From marc_grt at yahoo.fr  Thu Sep 29 14:42:08 2016
From: marc_grt at yahoo.fr (Marc Girondot)
Date: Thu, 29 Sep 2016 14:42:08 +0200
Subject: [Rd] Web site for MacOSX R-devel precompiled version
Message-ID: <c4292d67-7636-4463-8f4f-d4d9b6bb5878@yahoo.fr>

 From several years, I used this site http://r.research.att.com to get a 
precompiled version of macosx R-devol but the most recent version is 
frozen at the 18/8 build (r 71112).

Is there an alternative (except than compile myself !) ?

Thanks,

Marc


From bob at rud.is  Thu Sep 29 16:51:03 2016
From: bob at rud.is (Bob Rudis)
Date: Thu, 29 Sep 2016 10:51:03 -0400
Subject: [Rd] Web site for MacOSX R-devel precompiled version
In-Reply-To: <c4292d67-7636-4463-8f4f-d4d9b6bb5878@yahoo.fr>
References: <c4292d67-7636-4463-8f4f-d4d9b6bb5878@yahoo.fr>
Message-ID: <CAA-FpKXRXXuq13f23CQw+WF5K3wnf7TeKQ9OcrbSGHPoMLLbbw@mail.gmail.com>

I've had a TODO on the list for a while to produce a daily R-devel binary
build for macOS since I have some spare macOS compute cycles available. If
there's sufficient interest I can copy the build setup and start generating
them. I'm also a registered Apple developer so can make signed binaries as
well.

On Thu, Sep 29, 2016 at 8:42 AM, Marc Girondot via R-devel <
r-devel at r-project.org> wrote:

> From several years, I used this site http://r.research.att.com to get a
> precompiled version of macosx R-devol but the most recent version is frozen
> at the 18/8 build (r 71112).
>
> Is there an alternative (except than compile myself !) ?
>
> Thanks,
>
> Marc
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From winstonchang1 at gmail.com  Thu Sep 29 18:38:16 2016
From: winstonchang1 at gmail.com (Winston Chang)
Date: Thu, 29 Sep 2016 11:38:16 -0500
Subject: [Rd] Problems with sub() due to inability to set encoding of ASCII
	strings
Message-ID: <CAFOpNVHyHBO5KZJp=vYLWRynHzKDjR6wyOy0JK9EFevJi98Cew@mail.gmail.com>

I'm encountering a problem using sub() on strings in R 3.3.1 in
Windows, using both RGui and RStudio. The problem happens when the
starting string is ASCII, but the replacement string is UTF-8.


If we create an ASCII string x1, its encoding is marked as "unknown",
and doing a sub() on that string with a UTF-8 replacement results in
weird characters:

x1 <- "a b c"
Encoding(x1)
# [1] "unknown"

replacement <- "??"
Encoding(replacement)
# [1] "UTF-8"

(y1 <- sub("a", replacement, x1))
#[1] "?????? b c"
Encoding(y1)
# [1] "unknown"


If the starting string x2 has Chinese characters, it'll be marked as
UTF-8, and replacement works fine:

x2 <- "a b c ??"
Encoding(x2)
# [1] "UTF-8"

(y2 <- sub("a", replacement, x2))
# [1] "?? b c ??"
Encoding(y2)
# [1] "UTF-8"


It seems like the solution should be to mark the starting string as
UTF-8, but apparently it doesn't work if the string is ASCII, and so
the sub() still gives weird characters:

# Not possible to mark x1 as UTF-8
Encoding(x1) <- "UTF-8"
Encoding(x1)
# [1] "unknown"

(y3 <- sub("a", replacement, x1))
# [1] "?????? b c"
Encoding(y3)
# [1] "unknown"

It is possible to tell R that the final string y3 is UTF-8, but it
doesn't seem like this should be necessary:

Encoding(y3) <- "UTF-8"
y3
# [1] "?? b c"


Is there some way to mark the starting string x1 as UTF-8 so that the
result of sub() comes out marked as UTF-8? If the inputs are both
UTF-8, it shouldn't be necessary to explicitly tell R that the output
is also UTF-8.

-Winston


From simon.urbanek at r-project.org  Thu Sep 29 20:09:56 2016
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Thu, 29 Sep 2016 14:09:56 -0400
Subject: [Rd] Web site for MacOSX R-devel precompiled version
In-Reply-To: <c4292d67-7636-4463-8f4f-d4d9b6bb5878@yahoo.fr>
References: <c4292d67-7636-4463-8f4f-d4d9b6bb5878@yahoo.fr>
Message-ID: <3E9938CF-CBCA-4D01-B6DC-63B4616BA36F@r-project.org>


> On Sep 29, 2016, at 8:42 AM, Marc Girondot via R-devel <r-devel at r-project.org> wrote:
> 
> From several years, I used this site http://r.research.att.com to get a precompiled version of macosx R-devol but the most recent version is frozen at the 18/8 build (r 71112).
> 
> Is there an alternative (except than compile myself !) ?
> 

It is the official build - there may be other alternatives but they are not supported by us. The fact that is didn't update would be best reported so we can look into this. I just checked and the build are still working:

$ ls -l *.pkg
-rw-r--r--  1 urbanek  admin  77935686 Sep 28 23:32 R-devel-mavericks.pkg

but it seems that there is a problem with signing the package. I'll look into it.

Cheers,
Simon


> 
> Thanks,
> 
> Marc
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 


From ligges at statistik.tu-dortmund.de  Thu Sep 29 22:57:00 2016
From: ligges at statistik.tu-dortmund.de (Uwe Ligges)
Date: Thu, 29 Sep 2016 22:57:00 +0200
Subject: [Rd] winbuilder down
Message-ID: <9ba8d504-6640-75a1-4485-88719219b29b@statistik.tu-dortmund.de>

Dear all,

winbuilder is down.

Given current inspection results and circumstances, I expect a downtime 
of *at least* 24 hours.

This applies to on-demand check services, CRAN check service results and 
CRAN binary builds for Windows.

Best,
Uwe Ligges


From simon.urbanek at r-project.org  Fri Sep 30 02:38:20 2016
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Thu, 29 Sep 2016 20:38:20 -0400
Subject: [Rd] Web site for MacOSX R-devel precompiled version
In-Reply-To: <3E9938CF-CBCA-4D01-B6DC-63B4616BA36F@r-project.org>
References: <c4292d67-7636-4463-8f4f-d4d9b6bb5878@yahoo.fr>
	<3E9938CF-CBCA-4D01-B6DC-63B4616BA36F@r-project.org>
Message-ID: <E0019891-75A9-46B2-8812-6AADD214F188@r-project.org>

[Moving to R-SIG-Mac where it belongs]

The machine building 10.6 binaries has died and since it was responsible for merging the 10.9 and 10.6 binaries to the website it means neither were published. 10.9 binaries are now up.



> On Sep 29, 2016, at 2:09 PM, Simon Urbanek <simon.urbanek at r-project.org> wrote:
> 
> 
>> On Sep 29, 2016, at 8:42 AM, Marc Girondot via R-devel <r-devel at r-project.org> wrote:
>> 
>> From several years, I used this site http://r.research.att.com to get a precompiled version of macosx R-devol but the most recent version is frozen at the 18/8 build (r 71112).
>> 
>> Is there an alternative (except than compile myself !) ?
>> 
> 
> It is the official build - there may be other alternatives but they are not supported by us. The fact that is didn't update would be best reported so we can look into this. I just checked and the build are still working:
> 
> $ ls -l *.pkg
> -rw-r--r--  1 urbanek  admin  77935686 Sep 28 23:32 R-devel-mavericks.pkg
> 
> but it seems that there is a problem with signing the package. I'll look into it.
> 
> Cheers,
> Simon
> 
> 
>> 
>> Thanks,
>> 
>> Marc
>> 
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>> 
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 


From brodie.gaslam at yahoo.com  Fri Sep 30 04:05:48 2016
From: brodie.gaslam at yahoo.com (brodie gaslam)
Date: Fri, 30 Sep 2016 02:05:48 +0000 (UTC)
Subject: [Rd] A package that traces base functions
References: <160600841.7174401.1475201148793.ref@mail.yahoo.com>
Message-ID: <160600841.7174401.1475201148793@mail.yahoo.com>

I'm working on a package that implements a REPL.? A typical interaction with the package might look like:
> launch_REPL()REPL> 1 + 1[1] 2REPL> QDo you wish to save results? [y/n]REPL> ygoodbye ...>
This is very similar to what happens when in `browser()`: the REPL evaluates arbitrary R user expressions and offers some additional commands.
In order to implement functionality required for the REPL I must trace some functions in the base package.? The trace is removed `on.exit()` from the REPL, so the functions are only modified while the `launch_REPL` function is evaluating.? Unfortunately this is against the letter of the law (as per CRAN policy):
> A package must not tamper with the code already loaded into R: anyattempt to change code in the standard and recommended packages whichship with R is prohibited.
Is there any chance that this very limited (only during my function evaluation) modification of base functions with `trace` could be considered to meet the spirit of the law, if not the letter?? Package users would be duly notified this is happening.

Regards,
Brodie Gaslam.
PS: More details for those who care: the REPL among other things implements an environment that has for parent `as.environment(2)` so that objects in the global environment are not visible while in the REPL, but otherwise the full search path is.? Anytime the search path changes I need to update the REPL environment to re-point to `as.environment(2)`, which means I need to know when the search path changes.? I do this by tracing `library`/`attach`/`detach` and triggering a side effect that updates the REPL environment parent any time those are called.? The search path itself is untouched.? I cannot just parse user expressions searching for those functions as the user can use any arbitrary expressions, including sourcing files that contain the `library`, etc. calls.? 


	[[alternative HTML version deleted]]


From maechler at stat.math.ethz.ch  Fri Sep 30 14:51:52 2016
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 30 Sep 2016 14:51:52 +0200
Subject: [Rd] Coercion of 'exclude' in function 'factor' (was
 'droplevels' inappropriate change)
In-Reply-To: <22488.10847.162472.776657@stat.math.ethz.ch>
References: <1077826208.388023.1472832600456.ref@mail.yahoo.com>
	<1077826208.388023.1472832600456@mail.yahoo.com>
	<22488.10847.162472.776657@stat.math.ethz.ch>
Message-ID: <22510.24552.521217.998295@stat.math.ethz.ch>

>>>>> Martin Maechler <maechler at stat.math.ethz.ch>
>>>>>     on Tue, 13 Sep 2016 18:33:35 +0200 writes:

>>>>> Suharto Anggono Suharto Anggono via R-devel <r-devel at r-project.org>
>>>>>     on Fri, 2 Sep 2016 16:10:00 +0000 writes:

    >> I am basically fine with the change.
    >> How about using just the following?
    >> if(!is.character(exclude))
    >> exclude <- as.vector(exclude, typeof(x)) # may result in NA
    >> x <- as.character(x)

    >> It looks simpler and is, more or less, equivalent.

    > yes, but the current code should be slightly faster..

    >> In factor.Rd, in description of argument 'exclude', "(when \code{x} is a \code{factor} already)" can be removed.


    >> A larger change that, I think, is reasonable is entirely removing the code
    >> exclude <- as.vector(exclude, typeof(x)) # may result in NA

    >> The explicit coercion of 'exclude' is not necessary. 
    >> Function 'factor' works without it.

    >> The coercion of 'exclude' may lead to a surprise because it "may result in NA". 
    >> Example from https://stat.ethz.ch/pipermail/r-help/2005-April/069053.html :
    >> factor(as.integer(c(1,2,3,3,NA)), exclude=NaN)
    >> excludes NA.

    >> As a bonus, without the coercion of 'exclude', 'exclude' can be a factor if 'x' is a factor. This part of an example in https://stat.ethz.ch/pipermail/r-help/2011-April/276274.html works.
    >> cc <- c("x","y","NA")
    >> ff <- factor(cc)
    >> factor(ff,exclude=ff[3])

    > Yes, two good reasons for a change.  factor() would finally
    > behave according to the documentation which has been mentioning
    > that 'exclude' could be a factor: ((Until my R-devel changes of a
    > few weeks ago, i.e. at least in all recent released versions of R)),
    > the help page for factor has said

    > || If 'exclude' is used it should either be a factor with the same
    > || level set as 'x' or a set of codes for the levels to be excluded.

    >> However, the coercion of 'exclude' has been in function 'factor' in R "forever".

    > Indeed: On March 6, 1998, svn rev. 845, when the R source files got a
    > '.R' appended, and quite a long time before  R 1.0.0,
    > the factor function was as short as (but using an .Internal(.) !)

    > function (x, levels = sort(unique(x), na.last = TRUE), labels, exclude = NA, 
    > 	ordered = FALSE) 
    > {
    > 	if (length(x) == 0) 
    > 		return(character(0))
    > 	exclude <- as.vector(exclude, typeof(x))
    > 	levels <- levels[is.na(match(levels, exclude))]
    > 	x <- .Internal(factor(match(x, levels), length(levels), 
    > 		ordered))
    > 	if (missing(labels)) 
    > 		levels(x) <- levels
    > 	else levels(x) <- labels
    > 	x
    > }

    > and already contained that line.

    > Nevertheless, simplifying factor() by removing that line (or those
    > 2 lines, now!) seems to only have advantages....

    > I'm not yet committing to anything, but currently would strongly
    > consider it .. though *after* the
    > '<length-1-array>  OP  <non-array>'
    > issue has settled a bit.

  (Which it has;  the decision has been to keep it.)

I have now committed Suharto's proposal above, to entirely drop the
	exclude <- as.vector(exclude, typeof(x))
parts
in the factor() function...  which has the two advantages
mentioned above and simplifies the code (and documentation).

------------------------------------------------------------------------
r71424 | maechler | 2016-09-30 14:38:43 +0200 (Fri, 30 Sep 2016) | 1 line
Changed paths:
   M /trunk/doc/NEWS.Rd
   M /trunk/src/library/base/R/factor.R
   M /trunk/src/library/base/man/factor.Rd
   M /trunk/tests/reg-tests-1c.R

simplify factor(), allowing 'exclude= <factor>' as documented in R <= 3.3.x
------------------------------------------------------------------------

I do expect some "reaction" in CRAN/Bioconductor package space,
so the final word has not been spoken on this, but the new code
is more aestethic to me.

Thank you for the suggestion,
Martin 


    >> --------------------------------------------
    >> On Wed, 31/8/16, Martin Maechler <maechler at stat.math.ethz.ch> wrote:

    >> Subject: Re: [Rd] 'droplevels' inappropriate change

    >> Cc: "Martin Maechler" <maechler at stat.math.ethz.ch>
    >> Date: Wednesday, 31 August, 2016, 2:51 PM
 
>>>>> Martin Maechler <maechler at stat.math.ethz.ch>
>>>>>     on Sat, 27 Aug 2016 18:55:37 +0200 writes:

>>>>> Suharto Anggono Suharto Anggono via R-devel <r-devel at r-project.org>
>>>>>     on Sat, 27 Aug 2016 03:17:32 +0000 writes:

    >>>> In R devel r71157, 'droplevels' documentation, in "Arguments" section, says this about argument 'exclude'.
    >>>> passed to factor(); factor levels which should be excluded from the result even if present.  Note that this was implicitly NA in R <= 3.3.1 which did drop NA levels even when present in x, contrary to the documentation.  The current default is compatible with x[ , drop=FALSE].

    >>>> The part
    >>>> x[ , drop=FALSE]
    >>>> should be
    >>>> x[ , drop=TRUE]

    >> [[elided Yahoo spam]]
    >>> a "typo" by me. .. fixed now.

    >>>> Saying that 'exclude' is factor levels is not quite true for NA element. NA may be not an original level, but NA in 'exclude' affects the result.

    >>>> For a factor 'x', factor(x, exclude = exclude) doesn't really work for excluding in general. See, for example, https://stat.ethz.ch/pipermail/r-help/2005-September/079336.html .
    >>>> factor(factor(c("a","b","c")), exclude="c")

    >>>> However, this excludes "2":
    >>>> factor(factor(2:3), exclude=2)

    >>>> Rather unexpectedly, this excludes NA:
    >>>> factor(factor(c("a",NA), exclude=NULL), exclude="c")

    >>>> For a factor 'x', factor(x, exclude = exclude) can only exclude integer-like or NA levels. An explanation is in https://stat.ethz.ch/pipermail/r-help/2011-April/276274.html .

    >>> Well, Peter Dalgaard (in that R-devel e-mail, a bit more than 5
    >>> years ago) is confirming the problem there,  and suggesting (as
    >>> you, right?) that actually   `factor()` is not behaving
    >>> correctly here.

    >>> And your persistence is finally getting close to convince me
    >>> that it is not just droplevels(), but  factor() itself which
    >>> needs care here.

    >>> Interestingly, the following patch *does* pass 'make check-all'
    >>> (after small change in tests/reg-tests-1b.R which is ok),
    >>> and leads to behavior which is much closer to the documentation,
    >>> notably for your two examples above would give what one would
    >>> expect.

    >>> (( If the R-Hub would support experiments with branches of R-devel 
    >>> from R-core members,  I could just create such a branch and R Hub
    >>> would run 'R CMD check <pkg>'  for thousands of CRAN packages
    >>> and provide a web page with the *differences* in the package
    >>> check results ... so we could see ... ))

    >>> I do agree that we should strongly consider such a change.

    >> as nobody has commented, I've been liberal and have taken these
    >> no comments as consent.

    >> Hence I have committed

    >> ------------------------------------------------------------------------
    >> r71178 | maechler | 2016-08-31 09:45:40 +0200 (Wed, 31 Aug 2016) | 1 line
    >> Changed paths:
    >> M /trunk/doc/NEWS.Rd
    >> M /trunk/src/library/base/R/factor.R
    >> M /trunk/src/library/base/man/factor.Rd
    >> M /trunk/tests/reg-tests-1b.R
    >> M /trunk/tests/reg-tests-1c.R

    >> factor(x, exclude) more "rational" when x or exclude are character
    >> ------------------------------------------------------------------------

    >> which apart from documentation, examples, and regression tests
    >> is just the patch below.

    >> Martin Maechler
    >> ETH Zurich



    >>> --- factor.R    (revision 71157)
    >>> +++ factor.R    (working copy)
    >>> @@ -28,8 +28,12 @@
    >>> levels <- unique(y[ind])
    >>> }
    >>> force(ordered) # check if original x is an ordered factor
    >>> -    exclude <- as.vector(exclude, typeof(x)) # may result in NA
    >>> -    x <- as.character(x)
    >>> +    if(!is.character(x)) {
    >>> +    if(!is.character(exclude))
    >>> +        exclude <- as.vector(exclude, typeof(x)) # may result in NA
    >>> +    x <- as.character(x)
    >>> +    } else
    >>> +    exclude <- as.vector(exclude, typeof(x)) # may result in NA
    >>> ## levels could be a long vectors, but match will not handle that.
    >>> levels <- levels[is.na(match(levels, exclude))]
    >>> f <- match(x, levels)
    >> Delete Reply Reply All Forward Apply

    >> ______________________________________________
    >> R-devel at r-project.org mailing list
    >> https://stat.ethz.ch/mailman/listinfo/r-devel

    > ______________________________________________
    > R-devel at r-project.org mailing list
    > https://stat.ethz.ch/mailman/listinfo/r-devel


From bob at rud.is  Fri Sep 30 16:45:03 2016
From: bob at rud.is (Bob Rudis)
Date: Fri, 30 Sep 2016 10:45:03 -0400
Subject: [Rd] Sys.setFileTime()
Message-ID: <CAA-FpKUdCiOkY8k1EuNHQGs-xEy9PO0LFFWokoniUH4rE6LwQA@mail.gmail.com>

Since there has been a recent tweak to the functionality of
Sys.setFileTime() I thought it might be an opportune time to ask a question
regarding the decision to set both access and modification times
(i.e. settime.actime = settime.modtime = (int)ftime; ) vs provide a
parameter for each.

Might it be possible to change the behavior to accept two parameters (one
for active and one for modtime) but default to setting both the same (to
preserve existing functionality)?

If there was a compelling reason to have it set both and not allow distinct
settings, that's not an issue (I'll just keep it as a local utility
function in my personal pkg).

thx,

-Bob

	[[alternative HTML version deleted]]


From murdoch.duncan at gmail.com  Fri Sep 30 18:45:47 2016
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Fri, 30 Sep 2016 12:45:47 -0400
Subject: [Rd] Sys.setFileTime()
In-Reply-To: <CAA-FpKUdCiOkY8k1EuNHQGs-xEy9PO0LFFWokoniUH4rE6LwQA@mail.gmail.com>
References: <CAA-FpKUdCiOkY8k1EuNHQGs-xEy9PO0LFFWokoniUH4rE6LwQA@mail.gmail.com>
Message-ID: <8e695093-9646-9ceb-f94a-86f4c7825127@gmail.com>

On 30/09/2016 10:45 AM, Bob Rudis wrote:
> Since there has been a recent tweak to the functionality of
> Sys.setFileTime() I thought it might be an opportune time to ask a question
> regarding the decision to set both access and modification times
> (i.e. settime.actime = settime.modtime = (int)ftime; ) vs provide a
> parameter for each.

Do note that it doesn't do that on Windows; it only sets the mtime. 
However, one of us has a memory that on some versions of Windows that 
counts as an access, so both may end up being changed.

>
> Might it be possible to change the behavior to accept two parameters (one
> for active and one for modtime) but default to setting both the same (to
> preserve existing functionality)?
> If there was a compelling reason to have it set both and not allow distinct
> settings, that's not an issue (I'll just keep it as a local utility
> function in my personal pkg).

It would be possible to change, but would take time.  Since we don't 
need it for R's purposes, and you already have it, it seems like a low 
priority.

Duncan Murdoch


From ligges at statistik.tu-dortmund.de  Fri Sep 30 20:15:49 2016
From: ligges at statistik.tu-dortmund.de (Uwe Ligges)
Date: Fri, 30 Sep 2016 20:15:49 +0200
Subject: [Rd] winbuilder down
In-Reply-To: <9ba8d504-6640-75a1-4485-88719219b29b@statistik.tu-dortmund.de>
References: <9ba8d504-6640-75a1-4485-88719219b29b@statistik.tu-dortmund.de>
Message-ID: <46bf49a8-14cc-654d-d63f-ad3adb4192a4@statistik.tu-dortmund.de>



On 29.09.2016 22:57, Uwe Ligges wrote:
> Dear all,
>
> winbuilder is down.
>
> Given current inspection results and circumstances, I expect a downtime
> of *at least* 24 hours.
>
> This applies to on-demand check services, CRAN check service results and
> CRAN binary builds for Windows.
>
> Best,
> Uwe Ligges


Winbuilder is operational again, but please expect up to 2-day delays in 
Windows binary builds on CRAN.

Best,
Uwe


From edd at debian.org  Fri Sep 30 20:56:23 2016
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 30 Sep 2016 13:56:23 -0500
Subject: [Rd] winbuilder down
In-Reply-To: <46bf49a8-14cc-654d-d63f-ad3adb4192a4@statistik.tu-dortmund.de>
References: <9ba8d504-6640-75a1-4485-88719219b29b@statistik.tu-dortmund.de>
	<46bf49a8-14cc-654d-d63f-ad3adb4192a4@statistik.tu-dortmund.de>
Message-ID: <22510.46423.214959.200357@max.nulle.part>


On 30 September 2016 at 20:15, Uwe Ligges wrote:
| On 29.09.2016 22:57, Uwe Ligges wrote:
| > Dear all,
| >
| > winbuilder is down.
| >
| > Given current inspection results and circumstances, I expect a downtime
| > of *at least* 24 hours.
| >
| > This applies to on-demand check services, CRAN check service results and
| > CRAN binary builds for Windows.
| >
| > Best,
| > Uwe Ligges
| 
| 
| Winbuilder is operational again, but please expect up to 2-day delays in 
| Windows binary builds on CRAN.

A very big Thank You! from all of us for all your work on keeping that
beast^Hmachine and service going!

Dirk

-- 
http://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


