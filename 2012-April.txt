From maechler at stat.math.ethz.ch  Mon Apr  2 16:27:45 2012
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 2 Apr 2012 16:27:45 +0200
Subject: [Rd] CRAN policies
In-Reply-To: <E66794E69CFDE04D9A70842786030B9328F467@PA-MBX04.na.tibco.com>
References: <mailman.19.1333015207.14516.r-devel@r-project.org>
	<4F746B54.2050601@mayo.edu>
	<E66794E69CFDE04D9A70842786030B9328EC77@PA-MBX04.na.tibco.com>
	<loom.20120329T190333-975@post.gmane.org>
	<E66794E69CFDE04D9A70842786030B9328EDE3@PA-MBX04.na.tibco.com>
	<4F74B657.5040807@prodsyse.com>
	<E66794E69CFDE04D9A70842786030B9328EEFC@PA-MBX04.na.tibco.com>
	<CABdHhvGp4f6KAqa5ba8xB-JH0sh0y87J+xRZeTW9DSwzowpV2g@mail.gmail.com>
	<3D59DF35E7CF3E42BA70C23BD5CBE6A85F820247B0@exvic-mbx04.nexus.csiro.au>
	<E66794E69CFDE04D9A70842786030B9328F467@PA-MBX04.na.tibco.com>
Message-ID: <20345.46945.43935.237846@stat.math.ethz.ch>

>>>>> William Dunlap <wdunlap at tibco.com>
>>>>>     on Fri, 30 Mar 2012 16:07:52 +0000 writes:

    > It looks like you define a few functions that use substitute() or sys.call()
    > or similar functions to look at the unevaluated argument list.  E.g.,

    > "cq" <-
    > function( ...) {
    > # Saves putting in quotes!
    > # E.G.: quoted( first, second, third) is the same as c( 'first', 'second', 'third')
    > # wrapping by as.character means cq() returns character(0) not list()
    > as.character( sapply( as.list( match.call( expand.dots=TRUE))[-1], as.character))
    > }
    > %such.that% and %SUCH.THAT% do similar things.

    > Almost all the complaints from check involve calls to a
    > handful of such functions.  If you could tell
    > codetools:::checkUsage that that these functions did
    > nonstandard evaluation on all or some of their arguments
    > then the complaints would go away and other checks for
    > real errors like misspellings would still be done.

I agree very much with you, Bill.
Many (if not the majority) of my packages have given these false
positive notes for many months now... and I have to admit that
the effect indeed has been that I take notes much less seriously
nowadays.  This of course has never been the intention.

I'm pretty sure that most of us agree that it would be very
useful if not desirable to have a simple and robust way for
package authors to declare nonstandard evaluation to the
checkUsage() checks.
Maybe we should branch a new thread about this, for proposals on
how to go about this.

Martin


    > Another possible part of the problem is that if checkUsage
    > is checking a function like

    > f <- function(x) paste(x, cq(suffix), sep=".")
    > it attributes the out-of-scope suffix problem to 'f' and doesn't mention that the immediate
    > caller is 'cq', so you cannot easily filter output complaints about cq.  (CRAN would
    > not do such filtering, but a developer might.)

    > Bill Dunlap
    > Spotfire, TIBCO Software
    > wdunlap tibco.com


    >> -----Original Message-----
    >> From: r-devel-bounces at r-project.org [mailto:r-devel-bounces at r-project.org] On Behalf
    >> Of Mark.Bravington at csiro.au
    >> Sent: Thursday, March 29, 2012 6:30 PM
    >> Cc: r-devel at stat.math.ethz.ch
    >> Subject: Re: [Rd] CRAN policies
    >> 
    >> I'm concerned this thread is heading the wrong way, towards techno-fixes for imaginary
    >> problems. R package-building is already encumbered with a huge set of complicated
    >> rules, and more instructions/rules eg for metadata would make things worse not better.
    >> 
    >> 
    >> RCMD CHECK on the 'mvbutils' package generates over 300 Notes about "no visible
    >> binding...", which inevitably I just ignore. They arise because RCMD CHECK is too "stupid"
    >> to understand one of my preferred coding idioms (I'm not going to explain what-- that's
    >> beside the point). And RCMD CHECK always will be too "stupid" to understand everything
    >> that a rich language like R might quite reasonably cause experienced coders to do.
    >> 
    >> It should not be CRAN's business how I write my code, or even whether my code does
    >> what it is supposed to. It might be CRAN's business to try to work out whether my code
    >> breaks CRAN's policies, eg by causing R to crash horribly-- that's presumably what
    >> Warnings are for (but see below). And maybe there could be circumstances where an
    >> automatic check might be "worried" enough to alert the CRANia and require manual
    >> explanation and emails etc from a developer, but even that seems doomed given the
    >> growing deluge of packages.
    >> 
    >> RCMD CHECK currently functions both as a "sanitizer" for CRAN, and as a developer-tool.
    >> But the fact that the one programl does both things seems accidental to me, and I think
    >> this dual-use is muddying the discussion. There's a big distinction between (i) code-checks
    >> that developers themselves might or might not find useful-- which should be left to the
    >> developer, and will vary from person to person-- and (ii) code-checks that CRAN enforces
    >> for its own peace-of-mind. Maybe it's convenient to have both functions in the same
    >> place, and it'd be fine to use Notes for one and Warnings for the other, but the different
    >> purposes should surely be kept clear.
    >> 
    >> Personally, in building over 10 packages (only 2 on CRAN), I haven't found RCMD CHECK
    >> to be of any use, except for the code-documentation and example-running bits. I know
    >> other people have different opinions, but that's the point: one-size-does-not-fit-all when
    >> it comes to coding tools.
    >> 
    >> And wrto the Warnings themselves: I feel compelled to point out that it's logically
    >> impossible to fully check whether R code will do bad things. One has to wonder at what
    >> point adding new checks becomes futile or counterproductive. There must be over 2000
    >> people who have written CRAN packages by now; every extra check and non-back-
    >> compatible additional requirement runs the risk of generating false-negatives and
    >> incurring many extra person-hours to "fix" non-problems. Plus someone needs to
    >> document and explain the check (adding to the rule mountain), plus there is the time
    >> spent in discussions like this..!
    >> 
    >> Mark
    >> 
    >> Mark Bravington
    >> CSIRO CMIS
    >> Marine Lab
    >> Hobart
    >> Australia
    >> ________________________________________
    >> From: r-devel-bounces at r-project.org [r-devel-bounces at r-project.org] On Behalf Of
    >> Hadley Wickham [hadley at rice.edu]
    >> Sent: 30 March 2012 07:42
    >> To: William Dunlap
    >> Cc: r-devel at stat.math.ethz.ch; Spencer Graves
    >> Subject: Re: [Rd] CRAN policies
    >> 
    >> > Most of that stuff is already in codetools, at least when it is checking functions
    >> > with checkUsage().  E.g., arguments of ~ are not checked.  The  expr argument
    >> > to with() will not be checked if you add  skipWith=FALSE to the call to checkUsage.
    >> >
    >> >  > library(codetools)
    >> >
    >> >  > checkUsage(function(dataFrame) with(dataFrame, {Num/Den ; Resp ~ Pred}))
    >> >  <anonymous>: no visible binding for global variable 'Num' (:1)
    >> >  <anonymous>: no visible binding for global variable 'Den' (:1)
    >> >
    >> >  > checkUsage(function(dataFrame) with(dataFrame, {Num/Den ; Resp ~ Pred}),
    >> skipWith=TRUE)
    >> >
    >> >  > checkUsage(function(dataFrame) with(DataFrame, {Num/Den ; Resp ~ Pred}),
    >> skipWith=TRUE)
    >> >  <anonymous>: no visible binding for global variable 'DataFrame'
    >> >
    >> > The only part that I don't see is the mechanism to add code-walker functions to
    >> > the environment in codetools that has the standard list of them for functions with
    >> > nonstandard evaluation:
    >> >  > objects(codetools:::collectUsageHandlers, all=TRUE)
    >> >   [1] "$"             "$<-"           ".Internal"
    >> >   [4] "::"            ":::"           "@"
    >> >   [7] "@<-"           "{"             "~"
    >> >  [10] "<-"            "<<-"           "="
    >> >  [13] "assign"        "binomial"      "bquote"
    >> >  [16] "data"          "detach"        "expression"
    >> >  [19] "for"           "function"      "Gamma"
    >> >  [22] "gaussian"      "if"            "library"
    >> >  [25] "local"         "poisson"       "quasi"
    >> >  [28] "quasibinomial" "quasipoisson"  "quote"
    >> >  [31] "Quote"         "require"       "substitute"
    >> >  [34] "with"
    >> 
    >> It seems like we really need a standard way to add metadata to functions:
    >> 
    >> attr(with, "special_args") <- "expr"
    >> attr(lm, "special_args") <- c("formula", "weights", "subset")
    >> 
    >> This would be useful because it could automatically contribute to the
    >> documentation.
    >> 
    >> Similarly,
    >> 
    >> attr(my.new.method, "s3method") <- c("my.new", "method")
    >> 
    >> could be useful.
    >> 
    >> Hadley
    >> 
    >> 
    >> --
    >> Assistant Professor / Dobelman Family Junior Chair
    >> Department of Statistics / Rice University
    >> http://had.co.nz/
    >> 
    >> ______________________________________________
    >> R-devel at r-project.org mailing list
    >> https://stat.ethz.ch/mailman/listinfo/r-devel
    >> 
    >> ______________________________________________
    >> R-devel at r-project.org mailing list
    >> https://stat.ethz.ch/mailman/listinfo/r-devel

    > ______________________________________________
    > R-devel at r-project.org mailing list
    > https://stat.ethz.ch/mailman/listinfo/r-devel


From yarikoptic at gmail.com  Tue Apr  3 00:06:14 2012
From: yarikoptic at gmail.com (Yaroslav Halchenko)
Date: Mon, 2 Apr 2012 18:06:14 -0400
Subject: [Rd] R datasets ownership(copyright) and license
Message-ID: <20120402220614.GS22956@onerussian.com>

Dear R Developers,

Recently filed (and dismissed ;) ) law suit by Astrolabe against tz
database developers caused a lot of media-press and discussions and
created some kind of precedence in the USA [3].  But also it imho showed
that similar attacks might happen in the future, and possibly against
data sets which are not that obviously "factual" thus after all might
fall under copyright or IP protection if not in the states then in
some other jurisdictions.

And 'data copyright/license' question comes over and over again, I just
wanted to ask based on  what policies or advisories datasets were
selected to be shipped with R.   From a very very brief look at the
datasets, many of them appear to be factual data, thus at least at the
moment probably are not copyrightable in the states -- but is there
guarantee that they are not protected by copyright elsewhere if their
origin abroad?   But some seems to come from published works (still)
under copyright with "All rights reserved", e.g. datasets Harman23
and Harman74 [4].

Although similar question to mine was raised before [e.g. 1,2] I
have not found a straight answer e.g. from a list above or a mix of
them:

1. we simply did not look into it and adopted them with idea that if
   someone complains -- we remove corresponding pieces

2. we considered all datasets factual data thus not copyrightable (in
   USA? around the globe?)

3. for each (or some or majority) dataset we did collected information
   on possible copyright+license/IP holder and contacted them where
   unclear about the permission for reuse in a project under GPL license

Thank you in advance for the clarification!

P.S. Please do not take me wrong -- I am not trying to pick at
anyone.  I just wanted to get a better sense on the
procedures/assumptions R developers use while adopting data for the R
package, so that it could be of help for other projects.

[1] https://stat.ethz.ch/pipermail/r-help/2007-April/130422.html
[2] http://www.mail-archive.com/r-help at r-project.org/msg62486.html
[3] http://en.wikipedia.org/wiki/Tz_database
[4] it is interesting there that actual data comes from "unpublished PhD
    thesis", but once again from the U of Chicago who holds copyright
    for the book itself.

-- 
Yaroslav O. Halchenko
Postdoctoral Fellow,   Department of Psychological and Brain Sciences
Dartmouth College, 419 Moore Hall, Hinman Box 6207, Hanover, NH 03755
Phone: +1 (603) 646-9834                       Fax: +1 (603) 646-1419
WWW:   http://www.linkedin.com/in/yarik


From claudia.beleites at ipht-jena.de  Tue Apr  3 21:03:53 2012
From: claudia.beleites at ipht-jena.de (Claudia Beleites)
Date: Tue, 3 Apr 2012 21:03:53 +0200
Subject: [Rd] R datasets ownership(copyright) and license
In-Reply-To: <20120402220614.GS22956@onerussian.com>
References: <20120402220614.GS22956@onerussian.com>
Message-ID: <4F7B4999.3000309@ipht-jena.de>

Yaroslav,

coming from an experimental field, I use options 4 and 4a:

4. I measure the data myself, so I am the copyright holder.
4a. I publish data sets that are given to me in order to publish by the
person(s) who did the measurement. This is properly annotated in the
authors field.

So far, the data sets I put as example data into packages are small
subsets of real studies or data collected in pre-tests, so they are not
that sensitive/valuable. I plan to publish at least one "real" data set
(as own package) eventually. But we're not yet there.

Claudia




Am 03.04.2012 00:06, schrieb Yaroslav Halchenko:
> Dear R Developers,
> 
> Recently filed (and dismissed ;) ) law suit by Astrolabe against tz
> database developers caused a lot of media-press and discussions and
> created some kind of precedence in the USA [3].  But also it imho showed
> that similar attacks might happen in the future, and possibly against
> data sets which are not that obviously "factual" thus after all might
> fall under copyright or IP protection if not in the states then in
> some other jurisdictions.
> 
> And 'data copyright/license' question comes over and over again, I just
> wanted to ask based on  what policies or advisories datasets were
> selected to be shipped with R.   From a very very brief look at the
> datasets, many of them appear to be factual data, thus at least at the
> moment probably are not copyrightable in the states -- but is there
> guarantee that they are not protected by copyright elsewhere if their
> origin abroad?   But some seems to come from published works (still)
> under copyright with "All rights reserved", e.g. datasets Harman23
> and Harman74 [4].
> 
> Although similar question to mine was raised before [e.g. 1,2] I
> have not found a straight answer e.g. from a list above or a mix of
> them:
> 
> 1. we simply did not look into it and adopted them with idea that if
>    someone complains -- we remove corresponding pieces
> 
> 2. we considered all datasets factual data thus not copyrightable (in
>    USA? around the globe?)
> 
> 3. for each (or some or majority) dataset we did collected information
>    on possible copyright+license/IP holder and contacted them where
>    unclear about the permission for reuse in a project under GPL license
> 
> Thank you in advance for the clarification!
> 
> P.S. Please do not take me wrong -- I am not trying to pick at
> anyone.  I just wanted to get a better sense on the
> procedures/assumptions R developers use while adopting data for the R
> package, so that it could be of help for other projects.
> 
> [1] https://stat.ethz.ch/pipermail/r-help/2007-April/130422.html
> [2] http://www.mail-archive.com/r-help at r-project.org/msg62486.html
> [3] http://en.wikipedia.org/wiki/Tz_database
> [4] it is interesting there that actual data comes from "unpublished PhD
>     thesis", but once again from the U of Chicago who holds copyright
>     for the book itself.
> 


-- 
Claudia Beleites
Spectroscopy/Imaging
Institute of Photonic Technology
Albert-Einstein-Str. 9
07745 Jena
Germany

email: claudia.beleites at ipht-jena.de
phone: +49 3641 206-133
fax:   +49 2641 206-399


From hadley at rice.edu  Tue Apr  3 23:00:58 2012
From: hadley at rice.edu (Hadley Wickham)
Date: Tue, 3 Apr 2012 16:00:58 -0500
Subject: [Rd] R datasets ownership(copyright) and license
In-Reply-To: <20120402220614.GS22956@onerussian.com>
References: <20120402220614.GS22956@onerussian.com>
Message-ID: <CABdHhvHDngqDKR-nwhtJ=qFQGO0vZKf3z+-bXCHk=qa=0tBD4g@mail.gmail.com>

> 2. we considered all datasets factual data thus not copyrightable (in
> ? USA? around the globe?)

This is definitely true in the US, but not true globally.  I have no
idea under which jurisdiction a lawsuit would apply.

Hadley

-- 
Assistant Professor / Dobelman Family Junior Chair
Department of Statistics / Rice University
http://had.co.nz/


From spencer.graves at prodsyse.com  Tue Apr  3 23:19:02 2012
From: spencer.graves at prodsyse.com (Spencer Graves)
Date: Tue, 03 Apr 2012 14:19:02 -0700
Subject: [Rd] R datasets ownership(copyright) and license
In-Reply-To: <CABdHhvHDngqDKR-nwhtJ=qFQGO0vZKf3z+-bXCHk=qa=0tBD4g@mail.gmail.com>
References: <20120402220614.GS22956@onerussian.com>
	<CABdHhvHDngqDKR-nwhtJ=qFQGO0vZKf3z+-bXCHk=qa=0tBD4g@mail.gmail.com>
Message-ID: <4F7B6946.9080202@prodsyse.com>

On 4/3/2012 2:00 PM, Hadley Wickham wrote:
>> 2. we considered all datasets factual data thus not copyrightable (in
>>    USA? around the globe?)
> This is definitely true in the US, but not true globally.  I have no
> idea under which jurisdiction a lawsuit would apply.


       I'd be careful with the word "definitely".  The major media 
conglomerates and their industry associations have successfully 
destroyed competition to their hegemony in many areas.  For example, 
they sued college students for close to $100 billion, because their 
improvements of search engines made it easier for people in a university 
intranet to find copyrighted music placed by others in their "public" 
folder.  They successfully sued lawyers who advised MP3 that they had 
reasonable grounds to believe what they did would be legal and Venture 
Capitalists who funded Napster.  In each case, they won not on the law 
but on the fact that they had larger budgets for lawyers.  See Lessig 
(2004) Free Culture [book available from Amazon and also for free under 
the Creative Commons license;  see Wikipedia, "Free Culture (book), 
"http://en.wikipedia.org/wiki/Free_Culture_(book) 
<http://en.wikipedia.org/wiki/Free_Culture_%28book%29>"].


       Spencer Graves
>
> Hadley


From spencer.graves at structuremonitoring.com  Tue Apr  3 23:22:11 2012
From: spencer.graves at structuremonitoring.com (Spencer Graves)
Date: Tue, 03 Apr 2012 14:22:11 -0700
Subject: [Rd] R datasets ownership(copyright) and license
In-Reply-To: <CABdHhvHDngqDKR-nwhtJ=qFQGO0vZKf3z+-bXCHk=qa=0tBD4g@mail.gmail.com>
References: <20120402220614.GS22956@onerussian.com>
	<CABdHhvHDngqDKR-nwhtJ=qFQGO0vZKf3z+-bXCHk=qa=0tBD4g@mail.gmail.com>
Message-ID: <4F7B6A03.9070907@structuremonitoring.com>

On 4/3/2012 2:00 PM, Hadley Wickham wrote:
>> 2. we considered all datasets factual data thus not copyrightable (in
>>    USA? around the globe?)
> This is definitely true in the US, but not true globally.  I have no
> idea under which jurisdiction a lawsuit would apply.


       I'd be careful with the word "definitely".  The major media 
conglomerates and their industry associations have successfully 
destroyed competition to their hegemony in many areas.  For example, 
they sued college students for close to $100 billion, because their 
improvements of search engines made it easier for people in a university 
intranet to find copyrighted music placed by others in their "public" 
folder.  They successfully sued lawyers who advised MP3 that they had 
reasonable grounds to believe what they did would be legal and Venture 
Capitalists who funded Napster.  In each case, they won not on the law 
but on the fact that they had larger budgets for lawyers.  See Lessig 
(2004) Free Culture [book available from Amazon and also for free under 
the Creative Commons license;  see Wikipedia, "Free Culture (book), 
"http://en.wikipedia.org/wiki/Free_Culture_(book) 
<http://en.wikipedia.org/wiki/Free_Culture_%28book%29>"].


       Spencer Graves
>
> Hadley


-- 
Spencer Graves, PE, PhD
President and Chief Technology Officer
Structure Inspection and Monitoring, Inc.
751 Emerson Ct.
San Jos?, CA 95126
ph:  408-655-4567
web:  www.structuremonitoring.com


From r.ted.byers at gmail.com  Wed Apr  4 00:03:31 2012
From: r.ted.byers at gmail.com (Ted Byers)
Date: Tue, 3 Apr 2012 18:03:31 -0400
Subject: [Rd] R datasets ownership(copyright) and license
In-Reply-To: <CABdHhvHDngqDKR-nwhtJ=qFQGO0vZKf3z+-bXCHk=qa=0tBD4g@mail.gmail.com>
References: <20120402220614.GS22956@onerussian.com>
	<CABdHhvHDngqDKR-nwhtJ=qFQGO0vZKf3z+-bXCHk=qa=0tBD4g@mail.gmail.com>
Message-ID: <179d01cd11e5$9c1f1c80$d45d5580$@gmail.com>

> -----Original Message-----
> From: r-devel-bounces at r-project.org [mailto:r-devel-bounces at r-project.org]
> On Behalf Of Hadley Wickham
> Sent: April-03-12 5:01 PM
> To: r-devel at r-project.org; pystatsmodels at googlegroups.com; Dirk
> Eddelbuettel
> Subject: Re: [Rd] R datasets ownership(copyright) and license
> 
> > 2. we considered all datasets factual data thus not copyrightable (in
> > ? USA? around the globe?)
> 
> This is definitely true in the US, but not true globally.  I have no idea
under
> which jurisdiction a lawsuit would apply.
> 
> Hadley

Why worry about jurisdictions in which you neither work nor live?  

I would expect such rationality (factual data not being copyrightable) in
the US, Canada, Europe and Australasia, so they're not likely an issue.  And
I doubt any such country would try to impose their laws on someone living
and working elsewhere.  In many parts of Asia, where I have lived and worked
at least, copyright violation is rampant, and the perpetrators face no real
consequences; at least none I could see.  

As for banana republics, such as many countries in the Muslim world, like
Iran, I really don't care what their laws have to say.  They do have a
history of trying to impose their lunacy on the rest of the world (as
illustrated in the death threats from Muslim religious authorities against
the Danish cartoonists or Salmon Rushdie).  There are early histories of the
Byzantine empire, that are about as factual as such documents of that age
can be, but publishing some of  them in a Muslim country could get you
killed because they record 'crimes' committed by 'Muslim hordes'.  There
were riots in many Muslim countries not so long ago just because the Pope at
the time quoted one of those histories.  I can't imagine any western
democracy helping some back-water banana republic to impose sharia law, or
any other madness masquerading as law, on its own citizens.  I don't believe
this due to an irrational belief in the benevolence of such governments but
rather because I expect they would take a dim view of such egregious
extra-territoriality that such banana republics would be attempting.

My point is simply that if I have data, or other information, that I want to
publish, and Canadian law says it is not copyrightable because it is factual
data, then great; and if some back-water banana republic objects then I'd be
quite happy to tell them were to go and what to do with themselves when they
get there.   As a Canadian citizen living and working in Canada, only
Canadian law applies to me.  If Canadian law tells me that X is merely
factual data and thus not copyrightable, then that is enough.

Cheers

Ted


From yarikoptic at gmail.com  Wed Apr  4 00:19:07 2012
From: yarikoptic at gmail.com (Yaroslav Halchenko)
Date: Tue, 3 Apr 2012 18:19:07 -0400
Subject: [Rd] R datasets ownership(copyright) and license
In-Reply-To: <4F7B6946.9080202@prodsyse.com>
References: <20120402220614.GS22956@onerussian.com>
	<CABdHhvHDngqDKR-nwhtJ=qFQGO0vZKf3z+-bXCHk=qa=0tBD4g@mail.gmail.com>
	<4F7B6946.9080202@prodsyse.com>
Message-ID: <20120403221907.GL22869@onerussian.com>

I somewhat agree with Spencer -- as I have mentioned, the recent precedence
with tz database shows that such claims would not be taken as ungrounded right
away and things could easily go all the way to court -- and that might be a
really costly endeavor regardless who is right or wrong.  Proving that
data is factual, and not fictional/creative/original might be another challenge
in quite a few cases I bet.

While searching for more information -- I found IMHO a very nice (although a
bit dated) summary: http://www.bitlaw.com/copyright/database.html which,
if we talk about abroad-of-USA summarizes nicely: 

"sui generis right that prohibits the extraction or reutilization of any
database in which there has been a substantial investment in either obtaining,
verification, or presentation of the data contents. Under this second right,
there is no requirement for creativity or originality."

so -- I would be especially careful with data from EU ;-)

on the other hand above link clarifies to me that it is ok to claim a copyright
(e.g.  as it is in R) on the collection of factual unprotected (still unsure if
that is the case with R datasets) data.

On Tue, 03 Apr 2012, Spencer Graves wrote:
> On 4/3/2012 2:00 PM, Hadley Wickham wrote:
> >>2. we considered all datasets factual data thus not copyrightable (in
> >>   USA? around the globe?)
> >This is definitely true in the US, but not true globally.  I have no
> >idea under which jurisdiction a lawsuit would apply.

>       I'd be careful with the word "definitely".  The major media
> conglomerates and their industry associations have successfully
> destroyed competition to their hegemony in many areas.  For example,
> they sued college students for close to $100 billion, because their
> improvements of search engines made it easier for people in a
> university intranet to find copyrighted music placed by others in
> their "public" folder.  They successfully sued lawyers who advised
> MP3 that they had reasonable grounds to believe what they did would
> be legal and Venture Capitalists who funded Napster.  In each case,
> they won not on the law but on the fact that they had larger budgets
> for lawyers.  See Lessig (2004) Free Culture [book available from
> Amazon and also for free under the Creative Commons license;  see
> Wikipedia, "Free Culture (book),
> "http://en.wikipedia.org/wiki/Free_Culture_(book)
> <http://en.wikipedia.org/wiki/Free_Culture_%28book%29>"].


>       Spencer Graves

> >Hadley

> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


-- 
=------------------------------------------------------------------=
Keep in touch                                     www.onerussian.com
Yaroslav Halchenko                 www.ohloh.net/accounts/yarikoptic


From hadley at rice.edu  Wed Apr  4 00:36:30 2012
From: hadley at rice.edu (Hadley Wickham)
Date: Tue, 3 Apr 2012 17:36:30 -0500
Subject: [Rd] R datasets ownership(copyright) and license
In-Reply-To: <179d01cd11e5$9c1f1c80$d45d5580$@gmail.com>
References: <20120402220614.GS22956@onerussian.com>
	<CABdHhvHDngqDKR-nwhtJ=qFQGO0vZKf3z+-bXCHk=qa=0tBD4g@mail.gmail.com>
	<179d01cd11e5$9c1f1c80$d45d5580$@gmail.com>
Message-ID: <CABdHhvGE2TGYRTq+1YLG1YB29DZXk+PotcoOJ-sS1dnXfJPnwg@mail.gmail.com>

> I would expect such rationality (factual data not being copyrightable) in
> the US, Canada, Europe and Australasia, so they're not likely an issue. ?And
> I doubt any such country would try to impose their laws on someone living
> and working elsewhere. ?In many parts of Asia, where I have lived and worked
> at least, copyright violation is rampant, and the perpetrators face no real
> consequences; at least none I could see.

My understanding is that rationality is not the case in Europe - see
e.g. http://en.wikipedia.org/wiki/Database_Directive.

Hadley


-- 
Assistant Professor / Dobelman Family Junior Chair
Department of Statistics / Rice University
http://had.co.nz/


From hadley at rice.edu  Wed Apr  4 00:37:40 2012
From: hadley at rice.edu (Hadley Wickham)
Date: Tue, 3 Apr 2012 17:37:40 -0500
Subject: [Rd] R datasets ownership(copyright) and license
In-Reply-To: <20120403221907.GL22869@onerussian.com>
References: <20120402220614.GS22956@onerussian.com>
	<CABdHhvHDngqDKR-nwhtJ=qFQGO0vZKf3z+-bXCHk=qa=0tBD4g@mail.gmail.com>
	<4F7B6946.9080202@prodsyse.com> <20120403221907.GL22869@onerussian.com>
Message-ID: <CABdHhvEc_yM_iw-25VCasvf9CqqvHvp9BVfOvK1i0m=5ipj=rg@mail.gmail.com>

> I somewhat agree with Spencer -- as I have mentioned, the recent precedence
> with tz database shows that such claims would not be taken as ungrounded right
> away and things could easily go all the way to court -- and that might be a
> really costly endeavor regardless who is right or wrong. ?Proving that
> data is factual, and not fictional/creative/original might be another challenge
> in quite a few cases I bet.

I think it's generally easy to tell if something is a fact or not, and
I doubt any of the datasets in R are fictional.

Hadley


-- 
Assistant Professor / Dobelman Family Junior Chair
Department of Statistics / Rice University
http://had.co.nz/


From yarikoptic at gmail.com  Wed Apr  4 00:46:50 2012
From: yarikoptic at gmail.com (Yaroslav Halchenko)
Date: Tue, 3 Apr 2012 18:46:50 -0400
Subject: [Rd] R datasets ownership(copyright) and license
In-Reply-To: <CABdHhvEc_yM_iw-25VCasvf9CqqvHvp9BVfOvK1i0m=5ipj=rg@mail.gmail.com>
References: <20120402220614.GS22956@onerussian.com>
	<CABdHhvHDngqDKR-nwhtJ=qFQGO0vZKf3z+-bXCHk=qa=0tBD4g@mail.gmail.com>
	<4F7B6946.9080202@prodsyse.com>
	<20120403221907.GL22869@onerussian.com>
	<CABdHhvEc_yM_iw-25VCasvf9CqqvHvp9BVfOvK1i0m=5ipj=rg@mail.gmail.com>
Message-ID: <20120403224650.GN22869@onerussian.com>

;-)  Let's check where factual ends and fictional/personal/etc starts
and how easy to tell.

Are survey data asking for answers to specifically crafted original
questions (i.e. not just age/race/etc) factual? e.g.

\title{The Chatterjee--Price Attitude Data}
\description{
  From a survey of the clerical employees of a large financial
  organization, the data are aggregated from the questionnaires of the
  approximately 35 employees for each of 30 (randomly selected)
  departments.  The numbers give the percent proportion of favourable
  responses to seven questions in each department.}
\usage{attitude}

?

On Tue, 03 Apr 2012, Hadley Wickham wrote:

> > I somewhat agree with Spencer -- as I have mentioned, the recent precedence
> > with tz database shows that such claims would not be taken as ungrounded right
> > away and things could easily go all the way to court -- and that might be a
> > really costly endeavor regardless who is right or wrong. ?Proving that
> > data is factual, and not fictional/creative/original might be another challenge
> > in quite a few cases I bet.

> I think it's generally easy to tell if something is a fact or not, and
> I doubt any of the datasets in R are fictional.

> Hadley
-- 
=------------------------------------------------------------------=
Keep in touch                                     www.onerussian.com
Yaroslav Halchenko                 www.ohloh.net/accounts/yarikoptic


From hadley at rice.edu  Wed Apr  4 00:55:24 2012
From: hadley at rice.edu (Hadley Wickham)
Date: Tue, 3 Apr 2012 17:55:24 -0500
Subject: [Rd] R datasets ownership(copyright) and license
In-Reply-To: <20120403224650.GN22869@onerussian.com>
References: <20120402220614.GS22956@onerussian.com>
	<CABdHhvHDngqDKR-nwhtJ=qFQGO0vZKf3z+-bXCHk=qa=0tBD4g@mail.gmail.com>
	<4F7B6946.9080202@prodsyse.com> <20120403221907.GL22869@onerussian.com>
	<CABdHhvEc_yM_iw-25VCasvf9CqqvHvp9BVfOvK1i0m=5ipj=rg@mail.gmail.com>
	<20120403224650.GN22869@onerussian.com>
Message-ID: <CABdHhvH0fYg1hQgv2ZjmKWZmopBQPprMULqgzk2XrSRiHXfjRQ@mail.gmail.com>

On Tue, Apr 3, 2012 at 5:46 PM, Yaroslav Halchenko <yarikoptic at gmail.com> wrote:
> ;-) ?Let's check where factual ends and fictional/personal/etc starts
> and how easy to tell.
>
> Are survey data asking for answers to specifically crafted original
> questions (i.e. not just age/race/etc) factual? e.g.
>
> \title{The Chatterjee--Price Attitude Data}
> \description{
> ?From a survey of the clerical employees of a large financial
> ?organization, the data are aggregated from the questionnaires of the
> ?approximately 35 employees for each of 30 (randomly selected)
> ?departments. ?The numbers give the percent proportion of favourable
> ?responses to seven questions in each department.}
> \usage{attitude}

I don't see how their could be any confusion here - it is a fact
whether or not someone made a favourable response to a question.  I
agree that there might be murky areas, but I don't think this is one.

Hadley

-- 
Assistant Professor / Dobelman Family Junior Chair
Department of Statistics / Rice University
http://had.co.nz/


From spencer.graves at structuremonitoring.com  Wed Apr  4 03:58:05 2012
From: spencer.graves at structuremonitoring.com (Spencer Graves)
Date: Tue, 03 Apr 2012 18:58:05 -0700
Subject: [Rd] R datasets ownership(copyright) and license
In-Reply-To: <CABdHhvH0fYg1hQgv2ZjmKWZmopBQPprMULqgzk2XrSRiHXfjRQ@mail.gmail.com>
References: <20120402220614.GS22956@onerussian.com>
	<CABdHhvHDngqDKR-nwhtJ=qFQGO0vZKf3z+-bXCHk=qa=0tBD4g@mail.gmail.com>
	<4F7B6946.9080202@prodsyse.com>
	<20120403221907.GL22869@onerussian.com>
	<CABdHhvEc_yM_iw-25VCasvf9CqqvHvp9BVfOvK1i0m=5ipj=rg@mail.gmail.com>
	<20120403224650.GN22869@onerussian.com>
	<CABdHhvH0fYg1hQgv2ZjmKWZmopBQPprMULqgzk2XrSRiHXfjRQ@mail.gmail.com>
Message-ID: <4F7BAAAD.3080100@structuremonitoring.com>

On 4/3/2012 3:55 PM, Hadley Wickham wrote:
> On Tue, Apr 3, 2012 at 5:46 PM, Yaroslav Halchenko<yarikoptic at gmail.com>  wrote:
>> ;-)  Let's check where factual ends and fictional/personal/etc starts
>> and how easy to tell.
>>
>> Are survey data asking for answers to specifically crafted original
>> questions (i.e. not just age/race/etc) factual? e.g.
>>
>> \title{The Chatterjee--Price Attitude Data}
>> \description{
>>   From a survey of the clerical employees of a large financial
>>   organization, the data are aggregated from the questionnaires of the
>>   approximately 35 employees for each of 30 (randomly selected)
>>   departments.  The numbers give the percent proportion of favourable
>>   responses to seven questions in each department.}
>> \usage{attitude}
> I don't see how their could be any confusion here - it is a fact
> whether or not someone made a favourable response to a question.  I
> agree that there might be murky areas, but I don't think this is one.


ABUSE OF POWER IN COPYRIGHT LAW


       Lessig (2004) Free Culture "documents how (US) copyright power 
has expanded substantially since 1974 in five critical dimensions:


             * duration (from 32 to 95 years),


             * scope (from publishers to virtually everyone),


             * reach (to every view on a computer),


             * control (including "derivative works" defined so broadly 
that virtually any new content could be sued by some copyright holder as 
a "derivative work" of something), and


             * concentration and integration of the media industry."


[Quote from Wikipedia, "Free Culture (book)";  
"http://en.wikipedia.org/wiki/Free_Culture_(book) 
<http://en.wikipedia.org/wiki/Free_Culture_%28book%29>"]


       As noted earlier, the major media conglomerates have successfully 
used the ambiguities they got written into copyright law to block 
potential competitors and stifle creativity through the credible threats 
of lawsuits.


LIMITS ON ABUSE OF POWER IN COPYRIGHT LAW


       One copyright claim the industry lost (as noted in "Free 
Culture") was an attempt to collect royalties from Girl Scouts for songs 
sung around campfires.  They didn't lose that case in a courtroom -- the 
law still allows them to sue in such cases:  They lost in the court of 
public opinion.


       For data sets in R, I think we need to look at the copyrights 
claimed in the package:  If the copyright says, e.g., GNU GPL, we should 
not worry about it much.  And I agree with Hadley that we should not 
worry much about the datasets published in R packages.


       I'm not an attorney, but I've been told many times that you can 
copyright expression but not ideas -- and certainly not facts.  Thus, 
you can copyright the format of a table of physical constants but not 
the constants themselves nor the relationship described by the 
organization of that table.


       However, the major media industry has demonstrated a capacity to 
sue when they feel their hegemony on public opinion is threatened.  Our 
primary defense is the defense of Gandhi:  Refusing to remain silent -- 
e.g., people making salt in defiance of law saying they couldn't or 
(more recently) Girl Scouts signing in public and refusing to pay 
royalties.


       Best Wishes,
       Spencer
p.s.  The industry got the above extensions to copyright law by piously 
claiming they were needed " To promote the Progress of Science and 
useful Arts, by securing for limited Times to Authors and Inventors the 
exclusive Right to their respective Writings and Discoveries", as it 
says in Article I, Section 8, Clause 8 of the US Constitution 
(http://en.wikipedia.org/wiki/Copyright_law_of_the_United_States).  The 
claims of the industry as it pertains to academic research journals is 
completely bogus, because I have never received a dime for any of the 
technical papers I've written, even though I've been required to assign 
copyrights to some company, whose sole function in the age of the 
Internet is to prevent people from reading my work without paying the 
copyright holder:  This is an obstacle to "the progress of science and 
the useful arts."

>
> Hadley
>


-- 
Spencer Graves, PE, PhD
President and Chief Technology Officer
Structure Inspection and Monitoring, Inc.
751 Emerson Ct.
San Jos?, CA 95126
ph:  408-655-4567
web:  www.structuremonitoring.com


From dusa.adrian at gmail.com  Thu Apr  5 13:03:15 2012
From: dusa.adrian at gmail.com (Adrian Dusa)
Date: Thu, 5 Apr 2012 14:03:15 +0300
Subject: [Rd] "NA" vs. NA
Message-ID: <CAJ=0CtBXsC22CW8_UnyYvsi-w9PJc=iXYQbW6srC0d3VYs0O2Q@mail.gmail.com>

Dear All,

I assume this is an R-devel issue, apologies if I missed something
obvious. I have a dataframe where the row names are country codes,
based on ISO 3166, something like this:

------------
"v1"    "v2"
"UK"    1    2
"NA"    2    3
------------

It happens that "NA" is the country code for "Namibia", and that
creates problems on using this data within a package due to this:

Error in read.table(zfile, header = TRUE, as.is = FALSE) :
  missing values in 'row.names' are not allowed

I realise that NA is reserved in R, but I assumed that when quoted it
would be usable.
For the moment I simply changes the country code, but I wonder if
there's any (other) solution to circumvent this issue.

Thanks very much in advance,
Adrian

-- 
Adrian Dusa
Romanian Social Data Archive
1, Schitu Magureanu Bd.
050025 Bucharest sector 5
Romania
Tel.:+40 21 3126618 \
? ? ? ?+40 21 3120210 / int.101
Fax: +40 21 3158391


From Ted.Harding at wlandres.net  Thu Apr  5 13:22:03 2012
From: Ted.Harding at wlandres.net ( (Ted Harding))
Date: Thu, 05 Apr 2012 12:22:03 +0100 (BST)
Subject: [Rd] "NA" vs. NA
In-Reply-To: <CAJ=0CtBXsC22CW8_UnyYvsi-w9PJc=iXYQbW6srC0d3VYs0O2Q@mail.gmail.com>
Message-ID: <XFMail.20120405122203.Ted.Harding@wlandres.net>

On 05-Apr-2012 11:03:15 Adrian Dusa wrote:
> Dear All,
> 
> I assume this is an R-devel issue, apologies if I missed something
> obvious. I have a dataframe where the row names are country codes,
> based on ISO 3166, something like this:
> 
> ------------
> "v1"    "v2"
> "UK"    1    2
> "NA"    2    3
> ------------
> 
> It happens that "NA" is the country code for "Namibia", and that
> creates problems on using this data within a package due to this:
> 
> Error in read.table(zfile, header = TRUE, as.is = FALSE) :
>   missing values in 'row.names' are not allowed
> 
> I realise that NA is reserved in R, but I assumed that when quoted it
> would be usable.
> For the moment I simply changes the country code, but I wonder if
> there's any (other) solution to circumvent this issue.
> 
> Thanks very much in advance,
> Adrian

Hi Adrian,
The default in read.table() for the "na.strings" parameter is

  na.strings = "NA"

So, provided you have no "NA" in the data portion of your file
(or e.g. any missing values are simply blank) you could use
something like:

read.table(zfile, header = TRUE, as.is = FALSE, na.strings="OOPS")

which should avoid the problem.

Hoping this helps,
Ted.

-------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at wlandres.net>
Date: 05-Apr-2012  Time: 12:21:57
This message was sent by XFMail


From dusa.adrian at gmail.com  Thu Apr  5 13:27:01 2012
From: dusa.adrian at gmail.com (Adrian Dusa)
Date: Thu, 5 Apr 2012 14:27:01 +0300
Subject: [Rd] "NA" vs. NA
In-Reply-To: <XFMail.20120405122203.Ted.Harding@wlandres.net>
References: <CAJ=0CtBXsC22CW8_UnyYvsi-w9PJc=iXYQbW6srC0d3VYs0O2Q@mail.gmail.com>
	<XFMail.20120405122203.Ted.Harding@wlandres.net>
Message-ID: <CAJ=0CtAur8KCFNx+bWoj_-sM6q17eW+Giki1HgqPJh=z_jgzYA@mail.gmail.com>

Hi Ted,

On Thu, Apr 5, 2012 at 14:22, Ted Harding <Ted.Harding at wlandres.net> wrote:
> On 05-Apr-2012 11:03:15 Adrian Dusa wrote:
>> [...]
>
> Hi Adrian,
> The default in read.table() for the "na.strings" parameter is
>
> ?na.strings = "NA"
>
> So, provided you have no "NA" in the data portion of your file
> (or e.g. any missing values are simply blank) you could use
> something like:
>
> read.table(zfile, header = TRUE, as.is = FALSE, na.strings="OOPS")

Sure, that solves the problem on reading the data from R, but the
point was it "creates problems on using this data within a package".

The error is thrown by:
R CMD check --as-cran /path/to/my/package

(and I have no direct control over it...)

Best,
Adrian

-- 
Adrian Dusa
Romanian Social Data Archive
1, Schitu Magureanu Bd.
050025 Bucharest sector 5
Romania
Tel.:+40 21 3126618 \
? ? ? ?+40 21 3120210 / int.101
Fax: +40 21 3158391


From murdoch.duncan at gmail.com  Thu Apr  5 13:49:53 2012
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Thu, 05 Apr 2012 07:49:53 -0400
Subject: [Rd] "NA" vs. NA
In-Reply-To: <CAJ=0CtAur8KCFNx+bWoj_-sM6q17eW+Giki1HgqPJh=z_jgzYA@mail.gmail.com>
References: <CAJ=0CtBXsC22CW8_UnyYvsi-w9PJc=iXYQbW6srC0d3VYs0O2Q@mail.gmail.com>
	<XFMail.20120405122203.Ted.Harding@wlandres.net>
	<CAJ=0CtAur8KCFNx+bWoj_-sM6q17eW+Giki1HgqPJh=z_jgzYA@mail.gmail.com>
Message-ID: <4F7D86E1.9070601@gmail.com>

On 12-04-05 7:27 AM, Adrian Dusa wrote:
> Hi Ted,
>
> On Thu, Apr 5, 2012 at 14:22, Ted Harding<Ted.Harding at wlandres.net>  wrote:
>> On 05-Apr-2012 11:03:15 Adrian Dusa wrote:
>>> [...]
>>
>> Hi Adrian,
>> The default in read.table() for the "na.strings" parameter is
>>
>>   na.strings = "NA"
>>
>> So, provided you have no "NA" in the data portion of your file
>> (or e.g. any missing values are simply blank) you could use
>> something like:
>>
>> read.table(zfile, header = TRUE, as.is = FALSE, na.strings="OOPS")
>
> Sure, that solves the problem on reading the data from R, but the
> point was it "creates problems on using this data within a package".
>
> The error is thrown by:
> R CMD check --as-cran /path/to/my/package
>
> (and I have no direct control over it...)
>
> Best,
> Adrian
>

You still haven't explained what you are doing to cause the problem, so 
I'll guess that you have this file in your data directory of the 
package, in tab-delimited form.

Just store it in some other form, e.g. as a binary object to be loaded.

Duncan Murdoch


From dusa.adrian at gmail.com  Thu Apr  5 14:26:13 2012
From: dusa.adrian at gmail.com (Adrian Dusa)
Date: Thu, 5 Apr 2012 15:26:13 +0300
Subject: [Rd] "NA" vs. NA
In-Reply-To: <4F7D86E1.9070601@gmail.com>
References: <CAJ=0CtBXsC22CW8_UnyYvsi-w9PJc=iXYQbW6srC0d3VYs0O2Q@mail.gmail.com>
	<XFMail.20120405122203.Ted.Harding@wlandres.net>
	<CAJ=0CtAur8KCFNx+bWoj_-sM6q17eW+Giki1HgqPJh=z_jgzYA@mail.gmail.com>
	<4F7D86E1.9070601@gmail.com>
Message-ID: <CAJ=0CtD-C6Gh2Mn9tojWUGrciDLaPZkyuBDWeOyx1YhBTRJjfw@mail.gmail.com>

On Thu, Apr 5, 2012 at 14:49, Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
> On 12-04-05 7:27 AM, Adrian Dusa wrote:
>>[...]
>
> You still haven't explained what you are doing to cause the problem, so I'll
> guess that you have this file in your data directory of the package, in
> tab-delimited form.
>
> Just store it in some other form, e.g. as a binary object to be loaded.

Indeed, this is exactly what I did (tab-delimited file in the data
directory) and yes, this solves the issue.
Thanks very much,
Adrian

-- 
Adrian Dusa
Romanian Social Data Archive
1, Schitu Magureanu Bd.
050025 Bucharest sector 5
Romania
Tel.:+40 21 3126618 \
? ? ? ?+40 21 3120210 / int.101
Fax: +40 21 3158391


From hadley at rice.edu  Thu Apr  5 15:08:59 2012
From: hadley at rice.edu (Hadley Wickham)
Date: Thu, 5 Apr 2012 08:08:59 -0500
Subject: [Rd] R datasets ownership(copyright) and license
In-Reply-To: <A4E5A0B016B8CB41A485FC629B633CED39C06B75A2@GOLD.corp.lgc-group.com>
References: <20120402220614.GS22956@onerussian.com>
	<CABdHhvHDngqDKR-nwhtJ=qFQGO0vZKf3z+-bXCHk=qa=0tBD4g@mail.gmail.com>
	<179d01cd11e5$9c1f1c80$d45d5580$@gmail.com>
	<CABdHhvGE2TGYRTq+1YLG1YB29DZXk+PotcoOJ-sS1dnXfJPnwg@mail.gmail.com>
	<A4E5A0B016B8CB41A485FC629B633CED39C06B75A2@GOLD.corp.lgc-group.com>
Message-ID: <CABdHhvGW=9UK-VT=g+eqv=Sxgd1ikSs0vjQZJyoLBM8wPELX1A@mail.gmail.com>

>
> And you need not look so far afield for that particular lack of rationality. In the US, databases are covered by the Database and Collections of Information Misappropriation Act of 2003 (http://www.govtrack.us/congress/bills/108/hr3261) which says almost exactly the same thing; that a database that took a lot of time and effort to collate is protected against reproduction 'in commerce' without authorisation.

That bill died: http://www.govtrack.us/congress/bills/108/hr3261

Hadley

-- 
Assistant Professor / Dobelman Family Junior Chair
Department of Statistics / Rice University
http://had.co.nz/


From kevin.r.coombes at gmail.com  Thu Apr  5 16:04:49 2012
From: kevin.r.coombes at gmail.com (Kevin R. Coombes)
Date: Thu, 05 Apr 2012 09:04:49 -0500
Subject: [Rd] "NA" vs. NA
In-Reply-To: <CAJ=0CtD-C6Gh2Mn9tojWUGrciDLaPZkyuBDWeOyx1YhBTRJjfw@mail.gmail.com>
References: <CAJ=0CtBXsC22CW8_UnyYvsi-w9PJc=iXYQbW6srC0d3VYs0O2Q@mail.gmail.com>
	<XFMail.20120405122203.Ted.Harding@wlandres.net>
	<CAJ=0CtAur8KCFNx+bWoj_-sM6q17eW+Giki1HgqPJh=z_jgzYA@mail.gmail.com>
	<4F7D86E1.9070601@gmail.com>
	<CAJ=0CtD-C6Gh2Mn9tojWUGrciDLaPZkyuBDWeOyx1YhBTRJjfw@mail.gmail.com>
Message-ID: <4F7DA681.3000705@gmail.com>

Change the "na.strings" argument to read.table or read.csv when reading 
in the file.  By default, na.strings="NA".  If you do something like

countryCodes <- read.csv("mySourceFile.csv", na.strings="")

then your problem will go away.

On 4/5/2012 7:26 AM, Adrian Dusa wrote:
> On Thu, Apr 5, 2012 at 14:49, Duncan Murdoch<murdoch.duncan at gmail.com>  wrote:
>> On 12-04-05 7:27 AM, Adrian Dusa wrote:
>>> [...]
>> You still haven't explained what you are doing to cause the problem, so I'll
>> guess that you have this file in your data directory of the package, in
>> tab-delimited form.
>>
>> Just store it in some other form, e.g. as a binary object to be loaded.
> Indeed, this is exactly what I did (tab-delimited file in the data
> directory) and yes, this solves the issue.
> Thanks very much,
> Adrian
>


From spencer.graves at prodsyse.com  Thu Apr  5 16:20:59 2012
From: spencer.graves at prodsyse.com (Spencer Graves)
Date: Thu, 05 Apr 2012 07:20:59 -0700
Subject: [Rd] R datasets ownership(copyright) and license
In-Reply-To: <CABdHhvGW=9UK-VT=g+eqv=Sxgd1ikSs0vjQZJyoLBM8wPELX1A@mail.gmail.com>
References: <20120402220614.GS22956@onerussian.com>
	<CABdHhvHDngqDKR-nwhtJ=qFQGO0vZKf3z+-bXCHk=qa=0tBD4g@mail.gmail.com>
	<179d01cd11e5$9c1f1c80$d45d5580$@gmail.com>
	<CABdHhvGE2TGYRTq+1YLG1YB29DZXk+PotcoOJ-sS1dnXfJPnwg@mail.gmail.com>
	<A4E5A0B016B8CB41A485FC629B633CED39C06B75A2@GOLD.corp.lgc-group.com>
	<CABdHhvGW=9UK-VT=g+eqv=Sxgd1ikSs0vjQZJyoLBM8wPELX1A@mail.gmail.com>
Message-ID: <4F7DAA4B.5080705@prodsyse.com>

On 4/5/2012 6:08 AM, Hadley Wickham wrote:
>> And you need not look so far afield for that particular lack of rationality. In the US, databases are covered by the Database and Collections of Information Misappropriation Act of 2003 (http://www.govtrack.us/congress/bills/108/hr3261) which says almost exactly the same thing; that a database that took a lot of time and effort to collate is protected against reproduction 'in commerce' without authorisation.
> That bill died: http://www.govtrack.us/congress/bills/108/hr3261


       After having expressed how copyright law is out of control (and 
further efforts to strengthen enforcement were being sold by the Motion 
Picture Association of America to the US and other governments on the 
grounds that it would make it easier for tyrants to stifle dissent; 
http://en.wikipedia.org/wiki/Anti-Counterfeiting_Trade_Agreement#Motion_Picture_Association_of_America), 
now let me strengthen my support for Hadley's position:



       I think we should vigorously claim "fair use" wherever plausible 
(http://en.wikipedia.org/wiki/Fair_use) with a contingency plan to 
sabotage CRAN (including all mirrors) once per week if we are 
challenged.  This is crudely analogous to what happened when the  
American Society of Composers, Authors and Publishers (ASCAP) sued "the 
Girl Scouts for failing to pay for the songs that girls sang around Girl 
Scout campfires." (http://en.wikipedia.org/wiki/Free_Culture_(book) 
<http://en.wikipedia.org/wiki/Free_Culture_%28book%29>) If this happens, 
we should also appeal for help from the Electronic Frontiers Foundation, 
the American Library Association, the American Civil Liberties Union and 
others, who are working to challenge the industry's abuse of power.  In 
2006, Stanford initiated a "Fair Use" project to fight this abuse of 
power, and other initiatives are on-going, as documented in the 
Wikipedia "Fair Use" article.  The major media conglomerates in the US 
and internationally (ABC-Disney, CBS-Westinghouse, NBC-GE, 
CNN-TimeWarner, Fox-NewsCorp / Rupert Murdoch) have distorted the 
political process in the US, Great Britain and elsewhere to favor them 
and the major international corporate advertisers.  (See also my article 
on "Gateway Problems in US Politics & Economics" at 
"http://occupy.pbworks.com/w/page/52167684/Gateway%20Problems".)


       Spencer

> Hadley
>


-- 
Spencer Graves, PE, PhD
President and Chief Technology Officer
Structure Inspection and Monitoring, Inc.
751 Emerson Ct.
San Jos?, CA 95126
ph:  408-655-4567
web:  www.structuremonitoring.com


From hb at biostat.ucsf.edu  Thu Apr  5 22:17:17 2012
From: hb at biostat.ucsf.edu (Henrik Bengtsson)
Date: Thu, 5 Apr 2012 13:17:17 -0700
Subject: [Rd] Unlikely use case of debug() that cause R to crash
Message-ID: <CAFDcVCTGsfYVwrnwV3VYG1Yhov1bQx_PDkxsDEPuL3Bv7FvufQ@mail.gmail.com>

FYI,

PROBLEM:
R will crash if one tries to use debug() during startup, i.e. in a
user profile file.


REPRODUCIBLE EXAMPLE:
Create ~/.Rprofile with:

cat("~/.Rprofile...\n")
debug(cat)
cat("~/.Rprofile...done\n")

and do

% R --no-environ --no-environ --no-restore --silent

~/.Rprofile...
debugging in: cat("~/.Rprofile...done\n")
debug: {
    if (is.character(file))
        if (file == "")
            file <- stdout()
        else if (substring(file, 1L, 1L) == "|") {
            file <- pipe(substring(file, 2L), "w")
            on.exit(close(file))
        }
        else {
            file <- file(file, ifelse(append, "a", "w"))
            on.exit(close(file))
        }
    .Internal(cat(list(...), file, sep, fill, labels, append))
}
Browse[2]>

R will crash as soon as you try to do anything at the debug prompt.


> sessionInfo()
R version 2.15.0 Patched (2012-04-03 r58917)
Platform: x86_64-pc-mingw32/x64 (64-bit)

locale:
[1] LC_COLLATE=English_United States.1252
[2] LC_CTYPE=English_United States.1252
[3] LC_MONETARY=English_United States.1252
[4] LC_NUMERIC=C
[5] LC_TIME=English_United States.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

/Henrik


From lawrence.michael at gene.com  Fri Apr  6 00:35:30 2012
From: lawrence.michael at gene.com (Michael Lawrence)
Date: Thu, 5 Apr 2012 15:35:30 -0700
Subject: [Rd] issue with base:::namespaceImportMethods
Message-ID: <CAOQ5Nyf-3EJtZKWFUrLHc7JXp60XEx4HgA_sMkZGX_r1zOt0YA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20120405/b29258a2/attachment.pl>

From harold.petithomme at meteo.fr  Fri Apr  6 13:05:33 2012
From: harold.petithomme at meteo.fr (harold.petithomme)
Date: Fri, 06 Apr 2012 13:05:33 +0200
Subject: [Rd] Compatibility problem with R-2.9.0 and R-2.14.0
Message-ID: <4F7ECDFD.6060503@meteo.fr>

Hello all,

Using classical glm function with binomial family, I experienced a problem when using predict() over 
a glm object.
I found that family objects built on R-2.14.0 contain elements like :
.Call(C_logit_link,...)

But on R-2.9.0, the object C_logit_link can not be found. Instead, this prior version used to call 
more simply :
.Call("logit_link",...)

which causes no problem.

Is it possible to use R-2.14.0 family objects on prior R versions ?

I may have missed some note in R changes but tried some research at least.
Many thanks for help.

Kind regards,
Harold

-- 
*********************************************************
Harold PETITHOMME
Equipe Donn?es et Outils de Pr?vision (DPREVI/COMPAS/DOP)

M?t?o France - Direction de la Production
42, avenue G. Coriolis.
31057 Toulouse Cedex
France

Tel : (33/0)5.61.07.82.85
Fax : (33/0)5.61.07.86.09
E-mail : harold.petithomme at meteo.fr


From S.Ellison at LGCGroup.com  Thu Apr  5 14:08:48 2012
From: S.Ellison at LGCGroup.com (S Ellison)
Date: Thu, 5 Apr 2012 13:08:48 +0100
Subject: [Rd] R datasets ownership(copyright) and license
In-Reply-To: <CABdHhvGE2TGYRTq+1YLG1YB29DZXk+PotcoOJ-sS1dnXfJPnwg@mail.gmail.com>
References: <20120402220614.GS22956@onerussian.com>
	<CABdHhvHDngqDKR-nwhtJ=qFQGO0vZKf3z+-bXCHk=qa=0tBD4g@mail.gmail.com>
	<179d01cd11e5$9c1f1c80$d45d5580$@gmail.com>
	<CABdHhvGE2TGYRTq+1YLG1YB29DZXk+PotcoOJ-sS1dnXfJPnwg@mail.gmail.com>
Message-ID: <A4E5A0B016B8CB41A485FC629B633CED39C06B75A2@GOLD.corp.lgc-group.com>


> My understanding is that rationality is not the case in 
> Europe - see e.g. http://en.wikipedia.org/wiki/Database_Directive.

I know we don't always see England as part of Europe, but 'ouch' anyway... 

This is not copyright law. It is protection of databases, and that is a different set of legislation. It just looks like copyright because the things it prohibits are the same.

And you need not look so far afield for that particular lack of rationality. In the US, databases are covered by the Database and Collections of Information Misappropriation Act of 2003 (http://www.govtrack.us/congress/bills/108/hr3261) which says almost exactly the same thing; that a database that took a lot of time and effort to collate is protected against reproduction 'in commerce' without authorisation.

The bottom line is that while most small published datasets are _likely_ to be exempt from such protection, it is not, and never has been, safe to assume that you can use someone else's data without their permission. So we just have to make sure we ask before using datasets in R packages.

Steve Ellison*******************************************************************
This email and any attachments are confidential. Any use...{{dropped:8}}


From murdoch.duncan at gmail.com  Fri Apr  6 16:32:23 2012
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Fri, 06 Apr 2012 10:32:23 -0400
Subject: [Rd] Compatibility problem with R-2.9.0 and R-2.14.0
In-Reply-To: <4F7ECDFD.6060503@meteo.fr>
References: <4F7ECDFD.6060503@meteo.fr>
Message-ID: <4F7EFE77.5070602@gmail.com>

On 12-04-06 7:05 AM, harold.petithomme wrote:
> Hello all,
>
> Using classical glm function with binomial family, I experienced a problem when using predict() over
> a glm object.
> I found that family objects built on R-2.14.0 contain elements like :
> .Call(C_logit_link,...)
>
> But on R-2.9.0, the object C_logit_link can not be found. Instead, this prior version used to call
> more simply :
> .Call("logit_link",...)
>
> which causes no problem.
>
> Is it possible to use R-2.14.0 family objects on prior R versions ?
>
> I may have missed some note in R changes but tried some research at least.
> Many thanks for help.

As a general rule we try to make sure objects produced in older versions 
of R are usable in newer versions, but not vice versa.  So I would 
expect that there are a lot of examples of 2.14.0 objects which can't be 
used in R 2.9.0.

Duncan Murdoch


From mcneney at sfu.ca  Fri Apr  6 20:46:28 2012
From: mcneney at sfu.ca (Brad McNeney)
Date: Fri, 6 Apr 2012 11:46:28 -0700 (PDT)
Subject: [Rd] R CMD check returns NOTE about package data set as global
	variable
In-Reply-To: <156280092.18208556.1333737562321.JavaMail.root@jaguar9.sfu.ca>
Message-ID: <1701096951.18211343.1333737988103.JavaMail.root@jaguar9.sfu.ca>

I'm developing a package that comes with a data set called RutgersMapB36. One of the package's functions requires this data frame. A toy example is:

test<-function() {
  data(RutgersMapB36)
  return(RutgersMapB36[,1])
}


R CMD check returns a NOTE:

test: no visible binding for global variable 'RutgersMapB36'

Is there any way to avoid this NOTE?

Thanks,

Brad
---
Brad McNeney
Statistics and Actuarial Science
Simon Fraser University


From ripley at stats.ox.ac.uk  Fri Apr  6 21:18:14 2012
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 06 Apr 2012 20:18:14 +0100
Subject: [Rd] R CMD check returns NOTE about package data set as global
 variable
In-Reply-To: <1701096951.18211343.1333737988103.JavaMail.root@jaguar9.sfu.ca>
References: <1701096951.18211343.1333737988103.JavaMail.root@jaguar9.sfu.ca>
Message-ID: <4F7F4176.7080901@stats.ox.ac.uk>

On 06/04/2012 19:46, Brad McNeney wrote:
> I'm developing a package that comes with a data set called RutgersMapB36. One of the package's functions requires this data frame. A toy example is:
>
> test<-function() {
>    data(RutgersMapB36)
>    return(RutgersMapB36[,1])
> }
>
>
> R CMD check returns a NOTE:
>
> test: no visible binding for global variable 'RutgersMapB36'
>
> Is there any way to avoid this NOTE?

Use data("RutgersMapB36"), which many think is good practice in code.


>
> Thanks,
>
> Brad
> ---
> Brad McNeney
> Statistics and Actuarial Science
> Simon Fraser University
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From mcneney at sfu.ca  Fri Apr  6 21:33:53 2012
From: mcneney at sfu.ca (Brad McNeney)
Date: Fri, 6 Apr 2012 12:33:53 -0700 (PDT)
Subject: [Rd] R CMD check returns NOTE about package data set as global
 variable
In-Reply-To: <4F7F4176.7080901@stats.ox.ac.uk>
Message-ID: <1114888490.18258021.1333740833420.JavaMail.root@jaguar9.sfu.ca>

OK, thanks for the tip on good coding practice. I'm still getting the NOTE though when I make the suggested change.

In case it matters, I'm check'ing with

R version 2.15.0 (2012-03-30)
Platform: i386-pc-mingw32/i386 (32-bit)

Brad

----- Original Message -----
> From: "Prof Brian Ripley" <ripley at stats.ox.ac.uk>
> To: "Brad McNeney" <mcneney at sfu.ca>
> Cc: r-devel at r-project.org
> Sent: Friday, 6 April, 2012 12:18:14 PM
> Subject: Re: [Rd] R CMD check returns NOTE about package data set as global variable
> 
> On 06/04/2012 19:46, Brad McNeney wrote:
> > I'm developing a package that comes with a data set called
> > RutgersMapB36. One of the package's functions requires this data
> > frame. A toy example is:
> >
> > test<-function() {
> >    data(RutgersMapB36)
> >    return(RutgersMapB36[,1])
> > }
> >
> >
> > R CMD check returns a NOTE:
> >
> > test: no visible binding for global variable 'RutgersMapB36'
> >
> > Is there any way to avoid this NOTE?
> 
> Use data("RutgersMapB36"), which many think is good practice in code.
> 
> 
> >
> > Thanks,
> >
> > Brad
> > ---
> > Brad McNeney
> > Statistics and Actuarial Science
> > Simon Fraser University
> >
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
> 
> 
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>


From hadley at rice.edu  Fri Apr  6 21:38:11 2012
From: hadley at rice.edu (Hadley Wickham)
Date: Fri, 6 Apr 2012 14:38:11 -0500
Subject: [Rd] R CMD check returns NOTE about package data set as global
	variable
In-Reply-To: <1701096951.18211343.1333737988103.JavaMail.root@jaguar9.sfu.ca>
References: <156280092.18208556.1333737562321.JavaMail.root@jaguar9.sfu.ca>
	<1701096951.18211343.1333737988103.JavaMail.root@jaguar9.sfu.ca>
Message-ID: <CABdHhvFHG5JR+oE1h_RAZG89MoR6F5hzPX1Wke=m2HYLvdTVcg@mail.gmail.com>

Is the dataset something that package users will need, or just your
package's functions?
Hadley

On Fri, Apr 6, 2012 at 1:46 PM, Brad McNeney <mcneney at sfu.ca> wrote:
> I'm developing a package that comes with a data set called RutgersMapB36. One of the package's functions requires this data frame. A toy example is:
>
> test<-function() {
> ?data(RutgersMapB36)
> ?return(RutgersMapB36[,1])
> }
>
>
> R CMD check returns a NOTE:
>
> test: no visible binding for global variable 'RutgersMapB36'
>
> Is there any way to avoid this NOTE?
>
> Thanks,
>
> Brad
> ---
> Brad McNeney
> Statistics and Actuarial Science
> Simon Fraser University
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel



-- 
Assistant Professor / Dobelman Family Junior Chair
Department of Statistics / Rice University
http://had.co.nz/


From ripley at stats.ox.ac.uk  Fri Apr  6 21:43:22 2012
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 6 Apr 2012 20:43:22 +0100 (BST)
Subject: [Rd] R CMD check returns NOTE about package data set as global
 variable
In-Reply-To: <1114888490.18258021.1333740833420.JavaMail.root@jaguar9.sfu.ca>
References: <1114888490.18258021.1333740833420.JavaMail.root@jaguar9.sfu.ca>
Message-ID: <alpine.LFD.2.02.1204062036460.17273@gannet.stats.ox.ac.uk>

On Fri, 6 Apr 2012, Brad McNeney wrote:

> OK, thanks for the tip on good coding practice. I'm still getting the NOTE though when I make the suggested change.

Yes, you will:  data() is a function with side effects, which is 
contrary to the functional programming model being checked.  So there 
is no way to avoid all notes and use data().

If you want to make your code more understandable, consider using 
LazyData (see 'Writing R Extensions').  My view is that data() is a 
kludge from long ago when R had much less powerful memory management, 
except perhaps for very large datasets (at least 100MBs) when you may 
want to control when they are loaded into memory.

>
> In case it matters, I'm check'ing with
>
> R version 2.15.0 (2012-03-30)
> Platform: i386-pc-mingw32/i386 (32-bit)
>
> Brad
>
> ----- Original Message -----
>> From: "Prof Brian Ripley" <ripley at stats.ox.ac.uk>
>> To: "Brad McNeney" <mcneney at sfu.ca>
>> Cc: r-devel at r-project.org
>> Sent: Friday, 6 April, 2012 12:18:14 PM
>> Subject: Re: [Rd] R CMD check returns NOTE about package data set as global variable
>>
>> On 06/04/2012 19:46, Brad McNeney wrote:
>>> I'm developing a package that comes with a data set called
>>> RutgersMapB36. One of the package's functions requires this data
>>> frame. A toy example is:
>>>
>>> test<-function() {
>>>    data(RutgersMapB36)
>>>    return(RutgersMapB36[,1])
>>> }
>>>
>>>
>>> R CMD check returns a NOTE:
>>>
>>> test: no visible binding for global variable 'RutgersMapB36'
>>>
>>> Is there any way to avoid this NOTE?
>>
>> Use data("RutgersMapB36"), which many think is good practice in code.
>>
>>
>>>
>>> Thanks,
>>>
>>> Brad
>>> ---
>>> Brad McNeney
>>> Statistics and Actuarial Science
>>> Simon Fraser University
>>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>>
>> --
>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>> University of Oxford,             Tel:  +44 1865 272861 (self)
>> 1 South Parks Road,                     +44 1865 272866 (PA)
>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From pdalgd at gmail.com  Fri Apr  6 22:04:21 2012
From: pdalgd at gmail.com (peter dalgaard)
Date: Fri, 6 Apr 2012 22:04:21 +0200
Subject: [Rd] R CMD check returns NOTE about package data set as global
	variable
In-Reply-To: <1114888490.18258021.1333740833420.JavaMail.root@jaguar9.sfu.ca>
References: <1114888490.18258021.1333740833420.JavaMail.root@jaguar9.sfu.ca>
Message-ID: <6AF5C1C4-1F46-4731-88FE-3D25015F42F3@gmail.com>


On Apr 6, 2012, at 21:33 , Brad McNeney wrote:

> OK, thanks for the tip on good coding practice. I'm still getting the NOTE though when I make the suggested change.

Hm? It's not like Brian to get such things wrong, did you check properly?

Perhaps the code checker is not smart enough to know that data() creates global variables. (That would be heuristic at best. You can't actually be sure that data() creates objects with the name given as the argument -- in fact, several objects might be created, possibly none named as the argument). 

You are not using LazyData, right?  You might consider doing that and forgetting about data() entirely.

> In case it matters, I'm check'ing with
> 
> R version 2.15.0 (2012-03-30)
> Platform: i386-pc-mingw32/i386 (32-bit)
> 
> Brad
> 
> ----- Original Message -----
>> From: "Prof Brian Ripley" <ripley at stats.ox.ac.uk>
>> To: "Brad McNeney" <mcneney at sfu.ca>
>> Cc: r-devel at r-project.org
>> Sent: Friday, 6 April, 2012 12:18:14 PM
>> Subject: Re: [Rd] R CMD check returns NOTE about package data set as global variable
>> 
>> On 06/04/2012 19:46, Brad McNeney wrote:
>>> I'm developing a package that comes with a data set called
>>> RutgersMapB36. One of the package's functions requires this data
>>> frame. A toy example is:
>>> 
>>> test<-function() {
>>>   data(RutgersMapB36)
>>>   return(RutgersMapB36[,1])
>>> }
>>> 
>>> 
>>> R CMD check returns a NOTE:
>>> 
>>> test: no visible binding for global variable 'RutgersMapB36'
>>> 
>>> Is there any way to avoid this NOTE?
>> 
>> Use data("RutgersMapB36"), which many think is good practice in code.
>> 
>> 
>>> 
>>> Thanks,
>>> 
>>> Brad
>>> ---
>>> Brad McNeney
>>> Statistics and Actuarial Science
>>> Simon Fraser University
>>> 
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>> 
>> 
>> --
>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>> University of Oxford,             Tel:  +44 1865 272861 (self)
>> 1 South Parks Road,                     +44 1865 272866 (PA)
>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>> 
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From hpages at fhcrc.org  Fri Apr  6 22:23:01 2012
From: hpages at fhcrc.org (=?ISO-8859-1?Q?Herv=E9_Pag=E8s?=)
Date: Fri, 06 Apr 2012 13:23:01 -0700
Subject: [Rd] R CMD check returns NOTE about package data set as global
 variable
In-Reply-To: <1114888490.18258021.1333740833420.JavaMail.root@jaguar9.sfu.ca>
References: <1114888490.18258021.1333740833420.JavaMail.root@jaguar9.sfu.ca>
Message-ID: <4F7F50A5.7090304@fhcrc.org>

On 04/06/2012 12:33 PM, Brad McNeney wrote:
> OK, thanks for the tip on good coding practice. I'm still getting the NOTE though when I make the suggested change.

Because when you do return(RutgersMapB36[,1]), the code checker has no
way to know that the RutgersMapB36 variable is actually defined.

Try this:

test<-function() {
    RutgersMapB36 <- NULL
    data(RutgersMapB36)
    return(RutgersMapB36[,1])
}

Cheers,
H.

>
> In case it matters, I'm check'ing with
>
> R version 2.15.0 (2012-03-30)
> Platform: i386-pc-mingw32/i386 (32-bit)
>
> Brad
>
> ----- Original Message -----
>> From: "Prof Brian Ripley"<ripley at stats.ox.ac.uk>
>> To: "Brad McNeney"<mcneney at sfu.ca>
>> Cc: r-devel at r-project.org
>> Sent: Friday, 6 April, 2012 12:18:14 PM
>> Subject: Re: [Rd] R CMD check returns NOTE about package data set as global variable
>>
>> On 06/04/2012 19:46, Brad McNeney wrote:
>>> I'm developing a package that comes with a data set called
>>> RutgersMapB36. One of the package's functions requires this data
>>> frame. A toy example is:
>>>
>>> test<-function() {
>>>     data(RutgersMapB36)
>>>     return(RutgersMapB36[,1])
>>> }
>>>
>>>
>>> R CMD check returns a NOTE:
>>>
>>> test: no visible binding for global variable 'RutgersMapB36'
>>>
>>> Is there any way to avoid this NOTE?
>>
>> Use data("RutgersMapB36"), which many think is good practice in code.
>>
>>
>>>
>>> Thanks,
>>>
>>> Brad
>>> ---
>>> Brad McNeney
>>> Statistics and Actuarial Science
>>> Simon Fraser University
>>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>>
>> --
>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>> University of Oxford,             Tel:  +44 1865 272861 (self)
>> 1 South Parks Road,                     +44 1865 272866 (PA)
>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fhcrc.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From pdalgd at gmail.com  Fri Apr  6 22:33:14 2012
From: pdalgd at gmail.com (peter dalgaard)
Date: Fri, 6 Apr 2012 22:33:14 +0200
Subject: [Rd] R CMD check returns NOTE about package data set as global
	variable
In-Reply-To: <4F7F50A5.7090304@fhcrc.org>
References: <1114888490.18258021.1333740833420.JavaMail.root@jaguar9.sfu.ca>
	<4F7F50A5.7090304@fhcrc.org>
Message-ID: <027098F6-E168-469F-B5A8-D3667EC39A63@gmail.com>


On Apr 6, 2012, at 22:23 , Herv? Pag?s wrote:

> On 04/06/2012 12:33 PM, Brad McNeney wrote:
>> OK, thanks for the tip on good coding practice. I'm still getting the NOTE though when I make the suggested change.
> 
> Because when you do return(RutgersMapB36[,1]), the code checker has no
> way to know that the RutgersMapB36 variable is actually defined.
> 
> Try this:
> 
> test<-function() {
>   RutgersMapB36 <- NULL
>   data(RutgersMapB36)
>   return(RutgersMapB36[,1])
> }
> 

That might remove the NOTE, but as far as I can see, it also breaks the code... 

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From mcneney at sfu.ca  Fri Apr  6 22:52:38 2012
From: mcneney at sfu.ca (Brad McNeney)
Date: Fri, 6 Apr 2012 13:52:38 -0700 (PDT)
Subject: [Rd] R CMD check returns NOTE about package data set as global
 variable
In-Reply-To: <CABdHhvFHG5JR+oE1h_RAZG89MoR6F5hzPX1Wke=m2HYLvdTVcg@mail.gmail.com>
Message-ID: <1803048919.18287512.1333745558126.JavaMail.root@jaguar9.sfu.ca>

Package users should have access.

Brad 

----- Original Message -----
> From: "Hadley Wickham" <hadley at rice.edu>
> To: "Brad McNeney" <mcneney at sfu.ca>
> Cc: r-devel at r-project.org
> Sent: Friday, 6 April, 2012 12:38:11 PM
> Subject: Re: [Rd] R CMD check returns NOTE about package data set as global variable
> 
> Is the dataset something that package users will need, or just your
> package's functions?
> Hadley
> 
> On Fri, Apr 6, 2012 at 1:46 PM, Brad McNeney <mcneney at sfu.ca> wrote:
> > I'm developing a package that comes with a data set called
> > RutgersMapB36. One of the package's functions requires this data
> > frame. A toy example is:
> >
> > test<-function() {
> > ?data(RutgersMapB36)
> > ?return(RutgersMapB36[,1])
> > }
> >
> >
> > R CMD check returns a NOTE:
> >
> > test: no visible binding for global variable 'RutgersMapB36'
> >
> > Is there any way to avoid this NOTE?
> >
> > Thanks,
> >
> > Brad
> > ---
> > Brad McNeney
> > Statistics and Actuarial Science
> > Simon Fraser University
> >
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
> 
> 
> 
> --
> Assistant Professor / Dobelman Family Junior Chair
> Department of Statistics / Rice University
> http://had.co.nz/
> 


From mcneney at sfu.ca  Fri Apr  6 22:54:17 2012
From: mcneney at sfu.ca (Brad McNeney)
Date: Fri, 6 Apr 2012 13:54:17 -0700 (PDT)
Subject: [Rd] R CMD check returns NOTE about package data set as global
 variable
In-Reply-To: <alpine.LFD.2.02.1204062036460.17273@gannet.stats.ox.ac.uk>
Message-ID: <2067867532.18287973.1333745657817.JavaMail.root@jaguar9.sfu.ca>

Thanks (to all), using LazyData removes the note.

Brad

----- Original Message -----
> From: "Prof Brian Ripley" <ripley at stats.ox.ac.uk>
> To: "Brad McNeney" <mcneney at sfu.ca>
> Cc: r-devel at r-project.org
> Sent: Friday, 6 April, 2012 12:43:22 PM
> Subject: Re: [Rd] R CMD check returns NOTE about package data set as global variable
> 
> On Fri, 6 Apr 2012, Brad McNeney wrote:
> 
> > OK, thanks for the tip on good coding practice. I'm still getting
> > the NOTE though when I make the suggested change.
> 
> Yes, you will:  data() is a function with side effects, which is
> contrary to the functional programming model being checked.  So there
> is no way to avoid all notes and use data().
> 
> If you want to make your code more understandable, consider using
> LazyData (see 'Writing R Extensions').  My view is that data() is a
> kludge from long ago when R had much less powerful memory management,
> except perhaps for very large datasets (at least 100MBs) when you may
> want to control when they are loaded into memory.
> 
> >
> > In case it matters, I'm check'ing with
> >
> > R version 2.15.0 (2012-03-30)
> > Platform: i386-pc-mingw32/i386 (32-bit)
> >
> > Brad
> >
> > ----- Original Message -----
> >> From: "Prof Brian Ripley" <ripley at stats.ox.ac.uk>
> >> To: "Brad McNeney" <mcneney at sfu.ca>
> >> Cc: r-devel at r-project.org
> >> Sent: Friday, 6 April, 2012 12:18:14 PM
> >> Subject: Re: [Rd] R CMD check returns NOTE about package data set
> >> as global variable
> >>
> >> On 06/04/2012 19:46, Brad McNeney wrote:
> >>> I'm developing a package that comes with a data set called
> >>> RutgersMapB36. One of the package's functions requires this data
> >>> frame. A toy example is:
> >>>
> >>> test<-function() {
> >>>    data(RutgersMapB36)
> >>>    return(RutgersMapB36[,1])
> >>> }
> >>>
> >>>
> >>> R CMD check returns a NOTE:
> >>>
> >>> test: no visible binding for global variable 'RutgersMapB36'
> >>>
> >>> Is there any way to avoid this NOTE?
> >>
> >> Use data("RutgersMapB36"), which many think is good practice in
> >> code.
> >>
> >>
> >>>
> >>> Thanks,
> >>>
> >>> Brad
> >>> ---
> >>> Brad McNeney
> >>> Statistics and Actuarial Science
> >>> Simon Fraser University
> >>>
> >>> ______________________________________________
> >>> R-devel at r-project.org mailing list
> >>> https://stat.ethz.ch/mailman/listinfo/r-devel
> >>
> >>
> >> --
> >> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> >> Professor of Applied Statistics,
> >>  http://www.stats.ox.ac.uk/~ripley/
> >> University of Oxford,             Tel:  +44 1865 272861 (self)
> >> 1 South Parks Road,                     +44 1865 272866 (PA)
> >> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> >>
> >
> 
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>


From hpages at fhcrc.org  Fri Apr  6 23:02:44 2012
From: hpages at fhcrc.org (=?ISO-8859-1?Q?Herv=E9_Pag=E8s?=)
Date: Fri, 06 Apr 2012 14:02:44 -0700
Subject: [Rd] R CMD check returns NOTE about package data set as global
 variable
In-Reply-To: <027098F6-E168-469F-B5A8-D3667EC39A63@gmail.com>
References: <1114888490.18258021.1333740833420.JavaMail.root@jaguar9.sfu.ca>
	<4F7F50A5.7090304@fhcrc.org>
	<027098F6-E168-469F-B5A8-D3667EC39A63@gmail.com>
Message-ID: <4F7F59F4.5050207@fhcrc.org>

On 04/06/2012 01:33 PM, peter dalgaard wrote:
>
> On Apr 6, 2012, at 22:23 , Herv? Pag?s wrote:
>
>> On 04/06/2012 12:33 PM, Brad McNeney wrote:
>>> OK, thanks for the tip on good coding practice. I'm still getting the NOTE though when I make the suggested change.
>>
>> Because when you do return(RutgersMapB36[,1]), the code checker has no
>> way to know that the RutgersMapB36 variable is actually defined.
>>
>> Try this:
>>
>> test<-function() {
>>    RutgersMapB36<- NULL
>>    data(RutgersMapB36)
>>    return(RutgersMapB36[,1])
>> }
>>
>
> That might remove the NOTE, but as far as I can see, it also breaks the code...
>

oops, right...

This should remove the NOTE and work (hopefully):

test<-function() {
    data("RutgersMapB36")  # loads RutgersMapB36 in .GlobalEnv
    RutgersMapB36 <- get("RutgersMapB36", envir=.GlobalEnv)
    return(RutgersMapB36[,1])
}

Cheers,
H.


-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fhcrc.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From brian at braverock.com  Fri Apr  6 23:04:04 2012
From: brian at braverock.com (Brian G. Peterson)
Date: Fri, 06 Apr 2012 16:04:04 -0500
Subject: [Rd] R CMD check returns NOTE about package data set as global
 variable
In-Reply-To: <4F7F50A5.7090304@fhcrc.org>
References: <1114888490.18258021.1333740833420.JavaMail.root@jaguar9.sfu.ca>
	<4F7F50A5.7090304@fhcrc.org>
Message-ID: <1333746244.24441.44.camel@brian-desktop>

On Fri, 2012-04-06 at 13:23 -0700, Herv? Pag?s wrote:
> test<-function() {
>     RutgersMapB36 <- NULL
>     data(RutgersMapB36)
>     return(RutgersMapB36[,1])
> } 

That won't work, but this should:

RutgersMapB36 <- NULL
test<-function() {
    data(RutgersMapB36)
    return(RutgersMapB36[,1])
}

Honestly, this is just another example of a non-helpful 'global
variable' NOTE.  I've removed many of these from our packages, often by
resorting to useless workarounds like this one, but I have never once
gotten a valid NOTE out of this message.  We provided other examples
earlier in a different thread.

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From hb at biostat.ucsf.edu  Fri Apr  6 23:15:01 2012
From: hb at biostat.ucsf.edu (Henrik Bengtsson)
Date: Fri, 6 Apr 2012 14:15:01 -0700
Subject: [Rd] R CMD check returns NOTE about package data set as global
	variable
In-Reply-To: <027098F6-E168-469F-B5A8-D3667EC39A63@gmail.com>
References: <1114888490.18258021.1333740833420.JavaMail.root@jaguar9.sfu.ca>
	<4F7F50A5.7090304@fhcrc.org>
	<027098F6-E168-469F-B5A8-D3667EC39A63@gmail.com>
Message-ID: <CAFDcVCTcJiNSnP9mqACV7p74jn-8i26=mOJG4Nf+ANUdfPFOtQ@mail.gmail.com>

On Fri, Apr 6, 2012 at 1:33 PM, peter dalgaard <pdalgd at gmail.com> wrote:
>
> On Apr 6, 2012, at 22:23 , Herv? Pag?s wrote:
>
>> On 04/06/2012 12:33 PM, Brad McNeney wrote:
>>> OK, thanks for the tip on good coding practice. I'm still getting the NOTE though when I make the suggested change.
>>
>> Because when you do return(RutgersMapB36[,1]), the code checker has no
>> way to know that the RutgersMapB36 variable is actually defined.
>>
>> Try this:
>>
>> test<-function() {
>> ? RutgersMapB36 <- NULL
>> ? data(RutgersMapB36)
>> ? return(RutgersMapB36[,1])
>> }
>>
>
> That might remove the NOTE, but as far as I can see, it also breaks the code...

For data() per se, which by default clutter up the global environment,
you can do:

test<-function() {
  env <- new.env()
? data("RutgersMapB36", envir=env)
? env$RutgersMapB36[,1]
}

That is more explicit, and I do believe you won't get a NOTE about it.

Other than that, one can also use the following style (which still
seems to do the trick) for data(), attach(), load() et al., iff have
to use them:

test<-function() {
  # To avoid NOTEs by R CMD check
? RutgersMapB36 <- NULL; rm(RutgersMapB36);

? data(RutgersMapB36)
? return(RutgersMapB36[,1])
}


/Henrik

>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Email: pd.mes at cbs.dk ?Priv: PDalgd at gmail.com
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From hpages at fhcrc.org  Fri Apr  6 23:17:39 2012
From: hpages at fhcrc.org (=?UTF-8?B?SGVydsOpIFBhZ8Oocw==?=)
Date: Fri, 06 Apr 2012 14:17:39 -0700
Subject: [Rd] R CMD check returns NOTE about package data set as global
 variable
In-Reply-To: <1333746244.24441.44.camel@brian-desktop>
References: <1114888490.18258021.1333740833420.JavaMail.root@jaguar9.sfu.ca>
	<4F7F50A5.7090304@fhcrc.org>
	<1333746244.24441.44.camel@brian-desktop>
Message-ID: <4F7F5D73.4070002@fhcrc.org>

Hi Brian,

On 04/06/2012 02:04 PM, Brian G. Peterson wrote:
> On Fri, 2012-04-06 at 13:23 -0700, Herv? Pag?s wrote:
>> test<-function() {
>>      RutgersMapB36<- NULL
>>      data(RutgersMapB36)
>>      return(RutgersMapB36[,1])
>> }
>
> That won't work, but this should:
>
> RutgersMapB36<- NULL
> test<-function() {
>      data(RutgersMapB36)
>      return(RutgersMapB36[,1])
> }
>
> Honestly, this is just another example of a non-helpful 'global
> variable' NOTE.  I've removed many of these from our packages, often by
> resorting to useless workarounds like this one, but I have never once
> gotten a valid NOTE out of this message.  We provided other examples
> earlier in a different thread.
>

Other people might have a different experience. I've personally seen a
lot of true positive "no visible binding for global variable" notes in
the Bioconductor check results.

In that sense 'R CMD check' is no different from other code checking
tools like e.g. gcc -Wall. There are sometimes false positives, it's
unavoidable. Personally I can live with that.

Cheers,
H.

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fhcrc.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From pdalgd at gmail.com  Fri Apr  6 23:49:47 2012
From: pdalgd at gmail.com (peter dalgaard)
Date: Fri, 6 Apr 2012 23:49:47 +0200
Subject: [Rd] R CMD check returns NOTE about package data set as global
	variable
In-Reply-To: <1333746244.24441.44.camel@brian-desktop>
References: <1114888490.18258021.1333740833420.JavaMail.root@jaguar9.sfu.ca>
	<4F7F50A5.7090304@fhcrc.org>
	<1333746244.24441.44.camel@brian-desktop>
Message-ID: <8B7B10F8-00C8-4477-B73D-CC4F631C7001@gmail.com>


On Apr 6, 2012, at 23:04 , Brian G. Peterson wrote:

> Honestly, this is just another example of a non-helpful 'global
> variable' NOTE.  I've removed many of these from our packages, often by
> resorting to useless workarounds like this one, but I have never once
> gotten a valid NOTE out of this message.  We provided other examples
> earlier in a different thread.

Actually, this one is perfectly valid. It is saying that you are messing with global variables, which you might not want to do in package code. It is admittedly rather unlikely that the user has a variable called "RutgersMapB36" lying around for you to clobber, but suppose that it was "x" or "mydata"... 

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From jmc at r-project.org  Sat Apr  7 21:11:21 2012
From: jmc at r-project.org (John Chambers)
Date: Sat, 07 Apr 2012 12:11:21 -0700
Subject: [Rd] issue with base:::namespaceImportMethods
In-Reply-To: <CAOQ5Nyf-3EJtZKWFUrLHc7JXp60XEx4HgA_sMkZGX_r1zOt0YA@mail.gmail.com>
References: <CAOQ5Nyf-3EJtZKWFUrLHc7JXp60XEx4HgA_sMkZGX_r1zOt0YA@mail.gmail.com>
Message-ID: <4F809159.3000804@r-project.org>


Thanks, Michael.

It looks good.  I committed it to r-devel (rev 58925).

If nothing bad happens, we can merge it into 2.15.0 patched.  (Nothing I 
found in CRAN seems to test complicated importMethodsFrom usage.)

John

On 4/5/12 3:35 PM, Michael Lawrence wrote:
> Hi,
>
> I've noticed an issue with S4 methods and namespaces which only arises in
> particular, difficult to reproduce configurations. One example is the ggbio
> package in Bioconductor, which currently emits these warnings when its
> namespace is loaded:
>
> ----------------------
>
> library(ggbio)
> Loading required package: ggplot2
>
> Attaching package: ?ggbio?
>
> The following object(s) are masked from ?package:ggplot2?:
>
>      geom_rect, geom_segment, stat_identity, xlim
>
> Warning messages:
> 1: found methods to import for function ?append? but not the generic itself
> 2: found methods to import for function ?as.factor? but not the generic
> itself
> 3: found methods to import for function ?as.list? but not the generic
> itself
> 4: found methods to import for function ?aggregate? but not the generic
> itself
> 5: found methods to import for function ?as.table? but not the generic
> itself
> 6: found methods to import for function ?complete.cases? but not the
> generic itself
> 7: found methods to import for function ?cor? but not the generic itself
> 8: found methods to import for function ?diff? but not the generic itself
> 9: found methods to import for function ?drop? but not the generic itself
>
> ------------------------
>
> I tracked these warnings down to the behavior of the
> base:::namespaceImportMethods, which ends up calling
> base:::namespaceImportFrom with arguments that seem to violate the
> assumptions made in that function. It looks like base:::namespaceImportFrom
> (conditionally) assumes that the "vars", "generics" and "packages"
> arguments are parallel vectors. However, base:::namespaceImportMethods can
> end up filtering 'vars' so that it no longer parallels the other two. Maybe
> I am just misreading the code, but the following patch seems to fix things:
>
> Index: src/library/base/R/namespace.R
> ===================================================================
> --- src/library/base/R/namespace.R      (revision 58917)
> +++ src/library/base/R/namespace.R      (working copy)
> @@ -930,8 +930,10 @@
>
>   namespaceImportMethods<- function(self, ns, vars) {
>       allVars<- character()
> +    generics<- character()
> +    packages<- character()
>       allFuns<- methods:::.getGenerics(ns) # all the methods tables in ns
> -    packages<- attr(allFuns, "package")
> +    allPackages<- attr(allFuns, "package")
>       pkg<- methods:::getPackageName(ns)
>       if(!all(vars %in% allFuns)) {
>           message(gettextf("No methods found in \"%s\" for requests: %s",
> @@ -950,16 +952,23 @@
>           ## import methods tables if asked for
>           ## or if the corresponding generic was imported
>           g<- allFuns[[i]]
> +        p<- allPackages[[i]]
>           if(exists(g, envir = self, inherits = FALSE) # already imported
>              || g %in% vars) { # requested explicitly
> -            tbl<- methods:::.TableMetaName(g, packages[[i]])
> -            if(is.null(.mergeImportMethods(self, ns, tbl))) # a new
> methods
> table
>
> +            tbl<- methods:::.TableMetaName(g, p)
> +            if(is.null(.mergeImportMethods(self, ns, tbl))) { # a new
> methods t
> able
>
>                  allVars<- c(allVars, tbl) # import it;else, was merged
> +               generics<- c(generics, g)
> +               packages<- c(packages, p)
> +            }
>           }
>           if(g %in% vars&&  !exists(g, envir = self, inherits = FALSE)) {
>               if(exists(g, envir = ns)&&
> -               methods:::is(get(g, envir = ns), "genericFunction"))
> +               methods:::is(get(g, envir = ns), "genericFunction")) {
>                   allVars<- c(allVars, g)
> +                generics<- c(generics, g)
> +                packages<- c(packages, p)
> +            }
>               else { # should be primitive
>                   fun<- methods::getFunction(g, mustFind = FALSE, where =
> self)
>                   if(is.primitive(fun) || methods::is(fun,
> "genericFunction")) {}
> @@ -970,7 +979,7 @@
>               }
>           }
>       }
> -    namespaceImportFrom(self, asNamespace(ns), allVars, allFuns, packages)
> +    namespaceImportFrom(self, asNamespace(ns), allVars, generics, packages)
>   }
>
> -----------------------------
>
> Thanks for any advice,
>
> Michael
>
> PS: sessionInfo() (yes, ggbio has a LOT of dependencies):
>
>> sessionInfo()
> R Under development (unstable) (2012-04-04 r58917)
> Platform: x86_64-unknown-linux-gnu (64-bit)
>
> locale:
>   [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
>   [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
>   [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8
>   [7] LC_PAPER=C                 LC_NAME=C
>   [9] LC_ADDRESS=C               LC_TELEPHONE=C
> [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> other attached packages:
> [1] ggbio_1.2.0         ggplot2_0.9.0       BiocInstaller_1.4.3
>
> loaded via a namespace (and not attached):
>   [1] AnnotationDbi_1.18.0    Biobase_2.16.0          BiocGenerics_0.2.0
>   [4] biomaRt_2.12.0          Biostrings_2.24.1       biovizBase_1.2.0
>   [7] bitops_1.0-4.1          BSgenome_1.24.0         cluster_1.14.2
> [10] colorspace_1.1-1        DBI_0.2-5               dichromat_1.2-4
> [13] digest_0.5.2            GenomicFeatures_1.8.0   GenomicRanges_1.8.3
> [16] grid_2.16.0             gridExtra_0.9           Hmisc_3.9-3
> [19] IRanges_1.14.2          lattice_0.20-6          MASS_7.3-17
> [22] Matrix_1.0-6            memoise_0.1             munsell_0.3
> [25] plyr_1.7.1              proto_0.3-9.2           RColorBrewer_1.0-5
> [28] RCurl_1.91-1            reshape2_1.2.1          Rsamtools_1.8.0
> [31] RSQLite_0.11.1          rtracklayer_1.16.0      scales_0.2.0
> [34] snpStats_1.6.0          splines_2.16.0          stats4_2.16.0
> [37] stringr_0.6             survival_2.36-12        tools_2.16.0
> [40] VariantAnnotation_1.2.2 XML_3.9-4               zlibbioc_1.2.0
>
> 	[[alternative HTML version deleted]]
>
>
>
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From thosjleeper at gmail.com  Sat Apr  7 23:27:00 2012
From: thosjleeper at gmail.com (Thomas J. Leeper)
Date: Sat, 7 Apr 2012 16:27:00 -0500
Subject: [Rd] Display instruction text on package load
Message-ID: <CAOC91MTM+YPrsWhX4S31BRJcJ-YJddhGqXxZ8Oc4VngV31vWEQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20120407/f79a4b32/attachment.pl>

From edd at debian.org  Sun Apr  8 15:07:34 2012
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 8 Apr 2012 08:07:34 -0500
Subject: [Rd] Display instruction text on package load
In-Reply-To: <CAOC91MTM+YPrsWhX4S31BRJcJ-YJddhGqXxZ8Oc4VngV31vWEQ@mail.gmail.com>
References: <CAOC91MTM+YPrsWhX4S31BRJcJ-YJddhGqXxZ8Oc4VngV31vWEQ@mail.gmail.com>
Message-ID: <20353.36246.573357.907321@max.nulle.part>


On 7 April 2012 at 16:27, Thomas J. Leeper wrote:
| I'm currently working on a new package for R that interfaces with an API.
| Most of the package's functionality requires users to supply an access key
| that allows the package to work with the API. I want to display text
| immediately upon loading the package that instructs/reminds users to enter
| their access key before attempting to do anything (as the package won't
| really work without it). I've searched the relevant help files for creating
| packages but haven't been able find anything about how to do this. As an
| example, library(Zelig) does what I'm trying to achieve.

Start with

      help(packageStartupMessage)
      help(.onLoad)

and also peruse the list archives here---you probably want this in your
.onLoad() function. 

Maybe some of the packages interfacing Google's APIs and webservices have an
example.

Dirk

-- 
R/Finance 2012 Conference on May 11 and 12, 2012 at UIC in Chicago, IL
See agenda, registration details and more at http://www.RinFinance.com


From ripley at stats.ox.ac.uk  Sun Apr  8 15:07:46 2012
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 08 Apr 2012 14:07:46 +0100
Subject: [Rd] Display instruction text on package load
In-Reply-To: <CAOC91MTM+YPrsWhX4S31BRJcJ-YJddhGqXxZ8Oc4VngV31vWEQ@mail.gmail.com>
References: <CAOC91MTM+YPrsWhX4S31BRJcJ-YJddhGqXxZ8Oc4VngV31vWEQ@mail.gmail.com>
Message-ID: <4F818DA2.6000505@stats.ox.ac.uk>

On 07/04/2012 22:27, Thomas J. Leeper wrote:
> I'm currently working on a new package for R that interfaces with an API.
> Most of the package's functionality requires users to supply an access key
> that allows the package to work with the API. I want to display text
> immediately upon loading the package that instructs/reminds users to enter
> their access key before attempting to do anything (as the package won't
> really work without it). I've searched the relevant help files for creating
> packages but haven't been able find anything about how to do this. As an
> example, library(Zelig) does what I'm trying to achieve.
>
> Thanks for your help! And sorry if there's an obvious answer to this that I
> am oblivious to.
> -Thomas
>

?.onAttach
?packageStartupMessage

(You could also look at the sources for package Zelig, but actually it 
is a poor example since it ignores the arguments to .onAttach, and 
wastes time calling packageDescription() twice.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From murdoch.duncan at gmail.com  Sun Apr  8 15:07:59 2012
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Sun, 08 Apr 2012 09:07:59 -0400
Subject: [Rd] Display instruction text on package load
In-Reply-To: <CAOC91MTM+YPrsWhX4S31BRJcJ-YJddhGqXxZ8Oc4VngV31vWEQ@mail.gmail.com>
References: <CAOC91MTM+YPrsWhX4S31BRJcJ-YJddhGqXxZ8Oc4VngV31vWEQ@mail.gmail.com>
Message-ID: <4F818DAF.5080904@gmail.com>

On 12-04-07 5:27 PM, Thomas J. Leeper wrote:
> I'm currently working on a new package for R that interfaces with an API.
> Most of the package's functionality requires users to supply an access key
> that allows the package to work with the API. I want to display text
> immediately upon loading the package that instructs/reminds users to enter
> their access key before attempting to do anything (as the package won't
> really work without it). I've searched the relevant help files for creating
> packages but haven't been able find anything about how to do this. As an
> example, library(Zelig) does what I'm trying to achieve.
>
> Thanks for your help! And sorry if there's an obvious answer to this that I
> am oblivious to.
> -Thomas
>

You can create functions that run when your package is attached (put 
onto the search list) or loaded (loaded into memory, possibly because 
another package is using it).  You can read about these in the "Load 
Hooks" section of the Writing R Extensions manual, or the ?.onLoad help 
page.

Duncan Murdoch


From thosjleeper at gmail.com  Sun Apr  8 15:13:18 2012
From: thosjleeper at gmail.com (Thomas J. Leeper)
Date: Sun, 8 Apr 2012 08:13:18 -0500
Subject: [Rd] Display instruction text on package load
In-Reply-To: <CAOC91MTM+YPrsWhX4S31BRJcJ-YJddhGqXxZ8Oc4VngV31vWEQ@mail.gmail.com>
References: <CAOC91MTM+YPrsWhX4S31BRJcJ-YJddhGqXxZ8Oc4VngV31vWEQ@mail.gmail.com>
Message-ID: <CAOC91MRTmdQ4HWr_LQYd9ARA=H9iFFhMfg8ooPjoGXqsihAPSw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20120408/73d93834/attachment.pl>

From timhesterberg at gmail.com  Sun Apr  8 20:37:05 2012
From: timhesterberg at gmail.com (Tim Hesterberg)
Date: Sun, 08 Apr 2012 11:37:05 -0700
Subject: [Rd] Add DUP = FALSE when tabulate() calls .C("R_tabulate"
Message-ID: <yajfr4vyoysu.fsf@gmail.com>

In base/R/tabulate.R, tabulate() calls .C("R_tabulate";
I suggest adding DUP = FALSE to that call.


From hb at biostat.ucsf.edu  Sun Apr  8 22:38:05 2012
From: hb at biostat.ucsf.edu (Henrik Bengtsson)
Date: Sun, 8 Apr 2012 13:38:05 -0700
Subject: [Rd] Proposal: Mechanism for controlling the amount of testing 'R
 CMD check' performs
Message-ID: <CAFDcVCRt+ZDczWTNdpRd9ao2Rx=7_2fFkqK7ax5UBcSP6+0PLw@mail.gmail.com>

Hi,

I'd like to propose to introduce a standardized mechanism for
controlling the amount of testing that 'R CMD check' performs together
with an option to 'R CMD check' to specify if shallow or deep tests
should be performed.  I believe such a standardized feature would be
useful, especially because the recent CRAN Policies needs to limit the
computational load on CRAN servers, introducing an unfortunately
conflict between writing writing thorough system tests and having
light-weight tests for CRAN.

My solution to this has been to introduce use a system environment
'_R_CHECK_FULL_', which I set to TRUE (or leave blank), and have my
tests/*.R scripts test for this in order to either skip the test
completely or do for instance down sample the data tested, e.g.

if (Sys.getenv("_R_CHECK_FULL_") != "TRUE") {
  ...
}

Instead of such home-cooked solutions, I call for a more standardized
mechanism, e.g.

if (tools::checkLoadOK("CRAN")) {
  ...
}

and

R CMD check --allowed-load=CRAN ...

where predefined levels could be NONE < MINOR < CRAN < DEFAULT < MAJOR
< FULL, or similar.

This would also be useful in the iterative process of
developing/testing new features of one's package.

Comments?

/Henrik


From ripley at stats.ox.ac.uk  Mon Apr  9 10:48:29 2012
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 09 Apr 2012 09:48:29 +0100
Subject: [Rd] Add DUP = FALSE when tabulate() calls .C("R_tabulate"
In-Reply-To: <yajfr4vyoysu.fsf@gmail.com>
References: <yajfr4vyoysu.fsf@gmail.com>
Message-ID: <4F82A25D.7060901@stats.ox.ac.uk>

On 08/04/2012 19:37, Tim Hesterberg wrote:
> In base/R/tabulate.R, tabulate() calls .C("R_tabulate";
> I suggest adding DUP = FALSE to that call.

We can do that.  Note that R-patched already does less copying[*], but 
adding DUP=FALSE may avoid one copy of 'dup' if it is an integer vector. 
  We can avoid another for a factor ....

[*] tracemem() is still reporting 'copies' that are not full duplications.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From malang at sbumail.stonybrook.edu  Tue Apr 10 18:13:04 2012
From: malang at sbumail.stonybrook.edu (Mark Lang)
Date: Tue, 10 Apr 2012 16:13:04 +0000
Subject: [Rd] Higher Resolution Icons for Windows / Mac
In-Reply-To: <EF6341EDB5E43246A46919578B423D8D1874AA35@EXCHMB1.campus.stonybrook.edu>
References: <EF6341EDB5E43246A46919578B423D8D1874AA35@EXCHMB1.campus.stonybrook.edu>
Message-ID: <EF6341EDB5E43246A46919578B423D8D1874B80E@EXCHMB1.campus.stonybrook.edu>

Hi,
I have installed R on a Windows 7 machine and wanted to offer a set of icons that are higher resolution than the existing icon.? The icon set uses the R logo available from this link: http://developer.r-project.org/Logo/ , specifically http://developer.r-project.org/Logo/Rlogo.svg .? The first icon is the entire R Logo, while the second icon is simply the R logo without the associated oval behind it.? The icons should scale in Windows 7 to 256x256 pixels, and will scale downwards, as well.? There are no jagged edges on the icons.

I am offering these free and clear and for total use by the R project.  We have a few faculty here that want to use R, and as I have been installing the software I noticed the low resolution icon.  Creating a higher resolution icon is an easy task for me, so I created it and wanted to share it.  

I have uploaded the files here: http://www.somas.stonybrook.edu/downloads/Rprojecticons.zip 

Please let me know if there are any problems.  There are four files contained within the .zip file:  Rproject1.icns, Rproject1.ico, Rproject2.icns, Rproject2.ico.  The .icns files are Mac-formatted icon files while the .ico files are Windows-formatted icon files.  

Again, the files were created using your previously posted logos, so I give them to you under the Creative Commons Attribution ShareAlike license.

Thank you,

Mark Lang
School of Marine and Atmospheric Sciences
Stony Brook University
133 Endeavour Hall
Mark.lang at stonybrook.edu

http://somas.stonybrook.edu 


From proebuck at mdanderson.org  Tue Apr 10 21:00:15 2012
From: proebuck at mdanderson.org (Roebuck,Paul L)
Date: Tue, 10 Apr 2012 14:00:15 -0500
Subject: [Rd] R CMD check returns NOTE about package data set as global
 variable
In-Reply-To: <1333746244.24441.44.camel@brian-desktop>
Message-ID: <CBA9ED6F.2C279%proebuck@mdanderson.org>

On 4/6/12 4:04 PM, "Brian G. Peterson" <brian at braverock.com> wrote:

> Honestly, this is just another example of a non-helpful 'global
variable'
> NOTE.  I've removed many of these from our packages, often by
resorting to
> useless workarounds like this one, but I have never once
gotten a valid NOTE
> out of this message.  We provided other examples
earlier in a different
> thread.


While I have seen a couple valid ones, it gets really old having to explain
these NOTEs to user community. It would really be nice to have something
equivalent to LINT comment directives (i.e., NOTREACHED, ARGSUSED, etc.)
that could be used to suppress "noise" messages.


From Ken.Williams at windlogics.com  Wed Apr 11 17:28:02 2012
From: Ken.Williams at windlogics.com (Ken Williams)
Date: Wed, 11 Apr 2012 10:28:02 -0500
Subject: [Rd] [patch] giving library() a 'version' argument
Message-ID: <21A5E1E970CD46459ECBE86D6CC4B28C667AA79C@spexch01.WindLogics.local>

I've made a small enhancement to R that would help developers better control what versions of code we're using where.  Basically, to load a package in R, one currently does:

                library(whateverPackage)

and with the enhancement, you can ensure that you're getting at least version X of the package:

                library(whateverPackage, version=3.14)

Reasons one might want this include:

  * you know that in version X some bug was fixed
  * you know that in version X some feature was added
  * that's the first version you've actually tested it with & you don't want to vouch for earlier versions without testing
  * you develop on one machine & deploy on another machine you don't control, and you want runtime checks that the sysadmin installed what they were supposed to install

In general, I have an interest in helping R get better at various things that would help it play in a "production environment", for various values of that term. =)

The attached patch is made against revision 58980 of https://svn.r-project.org/R/trunk .  I think this is the first patch I've submitted to the R core, so please let me know if anything's amiss, or of course if there are reservations about the approach.

Thanks.

--
Ken Williams, Senior Research Scientist
WindLogics
http://windlogics.com



CONFIDENTIALITY NOTICE: This e-mail message is for the sole use of the intended recipient(s) and may contain confidential and privileged information. Any unauthorized review, use, disclosure or distribution of any kind is strictly prohibited. If you are not the intended recipient, please contact the sender via reply e-mail and destroy all copies of the original message. Thank you.

From therneau at mayo.edu  Wed Apr 11 22:41:44 2012
From: therneau at mayo.edu (Terry Therneau)
Date: Wed, 11 Apr 2012 15:41:44 -0500
Subject: [Rd] Vignette questions
Message-ID: <4F85EC88.8070604@mayo.edu>

Context: R2.15-0 on Ubuntu.

1. I get a WARNING from CMD check for "Package vignette(s) without 
corresponding PDF:
   In this case the vignettes directory had both the pdf and Rnw; do I 
need to move the pdf to inst/doc?

    I'm reluctant to add the pdf to the svn source on Rforge, per the 
usual rule that a code management system should not have both a primary 
source and a object dervived from it under version control.  However, if 
this is the suggested norm I could do so.

2. Close reading of the paragraph about vignette sources shows the 
following -- I think?  If I have a vignette that should not be rebuilt 
by "check" or "BUILD" I should put the .Rnw source and pdf in /inst/doc, 
and have the others that should be rebuilt in /vignettes.  This would 
include any that use "private R packages, screen snapshots, ...", or in 
my case one that takes just a little short of forever to run.

3. Do these unprocessed package also contribute to the index via 
\VignetteIndexEntry lines, or will I need to create a custom index?

Terry Therneau


From mdowle at mdowle.plus.com  Wed Apr 11 21:36:34 2012
From: mdowle at mdowle.plus.com (Matthew Dowle)
Date: Wed, 11 Apr 2012 20:36:34 +0100
Subject: [Rd] Byte compilation of packages on CRAN
Message-ID: <1334172994.7257.46.camel@netbook>

In DESCRIPTION if I set LazyLoad to 'yes' will data.table (for example)
then be byte compiled for users who install the binary package from CRAN
on Windows?
This question is based on reading section 1.2 of this document :
http://www.divms.uiowa.edu/~luke/R/compiler/compiler.pdf
I've searched r-devel and Stack Overflow history and have found
questions and answers relating to R CMD INSTALL and install.packages()
from source, but no answer (as yet) about why binary packages for
Windows appear not to be byte compiled. 
If so, is there any reason why all packages should not set LazyLoad to
'yes'. And if not, could LazyLoad be 'yes' by default?
Thanks,
Matthew


From xie at yihui.name  Wed Apr 11 23:28:39 2012
From: xie at yihui.name (Yihui Xie)
Date: Wed, 11 Apr 2012 16:28:39 -0500
Subject: [Rd] Vignette questions
In-Reply-To: <4F85EC88.8070604@mayo.edu>
References: <4F85EC88.8070604@mayo.edu>
Message-ID: <CANROs4c7b5ThXTXRXt1BARYF1O3SG2s39X3iQEXHAVvjZQttaQ@mail.gmail.com>

For 1, you should run R CMD check on the tar ball (pkg_x.x.tar.gz)
from R CMD build instead of the source directory. R CMD build will
build the PDF vignette into the tar ball.

For 2, I have been confused by ./vignettes vs ./inst/doc since
./vignettes was introduced. I might be able to figure it out by
try-and-err but I never tried, and I'm still sticking to ./inst/doc.
At least you can exclude the Rnw source in .Rbuildignore so that R can
only stare at your PDF documents and sigh.

For 3, I remember some of us requested that R could also respect
entries of non-Sweave vignettes (like the ones in ./demo/00Index), but
this is not possible as far as I know. However, I can tell you a dark
voodoo seems to be still working: you can write your own index.html
under ./inst/doc with your own links to vignettes.

Regards,
Yihui
--
Yihui Xie <xieyihui at gmail.com>
Phone: 515-294-2465 Web: http://yihui.name
Department of Statistics, Iowa State University
2215 Snedecor Hall, Ames, IA



On Wed, Apr 11, 2012 at 3:41 PM, Terry Therneau <therneau at mayo.edu> wrote:
> Context: R2.15-0 on Ubuntu.
>
> 1. I get a WARNING from CMD check for "Package vignette(s) without
> corresponding PDF:
> ?In this case the vignettes directory had both the pdf and Rnw; do I need to
> move the pdf to inst/doc?
>
> ? I'm reluctant to add the pdf to the svn source on Rforge, per the usual
> rule that a code management system should not have both a primary source and
> a object dervived from it under version control. ?However, if this is the
> suggested norm I could do so.
>
> 2. Close reading of the paragraph about vignette sources shows the following
> -- I think? ?If I have a vignette that should not be rebuilt by "check" or
> "BUILD" I should put the .Rnw source and pdf in /inst/doc, and have the
> others that should be rebuilt in /vignettes. ?This would include any that
> use "private R packages, screen snapshots, ...", or in my case one that
> takes just a little short of forever to run.
>
> 3. Do these unprocessed package also contribute to the index via
> \VignetteIndexEntry lines, or will I need to create a custom index?
>
> Terry Therneau
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From claudia.beleites at ipht-jena.de  Thu Apr 12 00:23:16 2012
From: claudia.beleites at ipht-jena.de (beleites,claudia)
Date: Thu, 12 Apr 2012 00:23:16 +0200
Subject: [Rd] Vignette questions
In-Reply-To: <CANROs4c7b5ThXTXRXt1BARYF1O3SG2s39X3iQEXHAVvjZQttaQ@mail.gmail.com>
References: <4F85EC88.8070604@mayo.edu>,
	<CANROs4c7b5ThXTXRXt1BARYF1O3SG2s39X3iQEXHAVvjZQttaQ@mail.gmail.com>
Message-ID: <A26E51EB75D20B4DAC8AEA2A883BF3997B50587DA5@uranus.ipht-jena.de>

Very quick & short answer:

I made the transition to  ./vignettes for hyperSpec (you can look at the source at r-forge) - it was a mess. It is almost working now (compression is missing, I'll have to figure out how to invoke ghostscript in an OS idependent way, qpdf doesn't give me the compression rates). 

I have .pdf is under version control only for those vignettes that I build externally (with the fake .Rnws that produce the proper indexing as vignette).  They are all (regardless whether BUILD and check affect them or not) in ./vignettes.
I think that strategy can at least partially solve the issue with non-Sweave vignettes as well as long as the result is a pdf.  
You may need to play with ./vignettes/.install_extras if additional, non pdf and non Rnw files are needed.


Best,

Claudia


________________________________________
Von: r-devel-bounces at r-project.org [r-devel-bounces at r-project.org] im Auftrag von Yihui Xie [xie at yihui.name]
Gesendet: Mittwoch, 11. April 2012 23:28
An: Terry Therneau
Cc: r-devel at r-project.org
Betreff: Re: [Rd] Vignette questions

For 1, you should run R CMD check on the tar ball (pkg_x.x.tar.gz)
from R CMD build instead of the source directory. R CMD build will
build the PDF vignette into the tar ball.

For 2, I have been confused by ./vignettes vs ./inst/doc since
./vignettes was introduced. I might be able to figure it out by
try-and-err but I never tried, and I'm still sticking to ./inst/doc.
At least you can exclude the Rnw source in .Rbuildignore so that R can
only stare at your PDF documents and sigh.

For 3, I remember some of us requested that R could also respect
entries of non-Sweave vignettes (like the ones in ./demo/00Index), but
this is not possible as far as I know. However, I can tell you a dark
voodoo seems to be still working: you can write your own index.html
under ./inst/doc with your own links to vignettes.

Regards,
Yihui
--
Yihui Xie <xieyihui at gmail.com>
Phone: 515-294-2465 Web: http://yihui.name
Department of Statistics, Iowa State University
2215 Snedecor Hall, Ames, IA



On Wed, Apr 11, 2012 at 3:41 PM, Terry Therneau <therneau at mayo.edu> wrote:
> Context: R2.15-0 on Ubuntu.
>
> 1. I get a WARNING from CMD check for "Package vignette(s) without
> corresponding PDF:
>  In this case the vignettes directory had both the pdf and Rnw; do I need to
> move the pdf to inst/doc?
>
>   I'm reluctant to add the pdf to the svn source on Rforge, per the usual
> rule that a code management system should not have both a primary source and
> a object dervived from it under version control.  However, if this is the
> suggested norm I could do so.
>
> 2. Close reading of the paragraph about vignette sources shows the following
> -- I think?  If I have a vignette that should not be rebuilt by "check" or
> "BUILD" I should put the .Rnw source and pdf in /inst/doc, and have the
> others that should be rebuilt in /vignettes.  This would include any that
> use "private R packages, screen snapshots, ...", or in my case one that
> takes just a little short of forever to run.
>
> 3. Do these unprocessed package also contribute to the index via
> \VignetteIndexEntry lines, or will I need to create a custom index?
>
> Terry Therneau
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

______________________________________________
R-devel at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-devel


From andre.zege at gmail.com  Thu Apr 12 00:53:10 2012
From: andre.zege at gmail.com (andre zege)
Date: Wed, 11 Apr 2012 18:53:10 -0400
Subject: [Rd] unexpectedly high memory use in R 2.14.0
Message-ID: <CACU3EkMATY=_GZJuOo5gGGKXFS4iOvCMdyeAtNNRmKqbULN7kw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20120411/7f0cf064/attachment.pl>

From pdalgd at gmail.com  Thu Apr 12 01:15:42 2012
From: pdalgd at gmail.com (peter dalgaard)
Date: Thu, 12 Apr 2012 01:15:42 +0200
Subject: [Rd] unexpectedly high memory use in R 2.14.0
In-Reply-To: <CACU3EkMATY=_GZJuOo5gGGKXFS4iOvCMdyeAtNNRmKqbULN7kw@mail.gmail.com>
References: <CACU3EkMATY=_GZJuOo5gGGKXFS4iOvCMdyeAtNNRmKqbULN7kw@mail.gmail.com>
Message-ID: <2DFCAA94-CFFA-45C5-A280-C099A40E7619@gmail.com>


On Apr 12, 2012, at 00:53 , andre zege wrote:

> I recently started using R 2.14.0 on a new machine and i am  experiencing
> what seems like unusually greedy memory use. It happens all the time, but
> to give a specific example, let's say i run the following code
> 
> --------
> 
> for(j in 1:length(files)){
>      load(file.path(dump.dir, files[j]))
>      mat.data[[j]]<-data
> }
> save(abind(mat.data, along=2), file.path(dump.dir, filename))

Hmm, did you preallocate mat.data? If not, you will be copying it repeatedly, and I'm not sure that this can be done by copying pointers only. 

Does it work better with 

mat.data <- lapply(files, function(name) {load(file.path(dump.dir, name); data})

?


> 
> ---------
> 
> It loads parts of multidimensional matrix into a list, then binds it along
> second dimension and saves on disk. Code works, although slowly, but what's
> strange is the amount of memory it uses.
> In particular, each chunk of data is between 50M to 100M, and altogether
> the binded matrix is 1.3G. One would expect that R would use roughly double
> that memory - to keep mat.data and its binded version separately, or 1G. I
> could imagine that for somehow it could use 3 times the size of matrix. But
> in fact it uses more than 5.5 times (almost all of my physical memory) and
> i think is swapping a lot to disk . For this particular task, my top output
> shows eating more than 7G of memory and using up 11G of virtual memory as
> well
> 
> $top
> 
> PID    USER      PR  NI  VIRT    RES  SHR   S %CPU %MEM    TIME+  COMMAND
> 8823  user        25   0  11g     7.2g  10m   R   99.7     92.9
> 5:55.05
> R
> 
> 8590   root       15   0  154m   16m   5948  S  0.5      0.2
> 23:22.40 Xorg
> 
> 
> I have strong suspicion that something is off with my R binary, i don't
> think i experienced things like that in a long time. Is this in line with
> what i am supposed to experience? Are there any ideas for diagnosing what
> is going on?
> Would appreciate any suggestions
> 
> Thanks
> Andre
> 
> 
> ==================================
> 
> Here is what i am running on:
> 
> 
> CentOS release 5.5 (Final)
> 
> 
>> sessionInfo()
> R version 2.14.0 (2011-10-31)
> Platform: x86_64-unknown-linux-gnu (64-bit)
> 
> locale:
> [1] en_US.UTF-8
> 
> attached base packages:
> [1] stats     graphics  grDevices datasets  utils     methods   base
> 
> other attached packages:
> [1] abind_1.4-0       rJava_0.9-3       R.utils_1.12.1    R.oo_1.9.3
> R.methodsS3_1.2.2
> 
> loaded via a namespace (and not attached):
> [1] codetools_0.2-8 tcltk_2.14.0    tools_2.14.0
> 
> 
> 
> I compiled R configure as follows
> /configure --prefix=/usr/local/R --enable-byte-compiled-packages=no
> --with-tcltk --enable-R-shlib=yes
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From pgilbert902 at gmail.com  Thu Apr 12 01:16:42 2012
From: pgilbert902 at gmail.com (Paul Gilbert)
Date: Wed, 11 Apr 2012 19:16:42 -0400
Subject: [Rd] Vignette questions
In-Reply-To: <4F85EC88.8070604@mayo.edu>
References: <4F85EC88.8070604@mayo.edu>
Message-ID: <4F8610DA.80908@gmail.com>



On 12-04-11 04:41 PM, Terry Therneau wrote:
> Context: R2.15-0 on Ubuntu.
>
> 1. I get a WARNING from CMD check for "Package vignette(s) without
> corresponding PDF:
> In this case the vignettes directory had both the pdf and Rnw; do I need
> to move the pdf to inst/doc?

Yes, you need to put the pdf in the inst/doc directory if it cannot be 
built by R-forge and CRAN check machines, but leave the Rnw in the 
vignettes directory.
>
> I'm reluctant to add the pdf to the svn source on Rforge, per the usual
> rule that a code management system should not have both a primary source
> and a object dervived from it under version control. However, if this is
> the suggested norm I could do so.

Yes, I think this is the norm if the vignette cannot be built on CRAN 
and R-forge, even though it does seem a bit strange. However, you do not 
necessarily need to update the vignette pdf in inst/doc every time you 
make a change to the package even though, in my opinion, the correct 
logic is to test remaking the vignette when you make a change to the 
package. You should do this testing, of course, you just do not need to 
put the new pdf in inst/doc and commit it to svn each time. (But you 
should probably do that before you build the final package to put on CRAN.)

>
> 2. Close reading of the paragraph about vignette sources shows the
> following -- I think? If I have a vignette that should not be rebuilt by
> "check" or "BUILD" I should put the .Rnw source and pdf in /inst/doc,
> and have the others that should be rebuilt in /vignettes. This would
> include any that use "private R packages, screen snapshots, ...", or in
> my case one that takes just a little short of forever to run.

I don't think it is intended to say that, and I didn't read it that way. 
I think putting the Rnw in inst/doc is supported (temporarily?) for 
historical reasons only. If it is not in vignettes/ and is found in 
inst/doc/, it is treated the same way as if it were in vignettes/.

You can include screen snapshots, etc, in either case. For your 
situation, what you probably do need to do is specify  "BuildVignettes: 
false" in the DESCRIPTION file. This prevents the pdf for inst/doc from 
being generated by the the Rnw. However, it does not prevent R CMD check 
from checking that the R code extracted from the Rnw actually runs, and 
generating an error if it does not. To prevent testing of the R code, 
you have to appeal directly to the CRAN and R-forge maintainers, and 
they will put the package on a special list. You do need to give them a 
good reason why the code should not be tested. I think they are 
sympathetic with "takes forever to run" and not very sympathetic with 
"does not work anymore". Generally, I think they want to consider doing 
this only in exceptional cases, so they do not get in a situation of 
having lots of broken vignettes. (One should stick with journal articles 
for recording broken code.)

> 3. Do these unprocessed package also contribute to the index via
> \VignetteIndexEntry lines, or will I need to create a custom index?

I'm not sure of the answer to this, but would be curious to know. You 
may need to rely on voodoo.

Paul
>
> Terry Therneau
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From andre.zege at gmail.com  Thu Apr 12 02:17:40 2012
From: andre.zege at gmail.com (andre zege)
Date: Wed, 11 Apr 2012 20:17:40 -0400
Subject: [Rd] unexpectedly high memory use in R 2.14.0
In-Reply-To: <2DFCAA94-CFFA-45C5-A280-C099A40E7619@gmail.com>
References: <CACU3EkMATY=_GZJuOo5gGGKXFS4iOvCMdyeAtNNRmKqbULN7kw@mail.gmail.com>
	<2DFCAA94-CFFA-45C5-A280-C099A40E7619@gmail.com>
Message-ID: <CACU3EkNYW7-PrpM=UgAyVrEWCN3gpGNQ8kEw-bjsqe1C5JZQgA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20120411/4b4de15b/attachment.pl>

From hb at biostat.ucsf.edu  Thu Apr 12 03:02:13 2012
From: hb at biostat.ucsf.edu (Henrik Bengtsson)
Date: Wed, 11 Apr 2012 18:02:13 -0700
Subject: [Rd] unexpectedly high memory use in R 2.14.0
In-Reply-To: <CACU3EkMATY=_GZJuOo5gGGKXFS4iOvCMdyeAtNNRmKqbULN7kw@mail.gmail.com>
References: <CACU3EkMATY=_GZJuOo5gGGKXFS4iOvCMdyeAtNNRmKqbULN7kw@mail.gmail.com>
Message-ID: <CAFDcVCSi1wgAMMHdOdHBK-SUNXqaWv8_=p3AX30nyFyNCz-sFQ@mail.gmail.com>

Leaving aside what's going on inside abind::abind(), maybe the
following sheds some light on what's is being wasted:

# Preallocate (probably doesn't make a difference because it's a list)
mat.data <- vector("list", length=length(files));
for (j in 1:length(files)){
     vars <- load(file.path(dump.dir, files[j]))
     mat.data[[j]]<-data;
      # Not needed anymore/remove everything loaded
     rm(list=vars);
}

data <- abind(mat.data, along=2);
# Not needed anymore
rm(mat.data);

save(data, file.path(dump.dir, filename))

My $.02
/Henrik

On Wed, Apr 11, 2012 at 3:53 PM, andre zege <andre.zege at gmail.com> wrote:
> I recently started using R 2.14.0 on a new machine and i am ?experiencing
> what seems like unusually greedy memory use. It happens all the time, but
> to give a specific example, let's say i run the following code
>
> --------
>
> for(j in 1:length(files)){
> ? ? ?load(file.path(dump.dir, files[j]))
> ? ? ?mat.data[[j]]<-data
> }
> save(abind(mat.data, along=2), file.path(dump.dir, filename))
>
> ---------
>
> It loads parts of multidimensional matrix into a list, then binds it along
> second dimension and saves on disk. Code works, although slowly, but what's
> strange is the amount of memory it uses.
> In particular, each chunk of data is between 50M to 100M, and altogether
> the binded matrix is 1.3G. One would expect that R would use roughly double
> that memory - to keep mat.data and its binded version separately, or 1G. I
> could imagine that for somehow it could use 3 times the size of matrix. But
> in fact it uses more than 5.5 times (almost all of my physical memory) and
> i think is swapping a lot to disk . For this particular task, my top output
> shows eating more than 7G of memory and using up 11G of virtual memory as
> well
>
> $top
>
> PID ? ?USER ? ? ?PR ?NI ?VIRT ? ?RES ?SHR ? S %CPU %MEM ? ?TIME+ ?COMMAND
> 8823 ?user ? ? ? ?25 ? 0 ?11g ? ? 7.2g ?10m ? R ? 99.7 ? ? 92.9
> 5:55.05
> R
>
> 8590 ? root ? ? ? 15 ? 0 ?154m ? 16m ? 5948 ?S ?0.5 ? ? ?0.2
> 23:22.40 Xorg
>
>
> I have strong suspicion that something is off with my R binary, i don't
> think i experienced things like that in a long time. Is this in line with
> what i am supposed to experience? Are there any ideas for diagnosing what
> is going on?
> Would appreciate any suggestions
>
> Thanks
> Andre
>
>
> ==================================
>
> Here is what i am running on:
>
>
> CentOS release 5.5 (Final)
>
>
>> sessionInfo()
> R version 2.14.0 (2011-10-31)
> Platform: x86_64-unknown-linux-gnu (64-bit)
>
> locale:
> [1] en_US.UTF-8
>
> attached base packages:
> [1] stats ? ? graphics ?grDevices datasets ?utils ? ? methods ? base
>
> other attached packages:
> [1] abind_1.4-0 ? ? ? rJava_0.9-3 ? ? ? R.utils_1.12.1 ? ?R.oo_1.9.3
> R.methodsS3_1.2.2
>
> loaded via a namespace (and not attached):
> [1] codetools_0.2-8 tcltk_2.14.0 ? ?tools_2.14.0
>
>
>
> I compiled R configure as follows
> /configure --prefix=/usr/local/R --enable-byte-compiled-packages=no
> --with-tcltk --enable-R-shlib=yes
>
> ? ? ? ?[[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From ripley at stats.ox.ac.uk  Thu Apr 12 07:21:51 2012
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 12 Apr 2012 06:21:51 +0100
Subject: [Rd] Byte compilation of packages on CRAN
In-Reply-To: <1334172994.7257.46.camel@netbook>
References: <1334172994.7257.46.camel@netbook>
Message-ID: <4F86666F.1000905@stats.ox.ac.uk>

On 11/04/2012 20:36, Matthew Dowle wrote:
> In DESCRIPTION if I set LazyLoad to 'yes' will data.table (for example)
> then be byte compiled for users who install the binary package from CRAN
> on Windows?

No.  LazyLoad is distinct from byte compilation.  All installed packages 
use lazy loading these days (for simplicity: a very few do not benefit 
from it as they use all their objects at startup).

> This question is based on reading section 1.2 of this document :
> http://www.divms.uiowa.edu/~luke/R/compiler/compiler.pdf
> I've searched r-devel and Stack Overflow history and have found
> questions and answers relating to R CMD INSTALL and install.packages()
> from source, but no answer (as yet) about why binary packages for
> Windows appear not to be byte compiled.
> If so, is there any reason why all packages should not set LazyLoad to
> 'yes'. And if not, could LazyLoad be 'yes' by default?

I wonder why you are not reading R's own documentation.  'Writing R 
Extensions' says

'The `LazyData' logical field controls whether the R datasets use 
lazy-loading. A `LazyLoad' field was used in versions prior to 2.14.0, 
but now is ignored.

The `ByteCompile' logical field controls if the package code is 
byte-compiled on installation: the default is currently not to, so this 
may be useful for a package known to benefit particularly from 
byte-compilation (which can take quite a long time and increases the 
installed size of the package).'

Note that the majority of CRAN packages benefit very little from 
byte-compilation because almost all the time of their computations is 
spent in compiled code.  And the increased size also may matter when the 
code is loaded into R.

> Thanks,
> Matthew
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From Ken.Williams at windlogics.com  Wed Apr 11 18:18:29 2012
From: Ken.Williams at windlogics.com (Ken Williams)
Date: Wed, 11 Apr 2012 11:18:29 -0500
Subject: [Rd] [patch] giving library() a 'version' argument
Message-ID: <21A5E1E970CD46459ECBE86D6CC4B28C667AA7D5@spexch01.WindLogics.local>

Apparently the patch file got eaten.  Let me try again with a .txt extension.

 -Ken

> -----Original Message-----
> From: Ken Williams
> Sent: Wednesday, April 11, 2012 10:28 AM
> To: r-devel at r-project.org
> Subject: [patch] giving library() a 'version' argument
>
> I've made a small enhancement to R that would help developers better
> control what versions of code we're using where.
> [...]


CONFIDENTIALITY NOTICE: This e-mail message is for the sole use of the intended recipient(s) and may contain confidential and privileged information. Any unauthorized review, use, disclosure or distribution of any kind is strictly prohibited. If you are not the intended recipient, please contact the sender via reply e-mail and destroy all copies of the original message. Thank you.
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: lib-version-patch.txt
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20120411/ac7ce0d7/attachment.txt>

From mdowle at mdowle.plus.com  Thu Apr 12 08:54:17 2012
From: mdowle at mdowle.plus.com (Matthew Dowle)
Date: Thu, 12 Apr 2012 07:54:17 +0100
Subject: [Rd] Byte compilation of packages on CRAN
In-Reply-To: <4F86666F.1000905@stats.ox.ac.uk>
References: <1334172994.7257.46.camel@netbook>
	<4F86666F.1000905@stats.ox.ac.uk>
Message-ID: <b3988354c7b278bc0958e5315c3c8edd.squirrel@webmail.plus.net>

> On 11/04/2012 20:36, Matthew Dowle wrote:
>> In DESCRIPTION if I set LazyLoad to 'yes' will data.table (for example)
>> then be byte compiled for users who install the binary package from CRAN
>> on Windows?
>
> No.  LazyLoad is distinct from byte compilation.  All installed packages
> use lazy loading these days (for simplicity: a very few do not benefit
> from it as they use all their objects at startup).
>
>> This question is based on reading section 1.2 of this document :
>> http://www.divms.uiowa.edu/~luke/R/compiler/compiler.pdf
>> I've searched r-devel and Stack Overflow history and have found
>> questions and answers relating to R CMD INSTALL and install.packages()
>> from source, but no answer (as yet) about why binary packages for
>> Windows appear not to be byte compiled.
>> If so, is there any reason why all packages should not set LazyLoad to
>> 'yes'. And if not, could LazyLoad be 'yes' by default?
>
> I wonder why you are not reading R's own documentation.  'Writing R
> Extensions' says
>
> 'The `LazyData' logical field controls whether the R datasets use
> lazy-loading. A `LazyLoad' field was used in versions prior to 2.14.0,
> but now is ignored.
>
> The `ByteCompile' logical field controls if the package code is
> byte-compiled on installation: the default is currently not to, so this
> may be useful for a package known to benefit particularly from
> byte-compilation (which can take quite a long time and increases the
> installed size of the package).'
>

Oops, somehow missed that. Thank you!

> Note that the majority of CRAN packages benefit very little from
> byte-compilation because almost all the time of their computations is
> spent in compiled code.  And the increased size also may matter when the
> code is loaded into R.
>
>> Thanks,
>> Matthew
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>


From ligges at statistik.tu-dortmund.de  Thu Apr 12 09:15:41 2012
From: ligges at statistik.tu-dortmund.de (Uwe Ligges)
Date: Thu, 12 Apr 2012 09:15:41 +0200
Subject: [Rd] Vignette questions
In-Reply-To: <4F8610DA.80908@gmail.com>
References: <4F85EC88.8070604@mayo.edu> <4F8610DA.80908@gmail.com>
Message-ID: <4F86811D.5040201@statistik.tu-dortmund.de>



On 12.04.2012 01:16, Paul Gilbert wrote:
>
>
> On 12-04-11 04:41 PM, Terry Therneau wrote:
>> Context: R2.15-0 on Ubuntu.
>>
>> 1. I get a WARNING from CMD check for "Package vignette(s) without
>> corresponding PDF:
>> In this case the vignettes directory had both the pdf and Rnw; do I need
>> to move the pdf to inst/doc?
>
> Yes, you need to put the pdf in the inst/doc directory if it cannot be
> built by R-forge and CRAN check machines, but leave the Rnw in the
> vignettes directory.

No, this is all done automatically by R CMD build, hence you do not need 
to worry.


>>
>> I'm reluctant to add the pdf to the svn source on Rforge, per the usual
>> rule that a code management system should not have both a primary source
>> and a object dervived from it under version control. However, if this is
>> the suggested norm I could do so.
>
> Yes, I think this is the norm if the vignette cannot be built on CRAN
> and R-forge,

Well, yours are that specific that they rely on third party software. 
Vignettes "only" depending on R and installed packages that are declared 
as dependencies can be build by CRAN.


> even though it does seem a bit strange. However, you do not
> necessarily need to update the vignette pdf in inst/doc every time you
> make a change to the package even though, in my opinion, the correct
> logic is to test remaking the vignette when you make a change to the
> package. You should do this testing, of course, you just do not need to
> put the new pdf in inst/doc and commit it to svn each time. (But you
> should probably do that before you build the final package to put on CRAN.)

R CMD build will rebuild vignettes unless you ask R not to do so.

Uwe


>>
>> 2. Close reading of the paragraph about vignette sources shows the
>> following -- I think? If I have a vignette that should not be rebuilt by
>> "check" or "BUILD" I should put the .Rnw source and pdf in /inst/doc,
>> and have the others that should be rebuilt in /vignettes. This would
>> include any that use "private R packages, screen snapshots, ...", or in
>> my case one that takes just a little short of forever to run.
>
> I don't think it is intended to say that, and I didn't read it that way.
> I think putting the Rnw in inst/doc is supported (temporarily?) for
> historical reasons only. If it is not in vignettes/ and is found in
> inst/doc/, it is treated the same way as if it were in vignettes/.
>
> You can include screen snapshots, etc, in either case. For your
> situation, what you probably do need to do is specify "BuildVignettes:
> false" in the DESCRIPTION file. This prevents the pdf for inst/doc from
> being generated by the the Rnw. However, it does not prevent R CMD check
> from checking that the R code extracted from the Rnw actually runs, and
> generating an error if it does not. To prevent testing of the R code,
> you have to appeal directly to the CRAN and R-forge maintainers, and
> they will put the package on a special list. You do need to give them a
> good reason why the code should not be tested. I think they are
> sympathetic with "takes forever to run" and not very sympathetic with
> "does not work anymore". Generally, I think they want to consider doing
> this only in exceptional cases, so they do not get in a situation of
> having lots of broken vignettes. (One should stick with journal articles
> for recording broken code.)
>
>> 3. Do these unprocessed package also contribute to the index via
>> \VignetteIndexEntry lines, or will I need to create a custom index?
>
> I'm not sure of the answer to this, but would be curious to know. You
> may need to rely on voodoo.
>
> Paul
>>
>> Terry Therneau
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From ligges at statistik.tu-dortmund.de  Thu Apr 12 09:16:38 2012
From: ligges at statistik.tu-dortmund.de (Uwe Ligges)
Date: Thu, 12 Apr 2012 09:16:38 +0200
Subject: [Rd] Vignette questions
In-Reply-To: <CANROs4c7b5ThXTXRXt1BARYF1O3SG2s39X3iQEXHAVvjZQttaQ@mail.gmail.com>
References: <4F85EC88.8070604@mayo.edu>
	<CANROs4c7b5ThXTXRXt1BARYF1O3SG2s39X3iQEXHAVvjZQttaQ@mail.gmail.com>
Message-ID: <4F868156.6050307@statistik.tu-dortmund.de>



On 11.04.2012 23:28, Yihui Xie wrote:
> For 1, you should run R CMD check on the tar ball (pkg_x.x.tar.gz)
> from R CMD build instead of the source directory. R CMD build will
> build the PDF vignette into the tar ball.


Just move vignette sources to :/vignettes in your package?

Uwe

> For 2, I have been confused by ./vignettes vs ./inst/doc since
> ./vignettes was introduced. I might be able to figure it out by
> try-and-err but I never tried, and I'm still sticking to ./inst/doc.
> At least you can exclude the Rnw source in .Rbuildignore so that R can
> only stare at your PDF documents and sigh.
>
> For 3, I remember some of us requested that R could also respect
> entries of non-Sweave vignettes (like the ones in ./demo/00Index), but
> this is not possible as far as I know. However, I can tell you a dark
> voodoo seems to be still working: you can write your own index.html
> under ./inst/doc with your own links to vignettes.
>
> Regards,
> Yihui
> --
> Yihui Xie<xieyihui at gmail.com>
> Phone: 515-294-2465 Web: http://yihui.name
> Department of Statistics, Iowa State University
> 2215 Snedecor Hall, Ames, IA
>
>
>
> On Wed, Apr 11, 2012 at 3:41 PM, Terry Therneau<therneau at mayo.edu>  wrote:
>> Context: R2.15-0 on Ubuntu.
>>
>> 1. I get a WARNING from CMD check for "Package vignette(s) without
>> corresponding PDF:
>>   In this case the vignettes directory had both the pdf and Rnw; do I need to
>> move the pdf to inst/doc?
>>
>>    I'm reluctant to add the pdf to the svn source on Rforge, per the usual
>> rule that a code management system should not have both a primary source and
>> a object dervived from it under version control.  However, if this is the
>> suggested norm I could do so.
>>
>> 2. Close reading of the paragraph about vignette sources shows the following
>> -- I think?  If I have a vignette that should not be rebuilt by "check" or
>> "BUILD" I should put the .Rnw source and pdf in /inst/doc, and have the
>> others that should be rebuilt in /vignettes.  This would include any that
>> use "private R packages, screen snapshots, ...", or in my case one that
>> takes just a little short of forever to run.
>>
>> 3. Do these unprocessed package also contribute to the index via
>> \VignetteIndexEntry lines, or will I need to create a custom index?
>>
>> Terry Therneau
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From ligges at statistik.tu-dortmund.de  Thu Apr 12 09:48:05 2012
From: ligges at statistik.tu-dortmund.de (Uwe Ligges)
Date: Thu, 12 Apr 2012 09:48:05 +0200
Subject: [Rd] Vignette questions
In-Reply-To: <A26E51EB75D20B4DAC8AEA2A883BF3997B50587DA5@uranus.ipht-jena.de>
References: <4F85EC88.8070604@mayo.edu>,
	<CANROs4c7b5ThXTXRXt1BARYF1O3SG2s39X3iQEXHAVvjZQttaQ@mail.gmail.com>
	<A26E51EB75D20B4DAC8AEA2A883BF3997B50587DA5@uranus.ipht-jena.de>
Message-ID: <4F8688B5.1010901@statistik.tu-dortmund.de>



On 12.04.2012 00:23, beleites,claudia wrote:
> Very quick&  short answer:
>
> I made the transition to  ./vignettes for hyperSpec (you can look at the source at r-forge) - it was a mess. It is almost working now (compression is missing, I'll have to figure out how to invoke ghostscript in an OS idependent way, qpdf doesn't give me the compression rates).


R CMD build --compact-vignettes=gs

Uwe



>
> I have .pdf is under version control only for those vignettes that I build externally (with the fake .Rnws that produce the proper indexing as vignette).  They are all (regardless whether BUILD and check affect them or not) in ./vignettes.
> I think that strategy can at least partially solve the issue with non-Sweave vignettes as well as long as the result is a pdf.
> You may need to play with ./vignettes/.install_extras if additional, non pdf and non Rnw files are needed.
>
>
> Best,
>
> Claudia
>
>
> ________________________________________
> Von: r-devel-bounces at r-project.org [r-devel-bounces at r-project.org] im Auftrag von Yihui Xie [xie at yihui.name]
> Gesendet: Mittwoch, 11. April 2012 23:28
> An: Terry Therneau
> Cc: r-devel at r-project.org
> Betreff: Re: [Rd] Vignette questions
>
> For 1, you should run R CMD check on the tar ball (pkg_x.x.tar.gz)
> from R CMD build instead of the source directory. R CMD build will
> build the PDF vignette into the tar ball.
>
> For 2, I have been confused by ./vignettes vs ./inst/doc since
> ./vignettes was introduced. I might be able to figure it out by
> try-and-err but I never tried, and I'm still sticking to ./inst/doc.
> At least you can exclude the Rnw source in .Rbuildignore so that R can
> only stare at your PDF documents and sigh.
>
> For 3, I remember some of us requested that R could also respect
> entries of non-Sweave vignettes (like the ones in ./demo/00Index), but
> this is not possible as far as I know. However, I can tell you a dark
> voodoo seems to be still working: you can write your own index.html
> under ./inst/doc with your own links to vignettes.
>
> Regards,
> Yihui
> --
> Yihui Xie<xieyihui at gmail.com>
> Phone: 515-294-2465 Web: http://yihui.name
> Department of Statistics, Iowa State University
> 2215 Snedecor Hall, Ames, IA
>
>
>
> On Wed, Apr 11, 2012 at 3:41 PM, Terry Therneau<therneau at mayo.edu>  wrote:
>> Context: R2.15-0 on Ubuntu.
>>
>> 1. I get a WARNING from CMD check for "Package vignette(s) without
>> corresponding PDF:
>>   In this case the vignettes directory had both the pdf and Rnw; do I need to
>> move the pdf to inst/doc?
>>
>>    I'm reluctant to add the pdf to the svn source on Rforge, per the usual
>> rule that a code management system should not have both a primary source and
>> a object dervived from it under version control.  However, if this is the
>> suggested norm I could do so.
>>
>> 2. Close reading of the paragraph about vignette sources shows the following
>> -- I think?  If I have a vignette that should not be rebuilt by "check" or
>> "BUILD" I should put the .Rnw source and pdf in /inst/doc, and have the
>> others that should be rebuilt in /vignettes.  This would include any that
>> use "private R packages, screen snapshots, ...", or in my case one that
>> takes just a little short of forever to run.
>>
>> 3. Do these unprocessed package also contribute to the index via
>> \VignetteIndexEntry lines, or will I need to create a custom index?
>>
>> Terry Therneau
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From murdoch.duncan at gmail.com  Thu Apr 12 14:21:48 2012
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Thu, 12 Apr 2012 08:21:48 -0400
Subject: [Rd] [patch] giving library() a 'version' argument
In-Reply-To: <21A5E1E970CD46459ECBE86D6CC4B28C667AA79C@spexch01.WindLogics.local>
References: <21A5E1E970CD46459ECBE86D6CC4B28C667AA79C@spexch01.WindLogics.local>
Message-ID: <4F86C8DC.7020608@gmail.com>

On 12-04-11 11:28 AM, Ken Williams wrote:
> I've made a small enhancement to R that would help developers better control what versions of code we're using where.  Basically, to load a package in R, one currently does:
>
>                  library(whateverPackage)
>
> and with the enhancement, you can ensure that you're getting at least version X of the package:
>
>                  library(whateverPackage, version=3.14)
>
> Reasons one might want this include:
>
>    * you know that in version X some bug was fixed
>    * you know that in version X some feature was added
>    * that's the first version you've actually tested it with&  you don't want to vouch for earlier versions without testing
>    * you develop on one machine&  deploy on another machine you don't control, and you want runtime checks that the sysadmin installed what they were supposed to install

I don't really see the need for this.  Packages already have a scheme 
for requiring a particular version of a package, so this would only be 
useful in scripts run outside of packages.  But what if your script 
requires a particular (perhaps obsolete) version of a package?  This 
change only puts a lower bound on the version number, and version 
requirements can be more elaborate than that.

I think my advice would be:

1.  Put your code in a package, and use the version specifications there.

2.  If you must write it in a script, then put a version test at the 
top, using packageVersion().

Duncan Murdoch


>
> In general, I have an interest in helping R get better at various things that would help it play in a "production environment", for various values of that term. =)
>
> The attached patch is made against revision 58980 of https://svn.r-project.org/R/trunk .  I think this is the first patch I've submitted to the R core, so please let me know if anything's amiss, or of course if there are reservations about the approach.
>
> Thanks.
>
> --
> Ken Williams, Senior Research Scientist
> WindLogics
> http://windlogics.com
>
>
>
> CONFIDENTIALITY NOTICE: This e-mail message is for the sole use of the intended recipient(s) and may contain confidential and privileged information. Any unauthorized review, use, disclosure or distribution of any kind is strictly prohibited. If you are not the intended recipient, please contact the sender via reply e-mail and destroy all copies of the original message. Thank you.
>
>
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From ripley at stats.ox.ac.uk  Thu Apr 12 14:53:59 2012
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 12 Apr 2012 13:53:59 +0100
Subject: [Rd] [patch] giving library() a 'version' argument
In-Reply-To: <4F86C8DC.7020608@gmail.com>
References: <21A5E1E970CD46459ECBE86D6CC4B28C667AA79C@spexch01.WindLogics.local>
	<4F86C8DC.7020608@gmail.com>
Message-ID: <4F86D067.1080604@stats.ox.ac.uk>

A very important point is that library() *had* a 'version' argument for 
several years, and this is not what it did.  So Mr Williams needs to do 
his homework ....

 From such a version of R:


  version: A character string denoting a version number of the package
           to be loaded, for use with _versioned installs_: see the
           section later in this document.
...

On 12/04/2012 13:21, Duncan Murdoch wrote:
> On 12-04-11 11:28 AM, Ken Williams wrote:
>> I've made a small enhancement to R that would help developers better
>> control what versions of code we're using where. Basically, to load a
>> package in R, one currently does:
>>
>> library(whateverPackage)
>>
>> and with the enhancement, you can ensure that you're getting at least
>> version X of the package:
>>
>> library(whateverPackage, version=3.14)
>>
>> Reasons one might want this include:
>>
>> * you know that in version X some bug was fixed
>> * you know that in version X some feature was added
>> * that's the first version you've actually tested it with& you don't
>> want to vouch for earlier versions without testing
>> * you develop on one machine& deploy on another machine you don't
>> control, and you want runtime checks that the sysadmin installed what
>> they were supposed to install
>
> I don't really see the need for this. Packages already have a scheme for
> requiring a particular version of a package, so this would only be
> useful in scripts run outside of packages. But what if your script
> requires a particular (perhaps obsolete) version of a package? This
> change only puts a lower bound on the version number, and version
> requirements can be more elaborate than that.
>
> I think my advice would be:
>
> 1. Put your code in a package, and use the version specifications there.
>
> 2. If you must write it in a script, then put a version test at the top,
> using packageVersion().
>
> Duncan Murdoch
>
>
>>
>> In general, I have an interest in helping R get better at various
>> things that would help it play in a "production environment", for
>> various values of that term. =)
>>
>> The attached patch is made against revision 58980 of
>> https://svn.r-project.org/R/trunk . I think this is the first patch
>> I've submitted to the R core, so please let me know if anything's
>> amiss, or of course if there are reservations about the approach.
>>
>> Thanks.
>>
>> --
>> Ken Williams, Senior Research Scientist
>> WindLogics
>> http://windlogics.com
>>
>>
>>
>> CONFIDENTIALITY NOTICE: This e-mail message is for the sole use of the
>> intended recipient(s) and may contain confidential and privileged
>> information. Any unauthorized review, use, disclosure or distribution
>> of any kind is strictly prohibited. If you are not the intended
>> recipient, please contact the sender via reply e-mail and destroy all
>> copies of the original message. Thank you.
>>
>>
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From therneau at mayo.edu  Thu Apr 12 15:17:33 2012
From: therneau at mayo.edu (Terry Therneau)
Date: Thu, 12 Apr 2012 08:17:33 -0500
Subject: [Rd] Vignette questions
In-Reply-To: <4F86811D.5040201@statistik.tu-dortmund.de>
References: <4F85EC88.8070604@mayo.edu> <4F8610DA.80908@gmail.com>
	<4F86811D.5040201@statistik.tu-dortmund.de>
Message-ID: <4F86D5ED.4040307@mayo.edu>

On 04/12/2012 02:15 AM, Uwe Ligges wrote:
>
> On 12.04.2012 01:16, Paul Gilbert wrote:
>>
>>
>> On 12-04-11 04:41 PM, Terry Therneau wrote:
>>> Context: R2.15-0 on Ubuntu.
>>>
>>> 1. I get a WARNING from CMD check for "Package vignette(s) without
>>> corresponding PDF:
>>> In this case the vignettes directory had both the pdf and Rnw; do I 
>>> need
>>> to move the pdf to inst/doc?
>>
>> Yes, you need to put the pdf in the inst/doc directory if it cannot be
>> built by R-forge and CRAN check machines, but leave the Rnw in the
>> vignettes directory.
>
> No, this is all done automatically by R CMD build, hence you do not 
> need to worry.

Suddenly it becomes clear:  the warning will disappear on its own when I 
apply CMD check to the tarball.

I was running it on the directory, as is my habit when I've just added a 
new feature and want to make sure I didn't break anything old, and had 
wandered down the "CRAN doesn't like warnings so I better fix this 
somehow" path.

Terry Therneau


From andre.zege at gmail.com  Thu Apr 12 16:14:01 2012
From: andre.zege at gmail.com (andre zege)
Date: Thu, 12 Apr 2012 10:14:01 -0400
Subject: [Rd] unexpectedly high memory use in R 2.14.0
In-Reply-To: <CAFDcVCSi1wgAMMHdOdHBK-SUNXqaWv8_=p3AX30nyFyNCz-sFQ@mail.gmail.com>
References: <CACU3EkMATY=_GZJuOo5gGGKXFS4iOvCMdyeAtNNRmKqbULN7kw@mail.gmail.com>
	<CAFDcVCSi1wgAMMHdOdHBK-SUNXqaWv8_=p3AX30nyFyNCz-sFQ@mail.gmail.com>
Message-ID: <CACU3EkNiAwzmuDzbCv__VLMpTPKwJKBFJU6DitODFj8T=VEsWw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20120412/44973f1f/attachment.pl>

From Ken.Williams at windlogics.com  Thu Apr 12 16:24:20 2012
From: Ken.Williams at windlogics.com (Ken Williams)
Date: Thu, 12 Apr 2012 09:24:20 -0500
Subject: [Rd] [patch] giving library() a 'version' argument
In-Reply-To: <4F86D067.1080604@stats.ox.ac.uk>
References: <21A5E1E970CD46459ECBE86D6CC4B28C667AA79C@spexch01.WindLogics.local>
	<4F86C8DC.7020608@gmail.com> <4F86D067.1080604@stats.ox.ac.uk>
Message-ID: <21A5E1E970CD46459ECBE86D6CC4B28C667AA97B@spexch01.WindLogics.local>


> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> Sent: Thursday, April 12, 2012 7:54 AM
> To: Duncan Murdoch
> Cc: Ken Williams; r-devel at r-project.org
> Subject: Re: [Rd] [patch] giving library() a 'version' argument
>
> A very important point is that library() *had* a 'version' argument for several
> years, and this is not what it did.

That is unfortunate.  So such a mechanism would need to use a different argument name.

For completeness in this thread, I dug up the fact that it seems to have been removed in the 2.9.0 release:

    o   Support for versioned installs (R CMD INSTALL --with-package-versions
        and install.packages(installWithVers = TRUE)) has been removed.
        Packages installed with versioned names will be ignored.

I'll address Duncan's concerns in a separate message.

 -Ken


CONFIDENTIALITY NOTICE: This e-mail message is for the s...{{dropped:7}}


From pgilbert902 at gmail.com  Thu Apr 12 16:42:51 2012
From: pgilbert902 at gmail.com (Paul Gilbert)
Date: Thu, 12 Apr 2012 10:42:51 -0400
Subject: [Rd] Vignette questions
In-Reply-To: <4F86811D.5040201@statistik.tu-dortmund.de>
References: <4F85EC88.8070604@mayo.edu> <4F8610DA.80908@gmail.com>
	<4F86811D.5040201@statistik.tu-dortmund.de>
Message-ID: <4F86E9EB.3060609@gmail.com>



On 12-04-12 03:15 AM, Uwe Ligges wrote:
>
>
> On 12.04.2012 01:16, Paul Gilbert wrote:
>>
>>
>> On 12-04-11 04:41 PM, Terry Therneau wrote:
>>> Context: R2.15-0 on Ubuntu.
>>>
>>> 1. I get a WARNING from CMD check for "Package vignette(s) without
>>> corresponding PDF:
>>> In this case the vignettes directory had both the pdf and Rnw; do I need
>>> to move the pdf to inst/doc?
>>
>> Yes, you need to put the pdf in the inst/doc directory if it cannot be
>> built by R-forge and CRAN check machines, but leave the Rnw in the
>> vignettes directory.
>
> No, this is all done automatically by R CMD build, hence you do not need
> to worry.

Now I am not sure if I am confused or if you missed the "if it cannot be 
built by R-forge and CRAN" part of my sentence. I understand that this 
is done automatically by R CMD build for vignettes that can be built on 
all, or most, R platforms. In the situation where R CMD build on R-forge 
will fail, or not result in a complete vignette pdf, I think it is 
necessary to put a good  pdf in inst/doc in order to get a build on 
R-forge that can be submitted to CRAN. That is, in situations like:

   -the vignette requires databases or drivers not generally available
   -the vignette (legitimately) takes forever to run
   -the vignette requires a cluster

I am now wondering what the recommended practice is. What I have been 
doing, which I thought was the recommended practice, is to put the 
vignette Rnw (Stex) file in vignettes/ and put a pdf, constructed on a 
machine that has appropriate resources, into inst/doc.  Is that the 
recommended way to proceed?

Related, some have commented that they put a pdf in inst/doc and then 
leave out the vignette Rnw file to avoid error messages. Is the 
discouraged or encouraged?

Paul
>
>
>>>
>>> I'm reluctant to add the pdf to the svn source on Rforge, per the usual
>>> rule that a code management system should not have both a primary source
>>> and a object dervived from it under version control. However, if this is
>>> the suggested norm I could do so.
>>
>> Yes, I think this is the norm if the vignette cannot be built on CRAN
>> and R-forge,
>
> Well, yours are that specific that they rely on third party software.
> Vignettes "only" depending on R and installed packages that are declared
> as dependencies can be build by CRAN.
>
>
>> even though it does seem a bit strange. However, you do not
>> necessarily need to update the vignette pdf in inst/doc every time you
>> make a change to the package even though, in my opinion, the correct
>> logic is to test remaking the vignette when you make a change to the
>> package. You should do this testing, of course, you just do not need to
>> put the new pdf in inst/doc and commit it to svn each time. (But you
>> should probably do that before you build the final package to put on
>> CRAN.)
>
> R CMD build will rebuild vignettes unless you ask R not to do so.
>
> Uwe
>
>
>>>
>>> 2. Close reading of the paragraph about vignette sources shows the
>>> following -- I think? If I have a vignette that should not be rebuilt by
>>> "check" or "BUILD" I should put the .Rnw source and pdf in /inst/doc,
>>> and have the others that should be rebuilt in /vignettes. This would
>>> include any that use "private R packages, screen snapshots, ...", or in
>>> my case one that takes just a little short of forever to run.
>>
>> I don't think it is intended to say that, and I didn't read it that way.
>> I think putting the Rnw in inst/doc is supported (temporarily?) for
>> historical reasons only. If it is not in vignettes/ and is found in
>> inst/doc/, it is treated the same way as if it were in vignettes/.
>>
>> You can include screen snapshots, etc, in either case. For your
>> situation, what you probably do need to do is specify "BuildVignettes:
>> false" in the DESCRIPTION file. This prevents the pdf for inst/doc from
>> being generated by the the Rnw. However, it does not prevent R CMD check
>> from checking that the R code extracted from the Rnw actually runs, and
>> generating an error if it does not. To prevent testing of the R code,
>> you have to appeal directly to the CRAN and R-forge maintainers, and
>> they will put the package on a special list. You do need to give them a
>> good reason why the code should not be tested. I think they are
>> sympathetic with "takes forever to run" and not very sympathetic with
>> "does not work anymore". Generally, I think they want to consider doing
>> this only in exceptional cases, so they do not get in a situation of
>> having lots of broken vignettes. (One should stick with journal articles
>> for recording broken code.)
>>
>>> 3. Do these unprocessed package also contribute to the index via
>>> \VignetteIndexEntry lines, or will I need to create a custom index?
>>
>> I'm not sure of the answer to this, but would be curious to know. You
>> may need to rely on voodoo.
>>
>> Paul
>>>
>>> Terry Therneau
>>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel


From ligges at statistik.tu-dortmund.de  Thu Apr 12 17:02:09 2012
From: ligges at statistik.tu-dortmund.de (Uwe Ligges)
Date: Thu, 12 Apr 2012 17:02:09 +0200
Subject: [Rd] Vignette questions
In-Reply-To: <4F86E9EB.3060609@gmail.com>
References: <4F85EC88.8070604@mayo.edu> <4F8610DA.80908@gmail.com>
	<4F86811D.5040201@statistik.tu-dortmund.de>
	<4F86E9EB.3060609@gmail.com>
Message-ID: <4F86EE71.1020302@statistik.tu-dortmund.de>



On 12.04.2012 16:42, Paul Gilbert wrote:
>
>
> On 12-04-12 03:15 AM, Uwe Ligges wrote:
>>
>>
>> On 12.04.2012 01:16, Paul Gilbert wrote:
>>>
>>>
>>> On 12-04-11 04:41 PM, Terry Therneau wrote:
>>>> Context: R2.15-0 on Ubuntu.
>>>>
>>>> 1. I get a WARNING from CMD check for "Package vignette(s) without
>>>> corresponding PDF:
>>>> In this case the vignettes directory had both the pdf and Rnw; do I
>>>> need
>>>> to move the pdf to inst/doc?
>>>
>>> Yes, you need to put the pdf in the inst/doc directory if it cannot be
>>> built by R-forge and CRAN check machines, but leave the Rnw in the
>>> vignettes directory.
>>
>> No, this is all done automatically by R CMD build, hence you do not need
>> to worry.
>
> Now I am not sure if I am confused or if you missed the "if it cannot be
> built by R-forge and CRAN" part of my sentence. I understand that this
> is done automatically by R CMD build for vignettes that can be built on
> all, or most, R platforms. In the situation where R CMD build on R-forge
> will fail, or not result in a complete vignette pdf, I think it is
> necessary to put a good pdf in inst/doc in order to get a build on
> R-forge that can be submitted to CRAN. That is, in situations like:
>
> -the vignette requires databases or drivers not generally available
> -the vignette (legitimately) takes forever to run
> -the vignette requires a cluster

I cannot comment on R-forge - I just submit packages manually.


> I am now wondering what the recommended practice is. What I have been
> doing, which I thought was the recommended practice, is to put the
> vignette Rnw (Stex) file in vignettes/ and put a pdf, constructed on a
> machine that has appropriate resources, into inst/doc. Is that the
> recommended way to proceed?

Sounds OK for such a specific case (although I have not tested nor 
thought about it intensely before).

> Related, some have commented that they put a pdf in inst/doc and then
> leave out the vignette Rnw file to avoid error messages. Is the
> discouraged or encouraged?

Finding bugs in packages happens frequently by executing the code in the 
vignettes. So if the vignette builds and it does not take ages to build 
the vignette, I'd encourage people to submit vignette sources (also note 
the license that may make it unavoidable to ship the sources).

Uwe


>
> Paul
>>
>>
>>>>
>>>> I'm reluctant to add the pdf to the svn source on Rforge, per the usual
>>>> rule that a code management system should not have both a primary
>>>> source
>>>> and a object dervived from it under version control. However, if
>>>> this is
>>>> the suggested norm I could do so.
>>>
>>> Yes, I think this is the norm if the vignette cannot be built on CRAN
>>> and R-forge,
>>
>> Well, yours are that specific that they rely on third party software.
>> Vignettes "only" depending on R and installed packages that are declared
>> as dependencies can be build by CRAN.
>>
>>
>>> even though it does seem a bit strange. However, you do not
>>> necessarily need to update the vignette pdf in inst/doc every time you
>>> make a change to the package even though, in my opinion, the correct
>>> logic is to test remaking the vignette when you make a change to the
>>> package. You should do this testing, of course, you just do not need to
>>> put the new pdf in inst/doc and commit it to svn each time. (But you
>>> should probably do that before you build the final package to put on
>>> CRAN.)
>>
>> R CMD build will rebuild vignettes unless you ask R not to do so.
>>
>> Uwe
>>
>>
>>>>
>>>> 2. Close reading of the paragraph about vignette sources shows the
>>>> following -- I think? If I have a vignette that should not be
>>>> rebuilt by
>>>> "check" or "BUILD" I should put the .Rnw source and pdf in /inst/doc,
>>>> and have the others that should be rebuilt in /vignettes. This would
>>>> include any that use "private R packages, screen snapshots, ...", or in
>>>> my case one that takes just a little short of forever to run.
>>>
>>> I don't think it is intended to say that, and I didn't read it that way.
>>> I think putting the Rnw in inst/doc is supported (temporarily?) for
>>> historical reasons only. If it is not in vignettes/ and is found in
>>> inst/doc/, it is treated the same way as if it were in vignettes/.
>>>
>>> You can include screen snapshots, etc, in either case. For your
>>> situation, what you probably do need to do is specify "BuildVignettes:
>>> false" in the DESCRIPTION file. This prevents the pdf for inst/doc from
>>> being generated by the the Rnw. However, it does not prevent R CMD check
>>> from checking that the R code extracted from the Rnw actually runs, and
>>> generating an error if it does not. To prevent testing of the R code,
>>> you have to appeal directly to the CRAN and R-forge maintainers, and
>>> they will put the package on a special list. You do need to give them a
>>> good reason why the code should not be tested. I think they are
>>> sympathetic with "takes forever to run" and not very sympathetic with
>>> "does not work anymore". Generally, I think they want to consider doing
>>> this only in exceptional cases, so they do not get in a situation of
>>> having lots of broken vignettes. (One should stick with journal articles
>>> for recording broken code.)
>>>
>>>> 3. Do these unprocessed package also contribute to the index via
>>>> \VignetteIndexEntry lines, or will I need to create a custom index?
>>>
>>> I'm not sure of the answer to this, but would be curious to know. You
>>> may need to rely on voodoo.
>>>
>>> Paul
>>>>
>>>> Terry Therneau
>>>>
>>>> ______________________________________________
>>>> R-devel at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel


From Ken.Williams at windlogics.com  Thu Apr 12 17:11:55 2012
From: Ken.Williams at windlogics.com (Ken Williams)
Date: Thu, 12 Apr 2012 10:11:55 -0500
Subject: [Rd] [patch] giving library() a 'version' argument
In-Reply-To: <4F86C8DC.7020608@gmail.com>
References: <21A5E1E970CD46459ECBE86D6CC4B28C667AA79C@spexch01.WindLogics.local>
	<4F86C8DC.7020608@gmail.com>
Message-ID: <21A5E1E970CD46459ECBE86D6CC4B28C667AA9BA@spexch01.WindLogics.local>



> -----Original Message-----
> From: Duncan Murdoch [mailto:murdoch.duncan at gmail.com]
> Sent: Thursday, April 12, 2012 7:22 AM
> To: Ken Williams
> Cc: r-devel at r-project.org
> Subject: Re: [Rd] [patch] giving library() a 'version' argument
>
> On 12-04-11 11:28 AM, Ken Williams wrote:
> >
> > Reasons one might want this include:
> >
> >    * you know that in version X some bug was fixed
> >    * you know that in version X some feature was added
> >    * that's the first version you've actually tested it with &  you don't want to
> >  vouch for earlier versions without testing
> >    * you develop on one machine &  deploy on another machine you don't
> > control, and you want runtime checks that the sysadmin installed what
> > they were supposed to install
>
> I don't really see the need for this.  Packages already have a scheme for
> requiring a particular version of a package, so this would only be useful in
> scripts run outside of packages.

The main distinction here is that the existing package mechanism enforces version requirements at *install* time, but this mechanism enforces it at *run* time.  So this indeed applies well to scripts run outside packages, but it's also useful inside packages when they're loading their dependencies at runtime.  I was trying to illustrate that with the 4 bullet points above (especially the last one) but I should have said so explicitly.

It can happen very easily that constraints that were satisfied at install time get out of whack by subsequent package installations, but the violations go undetected.  The result can be breakage, whether dramatic or subtle.

The main hats targeted here are really people (like me, of course) who are trying to "productionize" results, not so much people who are doing offline analysis.  In a production system

> But what if your script requires a particular
> (perhaps obsolete) version of a package?  This change only puts a lower
> bound on the version number, and version requirements can be more
> elaborate than that.

Certainly true; this was meant as a first iteration, and support for the more elaborate requirements specifications could certainly be added.

The more elaborate specs actually illustrate the need for a runtime mechanism nicely - if code X (which may be a package, or a script, it doesn't matter) requires exactly version 3.14 of package B, and someone in the production team upgrades version 3.14 to version 3.78 because "it's faster" or "it's less buggy" or "we just like to have the latest version of everything all the time", then someone needs to be alerted to the problem.  One alternative solution would be to use a full-fledged package management system like RPM or Deb to track all the dependencies, but yikes, that doesn't sound fun.

> I think my advice would be:
>
> 1.  Put your code in a package, and use the version specifications there.
>
> 2.  If you must write it in a script, then put a version test at the top, using packageVersion().

Certainly those are alternatives, but to us they are somewhat unsatisfactory.  The first option doesn't help with the crux of the problem, which is runtime enforcement.  The second is essentially the same solution I've proposed, but doesn't help anyone outside our organization who has the same problem.

 -Ken

CONFIDENTIALITY NOTICE: This e-mail message is for the s...{{dropped:7}}


From murdoch.duncan at gmail.com  Thu Apr 12 19:27:09 2012
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Thu, 12 Apr 2012 13:27:09 -0400
Subject: [Rd] [patch] giving library() a 'version' argument
In-Reply-To: <21A5E1E970CD46459ECBE86D6CC4B28C667AA9BA@spexch01.WindLogics.local>
References: <21A5E1E970CD46459ECBE86D6CC4B28C667AA79C@spexch01.WindLogics.local>
	<4F86C8DC.7020608@gmail.com>
	<21A5E1E970CD46459ECBE86D6CC4B28C667AA9BA@spexch01.WindLogics.local>
Message-ID: <4F87106D.9050402@gmail.com>

On 12/04/2012 11:11 AM, Ken Williams wrote:
>
> >  -----Original Message-----
> >  From: Duncan Murdoch [mailto:murdoch.duncan at gmail.com]
> >  Sent: Thursday, April 12, 2012 7:22 AM
> >  To: Ken Williams
> >  Cc: r-devel at r-project.org
> >  Subject: Re: [Rd] [patch] giving library() a 'version' argument
> >
> >  On 12-04-11 11:28 AM, Ken Williams wrote:
> >  >
> >  >  Reasons one might want this include:
> >  >
> >  >     * you know that in version X some bug was fixed
> >  >     * you know that in version X some feature was added
> >  >     * that's the first version you've actually tested it with&   you don't want to
> >  >   vouch for earlier versions without testing
> >  >     * you develop on one machine&   deploy on another machine you don't
> >  >  control, and you want runtime checks that the sysadmin installed what
> >  >  they were supposed to install
> >
> >  I don't really see the need for this.  Packages already have a scheme for
> >  requiring a particular version of a package, so this would only be useful in
> >  scripts run outside of packages.
>
> The main distinction here is that the existing package mechanism enforces version requirements at *install* time, but this mechanism enforces it at *run* time.

I haven't tested it, but according to the documentation in Writing R 
Extensions, the dependencies are enforced at the time library() is called.


>   So this indeed applies well to scripts run outside packages, but it's also useful inside packages when they're loading their dependencies at runtime.  I was trying to illustrate that with the 4 bullet points above (especially the last one) but I should have said so explicitly.

If the docs are wrong (or I misread them), you could equally put a 
run-time version test into the .onLoad function in a package.
>
> It can happen very easily that constraints that were satisfied at install time get out of whack by subsequent package installations, but the violations go undetected.  The result can be breakage, whether dramatic or subtle.
>
> The main hats targeted here are really people (like me, of course) who are trying to "productionize" results, not so much people who are doing offline analysis.  In a production system
>
> >  But what if your script requires a particular
> >  (perhaps obsolete) version of a package?  This change only puts a lower
> >  bound on the version number, and version requirements can be more
> >  elaborate than that.
>
> Certainly true; this was meant as a first iteration, and support for the more elaborate requirements specifications could certainly be added.
>
> The more elaborate specs actually illustrate the need for a runtime mechanism nicely - if code X (which may be a package, or a script, it doesn't matter) requires exactly version 3.14 of package B, and someone in the production team upgrades version 3.14 to version 3.78 because "it's faster" or "it's less buggy" or "we just like to have the latest version of everything all the time", then someone needs to be alerted to the problem.  One alternative solution would be to use a full-fledged package management system like RPM or Deb to track all the dependencies, but yikes, that doesn't sound fun.

But a single line at the top of the script would fix this:

stopifnot(packageVersion("foo") == "3.14")

Making the library() function more elaborate doesn't seem to add anything.


>
> >  I think my advice would be:
> >
> >  1.  Put your code in a package, and use the version specifications there.
> >
> >  2.  If you must write it in a script, then put a version test at the top, using packageVersion().
>
> Certainly those are alternatives, but to us they are somewhat unsatisfactory.  The first option doesn't help with the crux of the problem, which is runtime enforcement.  The second is essentially the same solution I've proposed, but doesn't help anyone outside our organization who has the same problem.

Another problem with putting this into library() is that packages aren't 
always loaded by library():  there is require(), and there are implicit 
loads triggered by dependencies of other packages.

Duncan Murdoch


From Ken.Williams at windlogics.com  Thu Apr 12 19:46:33 2012
From: Ken.Williams at windlogics.com (Ken Williams)
Date: Thu, 12 Apr 2012 12:46:33 -0500
Subject: [Rd] [patch] giving library() a 'version' argument
In-Reply-To: <4F87106D.9050402@gmail.com>
References: <21A5E1E970CD46459ECBE86D6CC4B28C667AA79C@spexch01.WindLogics.local>
	<4F86C8DC.7020608@gmail.com>
	<21A5E1E970CD46459ECBE86D6CC4B28C667AA9BA@spexch01.WindLogics.local>
	<4F87106D.9050402@gmail.com>
Message-ID: <21A5E1E970CD46459ECBE86D6CC4B28C667AAA5D@spexch01.WindLogics.local>



> -----Original Message-----
> From: Duncan Murdoch [mailto:murdoch.duncan at gmail.com]
> Sent: Thursday, April 12, 2012 12:27 PM
> To: Ken Williams
> Cc: r-devel at r-project.org
> Subject: Re: [Rd] [patch] giving library() a 'version' argument
>
> I haven't tested it, but according to the documentation in Writing R
> Extensions, the dependencies are enforced at the time library() is called.

Oh, I hadn't suspected that.  I can look into testing that, if it's true then of course that changes this all.  I probably won't be able to do that for a few days because I'll be traveling though.

I've never noticed a package failing to load at runtime because its prereq-version dependency wasn't met though.

> [...]
> But a single line at the top of the script would fix this:
>
> stopifnot(packageVersion("foo") == "3.14")

For the most common use case, that would look more like:

    stopifnot(compareVersion(packageVersion("foo"), "3.14") < 0)

which gets less declarative, and I'd argue less clear about exactly what it's trying to enforce.

And I can see myself (& presumably others) getting that comparison operator backwards a lot, having to look it up each time or copy-paste it from other code.

And then that still doesn't add nice error messages, that would be yet more code.

*And*, it doesn't actually behave correctly if the package is already loaded by other code, because it might have been loaded from a different location than the one that would be found in the packageVersion() call.  (Or am I maybe wrong about what packageVersion() does in that case?  I don't think the docs specify that behavior.)


For prior art on this whole concept, a useful precedent is the 'use()' function in Perl, which accepts a version argument, even though there is also robust version checking at installation/testing time.

>
> Another problem with putting this into library() is that packages aren't
> always loaded by library():  there is require(), and there are implicit
> loads triggered by dependencies of other packages.

That's not really a problem.  If someone wants to enforce a runtime dependency, they stick the enforcement line into their code, and it will correctly stop if the criterion is not met.

 -Ken

CONFIDENTIALITY NOTICE: This e-mail message is for the s...{{dropped:7}}


From proebuck at mdanderson.org  Thu Apr 12 20:03:06 2012
From: proebuck at mdanderson.org (Roebuck,Paul L)
Date: Thu, 12 Apr 2012 13:03:06 -0500
Subject: [Rd] [patch] giving library() a 'version' argument
In-Reply-To: <21A5E1E970CD46459ECBE86D6CC4B28C667AA9BA@spexch01.WindLogics.local>
Message-ID: <CBAC830A.2C48F%proebuck@mdanderson.org>

On 4/12/12 10:11 AM, Ken Williams wrote:

>> On 4/12/12 7:22 AM, Duncan Murdoch wrote:
> [SNIP]
> ...
> The main hats targeted here are really people (like me, of course) who are
> trying to "productionize" results, not so much people who are doing offline
> analysis.  In a production system
> 
>> But what if your script requires a particular
>> (perhaps obsolete) version of a package?  This change only puts a lower
>> bound on the version number, and version requirements can be more
>> elaborate than that.
> 
> Certainly true; this was meant as a first iteration, and support for the more
> elaborate requirements specifications could certainly be added.
> 
> The more elaborate specs actually illustrate the need for a runtime mechanism
> nicely - if code X (which may be a package, or a script, it doesn't matter)
> requires exactly version 3.14 of package B, and someone in the production team
> upgrades version 3.14 to version 3.78 because "it's faster" or "it's less
> buggy" or "we just like to have the latest version of everything all the
> time", then someone needs to be alerted to the problem.  One alternative
> solution would be to use a full-fledged package management system like RPM or
> Deb to track all the dependencies, but yikes, that doesn't sound fun.

I appreciate your contribution of both time and energy.

But I think the existing library() method is sufficient without
this modification. It's essentially syntactic sugar for:

library(MASS); stopifnot(packageVersion("MASS") >= "7.3"))

If your package requirements are that exacting, it would be far
simpler to just download all the specific versions to a single
directory and put that directory first in .libPaths().

Prayer never hurt either...

Our style here is to add sessionInfo() to the end of all scripts
and Sweave documents. As such we could reproduce exactly if
required. But I believe it would be impossible to track the
dependencies meaningfully across time.


From murdoch.duncan at gmail.com  Thu Apr 12 20:14:02 2012
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Thu, 12 Apr 2012 14:14:02 -0400
Subject: [Rd] [patch] giving library() a 'version' argument
In-Reply-To: <21A5E1E970CD46459ECBE86D6CC4B28C667AAA5D@spexch01.WindLogics.local>
References: <21A5E1E970CD46459ECBE86D6CC4B28C667AA79C@spexch01.WindLogics.local>
	<4F86C8DC.7020608@gmail.com>
	<21A5E1E970CD46459ECBE86D6CC4B28C667AA9BA@spexch01.WindLogics.local>
	<4F87106D.9050402@gmail.com>
	<21A5E1E970CD46459ECBE86D6CC4B28C667AAA5D@spexch01.WindLogics.local>
Message-ID: <4F871B6A.8010000@gmail.com>

On 12/04/2012 1:46 PM, Ken Williams wrote:
>
> >  -----Original Message-----
> >  From: Duncan Murdoch [mailto:murdoch.duncan at gmail.com]
> >  Sent: Thursday, April 12, 2012 12:27 PM
> >  To: Ken Williams
> >  Cc: r-devel at r-project.org
> >  Subject: Re: [Rd] [patch] giving library() a 'version' argument
> >
> >  I haven't tested it, but according to the documentation in Writing R
> >  Extensions, the dependencies are enforced at the time library() is called.
>
> Oh, I hadn't suspected that.  I can look into testing that, if it's true then of course that changes this all.  I probably won't be able to do that for a few days because I'll be traveling though.
>
> I've never noticed a package failing to load at runtime because its prereq-version dependency wasn't met though.
>
> >  [...]
> >  But a single line at the top of the script would fix this:
> >
> >  stopifnot(packageVersion("foo") == "3.14")
>
> For the most common use case, that would look more like:
>
>      stopifnot(compareVersion(packageVersion("foo"), "3.14")<  0)

The compareVersion call doesn't need to be explicit, i.e. you'll get the 
same result from

stopifnot(packageVersion("foo")<  "3.14")


which looks pretty clear to me.  It works in some quick tests, 
recognizing that rgl version 0.92.879 is bigger than 0.92.100 but less 
than 0.92.1000.

Duncan Murdoch

>
> which gets less declarative, and I'd argue less clear about exactly what it's trying to enforce.
>
> And I can see myself (&  presumably others) getting that comparison operator backwards a lot, having to look it up each time or copy-paste it from other code.
>
> And then that still doesn't add nice error messages, that would be yet more code.
>
> *And*, it doesn't actually behave correctly if the package is already loaded by other code, because it might have been loaded from a different location than the one that would be found in the packageVersion() call.  (Or am I maybe wrong about what packageVersion() does in that case?  I don't think the docs specify that behavior.)
>
>
> For prior art on this whole concept, a useful precedent is the 'use()' function in Perl, which accepts a version argument, even though there is also robust version checking at installation/testing time.
>
> >
> >  Another problem with putting this into library() is that packages aren't
> >  always loaded by library():  there is require(), and there are implicit
> >  loads triggered by dependencies of other packages.
>
> That's not really a problem.  If someone wants to enforce a runtime dependency, they stick the enforcement line into their code, and it will correctly stop if the criterion is not met.
>
>   -Ken
>
> CONFIDENTIALITY NOTICE: This e-mail message is for the sole use of the intended recipient(s) and may contain confidential and privileged information. Any unauthorized review, use, disclosure or distribution of any kind is strictly prohibited. If you are not the intended recipient, please contact the sender via reply e-mail and destroy all copies of the original message. Thank you.


From spluque at gmail.com  Thu Apr 12 20:20:02 2012
From: spluque at gmail.com (Sebastian P. Luque)
Date: Thu, 12 Apr 2012 13:20:02 -0500
Subject: [Rd] LazyData and CSV files in data/
Message-ID: <87wr5kzub1.fsf@kolob.subpolar.dyndns.org>

Hi,

I'm trying to use 'LazyData: yes' in a package.  A vignette and the
Examples section of help pages contain this:

---<--------------------cut here---------------start------------------->---
zz <- system.file(file.path("data", "fileA.csv"),
                  package="myPackage")
dfA <- read.csv(zz, sep=";", na.strings="")
---<--------------------cut here---------------end--------------------->---

However, building the package fails when working on the vignette:

Warning in file(file, "rt") :
  file("") only supports open = "w+" and open = "w+b": using the former

Error: processing vignette ?myPackage.Rnw? failed with diagnostics:
 chunk 5 (label = readin) 
Error in read.table(file = file, header = header, sep = sep, quote = quote,  : 
  no lines available in input

If these lines are removed from the vignette, then the package builds
and checks fine, and after installing the package these lines in the
help pages run fine.  Am I missing something?

Cheers,

-- 
Seb


From Ken.Williams at windlogics.com  Thu Apr 12 20:23:26 2012
From: Ken.Williams at windlogics.com (Ken Williams)
Date: Thu, 12 Apr 2012 13:23:26 -0500
Subject: [Rd] [patch] giving library() a 'version' argument
In-Reply-To: <CBAC830A.2C48F%proebuck@mdanderson.org>
References: <21A5E1E970CD46459ECBE86D6CC4B28C667AA9BA@spexch01.WindLogics.local>
	<CBAC830A.2C48F%proebuck@mdanderson.org>
Message-ID: <21A5E1E970CD46459ECBE86D6CC4B28C667AAA70@spexch01.WindLogics.local>



> -----Original Message-----
> From: Roebuck,Paul L [mailto:proebuck at mdanderson.org]
> Sent: Thursday, April 12, 2012 1:03 PM
> To: R-devel
> Cc: Ken Williams
> Subject: Re: [Rd] [patch] giving library() a 'version' argument
>
> On 4/12/12 10:11 AM, Ken Williams wrote:
>
> >> On 4/12/12 7:22 AM, Duncan Murdoch wrote:
> > [SNIP]
> > ...
> > The main hats targeted here are really people (like me, of course) who
> > are trying to "productionize" results, not so much people who are
> > doing offline analysis.  In a production system
> >
> >> But what if your script requires a particular (perhaps obsolete)
> >> version of a package?  This change only puts a lower bound on the
> >> version number, and version requirements can be more elaborate than
> >> that.
> >
> > Certainly true; this was meant as a first iteration, and support for
> > the more elaborate requirements specifications could certainly be added.
> >
> > The more elaborate specs actually illustrate the need for a runtime
> > mechanism nicely - if code X (which may be a package, or a script, it
> > doesn't matter) requires exactly version 3.14 of package B, and
> > someone in the production team upgrades version 3.14 to version 3.78
> > because "it's faster" or "it's less buggy" or "we just like to have
> > the latest version of everything all the time", then someone needs to
> > be alerted to the problem.  One alternative solution would be to use a
> > full-fledged package management system like RPM or Deb to track all the
> dependencies, but yikes, that doesn't sound fun.
>
> I appreciate your contribution of both time and energy.
>
> But I think the existing library() method is sufficient without this modification.
> It's essentially syntactic sugar for:
>
> library(MASS); stopifnot(packageVersion("MASS") >= "7.3"))

I was about to write back & say "that's not correct, if '7.10' is installed, a string comparison will do the wrong thing."

But apparently it does the *right* thing, because 'numeric_version' class implements the comparison operator.

I'd still prefer to "Huffman-code it" to something shorter, to encourage people to use it, but I can see why others could consider it good enough.

I could contribute a doc patch to the 'numeric_version' man page to make it clearer what's available.  The 3 comparisons there happen to turn out the same way when done as a string comparison.

I also do still have a question about what packageVersion() does when a package is already loaded - does it go look for it again, or does it check the version of what's already loaded?  A doc patch could help here too.

 -Ken

CONFIDENTIALITY NOTICE: This e-mail message is for the s...{{dropped:7}}


From proebuck at mdanderson.org  Thu Apr 12 20:41:02 2012
From: proebuck at mdanderson.org (Roebuck,Paul L)
Date: Thu, 12 Apr 2012 13:41:02 -0500
Subject: [Rd] R-2.15.0 and Exporting Methods Converted To S4 Generic
Message-ID: <CBAC8BEE.2C4A2%proebuck@mdanderson.org>

Late to the show on this release, unfortunately.
One of our production packages no longer builds under R-2.15.0
with the following message.

** testing if installed package can be loaded
Error: Function found when exporting methods from the namespace
'SuperCurve' which is not S4 generic: 'image'


Possibly relevant clues follow:

## From R/AllGenerics.R
if (!isGeneric("image")) {
    setGeneric("image",
               function(x, ...) standardGeneric("image"))
}

We have done the same for many S3 generics, converting them
to S4 and adding specific method-combinations as such...

## From various other R files...
setMethod("image", signature(x="RPPA"), ...
setMethod("image", signature(x="RPPADesign"), ...

And then exported them for use outside the package.

## NAMESPACE
import(methods)
importFrom("graphics", image)

exportClasses(....)
exportMethods(image)


If the problem is because of the exportMethods(), I'm left
stumped as to how to work around this. Making our S4 objects
useable externally was kind of the point.


Source package available, if needed.

oompa <- "http://bioinformatics.mdanderson.org/OOMPA/2.14"
download.packages("SuperCurve", destdir=".", repos=oompa, type="source")                        


From proebuck at mdanderson.org  Thu Apr 12 21:17:45 2012
From: proebuck at mdanderson.org (Roebuck,Paul L)
Date: Thu, 12 Apr 2012 14:17:45 -0500
Subject: [Rd] [patch] giving library() a 'version' argument
In-Reply-To: <21A5E1E970CD46459ECBE86D6CC4B28C667AAA8C@spexch01.WindLogics.local>
Message-ID: <CBAC9489.2C4E5%proebuck@mdanderson.org>

On 4/12/12 1:56 PM, "Ken Williams" <Ken.Williams at windlogics.com> wrote:

> On April 12, 2012 1:48 PM, Paul Roebuck wrote:
>
>> Not sure I follow you here. The packageVersion() method is essentially a
>> shortcut to packageDescription("MyPackage")$Version. I generally avoid
>> doing package upgrades in my scripts so the loaded package IS the installed
>> package (even when playing .libPaths() tricks).
> 
> The scenario is:
> 
> library(PackageX)  # Quietly loads version 1 of PackageY
> 
> # Try to load a specific version of PackageY
> .libPaths('directory/containing/PackageY/version-2')
> library(PackageY)  # actually does nothing, since it's already loaded
> stopifnot(packageVersion('PackageY') >= 2)  # ??
> 
> 
> The intention of the stopifnot() expression is to make sure version 2 is
> loaded.
> 
> If packageVersion() goes & looks for PackageY in .libPaths() even when the
> package is already loaded, it will provide the wrong answer - because it will
> find version 2, but version 1 is what's loaded.  However, if packageVersion()
> checks the version of what's already loaded, then it would do the right thing
> here.
> 
> I don't think the docs for packageDescription() clarify what happens in this
> case, but I could be missing it.

If you're going to "play" with .libPaths() like that,
it should be done prior to ANY libraries being loaded.

Going about the style you do below, you'd need to parse
sessionInfo() instead. packageVersion() as well as
packageDescription() give you the information based on
the first package in the path with the same name. If you
dynamically change the path, the returned information
could be different...


From xie at yihui.name  Thu Apr 12 22:07:53 2012
From: xie at yihui.name (Yihui Xie)
Date: Thu, 12 Apr 2012 15:07:53 -0500
Subject: [Rd] Vignette questions
In-Reply-To: <4F86E9EB.3060609@gmail.com>
References: <4F85EC88.8070604@mayo.edu> <4F8610DA.80908@gmail.com>
	<4F86811D.5040201@statistik.tu-dortmund.de>
	<4F86E9EB.3060609@gmail.com>
Message-ID: <CANROs4ft1JsxrcdBLa1BM2XO0DW+QFXUVEXUaJO6Bj=Q3q7Zag@mail.gmail.com>

If R-forge gives the warning, it is likely to be their configuration
problem (they may be running R CMD check on the original source
directory instead of the tarball), so it is better to report to
R-forge admins directly.

I believe Terry is correct and it is a better practice *not* to put
PDF vignettes under version control when they can be automatically
generated.

If vignettes are too complicated, I would suggest not to use vignettes
in a package at all. You can put a fake vignette in the package and
point to the real vignette elsewhere. You may see my knitr package for
example: http://cran.r-project.org/package=knitr I have a dozen of
real "vignettes" but they are not under ./inst/doc/ but under
./inst/examples/. They will not take CRAN any time to check or build,
but I can use them to check if my package works fine on them locally.
After I release a new version to CRAN, I build PDF vignettes and
upload to Github (https://github.com/yihui/knitr/downloads) so users
who do not want to build by themselves can download them directly.

So l'd like to propose to free CRAN from the burdensome tasks of
checkings if you can do it by yourself:
http://yihui.name/knitr/demo/vignette/

Regards,
Yihui
--
Yihui Xie <xieyihui at gmail.com>
Phone: 515-294-2465 Web: http://yihui.name
Department of Statistics, Iowa State University
2215 Snedecor Hall, Ames, IA



On Thu, Apr 12, 2012 at 9:42 AM, Paul Gilbert <pgilbert902 at gmail.com> wrote:
>
>
> Now I am not sure if I am confused or if you missed the "if it cannot be
> built by R-forge and CRAN" part of my sentence. I understand that this is
> done automatically by R CMD build for vignettes that can be built on all, or
> most, R platforms. In the situation where R CMD build on R-forge will fail,
> or not result in a complete vignette pdf, I think it is necessary to put a
> good ?pdf in inst/doc in order to get a build on R-forge that can be
> submitted to CRAN. That is, in situations like:
>
> ?-the vignette requires databases or drivers not generally available
> ?-the vignette (legitimately) takes forever to run
> ?-the vignette requires a cluster
>
> I am now wondering what the recommended practice is. What I have been doing,
> which I thought was the recommended practice, is to put the vignette Rnw
> (Stex) file in vignettes/ and put a pdf, constructed on a machine that has
> appropriate resources, into inst/doc. ?Is that the recommended way to
> proceed?
>
> Related, some have commented that they put a pdf in inst/doc and then leave
> out the vignette Rnw file to avoid error messages. Is the discouraged or
> encouraged?
>
> Paul
>


From hb at biostat.ucsf.edu  Thu Apr 12 23:55:15 2012
From: hb at biostat.ucsf.edu (Henrik Bengtsson)
Date: Thu, 12 Apr 2012 14:55:15 -0700
Subject: [Rd] Vignette questions
In-Reply-To: <CANROs4ft1JsxrcdBLa1BM2XO0DW+QFXUVEXUaJO6Bj=Q3q7Zag@mail.gmail.com>
References: <4F85EC88.8070604@mayo.edu> <4F8610DA.80908@gmail.com>
	<4F86811D.5040201@statistik.tu-dortmund.de>
	<4F86E9EB.3060609@gmail.com>
	<CANROs4ft1JsxrcdBLa1BM2XO0DW+QFXUVEXUaJO6Bj=Q3q7Zag@mail.gmail.com>
Message-ID: <CAFDcVCQBZhPARUQVbL4iemDPTd3QVwKqK3Hj7QZt243__2UC=A@mail.gmail.com>

To have a prebuilt/static PDF appear as package vignette in the help,
browseVignettes(), and on CRAN, you can do:

1. Copy the PDF to inst/doc/, e.g. inst/doc/manual.pdf
2. Create a dummy inst/doc/manual.Rnw that contains four lines or LaTeX code:

%\VignetteIndexEntry{User manual}
\documentclass{article}
\begin{document}
\end{document}

>From Section 2.1 in
http://cran.r-project.org/web/packages/R.rsp/vignettes/NonSweaveVignettes.pdf

/Henrik

On Thu, Apr 12, 2012 at 1:07 PM, Yihui Xie <xie at yihui.name> wrote:
> If R-forge gives the warning, it is likely to be their configuration
> problem (they may be running R CMD check on the original source
> directory instead of the tarball), so it is better to report to
> R-forge admins directly.
>
> I believe Terry is correct and it is a better practice *not* to put
> PDF vignettes under version control when they can be automatically
> generated.
>
> If vignettes are too complicated, I would suggest not to use vignettes
> in a package at all. You can put a fake vignette in the package and
> point to the real vignette elsewhere. You may see my knitr package for
> example: http://cran.r-project.org/package=knitr I have a dozen of
> real "vignettes" but they are not under ./inst/doc/ but under
> ./inst/examples/. They will not take CRAN any time to check or build,
> but I can use them to check if my package works fine on them locally.
> After I release a new version to CRAN, I build PDF vignettes and
> upload to Github (https://github.com/yihui/knitr/downloads) so users
> who do not want to build by themselves can download them directly.
>
> So l'd like to propose to free CRAN from the burdensome tasks of
> checkings if you can do it by yourself:
> http://yihui.name/knitr/demo/vignette/
>
> Regards,
> Yihui
> --
> Yihui Xie <xieyihui at gmail.com>
> Phone: 515-294-2465 Web: http://yihui.name
> Department of Statistics, Iowa State University
> 2215 Snedecor Hall, Ames, IA
>
>
>
> On Thu, Apr 12, 2012 at 9:42 AM, Paul Gilbert <pgilbert902 at gmail.com> wrote:
>>
>>
>> Now I am not sure if I am confused or if you missed the "if it cannot be
>> built by R-forge and CRAN" part of my sentence. I understand that this is
>> done automatically by R CMD build for vignettes that can be built on all, or
>> most, R platforms. In the situation where R CMD build on R-forge will fail,
>> or not result in a complete vignette pdf, I think it is necessary to put a
>> good ?pdf in inst/doc in order to get a build on R-forge that can be
>> submitted to CRAN. That is, in situations like:
>>
>> ?-the vignette requires databases or drivers not generally available
>> ?-the vignette (legitimately) takes forever to run
>> ?-the vignette requires a cluster
>>
>> I am now wondering what the recommended practice is. What I have been doing,
>> which I thought was the recommended practice, is to put the vignette Rnw
>> (Stex) file in vignettes/ and put a pdf, constructed on a machine that has
>> appropriate resources, into inst/doc. ?Is that the recommended way to
>> proceed?
>>
>> Related, some have commented that they put a pdf in inst/doc and then leave
>> out the vignette Rnw file to avoid error messages. Is the discouraged or
>> encouraged?
>>
>> Paul
>>
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From ligges at statistik.tu-dortmund.de  Fri Apr 13 10:40:03 2012
From: ligges at statistik.tu-dortmund.de (Uwe Ligges)
Date: Fri, 13 Apr 2012 10:40:03 +0200
Subject: [Rd] R-2.15.0 and Exporting Methods Converted To S4 Generic
In-Reply-To: <CBAC8BEE.2C4A2%proebuck@mdanderson.org>
References: <CBAC8BEE.2C4A2%proebuck@mdanderson.org>
Message-ID: <4F87E663.3020405@statistik.tu-dortmund.de>

You have to export the new generic as well.

Uwe Ligges


On 12.04.2012 20:41, Roebuck,Paul L wrote:
> Late to the show on this release, unfortunately.
> One of our production packages no longer builds under R-2.15.0
> with the following message.
>
> ** testing if installed package can be loaded
> Error: Function found when exporting methods from the namespace
> 'SuperCurve' which is not S4 generic: 'image'
>
>
> Possibly relevant clues follow:
>
> ## From R/AllGenerics.R
> if (!isGeneric("image")) {
>      setGeneric("image",
>                 function(x, ...) standardGeneric("image"))
> }
>
> We have done the same for many S3 generics, converting them
> to S4 and adding specific method-combinations as such...
>
> ## From various other R files...
> setMethod("image", signature(x="RPPA"), ...
> setMethod("image", signature(x="RPPADesign"), ...
>
> And then exported them for use outside the package.
>
> ## NAMESPACE
> import(methods)
> importFrom("graphics", image)
>
> exportClasses(....)
> exportMethods(image)
>
>
> If the problem is because of the exportMethods(), I'm left
> stumped as to how to work around this. Making our S4 objects
> useable externally was kind of the point.
>
>
> Source package available, if needed.
>
> oompa<- "http://bioinformatics.mdanderson.org/OOMPA/2.14"
> download.packages("SuperCurve", destdir=".", repos=oompa, type="source")
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From maechler at stat.math.ethz.ch  Fri Apr 13 10:52:30 2012
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 13 Apr 2012 10:52:30 +0200
Subject: [Rd] [patch] giving library() a 'version' argument
In-Reply-To: <CBAC9489.2C4E5%proebuck@mdanderson.org>
References: <21A5E1E970CD46459ECBE86D6CC4B28C667AAA8C@spexch01.WindLogics.local>
	<CBAC9489.2C4E5%proebuck@mdanderson.org>
Message-ID: <20359.59726.5194.479618@stat.math.ethz.ch>

>>>>> Roebuck,Paul L <proebuck at mdanderson.org>
>>>>>     on Thu, 12 Apr 2012 14:17:45 -0500 writes:

    > On 4/12/12 1:56 PM, "Ken Williams" <Ken.Williams at windlogics.com> wrote:
    >> On April 12, 2012 1:48 PM, Paul Roebuck wrote:
    >> 
    >>> Not sure I follow you here. The packageVersion() method is
    >>> essentially a shortcut to packageDescription("MyPackage")$Version. I
    >>> generally avoid doing package upgrades in my scripts so the loaded
    >>> package IS the installed package (even when playing .libPaths()
    >>> tricks).
    >> 
    >> The scenario is:
    >> 
    >> library(PackageX) # Quietly loads version 1 of PackageY
    >> 
    >> # Try to load a specific version of PackageY
    >> .libPaths('directory/containing/PackageY/version-2') library(PackageY)
    >> # actually does nothing, since it's already loaded
    >> stopifnot(packageVersion('PackageY') >= 2) # ??
    >> 
    >> 
    >> The intention of the stopifnot() expression is to make sure version 2
    >> is loaded.
    >> 
    >> If packageVersion() goes & looks for PackageY in .libPaths() even when
    >> the package is already loaded, it will provide the wrong answer -
    >> because it will find version 2, but version 1 is what's loaded.
    >> However, if packageVersion() checks the version of what's already
    >> loaded, then it would do the right thing here.
    >> 
    >> I don't think the docs for packageDescription() clarify what happens
    >> in this case, but I could be missing it.

    > If you're going to "play" with .libPaths() like that,
    > it should be done prior to ANY libraries being loaded.

    > Going about the style you do below, you'd need to parse
    > sessionInfo() instead. packageVersion() as well as
    > packageDescription() give you the information based on
    > the first package in the path with the same name. If you
    > dynamically change the path, the returned information
    > could be different...

Aa..h, now we are getting into a more interesting issue:

Fortunately, what you say above has *not* been true for a while
(I think, but am not sure, that I was involved in fixing it..):

Indeed nowadays,  packageDescription()  *)  *does*
use the correct package version, by inspecting the "path"
attribute of the package, in the same way as
	  searchpaths()

does --- a function, BTW, that I think should be known and used
more than I think it is.

*) packageVersion() is built on packageDescription() and hence
 is also correct accordingly.

--
Martin Maechler, ETH Zurich (and R core).


From spot023 at aucklanduni.ac.nz  Fri Apr 13 03:56:58 2012
From: spot023 at aucklanduni.ac.nz (Simon Potter)
Date: Fri, 13 Apr 2012 13:56:58 +1200
Subject: [Rd] Reference Class import() behaviour
Message-ID: <CADH01s5PUTtONXP=8HnBMtuHAHUKDnhUR93_7afw6T8y6EKfsQ@mail.gmail.com>

Dear All,

In a project I've been working on we've been using Reference Classes
and grid extensively. However, something that I have come across is
that when using the import() method on refclass objects, it does not
work as expected with grid grobs and viewports.

I have prepared test cases that illustrate the point but the general
idea is that importing appears to work fine for everything except
viewports and grid grobs.

Code to illustrate the point follows.

# This first case is an example to show that the importing process
# I have been using works for simple cases
test <- setRefClass("classTest", fields = c("nums", "testlist"),
                    methods = list(
    initialize = function(nums = 1:10, testlist = NULL) {
        nums <<- nums
        testlist <<- testlist
    },
    view = function() {
        print(nums)
        print(testlist)
    }))

test2 <- setRefClass("classTest2", contains = "classTest")

t1 <- test$new(11:20, list(test = "hello, world!"))
t1$view()
t2 <- test2$new()
t2$view()
t2$import(t1) # Now lets grab t1's nums and list
t2$view()


# Now observe that when we start dealing with grid grobs and viewports
# the importing process breaks.
a.gen <- setRefClass("classA", fields = c("vps", "gt", "nums"),
                 methods = list(
    initialize = function(vps = NULL, gt = NULL, nums = 1:10) {
        vps <<- vps
        gt <<- gt
        nums <<- nums
    },
    view = function() {
        print(vps)
        print(gt)
        print(nums)
    }))

b.gen <- setRefClass("classB", contains = "classA")

library(grid)
my.vps <- vpTree(viewport(name = "parent"),
                 vpList(viewport(name = "child")))
my.gtree <- gTree(name = "tree",
                  children = gList(rectGrob(vp = "parent"),
                                   textGrob("test", vp = "child")))

a <- a.gen$new(vps = my.vps, gt = my.gtree, nums = 11:20)
b <- b.gen$new()
a$view()
b$view()
# Everything works as expected so far...

# Now lets try importing the fields from a into b
# Note that viewports and gTrees break the import process.
# Numbers and lists work fine, as in the example above.
b$import(a)
b$view()

# A workaround is to perform manual assignment
b$vps <- a$vps
b$gt <- a$gt
b$nums <- a$nums
b$view() # Desired result

I'm not sure what's causing this, or whether I'm going about this in
the wrong way but any assistance would be much appreciated.

-- Simon


From mtmorgan at fhcrc.org  Fri Apr 13 15:05:57 2012
From: mtmorgan at fhcrc.org (Martin Morgan)
Date: Fri, 13 Apr 2012 06:05:57 -0700
Subject: [Rd] R-2.15.0 and Exporting Methods Converted To S4 Generic
In-Reply-To: <4F87E663.3020405@statistik.tu-dortmund.de>
References: <CBAC8BEE.2C4A2%proebuck@mdanderson.org>
	<4F87E663.3020405@statistik.tu-dortmund.de>
Message-ID: <4F8824B5.1050801@fhcrc.org>

On 04/13/2012 01:40 AM, Uwe Ligges wrote:
> You have to export the new generic as well.

that's not correct.

>
> Uwe Ligges
>
>
> On 12.04.2012 20:41, Roebuck,Paul L wrote:
>> Late to the show on this release, unfortunately.
>> One of our production packages no longer builds under R-2.15.0
>> with the following message.
>>
>> ** testing if installed package can be loaded
>> Error: Function found when exporting methods from the namespace
>> 'SuperCurve' which is not S4 generic: 'image'
>>
>>
>> Possibly relevant clues follow:
>>
>> ## From R/AllGenerics.R
>> if (!isGeneric("image")) {
>> setGeneric("image",
>> function(x, ...) standardGeneric("image"))
>> }

I think this paradigm is left over from an earlier time; at the time of 
package installation you know that your namespace has access to 
graphics::image, and that graphics::image is not an S4 generic. Also, 
your intention is to make an S4 generic without changing the signature. 
So just

setGeneric("image")

though the more explicit setGeneric is not incorrect, and the 
conditional promotion shouldn't be incorrect either.

>>
>> We have done the same for many S3 generics, converting them
>> to S4 and adding specific method-combinations as such...
>>
>> ## From various other R files...
>> setMethod("image", signature(x="RPPA"), ...
>> setMethod("image", signature(x="RPPADesign"), ...
>>
>> And then exported them for use outside the package.
>>
>> ## NAMESPACE
>> import(methods)
>> importFrom("graphics", image)
>>
>> exportClasses(....)
>> exportMethods(image)

conceptually, having created the generic it seems like you should export 
it, but exportMethods will export the generic as well.

Your package actually installs under R-devel, and I wonder if this is a 
manifestation of the bug reported here

https://stat.ethz.ch/pipermail/r-devel/2012-April/063783.html

A work-around seems to be an unconditional setGeneric("image").

Martin

>>
>>
>> If the problem is because of the exportMethods(), I'm left
>> stumped as to how to work around this. Making our S4 objects
>> useable externally was kind of the point.
>>
>>
>> Source package available, if needed.
>>
>> oompa<- "http://bioinformatics.mdanderson.org/OOMPA/2.14"
>> download.packages("SuperCurve", destdir=".", repos=oompa, type="source")
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


-- 
Computational Biology
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N. PO Box 19024 Seattle, WA 98109

Location: M1-B861
Telephone: 206 667-2793


From Ken.Williams at windlogics.com  Fri Apr 13 17:22:56 2012
From: Ken.Williams at windlogics.com (Ken Williams)
Date: Fri, 13 Apr 2012 10:22:56 -0500
Subject: [Rd] [patch] giving library() a 'version' argument
In-Reply-To: <20359.59726.5194.479618@stat.math.ethz.ch>
References: <21A5E1E970CD46459ECBE86D6CC4B28C667AAA8C@spexch01.WindLogics.local>
	<CBAC9489.2C4E5%proebuck@mdanderson.org>,
	<20359.59726.5194.479618@stat.math.ethz.ch>
Message-ID: <21A5E1E970CD46459ECBE86D6CC4B28C6691F14A@spexch01.WindLogics.local>


________________________________________
From: Martin Maechler [maechler at stat.math.ethz.ch]

> Indeed nowadays,  packageDescription()  *)  *does*
> use the correct package version, by inspecting the "path"
> attribute of the package, in the same way as
>           searchpaths()

Yeah, that's what I suspected, but only from reading the code of packageDescription().  It doesn't seem to mention this in the docs.  And I wasn't 100% confident from reading the code, in case it was a 'promise' or something like that.

I'm willing to write a doc patch but it'll take a few days, I'm traveling.

---
Ken Williams, Senior Research Scientist
Applied Mathematics Group, WindLogics Inc.

CONFIDENTIALITY NOTICE: This e-mail message is for the s...{{dropped:7}}


From dtenenba at fhcrc.org  Fri Apr 13 22:30:03 2012
From: dtenenba at fhcrc.org (Dan Tenenbaum)
Date: Fri, 13 Apr 2012 13:30:03 -0700
Subject: [Rd] Creating a reference manual during R CMD build
Message-ID: <CAF42j23o-rmCFVo5xaGyR7zK9h5G8fbxByEWu9H0j3MAuCXO6w@mail.gmail.com>

I'd like to have a way to create reference manuals during R CMD build
(and have them go in the build/ directory of the resulting package
tarball), whether or not any of my Rd files have \Sexpr macros in
them.

It seems like there are a couple of ways to suppress the building of
manuals during R CMD build: 1) using --no-manual and 2) putting
BuildManual: no in DESCRIPTION, but there are no corresponding ways to
force a reference manual to be built during R CMD build.

I know the manual is produced during R CMD check, but for the purposes
of putting together the Bioconductor website, it is simpler if the
manual is creating during build and then extracted from the source
tarball later when we propagate newly built packages to our website.
If we instead use the manuals produced during R CMD check, it's
possible that the package and manual can get out of sync.

Is this something that can be added?

I'd prefer for it to be a command-line switch to R CMD build (such as
--force-build-manual) rather than a package-level metadata item in the
DESCRIPTION file, because I want manuals built for all Bioconductor
packages without having to worry whether I've added the appropriate
line to the DESCRIPTION file.

Thanks very much,
Dan


From bbolker at gmail.com  Sun Apr 15 02:45:06 2012
From: bbolker at gmail.com (Ben Bolker)
Date: Sat, 14 Apr 2012 20:45:06 -0400
Subject: [Rd] R CMD check with non-standard .libPaths
Message-ID: <4F8A1A12.1040004@gmail.com>


  Does anyone have advice on how to instruct R CMD check to use a
non-standard set of libraries?  Here's the situation:

  I'm trying to do some automated checking on package dependencies of a
package I maintain.  In order to do that I've written code that takes
the list of the dependent packages and for each package (1) downloads
the most recent/available .tar.gz file; (2) installs the "Suggests:" and
"Depends:" packages for the package that are not already installed; (3)
runs R CMD check and stores the output.

  I wanted to do this in a way that would not necessarily bloat my base
installation, so I wanted to do step #2 into a new library.  Once I've
figured out what the missing dependencies are (depMiss), I

 install.packages(depMiss,lib=libdir)

Then I run R CMD check as follows.

   ss <- suppressWarnings(system(
   paste("export R_LIBS=./library; R CMD check",
               file.path(tarballdir,tn)),
                 intern=TRUE))

However, this only seems to work partially.  It does prevent the check
from failing with an error that the package doesn't exist: *BUT* when
the examples are actually run, I get results like this (this is from the
'agridat' package, which "Suggests:" the hglm package, which has hence
been installed in the 'libdir' directory).

[168] "168: Loading required package: hglm"

[169] "169: Warning in library(package, lib.loc = lib.loc,
character.only = TRUE, logical.return = TRUE,  :"
[170] "170:   there is no package called ?hglm?"

  Checking installed.packages() shows that hglm is indeed installed in
the appropriate directory.  agridat "Suggests:" hglm.  The
"crowder.germination" example in agridat tries require(hglm) and fails;
it therefore doesn't fit the relevant HGLM model -- when the example
tries to reference this model a few lines later, the example fails.
Admittedly this could be seen as a bug in the example (it shouldn't try
to access a model it knows it can't fit), but I wonder if there's a way
I can get the examples run to see the non-standard package location.

   I could (I guess) modify my .Rprofile temporarily ... ?  But I'm
curious if there's a right way to do this ...

  thanks
    Ben Bolker


From edd at debian.org  Sun Apr 15 03:49:05 2012
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sat, 14 Apr 2012 20:49:05 -0500
Subject: [Rd] R CMD check with non-standard .libPaths
In-Reply-To: <4F8A1A12.1040004@gmail.com>
References: <4F8A1A12.1040004@gmail.com>
Message-ID: <20362.10513.35718.904654@max.nulle.part>


On 14 April 2012 at 20:45, Ben Bolker wrote:
| 
|   Does anyone have advice on how to instruct R CMD check to use a
| non-standard set of libraries?  Here's the situation:

One way around is something like this:

edd at max:~$ tail -5 .R/check.Renviron

# edd Apr 2003  Allow local install in /usr/local, also add a directory for
#               Debian packaged CRAN packages, and finally the default dir 
# edd Jul 2007  Now use R_LIBS_SITE, not R_LIBS
R_LIBS_SITE=${R_LIBS_SITE-'/usr/local/lib/R/site-library:/usr/lib/R/site-library:/usr/lib/R/library'}
edd at max:~$ 

The file check.Renviron sets up R environment variables just for R CMD
check.  [ And IIRC there is also build.Renviron ]

|   I'm trying to do some automated checking on package dependencies of a
| package I maintain.  In order to do that I've written code that takes
| the list of the dependent packages and for each package (1) downloads
| the most recent/available .tar.gz file; (2) installs the "Suggests:" and
| "Depends:" packages for the package that are not already installed; (3)
| runs R CMD check and stores the output.
| 
|   I wanted to do this in a way that would not necessarily bloat my base
| installation, so I wanted to do step #2 into a new library.  Once I've
| figured out what the missing dependencies are (depMiss), I

So you could have a script setting the check.Renviron up alongside the
build-up and later tear-down of you test setup.

Would be good to publish such a set of scripts. I hacked up something much
smaller than that and much more ad-hoc too to test the reverse depends of one
of my package before uploading it.

Dirk
 


| 
|  install.packages(depMiss,lib=libdir)
| 
| Then I run R CMD check as follows.
| 
|    ss <- suppressWarnings(system(
|    paste("export R_LIBS=./library; R CMD check",
|                file.path(tarballdir,tn)),
|                  intern=TRUE))
| 
| However, this only seems to work partially.  It does prevent the check
| from failing with an error that the package doesn't exist: *BUT* when
| the examples are actually run, I get results like this (this is from the
| 'agridat' package, which "Suggests:" the hglm package, which has hence
| been installed in the 'libdir' directory).
| 
| [168] "168: Loading required package: hglm"
| 
| [169] "169: Warning in library(package, lib.loc = lib.loc,
| character.only = TRUE, logical.return = TRUE,  :"
| [170] "170:   there is no package called ?hglm?"
| 
|   Checking installed.packages() shows that hglm is indeed installed in
| the appropriate directory.  agridat "Suggests:" hglm.  The
| "crowder.germination" example in agridat tries require(hglm) and fails;
| it therefore doesn't fit the relevant HGLM model -- when the example
| tries to reference this model a few lines later, the example fails.
| Admittedly this could be seen as a bug in the example (it shouldn't try
| to access a model it knows it can't fit), but I wonder if there's a way
| I can get the examples run to see the non-standard package location.
| 
|    I could (I guess) modify my .Rprofile temporarily ... ?  But I'm
| curious if there's a right way to do this ...
| 
|   thanks
|     Ben Bolker
| 
| ______________________________________________
| R-devel at r-project.org mailing list
| https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
R/Finance 2012 Conference on May 11 and 12, 2012 at UIC in Chicago, IL
See agenda, registration details and more at http://www.RinFinance.com


From pburns at pburns.seanet.com  Sun Apr 15 18:55:24 2012
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Sun, 15 Apr 2012 17:55:24 +0100
Subject: [Rd] no carriage returns in BATCH output from 2.15.0
Message-ID: <4F8AFD7C.1030905@pburns.seanet.com>

It seems like I must be missing something
since I haven't been able to find mention
of this.

Under Windows 7 I'm not getting carriage returns
in the output of BATCH files using 2.15.0 (both
64-bit and 32-bit).  They are in the startup
messages, but not for the real output.  Is this
on purpose?

Pat


-- 
Patrick Burns
pburns at pburns.seanet.com
twitter: @portfolioprobe
http://www.portfolioprobe.com/blog
http://www.burns-stat.com
(home of 'Some hints for the R beginner'
and 'The R Inferno')


From ligges at statistik.tu-dortmund.de  Sun Apr 15 19:41:26 2012
From: ligges at statistik.tu-dortmund.de (Uwe Ligges)
Date: Sun, 15 Apr 2012 19:41:26 +0200
Subject: [Rd] no carriage returns in BATCH output from 2.15.0
In-Reply-To: <4F8AFD7C.1030905@pburns.seanet.com>
References: <4F8AFD7C.1030905@pburns.seanet.com>
Message-ID: <4F8B0846.1030508@statistik.tu-dortmund.de>



On 15.04.2012 18:55, Patrick Burns wrote:
> It seems like I must be missing something
> since I haven't been able to find mention
> of this.
>
> Under Windows 7 I'm not getting carriage returns
> in the output of BATCH files using 2.15.0 (both
> 64-bit and 32-bit). They are in the startup
> messages, but not for the real output. Is this
> on purpose?

This is probably a CR/LF issue - and you editor does not interpret 
Unix-like line endings correctly.

Uwe

>
> Pat
>
>


From murdoch.duncan at gmail.com  Sun Apr 15 22:06:31 2012
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Sun, 15 Apr 2012 16:06:31 -0400
Subject: [Rd] no carriage returns in BATCH output from 2.15.0
In-Reply-To: <4F8B0846.1030508@statistik.tu-dortmund.de>
References: <4F8AFD7C.1030905@pburns.seanet.com>
	<4F8B0846.1030508@statistik.tu-dortmund.de>
Message-ID: <4F8B2A47.1070203@gmail.com>

On 12-04-15 1:41 PM, Uwe Ligges wrote:
>
>
> On 15.04.2012 18:55, Patrick Burns wrote:
>> It seems like I must be missing something
>> since I haven't been able to find mention
>> of this.
>>
>> Under Windows 7 I'm not getting carriage returns
>> in the output of BATCH files using 2.15.0 (both
>> 64-bit and 32-bit). They are in the startup
>> messages, but not for the real output. Is this
>> on purpose?
>
> This is probably a CR/LF issue - and you editor does not interpret
> Unix-like line endings correctly.

The problem is that R writes CR/LF on some lines, just LF on others.  I 
see this in 2.15.0 and R-devel.  I'll try to track it down.

To reproduce, I put these lines into test.R:

x <- 1
x
2

and then ran

Rcmd BATCH test.R

The test.Rout file has the messed up line endings.

Duncan Murdoch


From pauljohn32 at gmail.com  Mon Apr 16 19:52:30 2012
From: pauljohn32 at gmail.com (Paul Johnson)
Date: Mon, 16 Apr 2012 12:52:30 -0500
Subject: [Rd] I wish xlim=c(0, NA) would work. How about I send you a patch?
Message-ID: <CAErODj9RJ60Xdp=sLu=pGRiBOY_CSsxnjUmqObxhHYtEhHyLxA@mail.gmail.com>

I'm looking for an R mentor.  I want to propose a change in management
of plot options xlim and ylim.

Did you ever want to change one coordinate in xlim or ylim? It happens
to me all the time.

x <- rnorm(100, m=5, s=1)
y <- rnorm(100, m=6, s=1)
plot(x,y)

## Oh, I want the "y axis" to show above x=0.

plot(x,y, xlim=c(0, ))

##Output: Error in c(0, ) : argument 2 is empty

 plot(x,y, xlim=c(0,NA ))
## Output: Error in plot.window(...) : need finite 'xlim' values


I wish that plot would let me supply just the min (or max) and then
calculate the other value where needed.
It is a little bit tedious for the user to do that for herself.  The
user must be knowledgeable enough to know where the maximum (MAX) is
supposed to be, and then supply xlim=c(0, MAX). I can't see any reason
for insisting users have that deeper understanding of how R calculates
ranges for plots.

Suppose the user is allowed to supply NA to signal R should fill in the blanks.

plot(x,y, xlim=c(0, NA))


In plot.default now, I find this code to manage xlim

   xlim <- if (is.null(xlim))
        range(xy$x[is.finite(xy$x)])

And I would change it to be something like
   ##get default range
   nxlim <- range(xy$x[is.finite(xy$x)])

   ## if xlim is NULL, so same as current
    xlim <- if (is.null(xlim)) nxlim
## Otherwise, replace NAs in xlim with values from nxlim
    else xlim[ is.na(xlim) ] <- nxlim[ is.na(xlim) ]


Who is the responsible party for plot.default.  How about it?

Think of how much happier users would be!

pj
-- 
Paul E. Johnson
Professor, Political Science ? ?Assoc. Director
1541 Lilac Lane, Room 504 ? ? Center for Research Methods
University of Kansas ? ? ? ? ? ? ? University of Kansas
http://pj.freefaculty.org ? ? ? ? ? ?http://quant.ku.edu


From murdoch.duncan at gmail.com  Mon Apr 16 20:13:59 2012
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Mon, 16 Apr 2012 14:13:59 -0400
Subject: [Rd] I wish xlim=c(0,
 NA) would work. How about I send you a patch?
In-Reply-To: <CAErODj9RJ60Xdp=sLu=pGRiBOY_CSsxnjUmqObxhHYtEhHyLxA@mail.gmail.com>
References: <CAErODj9RJ60Xdp=sLu=pGRiBOY_CSsxnjUmqObxhHYtEhHyLxA@mail.gmail.com>
Message-ID: <4F8C6167.5030905@gmail.com>

On 12-04-16 1:52 PM, Paul Johnson wrote:
> I'm looking for an R mentor.  I want to propose a change in management
> of plot options xlim and ylim.

Your suggestion sounds reasonable, but because plot limits are such a 
commonly used parameter, it would have to be done quite carefully.  The 
questions I'd ask before implementing it are:

  - Are there other locations besides plot.default where xlim and ylim 
are specified?  I'd like to have them updated consistently.

  - Are there any conflicting uses of NA for a limit in published packages?

  - Which package authors would need to be told about this change, so 
they could make a compatible change?

  - Where does it need to be documented?

Duncan Murdoch

>
> Did you ever want to change one coordinate in xlim or ylim? It happens
> to me all the time.
>
> x<- rnorm(100, m=5, s=1)
> y<- rnorm(100, m=6, s=1)
> plot(x,y)
>
> ## Oh, I want the "y axis" to show above x=0.
>
> plot(x,y, xlim=c(0, ))
>
> ##Output: Error in c(0, ) : argument 2 is empty
>
>   plot(x,y, xlim=c(0,NA ))
> ## Output: Error in plot.window(...) : need finite 'xlim' values
>
>
> I wish that plot would let me supply just the min (or max) and then
> calculate the other value where needed.
> It is a little bit tedious for the user to do that for herself.  The
> user must be knowledgeable enough to know where the maximum (MAX) is
> supposed to be, and then supply xlim=c(0, MAX). I can't see any reason
> for insisting users have that deeper understanding of how R calculates
> ranges for plots.
>
> Suppose the user is allowed to supply NA to signal R should fill in the blanks.
>
> plot(x,y, xlim=c(0, NA))
>
>
> In plot.default now, I find this code to manage xlim
>
>     xlim<- if (is.null(xlim))
>          range(xy$x[is.finite(xy$x)])
>
> And I would change it to be something like
>     ##get default range
>     nxlim<- range(xy$x[is.finite(xy$x)])
>
>     ## if xlim is NULL, so same as current
>      xlim<- if (is.null(xlim)) nxlim
> ## Otherwise, replace NAs in xlim with values from nxlim
>      else xlim[ is.na(xlim) ]<- nxlim[ is.na(xlim) ]
>
>
> Who is the responsible party for plot.default.  How about it?
>
> Think of how much happier users would be!
>
> pj


From lorenz at usgs.gov  Mon Apr 16 20:19:27 2012
From: lorenz at usgs.gov (David L Lorenz)
Date: Mon, 16 Apr 2012 13:19:27 -0500
Subject: [Rd] I wish xlim=c(0,
	NA) would work. How about I send you a patch?
In-Reply-To: <CAErODj9RJ60Xdp=sLu=pGRiBOY_CSsxnjUmqObxhHYtEhHyLxA@mail.gmail.com>
References: <CAErODj9RJ60Xdp=sLu=pGRiBOY_CSsxnjUmqObxhHYtEhHyLxA@mail.gmail.com>
Message-ID: <OFC9E224FD.29944776-ON862579E2.0064861F-862579E2.0064A494@usgs.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20120416/8e155084/attachment.pl>

From wdunlap at tibco.com  Mon Apr 16 20:26:17 2012
From: wdunlap at tibco.com (William Dunlap)
Date: Mon, 16 Apr 2012 18:26:17 +0000
Subject: [Rd] I wish xlim=c(0,
 NA) would work. How about I send you a patch?
In-Reply-To: <4F8C6167.5030905@gmail.com>
References: <CAErODj9RJ60Xdp=sLu=pGRiBOY_CSsxnjUmqObxhHYtEhHyLxA@mail.gmail.com>
	<4F8C6167.5030905@gmail.com>
Message-ID: <E66794E69CFDE04D9A70842786030B932960C1@PA-MBX04.na.tibco.com>

plot(1:10, xlim=c(10,1)) reverses the x axis.
If we allow plot(1:10, xlim=c(5,NA)), which
direction should it go?    Would this require new
parameters, {x,y}{min,max} or new paremeters
{x,y}{axisDirection}?

Bill Dunlap
Spotfire, TIBCO Software
wdunlap tibco.com


> -----Original Message-----
> From: r-devel-bounces at r-project.org [mailto:r-devel-bounces at r-project.org] On Behalf
> Of Duncan Murdoch
> Sent: Monday, April 16, 2012 11:14 AM
> To: Paul Johnson
> Cc: R Devel List
> Subject: Re: [Rd] I wish xlim=c(0, NA) would work. How about I send you a patch?
> 
> On 12-04-16 1:52 PM, Paul Johnson wrote:
> > I'm looking for an R mentor.  I want to propose a change in management
> > of plot options xlim and ylim.
> 
> Your suggestion sounds reasonable, but because plot limits are such a
> commonly used parameter, it would have to be done quite carefully.  The
> questions I'd ask before implementing it are:
> 
>   - Are there other locations besides plot.default where xlim and ylim
> are specified?  I'd like to have them updated consistently.
> 
>   - Are there any conflicting uses of NA for a limit in published packages?
> 
>   - Which package authors would need to be told about this change, so
> they could make a compatible change?
> 
>   - Where does it need to be documented?
> 
> Duncan Murdoch
> 
> >
> > Did you ever want to change one coordinate in xlim or ylim? It happens
> > to me all the time.
> >
> > x<- rnorm(100, m=5, s=1)
> > y<- rnorm(100, m=6, s=1)
> > plot(x,y)
> >
> > ## Oh, I want the "y axis" to show above x=0.
> >
> > plot(x,y, xlim=c(0, ))
> >
> > ##Output: Error in c(0, ) : argument 2 is empty
> >
> >   plot(x,y, xlim=c(0,NA ))
> > ## Output: Error in plot.window(...) : need finite 'xlim' values
> >
> >
> > I wish that plot would let me supply just the min (or max) and then
> > calculate the other value where needed.
> > It is a little bit tedious for the user to do that for herself.  The
> > user must be knowledgeable enough to know where the maximum (MAX) is
> > supposed to be, and then supply xlim=c(0, MAX). I can't see any reason
> > for insisting users have that deeper understanding of how R calculates
> > ranges for plots.
> >
> > Suppose the user is allowed to supply NA to signal R should fill in the blanks.
> >
> > plot(x,y, xlim=c(0, NA))
> >
> >
> > In plot.default now, I find this code to manage xlim
> >
> >     xlim<- if (is.null(xlim))
> >          range(xy$x[is.finite(xy$x)])
> >
> > And I would change it to be something like
> >     ##get default range
> >     nxlim<- range(xy$x[is.finite(xy$x)])
> >
> >     ## if xlim is NULL, so same as current
> >      xlim<- if (is.null(xlim)) nxlim
> > ## Otherwise, replace NAs in xlim with values from nxlim
> >      else xlim[ is.na(xlim) ]<- nxlim[ is.na(xlim) ]
> >
> >
> > Who is the responsible party for plot.default.  How about it?
> >
> > Think of how much happier users would be!
> >
> > pj
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From marc_schwartz at me.com  Mon Apr 16 21:09:14 2012
From: marc_schwartz at me.com (Marc Schwartz)
Date: Mon, 16 Apr 2012 14:09:14 -0500
Subject: [Rd] I wish xlim=c(0,
	NA) would work. How about I send you a patch?
In-Reply-To: <E66794E69CFDE04D9A70842786030B932960C1@PA-MBX04.na.tibco.com>
References: <CAErODj9RJ60Xdp=sLu=pGRiBOY_CSsxnjUmqObxhHYtEhHyLxA@mail.gmail.com>
	<4F8C6167.5030905@gmail.com>
	<E66794E69CFDE04D9A70842786030B932960C1@PA-MBX04.na.tibco.com>
Message-ID: <9A518888-0248-466A-B5D6-5AFADC97E1CF@me.com>

I would say that Bill's example of reversing the axis range, which I have used many times over the years, along with Duncan's list of potential consequences for a myriad of functions/packages, are arguments against this change. 

Classically, R's defaults are generally sensible and if one wants to alter them, it is not unreasonable to expect some understanding of what the arguments are doing. To know how to manipulate a plot's axis ranges is not overly complicated. Knowing that R, by default, will extend them by 4% is arguably a little more subtle, but reading the documentation (eg. ?par) will enlighten one to that point.

To simply use:

  xlim = c(0, max(x))

or

  xlim = c(0, max(x, na.rm = TRUE))

or 

  xlim = c(0, max(x[is.finite(x)]))

depending upon what values in x (or y) one might have to worry about, or the reverse for an unknown min and a fixed max, is a reasonable solution to the issue that Paul raises. It is a few more keystrokes than using NA and avoids the myriad known and potentially unanticipated side effects.

Regards,

Marc Schwartz

On Apr 16, 2012, at 1:26 PM, William Dunlap wrote:

> plot(1:10, xlim=c(10,1)) reverses the x axis.
> If we allow plot(1:10, xlim=c(5,NA)), which
> direction should it go?    Would this require new
> parameters, {x,y}{min,max} or new paremeters
> {x,y}{axisDirection}?
> 
> Bill Dunlap
> Spotfire, TIBCO Software
> wdunlap tibco.com
> 
> 
>> -----Original Message-----
>> From: r-devel-bounces at r-project.org [mailto:r-devel-bounces at r-project.org] On Behalf
>> Of Duncan Murdoch
>> Sent: Monday, April 16, 2012 11:14 AM
>> To: Paul Johnson
>> Cc: R Devel List
>> Subject: Re: [Rd] I wish xlim=c(0, NA) would work. How about I send you a patch?
>> 
>> On 12-04-16 1:52 PM, Paul Johnson wrote:
>>> I'm looking for an R mentor.  I want to propose a change in management
>>> of plot options xlim and ylim.
>> 
>> Your suggestion sounds reasonable, but because plot limits are such a
>> commonly used parameter, it would have to be done quite carefully.  The
>> questions I'd ask before implementing it are:
>> 
>>  - Are there other locations besides plot.default where xlim and ylim
>> are specified?  I'd like to have them updated consistently.
>> 
>>  - Are there any conflicting uses of NA for a limit in published packages?
>> 
>>  - Which package authors would need to be told about this change, so
>> they could make a compatible change?
>> 
>>  - Where does it need to be documented?
>> 
>> Duncan Murdoch
>> 
>>> 
>>> Did you ever want to change one coordinate in xlim or ylim? It happens
>>> to me all the time.
>>> 
>>> x<- rnorm(100, m=5, s=1)
>>> y<- rnorm(100, m=6, s=1)
>>> plot(x,y)
>>> 
>>> ## Oh, I want the "y axis" to show above x=0.
>>> 
>>> plot(x,y, xlim=c(0, ))
>>> 
>>> ##Output: Error in c(0, ) : argument 2 is empty
>>> 
>>>  plot(x,y, xlim=c(0,NA ))
>>> ## Output: Error in plot.window(...) : need finite 'xlim' values
>>> 
>>> 
>>> I wish that plot would let me supply just the min (or max) and then
>>> calculate the other value where needed.
>>> It is a little bit tedious for the user to do that for herself.  The
>>> user must be knowledgeable enough to know where the maximum (MAX) is
>>> supposed to be, and then supply xlim=c(0, MAX). I can't see any reason
>>> for insisting users have that deeper understanding of how R calculates
>>> ranges for plots.
>>> 
>>> Suppose the user is allowed to supply NA to signal R should fill in the blanks.
>>> 
>>> plot(x,y, xlim=c(0, NA))
>>> 
>>> 
>>> In plot.default now, I find this code to manage xlim
>>> 
>>>    xlim<- if (is.null(xlim))
>>>         range(xy$x[is.finite(xy$x)])
>>> 
>>> And I would change it to be something like
>>>    ##get default range
>>>    nxlim<- range(xy$x[is.finite(xy$x)])
>>> 
>>>    ## if xlim is NULL, so same as current
>>>     xlim<- if (is.null(xlim)) nxlim
>>> ## Otherwise, replace NAs in xlim with values from nxlim
>>>     else xlim[ is.na(xlim) ]<- nxlim[ is.na(xlim) ]
>>> 
>>> 
>>> Who is the responsible party for plot.default.  How about it?
>>> 
>>> Think of how much happier users would be!
>>> 
>>> pj


From murdoch.duncan at gmail.com  Mon Apr 16 21:15:31 2012
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Mon, 16 Apr 2012 15:15:31 -0400
Subject: [Rd] I wish xlim=c(0,
 NA) would work. How about I send you a patch?
In-Reply-To: <E66794E69CFDE04D9A70842786030B932960C1@PA-MBX04.na.tibco.com>
References: <CAErODj9RJ60Xdp=sLu=pGRiBOY_CSsxnjUmqObxhHYtEhHyLxA@mail.gmail.com>
	<4F8C6167.5030905@gmail.com>
	<E66794E69CFDE04D9A70842786030B932960C1@PA-MBX04.na.tibco.com>
Message-ID: <4F8C6FD3.4060703@gmail.com>

On 12-04-16 2:26 PM, William Dunlap wrote:
> plot(1:10, xlim=c(10,1)) reverses the x axis.
> If we allow plot(1:10, xlim=c(5,NA)), which
> direction should it go?    Would this require new
> parameters, {x,y}{min,max} or new paremeters
> {x,y}{axisDirection}?

I'd rather not add another parameter.  So if I were to implement this, 
I'd probably choose Paul's original suggestion.  If someone wants c(5, 
NA) to mean c(5, min(data)) rather than c(5, max(data)) they'd need to 
code it explicitly.

Duncan Murdoch

>
> Bill Dunlap
> Spotfire, TIBCO Software
> wdunlap tibco.com
>
>
>> -----Original Message-----
>> From: r-devel-bounces at r-project.org [mailto:r-devel-bounces at r-project.org] On Behalf
>> Of Duncan Murdoch
>> Sent: Monday, April 16, 2012 11:14 AM
>> To: Paul Johnson
>> Cc: R Devel List
>> Subject: Re: [Rd] I wish xlim=c(0, NA) would work. How about I send you a patch?
>>
>> On 12-04-16 1:52 PM, Paul Johnson wrote:
>>> I'm looking for an R mentor.  I want to propose a change in management
>>> of plot options xlim and ylim.
>>
>> Your suggestion sounds reasonable, but because plot limits are such a
>> commonly used parameter, it would have to be done quite carefully.  The
>> questions I'd ask before implementing it are:
>>
>>    - Are there other locations besides plot.default where xlim and ylim
>> are specified?  I'd like to have them updated consistently.
>>
>>    - Are there any conflicting uses of NA for a limit in published packages?
>>
>>    - Which package authors would need to be told about this change, so
>> they could make a compatible change?
>>
>>    - Where does it need to be documented?
>>
>> Duncan Murdoch
>>
>>>
>>> Did you ever want to change one coordinate in xlim or ylim? It happens
>>> to me all the time.
>>>
>>> x<- rnorm(100, m=5, s=1)
>>> y<- rnorm(100, m=6, s=1)
>>> plot(x,y)
>>>
>>> ## Oh, I want the "y axis" to show above x=0.
>>>
>>> plot(x,y, xlim=c(0, ))
>>>
>>> ##Output: Error in c(0, ) : argument 2 is empty
>>>
>>>    plot(x,y, xlim=c(0,NA ))
>>> ## Output: Error in plot.window(...) : need finite 'xlim' values
>>>
>>>
>>> I wish that plot would let me supply just the min (or max) and then
>>> calculate the other value where needed.
>>> It is a little bit tedious for the user to do that for herself.  The
>>> user must be knowledgeable enough to know where the maximum (MAX) is
>>> supposed to be, and then supply xlim=c(0, MAX). I can't see any reason
>>> for insisting users have that deeper understanding of how R calculates
>>> ranges for plots.
>>>
>>> Suppose the user is allowed to supply NA to signal R should fill in the blanks.
>>>
>>> plot(x,y, xlim=c(0, NA))
>>>
>>>
>>> In plot.default now, I find this code to manage xlim
>>>
>>>      xlim<- if (is.null(xlim))
>>>           range(xy$x[is.finite(xy$x)])
>>>
>>> And I would change it to be something like
>>>      ##get default range
>>>      nxlim<- range(xy$x[is.finite(xy$x)])
>>>
>>>      ## if xlim is NULL, so same as current
>>>       xlim<- if (is.null(xlim)) nxlim
>>> ## Otherwise, replace NAs in xlim with values from nxlim
>>>       else xlim[ is.na(xlim) ]<- nxlim[ is.na(xlim) ]
>>>
>>>
>>> Who is the responsible party for plot.default.  How about it?
>>>
>>> Think of how much happier users would be!
>>>
>>> pj
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel


From hb at biostat.ucsf.edu  Mon Apr 16 22:11:39 2012
From: hb at biostat.ucsf.edu (Henrik Bengtsson)
Date: Mon, 16 Apr 2012 13:11:39 -0700
Subject: [Rd] I wish xlim=c(0,
	NA) would work. How about I send you a patch?
In-Reply-To: <4F8C6FD3.4060703@gmail.com>
References: <CAErODj9RJ60Xdp=sLu=pGRiBOY_CSsxnjUmqObxhHYtEhHyLxA@mail.gmail.com>
	<4F8C6167.5030905@gmail.com>
	<E66794E69CFDE04D9A70842786030B932960C1@PA-MBX04.na.tibco.com>
	<4F8C6FD3.4060703@gmail.com>
Message-ID: <CAFDcVCR+daUAsb+R7FuBuOO8p94YFmr9-jCbyNL=Q5nLV628JQ@mail.gmail.com>

Reversing the direction of an axis currently needs an explicit 'xlim'
such that diff(xlim) < 0 [I think].  Thus, the assumption to do a
"forward" plot when "leaving out" one element in 'xlim' by setting it
to a missing value is not too bad.

However, what about using +Inf and -Inf as instead?  Disclaimer: I
haven't bumped into one, but it could be that there plot functions
that include -Inf/+Inf.  Example:

## Case #1: Infer 2nd argument in 'xlim' from data
# (a) Forward
plot(x, xlim=c(0,+Inf))
# (b) Reverse
plot(x, xlim=c(0,-Inf))

## Case #2: Infer 1st argument in 'xlim' from data
# (a) Forward
plot(x, xlim=c(-Inf,0))
# (b) Reverse
plot(x, xlim=c(+Inf,0))

## Case #3: Infer both arguments in 'xlim' from data
# (a) Forward
plot(x, xlim=c(-Inf,+Inf))
# (b) Reverse
plot(x, xlim=c(+Inf,-Inf))

Note how the latter also supports a use case currently not supported.

/Henrik


On Mon, Apr 16, 2012 at 12:15 PM, Duncan Murdoch
<murdoch.duncan at gmail.com> wrote:
> On 12-04-16 2:26 PM, William Dunlap wrote:
>>
>> plot(1:10, xlim=c(10,1)) reverses the x axis.
>> If we allow plot(1:10, xlim=c(5,NA)), which
>> direction should it go? ? ?Would this require new
>> parameters, {x,y}{min,max} or new paremeters
>> {x,y}{axisDirection}?
>
>
> I'd rather not add another parameter. ?So if I were to implement this, I'd
> probably choose Paul's original suggestion. ?If someone wants c(5, NA) to
> mean c(5, min(data)) rather than c(5, max(data)) they'd need to code it
> explicitly.
>
> Duncan Murdoch
>
>
>>
>> Bill Dunlap
>> Spotfire, TIBCO Software
>> wdunlap tibco.com
>>
>>
>>> -----Original Message-----
>>> From: r-devel-bounces at r-project.org
>>> [mailto:r-devel-bounces at r-project.org] On Behalf
>>> Of Duncan Murdoch
>>> Sent: Monday, April 16, 2012 11:14 AM
>>> To: Paul Johnson
>>> Cc: R Devel List
>>> Subject: Re: [Rd] I wish xlim=c(0, NA) would work. How about I send you a
>>> patch?
>>>
>>> On 12-04-16 1:52 PM, Paul Johnson wrote:
>>>>
>>>> I'm looking for an R mentor. ?I want to propose a change in management
>>>> of plot options xlim and ylim.
>>>
>>>
>>> Your suggestion sounds reasonable, but because plot limits are such a
>>> commonly used parameter, it would have to be done quite carefully. ?The
>>> questions I'd ask before implementing it are:
>>>
>>> ? - Are there other locations besides plot.default where xlim and ylim
>>> are specified? ?I'd like to have them updated consistently.
>>>
>>> ? - Are there any conflicting uses of NA for a limit in published
>>> packages?
>>>
>>> ? - Which package authors would need to be told about this change, so
>>> they could make a compatible change?
>>>
>>> ? - Where does it need to be documented?
>>>
>>> Duncan Murdoch
>>>
>>>>
>>>> Did you ever want to change one coordinate in xlim or ylim? It happens
>>>> to me all the time.
>>>>
>>>> x<- rnorm(100, m=5, s=1)
>>>> y<- rnorm(100, m=6, s=1)
>>>> plot(x,y)
>>>>
>>>> ## Oh, I want the "y axis" to show above x=0.
>>>>
>>>> plot(x,y, xlim=c(0, ))
>>>>
>>>> ##Output: Error in c(0, ) : argument 2 is empty
>>>>
>>>> ? plot(x,y, xlim=c(0,NA ))
>>>> ## Output: Error in plot.window(...) : need finite 'xlim' values
>>>>
>>>>
>>>> I wish that plot would let me supply just the min (or max) and then
>>>> calculate the other value where needed.
>>>> It is a little bit tedious for the user to do that for herself. ?The
>>>> user must be knowledgeable enough to know where the maximum (MAX) is
>>>> supposed to be, and then supply xlim=c(0, MAX). I can't see any reason
>>>> for insisting users have that deeper understanding of how R calculates
>>>> ranges for plots.
>>>>
>>>> Suppose the user is allowed to supply NA to signal R should fill in the
>>>> blanks.
>>>>
>>>> plot(x,y, xlim=c(0, NA))
>>>>
>>>>
>>>> In plot.default now, I find this code to manage xlim
>>>>
>>>> ? ? xlim<- if (is.null(xlim))
>>>> ? ? ? ? ?range(xy$x[is.finite(xy$x)])
>>>>
>>>> And I would change it to be something like
>>>> ? ? ##get default range
>>>> ? ? nxlim<- range(xy$x[is.finite(xy$x)])
>>>>
>>>> ? ? ## if xlim is NULL, so same as current
>>>> ? ? ?xlim<- if (is.null(xlim)) nxlim
>>>> ## Otherwise, replace NAs in xlim with values from nxlim
>>>> ? ? ?else xlim[ is.na(xlim) ]<- nxlim[ is.na(xlim) ]
>>>>
>>>>
>>>> Who is the responsible party for plot.default. ?How about it?
>>>>
>>>> Think of how much happier users would be!
>>>>
>>>> pj
>>>
>>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From 538280 at gmail.com  Mon Apr 16 22:20:17 2012
From: 538280 at gmail.com (Greg Snow)
Date: Mon, 16 Apr 2012 14:20:17 -0600
Subject: [Rd] I wish xlim=c(0,
	NA) would work. How about I send you a patch?
In-Reply-To: <CAErODj9RJ60Xdp=sLu=pGRiBOY_CSsxnjUmqObxhHYtEhHyLxA@mail.gmail.com>
References: <CAErODj9RJ60Xdp=sLu=pGRiBOY_CSsxnjUmqObxhHYtEhHyLxA@mail.gmail.com>
Message-ID: <CAFEqCdzg13nc_HvuKCA0xiS1F=YqKKvz4VKXn8_4cfqUmGABNg@mail.gmail.com>

The simple work around is to use the range function, if you use
something like:  xlim=range(0,x) then 0 will be included in the range
of the x axis (and if there are values less than 0 then those values
will be included as well) and the max is computed from the data as
usual.  The range function will also accept multiple vectors and make
the range big enough to include all of them on the plot (this is what
I use when I will be adding additional information using points or
lines).

With this functionality in range I don't really see much need for the
proposed change, maybe an example on the plot help page to show this
would suffice.

On Mon, Apr 16, 2012 at 11:52 AM, Paul Johnson <pauljohn32 at gmail.com> wrote:
> I'm looking for an R mentor. ?I want to propose a change in management
> of plot options xlim and ylim.
>
> Did you ever want to change one coordinate in xlim or ylim? It happens
> to me all the time.
>
> x <- rnorm(100, m=5, s=1)
> y <- rnorm(100, m=6, s=1)
> plot(x,y)
>
> ## Oh, I want the "y axis" to show above x=0.
>
> plot(x,y, xlim=c(0, ))
>
> ##Output: Error in c(0, ) : argument 2 is empty
>
> ?plot(x,y, xlim=c(0,NA ))
> ## Output: Error in plot.window(...) : need finite 'xlim' values
>
>
> I wish that plot would let me supply just the min (or max) and then
> calculate the other value where needed.
> It is a little bit tedious for the user to do that for herself. ?The
> user must be knowledgeable enough to know where the maximum (MAX) is
> supposed to be, and then supply xlim=c(0, MAX). I can't see any reason
> for insisting users have that deeper understanding of how R calculates
> ranges for plots.
>
> Suppose the user is allowed to supply NA to signal R should fill in the blanks.
>
> plot(x,y, xlim=c(0, NA))
>
>
> In plot.default now, I find this code to manage xlim
>
> ? xlim <- if (is.null(xlim))
> ? ? ? ?range(xy$x[is.finite(xy$x)])
>
> And I would change it to be something like
> ? ##get default range
> ? nxlim <- range(xy$x[is.finite(xy$x)])
>
> ? ## if xlim is NULL, so same as current
> ? ?xlim <- if (is.null(xlim)) nxlim
> ## Otherwise, replace NAs in xlim with values from nxlim
> ? ?else xlim[ is.na(xlim) ] <- nxlim[ is.na(xlim) ]
>
>
> Who is the responsible party for plot.default. ?How about it?
>
> Think of how much happier users would be!
>
> pj
> --
> Paul E. Johnson
> Professor, Political Science ? ?Assoc. Director
> 1541 Lilac Lane, Room 504 ? ? Center for Research Methods
> University of Kansas ? ? ? ? ? ? ? University of Kansas
> http://pj.freefaculty.org ? ? ? ? ? ?http://quant.ku.edu
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel



-- 
Gregory (Greg) L. Snow Ph.D.
538280 at gmail.com


From baptiste.auguie at googlemail.com  Mon Apr 16 22:29:20 2012
From: baptiste.auguie at googlemail.com (baptiste auguie)
Date: Tue, 17 Apr 2012 08:29:20 +1200
Subject: [Rd] I wish xlim=c(0,
	NA) would work. How about I send you a patch?
In-Reply-To: <CAFEqCdzg13nc_HvuKCA0xiS1F=YqKKvz4VKXn8_4cfqUmGABNg@mail.gmail.com>
References: <CAErODj9RJ60Xdp=sLu=pGRiBOY_CSsxnjUmqObxhHYtEhHyLxA@mail.gmail.com>
	<CAFEqCdzg13nc_HvuKCA0xiS1F=YqKKvz4VKXn8_4cfqUmGABNg@mail.gmail.com>
Message-ID: <CANLFJPoS19HtDsfvqjScOXToUqRkUgOcaM6+wvRA-ZqkZ8QU4A@mail.gmail.com>

Hi,

Using range wouldn't help if you wanted to restrict one of the limits,
not stretch it

plot(1:11, y <- seq(-5, 5), ylim= range(0, y))

baptiste

On 17 April 2012 08:20, Greg Snow <538280 at gmail.com> wrote:
> The simple work around is to use the range function, if you use
> something like: ?xlim=range(0,x) then 0 will be included in the range
> of the x axis (and if there are values less than 0 then those values
> will be included as well) and the max is computed from the data as
> usual. ?The range function will also accept multiple vectors and make
> the range big enough to include all of them on the plot (this is what
> I use when I will be adding additional information using points or
> lines).
>
> With this functionality in range I don't really see much need for the
> proposed change, maybe an example on the plot help page to show this
> would suffice.
>
> On Mon, Apr 16, 2012 at 11:52 AM, Paul Johnson <pauljohn32 at gmail.com> wrote:
>> I'm looking for an R mentor. ?I want to propose a change in management
>> of plot options xlim and ylim.
>>
>> Did you ever want to change one coordinate in xlim or ylim? It happens
>> to me all the time.
>>
>> x <- rnorm(100, m=5, s=1)
>> y <- rnorm(100, m=6, s=1)
>> plot(x,y)
>>
>> ## Oh, I want the "y axis" to show above x=0.
>>
>> plot(x,y, xlim=c(0, ))
>>
>> ##Output: Error in c(0, ) : argument 2 is empty
>>
>> ?plot(x,y, xlim=c(0,NA ))
>> ## Output: Error in plot.window(...) : need finite 'xlim' values
>>
>>
>> I wish that plot would let me supply just the min (or max) and then
>> calculate the other value where needed.
>> It is a little bit tedious for the user to do that for herself. ?The
>> user must be knowledgeable enough to know where the maximum (MAX) is
>> supposed to be, and then supply xlim=c(0, MAX). I can't see any reason
>> for insisting users have that deeper understanding of how R calculates
>> ranges for plots.
>>
>> Suppose the user is allowed to supply NA to signal R should fill in the blanks.
>>
>> plot(x,y, xlim=c(0, NA))
>>
>>
>> In plot.default now, I find this code to manage xlim
>>
>> ? xlim <- if (is.null(xlim))
>> ? ? ? ?range(xy$x[is.finite(xy$x)])
>>
>> And I would change it to be something like
>> ? ##get default range
>> ? nxlim <- range(xy$x[is.finite(xy$x)])
>>
>> ? ## if xlim is NULL, so same as current
>> ? ?xlim <- if (is.null(xlim)) nxlim
>> ## Otherwise, replace NAs in xlim with values from nxlim
>> ? ?else xlim[ is.na(xlim) ] <- nxlim[ is.na(xlim) ]
>>
>>
>> Who is the responsible party for plot.default. ?How about it?
>>
>> Think of how much happier users would be!
>>
>> pj
>> --
>> Paul E. Johnson
>> Professor, Political Science ? ?Assoc. Director
>> 1541 Lilac Lane, Room 504 ? ? Center for Research Methods
>> University of Kansas ? ? ? ? ? ? ? University of Kansas
>> http://pj.freefaculty.org ? ? ? ? ? ?http://quant.ku.edu
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>
>
>
> --
> Gregory (Greg) L. Snow Ph.D.
> 538280 at gmail.com
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From kasperdanielhansen at gmail.com  Mon Apr 16 22:43:15 2012
From: kasperdanielhansen at gmail.com (Kasper Daniel Hansen)
Date: Mon, 16 Apr 2012 16:43:15 -0400
Subject: [Rd] I wish xlim=c(0,
	NA) would work. How about I send you a patch?
In-Reply-To: <CANLFJPoS19HtDsfvqjScOXToUqRkUgOcaM6+wvRA-ZqkZ8QU4A@mail.gmail.com>
References: <CAErODj9RJ60Xdp=sLu=pGRiBOY_CSsxnjUmqObxhHYtEhHyLxA@mail.gmail.com>
	<CAFEqCdzg13nc_HvuKCA0xiS1F=YqKKvz4VKXn8_4cfqUmGABNg@mail.gmail.com>
	<CANLFJPoS19HtDsfvqjScOXToUqRkUgOcaM6+wvRA-ZqkZ8QU4A@mail.gmail.com>
Message-ID: <CAC2h7uu2f9+Sp=Hz-pj7hPCFRXQmBVm8x+CKJC6wg2iapZFkzg@mail.gmail.com>

On Mon, Apr 16, 2012 at 4:29 PM, baptiste auguie
<baptiste.auguie at googlemail.com> wrote:
> Hi,
>
> Using range wouldn't help if you wanted to restrict one of the limits,
> not stretch it
>
> plot(1:11, y <- seq(-5, 5), ylim= range(0, y))

range(pmin(0,y))

Kasper


>
> baptiste
>
> On 17 April 2012 08:20, Greg Snow <538280 at gmail.com> wrote:
>> The simple work around is to use the range function, if you use
>> something like: ?xlim=range(0,x) then 0 will be included in the range
>> of the x axis (and if there are values less than 0 then those values
>> will be included as well) and the max is computed from the data as
>> usual. ?The range function will also accept multiple vectors and make
>> the range big enough to include all of them on the plot (this is what
>> I use when I will be adding additional information using points or
>> lines).
>>
>> With this functionality in range I don't really see much need for the
>> proposed change, maybe an example on the plot help page to show this
>> would suffice.
>>
>> On Mon, Apr 16, 2012 at 11:52 AM, Paul Johnson <pauljohn32 at gmail.com> wrote:
>>> I'm looking for an R mentor. ?I want to propose a change in management
>>> of plot options xlim and ylim.
>>>
>>> Did you ever want to change one coordinate in xlim or ylim? It happens
>>> to me all the time.
>>>
>>> x <- rnorm(100, m=5, s=1)
>>> y <- rnorm(100, m=6, s=1)
>>> plot(x,y)
>>>
>>> ## Oh, I want the "y axis" to show above x=0.
>>>
>>> plot(x,y, xlim=c(0, ))
>>>
>>> ##Output: Error in c(0, ) : argument 2 is empty
>>>
>>> ?plot(x,y, xlim=c(0,NA ))
>>> ## Output: Error in plot.window(...) : need finite 'xlim' values
>>>
>>>
>>> I wish that plot would let me supply just the min (or max) and then
>>> calculate the other value where needed.
>>> It is a little bit tedious for the user to do that for herself. ?The
>>> user must be knowledgeable enough to know where the maximum (MAX) is
>>> supposed to be, and then supply xlim=c(0, MAX). I can't see any reason
>>> for insisting users have that deeper understanding of how R calculates
>>> ranges for plots.
>>>
>>> Suppose the user is allowed to supply NA to signal R should fill in the blanks.
>>>
>>> plot(x,y, xlim=c(0, NA))
>>>
>>>
>>> In plot.default now, I find this code to manage xlim
>>>
>>> ? xlim <- if (is.null(xlim))
>>> ? ? ? ?range(xy$x[is.finite(xy$x)])
>>>
>>> And I would change it to be something like
>>> ? ##get default range
>>> ? nxlim <- range(xy$x[is.finite(xy$x)])
>>>
>>> ? ## if xlim is NULL, so same as current
>>> ? ?xlim <- if (is.null(xlim)) nxlim
>>> ## Otherwise, replace NAs in xlim with values from nxlim
>>> ? ?else xlim[ is.na(xlim) ] <- nxlim[ is.na(xlim) ]
>>>
>>>
>>> Who is the responsible party for plot.default. ?How about it?
>>>
>>> Think of how much happier users would be!
>>>
>>> pj
>>> --
>>> Paul E. Johnson
>>> Professor, Political Science ? ?Assoc. Director
>>> 1541 Lilac Lane, Room 504 ? ? Center for Research Methods
>>> University of Kansas ? ? ? ? ? ? ? University of Kansas
>>> http://pj.freefaculty.org ? ? ? ? ? ?http://quant.ku.edu
>>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>>
>>
>> --
>> Gregory (Greg) L. Snow Ph.D.
>> 538280 at gmail.com
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From armstrong.whit at gmail.com  Mon Apr 16 23:53:02 2012
From: armstrong.whit at gmail.com (Whit Armstrong)
Date: Mon, 16 Apr 2012 17:53:02 -0400
Subject: [Rd] eval a SYMSXP from C
Message-ID: <CAMi=pg6-q5EOU1YHXiSqyjCvQ5vGtNNU2P8xjpr9sDB21nvzJw@mail.gmail.com>

Can someone offer some advice on how to properly evaluate a SYMSXP
from a .Call ?

I have the following in R:

variable xn, with an attribute "mu" which references the variable mu
in the global environment.

I know "references" is a loose term; mu was defined in this fashion as
a way to implement deferred binding:

foo <- function(x,mu) {
    attr(x,"mu") <- substitute(mu)
    x
}

mu <- 2.0
xn <- foo(rnorm(100),mu)

> typeof(attr(xn,"mu"))
[1] "symbol"
> eval(attr(xn,"mu"))
[1] 2
>

In a .Call, I am attempting to eval the SYMSXP as follows:

SEXP mu_ = Rf_getAttrib(x_,Rf_install("mu"));

if(TYPEOF(mu_)==SYMSXP) {
  mu_ = Rf_eval(Rf_lang1(mu_),R_GlobalEnv);
}

However, when running this code, I get the following error:
Error in logp(xn) : could not find function "mu"

Do I need to create an expression of c("get", "mu") to force the name
lookup to evaluate the SYMSXP?

Thanks,
Whit


From murdoch.duncan at gmail.com  Tue Apr 17 02:01:55 2012
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Mon, 16 Apr 2012 20:01:55 -0400
Subject: [Rd] eval a SYMSXP from C
In-Reply-To: <CAMi=pg6-q5EOU1YHXiSqyjCvQ5vGtNNU2P8xjpr9sDB21nvzJw@mail.gmail.com>
References: <CAMi=pg6-q5EOU1YHXiSqyjCvQ5vGtNNU2P8xjpr9sDB21nvzJw@mail.gmail.com>
Message-ID: <4F8CB2F3.8060802@gmail.com>

On 12-04-16 5:53 PM, Whit Armstrong wrote:
> Can someone offer some advice on how to properly evaluate a SYMSXP
> from a .Call ?
>
> I have the following in R:
>
> variable xn, with an attribute "mu" which references the variable mu
> in the global environment.
>
> I know "references" is a loose term; mu was defined in this fashion as
> a way to implement deferred binding:
>
> foo<- function(x,mu) {
>      attr(x,"mu")<- substitute(mu)
>      x
> }
>
> mu<- 2.0
> xn<- foo(rnorm(100),mu)
>
>> typeof(attr(xn,"mu"))
> [1] "symbol"
>> eval(attr(xn,"mu"))
> [1] 2
>>
>
> In a .Call, I am attempting to eval the SYMSXP as follows:
>
> SEXP mu_ = Rf_getAttrib(x_,Rf_install("mu"));
>
> if(TYPEOF(mu_)==SYMSXP) {
>    mu_ = Rf_eval(Rf_lang1(mu_),R_GlobalEnv);
> }
>
> However, when running this code, I get the following error:
> Error in logp(xn) : could not find function "mu"
>
> Do I need to create an expression of c("get", "mu") to force the name
> lookup to evaluate the SYMSXP?
>

Rf_lang1(mu_) will produce mu(), not mu.  I think you just want to 
evaluate mu_.

There are lots of possible types of object returned by substitute(mu), 
but you should be able to evaluate all of them.  You probably don't want 
to evaluate them in R_GlobalEnv, you want to evaluate them in the 
environment where the call was made to foo, so that you'll get local 
variables if it was called from a function.

Duncan Murdoch


From armstrong.whit at gmail.com  Tue Apr 17 02:48:32 2012
From: armstrong.whit at gmail.com (Whit Armstrong)
Date: Mon, 16 Apr 2012 20:48:32 -0400
Subject: [Rd] eval a SYMSXP from C
In-Reply-To: <4F8CB2F3.8060802@gmail.com>
References: <CAMi=pg6-q5EOU1YHXiSqyjCvQ5vGtNNU2P8xjpr9sDB21nvzJw@mail.gmail.com>
	<4F8CB2F3.8060802@gmail.com>
Message-ID: <CAMi=pg6k0YmrOJxTjRM9atan2siOYfPZ8Y54Sj8AU74MC9ypwQ@mail.gmail.com>

Thanks, Duncan.

Your suggestion works.

And thanks for the hint, about the env.  I suppose I should preserve
the env of the original function where the substitute was called...

Cheers,
Whit


On Mon, Apr 16, 2012 at 8:01 PM, Duncan Murdoch
<murdoch.duncan at gmail.com> wrote:
> On 12-04-16 5:53 PM, Whit Armstrong wrote:
>>
>> Can someone offer some advice on how to properly evaluate a SYMSXP
>> from a .Call ?
>>
>> I have the following in R:
>>
>> variable xn, with an attribute "mu" which references the variable mu
>> in the global environment.
>>
>> I know "references" is a loose term; mu was defined in this fashion as
>> a way to implement deferred binding:
>>
>> foo<- function(x,mu) {
>> ? ? attr(x,"mu")<- substitute(mu)
>> ? ? x
>> }
>>
>> mu<- 2.0
>> xn<- foo(rnorm(100),mu)
>>
>>> typeof(attr(xn,"mu"))
>>
>> [1] "symbol"
>>>
>>> eval(attr(xn,"mu"))
>>
>> [1] 2
>>>
>>>
>>
>> In a .Call, I am attempting to eval the SYMSXP as follows:
>>
>> SEXP mu_ = Rf_getAttrib(x_,Rf_install("mu"));
>>
>> if(TYPEOF(mu_)==SYMSXP) {
>> ? mu_ = Rf_eval(Rf_lang1(mu_),R_GlobalEnv);
>> }
>>
>> However, when running this code, I get the following error:
>> Error in logp(xn) : could not find function "mu"
>>
>> Do I need to create an expression of c("get", "mu") to force the name
>> lookup to evaluate the SYMSXP?
>>
>
> Rf_lang1(mu_) will produce mu(), not mu. ?I think you just want to evaluate
> mu_.
>
> There are lots of possible types of object returned by substitute(mu), but
> you should be able to evaluate all of them. ?You probably don't want to
> evaluate them in R_GlobalEnv, you want to evaluate them in the environment
> where the call was made to foo, so that you'll get local variables if it was
> called from a function.
>
> Duncan Murdoch


From hb at biostat.ucsf.edu  Tue Apr 17 08:19:08 2012
From: hb at biostat.ucsf.edu (Henrik Bengtsson)
Date: Mon, 16 Apr 2012 23:19:08 -0700
Subject: [Rd] R --debug crashes R on Windows
Message-ID: <CAFDcVCT2-KZDFcEqXOGz-VN-kVszhGc7JV-fXS-62JL3XD_enw@mail.gmail.com>

'R --debug' and 'Rterm --debug' crash R on Windows.  Indeed, --debug
is not a documented command line option, but it still causes a crash.

This occurs with R v2.15.0 patched and R devel - both from 2012-04-09 (r58947).

/Henrik


From renaud at mancala.cbio.uct.ac.za  Tue Apr 17 10:01:18 2012
From: renaud at mancala.cbio.uct.ac.za (Renaud Gaujoux)
Date: Tue, 17 Apr 2012 10:01:18 +0200
Subject: [Rd] url, readLines, source behind a proxy
Message-ID: <4F8D234E.1070709@cbio.uct.ac.za>

Hi,

when I run R CMD check with flag --as-cran, the process hangs at stage:

* checking CRAN incoming feasibility ...

I am pretty sure it is a proxy issue.
I looked at the check code in the tools package and it seems that the 
issue is in the local function `.repository_db()` (defined in 
`tools:::.check_package_CRAN_incoming()`), which eventually calls 
`url()` with argument open="rb", that hangs probably because it does not 
use the proxy settings.
I had a similar issue with `source()`, which apparently uses internal 
network functions (not as download.file), but is supposed to work behind 
a proxy (correct?).
Does anybody else have this problem?

I was wondering if there is a way around, as I would like to be able to 
use --as-cran for my checks.
Thank you.

Renaud

-- 
Renaud Gaujoux
Computational Biology - University of Cape Town
South Africa


From martin.becker at mx.uni-saarland.de  Tue Apr 17 12:13:45 2012
From: martin.becker at mx.uni-saarland.de (Martin Becker)
Date: Tue, 17 Apr 2012 12:13:45 +0200
Subject: [Rd] Minor bug: plot.table and Axis.table (partially) ignore
 graphical parameters (patch included)
Message-ID: <4F8D4259.3060004@mx.uni-saarland.de>

Dear developers,

currently (rev 59060), plot.table and Axis.table do not forward their 
'...' argument to their calls to axis(). Thus, some graphical parameters 
(such as col.axis, cex.axis, font.axis) in '...' are ignored (for 
plot.table: partially ignored [for the x-axis]), which seems to be a 
minor bug. As a minimal reproducible example, see e.g.:

   plot(table(rbinom(100,5,0.5)), col.axis="red", cex.axis=2, font.axis=2)

Apparently, this behaviour is mainly caused by a suboptimal 
implementation of (undocumented) support for the logical parameter 
'axes' in plot.table's as well as Axis.table's '...' argument.

I propose to change the implementation as follows:
- make 'axes' an explicit argument of plot.table (as in plot.default, 
with default TRUE)
- drop support for the 'axes' flag in Axis.table's '...' argument (which 
is neither documented nor supported by other Axis.* functions AFAICS)
- in plot.table and Axis.table, forward '...' to axis() (in plot.table: 
"filtered" via a localaxis()-function similar to localAxis() in 
plot.default)

This would remove the minor bug and simplify the code. As a downside, 
the change breaks code which uses 'axes' as '...'-argument in a call to 
Axis.table(). But, as already mentioned, this is neither documented nor 
working for other methods of Axis.

So, please consider applying the attached (not fully tested, but 
trivial) patch.

Thank you and best wishes,
Martin

-- 
Dr. Martin Becker, Akad. Rat
Statistics and Econometrics
Saarland University
Campus C3 1, Room 217
66123 Saarbruecken
Germany

-------------- next part --------------
diff -rupN rev59060/trunk/src/library/graphics/R/axis.R patched/trunk/src/library/graphics/R/axis.R
--- rev59060/trunk/src/library/graphics/R/axis.R	2012-04-17 10:18:28.054985800 +0200
+++ patched/trunk/src/library/graphics/R/axis.R	2012-04-17 10:22:01.636201900 +0200
@@ -54,10 +54,7 @@ Axis.table <- function(x, at, ..., side
          x0 <- if (is.num) xx else seq.int(x)
          if(missing(at)) at <- x0
          if(missing(labels)) labels <- nx
-         xaxt <- if (length(as <- list(...))) {
-             if (!is.null(as$axes) && !as$axes) "n" else as$xaxt
-         }## else NULL
-         axis(side, at = at, labels = labels, xaxt = xaxt)
+         axis(side, at = at, labels = labels, ...)
      }
      else stop("only for 1-D table")
 }
diff -rupN rev59060/trunk/src/library/graphics/R/plot.R patched/trunk/src/library/graphics/R/plot.R
--- rev59060/trunk/src/library/graphics/R/plot.R	2012-04-17 10:19:16.492756300 +0200
+++ patched/trunk/src/library/graphics/R/plot.R	2012-04-17 12:01:06.404223200 +0200
@@ -105,8 +105,9 @@ plot.factor <- function(x, y, legend.tex
 ##   - if "h", make the default lwd depend on number of classes instead of lwd=2
 plot.table <-
     function(x, type = "h", ylim = c(0, max(x)), lwd = 2,
-             xlab = NULL, ylab = NULL, frame.plot = is.num, ...)
+             xlab = NULL, ylab = NULL, frame.plot = is.num, axes = TRUE, ...)
 {
+    localaxis <- function(..., col, bg, pch, cex, lty, lwd) axis(...)
     xnam <- deparse(substitute(x))
     rnk <- length(dim(x))
     if(rnk == 0L) stop("invalid table 'x'")
@@ -121,11 +122,7 @@ plot.table <-
 	plot(x0, unclass(x), type = type,
 	     ylim = ylim, xlab = xlab, ylab = ylab, frame.plot = frame.plot,
 	     lwd = lwd, ..., xaxt = "n")
-	xaxt <-
-	    if(length(as <- list(...))) {
-		if(!is.null(as$axes) && !as$axes) "n" else as$xaxt
-	    }## else NULL
-	axis(1, at = x0, labels = nx, xaxt = xaxt)
+	if (axes) localaxis(1, at = x0, labels = nx, ...)
     } else {
 	if(length(as <- list(...)) && !is.null(as$main)) # use 'main'
 	    mosaicplot(x, xlab = xlab, ylab = ylab, ...)
diff -rupN rev59060/trunk/src/library/graphics/man/plot.table.Rd patched/trunk/src/library/graphics/man/plot.table.Rd
--- rev59060/trunk/src/library/graphics/man/plot.table.Rd	2012-04-17 10:47:26.492418700 +0200
+++ patched/trunk/src/library/graphics/man/plot.table.Rd	2012-04-17 10:48:53.812413100 +0200
@@ -16,7 +16,7 @@
 }
 \usage{
 \method{plot}{table}(x, type = "h", ylim = c(0, max(x)), lwd = 2,
-     xlab = NULL, ylab = NULL, frame.plot = is.num, \dots)
+     xlab = NULL, ylab = NULL, frame.plot = is.num, axes = TRUE, \dots)
 \method{points}{table}(x, y = NULL, type = "h", lwd = 2, \dots)
 \method{lines}{table}(x, y = NULL, type = "h", lwd = 2, \dots)
 }
@@ -30,6 +30,9 @@
   \item{frame.plot}{logical indicating if a frame (\code{\link{box}})
     should be drawn in the 1D case.  Defaults to true when \code{x} has
     \code{\link{dimnames}} coerce-able to numbers.}
+  \item{axes}{a logical value indicating whether both axes should be drawn on
+    the plot.  Use \link{graphical parameter} \code{"xaxt"} or \code{"yaxt"}
+    to suppress just one of the axes.}
   \item{\dots}{further graphical arguments, see \code{\link{plot.default}}.}
 }
 \seealso{

From murdoch.duncan at gmail.com  Tue Apr 17 12:43:03 2012
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Tue, 17 Apr 2012 06:43:03 -0400
Subject: [Rd] R --debug crashes R on Windows
In-Reply-To: <CAFDcVCT2-KZDFcEqXOGz-VN-kVszhGc7JV-fXS-62JL3XD_enw@mail.gmail.com>
References: <CAFDcVCT2-KZDFcEqXOGz-VN-kVszhGc7JV-fXS-62JL3XD_enw@mail.gmail.com>
Message-ID: <4F8D4937.5060103@gmail.com>

On 12-04-17 2:19 AM, Henrik Bengtsson wrote:
> 'R --debug' and 'Rterm --debug' crash R on Windows.  Indeed, --debug
> is not a documented command line option, but it still causes a crash.
>
> This occurs with R v2.15.0 patched and R devel - both from 2012-04-09 (r58947).

What do you mean by "crash"?  For me, they do nothing, i.e. both 
commands just exit immediately.  What the source shows they are trying 
to do is break to a debugger upon startup, but if there's no debugger 
there, it might look like a crash.

Duncan Murdoch


From hb at biostat.ucsf.edu  Tue Apr 17 20:38:56 2012
From: hb at biostat.ucsf.edu (Henrik Bengtsson)
Date: Tue, 17 Apr 2012 11:38:56 -0700
Subject: [Rd] url, readLines, source behind a proxy
In-Reply-To: <4F8D234E.1070709@cbio.uct.ac.za>
References: <4F8D234E.1070709@cbio.uct.ac.za>
Message-ID: <CAFDcVCQx1R7gJYnOPC3yzayJxi-Ts9zzaQd5T0bFSnLcOv8nCg@mail.gmail.com>

On Tue, Apr 17, 2012 at 1:01 AM, Renaud Gaujoux
<renaud at mancala.cbio.uct.ac.za> wrote:
> Hi,
>
> when I run R CMD check with flag --as-cran, the process hangs at stage:
>
> * checking CRAN incoming feasibility ...

Doesn't it time-out eventually?  I'm not behind a proxy but when I've
been running 'R CMD check' whenon very poor 3G connection, it had
eventually timed out.

/Henrik

>
> I am pretty sure it is a proxy issue.
> I looked at the check code in the tools package and it seems that the issue
> is in the local function `.repository_db()` (defined in
> `tools:::.check_package_CRAN_incoming()`), which eventually calls `url()`
> with argument open="rb", that hangs probably because it does not use the
> proxy settings.
> I had a similar issue with `source()`, which apparently uses internal
> network functions (not as download.file), but is supposed to work behind a
> proxy (correct?).
> Does anybody else have this problem?
>
> I was wondering if there is a way around, as I would like to be able to use
> --as-cran for my checks.
> Thank you.
>
> Renaud
>
> --
> Renaud Gaujoux
> Computational Biology - University of Cape Town
> South Africa
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From hb at biostat.ucsf.edu  Tue Apr 17 21:01:24 2012
From: hb at biostat.ucsf.edu (Henrik Bengtsson)
Date: Tue, 17 Apr 2012 12:01:24 -0700
Subject: [Rd] R --debug crashes R on Windows
In-Reply-To: <4F8D4937.5060103@gmail.com>
References: <CAFDcVCT2-KZDFcEqXOGz-VN-kVszhGc7JV-fXS-62JL3XD_enw@mail.gmail.com>
	<4F8D4937.5060103@gmail.com>
Message-ID: <CAFDcVCT+iu=NThsJyiV8GQyrqJyKSPsffGbeHtBDVtO6NsRJzw@mail.gmail.com>

I really meant crash and left my report short thinking others would be
able to reproduce this immediately.  Details:

* Launching 'Rterm --debug' from the Windows Command prompt gives me a
"R for Windows terminal front-end has stopped working: A problem
caused the ... [Close program]" dialog without any kind of output from
R itself.
* It happens with both 'i386\Rterm.exe --debug' and 'x64\Rterm.exe --debug'
* When doing 'Rgui.exe --debug', there is no message (nor an error
message/dialog) and it exits immediately.
(* Trying random non-existing command line options (with R, Rterm and
Rgui) are properly ignored and gives a warning, e.g. WARNING: unknown
option '--dror' when launching R.)
* It happens with R v2.10.0, R v2.14.2, R v2.15.0patched, and R devel
(binaries from CRAN).   On R v2.13.0, it *hangs* (!=exit) without the
dialog and it responds to Ctrl-C.
* Trying to use WinDbg, I get the following from the core dump:

Microsoft (R) Windows Debugger Version 6.12.0002.633 AMD64
Copyright (c) Microsoft Corporation. All rights reserved.

Loading Dump File [C:\Users\hb\AppData\Local\CrashDumps\Rterm.exe.6212.dmp]
User Mini Dump File: Only registers, stack and portions of memory are available

Symbol search path is:
SRV*C:\SymCache*http://msdl.microsoft.com/download/symbols
Executable search path is:
Windows 7 Version 7601 (Service Pack 1) MP (4 procs) Free x64
Product: WinNt, suite: SingleUserTS
Machine Name:
Debug session time: Tue Apr 17 11:50:48.000 2012 (UTC - 7:00)
System Uptime: not available
Process Uptime: 0 days 0:00:16.000
............................
This dump file has a breakpoint exception stored in it.
The stored exception information can be accessed via .ecxr.
ntdll!ZwWaitForMultipleObjects+0xa:
00000000`77a618ca c3              ret

I don't find this particularly important, but wanted to share in case
there is some deprecated.obsolete code in there for option '--debug'.
>From your reply it sounds as if --debug is intended, so ok with me.

/Henrik

On Tue, Apr 17, 2012 at 3:43 AM, Duncan Murdoch
<murdoch.duncan at gmail.com> wrote:
> On 12-04-17 2:19 AM, Henrik Bengtsson wrote:
>>
>> 'R --debug' and 'Rterm --debug' crash R on Windows. ?Indeed, --debug
>> is not a documented command line option, but it still causes a crash.
>>
>> This occurs with R v2.15.0 patched and R devel - both from 2012-04-09
>> (r58947).
>
>
> What do you mean by "crash"? ?For me, they do nothing, i.e. both commands
> just exit immediately. ?What the source shows they are trying to do is break
> to a debugger upon startup, but if there's no debugger there, it might look
> like a crash.
>
> Duncan Murdoch
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From kevin.r.coombes at gmail.com  Tue Apr 17 21:12:59 2012
From: kevin.r.coombes at gmail.com (Kevin R. Coombes)
Date: Tue, 17 Apr 2012 14:12:59 -0500
Subject: [Rd] R --debug crashes R on Windows
In-Reply-To: <CAFDcVCT+iu=NThsJyiV8GQyrqJyKSPsffGbeHtBDVtO6NsRJzw@mail.gmail.com>
References: <CAFDcVCT2-KZDFcEqXOGz-VN-kVszhGc7JV-fXS-62JL3XD_enw@mail.gmail.com>
	<4F8D4937.5060103@gmail.com>
	<CAFDcVCT+iu=NThsJyiV8GQyrqJyKSPsffGbeHtBDVtO6NsRJzw@mail.gmail.com>
Message-ID: <4F8DC0BB.9020408@gmail.com>

I can reproduce the "R for Windows terminal front-end has stopped 
working" with both R-2.14.1 and R-2.13.0 (i386), and get the "hang but 
not crash" with both of the x64 binaries for those versions.

On 4/17/2012 2:01 PM, Henrik Bengtsson wrote:
> I really meant crash and left my report short thinking others would be
> able to reproduce this immediately.  Details:
>
> * Launching 'Rterm --debug' from the Windows Command prompt gives me a
> "R for Windows terminal front-end has stopped working: A problem
> caused the ... [Close program]" dialog without any kind of output from
> R itself.
> * It happens with both 'i386\Rterm.exe --debug' and 'x64\Rterm.exe --debug'
> * When doing 'Rgui.exe --debug', there is no message (nor an error
> message/dialog) and it exits immediately.
> (* Trying random non-existing command line options (with R, Rterm and
> Rgui) are properly ignored and gives a warning, e.g. WARNING: unknown
> option '--dror' when launching R.)
> * It happens with R v2.10.0, R v2.14.2, R v2.15.0patched, and R devel
> (binaries from CRAN).   On R v2.13.0, it *hangs* (!=exit) without the
> dialog and it responds to Ctrl-C.
> * Trying to use WinDbg, I get the following from the core dump:
>
> Microsoft (R) Windows Debugger Version 6.12.0002.633 AMD64
> Copyright (c) Microsoft Corporation. All rights reserved.
>
> Loading Dump File [C:\Users\hb\AppData\Local\CrashDumps\Rterm.exe.6212.dmp]
> User Mini Dump File: Only registers, stack and portions of memory are available
>
> Symbol search path is:
> SRV*C:\SymCache*http://msdl.microsoft.com/download/symbols
> Executable search path is:
> Windows 7 Version 7601 (Service Pack 1) MP (4 procs) Free x64
> Product: WinNt, suite: SingleUserTS
> Machine Name:
> Debug session time: Tue Apr 17 11:50:48.000 2012 (UTC - 7:00)
> System Uptime: not available
> Process Uptime: 0 days 0:00:16.000
> ............................
> This dump file has a breakpoint exception stored in it.
> The stored exception information can be accessed via .ecxr.
> ntdll!ZwWaitForMultipleObjects+0xa:
> 00000000`77a618ca c3              ret
>
> I don't find this particularly important, but wanted to share in case
> there is some deprecated.obsolete code in there for option '--debug'.
> > From your reply it sounds as if --debug is intended, so ok with me.
>
> /Henrik
>
> On Tue, Apr 17, 2012 at 3:43 AM, Duncan Murdoch
> <murdoch.duncan at gmail.com>  wrote:
>> On 12-04-17 2:19 AM, Henrik Bengtsson wrote:
>>> 'R --debug' and 'Rterm --debug' crash R on Windows.  Indeed, --debug
>>> is not a documented command line option, but it still causes a crash.
>>>
>>> This occurs with R v2.15.0 patched and R devel - both from 2012-04-09
>>> (r58947).
>>
>> What do you mean by "crash"?  For me, they do nothing, i.e. both commands
>> just exit immediately.  What the source shows they are trying to do is break
>> to a debugger upon startup, but if there's no debugger there, it might look
>> like a crash.
>>
>> Duncan Murdoch
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From hb at biostat.ucsf.edu  Tue Apr 17 23:20:17 2012
From: hb at biostat.ucsf.edu (Henrik Bengtsson)
Date: Tue, 17 Apr 2012 14:20:17 -0700
Subject: [Rd] Setting up the NAMESPACE file "automatically" during R CMD
	build?
Message-ID: <CAFDcVCQOqXqR3LPAyPahWcPxj8nwK+5x_1AYsmSqjCKct6yraQ@mail.gmail.com>

Hi,

I'd like to compile the NAMESPACE file from a template where things
such as S3method() statements are automatically inserted based on code
inspection, because doing it by hand is too error prone.  I am
currently generating a NAMESPACE file this way which I copy to my
package root, but I'd like to automate this so it is done when
building the *.tar.gz source (during 'R CMD build'), such that is
there for building binaries and when installing the package.

What is the best way to do this?  I was thinking of utilizing a
<pkg>/configure script, but as far I understand "Writing R
Extensions", that is only run by 'R CMD INSTALL' not 'R CMD build'.
Am I asking the wrong question?

Thanks,

Henrik


From andre.zege at gmail.com  Tue Apr 17 23:24:36 2012
From: andre.zege at gmail.com (andre zege)
Date: Tue, 17 Apr 2012 17:24:36 -0400
Subject: [Rd] R-2.15 compile error: fatal error: internal consistency failure
Message-ID: <CACU3EkN08eYEvufjJBCWuG7o9E9D-qYjBMQQ8yNkgnQyTdvv0g@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20120417/24e1a734/attachment.pl>

From murdoch.duncan at gmail.com  Wed Apr 18 01:51:25 2012
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Tue, 17 Apr 2012 19:51:25 -0400
Subject: [Rd] R-2.15 compile error: fatal error: internal consistency
 failure
In-Reply-To: <CACU3EkN08eYEvufjJBCWuG7o9E9D-qYjBMQQ8yNkgnQyTdvv0g@mail.gmail.com>
References: <CACU3EkN08eYEvufjJBCWuG7o9E9D-qYjBMQQ8yNkgnQyTdvv0g@mail.gmail.com>
Message-ID: <4F8E01FD.1010807@gmail.com>

On 12-04-17 5:24 PM, andre zege wrote:
> I am unable to compile R-2.15.0 source. I configured it without problems
> with options that i used many times before
>
> ./configure --prefix=/home/andre/R-2.15.0
> --enable-byte-compiled-packages=no --with-tcltk --enable-R-shlib=yes
>
> Then when i started making it, it died while making lapack, particularly on
> the line
>
> gfortran  -fopenmp -fpic  -g -O2  -c dlapack3.f -o dlapack3.o
> dlapack3.f: In function ?dsbgst?:
> dlapack3.f:12097: fatal error: internal consistency failure
> compilation terminated.
> make[4]: *** [dlapack3.o] Error 1
>
> Could anyone give me a clue what is going wrong and how could i fix that? I
> am running Centos 5.5, in particular, the following
>
> $ more /proc/version
> Linux version 2.6.18-194.el5 (mockbuild at builder10.centos.org) (gcc version
> 4.1.2 20080704 (Red Hat 4.1.2-48)) #1 SMP Fri Apr 2 14:58:14 EDT 2010

That looks like a message from your compiler.  I think gcc 4.1.2 is 
fairly old (Windows builds are using gcc 4.6.3).  Perhaps it's time to 
upgrade.

Duncan Murdoch


From renaud at mancala.cbio.uct.ac.za  Wed Apr 18 07:22:14 2012
From: renaud at mancala.cbio.uct.ac.za (Renaud Gaujoux)
Date: Wed, 18 Apr 2012 07:22:14 +0200 (SAST)
Subject: [Rd] url, readLines, source behind a proxy
In-Reply-To: <CAFDcVCQx1R7gJYnOPC3yzayJxi-Ts9zzaQd5T0bFSnLcOv8nCg@mail.gmail.com>
References: <4F8D234E.1070709@cbio.uct.ac.za>
	<CAFDcVCQx1R7gJYnOPC3yzayJxi-Ts9zzaQd5T0bFSnLcOv8nCg@mail.gmail.com>
Message-ID: <Pine.LNX.4.64.1204180705060.27275@mancala.cbio.uct.ac.za>

Hi Henrik,

I am sure it would time out indeed. I am fine with time out due to poor 
connection, but get a bit frustrated if my connection is fine and timeout 
occurs because proxy settings are not read. All the more if I have to run 
R CMD check multiple times and wait for the timeout.
But the issue might also be coming from my proxy, which somehow does not 
like the way url() reads remote content.

Could anybody behind a proxy check if the issue can be reproduced?
My proxy is in fact provided by cntml, which acts as a local proxy that 
takes care of tricky authentication protocols with the actual university 
proxy, not natively supported by my system (Ubuntu). Anybody in this case?

Thanks.

Renaud 

On Tue, 17 Apr 2012, Henrik Bengtsson wrote:

> On Tue, Apr 17, 2012 at 1:01 AM, Renaud Gaujoux
> <renaud at mancala.cbio.uct.ac.za> wrote:
> > Hi,
> >
> > when I run R CMD check with flag --as-cran, the process hangs at stage:
> >
> > * checking CRAN incoming feasibility ...
> 
> Doesn't it time-out eventually?  I'm not behind a proxy but when I've
> been running 'R CMD check' whenon very poor 3G connection, it had
> eventually timed out.
> 
> /Henrik
> 
> >
> > I am pretty sure it is a proxy issue.
> > I looked at the check code in the tools package and it seems that the issue
> > is in the local function `.repository_db()` (defined in
> > `tools:::.check_package_CRAN_incoming()`), which eventually calls `url()`
> > with argument open="rb", that hangs probably because it does not use the
> > proxy settings.
> > I had a similar issue with `source()`, which apparently uses internal
> > network functions (not as download.file), but is supposed to work behind a
> > proxy (correct?).
> > Does anybody else have this problem?
> >
> > I was wondering if there is a way around, as I would like to be able to use
> > --as-cran for my checks.
> > Thank you.
> >
> > Renaud
> >
> > --
> > Renaud Gaujoux
> > Computational Biology - University of Cape Town
> > South Africa
> >
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
> 


From ripley at stats.ox.ac.uk  Wed Apr 18 09:19:07 2012
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 18 Apr 2012 08:19:07 +0100
Subject: [Rd] R-2.15 compile error: fatal error: internal consistency
 failure
In-Reply-To: <4F8E01FD.1010807@gmail.com>
References: <CACU3EkN08eYEvufjJBCWuG7o9E9D-qYjBMQQ8yNkgnQyTdvv0g@mail.gmail.com>
	<4F8E01FD.1010807@gmail.com>
Message-ID: <4F8E6AEB.1010206@stats.ox.ac.uk>

On 18/04/2012 00:51, Duncan Murdoch wrote:
> On 12-04-17 5:24 PM, andre zege wrote:
>> I am unable to compile R-2.15.0 source. I configured it without problems
>> with options that i used many times before
>>
>> ./configure --prefix=/home/andre/R-2.15.0
>> --enable-byte-compiled-packages=no --with-tcltk --enable-R-shlib=yes

>> Then when i started making it, it died while making lapack,
>> particularly on
>> the line
>>
>> gfortran -fopenmp -fpic -g -O2 -c dlapack3.f -o dlapack3.o
>> dlapack3.f: In function ?dsbgst?:
>> dlapack3.f:12097: fatal error: internal consistency failure
>> compilation terminated.
>> make[4]: *** [dlapack3.o] Error 1
>>
>> Could anyone give me a clue what is going wrong and how could i fix
>> that? I
>> am running Centos 5.5, in particular, the following
>>
>> $ more /proc/version
>> Linux version 2.6.18-194.el5 (mockbuild at builder10.centos.org) (gcc
>> version
>> 4.1.2 20080704 (Red Hat 4.1.2-48)) #1 SMP Fri Apr 2 14:58:14 EDT 2010
>
> That looks like a message from your compiler. I think gcc 4.1.2 is
> fairly old (Windows builds are using gcc 4.6.3). Perhaps it's time to
> upgrade.

Correct, it is very old (the date shows 2008).  But then so are the 
lapack sources, and that file is unchanged since 2006 (and R compiled on 
Linux perfectly well over those years).  So if R compiled on this system 
before, the system has changed ....

Your first port of call is to see if there are missing patches on your 
OS, then report to the vendor.  I might see if a lower optimization 
level would work.

>
> Duncan Murdoch
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From therneau at mayo.edu  Wed Apr 18 15:02:47 2012
From: therneau at mayo.edu (Terry Therneau)
Date: Wed, 18 Apr 2012 08:02:47 -0500
Subject: [Rd] Method=df for coxph in survival package
Message-ID: <4F8EBB77.3070309@mayo.edu>

In that particular example the value of "4" was pulled out of the air.  
There is no particular justification.

There is a strong relationship between the "effective" degrees of 
freedom and the variance of the random effect, and I often find the df 
scale easier to interpret.  See the Hodges and Sargent paper in 
Biometrika (2001) for a nice explanation of the connection in linear models.

Terry T.

=======  begin included message =========

I've been following the example in the R help page:
http://stat.ethz.ch/R-manual/R-devel/library/survival/html/frailty.html


library(survival);
coxph(Surv(time, status) ~ age + frailty(inst, df=4), lung)


Here, in this particular example they fixed the degrees of freedom for the
random institution effects to be 4.
But, how did they decide?


From josh.m.ulrich at gmail.com  Wed Apr 18 17:04:49 2012
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Wed, 18 Apr 2012 10:04:49 -0500
Subject: [Rd] url, readLines, source behind a proxy
In-Reply-To: <Pine.LNX.4.64.1204180705060.27275@mancala.cbio.uct.ac.za>
References: <4F8D234E.1070709@cbio.uct.ac.za>
	<CAFDcVCQx1R7gJYnOPC3yzayJxi-Ts9zzaQd5T0bFSnLcOv8nCg@mail.gmail.com>
	<Pine.LNX.4.64.1204180705060.27275@mancala.cbio.uct.ac.za>
Message-ID: <CAPPM_gS7LA-rv=__y_QbUi4ketxT9dn3PU=6dm+Sv9q5nU8nkw@mail.gmail.com>

Hi Renaud,

On Wed, Apr 18, 2012 at 12:22 AM, Renaud Gaujoux
<renaud at mancala.cbio.uct.ac.za> wrote:
> Hi Henrik,
>
<snip>
>
> Could anybody behind a proxy check if the issue can be reproduced?
> My proxy is in fact provided by cntml, which acts as a local proxy that
> takes care of tricky authentication protocols with the actual university
> proxy, not natively supported by my system (Ubuntu). Anybody in this case?
>
I can replicate this on a WinXP system, where I normally have to use
the --internet2 flag to get internet access through a proxy.

?download.file has a section on "Setting Proxies", which describes how
to use environment variables to set proxy information.  Setting
http_proxy='http://my.proxy.com/' was enough for me to get R CMD
check to run successfully with the --as-cran flag.

> Thanks.
> Renaud
>

Best,
--
Joshua Ulrich  |  FOSS Trading: www.fosstrading.com

R/Finance 2012: Applied Finance with R
www.RinFinance.com


> On Tue, 17 Apr 2012, Henrik Bengtsson wrote:
>
>> On Tue, Apr 17, 2012 at 1:01 AM, Renaud Gaujoux
>> <renaud at mancala.cbio.uct.ac.za> wrote:
>> > Hi,
>> >
>> > when I run R CMD check with flag --as-cran, the process hangs at stage:
>> >
>> > * checking CRAN incoming feasibility ...
>>
>> Doesn't it time-out eventually? ?I'm not behind a proxy but when I've
>> been running 'R CMD check' whenon very poor 3G connection, it had
>> eventually timed out.
>>
>> /Henrik
>>
>> >
>> > I am pretty sure it is a proxy issue.
>> > I looked at the check code in the tools package and it seems that the issue
>> > is in the local function `.repository_db()` (defined in
>> > `tools:::.check_package_CRAN_incoming()`), which eventually calls `url()`
>> > with argument open="rb", that hangs probably because it does not use the
>> > proxy settings.
>> > I had a similar issue with `source()`, which apparently uses internal
>> > network functions (not as download.file), but is supposed to work behind a
>> > proxy (correct?).
>> > Does anybody else have this problem?
>> >
>> > I was wondering if there is a way around, as I would like to be able to use
>> > --as-cran for my checks.
>> > Thank you.
>> >
>> > Renaud
>> >
>> > --
>> > Renaud Gaujoux
>> > Computational Biology - University of Cape Town
>> > South Africa
>> >
>> > ______________________________________________
>> > R-devel at r-project.org mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From nbezirgi at ee.duth.gr  Wed Apr 18 19:40:32 2012
From: nbezirgi at ee.duth.gr (Nikolaos Bezirgiannidis)
Date: Wed, 18 Apr 2012 17:40:32 +0000
Subject: [Rd] C - R integration: Memory Issues
Message-ID: <4eae997ce7a91027c9a116e74bfe0e62@webmail.duth.gr>

 Hi all,

 I am a PhD student and I am working on a C project that involves some 
 statistical calculations. So, I tried to embed R into C, in order to 
 call R functions from a C program. My program seems to get the correct 
 results from R. However, it appears to have a lot of memory allocation 
 issues, in contrast to the small amounts of memory that my code 
 allocates. Some additional info that might be useful: I have build R 
 from source with shared libraries enabled and the compiler I use is gcc 
 version 4.61 in a Ubuntu 11.10 linux machine.

 This is my function:

 static int prediction(double *berHistory, int berValues, double *ber)
 {
      SEXP e;
      SEXP bers;
      SEXP mean;
      int     i;
      int     errorOccurred;
      static int init = 0;
      char *argv[] = {"REmbeddedPostgres", "--gui=none", "--silent"};
      int argc = sizeof(argv)/sizeof(argv[0]);

      // Initialize Embedded R
      if (init == 0)
      {
           Rf_initEmbeddedR(argc, argv);
      }
      init = 1;

      // Allocate bers and copy values
      PROTECT(bers = allocVector(REALSXP, berValues));
 
      for (i = 0; i < berValues; i++)
      {
           REAL(bers)[i] = berHistory[i];
      }

      PROTECT(mean = allocVector(REALSXP, 1));
      PROTECT(e = lang2(install("mean"), bers));
      mean = R_tryEval(e, R_GlobalEnv, &errorOccurred);
      if (errorOccurred)
      {
           printf("error occurred in mean\n");
      }

      for (i = 0; i < berValues; i++)
      {
           REAL(bers)[i] = REAL(bers)[i] / REAL(mean)[0];
      }

      *ber = REAL(mean)[0];

      Rf_endEmbeddedR(0);
      UNPROTECT(3);
      return 0;
 }


 And these are the errors from Valgrind output:

 HEAP SUMMARY:
 ==2909==     in use at exit: 18,832,260 bytes in 6,791 blocks
 ==2909==   total heap usage: 21,758 allocs, 14,967 frees, 30,803,476 
 bytes allocated
 ==2909==
 ==2909== 160 (40 direct, 120 indirect) bytes in 1 blocks are definitely 
 lost in loss record 179 of 1,398
 ==2909==    at 0x4028876: malloc (vg_replace_malloc.c:236)
 ==2909==    by 0x41B364C: nss_parse_service_list (nsswitch.c:626)
 ==2909==    by 0x41B3C59: __nss_database_lookup (nsswitch.c:167)
 ==2909==    by 0x59272F8: ???
 ==2909==    by 0x5928CCC: ???
 ==2909==    by 0x416ABA6: getpwuid_r@@GLIBC_2.1.2 (getXXbyYY_r.c:256)
 ==2909==    by 0x416A4ED: getpwuid (getXXbyYY.c:117)
 ==2909==    by 0x439CCB9: do_fileinfo (platform.c:944)
 ==2909==    by 0x43289ED: bcEval (eval.c:4430)
 ==2909==    by 0x4332CA4: Rf_eval (eval.c:397)
 ==2909==    by 0x43377E0: Rf_applyClosure (eval.c:855)
 ==2909==    by 0x432F17E: bcEval (eval.c:4410)
 ==2909==
 ==2909== 160 (40 direct, 120 indirect) bytes in 1 blocks are definitely 
 lost in loss record 180 of 1,398
 ==2909==    at 0x4028876: malloc (vg_replace_malloc.c:236)
 ==2909==    by 0x41B364C: nss_parse_service_list (nsswitch.c:626)
 ==2909==    by 0x41B3C59: __nss_database_lookup (nsswitch.c:167)
 ==2909==    by 0x5926148: ???
 ==2909==    by 0x5926F3C: ???
 ==2909==    by 0x41694A6: getgrgid_r@@GLIBC_2.1.2 (getXXbyYY_r.c:256)
 ==2909==    by 0x4168CAD: getgrgid (getXXbyYY.c:117)
 ==2909==    by 0x439CCEB: do_fileinfo (platform.c:947)
 ==2909==    by 0x43289ED: bcEval (eval.c:4430)
 ==2909==    by 0x4332CA4: Rf_eval (eval.c:397)
 ==2909==    by 0x43377E0: Rf_applyClosure (eval.c:855)
 ==2909==    by 0x432F17E: bcEval (eval.c:4410)
 ==2909==
 ==2909== LEAK SUMMARY:
 ==2909==    definitely lost: 80 bytes in 2 blocks
 ==2909==    indirectly lost: 240 bytes in 20 blocks
 ==2909==      possibly lost: 0 bytes in 0 blocks
 ==2909==    still reachable: 18,831,940 bytes in 6,769 blocks

 Reachable error summary is far too long to include in this mail. The 
 interesting thing is that reachable errors are all caused by this small 
 function.

 Any ideas? I would also appreciate any suggestions on how to improve 
 the R-C integration in my code.

 Thank you in advance,
 Nikos


From macqueen1 at llnl.gov  Wed Apr 18 23:55:51 2012
From: macqueen1 at llnl.gov (MacQueen, Don)
Date: Wed, 18 Apr 2012 14:55:51 -0700
Subject: [Rd] I wish xlim=c(0,
 NA) would work. How about I send you a patch?
In-Reply-To: <4F8C6FD3.4060703@gmail.com>
Message-ID: <CBB48584.93876%macqueen1@llnl.gov>

Another potential problem with the xlim=c(5,NA) type of approach is
that if I am calculating the limits (as I often do) and have a bug
in my calculation, such that I get an NA for one of the limits,
then my plot would succeed rather than fail -- and succeed with
hard to predict results. This will make it harder for me to
discover my bug, I think.

Where this potential problem lies in a scale of "seriousness" relative
to other potential problems ... I have no idea.

-Don

-- 
Don MacQueen

Lawrence Livermore National Laboratory
7000 East Ave., L-627
Livermore, CA 94550
925-423-1062





On 4/16/12 12:15 PM, "Duncan Murdoch" <murdoch.duncan at gmail.com> wrote:

>On 12-04-16 2:26 PM, William Dunlap wrote:
>> plot(1:10, xlim=c(10,1)) reverses the x axis.
>> If we allow plot(1:10, xlim=c(5,NA)), which
>> direction should it go?    Would this require new
>> parameters, {x,y}{min,max} or new paremeters
>> {x,y}{axisDirection}?
>
>I'd rather not add another parameter.  So if I were to implement this,
>I'd probably choose Paul's original suggestion.  If someone wants c(5,
>NA) to mean c(5, min(data)) rather than c(5, max(data)) they'd need to
>code it explicitly.
>
>Duncan Murdoch
>
>>
>> Bill Dunlap
>> Spotfire, TIBCO Software
>> wdunlap tibco.com
>>
>>
>>> -----Original Message-----
>>> From: r-devel-bounces at r-project.org
>>>[mailto:r-devel-bounces at r-project.org] On Behalf
>>> Of Duncan Murdoch
>>> Sent: Monday, April 16, 2012 11:14 AM
>>> To: Paul Johnson
>>> Cc: R Devel List
>>> Subject: Re: [Rd] I wish xlim=c(0, NA) would work. How about I send
>>>you a patch?
>>>
>>> On 12-04-16 1:52 PM, Paul Johnson wrote:
>>>> I'm looking for an R mentor.  I want to propose a change in management
>>>> of plot options xlim and ylim.
>>>
>>> Your suggestion sounds reasonable, but because plot limits are such a
>>> commonly used parameter, it would have to be done quite carefully.  The
>>> questions I'd ask before implementing it are:
>>>
>>>    - Are there other locations besides plot.default where xlim and ylim
>>> are specified?  I'd like to have them updated consistently.
>>>
>>>    - Are there any conflicting uses of NA for a limit in published
>>>packages?
>>>
>>>    - Which package authors would need to be told about this change, so
>>> they could make a compatible change?
>>>
>>>    - Where does it need to be documented?
>>>
>>> Duncan Murdoch
>>>
>>>>
>>>> Did you ever want to change one coordinate in xlim or ylim? It happens
>>>> to me all the time.
>>>>
>>>> x<- rnorm(100, m=5, s=1)
>>>> y<- rnorm(100, m=6, s=1)
>>>> plot(x,y)
>>>>
>>>> ## Oh, I want the "y axis" to show above x=0.
>>>>
>>>> plot(x,y, xlim=c(0, ))
>>>>
>>>> ##Output: Error in c(0, ) : argument 2 is empty
>>>>
>>>>    plot(x,y, xlim=c(0,NA ))
>>>> ## Output: Error in plot.window(...) : need finite 'xlim' values
>>>>
>>>>
>>>> I wish that plot would let me supply just the min (or max) and then
>>>> calculate the other value where needed.
>>>> It is a little bit tedious for the user to do that for herself.  The
>>>> user must be knowledgeable enough to know where the maximum (MAX) is
>>>> supposed to be, and then supply xlim=c(0, MAX). I can't see any reason
>>>> for insisting users have that deeper understanding of how R calculates
>>>> ranges for plots.
>>>>
>>>> Suppose the user is allowed to supply NA to signal R should fill in
>>>>the blanks.
>>>>
>>>> plot(x,y, xlim=c(0, NA))
>>>>
>>>>
>>>> In plot.default now, I find this code to manage xlim
>>>>
>>>>      xlim<- if (is.null(xlim))
>>>>           range(xy$x[is.finite(xy$x)])
>>>>
>>>> And I would change it to be something like
>>>>      ##get default range
>>>>      nxlim<- range(xy$x[is.finite(xy$x)])
>>>>
>>>>      ## if xlim is NULL, so same as current
>>>>       xlim<- if (is.null(xlim)) nxlim
>>>> ## Otherwise, replace NAs in xlim with values from nxlim
>>>>       else xlim[ is.na(xlim) ]<- nxlim[ is.na(xlim) ]
>>>>
>>>>
>>>> Who is the responsible party for plot.default.  How about it?
>>>>
>>>> Think of how much happier users would be!
>>>>
>>>> pj
>>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>
>______________________________________________
>R-devel at r-project.org mailing list
>https://stat.ethz.ch/mailman/listinfo/r-devel


From edd at debian.org  Thu Apr 19 14:21:12 2012
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 19 Apr 2012 07:21:12 -0500
Subject: [Rd] C - R integration: Memory Issues
In-Reply-To: <4eae997ce7a91027c9a116e74bfe0e62@webmail.duth.gr>
References: <4eae997ce7a91027c9a116e74bfe0e62@webmail.duth.gr>
Message-ID: <20368.824.429418.688401@max.nulle.part>


On 18 April 2012 at 17:40, Nikolaos Bezirgiannidis wrote:
|  Hi all,
| 
|  I am a PhD student and I am working on a C project that involves some 
|  statistical calculations. So, I tried to embed R into C, in order to 
|  call R functions from a C program. My program seems to get the correct 
|  results from R. However, it appears to have a lot of memory allocation 
|  issues, in contrast to the small amounts of memory that my code 
|  allocates. Some additional info that might be useful: I have build R 
|  from source with shared libraries enabled and the compiler I use is gcc 
|  version 4.61 in a Ubuntu 11.10 linux machine.

[ Well I suspect "sudo apt-get install r-base" would have given you the same;
  see the README at $CRAN/src/bin/linux/ubuntu ]
 
|  This is my function:
| 
|  static int prediction(double *berHistory, int berValues, double *ber)
|  {
|       SEXP e;
|       SEXP bers;
|       SEXP mean;
|       int     i;
|       int     errorOccurred;
|       static int init = 0;
|       char *argv[] = {"REmbeddedPostgres", "--gui=none", "--silent"};
|       int argc = sizeof(argv)/sizeof(argv[0]);
| 
|       // Initialize Embedded R
|       if (init == 0)
|       {
|            Rf_initEmbeddedR(argc, argv);
|       }
|       init = 1;
| 
|       // Allocate bers and copy values
|       PROTECT(bers = allocVector(REALSXP, berValues));
|  
|       for (i = 0; i < berValues; i++)
|       {
|            REAL(bers)[i] = berHistory[i];
|       }
| 
|       PROTECT(mean = allocVector(REALSXP, 1));
|       PROTECT(e = lang2(install("mean"), bers));
|       mean = R_tryEval(e, R_GlobalEnv, &errorOccurred);
|       if (errorOccurred)
|       {
|            printf("error occurred in mean\n");
|       }
| 
|       for (i = 0; i < berValues; i++)
|       {
|            REAL(bers)[i] = REAL(bers)[i] / REAL(mean)[0];
|       }
| 
|       *ber = REAL(mean)[0];
| 
|       Rf_endEmbeddedR(0);
|       UNPROTECT(3);
|       return 0;
|  }
|
|  And these are the errors from Valgrind output:
| 
|  HEAP SUMMARY:
|  ==2909==     in use at exit: 18,832,260 bytes in 6,791 blocks
|  ==2909==   total heap usage: 21,758 allocs, 14,967 frees, 30,803,476 
|  bytes allocated
|  ==2909==
|  ==2909== 160 (40 direct, 120 indirect) bytes in 1 blocks are definitely 
|  lost in loss record 179 of 1,398
|  ==2909==    at 0x4028876: malloc (vg_replace_malloc.c:236)
|  ==2909==    by 0x41B364C: nss_parse_service_list (nsswitch.c:626)
|  ==2909==    by 0x41B3C59: __nss_database_lookup (nsswitch.c:167)
|  ==2909==    by 0x59272F8: ???
|  ==2909==    by 0x5928CCC: ???
|  ==2909==    by 0x416ABA6: getpwuid_r@@GLIBC_2.1.2 (getXXbyYY_r.c:256)
|  ==2909==    by 0x416A4ED: getpwuid (getXXbyYY.c:117)
|  ==2909==    by 0x439CCB9: do_fileinfo (platform.c:944)
|  ==2909==    by 0x43289ED: bcEval (eval.c:4430)
|  ==2909==    by 0x4332CA4: Rf_eval (eval.c:397)
|  ==2909==    by 0x43377E0: Rf_applyClosure (eval.c:855)
|  ==2909==    by 0x432F17E: bcEval (eval.c:4410)
|  ==2909==
|  ==2909== 160 (40 direct, 120 indirect) bytes in 1 blocks are definitely 
|  lost in loss record 180 of 1,398
|  ==2909==    at 0x4028876: malloc (vg_replace_malloc.c:236)
|  ==2909==    by 0x41B364C: nss_parse_service_list (nsswitch.c:626)
|  ==2909==    by 0x41B3C59: __nss_database_lookup (nsswitch.c:167)
|  ==2909==    by 0x5926148: ???
|  ==2909==    by 0x5926F3C: ???
|  ==2909==    by 0x41694A6: getgrgid_r@@GLIBC_2.1.2 (getXXbyYY_r.c:256)
|  ==2909==    by 0x4168CAD: getgrgid (getXXbyYY.c:117)
|  ==2909==    by 0x439CCEB: do_fileinfo (platform.c:947)
|  ==2909==    by 0x43289ED: bcEval (eval.c:4430)
|  ==2909==    by 0x4332CA4: Rf_eval (eval.c:397)
|  ==2909==    by 0x43377E0: Rf_applyClosure (eval.c:855)
|  ==2909==    by 0x432F17E: bcEval (eval.c:4410)
|  ==2909==
|  ==2909== LEAK SUMMARY:
|  ==2909==    definitely lost: 80 bytes in 2 blocks
|  ==2909==    indirectly lost: 240 bytes in 20 blocks
|  ==2909==      possibly lost: 0 bytes in 0 blocks
|  ==2909==    still reachable: 18,831,940 bytes in 6,769 blocks

Well I think there is no real bad error here. You lost 80 and 240 bytes. That
is nothing to worry about, and somewhat normal. R is a dynamic system,
valgrind measures "with some error".

You can compare the result to doing an allocation of a longer vector and not
freeing it.

You could see if freeing your 'mean' variable at the end makes a difference,
or using a pointer to a single (stack) instance instead of an allocation
makes a difference.  Likewise, you could returns bers as well. Or free
it. Right now I am not entirely what it is that your 'prediction' function is
trying to do.

|  Reachable error summary is far too long to include in this mail. The 
|  interesting thing is that reachable errors are all caused by this small 
|  function.
| 
|  Any ideas? I would also appreciate any suggestions on how to improve 
|  the R-C integration in my code.

Have you considered using R and C++ instead, and looked at Rcpp and RInside?  

     http://dirk.eddelbuettel.com/code/rcpp.html 

     http://dirk.eddelbuettel.com/code/rinside.html 

RInside in particular is a lot simpler, at least to me. But some people
really want plain C in which case you know which route to take

Dirk

-- 
R/Finance 2012 Conference on May 11 and 12, 2012 at UIC in Chicago, IL
See agenda, registration details and more at http://www.RinFinance.com


From rvaradhan at jhmi.edu  Thu Apr 19 20:12:30 2012
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Thu, 19 Apr 2012 18:12:30 +0000
Subject: [Rd] Column(row)wise minimum and maximum
Message-ID: <2F9EA67EF9AE1C48A147CB41BE2E15C31CD747@DOM-EB-MAIL2.win.ad.jhu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20120419/2456aae9/attachment.pl>

From oliver at first.in-berlin.de  Thu Apr 19 20:31:56 2012
From: oliver at first.in-berlin.de (oliver)
Date: Thu, 19 Apr 2012 20:31:56 +0200
Subject: [Rd] Column(row)wise minimum and maximum
In-Reply-To: <2F9EA67EF9AE1C48A147CB41BE2E15C31CD747@DOM-EB-MAIL2.win.ad.jhu.edu>
References: <2F9EA67EF9AE1C48A147CB41BE2E15C31CD747@DOM-EB-MAIL2.win.ad.jhu.edu>
Message-ID: <20120419183156.GA2692@siouxsie>

On Thu, Apr 19, 2012 at 06:12:30PM +0000, Ravi Varadhan wrote:
> Hi,
> 
> Currently, the "base" has colSums, colMeans.  It seems that it would be
> useful to extend this to also include colMin, colMax (of course, rowMin and
> rowMax, as well) in order to facilitate faster computations for large vectors
> (compared to using apply).  Has this been considered before?  Please forgive me
> if this has already been discussed before.
[...]

Not sure if the performance of apply is so much of a problem,
but also from a view of consistency of the provided
functions, I think offering such functions would make
it more clear and consitent to use R here, because
all those functions are then available for row and col
and the functionality is appended in the name (Mean, Sum etc.).

I think the basic things that should be available woud be:

 - mean
 - median
 - min
 - max
 - var
 - sd

If both then are available for rows and columns,
this would be fine.



Ciao,
   Oliver


From oliver at first.in-berlin.de  Thu Apr 19 20:34:13 2012
From: oliver at first.in-berlin.de (oliver)
Date: Thu, 19 Apr 2012 20:34:13 +0200
Subject: [Rd] Column(row)wise minimum and maximum
In-Reply-To: <20120419183156.GA2692@siouxsie>
References: <2F9EA67EF9AE1C48A147CB41BE2E15C31CD747@DOM-EB-MAIL2.win.ad.jhu.edu>
	<20120419183156.GA2692@siouxsie>
Message-ID: <20120419183413.GC2510@siouxsie>

On Thu, Apr 19, 2012 at 08:31:56PM +0200, oliver wrote:
> On Thu, Apr 19, 2012 at 06:12:30PM +0000, Ravi Varadhan wrote:
> > Hi,
> > 
> > Currently, the "base" has colSums, colMeans.  It seems that it would be
> > useful to extend this to also include colMin, colMax (of course, rowMin and
> > rowMax, as well) in order to facilitate faster computations for large vectors
> > (compared to using apply).  Has this been considered before?  Please forgive me
> > if this has already been discussed before.
> [...]
> 
> Not sure if the performance of apply is so much of a problem,
> but also from a view of consistency of the provided
> functions, I think offering such functions would make
> it more clear and consitent to use R here, because
> all those functions are then available for row and col
> and the functionality is appended in the name (Mean, Sum etc.).
> 
> I think the basic things that should be available woud be:
> 
>  - mean
>  - median
>  - min
>  - max
>  - var
>  - sd
   - sum   :-)
   - diff


Ciao,
   Oliver


From rvaradhan at jhmi.edu  Thu Apr 19 20:39:20 2012
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Thu, 19 Apr 2012 18:39:20 +0000
Subject: [Rd] Column(row)wise minimum and maximum
In-Reply-To: <20120419183413.GC2510@siouxsie>
References: <2F9EA67EF9AE1C48A147CB41BE2E15C31CD747@DOM-EB-MAIL2.win.ad.jhu.edu>
	<20120419183156.GA2692@siouxsie> <20120419183413.GC2510@siouxsie>
Message-ID: <2F9EA67EF9AE1C48A147CB41BE2E15C31CD7BB@DOM-EB-MAIL2.win.ad.jhu.edu>

Oliver,

It is mainly a speed issue (and also compactness!), at least for me.  Using `apply' is so much slower.   I agree with you that having column and row operations available for "basic" stats operations in "base" would be great.

David - I am aware of capabilities in other packages, but I am hoping that such basic operations would be part of "base" distribution.  

Ravi

-----Original Message-----
From: oliver [mailto:oliver at first.in-berlin.de] 
Sent: Thursday, April 19, 2012 2:34 PM
To: Ravi Varadhan
Cc: r-devel at r-project.org
Subject: Re: [Rd] Column(row)wise minimum and maximum

On Thu, Apr 19, 2012 at 08:31:56PM +0200, oliver wrote:
> On Thu, Apr 19, 2012 at 06:12:30PM +0000, Ravi Varadhan wrote:
> > Hi,
> > 
> > Currently, the "base" has colSums, colMeans.  It seems that it would 
> > be useful to extend this to also include colMin, colMax (of course, 
> > rowMin and rowMax, as well) in order to facilitate faster 
> > computations for large vectors (compared to using apply).  Has this 
> > been considered before?  Please forgive me if this has already been discussed before.
> [...]
> 
> Not sure if the performance of apply is so much of a problem, but also 
> from a view of consistency of the provided functions, I think offering 
> such functions would make it more clear and consitent to use R here, 
> because all those functions are then available for row and col and the 
> functionality is appended in the name (Mean, Sum etc.).
> 
> I think the basic things that should be available woud be:
> 
>  - mean
>  - median
>  - min
>  - max
>  - var
>  - sd
   - sum   :-)
   - diff


Ciao,
   Oliver

From hb at biostat.ucsf.edu  Thu Apr 19 21:57:14 2012
From: hb at biostat.ucsf.edu (Henrik Bengtsson)
Date: Thu, 19 Apr 2012 12:57:14 -0700
Subject: [Rd] Column(row)wise minimum and maximum
In-Reply-To: <2F9EA67EF9AE1C48A147CB41BE2E15C31CD7BB@DOM-EB-MAIL2.win.ad.jhu.edu>
References: <2F9EA67EF9AE1C48A147CB41BE2E15C31CD747@DOM-EB-MAIL2.win.ad.jhu.edu>
	<20120419183156.GA2692@siouxsie> <20120419183413.GC2510@siouxsie>
	<2F9EA67EF9AE1C48A147CB41BE2E15C31CD7BB@DOM-EB-MAIL2.win.ad.jhu.edu>
Message-ID: <CAFDcVCQ9e+5rnc441BEftQOec-4DcWtotnDZWg0SJKPTUv6B+w@mail.gmail.com>

This is why the matrixStats package was created, cf.
http://cran.r-project.org/web/packages/matrixStats/

1. Yes, it would be nice to have them in one of the default packages.
2. We decided to focus on/constrain ourselves matrices in matrixStats.
 We decided not to go into arrays with length(dim(.)) >= 3.

/Henrik

On Thu, Apr 19, 2012 at 11:39 AM, Ravi Varadhan <rvaradhan at jhmi.edu> wrote:
> Oliver,
>
> It is mainly a speed issue (and also compactness!), at least for me. ?Using `apply' is so much slower. ? I agree with you that having column and row operations available for "basic" stats operations in "base" would be great.
>
> David - I am aware of capabilities in other packages, but I am hoping that such basic operations would be part of "base" distribution.
>
> Ravi
>
> -----Original Message-----
> From: oliver [mailto:oliver at first.in-berlin.de]
> Sent: Thursday, April 19, 2012 2:34 PM
> To: Ravi Varadhan
> Cc: r-devel at r-project.org
> Subject: Re: [Rd] Column(row)wise minimum and maximum
>
> On Thu, Apr 19, 2012 at 08:31:56PM +0200, oliver wrote:
>> On Thu, Apr 19, 2012 at 06:12:30PM +0000, Ravi Varadhan wrote:
>> > Hi,
>> >
>> > Currently, the "base" has colSums, colMeans. ?It seems that it would
>> > be useful to extend this to also include colMin, colMax (of course,
>> > rowMin and rowMax, as well) in order to facilitate faster
>> > computations for large vectors (compared to using apply). ?Has this
>> > been considered before? ?Please forgive me if this has already been discussed before.
>> [...]
>>
>> Not sure if the performance of apply is so much of a problem, but also
>> from a view of consistency of the provided functions, I think offering
>> such functions would make it more clear and consitent to use R here,
>> because all those functions are then available for row and col and the
>> functionality is appended in the name (Mean, Sum etc.).
>>
>> I think the basic things that should be available woud be:
>>
>> ?- mean
>> ?- median
>> ?- min
>> ?- max
>> ?- var
>> ?- sd
> ? - sum ? :-)
> ? - diff
>
>
> Ciao,
> ? Oliver
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From wdunlap at tibco.com  Thu Apr 19 22:14:35 2012
From: wdunlap at tibco.com (William Dunlap)
Date: Thu, 19 Apr 2012 20:14:35 +0000
Subject: [Rd] Column(row)wise minimum and maximum
In-Reply-To: <2F9EA67EF9AE1C48A147CB41BE2E15C31CD747@DOM-EB-MAIL2.win.ad.jhu.edu>
References: <2F9EA67EF9AE1C48A147CB41BE2E15C31CD747@DOM-EB-MAIL2.win.ad.jhu.edu>
Message-ID: <E66794E69CFDE04D9A70842786030B932972F4@PA-MBX04.na.tibco.com>

S+ has such functions with a slightly different naming convention:
it puts an 's' after each function.  E.g., colMaxs instead of your
colMax.

 "colMaxs"      "colMeans"     "colMedians"   "colMins"      "colProds"    
 "colQuantiles" "colRanges"    "colStdevs"    "colSums"      "colVars"     
 "rowMaxs"   "rowMeans"  "rowMins"   "rowRanges" "rowStdevs" "rowSums"   "rowVars"  

It also has a similar family of functions with 'col'/'row' replaced by 'group'
to handle common calls to tapply(split(x,group),summaryFunc)
  "groupAlls"       "groupAnys"       "groupMaxs"       "groupMeans"      "groupMins"      
  "groupProds"      "groupRanges"     "groupStdevs"     "groupSums"       "groupVars"    
and lower level versions of those called "igroup<Summarys>" where the
group vector must be a vector of small positive integers (like tabulate's
input, while the 'group<Summarys>' are like table and coerce group to be
a factor).

They certainly don't need to be in the base product, but they are handy.

Bill Dunlap
Spotfire, TIBCO Software
wdunlap tibco.com


> -----Original Message-----
> From: r-devel-bounces at r-project.org [mailto:r-devel-bounces at r-project.org] On Behalf
> Of Ravi Varadhan
> Sent: Thursday, April 19, 2012 11:13 AM
> To: r-devel at r-project.org
> Subject: [Rd] Column(row)wise minimum and maximum
> 
> Hi,
> 
> Currently, the "base" has colSums, colMeans.  It seems that it would be useful to extend
> this to also include colMin, colMax (of course, rowMin and rowMax, as well) in order to
> facilitate faster computations for large vectors (compared to using apply).  Has this been
> considered before?  Please forgive me if this has already been discussed before.
> 
> Thanks,
> Ravi
> 
> Ravi Varadhan, Ph.D.
> Assistant Professor
> The Center on Aging and Health
> Division of Geriatric Medicine & Gerontology
> Johns Hopkins University
> rvaradhan at jhmi.edu<mailto:rvaradhan at jhmi.edu>
> 410-502-2619
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From rvaradhan at jhmi.edu  Thu Apr 19 22:33:22 2012
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Thu, 19 Apr 2012 20:33:22 +0000
Subject: [Rd] Column(row)wise minimum and maximum
In-Reply-To: <CAFDcVCQ9e+5rnc441BEftQOec-4DcWtotnDZWg0SJKPTUv6B+w@mail.gmail.com>
References: <2F9EA67EF9AE1C48A147CB41BE2E15C31CD747@DOM-EB-MAIL2.win.ad.jhu.edu>
	<20120419183156.GA2692@siouxsie> <20120419183413.GC2510@siouxsie>
	<2F9EA67EF9AE1C48A147CB41BE2E15C31CD7BB@DOM-EB-MAIL2.win.ad.jhu.edu>
	<CAFDcVCQ9e+5rnc441BEftQOec-4DcWtotnDZWg0SJKPTUv6B+w@mail.gmail.com>
Message-ID: <2F9EA67EF9AE1C48A147CB41BE2E15C31CD909@DOM-EB-MAIL2.win.ad.jhu.edu>

Thanks, Henrik, for the helpful response.  Your package was helpful indeed!  

However, I would still like to see this in the base package.

Ravi

-----Original Message-----
From: henrik.bengtsson at gmail.com [mailto:henrik.bengtsson at gmail.com] On Behalf Of Henrik Bengtsson
Sent: Thursday, April 19, 2012 3:57 PM
To: Ravi Varadhan
Cc: oliver; r-devel at r-project.org
Subject: Re: [Rd] Column(row)wise minimum and maximum

This is why the matrixStats package was created, cf.
http://cran.r-project.org/web/packages/matrixStats/

1. Yes, it would be nice to have them in one of the default packages.
2. We decided to focus on/constrain ourselves matrices in matrixStats.
 We decided not to go into arrays with length(dim(.)) >= 3.

/Henrik

On Thu, Apr 19, 2012 at 11:39 AM, Ravi Varadhan <rvaradhan at jhmi.edu> wrote:
> Oliver,
>
> It is mainly a speed issue (and also compactness!), at least for me. ?Using `apply' is so much slower. ? I agree with you that having column and row operations available for "basic" stats operations in "base" would be great.
>
> David - I am aware of capabilities in other packages, but I am hoping that such basic operations would be part of "base" distribution.
>
> Ravi
>
> -----Original Message-----
> From: oliver [mailto:oliver at first.in-berlin.de]
> Sent: Thursday, April 19, 2012 2:34 PM
> To: Ravi Varadhan
> Cc: r-devel at r-project.org
> Subject: Re: [Rd] Column(row)wise minimum and maximum
>
> On Thu, Apr 19, 2012 at 08:31:56PM +0200, oliver wrote:
>> On Thu, Apr 19, 2012 at 06:12:30PM +0000, Ravi Varadhan wrote:
>> > Hi,
>> >
>> > Currently, the "base" has colSums, colMeans. ?It seems that it 
>> > would be useful to extend this to also include colMin, colMax (of 
>> > course, rowMin and rowMax, as well) in order to facilitate faster 
>> > computations for large vectors (compared to using apply). ?Has this 
>> > been considered before? ?Please forgive me if this has already been discussed before.
>> [...]
>>
>> Not sure if the performance of apply is so much of a problem, but 
>> also from a view of consistency of the provided functions, I think 
>> offering such functions would make it more clear and consitent to use 
>> R here, because all those functions are then available for row and 
>> col and the functionality is appended in the name (Mean, Sum etc.).
>>
>> I think the basic things that should be available woud be:
>>
>> ?- mean
>> ?- median
>> ?- min
>> ?- max
>> ?- var
>> ?- sd
> ? - sum ? :-)
> ? - diff
>
>
> Ciao,
> ? Oliver
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From mailinglist.honeypot at gmail.com  Thu Apr 19 23:56:27 2012
From: mailinglist.honeypot at gmail.com (Steve Lianoglou)
Date: Thu, 19 Apr 2012 17:56:27 -0400
Subject: [Rd] Column(row)wise minimum and maximum
In-Reply-To: <2F9EA67EF9AE1C48A147CB41BE2E15C31CD909@DOM-EB-MAIL2.win.ad.jhu.edu>
References: <2F9EA67EF9AE1C48A147CB41BE2E15C31CD747@DOM-EB-MAIL2.win.ad.jhu.edu>
	<20120419183156.GA2692@siouxsie> <20120419183413.GC2510@siouxsie>
	<2F9EA67EF9AE1C48A147CB41BE2E15C31CD7BB@DOM-EB-MAIL2.win.ad.jhu.edu>
	<CAFDcVCQ9e+5rnc441BEftQOec-4DcWtotnDZWg0SJKPTUv6B+w@mail.gmail.com>
	<2F9EA67EF9AE1C48A147CB41BE2E15C31CD909@DOM-EB-MAIL2.win.ad.jhu.edu>
Message-ID: <CAHA9McNY75FV+QBTeS3dAhssHYWnCfOq8at04RqwnVdCGKY+ag@mail.gmail.com>

Hi,

On Thu, Apr 19, 2012 at 4:33 PM, Ravi Varadhan <rvaradhan at jhmi.edu> wrote:
> Thanks, Henrik, for the helpful response. ?Your package was helpful indeed!
>
> However, I would still like to see this in the base package.

For what it's worth, I think they'd be useful to have there, too.

-steve

>
> Ravi
>
> -----Original Message-----
> From: henrik.bengtsson at gmail.com [mailto:henrik.bengtsson at gmail.com] On Behalf Of Henrik Bengtsson
> Sent: Thursday, April 19, 2012 3:57 PM
> To: Ravi Varadhan
> Cc: oliver; r-devel at r-project.org
> Subject: Re: [Rd] Column(row)wise minimum and maximum
>
> This is why the matrixStats package was created, cf.
> http://cran.r-project.org/web/packages/matrixStats/
>
> 1. Yes, it would be nice to have them in one of the default packages.
> 2. We decided to focus on/constrain ourselves matrices in matrixStats.
> ?We decided not to go into arrays with length(dim(.)) >= 3.
>
> /Henrik
>
> On Thu, Apr 19, 2012 at 11:39 AM, Ravi Varadhan <rvaradhan at jhmi.edu> wrote:
>> Oliver,
>>
>> It is mainly a speed issue (and also compactness!), at least for me. ?Using `apply' is so much slower. ? I agree with you that having column and row operations available for "basic" stats operations in "base" would be great.
>>
>> David - I am aware of capabilities in other packages, but I am hoping that such basic operations would be part of "base" distribution.
>>
>> Ravi
>>
>> -----Original Message-----
>> From: oliver [mailto:oliver at first.in-berlin.de]
>> Sent: Thursday, April 19, 2012 2:34 PM
>> To: Ravi Varadhan
>> Cc: r-devel at r-project.org
>> Subject: Re: [Rd] Column(row)wise minimum and maximum
>>
>> On Thu, Apr 19, 2012 at 08:31:56PM +0200, oliver wrote:
>>> On Thu, Apr 19, 2012 at 06:12:30PM +0000, Ravi Varadhan wrote:
>>> > Hi,
>>> >
>>> > Currently, the "base" has colSums, colMeans. ?It seems that it
>>> > would be useful to extend this to also include colMin, colMax (of
>>> > course, rowMin and rowMax, as well) in order to facilitate faster
>>> > computations for large vectors (compared to using apply). ?Has this
>>> > been considered before? ?Please forgive me if this has already been discussed before.
>>> [...]
>>>
>>> Not sure if the performance of apply is so much of a problem, but
>>> also from a view of consistency of the provided functions, I think
>>> offering such functions would make it more clear and consitent to use
>>> R here, because all those functions are then available for row and
>>> col and the functionality is appended in the name (Mean, Sum etc.).
>>>
>>> I think the basic things that should be available woud be:
>>>
>>> ?- mean
>>> ?- median
>>> ?- min
>>> ?- max
>>> ?- var
>>> ?- sd
>> ? - sum ? :-)
>> ? - diff
>>
>>
>> Ciao,
>> ? Oliver
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel



-- 
Steve Lianoglou
Graduate Student: Computational Systems Biology
?| Memorial Sloan-Kettering Cancer Center
?| Weill Medical College of Cornell University
Contact Info: http://cbio.mskcc.org/~lianos/contact


From holger.schw at gmx.de  Fri Apr 20 00:14:38 2012
From: holger.schw at gmx.de (Holger Schwender)
Date: Fri, 20 Apr 2012 00:14:38 +0200
Subject: [Rd] Column(row)wise minimum and maximum
In-Reply-To: <2F9EA67EF9AE1C48A147CB41BE2E15C31CD747@DOM-EB-MAIL2.win.ad.jhu.edu>
References: <2F9EA67EF9AE1C48A147CB41BE2E15C31CD747@DOM-EB-MAIL2.win.ad.jhu.edu>
Message-ID: <20120419221438.3210@gmx.net>

Hi Ravi,

rowMin and rowMax are available in the Biobase package (available from BioConductor).

Best,
Holger


-------- Original-Nachricht --------
> Datum: Thu, 19 Apr 2012 18:12:30 +0000
> Von: Ravi Varadhan <rvaradhan at jhmi.edu>
> An: "r-devel at r-project.org" <r-devel at r-project.org>
> Betreff: [Rd] Column(row)wise minimum and maximum

> Hi,
> 
> Currently, the "base" has colSums, colMeans.  It seems that it would be
> useful to extend this to also include colMin, colMax (of course, rowMin and
> rowMax, as well) in order to facilitate faster computations for large
> vectors (compared to using apply).  Has this been considered before?  Please
> forgive me if this has already been discussed before.
> 
> Thanks,
> Ravi
> 
> Ravi Varadhan, Ph.D.
> Assistant Professor
> The Center on Aging and Health
> Division of Geriatric Medicine & Gerontology
> Johns Hopkins University
> rvaradhan at jhmi.edu<mailto:rvaradhan at jhmi.edu>
> 410-502-2619
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

--


From cole.beck at Vanderbilt.Edu  Fri Apr 20 00:34:29 2012
From: cole.beck at Vanderbilt.Edu (Cole Beck)
Date: Thu, 19 Apr 2012 17:34:29 -0500
Subject: [Rd] combining large list of data.frames
Message-ID: <4F9092F5.8070705@vanderbilt.edu>

It's normal for me to create a list of data.frames and then use 
do.call('rbind', list(...)) to create a single data.frame.  However, 
I've noticed as the size of the list grows large, it is perhaps better 
to do this in chunks.  As an example here's a list of 20,000 similar 
data.frames.

# create list of data.frames
dat <- vector("list", 20000)
for(i in seq_along(dat)) {
   size <- sample(1:30, 1)
   dat[[i]] <- data.frame(id=rep(i, size), value=rnorm(size), 
letter=sample(LETTERS, size, replace=TRUE), ind=sample(c(TRUE,FALSE), 
size, replace=TRUE))
}
# combine into one data.frame, normal usage
# system.time(do.call('rbind', dat)) # takes 2-3 minutes
combine <- function(x, steps=NA, verbose=FALSE) {
   nr <- length(x)
   if(is.na(steps)) steps <- nr
   while(nr %% steps != 0) steps <- steps+1
   if(verbose) cat(sprintf("step size: %s\r\n", steps))
   dl <- vector("list", steps)
   for(i in seq(steps)) {
     ix <- seq(from=(i-1)*nr/steps+1, length.out=nr/steps)
     dl[[i]] <- do.call("rbind", x[ix])
   }
   do.call("rbind", dl)
}
# combine into one data.frame
system.time(combine(dat, 100)) # takes 5-10 seconds

I'm very surprised by this result.  Does this improvement seem 
reasonable?  I would think "do.call" could utilize something similar by 
default when the length of "args" is too high.  Is using "do.call" not 
recommended in this scenario?

Regards,
Cole Beck


From nbezirgi at ee.duth.gr  Fri Apr 20 11:06:52 2012
From: nbezirgi at ee.duth.gr (Nikolaos Bezirgiannidis)
Date: Fri, 20 Apr 2012 09:06:52 +0000
Subject: [Rd] C - R integration: Memory Issues
In-Reply-To: <20368.824.429418.688401@max.nulle.part>
References: <4eae997ce7a91027c9a116e74bfe0e62@webmail.duth.gr>
	<20368.824.429418.688401@max.nulle.part>
Message-ID: <5f2eed30c28c417068a65dddf6388771@webmail.duth.gr>

 On Thu, 19 Apr 2012 07:21:12 -0500, Dirk Eddelbuettel wrote:
> On 18 April 2012 at 17:40, Nikolaos Bezirgiannidis wrote:
> |  Hi all,
> |
> |  I am a PhD student and I am working on a C project that involves 
> some
> |  statistical calculations. So, I tried to embed R into C, in order 
> to
> |  call R functions from a C program. My program seems to get the 
> correct
> |  results from R. However, it appears to have a lot of memory 
> allocation
> |  issues, in contrast to the small amounts of memory that my code
> |  allocates. Some additional info that might be useful: I have build 
> R
> |  from source with shared libraries enabled and the compiler I use 
> is gcc
> |  version 4.61 in a Ubuntu 11.10 linux machine.
>
> [ Well I suspect "sudo apt-get install r-base" would have given you 
> the same;
>   see the README at $CRAN/src/bin/linux/ubuntu ]
>
> |  This is my function:
> |
> |  static int prediction(double *berHistory, int berValues, double 
> *ber)
> |  {
> |       SEXP e;
> |       SEXP bers;
> |       SEXP mean;
> |       int     i;
> |       int     errorOccurred;
> |       static int init = 0;
> |       char *argv[] = {"REmbeddedPostgres", "--gui=none", 
> "--silent"};
> |       int argc = sizeof(argv)/sizeof(argv[0]);
> |
> |       // Initialize Embedded R
> |       if (init == 0)
> |       {
> |            Rf_initEmbeddedR(argc, argv);
> |       }
> |       init = 1;
> |
> |       // Allocate bers and copy values
> |       PROTECT(bers = allocVector(REALSXP, berValues));
> |
> |       for (i = 0; i < berValues; i++)
> |       {
> |            REAL(bers)[i] = berHistory[i];
> |       }
> |
> |       PROTECT(mean = allocVector(REALSXP, 1));
> |       PROTECT(e = lang2(install("mean"), bers));
> |       mean = R_tryEval(e, R_GlobalEnv, &errorOccurred);
> |       if (errorOccurred)
> |       {
> |            printf("error occurred in mean\n");
> |       }
> |
> |       for (i = 0; i < berValues; i++)
> |       {
> |            REAL(bers)[i] = REAL(bers)[i] / REAL(mean)[0];
> |       }
> |
> |       *ber = REAL(mean)[0];
> |
> |       Rf_endEmbeddedR(0);
> |       UNPROTECT(3);
> |       return 0;
> |  }
> |
> |  And these are the errors from Valgrind output:
> |
> |  HEAP SUMMARY:
> |  ==2909==     in use at exit: 18,832,260 bytes in 6,791 blocks
> |  ==2909==   total heap usage: 21,758 allocs, 14,967 frees, 
> 30,803,476
> |  bytes allocated
> |  ==2909==
> |  ==2909== 160 (40 direct, 120 indirect) bytes in 1 blocks are 
> definitely
> |  lost in loss record 179 of 1,398
> |  ==2909==    at 0x4028876: malloc (vg_replace_malloc.c:236)
> |  ==2909==    by 0x41B364C: nss_parse_service_list (nsswitch.c:626)
> |  ==2909==    by 0x41B3C59: __nss_database_lookup (nsswitch.c:167)
> |  ==2909==    by 0x59272F8: ???
> |  ==2909==    by 0x5928CCC: ???
> |  ==2909==    by 0x416ABA6: getpwuid_r@@GLIBC_2.1.2 
> (getXXbyYY_r.c:256)
> |  ==2909==    by 0x416A4ED: getpwuid (getXXbyYY.c:117)
> |  ==2909==    by 0x439CCB9: do_fileinfo (platform.c:944)
> |  ==2909==    by 0x43289ED: bcEval (eval.c:4430)
> |  ==2909==    by 0x4332CA4: Rf_eval (eval.c:397)
> |  ==2909==    by 0x43377E0: Rf_applyClosure (eval.c:855)
> |  ==2909==    by 0x432F17E: bcEval (eval.c:4410)
> |  ==2909==
> |  ==2909== 160 (40 direct, 120 indirect) bytes in 1 blocks are 
> definitely
> |  lost in loss record 180 of 1,398
> |  ==2909==    at 0x4028876: malloc (vg_replace_malloc.c:236)
> |  ==2909==    by 0x41B364C: nss_parse_service_list (nsswitch.c:626)
> |  ==2909==    by 0x41B3C59: __nss_database_lookup (nsswitch.c:167)
> |  ==2909==    by 0x5926148: ???
> |  ==2909==    by 0x5926F3C: ???
> |  ==2909==    by 0x41694A6: getgrgid_r@@GLIBC_2.1.2 
> (getXXbyYY_r.c:256)
> |  ==2909==    by 0x4168CAD: getgrgid (getXXbyYY.c:117)
> |  ==2909==    by 0x439CCEB: do_fileinfo (platform.c:947)
> |  ==2909==    by 0x43289ED: bcEval (eval.c:4430)
> |  ==2909==    by 0x4332CA4: Rf_eval (eval.c:397)
> |  ==2909==    by 0x43377E0: Rf_applyClosure (eval.c:855)
> |  ==2909==    by 0x432F17E: bcEval (eval.c:4410)
> |  ==2909==
> |  ==2909== LEAK SUMMARY:
> |  ==2909==    definitely lost: 80 bytes in 2 blocks
> |  ==2909==    indirectly lost: 240 bytes in 20 blocks
> |  ==2909==      possibly lost: 0 bytes in 0 blocks
> |  ==2909==    still reachable: 18,831,940 bytes in 6,769 blocks
>
> Well I think there is no real bad error here. You lost 80 and 240 
> bytes. That
> is nothing to worry about, and somewhat normal. R is a dynamic 
> system,
> valgrind measures "with some error".
>
> You can compare the result to doing an allocation of a longer vector 
> and not
> freeing it.
>
> You could see if freeing your 'mean' variable at the end makes a 
> difference,
> or using a pointer to a single (stack) instance instead of an 
> allocation
> makes a difference.  Likewise, you could returns bers as well. Or 
> free
> it. Right now I am not entirely what it is that your 'prediction' 
> function is
> trying to do.

 It is actually a more complex function with many more allocations and 
 R_tryEval calls; the simplified one that I included leads to exactly the 
 same lost bytes, which means that it is not caused by allocVector, but 
 probably by the  R_tryEval call. Is there any other, more proper way to 
 evaluate R expressions from C?

 In my function I only need to return ber, so I could easily free all R 
 objects (bers and mean here). But C's free() has no result and I am not 
 familiar with any R function that frees objects allocated by 
 allocVector. I have also seen in some R tutorials and examples that 
 objects allocated by allocVector are not freed in any way.

>
> |  Reachable error summary is far too long to include in this mail. 
> The
> |  interesting thing is that reachable errors are all caused by this 
> small
> |  function.
> |
> |  Any ideas? I would also appreciate any suggestions on how to 
> improve
> |  the R-C integration in my code.
>
> Have you considered using R and C++ instead, and looked at Rcpp and
> RInside?
>
>      http://dirk.eddelbuettel.com/code/rcpp.html
>
>      http://dirk.eddelbuettel.com/code/rinside.html
>
> RInside in particular is a lot simpler, at least to me. But some 
> people
> really want plain C in which case you know which route to take
>
> Dirk

 No, unfortunately I cannot switch to C++, because my program is a part 
 of a bigger C project.

 Thanks for your reply and suggestions,
 Nikos


From simon.urbanek at r-project.org  Fri Apr 20 15:51:30 2012
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Fri, 20 Apr 2012 09:51:30 -0400
Subject: [Rd] C - R integration: Memory Issues
In-Reply-To: <5f2eed30c28c417068a65dddf6388771@webmail.duth.gr>
References: <4eae997ce7a91027c9a116e74bfe0e62@webmail.duth.gr>
	<20368.824.429418.688401@max.nulle.part>
	<5f2eed30c28c417068a65dddf6388771@webmail.duth.gr>
Message-ID: <92217B5B-A391-487F-BBFF-D94EC59B54FB@r-project.org>


On Apr 20, 2012, at 5:06 AM, Nikolaos Bezirgiannidis wrote:

> On Thu, 19 Apr 2012 07:21:12 -0500, Dirk Eddelbuettel wrote:
>> On 18 April 2012 at 17:40, Nikolaos Bezirgiannidis wrote:
>> |  Hi all,
>> |
>> |  I am a PhD student and I am working on a C project that involves some
>> |  statistical calculations. So, I tried to embed R into C, in order to
>> |  call R functions from a C program. My program seems to get the correct
>> |  results from R. However, it appears to have a lot of memory allocation
>> |  issues, in contrast to the small amounts of memory that my code
>> |  allocates. Some additional info that might be useful: I have build R
>> |  from source with shared libraries enabled and the compiler I use is gcc
>> |  version 4.61 in a Ubuntu 11.10 linux machine.
>> 
>> [ Well I suspect "sudo apt-get install r-base" would have given you the same;
>>  see the README at $CRAN/src/bin/linux/ubuntu ]
>> 
>> |  This is my function:
>> |
>> |  static int prediction(double *berHistory, int berValues, double *ber)
>> |  {
>> |       SEXP e;
>> |       SEXP bers;
>> |       SEXP mean;
>> |       int     i;
>> |       int     errorOccurred;
>> |       static int init = 0;
>> |       char *argv[] = {"REmbeddedPostgres", "--gui=none", "--silent"};
>> |       int argc = sizeof(argv)/sizeof(argv[0]);
>> |
>> |       // Initialize Embedded R
>> |       if (init == 0)
>> |       {
>> |            Rf_initEmbeddedR(argc, argv);
>> |       }
>> |       init = 1;
>> |
>> |       // Allocate bers and copy values
>> |       PROTECT(bers = allocVector(REALSXP, berValues));
>> |
>> |       for (i = 0; i < berValues; i++)
>> |       {
>> |            REAL(bers)[i] = berHistory[i];
>> |       }
>> |
>> |       PROTECT(mean = allocVector(REALSXP, 1));
>> |       PROTECT(e = lang2(install("mean"), bers));
>> |       mean = R_tryEval(e, R_GlobalEnv, &errorOccurred);
>> |       if (errorOccurred)
>> |       {
>> |            printf("error occurred in mean\n");
>> |       }
>> |
>> |       for (i = 0; i < berValues; i++)
>> |       {
>> |            REAL(bers)[i] = REAL(bers)[i] / REAL(mean)[0];
>> |       }
>> |
>> |       *ber = REAL(mean)[0];
>> |
>> |       Rf_endEmbeddedR(0);
>> |       UNPROTECT(3);
>> |       return 0;
>> |  }
>> |
>> |  And these are the errors from Valgrind output:
>> |
>> |  HEAP SUMMARY:
>> |  ==2909==     in use at exit: 18,832,260 bytes in 6,791 blocks
>> |  ==2909==   total heap usage: 21,758 allocs, 14,967 frees, 30,803,476
>> |  bytes allocated
>> |  ==2909==
>> |  ==2909== 160 (40 direct, 120 indirect) bytes in 1 blocks are definitely
>> |  lost in loss record 179 of 1,398
>> |  ==2909==    at 0x4028876: malloc (vg_replace_malloc.c:236)
>> |  ==2909==    by 0x41B364C: nss_parse_service_list (nsswitch.c:626)
>> |  ==2909==    by 0x41B3C59: __nss_database_lookup (nsswitch.c:167)
>> |  ==2909==    by 0x59272F8: ???
>> |  ==2909==    by 0x5928CCC: ???
>> |  ==2909==    by 0x416ABA6: getpwuid_r@@GLIBC_2.1.2 (getXXbyYY_r.c:256)
>> |  ==2909==    by 0x416A4ED: getpwuid (getXXbyYY.c:117)
>> |  ==2909==    by 0x439CCB9: do_fileinfo (platform.c:944)
>> |  ==2909==    by 0x43289ED: bcEval (eval.c:4430)
>> |  ==2909==    by 0x4332CA4: Rf_eval (eval.c:397)
>> |  ==2909==    by 0x43377E0: Rf_applyClosure (eval.c:855)
>> |  ==2909==    by 0x432F17E: bcEval (eval.c:4410)
>> |  ==2909==
>> |  ==2909== 160 (40 direct, 120 indirect) bytes in 1 blocks are definitely
>> |  lost in loss record 180 of 1,398
>> |  ==2909==    at 0x4028876: malloc (vg_replace_malloc.c:236)
>> |  ==2909==    by 0x41B364C: nss_parse_service_list (nsswitch.c:626)
>> |  ==2909==    by 0x41B3C59: __nss_database_lookup (nsswitch.c:167)
>> |  ==2909==    by 0x5926148: ???
>> |  ==2909==    by 0x5926F3C: ???
>> |  ==2909==    by 0x41694A6: getgrgid_r@@GLIBC_2.1.2 (getXXbyYY_r.c:256)
>> |  ==2909==    by 0x4168CAD: getgrgid (getXXbyYY.c:117)
>> |  ==2909==    by 0x439CCEB: do_fileinfo (platform.c:947)
>> |  ==2909==    by 0x43289ED: bcEval (eval.c:4430)
>> |  ==2909==    by 0x4332CA4: Rf_eval (eval.c:397)
>> |  ==2909==    by 0x43377E0: Rf_applyClosure (eval.c:855)
>> |  ==2909==    by 0x432F17E: bcEval (eval.c:4410)
>> |  ==2909==
>> |  ==2909== LEAK SUMMARY:
>> |  ==2909==    definitely lost: 80 bytes in 2 blocks
>> |  ==2909==    indirectly lost: 240 bytes in 20 blocks
>> |  ==2909==      possibly lost: 0 bytes in 0 blocks
>> |  ==2909==    still reachable: 18,831,940 bytes in 6,769 blocks
>> 
>> Well I think there is no real bad error here. You lost 80 and 240 bytes. That
>> is nothing to worry about, and somewhat normal. R is a dynamic system,
>> valgrind measures "with some error".
>> 
>> You can compare the result to doing an allocation of a longer vector and not
>> freeing it.
>> 
>> You could see if freeing your 'mean' variable at the end makes a difference,
>> or using a pointer to a single (stack) instance instead of an allocation
>> makes a difference.  Likewise, you could returns bers as well. Or free
>> it. Right now I am not entirely what it is that your 'prediction' function is
>> trying to do.
> 
> It is actually a more complex function with many more allocations and R_tryEval calls; the simplified one that I included leads to exactly the same lost bytes, which means that it is not caused by allocVector, but probably by the  R_tryEval call. Is there any other, more proper way to evaluate R expressions from C?
> 
> In my function I only need to return ber, so I could easily free all R objects (bers and mean here). But C's free() has no result and I am not familiar with any R function that frees objects allocated by allocVector. I have also seen in some R tutorials and examples that objects allocated by allocVector are not freed in any way.
> 

R uses a garbage collector, so I think you're chasing a ghost here... 

But if you look at the output of lost blocks this is simply storage allocated by your glibc for non-reentrant calls so it has nothing to do with R.

Cheers,
Simon


>> 
>> |  Reachable error summary is far too long to include in this mail. The
>> |  interesting thing is that reachable errors are all caused by this small
>> |  function.
>> |
>> |  Any ideas? I would also appreciate any suggestions on how to improve
>> |  the R-C integration in my code.
>> 
>> Have you considered using R and C++ instead, and looked at Rcpp and
>> RInside?
>> 
>>     http://dirk.eddelbuettel.com/code/rcpp.html
>> 
>>     http://dirk.eddelbuettel.com/code/rinside.html
>> 
>> RInside in particular is a lot simpler, at least to me. But some people
>> really want plain C in which case you know which route to take
>> 
>> Dirk
> 
> No, unfortunately I cannot switch to C++, because my program is a part of a bigger C project.
> 
> Thanks for your reply and suggestions,
> Nikos
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 
> 


From lachmann at eva.mpg.de  Fri Apr 20 17:50:24 2012
From: lachmann at eva.mpg.de (ghostwheel)
Date: Fri, 20 Apr 2012 08:50:24 -0700 (PDT)
Subject: [Rd] Using 'dimname names' in aperm() and apply()
In-Reply-To: <alpine.LFD.2.00.1007292052180.20108@gannet.stats.ox.ac.uk>
References: <AANLkTim-ojUA-d7f5uGGS=+UbL6jAkED2GEP=X9pZpwr@mail.gmail.com>
	<alpine.LFD.2.00.1007292052180.20108@gannet.stats.ox.ac.uk>
Message-ID: <1334937024867-4574463.post@n4.nabble.com>

I'm replying here to quite an old thread started by me.

I think the dimnames facility is underused in R.
I'm currently using a 5-dimensional array. It is quite cumbersome to have to
write
y[,,,,"0"]

Since the array does have dimnames,  I would like to be able to say instead
y[tree="0"]

Below is a function that enables this functionality
--
myslice=function( input.array,...){
  args=list(...)
  if( length(names(args))==0 ) {
    d = args
  } else {
    d=dimnames( input.array )
    for(n in names(args)) {
      i=args[[n]]
      if( is.numeric(i)) {
        if( length(d[[n]]) == 0) d[[n]]=i
        else d[[n]]=d[[n]][i]
      } else d[[n]]=args[[n]]
    }
  }
  dd=dim( input.array )
  for(i in seq(along=dd))
    if( length(d[[i]])==0) d[[i]] = seq( dd[i])
  i=sapply(d,length)>1
  new.dimnames=dimnames(input.array)[i]
  d=c(list( input.array ),d)
  ret = do.call("[",d)
  dimnames(ret) = new.dimnames
  ret
}
---
I'm not sure what is the best way to replace "[" for arrays with this
function. I should also implement [<- in the same manner.

One additional feature of this function is that it keeps dimnames.
So, if we have
A=array(1:24,c(2,3,4),dimnames=list(a=c(),b=c(),c=c()))
(btw, why can't I do A=array(1:24,c(a=2,b=3,c=4))?)

Then A[1,,] will give a matrix without dimnames:
--
> A[1,,]
     [,1] [,2] [,3] [,4]
[1,]    1    7   13   19
[2,]    3    9   15   21
[3,]    5   11   17   23
--
Whereas the new function keeps the dimnames:
--
> myslice(A,a=1)
      c
b      [,1] [,2] [,3] [,4]
  [1,]    1    7   13   19
  [2,]    3    9   15   21
  [3,]    5   11   17   23
--

Another function where using dimnames for MARGIN would be slice.index

Thanks!
Michael

--
View this message in context: http://r.789695.n4.nabble.com/Using-dimname-names-in-aperm-and-apply-tp2307013p4574463.html
Sent from the R devel mailing list archive at Nabble.com.


From hb at biostat.ucsf.edu  Fri Apr 20 23:00:08 2012
From: hb at biostat.ucsf.edu (Henrik Bengtsson)
Date: Fri, 20 Apr 2012 14:00:08 -0700
Subject: [Rd] R CMD check: Sys.getenv("R_GSCMD") cannot contain full
 pathname (contrary to docs)
Message-ID: <CAFDcVCTR0YdHxRBQ+YOt=YzLSmj5g27HpvPF1Jk9R9LQ47vLXA@mail.gmail.com>

Hi,

in help("R_GSCMD") it says "R_GSCMD: Optional. The path to
Ghostscript, used by dev2bitmap, bitmap and embedFonts. Consulted when
those functions are invoked.".  However, if 'R_GSCMD' contains a full
pathname to the Ghostscript executable (as above "path" indicates),
e.g.

> Sys.getenv("R_GSCMD")
[1] "C:\\Program Files\\gs\\gs8.71\\bin\\gswin32c.exe"

then 'R CMD check' will report

* checking sizes of PDF files under 'inst/doc' ... NOTE
Unable to find GhostScript executable to run checks on size reduction

Setting 'R_GSCMD' to "gswin32c.exe" and asserting that it is the PATH
(this is on Windows), it works.


TROUBLESHOOTING:
In the local function check_doc_size() of tools:::.check_packages()
[cf. http://svn.r-project.org/R/trunk/src/library/tools/R/check.R],
the Ghostscript executable is retrieved as:

 gs_cmd <- find_gs_cmd(Sys.getenv("R_GSCMD", ""))

and if nzchar(gs_cmd) is FALSE, then the 'R CMD check' NOTE appears.
Inspecting tools:::find_gs_cmd, this corresponds to

 gs_cmd <- Sys.which(Sys.getenv("R_GSCMD", ""))

iff 'R_GSCMD' is set.  In other words, tools:::find_gs_cmd() assumes a
basename not a full pathname.


SUGGESTIONS:
Update tools:::find_gs_cmd() to test for file existence before turning
to Sys.which(), e.g.

find_gs_cmd <- function (gs_cmd)
{
    if (!nzchar(gs_cmd)) {
        if (.Platform$OS.type == "windows") {
            gs_cmd <- Sys.which("gswin64c")
            if (!nzchar(gs_cmd))
                gs_cmd <- Sys.which("gswin32c")
            gs_cmd
        }
        else Sys.which("gs")
    } else {
      if (file.exists(gs_cmd) && !file.info(gs_cmd)$isdir)
        gs_cmd
      else
        Sys.which(gs_cmd)
   }
}


/Henrik

> sessionInfo()
R version 2.15.0 Patched (2012-04-08 r58935)
Platform: x86_64-pc-mingw32/x64 (64-bit)

locale:
[1] LC_COLLATE=English_United States.1252
[2] LC_CTYPE=English_United States.1252
[3] LC_MONETARY=English_United States.1252
[4] LC_NUMERIC=C
[5] LC_TIME=English_United States.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

loaded via a namespace (and not attached):
[1] R.methodsS3_1.4.0 tools_2.15.0


From patrick.aboyoun at oracle.com  Sat Apr 21 02:55:15 2012
From: patrick.aboyoun at oracle.com (Patrick Aboyoun)
Date: Fri, 20 Apr 2012 17:55:15 -0700
Subject: [Rd] combining large list of data.frames
In-Reply-To: <4F9092F5.8070705@vanderbilt.edu>
References: <4F9092F5.8070705@vanderbilt.edu>
Message-ID: <4F920573.9080005@oracle.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20120420/f277ac71/attachment.pl>

From armstrong.whit at gmail.com  Sat Apr 21 15:18:03 2012
From: armstrong.whit at gmail.com (Whit Armstrong)
Date: Sat, 21 Apr 2012 09:18:03 -0400
Subject: [Rd] R CMD check -- non S3 method warning
Message-ID: <CAMi=pg43sztz0rX-DVpDTbv4S3WhbtvmYpe7YMWcAjGdcg9uwQ@mail.gmail.com>

I'm trying to R CMD check a package, and I'm getting the 'checking S3
generic/method consistency' warning.

I have written a function 'gamma.dist' which is _not_ an S3 method,
but happens to collide with the 'gamma' function from the R::base
namespace.

Is there a way of telling CMD check that the name is intentional and
is not meant to be an S3 method?

My function is defined as follows:

gamma.dist <- function(x,alpha,beta,observed=FALSE) {
    attr(x,"distributed") <- "gamma"
    attr(x,"alpha") <- substitute(alpha)
    attr(x,"beta") <- substitute(beta)
    attr(x,"observed") <- observed
    attr(x,"env") <- new.env()
    class(x) <- "mcmc.object"
    x
}


with corresponding man page:
\name{normal.dist}
\alias{normal.dist}
\alias{uniform.dist}
\alias{gamma.dist}
\alias{bernoulli.dist}
\alias{binomial.dist}
...
...

\usage{
normal.dist(x, mu, tau, observed = FALSE)
uniform.dist(x, lower, upper, observed = FALSE)
gamma.dist(x, alpha, beta, observed = FALSE)
bernoulli.dist(x, p, observed = FALSE)
binomial.dist(x, n, p, observed = FALSE)
}

I am reluctant to change the name of the function b/c all my
distribution methods use the same convention of 'distribution
type'.dist.

Do CRAN maintainers view this warning as a blocker for the package
(full make check output below, which includes a few additional
warnings I'm in the process of fixing)?

Thanks,
Whit



* checking for LF line-endings in source and make files
* checking for empty or unneeded directories
* building ?rcppbugs_0.0.1.tar.gz?

* using log directory ?/home/warmstrong/dvl/R.packages/rcppbugs.Rcheck?
* using R version 2.15.0 (2012-03-30)
* using platform: x86_64-pc-linux-gnu (64-bit)
* using session charset: UTF-8
* checking for file ?rcppbugs/DESCRIPTION? ... OK
* this is package ?rcppbugs? version ?0.0.1?
* checking package namespace information ... OK
* checking package dependencies ... OK
* checking if this is a source package ... OK
* checking if there is a namespace ... OK
* checking for executable files ... OK
* checking whether package ?rcppbugs? can be installed ... OK
* checking installed package size ... NOTE
  installed size is  8.4Mb
  sub-directories of 1Mb or more:
    libs   8.1Mb
* checking package directory ... OK
* checking for portable file names ... OK
* checking for sufficient/correct file permissions ... OK
* checking DESCRIPTION meta-information ... OK
* checking top-level files ... OK
* checking index information ... OK
* checking package subdirectories ... OK
* checking R files for non-ASCII characters ... OK
* checking R files for syntax errors ... OK
* checking whether the package can be loaded ... OK
* checking whether the package can be loaded with stated dependencies ... OK
* checking whether the package can be unloaded cleanly ... OK
* checking whether the namespace can be loaded with stated dependencies ... OK
* checking whether the namespace can be unloaded cleanly ... OK
* checking for unstated dependencies in R code ... OK
* checking S3 generic/method consistency ... WARNING
gamma:
  function(x)
gamma.dist:
  function(x, alpha, beta, observed)

See section ?Generic functions and methods? of the ?Writing R
Extensions? manual.
* checking replacement functions ... OK
* checking foreign function calls ... OK
* checking R code for possible problems ... OK
* checking Rd files ... OK
* checking Rd metadata ... OK
* checking Rd cross-references ... OK
* checking for missing documentation entries ... OK
* checking for code/documentation mismatches ... OK
* checking Rd \usage sections ... NOTE
S3 methods shown with full name in documentation object 'normal.dist':
  ?gamma.dist?

The \usage entries for S3 methods should use the \method markup and
not their full name.
See the chapter ?Writing R documentation files? in the ?Writing R
Extensions? manual.
* checking Rd contents ... OK
* checking for unstated dependencies in examples ... OK
* checking line endings in C/C++/Fortran sources/headers ... OK
* checking line endings in Makefiles ... OK
* checking for portable compilation flags in Makevars ... WARNING
Non-portable flags in variable ?PKG_CXXFLAGS?:
  -Wall -std=c++0x
* checking for portable use of $(BLAS_LIBS) and $(LAPACK_LIBS) ... OK
* checking compiled code ... OK
* checking examples ... OK
* checking PDF version of manual ... OK

WARNING: There were 2 warnings.
NOTE: There were 2 notes.
See
  ?/home/warmstrong/dvl/R.packages/rcppbugs.Rcheck/00check.log?
for details.

warmstrong at krypton:~/dvl/R.packages$ R
> gamma
function (x)  .Primitive("gamma")


From mxkuhn at gmail.com  Sat Apr 21 16:28:25 2012
From: mxkuhn at gmail.com (Max Kuhn)
Date: Sat, 21 Apr 2012 10:28:25 -0400
Subject: [Rd] csv version of data in an R object
Message-ID: <CAJ9CoW=rvNmPoAkb5d1qUdLL+RJMgUt2kAh82WWKs9W1mEH+xw@mail.gmail.com>

For a package, I need to write a csv version of a data set to an R
object. Right now, I use:

    out <- capture.output(
                          write.table(x,
                                      sep = ",",
                                      na = "?",
                                      file = "",
                                      quote = FALSE,
                                      row.names = FALSE,
                                      col.names = FALSE))

To me, this is fairly slow; 131 seconds for a data frame with 8100
rows and 1400 columns.

The data will be in a data frame; I know write.table() would be faster
with a matrix. I was looking into converting the data frame to a
character matrix using as.matrix() or, better yet, format() prior to
the call above. However, I'm not sure what an appropriate value of
'digits' should be so that the character version of numeric data has
acceptable fidelity.

I also tried using a text connection and sink() as shown in
?textConnection but there was no speedup.

Any suggestions or ideas?

Thanks,

Max


> sessionInfo()
R version 2.14.0 (2011-10-31)
Platform: x86_64-apple-darwin9.8.0/x86_64 (64-bit)

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

loaded via a namespace (and not attached):
[1] tools_2.14.0


From ripley at stats.ox.ac.uk  Sat Apr 21 16:49:35 2012
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 21 Apr 2012 15:49:35 +0100
Subject: [Rd] R CMD check -- non S3 method warning
In-Reply-To: <CAMi=pg43sztz0rX-DVpDTbv4S3WhbtvmYpe7YMWcAjGdcg9uwQ@mail.gmail.com>
References: <CAMi=pg43sztz0rX-DVpDTbv4S3WhbtvmYpe7YMWcAjGdcg9uwQ@mail.gmail.com>
Message-ID: <4F92C8FF.3040505@stats.ox.ac.uk>

On 21/04/2012 14:18, Whit Armstrong wrote:
> I'm trying to R CMD check a package, and I'm getting the 'checking S3
> generic/method consistency' warning.
>
> I have written a function 'gamma.dist' which is _not_ an S3 method,
> but happens to collide with the 'gamma' function from the R::base
> namespace.
>
> Is there a way of telling CMD check that the name is intentional and
> is not meant to be an S3 method?

More importantly, there is no way to tell the generic, and there is a 
"dist" class in use in the 'stats' package.

>
> My function is defined as follows:
>
> gamma.dist<- function(x,alpha,beta,observed=FALSE) {
>      attr(x,"distributed")<- "gamma"
>      attr(x,"alpha")<- substitute(alpha)
>      attr(x,"beta")<- substitute(beta)
>      attr(x,"observed")<- observed
>      attr(x,"env")<- new.env()
>      class(x)<- "mcmc.object"
>      x
> }
>
>
> with corresponding man page:
> \name{normal.dist}
> \alias{normal.dist}
> \alias{uniform.dist}
> \alias{gamma.dist}
> \alias{bernoulli.dist}
> \alias{binomial.dist}
> ...
> ...
>
> \usage{
> normal.dist(x, mu, tau, observed = FALSE)
> uniform.dist(x, lower, upper, observed = FALSE)
> gamma.dist(x, alpha, beta, observed = FALSE)
> bernoulli.dist(x, p, observed = FALSE)
> binomial.dist(x, n, p, observed = FALSE)
> }
>
> I am reluctant to change the name of the function b/c all my
> distribution methods use the same convention of 'distribution
> type'.dist.
>
> Do CRAN maintainers view this warning as a blocker for the package
> (full make check output below, which includes a few additional
> warnings I'm in the process of fixing)?

That's a question for them, not R-devel, isn't it?

>
> Thanks,
> Whit
>
>
>
> * checking for LF line-endings in source and make files
> * checking for empty or unneeded directories
> * building ?rcppbugs_0.0.1.tar.gz?
>
> * using log directory ?/home/warmstrong/dvl/R.packages/rcppbugs.Rcheck?
> * using R version 2.15.0 (2012-03-30)
> * using platform: x86_64-pc-linux-gnu (64-bit)
> * using session charset: UTF-8
> * checking for file ?rcppbugs/DESCRIPTION? ... OK
> * this is package ?rcppbugs? version ?0.0.1?
> * checking package namespace information ... OK
> * checking package dependencies ... OK
> * checking if this is a source package ... OK
> * checking if there is a namespace ... OK
> * checking for executable files ... OK
> * checking whether package ?rcppbugs? can be installed ... OK
> * checking installed package size ... NOTE
>    installed size is  8.4Mb
>    sub-directories of 1Mb or more:
>      libs   8.1Mb
> * checking package directory ... OK
> * checking for portable file names ... OK
> * checking for sufficient/correct file permissions ... OK
> * checking DESCRIPTION meta-information ... OK
> * checking top-level files ... OK
> * checking index information ... OK
> * checking package subdirectories ... OK
> * checking R files for non-ASCII characters ... OK
> * checking R files for syntax errors ... OK
> * checking whether the package can be loaded ... OK
> * checking whether the package can be loaded with stated dependencies ... OK
> * checking whether the package can be unloaded cleanly ... OK
> * checking whether the namespace can be loaded with stated dependencies ... OK
> * checking whether the namespace can be unloaded cleanly ... OK
> * checking for unstated dependencies in R code ... OK
> * checking S3 generic/method consistency ... WARNING
> gamma:
>    function(x)
> gamma.dist:
>    function(x, alpha, beta, observed)
>
> See section ?Generic functions and methods? of the ?Writing R
> Extensions? manual.
> * checking replacement functions ... OK
> * checking foreign function calls ... OK
> * checking R code for possible problems ... OK
> * checking Rd files ... OK
> * checking Rd metadata ... OK
> * checking Rd cross-references ... OK
> * checking for missing documentation entries ... OK
> * checking for code/documentation mismatches ... OK
> * checking Rd \usage sections ... NOTE
> S3 methods shown with full name in documentation object 'normal.dist':
>    ?gamma.dist?
>
> The \usage entries for S3 methods should use the \method markup and
> not their full name.
> See the chapter ?Writing R documentation files? in the ?Writing R
> Extensions? manual.
> * checking Rd contents ... OK
> * checking for unstated dependencies in examples ... OK
> * checking line endings in C/C++/Fortran sources/headers ... OK
> * checking line endings in Makefiles ... OK
> * checking for portable compilation flags in Makevars ... WARNING
> Non-portable flags in variable ?PKG_CXXFLAGS?:
>    -Wall -std=c++0x
> * checking for portable use of $(BLAS_LIBS) and $(LAPACK_LIBS) ... OK
> * checking compiled code ... OK
> * checking examples ... OK
> * checking PDF version of manual ... OK
>
> WARNING: There were 2 warnings.
> NOTE: There were 2 notes.
> See
>    ?/home/warmstrong/dvl/R.packages/rcppbugs.Rcheck/00check.log?
> for details.
>
> warmstrong at krypton:~/dvl/R.packages$ R
>> gamma
> function (x)  .Primitive("gamma")
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ggrothendieck at gmail.com  Sat Apr 21 19:03:28 2012
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 21 Apr 2012 13:03:28 -0400
Subject: [Rd] Problem with args
Message-ID: <CAP01uR=U_Dy8hU3YV6DQJF5oG94rMs1K--m-2607E_5nkCAkSA@mail.gmail.com>

args ought to check that its argument is a function:

> max <- 3
> args(max)
NULL

e.g.

> args <- function(name) {
+     name <- match.fun(name)
+     base::args(name)
+ }
> args(max)
function (..., na.rm = FALSE)
NULL


-- 
Statistics & Software Consulting
GKX Group, GKX Associates Inc.
tel: 1-877-GKX-GROUP
email: ggrothendieck at gmail.com


From simon.urbanek at r-project.org  Sat Apr 21 19:38:51 2012
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Sat, 21 Apr 2012 13:38:51 -0400
Subject: [Rd] Problem with args
In-Reply-To: <CAP01uR=U_Dy8hU3YV6DQJF5oG94rMs1K--m-2607E_5nkCAkSA@mail.gmail.com>
References: <CAP01uR=U_Dy8hU3YV6DQJF5oG94rMs1K--m-2607E_5nkCAkSA@mail.gmail.com>
Message-ID: <991E37E4-561E-4482-AAEB-3BF400FC14F2@r-project.org>


On Apr 21, 2012, at 1:03 PM, Gabor Grothendieck wrote:

> args ought to check that its argument is a function:
> 
>> max <- 3
>> args(max)
> NULL
> 
> e.g.
> 
>> args <- function(name) {
> +     name <- match.fun(name)
> +     base::args(name)
> + }
>> args(max)
> function (..., na.rm = FALSE)
> NULL
> 

You may want to read the docs in the first place:

Value:
[...]

     'NULL' in case of a non-function.

Cheers,
S


From ggrothendieck at gmail.com  Sat Apr 21 19:45:24 2012
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 21 Apr 2012 13:45:24 -0400
Subject: [Rd] Problem with args
In-Reply-To: <991E37E4-561E-4482-AAEB-3BF400FC14F2@r-project.org>
References: <CAP01uR=U_Dy8hU3YV6DQJF5oG94rMs1K--m-2607E_5nkCAkSA@mail.gmail.com>
	<991E37E4-561E-4482-AAEB-3BF400FC14F2@r-project.org>
Message-ID: <CAP01uRmGjG3CMegARah6O==2wYOu6vGX1MRr5+-7H0SuwVuarQ@mail.gmail.com>

On Sat, Apr 21, 2012 at 1:38 PM, Simon Urbanek
<simon.urbanek at r-project.org> wrote:
>
> On Apr 21, 2012, at 1:03 PM, Gabor Grothendieck wrote:
>
>> args ought to check that its argument is a function:
>>
>>> max <- 3
>>> args(max)
>> NULL
>>
>> e.g.
>>
>>> args <- function(name) {
>> + ? ? name <- match.fun(name)
>> + ? ? base::args(name)
>> + }
>>> args(max)
>> function (..., na.rm = FALSE)
>> NULL
>>
>
> You may want to read the docs in the first place:
>
> Value:
> [...]
>
> ? ? 'NULL' in case of a non-function.
>

My post wasn't about the return value -- it was about the fact that
the function name can be masked.  Try the example I posted.  I also
posted a solution.

-- 
Statistics & Software Consulting
GKX Group, GKX Associates Inc.
tel: 1-877-GKX-GROUP
email: ggrothendieck at gmail.com


From simon.urbanek at r-project.org  Sat Apr 21 20:10:51 2012
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Sat, 21 Apr 2012 14:10:51 -0400
Subject: [Rd] Problem with args
In-Reply-To: <CAP01uRmGjG3CMegARah6O==2wYOu6vGX1MRr5+-7H0SuwVuarQ@mail.gmail.com>
References: <CAP01uR=U_Dy8hU3YV6DQJF5oG94rMs1K--m-2607E_5nkCAkSA@mail.gmail.com>
	<991E37E4-561E-4482-AAEB-3BF400FC14F2@r-project.org>
	<CAP01uRmGjG3CMegARah6O==2wYOu6vGX1MRr5+-7H0SuwVuarQ@mail.gmail.com>
Message-ID: <9B374D55-4AC3-423D-AB64-620693B69A7D@r-project.org>


On Apr 21, 2012, at 1:45 PM, Gabor Grothendieck wrote:

> On Sat, Apr 21, 2012 at 1:38 PM, Simon Urbanek
> <simon.urbanek at r-project.org> wrote:
>> 
>> On Apr 21, 2012, at 1:03 PM, Gabor Grothendieck wrote:
>> 
>>> args ought to check that its argument is a function:
>>> 
>>>> max <- 3
>>>> args(max)
>>> NULL
>>> 
>>> e.g.
>>> 
>>>> args <- function(name) {
>>> +     name <- match.fun(name)
>>> +     base::args(name)
>>> + }
>>>> args(max)
>>> function (..., na.rm = FALSE)
>>> NULL
>>> 
>> 
>> You may want to read the docs in the first place:
>> 
>> Value:
>> [...]
>> 
>>     'NULL' in case of a non-function.
>> 
> 
> My post wasn't about the return value -- it was about the fact that the function name can be masked.  Try the example I posted.  I also posted a solution.
> 

You're passing a non-function (the value of 3) so, obviously you get NULL. I think you're confusing function calls with values. What you probably intended was

> args("max")
function (..., na.rm = FALSE) 
NULL

(again, see the documentation).


From ggrothendieck at gmail.com  Sat Apr 21 20:18:50 2012
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 21 Apr 2012 14:18:50 -0400
Subject: [Rd] Problem with args
In-Reply-To: <9B374D55-4AC3-423D-AB64-620693B69A7D@r-project.org>
References: <CAP01uR=U_Dy8hU3YV6DQJF5oG94rMs1K--m-2607E_5nkCAkSA@mail.gmail.com>
	<991E37E4-561E-4482-AAEB-3BF400FC14F2@r-project.org>
	<CAP01uRmGjG3CMegARah6O==2wYOu6vGX1MRr5+-7H0SuwVuarQ@mail.gmail.com>
	<9B374D55-4AC3-423D-AB64-620693B69A7D@r-project.org>
Message-ID: <CAP01uRk_UgVOkg5ULr7wsZd0-C64HRet+ewDadPBLuGbcyxx=w@mail.gmail.com>

On Sat, Apr 21, 2012 at 2:10 PM, Simon Urbanek
<simon.urbanek at r-project.org> wrote:
>
> On Apr 21, 2012, at 1:45 PM, Gabor Grothendieck wrote:
>
>> On Sat, Apr 21, 2012 at 1:38 PM, Simon Urbanek
>> <simon.urbanek at r-project.org> wrote:
>>>
>>> On Apr 21, 2012, at 1:03 PM, Gabor Grothendieck wrote:
>>>
>>>> args ought to check that its argument is a function:
>>>>
>>>>> max <- 3
>>>>> args(max)
>>>> NULL
>>>>
>>>> e.g.
>>>>
>>>>> args <- function(name) {
>>>> + ? ? name <- match.fun(name)
>>>> + ? ? base::args(name)
>>>> + }
>>>>> args(max)
>>>> function (..., na.rm = FALSE)
>>>> NULL
>>>>
>>>
>>> You may want to read the docs in the first place:
>>>
>>> Value:
>>> [...]
>>>
>>> ? ? 'NULL' in case of a non-function.
>>>
>>
>> My post wasn't about the return value -- it was about the fact that the function name can be masked. ?Try the example I posted. ?I also posted a solution.
>>
>
> You're passing a non-function (the value of 3) so, obviously you get NULL. I think you're confusing function calls with values. What you probably intended was
>

That's only a workaround but it does not address the problem that args
ought not to work that way.  args can gather all the information
needed to fetch the correct object but doesn't.  The second example
shows how it should work.

-- 
Statistics & Software Consulting
GKX Group, GKX Associates Inc.
tel: 1-877-GKX-GROUP
email: ggrothendieck at gmail.com


From b.rowlingson at lancaster.ac.uk  Sat Apr 21 21:59:50 2012
From: b.rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Sat, 21 Apr 2012 20:59:50 +0100
Subject: [Rd] csv version of data in an R object
In-Reply-To: <CAJ9CoW=rvNmPoAkb5d1qUdLL+RJMgUt2kAh82WWKs9W1mEH+xw@mail.gmail.com>
References: <CAJ9CoW=rvNmPoAkb5d1qUdLL+RJMgUt2kAh82WWKs9W1mEH+xw@mail.gmail.com>
Message-ID: <CANVKczNwQA77MS2e_buw_nmwNEbP-a-Rn3hVqb2Pdz93ctnoDQ@mail.gmail.com>

On Sat, Apr 21, 2012 at 3:28 PM, Max Kuhn <mxkuhn at gmail.com> wrote:
> For a package, I need to write a csv version of a data set to an R
> object. Right now, I use:
>
> ? ?out <- capture.output(
> ? ? ? ? ? ? ? ? ? ? ? ? ?write.table(x,
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?sep = ",",
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?na = "?",
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?file = "",
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?quote = FALSE,
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?row.names = FALSE,
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?col.names = FALSE))
>
> To me, this is fairly slow; 131 seconds for a data frame with 8100
> rows and 1400 columns.
>
> The data will be in a data frame; I know write.table() would be faster
> with a matrix. I was looking into converting the data frame to a
> character matrix using as.matrix() or, better yet, format() prior to
> the call above. However, I'm not sure what an appropriate value of
> 'digits' should be so that the character version of numeric data has
> acceptable fidelity.
>
> I also tried using a text connection and sink() as shown in
> ?textConnection but there was no speedup.
>

 You could try a loop over each row, and use 'paste' to join each
element in a row by commas. Then use 'paste' again to join everything
you've got (a vector of rows) by a '\n' character.

something like: paste(apply(x,1,paste,collapse=","),collapse="\n")   # untested

you probably also want to stick a final \n on it.

Is it faster? I don't know!

Barry


From chrisstevensphotography at ihug.co.nz  Sun Apr 22 02:49:14 2012
From: chrisstevensphotography at ihug.co.nz (chris90nz)
Date: Sat, 21 Apr 2012 17:49:14 -0700 (PDT)
Subject: [Rd] Sending lists using .C
Message-ID: <1335055754110-4577449.post@n4.nabble.com>

Hi,

I posted around a week ago and I will try this time with a more detailed
description.

I am converting R code to C code to boost speed as currently it takes a few
days to run. Now I will try describe this with as detail as possible.

I have a list of size 47. Each element of the list is itself a list of size
7, and each of these lists are arbitrarily long lists of integers.

I.e. If my main list is called Gcompat, then I could have Gcompat[[1]][[1]]
= 5, and Gcompat[[1]][[3]] = 1, 3, 6.

So Gcompat[[i]][[j]] = list of integers, where 1<=i<=47, 1<=j<=7. 

I have written the following code which will let me pass it through using .C
as two vectors, one containing the numbers contained in the lists all
concatenated, the other containing the indicies at which each list starts.

#########
Gcompat_values = vector()
Gcompat_lengths = vector()
Gcompat_indicies = as.vector(0)
   
for(i in 1:S) {
  for (j in 1:L) {
    Gcompat_values = c(Gcompat_values, Gcompat[[i]][[j]])
    Gcompat_lengths = c(Gcompat_lengths, length(Gcompat[[i]][[j]]))
  }
}
   
for (i in 2:(length(Gcompat_lengths)+1)) {
  Gcompat_indicies[i] = Gcompat_indicies[i-1]+Gcompat_lengths[i-1]
}
########

This approach works, but is still slow (10,000 times doing this takes
~30seconds).

I know you can pass a list through by having the C function take as a
parameter SEXP*. However I am not familiar with using Rinternals.h and am a
little lost as to how I would access elements of my big list if I just pass
the whole thing through to a SEXP*.

The other thing is I have 2 lists to pass through like this, and another
with an arbitrary sized matrix instead of a list of integers, so I think the
only way I can do this efficiently is if I do pass these lists through to a
SEXP*.

Any thoughts would be greatly appreciated.

Cheers,

Chris 

--
View this message in context: http://r.789695.n4.nabble.com/Sending-lists-using-C-tp4577449p4577449.html
Sent from the R devel mailing list archive at Nabble.com.


From brian at braverock.com  Sun Apr 22 12:08:53 2012
From: brian at braverock.com (Brian G. Peterson)
Date: Sun, 22 Apr 2012 05:08:53 -0500
Subject: [Rd] Sending lists using .C
In-Reply-To: <1335055754110-4577449.post@n4.nabble.com>
References: <1335055754110-4577449.post@n4.nabble.com>
Message-ID: <1335089333.17905.94.camel@brian-desktop>

On Sat, 2012-04-21 at 17:49 -0700, chris90nz wrote:
<...>
> I have written the following code which will let me pass it through
> using .C
> as two vectors, one containing the numbers contained in the lists all
> concatenated, the other containing the indicies at which each list
> starts.
<...>
> This approach works, but is still slow (10,000 times doing this takes
> ~30seconds).
> 
> I know you can pass a list through by having the C function take as a
> parameter SEXP*. However I am not familiar with using Rinternals.h and
> am a little lost as to how I would access elements of my big list if I
> just pass the whole thing through to a SEXP*.
> 
> The other thing is I have 2 lists to pass through like this, and
> another with an arbitrary sized matrix instead of a list of integers,
> so I think the only way I can do this efficiently is if I do pass
> these lists through to a SEXP*.

The fact that you admit to being  'a little lost' suggests that you
should use a higher level interface.  The list archives here have a
wealth of information, most recently a very good summary of the more
modern .Call interface here:
http://www.mail-archive.com/r-devel at r-project.org/msg26578.html
and here:
http://www.mail-archive.com/r-devel at r-project.org/msg26631.html

as an even higher level interface, perhaps you should consider RCpp,
which contains even more data structures for handling large R data
structures, and more hand holding.  Reference slides here:
http://dirk.eddelbuettel.com/bio/presentations.html
(see especially the full day workshop from 2010)

Regards,

   - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From gmbecker at ucdavis.edu  Sun Apr 22 18:42:57 2012
From: gmbecker at ucdavis.edu (Gabriel Becker)
Date: Sun, 22 Apr 2012 09:42:57 -0700
Subject: [Rd] csv version of data in an R object
In-Reply-To: <CANVKczNwQA77MS2e_buw_nmwNEbP-a-Rn3hVqb2Pdz93ctnoDQ@mail.gmail.com>
References: <CAJ9CoW=rvNmPoAkb5d1qUdLL+RJMgUt2kAh82WWKs9W1mEH+xw@mail.gmail.com>
	<CANVKczNwQA77MS2e_buw_nmwNEbP-a-Rn3hVqb2Pdz93ctnoDQ@mail.gmail.com>
Message-ID: <CADwqtCMCYr2EhGH9GFZs6U_9woJ1fDmuH2AzMrYe-AKTWckZwA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20120422/1c2f6a10/attachment.pl>

From lachmann at eva.mpg.de  Mon Apr 23 15:24:15 2012
From: lachmann at eva.mpg.de (ghostwheel)
Date: Mon, 23 Apr 2012 06:24:15 -0700 (PDT)
Subject: [Rd] I wish xlim=c(0,
	NA) would work. How about I send you a patch?
In-Reply-To: <CAFEqCdzg13nc_HvuKCA0xiS1F=YqKKvz4VKXn8_4cfqUmGABNg@mail.gmail.com>
References: <CAErODj9RJ60Xdp=sLu=pGRiBOY_CSsxnjUmqObxhHYtEhHyLxA@mail.gmail.com>
	<CAFEqCdzg13nc_HvuKCA0xiS1F=YqKKvz4VKXn8_4cfqUmGABNg@mail.gmail.com>
Message-ID: <1335187455478-4580388.post@n4.nabble.com>

I have also often longed for such a shortcut.

The problem is that most often, my plot statements do not look like this:
plot(y)

but instead like this:
plot( some_very_long_expression_involving(x),
some_other_very_long_expression_involving(x) )

And since I'm working with a GUI, I often go up and change this or that
expression.
Then it starts to be ugly to have to work with
range(pretty(ugly_expressions())). Of course, I could always do
x=some_very_long_expression, y=some_other_very_long_expression; plot(x,y)

Another possible solution would be to use:
plot(x,y, ylim=c(min=9) )
in order to identify which limit I'd like to set. That shouldn't usually
mess up anything, right? 

Michael

--
View this message in context: http://r.789695.n4.nabble.com/I-wish-xlim-c-0-NA-would-work-How-about-I-send-you-a-patch-tp4562269p4580388.html
Sent from the R devel mailing list archive at Nabble.com.


From ripley at stats.ox.ac.uk  Mon Apr 23 17:39:00 2012
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 23 Apr 2012 16:39:00 +0100
Subject: [Rd] url, readLines, source behind a proxy
In-Reply-To: <CAPPM_gS7LA-rv=__y_QbUi4ketxT9dn3PU=6dm+Sv9q5nU8nkw@mail.gmail.com>
References: <4F8D234E.1070709@cbio.uct.ac.za>
	<CAFDcVCQx1R7gJYnOPC3yzayJxi-Ts9zzaQd5T0bFSnLcOv8nCg@mail.gmail.com>
	<Pine.LNX.4.64.1204180705060.27275@mancala.cbio.uct.ac.za>
	<CAPPM_gS7LA-rv=__y_QbUi4ketxT9dn3PU=6dm+Sv9q5nU8nkw@mail.gmail.com>
Message-ID: <4F957794.5090806@stats.ox.ac.uk>

On 18/04/2012 16:04, Joshua Ulrich wrote:
> Hi Renaud,
>
> On Wed, Apr 18, 2012 at 12:22 AM, Renaud Gaujoux
> <renaud at mancala.cbio.uct.ac.za>  wrote:
>> Hi Henrik,
>>
> <snip>
>>
>> Could anybody behind a proxy check if the issue can be reproduced?
>> My proxy is in fact provided by cntml, which acts as a local proxy that
>> takes care of tricky authentication protocols with the actual university
>> proxy, not natively supported by my system (Ubuntu). Anybody in this case?
>>
> I can replicate this on a WinXP system, where I normally have to use
> the --internet2 flag to get internet access through a proxy.
>
> ?download.file has a section on "Setting Proxies", which describes how
> to use environment variables to set proxy information.  Setting
> http_proxy='http://my.proxy.com/' was enough for me to get R CMD
> check to run successfully with the --as-cran flag.

I guess that the simplest way on Windows is to ensure that --internet2 
is set.  In R-patched there is a new environment variable 
R_WIN_INTERNET2 which lets you do that (set it in ~/.R/check.Renviron).

[Setting proxies is so 20th century -- even moderately competent 
sysadmins worked out how to use transparent caching proxies ca 1995. 
Which is why the R developers give it a low priority.]


>
>> Thanks.
>> Renaud
>>
>
> Best,
> --
> Joshua Ulrich  |  FOSS Trading: www.fosstrading.com
>
> R/Finance 2012: Applied Finance with R
> www.RinFinance.com
>
>
>> On Tue, 17 Apr 2012, Henrik Bengtsson wrote:
>>
>>> On Tue, Apr 17, 2012 at 1:01 AM, Renaud Gaujoux
>>> <renaud at mancala.cbio.uct.ac.za>  wrote:
>>>> Hi,
>>>>
>>>> when I run R CMD check with flag --as-cran, the process hangs at stage:
>>>>
>>>> * checking CRAN incoming feasibility ...
>>>
>>> Doesn't it time-out eventually?  I'm not behind a proxy but when I've
>>> been running 'R CMD check' whenon very poor 3G connection, it had
>>> eventually timed out.
>>>
>>> /Henrik
>>>
>>>>
>>>> I am pretty sure it is a proxy issue.
>>>> I looked at the check code in the tools package and it seems that the issue
>>>> is in the local function `.repository_db()` (defined in
>>>> `tools:::.check_package_CRAN_incoming()`), which eventually calls `url()`
>>>> with argument open="rb", that hangs probably because it does not use the
>>>> proxy settings.
>>>> I had a similar issue with `source()`, which apparently uses internal
>>>> network functions (not as download.file), but is supposed to work behind a
>>>> proxy (correct?).
>>>> Does anybody else have this problem?
>>>>
>>>> I was wondering if there is a way around, as I would like to be able to use
>>>> --as-cran for my checks.
>>>> Thank you.
>>>>
>>>> Renaud
>>>>
>>>> --
>>>> Renaud Gaujoux
>>>> Computational Biology - University of Cape Town
>>>> South Africa
>>>>
>>>> ______________________________________________
>>>> R-devel at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From cole.beck at Vanderbilt.Edu  Mon Apr 23 19:07:43 2012
From: cole.beck at Vanderbilt.Edu (Cole Beck)
Date: Mon, 23 Apr 2012 12:07:43 -0500
Subject: [Rd] combining large list of data.frames
In-Reply-To: <4F920573.9080005@oracle.com>
References: <4F9092F5.8070705@vanderbilt.edu> <4F920573.9080005@oracle.com>
Message-ID: <4F958C5F.6060008@vanderbilt.edu>

Thanks Patrick, this is a nice solution.  Regarding a patch I'm inclined 
to believe you're correct, though it is certainly something to consider.

Cheers,
Cole

On 04/20/2012 07:55 PM, Patrick Aboyoun wrote:
> Cole,
> Bioconductor's high throughput sequencing infrastructure package IRanges
> contains code that may be useful for speeding up base::rbind.data.frame.
> I've extracted the salient bits from that rbind method, but left the
> corner case handling code out. IRanges's rbind method took the approach
> of treating a data set as a list of equal length columns, and so it
> contains a number of lapplys and vector concatenation calls. Given that
> base::rbind.data.frame sits at the core of many operations, I'm not sure
> if patches would be accepted for it, but I could take a crack at it.
>
> biocRBind <- function(..., deparse.level=1)
> {
> ## Simplified version of IRanges's rbind method for DataFrame
> ## Removed all data checks, ignored row names
> args <- list(...)
> df <- args[[1L]]
> cn <- colnames(df)
> cl <- unlist(lapply(as.list(df, use.names = FALSE), class))
> factors <- unlist(lapply(as.list(df, use.names = FALSE), is.factor))
> cols <- lapply(seq_len(length(df)), function(i) {
> cols <- lapply(args, `[[`, cn[i])
> if (factors[i]) { # combine factor levels, coerce to character
> levs <- unique(unlist(lapply(cols, levels), use.names=FALSE))
> cols <- lapply(cols, as.character)
> }
> combined <- do.call(c, unname(cols))
> if (factors[i])
> combined <- factor(combined, levs)
> as(combined, cl[i])
> })
> names(cols) <- colnames(df)
> do.call(data.frame, cols)
> }
>
> # create list of data.frames
> set.seed(123)
> dat <- vector("list", 20000)
> for(i in seq_along(dat)) {
> size <- sample(1:30, 1)
> dat[[i]] <- data.frame(id=rep(i, size), value=rnorm(size),
> letter=sample(LETTERS, size, replace=TRUE), ind=sample(c(TRUE,FALSE),
> size, replace=TRUE))
> }
>
> # sample runs
>  > system.time(do.call(biocRBind, dat))
> user system elapsed
> 2.120 0.000 2.125
>  > system.time(do.call(biocRBind, dat))
> user system elapsed
> 2.092 0.000 2.091
>  > system.time(do.call(biocRBind, dat))
> user system elapsed
> 2.080 0.000 2.077
>  > sessionInfo()
> R Under development (unstable) (2012-04-19 r59111)
> Platform: x86_64-unknown-linux-gnu (64-bit)
>
> locale:
> [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C
> [3] LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8
> [5] LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8
> [7] LC_PAPER=C LC_NAME=C
> [9] LC_ADDRESS=C LC_TELEPHONE=C
> [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C
>
> attached base packages:
> [1] stats graphics grDevices utils datasets methods base
>
> loaded via a namespace (and not attached):
> [1] tools_2.16.0
>
>
> Cheers,
> Patrick
>
>
> On 4/19/2012 3:34 PM, Cole Beck wrote:
>> It's normal for me to create a list of data.frames and then use
>> do.call('rbind', list(...)) to create a single data.frame. However,
>> I've noticed as the size of the list grows large, it is perhaps better
>> to do this in chunks. As an example here's a list of 20,000 similar
>> data.frames.
>>
>> # create list of data.frames
>> dat <- vector("list", 20000)
>> for(i in seq_along(dat)) {
>> size <- sample(1:30, 1)
>> dat[[i]] <- data.frame(id=rep(i, size), value=rnorm(size),
>> letter=sample(LETTERS, size, replace=TRUE), ind=sample(c(TRUE,FALSE),
>> size, replace=TRUE))
>> }
>> # combine into one data.frame, normal usage
>> # system.time(do.call('rbind', dat)) # takes 2-3 minutes
>> combine <- function(x, steps=NA, verbose=FALSE) {
>> nr <- length(x)
>> if(is.na(steps)) steps <- nr
>> while(nr %% steps != 0) steps <- steps+1
>> if(verbose) cat(sprintf("step size: %s\r\n", steps))
>> dl <- vector("list", steps)
>> for(i in seq(steps)) {
>> ix <- seq(from=(i-1)*nr/steps+1, length.out=nr/steps)
>> dl[[i]] <- do.call("rbind", x[ix])
>> }
>> do.call("rbind", dl)
>> }
>> # combine into one data.frame
>> system.time(combine(dat, 100)) # takes 5-10 seconds
>>
>> I'm very surprised by this result. Does this improvement seem
>> reasonable? I would think "do.call" could utilize something similar by
>> default when the length of "args" is too high. Is using "do.call" not
>> recommended in this scenario?
>>
>> Regards,
>> Cole Beck
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel


From renaud at mancala.cbio.uct.ac.za  Tue Apr 24 14:46:49 2012
From: renaud at mancala.cbio.uct.ac.za (Renaud Gaujoux)
Date: Tue, 24 Apr 2012 14:46:49 +0200
Subject: [Rd] url, readLines, source behind a proxy
In-Reply-To: <4F957794.5090806@stats.ox.ac.uk>
References: <4F8D234E.1070709@cbio.uct.ac.za>
	<CAFDcVCQx1R7gJYnOPC3yzayJxi-Ts9zzaQd5T0bFSnLcOv8nCg@mail.gmail.com>
	<Pine.LNX.4.64.1204180705060.27275@mancala.cbio.uct.ac.za>
	<CAPPM_gS7LA-rv=__y_QbUi4ketxT9dn3PU=6dm+Sv9q5nU8nkw@mail.gmail.com>
	<4F957794.5090806@stats.ox.ac.uk>
Message-ID: <4F96A0B9.4090108@cbio.uct.ac.za>


On 23/04/2012 17:39, Prof Brian Ripley wrote:
> On 18/04/2012 16:04, Joshua Ulrich wrote:
>> Hi Renaud,
>>
>> On Wed, Apr 18, 2012 at 12:22 AM, Renaud Gaujoux
>> <renaud at mancala.cbio.uct.ac.za>  wrote:
>>> Hi Henrik,
>>>
>> <snip>
>>>
>>> Could anybody behind a proxy check if the issue can be reproduced?
>>> My proxy is in fact provided by cntml, which acts as a local proxy that
>>> takes care of tricky authentication protocols with the actual 
>>> university
>>> proxy, not natively supported by my system (Ubuntu). Anybody in this 
>>> case?
>>>
>> I can replicate this on a WinXP system, where I normally have to use
>> the --internet2 flag to get internet access through a proxy.
>>
>> ?download.file has a section on "Setting Proxies", which describes how
>> to use environment variables to set proxy information.  Setting
>> http_proxy='http://my.proxy.com/' was enough for me to get R CMD
>> check to run successfully with the --as-cran flag.
>
> I guess that the simplest way on Windows is to ensure that --internet2 
> is set.  In R-patched there is a new environment variable 
> R_WIN_INTERNET2 which lets you do that (set it in ~/.R/check.Renviron).
>
> [Setting proxies is so 20th century -- even moderately competent 
> sysadmins worked out how to use transparent caching proxies ca 1995. 
> Which is why the R developers give it a low priority.]
I completely understand the low priority -- fast-illimited-internet 
based --  point of view. I wish I could live without such a fussy proxy, 
but I have not much choice.
I like to understand why things work and do not work though.
Is there any special feature my proxy should have to allow 
readLines/source to correctly read remote data? What makes its access 
different from wget?

Thank you for your insights on this.

>
>
>>
>>> Thanks.
>>> Renaud
>>>
>>
>> Best,
>> -- 
>> Joshua Ulrich  |  FOSS Trading: www.fosstrading.com
>>
>> R/Finance 2012: Applied Finance with R
>> www.RinFinance.com
>>
>>
>>> On Tue, 17 Apr 2012, Henrik Bengtsson wrote:
>>>
>>>> On Tue, Apr 17, 2012 at 1:01 AM, Renaud Gaujoux
>>>> <renaud at mancala.cbio.uct.ac.za>  wrote:
>>>>> Hi,
>>>>>
>>>>> when I run R CMD check with flag --as-cran, the process hangs at 
>>>>> stage:
>>>>>
>>>>> * checking CRAN incoming feasibility ...
>>>>
>>>> Doesn't it time-out eventually?  I'm not behind a proxy but when I've
>>>> been running 'R CMD check' whenon very poor 3G connection, it had
>>>> eventually timed out.
>>>>
>>>> /Henrik
>>>>
>>>>>
>>>>> I am pretty sure it is a proxy issue.
>>>>> I looked at the check code in the tools package and it seems that 
>>>>> the issue
>>>>> is in the local function `.repository_db()` (defined in
>>>>> `tools:::.check_package_CRAN_incoming()`), which eventually calls 
>>>>> `url()`
>>>>> with argument open="rb", that hangs probably because it does not 
>>>>> use the
>>>>> proxy settings.
>>>>> I had a similar issue with `source()`, which apparently uses internal
>>>>> network functions (not as download.file), but is supposed to work 
>>>>> behind a
>>>>> proxy (correct?).
>>>>> Does anybody else have this problem?
>>>>>
>>>>> I was wondering if there is a way around, as I would like to be 
>>>>> able to use
>>>>> --as-cran for my checks.
>>>>> Thank you.
>>>>>
>>>>> Renaud
>>>>>
>>>>> -- 
>>>>> Renaud Gaujoux
>>>>> Computational Biology - University of Cape Town
>>>>> South Africa
>>>>>
>>>>> ______________________________________________
>>>>> R-devel at r-project.org mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>
>>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>
>


From ronggui.huang at gmail.com  Tue Apr 24 15:36:35 2012
From: ronggui.huang at gmail.com (Wincent)
Date: Tue, 24 Apr 2012 21:36:35 +0800
Subject: [Rd] nobs.glm
Message-ID: <CANQBBMsUcKHZeLpN5a1mCnzHhTMNNeSBwiPTUdydc=2jp8netw@mail.gmail.com>

Hi all,

The nobs method  of (MASS:::polr class) takes into account of weight,
but nobs method of glm does not. I wonder what is the rationale of
such design behind nobs.glm. Thanks in advance. Best Regards.

> library(MASS)
> house.plr <- polr(Sat ~ Infl + Type + Cont, weights = Freq, data = housing)
> house.logit <- glm(I(Sat=='High') ~ Infl + Type + Cont, binomial,weights = Freq, data = housing)
> nobs(house.plr)
[1] 1681
> nobs(house.logit)
[1] 72


-- 
Wincent Ronggui HUANG
Sociology Department of Fudan University
PhD of City University of Hong Kong
http://homepage.fudan.edu.cn/rghuang/cv/


From ripley at stats.ox.ac.uk  Tue Apr 24 15:56:55 2012
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 24 Apr 2012 14:56:55 +0100
Subject: [Rd] nobs.glm
In-Reply-To: <CANQBBMsUcKHZeLpN5a1mCnzHhTMNNeSBwiPTUdydc=2jp8netw@mail.gmail.com>
References: <CANQBBMsUcKHZeLpN5a1mCnzHhTMNNeSBwiPTUdydc=2jp8netw@mail.gmail.com>
Message-ID: <4F96B127.7060205@stats.ox.ac.uk>

On 24/04/2012 14:36, Wincent wrote:
> Hi all,
>
> The nobs method  of (MASS:::polr class) takes into account of weight,
> but nobs method of glm does not. I wonder what is the rationale of
> such design behind nobs.glm. Thanks in advance. Best Regards.
>
>> library(MASS)
>> house.plr<- polr(Sat ~ Infl + Type + Cont, weights = Freq, data = housing)
>> house.logit<- glm(I(Sat=='High') ~ Infl + Type + Cont, binomial,weights = Freq, data = housing)
>> nobs(house.plr)
> [1] 1681
>> nobs(house.logit)
> [1] 72
>

Well, the interpretation of 'weights' for a GLM depends on the family. 
They may be equivalent to duplicated observations for a binomial GLM, 
but they are not for a Gaussian one.  The nobs method for class "glm" 
(there is no visible nobs.glm) follows the "lm" method.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From charles-listes+rd at plessy.org  Tue Apr 24 15:00:35 2012
From: charles-listes+rd at plessy.org (Charles Plessy)
Date: Tue, 24 Apr 2012 22:00:35 +0900
Subject: [Rd] Where to store and how to load support data ?
Message-ID: <20120424130035.GB24433@falafel.plessy.net>

Dear R developers,

I am writing a R module, which contains a function that needs support data (a
table with two columns).

I wonder how to make sure the data is available to the function, without making
the function reload the data each time it is executed.  Is it what the lazy
mechanism takes care of ?  Or should I use something like .onLoad ?

Also I am wondering about where the data should be located.  Is ./data appropriate ?

Perhaps somebody can suggest the name of a package that I can study to find the
answer myself ?

Have a nice day,

-- 
Charles Plessy
Tsurumi, Kanagawa, Japan


From mpocernich at neptuneinc.org  Tue Apr 24 17:33:49 2012
From: mpocernich at neptuneinc.org (Matt Pocernich)
Date: Tue, 24 Apr 2012 09:33:49 -0600
Subject: [Rd] Specifying a function as not being and  S3 Class function
Message-ID: <D8A3B091-1D93-4568-AE3F-2334AFA8CAD1@neptuneinc.org>


I am compiling a library with legacy code which has functions named with periods in the names - but are not S3 class functions.    For example for example,  summary.agriculture is not an extension of the summary function for and 'agriculture. class object - it is just poorly named.  

Is it possible to  keep from triggering the following warning when I check the package? 

* checking S3 generic/method consistency ... WARNING

summary:
  function(object, ...)
summary.agriculture:
  function(x, analyte.names, results.col, analyte.col, by, det.col,
           iQuantiles, iDetStats, iSW, iUCL, iLand, conf.level, iUTL,
           tol.level, utl.conf.level, iND, sig.figs)

I know that the best answer would be to rename with a better naming convention, but that would cause issues with legacy applications.

Thanks,

Matt


From hb at biostat.ucsf.edu  Tue Apr 24 17:57:46 2012
From: hb at biostat.ucsf.edu (Henrik Bengtsson)
Date: Tue, 24 Apr 2012 08:57:46 -0700
Subject: [Rd] url, readLines, source behind a proxy
In-Reply-To: <4F96A0B9.4090108@cbio.uct.ac.za>
References: <4F8D234E.1070709@cbio.uct.ac.za>
	<CAFDcVCQx1R7gJYnOPC3yzayJxi-Ts9zzaQd5T0bFSnLcOv8nCg@mail.gmail.com>
	<Pine.LNX.4.64.1204180705060.27275@mancala.cbio.uct.ac.za>
	<CAPPM_gS7LA-rv=__y_QbUi4ketxT9dn3PU=6dm+Sv9q5nU8nkw@mail.gmail.com>
	<4F957794.5090806@stats.ox.ac.uk> <4F96A0B9.4090108@cbio.uct.ac.za>
Message-ID: <CAFDcVCQ4y=1wjz-mc0KUjhyTvgBW929Ja3qFHk2W615rSyeARw@mail.gmail.com>

Looking at the source code (src/library/tools/R/check.R and
src/library/tools/R/QC.R), I found that...

WORKAROUND:
You can trick 'R CMD check' to quickly skip the
"check_package_CRAN_incoming" test by providing it with invalid URLs
to repositories by setting system environment
'_R_CHECK_XREFS_REPOSITORIES_' to a non-empty URL. For example:

% export _R_CHECK_XREFS_REPOSITORIES_="invalidURL"
% R CMD check --as-cran ...

gives:

* checking CRAN incoming feasibility ...NB: need Internet access to
use CRAN incoming checks
 OK

/Henrik

On Tue, Apr 24, 2012 at 5:46 AM, Renaud Gaujoux
<renaud at mancala.cbio.uct.ac.za> wrote:
>
> On 23/04/2012 17:39, Prof Brian Ripley wrote:
>>
>> On 18/04/2012 16:04, Joshua Ulrich wrote:
>>>
>>> Hi Renaud,
>>>
>>> On Wed, Apr 18, 2012 at 12:22 AM, Renaud Gaujoux
>>> <renaud at mancala.cbio.uct.ac.za> ?wrote:
>>>>
>>>> Hi Henrik,
>>>>
>>> <snip>
>>>>
>>>>
>>>> Could anybody behind a proxy check if the issue can be reproduced?
>>>> My proxy is in fact provided by cntml, which acts as a local proxy that
>>>> takes care of tricky authentication protocols with the actual university
>>>> proxy, not natively supported by my system (Ubuntu). Anybody in this
>>>> case?
>>>>
>>> I can replicate this on a WinXP system, where I normally have to use
>>> the --internet2 flag to get internet access through a proxy.
>>>
>>> ?download.file has a section on "Setting Proxies", which describes how
>>> to use environment variables to set proxy information. ?Setting
>>> http_proxy='http://my.proxy.com/' was enough for me to get R CMD
>>> check to run successfully with the --as-cran flag.
>>
>>
>> I guess that the simplest way on Windows is to ensure that --internet2 is
>> set. ?In R-patched there is a new environment variable R_WIN_INTERNET2 which
>> lets you do that (set it in ~/.R/check.Renviron).
>>
>> [Setting proxies is so 20th century -- even moderately competent sysadmins
>> worked out how to use transparent caching proxies ca 1995. Which is why the
>> R developers give it a low priority.]
>
> I completely understand the low priority -- fast-illimited-internet based --
> ?point of view. I wish I could live without such a fussy proxy, but I have
> not much choice.
> I like to understand why things work and do not work though.
> Is there any special feature my proxy should have to allow readLines/source
> to correctly read remote data? What makes its access different from wget?
>
> Thank you for your insights on this.
>
>
>>
>>
>>>
>>>> Thanks.
>>>> Renaud
>>>>
>>>
>>> Best,
>>> --
>>> Joshua Ulrich ?| ?FOSS Trading: www.fosstrading.com
>>>
>>> R/Finance 2012: Applied Finance with R
>>> www.RinFinance.com
>>>
>>>
>>>> On Tue, 17 Apr 2012, Henrik Bengtsson wrote:
>>>>
>>>>> On Tue, Apr 17, 2012 at 1:01 AM, Renaud Gaujoux
>>>>> <renaud at mancala.cbio.uct.ac.za> ?wrote:
>>>>>>
>>>>>> Hi,
>>>>>>
>>>>>> when I run R CMD check with flag --as-cran, the process hangs at
>>>>>> stage:
>>>>>>
>>>>>> * checking CRAN incoming feasibility ...
>>>>>
>>>>>
>>>>> Doesn't it time-out eventually? ?I'm not behind a proxy but when I've
>>>>> been running 'R CMD check' whenon very poor 3G connection, it had
>>>>> eventually timed out.
>>>>>
>>>>> /Henrik
>>>>>
>>>>>>
>>>>>> I am pretty sure it is a proxy issue.
>>>>>> I looked at the check code in the tools package and it seems that the
>>>>>> issue
>>>>>> is in the local function `.repository_db()` (defined in
>>>>>> `tools:::.check_package_CRAN_incoming()`), which eventually calls
>>>>>> `url()`
>>>>>> with argument open="rb", that hangs probably because it does not use
>>>>>> the
>>>>>> proxy settings.
>>>>>> I had a similar issue with `source()`, which apparently uses internal
>>>>>> network functions (not as download.file), but is supposed to work
>>>>>> behind a
>>>>>> proxy (correct?).
>>>>>> Does anybody else have this problem?
>>>>>>
>>>>>> I was wondering if there is a way around, as I would like to be able
>>>>>> to use
>>>>>> --as-cran for my checks.
>>>>>> Thank you.
>>>>>>
>>>>>> Renaud
>>>>>>
>>>>>> --
>>>>>> Renaud Gaujoux
>>>>>> Computational Biology - University of Cape Town
>>>>>> South Africa
>>>>>>
>>>>>> ______________________________________________
>>>>>> R-devel at r-project.org mailing list
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>>
>>>>>
>>>>
>>>> ______________________________________________
>>>> R-devel at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>
>>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>>
>>
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From oliver at first.in-berlin.de  Tue Apr 24 18:31:37 2012
From: oliver at first.in-berlin.de (oliver)
Date: Tue, 24 Apr 2012 18:31:37 +0200
Subject: [Rd] Using other peoples packages from within C-based R-extension
Message-ID: <20120424163137.GE2152@siouxsie>

Hello,

what if I want to write a package mixed R/C-extension
and want to use code that is provided by other peoples packages?

How for example can I use one of the provided wavelet packages
from within my C-based R-extension?

Somehow I would need to load the other packages and have access to the
functions they provide.
I mean I don't want to use the other packages via R-level, but directly
on the C-layer. Something like shared libs (dlopen and such stuff)
but via R-API.

Is there a general aproach to this, and how to do it?


Ciao,
   Oliver


From murdoch.duncan at gmail.com  Tue Apr 24 18:39:16 2012
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Tue, 24 Apr 2012 12:39:16 -0400
Subject: [Rd] Using other peoples packages from within C-based
	R-extension
In-Reply-To: <20120424163137.GE2152@siouxsie>
References: <20120424163137.GE2152@siouxsie>
Message-ID: <4F96D734.4020409@gmail.com>

On 24/04/2012 12:31 PM, oliver wrote:
> Hello,
>
> what if I want to write a package mixed R/C-extension
> and want to use code that is provided by other peoples packages?
>
> How for example can I use one of the provided wavelet packages
> from within my C-based R-extension?
>
> Somehow I would need to load the other packages and have access to the
> functions they provide.
> I mean I don't want to use the other packages via R-level, but directly
> on the C-layer. Something like shared libs (dlopen and such stuff)
> but via R-API.
>
> Is there a general aproach to this, and how to do it?

See "Registering native routines" in the Writing R Extensions manual.

Duncan Murdoch


From alireza.s.mahani at gmail.com  Tue Apr 24 18:41:25 2012
From: alireza.s.mahani at gmail.com (Alireza Mahani)
Date: Tue, 24 Apr 2012 09:41:25 -0700 (PDT)
Subject: [Rd] Creating model frame from R formula inside compiled code
Message-ID: <1335285685506-4584124.post@n4.nabble.com>

I am developing a custom regression package, which accepts a formula object
as way of setting up the model matrix and response variable from a data
frame. For large data sets, I expect that going through R memory might be
too slow, so I'm thinking about reading the data directly into C (e.g. from
a database) and then parsing the formula inside of C (by calling an R
function such as model.frame()). Is this possible? An alternative solution
is to read the large data set into R in chunks, create the model matrix for
each chunk and save the matrix chunk to disk, and then load and piece
together these chunks inside of C. But my solution seems sub-optimal to me.
Any suggestions? Thank you!


--
View this message in context: http://r.789695.n4.nabble.com/Creating-model-frame-from-R-formula-inside-compiled-code-tp4584124p4584124.html
Sent from the R devel mailing list archive at Nabble.com.


From S.Ellison at LGCGroup.com  Tue Apr 24 18:50:15 2012
From: S.Ellison at LGCGroup.com (S Ellison)
Date: Tue, 24 Apr 2012 17:50:15 +0100
Subject: [Rd] Specifying a function as not being and  S3 Class function
In-Reply-To: <D8A3B091-1D93-4568-AE3F-2334AFA8CAD1@neptuneinc.org>
References: <D8A3B091-1D93-4568-AE3F-2334AFA8CAD1@neptuneinc.org>
Message-ID: <A4E5A0B016B8CB41A485FC629B633CED39C0F7A343@GOLD.corp.lgc-group.com>

> Is it possible to  keep from triggering the following warning 
> when I check the package? 
>
> summary:
>   function(object, ...)
> summary.agriculture:
>   function(x, analyte.names, results.col, analyte.col, by, det.col,
> [clip]

Part of the solution is to add ... to the legacy function; that is required by the generic and is missing in your own function. Adding ... will not break existing code.

The name of the initial argument will still cause problems. But I've kludged round a similar issue (an intentional difference in required parameters, in my case) by replacing something like

obj.summary(x, y, z, ...) 

with something like
obj.summary(object, y, z, x=object, ...)

This preserves legacy argument order, is consistent with summary(object, ...) and retains the named argument x to avoid code changes. 

But it is clearly a kludge. It also runs the risk of accidental overwriting of x if someone specifies too many unnamed parameters. That should not happen in working legacy code, of course, as that would have broken if you included a surplus parameter in a function call with no ... . If it _is_ a problem you could try obj.summary(object, y, z, ..., x=object), which would avoid the accidental assignment by requiring exact match naming, but I cannot recall offhand if that construct would be considered consistent with the generic using the current CMD check. 

Steve Ellison

> -----Original Message-----
> From: r-devel-bounces at r-project.org 
> [mailto:r-devel-bounces at r-project.org] On Behalf Of Matt Pocernich
> Sent: 24 April 2012 16:34
> To: r-devel at r-project.org
> Subject: [Rd] Specifying a function as not being and S3 Class function
> 
> 
> I am compiling a library with legacy code which has functions 
> named with periods in the names - but are not S3 class 
> functions.    For example for example,  summary.agriculture 
> is not an extension of the summary function for and 
> 'agriculture. class object - it is just poorly named.  
> 
> Is it possible to  keep from triggering the following warning 
> when I check the package? 
> 
> * checking S3 generic/method consistency ... WARNING
> 
> summary:
>   function(object, ...)
> summary.agriculture:
>   function(x, analyte.names, results.col, analyte.col, by, det.col,
>            iQuantiles, iDetStats, iSW, iUCL, iLand, conf.level, iUTL,
>            tol.level, utl.conf.level, iND, sig.figs)
> 
> I know that the best answer would be to rename with a better 
> naming convention, but that would cause issues with legacy 
> applications.
> 
> Thanks,
> 
> Matt
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> *******************************************************************
This email and any attachments are confidential. Any use...{{dropped:8}}


From edd at debian.org  Tue Apr 24 19:02:33 2012
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue, 24 Apr 2012 12:02:33 -0500
Subject: [Rd] Using other peoples packages from within
	C-based	R-extension
In-Reply-To: <4F96D734.4020409@gmail.com>
References: <20120424163137.GE2152@siouxsie>
	<4F96D734.4020409@gmail.com>
Message-ID: <20374.56489.894035.993363@max.nulle.part>


On 24 April 2012 at 12:39, Duncan Murdoch wrote:
| On 24/04/2012 12:31 PM, oliver wrote:
| > Hello,
| >
| > what if I want to write a package mixed R/C-extension
| > and want to use code that is provided by other peoples packages?
| >
| > How for example can I use one of the provided wavelet packages
| > from within my C-based R-extension?
| >
| > Somehow I would need to load the other packages and have access to the
| > functions they provide.
| > I mean I don't want to use the other packages via R-level, but directly
| > on the C-layer. Something like shared libs (dlopen and such stuff)
| > but via R-API.
| >
| > Is there a general aproach to this, and how to do it?
| 
| See "Registering native routines" in the Writing R Extensions manual.

And there are over 120 packages providing access:

   CITAN Cubist GOSim KernSmooth MASS MSBVAR Matrix NetComp PMA PopGenome
   QuACN RCurl RODBC RTextTools Rcpp Rigroup Rlabkey Rmosek Rmpfr Rook Rserve
   Runuran SASxport SMCP SoDA TraMineR XML actuar adaptivetau akima aster
   aster2 bcv bda blme boolfun bstats canvas caret catnet cgh chron class
   climdex.pcic clpAPI clue cluster copula cplexAPI cplm datamap devEMF
   edesign expm fastICA fastcluster ff flsa foreign fracdiff fuzzyRankTests
   gb glpkAPI gmp gputools grpreg gsmoothr heavy hypred ifs ifultools int64
   interactivity kza lattice lfe lme4 locfit lpSolveAPI markdown mgcv minqa
   mugnet ncvreg nlme nnet pedigreemm phangorn phmm potts ppstat qtbase
   qtpaint quadprog rPorta randtoolbox rcdd rdyncall rgeos rggobi rmongodb
   rngWELL robustbase rpart rphast rrp rtfbs sde sensitivityPStrat sp spatial
   spdep spsurvey spt tree tripack uncompress vines xlsReadWrite xts yaml zoo

Matrix and lme4 is the prototypical example by R Core, MASS also provides
something.  I'd probably start with zoo and xts ...

Dirk

PS Rcpp et al don't do it as you can't register C++ routines IIRC.

-- 
R/Finance 2012 Conference on May 11 and 12, 2012 at UIC in Chicago, IL
See agenda, registration details and more at http://www.RinFinance.com


From amredd at gmail.com  Tue Apr 24 19:23:00 2012
From: amredd at gmail.com (Andrew Redd)
Date: Tue, 24 Apr 2012 11:23:00 -0600
Subject: [Rd] Write unix format files on windows and vice versa
Message-ID: <CAK_CNybknyMi0MaTqT7-FxWrkXe02fTPYZJXZYR0L=GOUffxpg@mail.gmail.com>

I go back and forth between windows and linux, and find myself running
into problem with line endings.  Is there a way to control the line
ending conversion when writing files, such as write and cat?  More
explicitly I want to be able to write files with LF line endings
rather than CRLF line ending on windows; and CRLF line endings instead
of LF on linux, and I want to be able to control when the conversion
is made and/or choose the line endings that I want.

As far as I can tell the conversion is not optional and buried deep in
compiled code. Is there a possible work around?

Thanks,
Andrew


From oliver at first.in-berlin.de  Tue Apr 24 19:35:32 2012
From: oliver at first.in-berlin.de (oliver)
Date: Tue, 24 Apr 2012 19:35:32 +0200
Subject: [Rd] Using other peoples packages from within C-based
 R-extension
In-Reply-To: <20374.56489.894035.993363@max.nulle.part>
References: <20120424163137.GE2152@siouxsie> <4F96D734.4020409@gmail.com>
	<20374.56489.894035.993363@max.nulle.part>
Message-ID: <20120424173532.GF2152@siouxsie>

Hello, 

OK, thanks for the information...


On Tue, Apr 24, 2012 at 12:02:33PM -0500, Dirk Eddelbuettel wrote:
> 
> On 24 April 2012 at 12:39, Duncan Murdoch wrote:
> | On 24/04/2012 12:31 PM, oliver wrote:
> | > Hello,
> | >
> | > what if I want to write a package mixed R/C-extension
> | > and want to use code that is provided by other peoples packages?
> | >
> | > How for example can I use one of the provided wavelet packages
> | > from within my C-based R-extension?
> | >
> | > Somehow I would need to load the other packages and have access to the
> | > functions they provide.
> | > I mean I don't want to use the other packages via R-level, but directly
> | > on the C-layer. Something like shared libs (dlopen and such stuff)
> | > but via R-API.
> | >
> | > Is there a general aproach to this, and how to do it?
> | 
> | See "Registering native routines" in the Writing R Extensions manual.
> 
> And there are over 120 packages providing access:
> 
>    CITAN Cubist GOSim KernSmooth MASS MSBVAR Matrix NetComp PMA PopGenome
>    QuACN RCurl RODBC RTextTools Rcpp Rigroup Rlabkey Rmosek Rmpfr Rook Rserve
>    Runuran SASxport SMCP SoDA TraMineR XML actuar adaptivetau akima aster
>    aster2 bcv bda blme boolfun bstats canvas caret catnet cgh chron class
>    climdex.pcic clpAPI clue cluster copula cplexAPI cplm datamap devEMF
>    edesign expm fastICA fastcluster ff flsa foreign fracdiff fuzzyRankTests
>    gb glpkAPI gmp gputools grpreg gsmoothr heavy hypred ifs ifultools int64
>    interactivity kza lattice lfe lme4 locfit lpSolveAPI markdown mgcv minqa
>    mugnet ncvreg nlme nnet pedigreemm phangorn phmm potts ppstat qtbase
>    qtpaint quadprog rPorta randtoolbox rcdd rdyncall rgeos rggobi rmongodb
>    rngWELL robustbase rpart rphast rrp rtfbs sde sensitivityPStrat sp spatial
>    spdep spsurvey spt tree tripack uncompress vines xlsReadWrite xts yaml zoo
[...]

But no wavelets stuff... (?)
(It was more than an example, I'm look for wavelet decompositioning.)


> 
> Matrix and lme4 is the prototypical example by R Core, MASS also provides
> something.  I'd probably start with zoo and xts ...
[...]

You mean with "start with" that I could look how to allow exporting
for my own package?

At the moment I'm rather looking for how to import symbols and access fnuctionality
of othera people's packages ...


Ciao,
   Oliver


From murdoch.duncan at gmail.com  Tue Apr 24 19:48:13 2012
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Tue, 24 Apr 2012 13:48:13 -0400
Subject: [Rd] Write unix format files on windows and vice versa
In-Reply-To: <CAK_CNybknyMi0MaTqT7-FxWrkXe02fTPYZJXZYR0L=GOUffxpg@mail.gmail.com>
References: <CAK_CNybknyMi0MaTqT7-FxWrkXe02fTPYZJXZYR0L=GOUffxpg@mail.gmail.com>
Message-ID: <4F96E75D.2090305@gmail.com>

On 24/04/2012 1:23 PM, Andrew Redd wrote:
> I go back and forth between windows and linux, and find myself running
> into problem with line endings.  Is there a way to control the line
> ending conversion when writing files, such as write and cat?  More
> explicitly I want to be able to write files with LF line endings
> rather than CRLF line ending on windows; and CRLF line endings instead
> of LF on linux, and I want to be able to control when the conversion
> is made and/or choose the line endings that I want.
>
> As far as I can tell the conversion is not optional and buried deep in
> compiled code. Is there a possible work around?

On Windows you will write CRLF if the file was opened as a text file, 
and LF only if it was opened as a binary file.  For example:

writeLines(letters, "crlf.txt")
con <- file("lf.txt", open="wb")
writeLines(letters, con)
close(con)

On Unix you can write CRLF by specifying that as the sep arg.  So the 
following will work on both systems:

con <- file("crlf.txt", open="wb")
writeLines(letters, con, sep="\r\n")
close(con)

(and the explicit connection isn't really necessary on Unix, but it is 
on Windows).

If you're not using the writeLines() function, it's probably a bit 
harder to set CRLF as a default on Unix than it is to set LF as a 
default on Windows.

Duncan Murdoch


From Ted.Harding at wlandres.net  Tue Apr 24 19:56:25 2012
From: Ted.Harding at wlandres.net ( (Ted Harding))
Date: Tue, 24 Apr 2012 18:56:25 +0100 (BST)
Subject: [Rd] Write unix format files on windows and vice versa
In-Reply-To: <CAK_CNybknyMi0MaTqT7-FxWrkXe02fTPYZJXZYR0L=GOUffxpg@mail.gmail.com>
Message-ID: <XFMail.20120424185625.Ted.Harding@wlandres.net>

On 24-Apr-2012 17:23:00 Andrew Redd wrote:
> I go back and forth between windows and linux, and find myself
> running into problem with line endings. Is there a way to
> control the line ending conversion when writing files, such as
> write and cat? More explicitly I want to be able to write files
> with LF line endings rather than CRLF line ending on windows;
> and CRLF line endings instead of LF on linux, and I want to be
> able to control when the conversion is made and/or choose the
> line endings that I want.
> 
> As far as I can tell the conversion is not optional and buried
> deep in compiled code. Is there a possible work around?
> 
> Thanks,
> Andrew

Rather than write the Linux version out in Windows (or the other
way round in Linux), you might find it more useful to use an
external conversion utility.

One such is unix2dos/dos2unix (the same program in either case,
whose function depends on what name you call it by). This used
to be standard on older Linux systems, but may need to be explicitly
installed on your system. It seems there may also be a version for
Windows -- see:

  http://download.cnet.com/Unix2DOS/3000-2381_4-10488164.html


Then, when you create a file in Windows, you could transfer it to
Linux and covert it to "Unix"; and also keep it as it is for use
on Windows. Conversely, when you create a file in Linux, you
coled convert it to "Windows" and transfer it to Windows; and
also keep it as it is for use on Linux.

It is also possible to do the CRLF-->LF or LF-->CRLF using 'sed'
in Linux.

For some further detail see

   http://en.wikipedia.org/wiki/Unix2dos

Hoping this helps,
Ted.

-------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at wlandres.net>
Date: 24-Apr-2012  Time: 18:56:21
This message was sent by XFMail


From jeffrey.ryan at lemnica.com  Tue Apr 24 20:35:29 2012
From: jeffrey.ryan at lemnica.com (Jeffrey Ryan)
Date: Tue, 24 Apr 2012 13:35:29 -0500
Subject: [Rd] Using other peoples packages from within C-based
	R-extension
In-Reply-To: <20120424173532.GF2152@siouxsie>
Message-ID: <CBBC5C91.3ADA5%jeff.a.ryan@gmail.com>

This link may be of help as well...

https://stat.ethz.ch/pipermail/r-devel/2008-November/051262.html


HTH
Jeff

On 4/24/12 12:35 PM, "oliver" <oliver at first.in-berlin.de> wrote:

>Hello, 
>
>OK, thanks for the information...
>
>
>On Tue, Apr 24, 2012 at 12:02:33PM -0500, Dirk Eddelbuettel wrote:
>> 
>> On 24 April 2012 at 12:39, Duncan Murdoch wrote:
>> | On 24/04/2012 12:31 PM, oliver wrote:
>> | > Hello,
>> | >
>> | > what if I want to write a package mixed R/C-extension
>> | > and want to use code that is provided by other peoples packages?
>> | >
>> | > How for example can I use one of the provided wavelet packages
>> | > from within my C-based R-extension?
>> | >
>> | > Somehow I would need to load the other packages and have access to
>>the
>> | > functions they provide.
>> | > I mean I don't want to use the other packages via R-level, but
>>directly
>> | > on the C-layer. Something like shared libs (dlopen and such stuff)
>> | > but via R-API.
>> | >
>> | > Is there a general aproach to this, and how to do it?
>> | 
>> | See "Registering native routines" in the Writing R Extensions manual.
>> 
>> And there are over 120 packages providing access:
>> 
>>    CITAN Cubist GOSim KernSmooth MASS MSBVAR Matrix NetComp PMA
>>PopGenome
>>    QuACN RCurl RODBC RTextTools Rcpp Rigroup Rlabkey Rmosek Rmpfr Rook
>>Rserve
>>    Runuran SASxport SMCP SoDA TraMineR XML actuar adaptivetau akima
>>aster
>>    aster2 bcv bda blme boolfun bstats canvas caret catnet cgh chron
>>class
>>    climdex.pcic clpAPI clue cluster copula cplexAPI cplm datamap devEMF
>>    edesign expm fastICA fastcluster ff flsa foreign fracdiff
>>fuzzyRankTests
>>    gb glpkAPI gmp gputools grpreg gsmoothr heavy hypred ifs ifultools
>>int64
>>    interactivity kza lattice lfe lme4 locfit lpSolveAPI markdown mgcv
>>minqa
>>    mugnet ncvreg nlme nnet pedigreemm phangorn phmm potts ppstat qtbase
>>    qtpaint quadprog rPorta randtoolbox rcdd rdyncall rgeos rggobi
>>rmongodb
>>    rngWELL robustbase rpart rphast rrp rtfbs sde sensitivityPStrat sp
>>spatial
>>    spdep spsurvey spt tree tripack uncompress vines xlsReadWrite xts
>>yaml zoo
>[...]
>
>But no wavelets stuff... (?)
>(It was more than an example, I'm look for wavelet decompositioning.)
>
>
>> 
>> Matrix and lme4 is the prototypical example by R Core, MASS also
>>provides
>> something.  I'd probably start with zoo and xts ...
>[...]
>
>You mean with "start with" that I could look how to allow exporting
>for my own package?
>
>At the moment I'm rather looking for how to import symbols and access
>fnuctionality
>of othera people's packages ...
>
>
>Ciao,
>   Oliver
>
>______________________________________________
>R-devel at r-project.org mailing list
>https://stat.ethz.ch/mailman/listinfo/r-devel


From Thierry.ONKELINX at inbo.be  Wed Apr 25 10:58:42 2012
From: Thierry.ONKELINX at inbo.be (ONKELINX, Thierry)
Date: Wed, 25 Apr 2012 08:58:42 +0000
Subject: [Rd] Strange bug in my package
Message-ID: <AA818EAD2576BC488B4F623941DA742757366F54@inbomail.inbo.be>

Dear all,

I get a bug in the examples of my AFLP package on R-forge (https://r-forge.r-project.org/R/?group_id=1027) but only on the Linux version. The windows version compiles. The Mac version skips the examples and compiles. The strange thing is that the packages compiles on my Ubuntu 10.10 machine with R 2.15.0. Therefore I can't reproduce the error.

I have traced the problem at
residuals(outliers(data))

outliers(data) gives an AFLP.outlier object (S4) and I have defined

setMethod("residuals", signature(object = "AFLP.outlier"),
        function(object, ...){
                object at Residual
        }
)

setMethod("resid", signature(object = "AFLP.outlier"),
        function(object, ...){
                object at Residual
        }

And the NAMESPACE is

importFrom(stats, hclust, princomp, residuals, resid)
exportPattern(".")

Any suggestions are welcome.

Best regards,

Thierry

ir. Thierry Onkelinx
Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest
team Biometrie & Kwaliteitszorg / team Biometrics & Quality Assurance
Kliniekstraat 25
1070 Anderlecht
Belgium
+ 32 2 525 02 51
+ 32 54 43 61 85
Thierry.Onkelinx at inbo.be
www.inbo.be

To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
~ Sir Ronald Aylmer Fisher

The plural of anecdote is not data.
~ Roger Brinner

The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey


* * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * *
Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.


From andy_liaw at merck.com  Wed Apr 25 14:34:00 2012
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 25 Apr 2012 08:34:00 -0400
Subject: [Rd] Strange behavior of model.frame() when given subset
Message-ID: <D5FA03935F7418419332B61CA255F65F6B5291D09C@USCTMXP51012.merck.com>

Dear R-devel,

I recent got a bug report from a locfit user about the use of the subset argument when calling locfit().  Basically the symptom is that the following two calls should produce the same result, but they don't:

locfit(y ~ lp(x, h=1), data=subset(dat, x > 1))
locfit(y ~ lp(x, h=1), data=dat, subset= x > 1)

I've tracked the problem down to something shown in the following example, but have no idea how to get further:

R> x <- 1:5
R> y <- sample(5)
R> m1 <- model.frame(y ~ lp(x))
R> m2 <- model.frame(y ~ lp(x), subset=x>1)
R> class(m1[[2]])
[1] "lp"
R> class(m2[[2]])
[1] "matrix"

So basically model.frame() seems to treat the lp() term differently depending on whether the subset argument is present or not.  Is this supposed to happen?  str(m1) and str(m2) show that besides having one row less and the lp() term being of class matrix instead of "lp", there's no difference between m1 and m2.

I'd really appreciate it if anyone shed some light on this.

Best,
Andy
Merck Research Labs

Notice:  This e-mail message, together with any attachme...{{dropped:11}}


From andy_liaw at merck.com  Wed Apr 25 15:41:57 2012
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 25 Apr 2012 09:41:57 -0400
Subject: [Rd] Strange behavior of model.frame() when given subset
In-Reply-To: <D5FA03935F7418419332B61CA255F65F6B5291D09C@USCTMXP51012.merck.com>
References: <D5FA03935F7418419332B61CA255F65F6B5291D09C@USCTMXP51012.merck.com>
Message-ID: <D5FA03935F7418419332B61CA255F65F6B5291D155@USCTMXP51012.merck.com>

Never mind.  Found the problem:  The package has been missing a subset method for the "lp" class since [...].  Adding "[.lp" solved the problem.

Cheers,
Andy

-----Original Message-----
From: r-devel-bounces at r-project.org [mailto:r-devel-bounces at r-project.org] On Behalf Of Liaw, Andy
Sent: Wednesday, April 25, 2012 8:34 AM
To: r-devel at r-project.org
Subject: [Rd] Strange behavior of model.frame() when given subset

Dear R-devel,

I recent got a bug report from a locfit user about the use of the subset argument when calling locfit().  Basically the symptom is that the following two calls should produce the same result, but they don't:

locfit(y ~ lp(x, h=1), data=subset(dat, x > 1))
locfit(y ~ lp(x, h=1), data=dat, subset= x > 1)

I've tracked the problem down to something shown in the following example, but have no idea how to get further:

R> x <- 1:5
R> y <- sample(5)
R> m1 <- model.frame(y ~ lp(x))
R> m2 <- model.frame(y ~ lp(x), subset=x>1)
R> class(m1[[2]])
[1] "lp"
R> class(m2[[2]])
[1] "matrix"

So basically model.frame() seems to treat the lp() term differently depending on whether the subset argument is present or not.  Is this supposed to happen?  str(m1) and str(m2) show that besides having one row less and the lp() term being of class matrix instead of "lp", there's no difference between m1 and m2.

I'd really appreciate it if anyone shed some light on this.

Best,
Andy
Merck Research Labs

Notice:  This e-mail message, together with any attachme...{{dropped:11}}

______________________________________________
R-devel at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-devel
Notice:  This e-mail message, together with any attachme...{{dropped:11}}


From murdoch.duncan at gmail.com  Wed Apr 25 20:55:09 2012
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Wed, 25 Apr 2012 14:55:09 -0400
Subject: [Rd] checkPoFiles function added to R-devel
Message-ID: <4F98488D.8010604@gmail.com>

I've just added a couple of functions to the tools package to check for 
bad format strings in the .po files that translators produce.  See 
?tools::checkPoFiles for the docs.  They detect cases where the 
translation has different format conversions than the original English 
message.  This isn't always an error, but it often is.

Hopefully this will make it easier to detect problems before they're 
released.

Duncan Murdoch


From Robert.McGehee at geodecapital.com  Wed Apr 25 23:18:41 2012
From: Robert.McGehee at geodecapital.com (McGehee, Robert)
Date: Wed, 25 Apr 2012 17:18:41 -0400
Subject: [Rd] delayedAssign changing values
Message-ID: <17B09E7789D3104E8F5EEB0582A8D66FCD1EAC8145@MSGRTPCCRF2WIN.DMN1.FMR.COM>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20120425/503ea94b/attachment.pl>

From ronggui.huang at gmail.com  Thu Apr 26 02:16:50 2012
From: ronggui.huang at gmail.com (Wincent)
Date: Thu, 26 Apr 2012 08:16:50 +0800
Subject: [Rd] nobs.glm
In-Reply-To: <4F96B127.7060205@stats.ox.ac.uk>
References: <CANQBBMsUcKHZeLpN5a1mCnzHhTMNNeSBwiPTUdydc=2jp8netw@mail.gmail.com>
	<4F96B127.7060205@stats.ox.ac.uk>
Message-ID: <CANQBBMssVNhDhCCoceJHOnaC3daF3wnTp89edsuyyqQxVYOjDQ@mail.gmail.com>

Thanks, Professor Ripley. It helps a lot. As a follow-up question, is
there any recommended references I can look into to figure out the
interpretation of "weights" for various families?

Thanks very much in advance

Best regards

Ronggui

On 24 April 2012 21:56, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> On 24/04/2012 14:36, Wincent wrote:
>>
>> Hi all,
>>
>> The nobs method ?of (MASS:::polr class) takes into account of weight,
>> but nobs method of glm does not. I wonder what is the rationale of
>> such design behind nobs.glm. Thanks in advance. Best Regards.
>>
>>> library(MASS)
>>> house.plr<- polr(Sat ~ Infl + Type + Cont, weights = Freq, data =
>>> housing)
>>> house.logit<- glm(I(Sat=='High') ~ Infl + Type + Cont, binomial,weights =
>>> Freq, data = housing)
>>> nobs(house.plr)
>>
>> [1] 1681
>>>
>>> nobs(house.logit)
>>
>> [1] 72
>>
>
> Well, the interpretation of 'weights' for a GLM depends on the family. They
> may be equivalent to duplicated observations for a binomial GLM, but they
> are not for a Gaussian one. ?The nobs method for class "glm" (there is no
> visible nobs.glm) follows the "lm" method.
>
> --
> Brian D. Ripley, ? ? ? ? ? ? ? ? ?ripley at stats.ox.ac.uk
> Professor of Applied Statistics, ?http://www.stats.ox.ac.uk/~ripley/
> University of Oxford, ? ? ? ? ? ? Tel: ?+44 1865 272861 (self)
> 1 South Parks Road, ? ? ? ? ? ? ? ? ? ? +44 1865 272866 (PA)
> Oxford OX1 3TG, UK ? ? ? ? ? ? ? ?Fax: ?+44 1865 272595



-- 
Wincent Ronggui HUANG
Sociology Department of Fudan University
PhD of City University of Hong Kong
http://homepage.fudan.edu.cn/rghuang/cv/


From lachmann at eva.mpg.de  Thu Apr 26 11:31:01 2012
From: lachmann at eva.mpg.de (ghostwheel)
Date: Thu, 26 Apr 2012 02:31:01 -0700 (PDT)
Subject: [Rd] I wish xlim=c(0,
	NA) would work. How about I send you a patch?
In-Reply-To: <1335187455478-4580388.post@n4.nabble.com>
References: <CAErODj9RJ60Xdp=sLu=pGRiBOY_CSsxnjUmqObxhHYtEhHyLxA@mail.gmail.com>
	<CAFEqCdzg13nc_HvuKCA0xiS1F=YqKKvz4VKXn8_4cfqUmGABNg@mail.gmail.com>
	<1335187455478-4580388.post@n4.nabble.com>
Message-ID: <1335432661373-4589316.post@n4.nabble.com>

The following seems to work well, and I don't think it'll break anything.
The only problem I see is if someone says xlim=c(min=9, max=0), which should
give an error/warning message, but won't.

Michael
---
plot.default=
function (x, y = NULL, type = "p", xlim = NULL, ylim = NULL,
      log = "", main = NULL, sub = NULL, xlab = NULL, ylab = NULL,
      ann = par("ann"), axes = TRUE, frame.plot = axes, panel.first = NULL,
      panel.last = NULL, asp = NA, ...)
  {
        localAxis <- function(..., col, bg, pch, cex, lty, lwd) Axis(...)
            localBox <- function(..., col, bg, pch, cex, lty, lwd) box(...)
            localWindow <- function(..., col, bg, pch, cex, lty, lwd)
plot.window(...)
            localTitle <- function(..., col, bg, pch, cex, lty, lwd)
title(...)
            xlabel <- if (!missing(x))
                      deparse(substitute(x))
            ylabel <- if (!missing(y))
                      deparse(substitute(y))
            xy <- xy.coords(x, y, xlabel, ylabel, log)
            xlab <- if (is.null(xlab))
                      xy$xlab
            else xlab
            ylab <- if (is.null(ylab))
                      xy$ylab
            else ylab
            xlim <- if (is.null(xlim))
                      range(xy$x[is.finite(xy$x)])
            else if( length(xlim)==1 & names(xlim)=="min" ) {
              c(xlim,range(xy$x[is.finite(xy$x)])[2]) }
            else if( length(xlim)==1 & names(xlim)=="max" ) {
              c( range(xy$x[is.finite(xy$x)])[1], xlim) }        
        else xlim
        ylim <- if (is.null(ylim))
                      range(xy$y[is.finite(xy$y)])
            else if( length(ylim)==1 & names(ylim)=="min" ) {
              c( ylim,range(xy$y[is.finite(xy$y)])[2]) }
            else if( length(ylim)==1 & names(ylim)=="max" ) {
              c( range(xy$y[is.finite(xy$y)])[1], ylim) }        
            else ylim
            dev.hold()
            on.exit(dev.flush())
            plot.new()
            localWindow(xlim, ylim, log, asp, ...)
            panel.first
            plot.xy(xy, type, ...)
            panel.last
            if (axes) {
                      localAxis(if (is.null(y))
                                            xy$x
                                        else x, side = 1, ...)
                              localAxis(if (is.null(y))
                                                    x
                                                else y, side = 2, ...)
                    }
            if (frame.plot)
                      localBox(...)
            if (ann)
                      localTitle(main = main, sub = sub, xlab = xlab, ylab =
ylab,
                                             ...)
            invisible()
      }

--
View this message in context: http://r.789695.n4.nabble.com/I-wish-xlim-c-0-NA-would-work-How-about-I-send-you-a-patch-tp4562269p4589316.html
Sent from the R devel mailing list archive at Nabble.com.


From lachmann at eva.mpg.de  Thu Apr 26 12:21:24 2012
From: lachmann at eva.mpg.de (ghostwheel)
Date: Thu, 26 Apr 2012 03:21:24 -0700 (PDT)
Subject: [Rd] I wish xlim=c(0,
	NA) would work. How about I send you a patch?
In-Reply-To: <1335432661373-4589316.post@n4.nabble.com>
References: <CAErODj9RJ60Xdp=sLu=pGRiBOY_CSsxnjUmqObxhHYtEhHyLxA@mail.gmail.com>
	<CAFEqCdzg13nc_HvuKCA0xiS1F=YqKKvz4VKXn8_4cfqUmGABNg@mail.gmail.com>
	<1335187455478-4580388.post@n4.nabble.com>
	<1335432661373-4589316.post@n4.nabble.com>
Message-ID: <1335435684455-4589400.post@n4.nabble.com>

Sorry, the previous had a bug and was quite ugly. This is a bit better:
--
function (x, y = NULL, type = "p", xlim = NULL, ylim = NULL,
      log = "", main = NULL, sub = NULL, xlab = NULL, ylab = NULL,
      ann = par("ann"), axes = TRUE, frame.plot = axes, panel.first = NULL,
      panel.last = NULL, asp = NA, ...)
  {
        localAxis <- function(..., col, bg, pch, cex, lty, lwd) Axis(...)
            localBox <- function(..., col, bg, pch, cex, lty, lwd) box(...)
            localWindow <- function(..., col, bg, pch, cex, lty, lwd)
plot.window(...)
            localTitle <- function(..., col, bg, pch, cex, lty, lwd)
title(...)
            xlabel <- if (!missing(x))
                      deparse(substitute(x))
            ylabel <- if (!missing(y))
                      deparse(substitute(y))
            xy <- xy.coords(x, y, xlabel, ylabel, log)
            xlab <- if (is.null(xlab))
                      xy$xlab
            else xlab
            ylab <- if (is.null(ylab))
                      xy$ylab
            else ylab
            xlim <- if (is.null(xlim))
                      range(xy$x[is.finite(xy$x)])
        else if( length(xlim)==1 ) {
          xlim.in=xlim
          xlim = range(xy$x[is.finite(xy$x)])
          xlim[pmatch( names(xlim.in),c("min","max") )] = xlim.in
          xlim
        } else xlim
        ylim <- if (is.null(ylim))
                      range(xy$y[is.finite(xy$y)])
            else  if( length(ylim)==1 ) {
          ylim.in=ylim
          ylim = range(xy$y[is.finite(xy$y)])
          ylim[pmatch( names(ylim.in),c("min","max") )] = ylim.in
          ylim
        }   else ylim
            dev.hold()
            on.exit(dev.flush())
            plot.new()
            localWindow(xlim, ylim, log, asp, ...)
            panel.first
            plot.xy(xy, type, ...)
            panel.last
            if (axes) {
                      localAxis(if (is.null(y))
                                            xy$x
                                        else x, side = 1, ...)
                              localAxis(if (is.null(y))
                                                    x
                                                else y, side = 2, ...)
                    }
            if (frame.plot)
                      localBox(...)
            if (ann)
                      localTitle(main = main, sub = sub, xlab = xlab, ylab =
ylab,
                                             ...)
            invisible()
      }
--

--
View this message in context: http://r.789695.n4.nabble.com/I-wish-xlim-c-0-NA-would-work-How-about-I-send-you-a-patch-tp4562269p4589400.html
Sent from the R devel mailing list archive at Nabble.com.


From yanglei_cq at 126.com  Thu Apr 26 08:00:21 2012
From: yanglei_cq at 126.com (yangleicq)
Date: Wed, 25 Apr 2012 23:00:21 -0700 (PDT)
Subject: [Rd] How does .Fortran "dqrls" work?
Message-ID: <1335420021941-4588973.post@n4.nabble.com>

Hi, all.
I want to write some functions like glm() so i studied it.
In glm.fit(), it calls a fortran subroutine named  "dqrfit" to compute least
squares solutions
 to the system
              x * b = y

To learn how "dqrfit" works, I just follow how glm() calls "dqrfit" by my
own example, my codes are given below:

>  qr <-
> matrix(c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14,2.3,1.7,1.3,1.7,1.7,1.6,1,1.7,1.7,1.7),ncol=2)
> qr 
      [,1] [,2]
 [1,] 4.17  2.3
 [2,] 5.58  1.7
 [3,] 5.18  1.3
 [4,] 6.11  1.7
 [5,] 4.50  1.7
 [6,] 4.61  1.6
 [7,] 5.17  1.0
 [8,] 4.53  1.7
 [9,] 5.33  1.7
[10,] 5.14  1.7
> n=10
>  p=2
>  y <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
>  ny=1L
>  tol=1e-07
>  coefficients=double(p)
>  residuals=double(n)
>  effects=double(n)
>  rank=integer(1L)
>  pivot=1:n
>  qraux=double(n)
>  work=double(2*n)
>  
>  
>  
>  fittt<-.Fortran("dqrls", qr =qr, n = n, 
+                 p = p, y = y, ny = ny, tol = tol,
coefficients=coefficients,
+                 residuals = residuals, effects = effects, 
+                 rank = rank, pivot = pivot, qraux = qraux, 
+                 work = work, PACKAGE = "base")
>  
>  fittt$coefficients
[1] 0 0

but when i use lm() which also calls "dqrls" internally to fit this model,
it gives reasonable result.

> lm(y~qr)

Call:
lm(formula = y ~ qr)

Coefficients:
(Intercept)          qr1          qr2  
    11.1766      -0.8833      -1.2628  


when I change the coefficients to be c(1,1), the output from "dqrls", 
fittt$coefficients also equals to c(1,1). That means the  .Fortran("dqrls",
qr=qr,n=n,p=p,...) did nothing to the coefficients! I don't know why, is
there anything I did wrong or missed?  How can I get the result from "dqrls"
as what lm() or glm() gets from "dqrls"?

Thanks in advance. Best Regards.



--
View this message in context: http://r.789695.n4.nabble.com/How-does-Fortran-dqrls-work-tp4588973p4588973.html
Sent from the R devel mailing list archive at Nabble.com.


From j.nybeck at gmail.com  Wed Apr 25 15:50:38 2012
From: j.nybeck at gmail.com (Joel)
Date: Wed, 25 Apr 2012 13:50:38 +0000
Subject: [Rd] Use of R in C#
References: <C3D16A4C8BD3F843860C73573C70DD952E255192@TK5EX14MBXC103.redmond.corp.microsoft.com>
Message-ID: <loom.20120425T154547-376@post.gmane.org>

Jeff Abrams <jeffab <at> microsoft.com> writes:

> 
> I have a C# program that requires the run of a logistic regression.  I have
downloaded the R 2.11 package, and
> have added the following references to my code:
> 
> STATCONNECTORCLNTLib;
> StatConnectorCommonLib;
> STATCONNECTORSRVLib;
> 
> In my code I have:
> StatConnector scs = new STATCONNECTORSRVLib.StatConnectorClass();
> scs.Init("R");
> 
> The init step returns the below error which seems to indicate a connection
failure.
> 
> System.Runtime.InteropServices.COMException was unhandled
>   Message="Exception from HRESULT: 0x80040013"
>   Source="Interop.STATCONNECTORSRVLib"
>   ErrorCode=-2147221485
> 
> Any ideas?
> 
> Jeff Abrams
> Research Engineer
> Auction Analytics, Microsoft
> 
> 	[[alternative HTML version deleted]]
> 
> 


For those who say this is not an R issue, it is... I believe this is a issue
with the R registration within Windows. I have all the requisite .dll's in the R
install file, and have the R_USER and R_HOME environment variables, yet I am
still seeing this issue. I can use other .dll's in this way, so this is NOT a
COM issue. It is an R problem. Any thoughts would be helpful.


From jonathan.shore at gmail.com  Thu Apr 26 14:43:51 2012
From: jonathan.shore at gmail.com (Jonathan Shore)
Date: Thu, 26 Apr 2012 05:43:51 -0700
Subject: [Rd] Use of R in C#
In-Reply-To: <loom.20120425T154547-376@post.gmane.org>
References: <C3D16A4C8BD3F843860C73573C70DD952E255192@TK5EX14MBXC103.redmond.corp.microsoft.com>
	<loom.20120425T154547-376@post.gmane.org>
Message-ID: <CABaweDOaJzSg04Q8gmNP3UUva_D+65ecKd1=SfefyZhtQ1tzow@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20120426/cd42cc8f/attachment.pl>

From Robert.McGehee at geodecapital.com  Thu Apr 26 16:35:06 2012
From: Robert.McGehee at geodecapital.com (McGehee, Robert)
Date: Thu, 26 Apr 2012 10:35:06 -0400
Subject: [Rd] delayedAssign changing values
In-Reply-To: <17B09E7789D3104E8F5EEB0582A8D66FCD1EAC8145@MSGRTPCCRF2WIN.DMN1.FMR.COM>
References: <17B09E7789D3104E8F5EEB0582A8D66FCD1EAC8145@MSGRTPCCRF2WIN.DMN1.FMR.COM>
Message-ID: <17B09E7789D3104E8F5EEB0582A8D66FCD1EAC84C5@MSGRTPCCRF2WIN.DMN1.FMR.COM>

For the amusement of the listserver:

Making use of the counter-intuitive assignment properties of delayedAssign, a co-worked challenged me to construct a delayedAssign of 'x' that causes 'x' to change its value _every_ time it is evaluated. The example below does this; each time 'x' is evaluated it is updated to be the next value in the Fibonnacci sequence.
 
cmd <- parse(text=
             "delayedAssign(\"x\", {
                x <- y[1]+y[2]
                y[1] <- y[2]
                y[2] <- x
                eval(cmd)
                y[1]
              })")
y <- c(0,1)
eval(cmd)
for (i in 1:20) print(x)
[1] 1
[1] 1
[1] 2
[1] 3
[1] 5
[1] 8
[1] 13
[1] 21
[1] 34
[1] 55
[1] 89
[1] 144
[1] 233
[1] 377
[1] 610
[1] 987
[1] 1597
[1] 2584
[1] 4181
[1] 6765

Cheers, Robert

-----Original Message-----
From: r-devel-bounces at r-project.org [mailto:r-devel-bounces at r-project.org] On Behalf Of McGehee, Robert
Sent: Wednesday, April 25, 2012 5:19 PM
To: r-devel at r-project.org
Subject: [Rd] delayedAssign changing values

I'm not sure if this is a known peculiarity or a bug, but I stumbled across what I think is very odd behavior from delayedAssign. In the below example x switches values the first two times it is evaluated.

> delayedAssign("x", {x <- 2; x+3})
> x==x
[1] FALSE

> delayedAssign("x", {x <- 2; x+3})
> x
[1] 5
> x
[1] 2

The ?delayedAssign documentation says that "after [evaluation], the value is fixed and the expression will not be evaluated again." However, this appears not to be true. Is this a bug, or just a good way to write extremely obfuscated code?

Robert McGehee, CFA
Geode Capital Management, LLC
One Post Office Square, 28th Floor | Boston, MA | 02109
Direct: (617)392-8396

This e-mail, and any attachments hereto, are intended fo...{{dropped:14}}

______________________________________________
R-devel at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-devel


From simon.urbanek at r-project.org  Thu Apr 26 17:26:01 2012
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Thu, 26 Apr 2012 11:26:01 -0400
Subject: [Rd] delayedAssign changing values
In-Reply-To: <17B09E7789D3104E8F5EEB0582A8D66FCD1EAC8145@MSGRTPCCRF2WIN.DMN1.FMR.COM>
References: <17B09E7789D3104E8F5EEB0582A8D66FCD1EAC8145@MSGRTPCCRF2WIN.DMN1.FMR.COM>
Message-ID: <ED11A19C-0066-4D2C-845C-C41747A94BF5@r-project.org>


On Apr 25, 2012, at 5:18 PM, McGehee, Robert wrote:

> I'm not sure if this is a known peculiarity or a bug, but I stumbled across what I think is very odd behavior from delayedAssign. In the below example x switches values the first two times it is evaluated.
> 
>> delayedAssign("x", {x <- 2; x+3})
>> x==x
> [1] FALSE
> 
>> delayedAssign("x", {x <- 2; x+3})
>> x
> [1] 5
>> x
> [1] 2
> 
> The ?delayedAssign documentation says that "after [evaluation], the value is fixed and the expression will not be evaluated again." However, this appears not to be true.

It is actually true, the value is not evaluated twice: your evaluation of "x" has modified the value of "x" (hence it is no longer a promise) so what you get in the next evaluation is the value that you have set as a side-effect of the promise evaluation. However, the forced value of the promise it still the value returned by the evaluated expression. YOu can see that it is the case by simply adding a function with side-effect (like cat) into your expression.


> Is this a bug, or just a good way to write extremely obfuscated code?
> 

That is a good question and I don't know the answer. The docs don't say which value will be fixed so it could be either way, but intuitively I would expect the promise evaluation to override side-effects.

Cheers,
Simon


From ligges at statistik.tu-dortmund.de  Thu Apr 26 17:47:22 2012
From: ligges at statistik.tu-dortmund.de (Uwe Ligges)
Date: Thu, 26 Apr 2012 17:47:22 +0200
Subject: [Rd] Strange bug in my package
In-Reply-To: <AA818EAD2576BC488B4F623941DA742757366F54@inbomail.inbo.be>
References: <AA818EAD2576BC488B4F623941DA742757366F54@inbomail.inbo.be>
Message-ID: <4F996E0A.1000601@statistik.tu-dortmund.de>



On 25.04.2012 10:58, ONKELINX, Thierry wrote:
> Dear all,
>
> I get a bug in the examples of my AFLP package on R-forge (https://r-forge.r-project.org/R/?group_id=1027) but only on the Linux version. The windows version compiles. The Mac version skips the examples and compiles. The strange thing is that the packages compiles on my Ubuntu 10.10 machine with R 2.15.0. Therefore I can't reproduce the error.

I don't see any error on the R-forge check results.

Uwe Ligges

>
> I have traced the problem at
> residuals(outliers(data))
>
> outliers(data) gives an AFLP.outlier object (S4) and I have defined
>
> setMethod("residuals", signature(object = "AFLP.outlier"),
>          function(object, ...){
>                  object at Residual
>          }
> )
>
> setMethod("resid", signature(object = "AFLP.outlier"),
>          function(object, ...){
>                  object at Residual
>          }
>
> And the NAMESPACE is
>
> importFrom(stats, hclust, princomp, residuals, resid)
> exportPattern(".")
>
> Any suggestions are welcome.
>
> Best regards,
>
> Thierry
>
> ir. Thierry Onkelinx
> Instituut voor natuur- en bosonderzoek / Research Institute for Nature and Forest
> team Biometrie&  Kwaliteitszorg / team Biometrics&  Quality Assurance
> Kliniekstraat 25
> 1070 Anderlecht
> Belgium
> + 32 2 525 02 51
> + 32 54 43 61 85
> Thierry.Onkelinx at inbo.be
> www.inbo.be
>
> To call in the statistician after the experiment is done may be no more than asking him to perform a post-mortem examination: he may be able to say what the experiment died of.
> ~ Sir Ronald Aylmer Fisher
>
> The plural of anecdote is not data.
> ~ Roger Brinner
>
> The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
>
> * * * * * * * * * * * * * D I S C L A I M E R * * * * * * * * * * * * *
> Dit bericht en eventuele bijlagen geven enkel de visie van de schrijver weer en binden het INBO onder geen enkel beding, zolang dit bericht niet bevestigd is door een geldig ondertekend document.
> The views expressed in this message and any annex are purely those of the writer and may not be regarded as stating an official position of INBO, as long as the message is not confirmed by a duly signed document.
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From lachmann at eva.mpg.de  Thu Apr 26 17:59:51 2012
From: lachmann at eva.mpg.de (ghostwheel)
Date: Thu, 26 Apr 2012 08:59:51 -0700 (PDT)
Subject: [Rd] delayedAssign changing values
In-Reply-To: <ED11A19C-0066-4D2C-845C-C41747A94BF5@r-project.org>
References: <17B09E7789D3104E8F5EEB0582A8D66FCD1EAC8145@MSGRTPCCRF2WIN.DMN1.FMR.COM>
	<ED11A19C-0066-4D2C-845C-C41747A94BF5@r-project.org>
Message-ID: <1335455991422-4590242.post@n4.nabble.com>

It is really strange that the delayedAssign is evaluated in the environment
it is called from, and thus can have side effects.
so
x=2
y=3
delayedAssign("x", {y <- 7; y+3}) 

gives
> x
[1] 10
> y
[1] 7

Both x and y changed.
More intuitive would have been the behavior
x=2
y=3
delayedAssign("x", local({y <- 7; y+3}) ) 
> x
[1] 10
> y
[1] 3
which only changes x.
Or, at least that should be the default behavior....

Michael

--
View this message in context: http://r.789695.n4.nabble.com/delayedAssign-changing-values-tp4588108p4590242.html
Sent from the R devel mailing list archive at Nabble.com.


From simon.urbanek at r-project.org  Thu Apr 26 18:53:42 2012
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Thu, 26 Apr 2012 12:53:42 -0400
Subject: [Rd] delayedAssign changing values
In-Reply-To: <1335455991422-4590242.post@n4.nabble.com>
References: <17B09E7789D3104E8F5EEB0582A8D66FCD1EAC8145@MSGRTPCCRF2WIN.DMN1.FMR.COM>
	<ED11A19C-0066-4D2C-845C-C41747A94BF5@r-project.org>
	<1335455991422-4590242.post@n4.nabble.com>
Message-ID: <D0BEE478-C3F3-47BB-B468-AC0098B94433@r-project.org>


On Apr 26, 2012, at 11:59 AM, ghostwheel wrote:

> It is really strange that the delayedAssign is evaluated in the environment it is called from,

Not quite, it is evaluated in the environment you specify - and you have control over both environments ... see ?delayedAssign


> and thus can have side effects.
> so
> x=2
> y=3
> delayedAssign("x", {y <- 7; y+3}) 
> 
> gives
>> x
> [1] 10
>> y
> [1] 7
> 
> Both x and y changed.
> More intuitive would have been the behavior
> x=2
> y=3
> delayedAssign("x", local({y <- 7; y+3}) ) 
>> x
> [1] 10
>> y
> [1] 3
> which only changes x.
> Or, at least that should be the default behavior....
> 

That is questionable - I think it is more logical for both environments to be the same as default. Just think if it -- the point here is to access lazy evaluation which is exactly what it does - lazy evaluation takes place in the original environment, not in another one.

Cheers,
Simon



> Michael
> 
> --
> View this message in context: http://r.789695.n4.nabble.com/delayedAssign-changing-values-tp4588108p4590242.html
> Sent from the R devel mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 
> 


From lachmann at eva.mpg.de  Fri Apr 27 00:10:34 2012
From: lachmann at eva.mpg.de (ghostwheel)
Date: Thu, 26 Apr 2012 15:10:34 -0700 (PDT)
Subject: [Rd] delayedAssign changing values
In-Reply-To: <D0BEE478-C3F3-47BB-B468-AC0098B94433@r-project.org>
References: <17B09E7789D3104E8F5EEB0582A8D66FCD1EAC8145@MSGRTPCCRF2WIN.DMN1.FMR.COM>
	<ED11A19C-0066-4D2C-845C-C41747A94BF5@r-project.org>
	<1335455991422-4590242.post@n4.nabble.com>
	<D0BEE478-C3F3-47BB-B468-AC0098B94433@r-project.org>
Message-ID: <1335478234841-4591137.post@n4.nabble.com>


Simon Urbanek wrote
> 
>> More intuitive would have been the behavior
>> delayedAssign("x", local({y <- 7; y+3}) ) 
>> which only changes x.
> 
> That is questionable - I think it is more logical for both environments to
> be the same as default. Just think if it -- the point here is to access
> lazy evaluation which is exactly what it does - lazy evaluation takes
> place in the original environment, not in another one.
> 

I think I finally understand. My intuition just came from looking at
?delayedAssign.
But delayedAssign came to replace delay(), which "creates a promise to
evaluate the given expression".
When one thinks of delay(), what you said makes sense, you just delay
executing a certain expression in the parent frame.

I think, though, that with the current way it is described and called,
delayedAssign should by default only have the side effect of changing the
variable, i.e. use eval.env=new.env().

The manual states:
This function is invoked for its side effect, which is assigning a promise
to evaluate value to the variable x.

I think that is a nice clear side effect - changing a variable when it is
evaluated...like a delayed "<<-".
Otherwise it seems to me that delayedAssign could cause debugging
nightmares. Luckily, it currently doesn't seem to widely used to cause
them....

But you are right that it might be a bit strange that assign.env and
eval.env are different. Maybe that is why there are two different parameters
- to make the side effects clearer?
I tried to find anywhere uses of delayedAssign which make positive use of
side effects other than the assignment, and couldn't find any. Does anyone
know of such a use?

P.S. the end of ?delayedAssign contains this cryptic code:

e <- (function(x, y = 1, z) environment())(1+2, "y", {cat(" HO! "); pi+2})
(le <- as.list(e)) # evaluates the promises

Which I think is another way to create a promise, other than delayedAssign.
But it is really unclear why it sits there at the bottom of the document.
There should probably be more explanation of what this is....


--
View this message in context: http://r.789695.n4.nabble.com/delayedAssign-changing-values-tp4588108p4591137.html
Sent from the R devel mailing list archive at Nabble.com.


From pdalgd at gmail.com  Fri Apr 27 01:37:53 2012
From: pdalgd at gmail.com (peter dalgaard)
Date: Fri, 27 Apr 2012 01:37:53 +0200
Subject: [Rd] delayedAssign changing values
In-Reply-To: <1335478234841-4591137.post@n4.nabble.com>
References: <17B09E7789D3104E8F5EEB0582A8D66FCD1EAC8145@MSGRTPCCRF2WIN.DMN1.FMR.COM>
	<ED11A19C-0066-4D2C-845C-C41747A94BF5@r-project.org>
	<1335455991422-4590242.post@n4.nabble.com>
	<D0BEE478-C3F3-47BB-B468-AC0098B94433@r-project.org>
	<1335478234841-4591137.post@n4.nabble.com>
Message-ID: <C759C566-896A-4ABF-AF44-3EB64A386BCC@gmail.com>


On Apr 27, 2012, at 00:10 , ghostwheel wrote:

> 
> Simon Urbanek wrote
>> 
>>> More intuitive would have been the behavior
>>> delayedAssign("x", local({y <- 7; y+3}) ) 
>>> which only changes x.
>> 
>> That is questionable - I think it is more logical for both environments to
>> be the same as default. Just think if it -- the point here is to access
>> lazy evaluation which is exactly what it does - lazy evaluation takes
>> place in the original environment, not in another one.
>> 
> 
> I think I finally understand. My intuition just came from looking at
> ?delayedAssign.
> But delayedAssign came to replace delay(), which "creates a promise to
> evaluate the given expression".
> When one thinks of delay(), what you said makes sense, you just delay
> executing a certain expression in the parent frame.
> 
> I think, though, that with the current way it is described and called,
> delayedAssign should by default only have the side effect of changing the
> variable, i.e. use eval.env=new.env().

That's not possible. It involves evaluating an expression, and there is no limit to what side effect this can have.


> 
> The manual states:
> This function is invoked for its side effect, which is assigning a promise
> to evaluate value to the variable x.
> 
> I think that is a nice clear side effect - changing a variable when it is
> evaluated...like a delayed "<<-".
> Otherwise it seems to me that delayedAssign could cause debugging
> nightmares. Luckily, it currently doesn't seem to widely used to cause
> them....

Just don't do that, then.... However, lazy evaluation _per se_ does cause nightmares, or at least surprising behavior. My favorite one (because it actually involves a relevant piece of statistics) is

loglike <- function(x,n) function(p) dbinom(x, n, p, log=TRUE)
n <- 10
x <- 7
ll <- loglike(x, n)
x <- 1
curve(ll) # max at 0.1


which has the issue that x (and n too) is not evaluated until the ll function is called, at which time it may have been changed from the value it had when ll was created. 



> 
> But you are right that it might be a bit strange that assign.env and
> eval.env are different. Maybe that is why there are two different parameters
> - to make the side effects clearer?
> I tried to find anywhere uses of delayedAssign which make positive use of
> side effects other than the assignment, and couldn't find any. Does anyone
> know of such a use?
> 

They'll have to be rather contrived, but printing is one, and perhaps maintaining a count of function calls could be another.


> P.S. the end of ?delayedAssign contains this cryptic code:
> 
> e <- (function(x, y = 1, z) environment())(1+2, "y", {cat(" HO! "); pi+2})
> (le <- as.list(e)) # evaluates the promises
> 
> Which I think is another way to create a promise, other than delayedAssign.
> But it is really unclear why it sits there at the bottom of the document.
> There should probably be more explanation of what this is....

It's actually the _normal_ way to create a promise, namely binding actual arguments to formal arguments.  It is just that some trickery is used in order to make the situation visible. 

I agree that the example looks a bit out of place, though. Perhaps there ought to be a help page on lazy evaluation and a reference to it?  (Any volunteers?)

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From notoriousnoah at gmail.com  Thu Apr 26 15:13:57 2012
From: notoriousnoah at gmail.com (notoriousnoah)
Date: Thu, 26 Apr 2012 06:13:57 -0700 (PDT)
Subject: [Rd] Use of R in C#
In-Reply-To: <loom.20120425T154547-376@post.gmane.org>
References: <C3D16A4C8BD3F843860C73573C70DD952E255192@TK5EX14MBXC103.redmond.corp.microsoft.com>
	<loom.20120425T154547-376@post.gmane.org>
Message-ID: <1335446037124-4589776.post@n4.nabble.com>

Don't use this StatConn R Program, it is really poorly written software. 
Instead, use RDotNet, it is much easier to get up and running, and very
simple to use:

http://rdotnet.codeplex.com/


--
View this message in context: http://r.789695.n4.nabble.com/Use-of-R-in-C-tp2165189p4589776.html
Sent from the R devel mailing list archive at Nabble.com.


From j.nybeck at gmail.com  Thu Apr 26 15:57:22 2012
From: j.nybeck at gmail.com (Joel Nybeck)
Date: Thu, 26 Apr 2012 09:57:22 -0400
Subject: [Rd] Use of R in C#
In-Reply-To: <CABaweDOaJzSg04Q8gmNP3UUva_D+65ecKd1=SfefyZhtQ1tzow@mail.gmail.com>
References: <C3D16A4C8BD3F843860C73573C70DD952E255192@TK5EX14MBXC103.redmond.corp.microsoft.com>
	<loom.20120425T154547-376@post.gmane.org>
	<CABaweDOaJzSg04Q8gmNP3UUva_D+65ecKd1=SfefyZhtQ1tzow@mail.gmail.com>
Message-ID: <CAKRF99ywr0hf2tiRFwg7FbXZ8TEfwp=cXPu1dLZfr_a34guqcg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20120426/13371dc3/attachment.pl>

From klebyn at yahoo.com.br  Thu Apr 26 18:36:34 2012
From: klebyn at yahoo.com.br (Cleber N.Borges)
Date: Thu, 26 Apr 2012 13:36:34 -0300
Subject: [Rd] Some graphical parameters don't works in plot.table and
	plot.TukeyHSD.
Message-ID: <4F997992.7060908@yahoo.com.br>

Hello all,

I would like to relate this behaviour of: plot.table & plot.TukeyHSD.
They don't work with some graphical parameters.

Thanks for your attetion.
Cleber

 > ### example:
 > plot( table(1:10), cex.axis=0.6)
 > plot( table(1:10), las=2)
 >
 > tukaov <- TukeyHSD( aov(breaks ~ wool + tension, data = warpbreaks) )
 >
 > plot( tukaov, main='xxx' )
Error in plot.default(c(xi[, "lwr"], xi[, "upr"]), rep.int(yvals, 2),  :
   formal argument "main" matched by multiple actual arguments
 > plot( tukaov, xlab='xxx' )
Error in plot.default(c(xi[, "lwr"], xi[, "upr"]), rep.int(yvals, 2),  :
   formal argument "xlab" matched by multiple actual arguments
 >
 > sessionInfo()
R version 2.15.0 (2012-03-30)
Platform: x86_64-pc-mingw32/x64 (64-bit)

locale:
[1] LC_COLLATE=Portuguese_Brazil.1252  LC_CTYPE=Portuguese_Brazil.1252
[3] LC_MONETARY=Portuguese_Brazil.1252 LC_NUMERIC=C
[5] LC_TIME=Portuguese_Brazil.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

loaded via a namespace (and not attached):
[1] tools_2.15.0


From cberry at tajo.ucsd.edu  Thu Apr 26 18:50:18 2012
From: cberry at tajo.ucsd.edu (cberry at tajo.ucsd.edu)
Date: Thu, 26 Apr 2012 09:50:18 -0700
Subject: [Rd] How does .Fortran "dqrls" work?
References: <1335420021941-4588973.post@n4.nabble.com>
Message-ID: <87aa1ybfp1.fsf@tajo.ucsd.edu>

yangleicq <yanglei_cq at 126.com> writes:

> Hi, all.
> I want to write some functions like glm() so i studied it.
> In glm.fit(), it calls a fortran subroutine named  "dqrfit" to compute least
> squares solutions
>  to the system
>               x * b = y
>
> To learn how "dqrfit" works, I just follow how glm() calls "dqrfit" by my
> own example, my codes are given below:
>
>>  qr <-
>> matrix(c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14,2.3,1.7,1.3,1.7,1.7,1.6,1,1.7,1.7,1.7),ncol=2)
>> qr 
>       [,1] [,2]
>  [1,] 4.17  2.3
>  [2,] 5.58  1.7
>  [3,] 5.18  1.3
>  [4,] 6.11  1.7
>  [5,] 4.50  1.7
>  [6,] 4.61  1.6
>  [7,] 5.17  1.0
>  [8,] 4.53  1.7
>  [9,] 5.33  1.7
> [10,] 5.14  1.7
>> n=10
>>  p=2
>>  y <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
>>  ny=1L
>>  tol=1e-07
>>  coefficients=double(p)
>>  residuals=double(n)
>>  effects=double(n)
>>  rank=integer(1L)
>>  pivot=1:n
>>  qraux=double(n)
>>  work=double(2*n)
>>  
>>  
>>  
>>  fittt<-.Fortran("dqrls", qr =qr, n = n, 
> +                 p = p, y = y, ny = ny, tol = tol,
> coefficients=coefficients,
> +                 residuals = residuals, effects = effects, 
> +                 rank = rank, pivot = pivot, qraux = qraux, 
> +                 work = work, PACKAGE = "base")
>>  
>>  fittt$coefficients
> [1] 0 0

You have the args for .Fortran wrong. Try:

> fargs <- structure(list("dqrls", qr = structure(c(1, 1, 1, 1, 1, 1, 1, 
+ 1, 1, 1, 4.17, 5.58, 5.18, 6.11, 4.5, 4.61, 5.17, 4.53, 5.33, 
+ 5.14, 2.3, 1.7, 1.3, 1.7, 1.7, 1.6, 1, 1.7, 1.7, 1.7), .Dim = c(10L, 
+ 3L)), n = 10L, p = 3L, y = c(4.81, 4.17, 4.41, 3.59, 5.87, 3.83, 
+ 6.03, 4.89, 4.32, 4.69), ny = 1L, tol = 1e-11, coefficients = c(0, 
+ 0, 0), residuals = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0), effects = c(0, 
+ 0, 0, 0, 0, 0, 0, 0, 0, 0), rank = 0L, pivot = 1:3, qraux = c(0, 
+ 0, 0), work = c(0, 0, 0, 0, 0, 0), PACKAGE = "base"), .Names = c("", 
+ "qr", "n", "p", "y", "ny", "tol", "coefficients", "residuals", 
+ "effects", "rank", "pivot", "qraux", "work", "PACKAGE"))
> do.call(.Fortran,fargs)$coef
[1] 11.176571 -0.883272 -1.262772
>

TIP: It often helps to use something like

     debug(function.calling.Fortran) 

and then step thru the function till the call you want to study is
invoked. Then inspect the inputs one-by-one and tinker with them and
recall the function or save them via 

         dput( list(...) , file="fargs" ) 

so you can later invoke the function as above.

HTH,

Chuck 


>
> but when i use lm() which also calls "dqrls" internally to fit this model,
> it gives reasonable result.
>
>> lm(y~qr)
>
> Call:
> lm(formula = y ~ qr)
>
> Coefficients:
> (Intercept)          qr1          qr2  
>     11.1766      -0.8833      -1.2628  
>
>
> when I change the coefficients to be c(1,1), the output from "dqrls", 
> fittt$coefficients also equals to c(1,1). That means the  .Fortran("dqrls",
> qr=qr,n=n,p=p,...) did nothing to the coefficients! I don't know why, is
> there anything I did wrong or missed?  How can I get the result from "dqrls"
> as what lm() or glm() gets from "dqrls"?
>
> Thanks in advance. Best Regards.
>
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/How-does-Fortran-dqrls-work-tp4588973p4588973.html
> Sent from the R devel mailing list archive at Nabble.com.
>

-- 
Charles C. Berry                            Dept of Family/Preventive Medicine
cberry at ucsd edu			    UC San Diego
http://famprevmed.ucsd.edu/faculty/cberry/  La Jolla, San Diego 92093-0901


From lorenz at usgs.gov  Fri Apr 27 14:56:12 2012
From: lorenz at usgs.gov (David L Lorenz)
Date: Fri, 27 Apr 2012 07:56:12 -0500
Subject: [Rd] How does .Fortran "dqrls" work?
In-Reply-To: <87aa1ybfp1.fsf@tajo.ucsd.edu>
References: <1335420021941-4588973.post@n4.nabble.com>
	<87aa1ybfp1.fsf@tajo.ucsd.edu>
Message-ID: <OF1A76458B.6C67A9A1-ON862579ED.0046C423-862579ED.0047102A@usgs.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20120427/d8848df3/attachment.pl>

From murdoch.duncan at gmail.com  Fri Apr 27 15:42:58 2012
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Fri, 27 Apr 2012 09:42:58 -0400
Subject: [Rd] How does .Fortran "dqrls" work?
In-Reply-To: <OF1A76458B.6C67A9A1-ON862579ED.0046C423-862579ED.0047102A@usgs.gov>
References: <1335420021941-4588973.post@n4.nabble.com>
	<87aa1ybfp1.fsf@tajo.ucsd.edu>
	<OF1A76458B.6C67A9A1-ON862579ED.0046C423-862579ED.0047102A@usgs.gov>
Message-ID: <4F9AA262.40001@gmail.com>

On 12-04-27 8:56 AM, David L Lorenz wrote:
>    Of course, what you could do is Google dqrls and get the source and
> documentation. That is because it is in the publically available linpack.
> If it were not publically available that would not work. Theoretically,
> all FORTRAN or C code in R should be publically available.

I'm not sure what you mean by "theoretically".  R is open source, the 
source code is available.  Uwe Ligges wrote an article in R News a few 
years ago telling you where to look for it.

Are you thinking of things that aren't really "in R"?  R can load 
packages that aren't open source (though most of them are), and it can 
make calls to the run-time or operating system libraries, and with some 
compilers/operating systems, those may not be.

Duncan Murdoch

> Dave
>
>
>
>
> From:
> <cberry at tajo.ucsd.edu>
> To:
> <r-devel at stat.math.ethz.ch>
> Date:
> 04/27/2012 06:28 AM
> Subject:
> Re: [Rd] How does .Fortran "dqrls" work?
> Sent by:
> r-devel-bounces at r-project.org
>
>
>
> yangleicq<yanglei_cq at 126.com>  writes:
>
>> Hi, all.
>> I want to write some functions like glm() so i studied it.
>> In glm.fit(), it calls a fortran subroutine named  "dqrfit" to compute
> least
>> squares solutions
>>   to the system
>>                x * b = y
>>
>> To learn how "dqrfit" works, I just follow how glm() calls "dqrfit" by
> my
>> own example, my codes are given below:
>>
>>>   qr<-
>>>
> matrix(c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14,2.3,1.7,1.3,1.7,1.7,1.6,1,1.7,1.7,1.7),ncol=2)
>>> qr
>>        [,1] [,2]
>>   [1,] 4.17  2.3
>>   [2,] 5.58  1.7
>>   [3,] 5.18  1.3
>>   [4,] 6.11  1.7
>>   [5,] 4.50  1.7
>>   [6,] 4.61  1.6
>>   [7,] 5.17  1.0
>>   [8,] 4.53  1.7
>>   [9,] 5.33  1.7
>> [10,] 5.14  1.7
>>> n=10
>>>   p=2
>>>   y<- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
>>>   ny=1L
>>>   tol=1e-07
>>>   coefficients=double(p)
>>>   residuals=double(n)
>>>   effects=double(n)
>>>   rank=integer(1L)
>>>   pivot=1:n
>>>   qraux=double(n)
>>>   work=double(2*n)
>>>
>>>
>>>
>>>   fittt<-.Fortran("dqrls", qr =qr, n = n,
>> +                 p = p, y = y, ny = ny, tol = tol,
>> coefficients=coefficients,
>> +                 residuals = residuals, effects = effects,
>> +                 rank = rank, pivot = pivot, qraux = qraux,
>> +                 work = work, PACKAGE = "base")
>>>
>>>   fittt$coefficients
>> [1] 0 0
>
> You have the args for .Fortran wrong. Try:
>
>> fargs<- structure(list("dqrls", qr = structure(c(1, 1, 1, 1, 1, 1, 1,
> + 1, 1, 1, 4.17, 5.58, 5.18, 6.11, 4.5, 4.61, 5.17, 4.53, 5.33,
> + 5.14, 2.3, 1.7, 1.3, 1.7, 1.7, 1.6, 1, 1.7, 1.7, 1.7), .Dim = c(10L,
> + 3L)), n = 10L, p = 3L, y = c(4.81, 4.17, 4.41, 3.59, 5.87, 3.83,
> + 6.03, 4.89, 4.32, 4.69), ny = 1L, tol = 1e-11, coefficients = c(0,
> + 0, 0), residuals = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0), effects = c(0,
> + 0, 0, 0, 0, 0, 0, 0, 0, 0), rank = 0L, pivot = 1:3, qraux = c(0,
> + 0, 0), work = c(0, 0, 0, 0, 0, 0), PACKAGE = "base"), .Names = c("",
> + "qr", "n", "p", "y", "ny", "tol", "coefficients", "residuals",
> + "effects", "rank", "pivot", "qraux", "work", "PACKAGE"))
>> do.call(.Fortran,fargs)$coef
> [1] 11.176571 -0.883272 -1.262772
>>
>
> TIP: It often helps to use something like
>
>       debug(function.calling.Fortran)
>
> and then step thru the function till the call you want to study is
> invoked. Then inspect the inputs one-by-one and tinker with them and
> recall the function or save them via
>
>           dput( list(...) , file="fargs" )
>
> so you can later invoke the function as above.
>
> HTH,
>
> Chuck
>
>
>>
>> but when i use lm() which also calls "dqrls" internally to fit this
> model,
>> it gives reasonable result.
>>
>>> lm(y~qr)
>>
>> Call:
>> lm(formula = y ~ qr)
>>
>> Coefficients:
>> (Intercept)          qr1          qr2
>>      11.1766      -0.8833      -1.2628
>>
>>
>> when I change the coefficients to be c(1,1), the output from "dqrls",
>> fittt$coefficients also equals to c(1,1). That means the
> .Fortran("dqrls",
>> qr=qr,n=n,p=p,...) did nothing to the coefficients! I don't know why, is
>> there anything I did wrong or missed?  How can I get the result from
> "dqrls"
>> as what lm() or glm() gets from "dqrls"?
>>
>> Thanks in advance. Best Regards.
>>
>>
>>
>> --
>> View this message in context:
> http://r.789695.n4.nabble.com/How-does-Fortran-dqrls-work-tp4588973p4588973.html
>
>> Sent from the R devel mailing list archive at Nabble.com.
>>
>


From bhh at xs4all.nl  Fri Apr 27 15:51:03 2012
From: bhh at xs4all.nl (Berend Hasselman)
Date: Fri, 27 Apr 2012 15:51:03 +0200
Subject: [Rd] How does .Fortran "dqrls" work?
In-Reply-To: <OF1A76458B.6C67A9A1-ON862579ED.0046C423-862579ED.0047102A@usgs.gov>
References: <1335420021941-4588973.post@n4.nabble.com>
	<87aa1ybfp1.fsf@tajo.ucsd.edu>
	<OF1A76458B.6C67A9A1-ON862579ED.0046C423-862579ED.0047102A@usgs.gov>
Message-ID: <574C2914-D5AE-47F4-9AD6-B30F20A00AE1@xs4all.nl>


On 27-04-2012, at 14:56, David L Lorenz wrote:

>  Of course, what you could do is Google dqrls and get the source and 
> documentation. That is because it is in the publically available linpack. 
> If it were not publically available that would not work. Theoretically, 
> all FORTRAN or C code in R should be publically available.
> Dave

Huh?

Just get the source code of R. 
Do a bit of digging and you'll find it.

@yang:

you should have set

n <- 10L
p <- 2L

and see reply by Chuck.

In your case you should have done

lm(y ~ qr + 0)

Berend

> 
> 
> From:
> <cberry at tajo.ucsd.edu>
> To:
> <r-devel at stat.math.ethz.ch>
> Date:
> 04/27/2012 06:28 AM
> Subject:
> Re: [Rd] How does .Fortran "dqrls" work?
> Sent by:
> r-devel-bounces at r-project.org
> 
> 
> 
> yangleicq <yanglei_cq at 126.com> writes:
> 
>> Hi, all.
>> I want to write some functions like glm() so i studied it.
>> In glm.fit(), it calls a fortran subroutine named  "dqrfit" to compute 
> least
>> squares solutions
>> to the system
>>              x * b = y
>> 
>> To learn how "dqrfit" works, I just follow how glm() calls "dqrfit" by 
> my
>> own example, my codes are given below:
>> 
>>> qr <-
>>> 
> matrix(c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14,2.3,1.7,1.3,1.7,1.7,1.6,1,1.7,1.7,1.7),ncol=2)
>>> qr 
>>      [,1] [,2]
>> [1,] 4.17  2.3
>> [2,] 5.58  1.7
>> [3,] 5.18  1.3
>> [4,] 6.11  1.7
>> [5,] 4.50  1.7
>> [6,] 4.61  1.6
>> [7,] 5.17  1.0
>> [8,] 4.53  1.7
>> [9,] 5.33  1.7
>> [10,] 5.14  1.7
>>> n=10
>>> p=2
>>> y <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
>>> ny=1L
>>> tol=1e-07
>>> coefficients=double(p)
>>> residuals=double(n)
>>> effects=double(n)
>>> rank=integer(1L)
>>> pivot=1:n
>>> qraux=double(n)
>>> work=double(2*n)
>>> 
>>> 
>>> 
>>> fittt<-.Fortran("dqrls", qr =qr, n = n, 
>> +                 p = p, y = y, ny = ny, tol = tol,
>> coefficients=coefficients,
>> +                 residuals = residuals, effects = effects, 
>> +                 rank = rank, pivot = pivot, qraux = qraux, 
>> +                 work = work, PACKAGE = "base")
>>> 
>>> fittt$coefficients
>> [1] 0 0
> 
> You have the args for .Fortran wrong. Try:
> 
>> fargs <- structure(list("dqrls", qr = structure(c(1, 1, 1, 1, 1, 1, 1, 
> + 1, 1, 1, 4.17, 5.58, 5.18, 6.11, 4.5, 4.61, 5.17, 4.53, 5.33, 
> + 5.14, 2.3, 1.7, 1.3, 1.7, 1.7, 1.6, 1, 1.7, 1.7, 1.7), .Dim = c(10L, 
> + 3L)), n = 10L, p = 3L, y = c(4.81, 4.17, 4.41, 3.59, 5.87, 3.83, 
> + 6.03, 4.89, 4.32, 4.69), ny = 1L, tol = 1e-11, coefficients = c(0, 
> + 0, 0), residuals = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0), effects = c(0, 
> + 0, 0, 0, 0, 0, 0, 0, 0, 0), rank = 0L, pivot = 1:3, qraux = c(0, 
> + 0, 0), work = c(0, 0, 0, 0, 0, 0), PACKAGE = "base"), .Names = c("", 
> + "qr", "n", "p", "y", "ny", "tol", "coefficients", "residuals", 
> + "effects", "rank", "pivot", "qraux", "work", "PACKAGE"))
>> do.call(.Fortran,fargs)$coef
> [1] 11.176571 -0.883272 -1.262772
>> 
> 
> TIP: It often helps to use something like
> 
>     debug(function.calling.Fortran) 
> 
> and then step thru the function till the call you want to study is
> invoked. Then inspect the inputs one-by-one and tinker with them and
> recall the function or save them via 
> 
>         dput( list(...) , file="fargs" ) 
> 
> so you can later invoke the function as above.
> 
> HTH,
> 
> Chuck 
> 
> 
>> 
>> but when i use lm() which also calls "dqrls" internally to fit this 
> model,
>> it gives reasonable result.
>> 
>>> lm(y~qr)
>> 
>> Call:
>> lm(formula = y ~ qr)
>> 
>> Coefficients:
>> (Intercept)          qr1          qr2 
>>    11.1766      -0.8833      -1.2628 
>> 
>> 
>> when I change the coefficients to be c(1,1), the output from "dqrls", 
>> fittt$coefficients also equals to c(1,1). That means the 
> .Fortran("dqrls",
>> qr=qr,n=n,p=p,...) did nothing to the coefficients! I don't know why, is
>> there anything I did wrong or missed?  How can I get the result from 
> "dqrls"
>> as what lm() or glm() gets from "dqrls"?
>> 
>> Thanks in advance. Best Regards.
>> 
>> 
>> 
>> --
>> View this message in context: 
> http://r.789695.n4.nabble.com/How-does-Fortran-dqrls-work-tp4588973p4588973.html
> 
>> Sent from the R devel mailing list archive at Nabble.com.
>> 
> 
> -- 
> Charles C. Berry                            Dept of Family/Preventive 
> Medicine
> cberry at ucsd edu                                                   UC 
> San Diego
> http://famprevmed.ucsd.edu/faculty/cberry/  La Jolla, San Diego 92093-0901
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From jmc at r-project.org  Sat Apr 28 02:02:37 2012
From: jmc at r-project.org (John Chambers)
Date: Fri, 27 Apr 2012 17:02:37 -0700
Subject: [Rd] globalVariables()
Message-ID: <4F9B339D.8070907@r-project.org>

There have been several threads on this list about the Notes generated 
in the check process regarding "global variables" with no visible 
definition.  These can be useful but false positives tend to obscure 
valid notes.

The function globalVariables() has been added to the utils package, in 
the development version and in 2.15.0 patched (as of revision 59233).   
Calls to it declare variables that should be regarded as defined when a 
package's R code is checked, even if codetools does not find a 
corresponding global assignment.  The resulting vector of names from the 
package is passed to codetools by the checking code.

Package authors should insert calls into the package's source, 
preferably near where they do something sensible that defines the 
objects in question.  See ?globalVariables.

(One instance of the problem is that fields and methods used in 
reference class methods were considered undefined.  A corresponding 
revision to setRefClass() and partners declares fields and methods to 
globalVariables().  Packages do not need to call globalVariables() 
explicitly for fields and methods, and should not.)

John


From indra_calisto at yahoo.com  Sat Apr 28 03:47:35 2012
From: indra_calisto at yahoo.com (Indrajit Sengupta)
Date: Fri, 27 Apr 2012 18:47:35 -0700 (PDT)
Subject: [Rd] Unable to install rggobi in R 2.15
Message-ID: <1335577655.10862.YahooMailNeo@web65413.mail.ac4.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20120427/be07bffd/attachment.pl>

From rds37 at cam.ac.uk  Fri Apr 27 15:54:45 2012
From: rds37 at cam.ac.uk (Rajen Shah)
Date: Fri, 27 Apr 2012 14:54:45 +0100
Subject: [Rd] Creating a package which contains stand-alone C code
In-Reply-To: <CAFu5mM6yd2sJ_sVx2b7p_By7EtniPs+JJG7hQ9iFh=g+fObrTA@mail.gmail.com>
References: <CAFu5mM6yd2sJ_sVx2b7p_By7EtniPs+JJG7hQ9iFh=g+fObrTA@mail.gmail.com>
Message-ID: <CAFu5mM6TX6wmHyb-2c6EvmzKhk-jbnoMd2ordin8JuOGNc9rhg@mail.gmail.com>

I would like to create an R package which uses some C code, which in
turn uses MPI. At the moment I'm only interested in creating this
package for UNIX-like systems. The way I envisage this working is for
the R package to contain an R function which uses the system call to
run the C code as a separate process (passing to the C code the
location of a file of data). The C code can then do what it needs to
do with the data, send its output to a file, and when it has finished
R can read the output from the file.

My question is, is it possible to create an R package which contains C
code which can then be called by an R function in that package using
the system call? The obstacles seem to be (i) compiling this C code in
the right way for it to be called by system, and (ii) giving the R
function responsible for calling the C code (via system) the location
of the executable.

>From my understanding of the R extensions manual (i) can be solved
using careful configure script and Makevars file (though I don't know
the details), and (ii) would require me to provide an R script
`src/install.libs.R?, which would need to copy the executable to the
right place, and modify my R function which uses the system call so it
knows where to look for the executable.

The reason I?m interested in calling the C code in this peculiar way,
rather than using the .C interface, for example, is that I?m worried
about using MPI from within R. At least, my knowledge of both R and
MPI is insufficient to be confident that calling MPI from within R
will run smoothly. Also, this way I can debug the C program entirely
on its own.

I realise there is an R package, Rmpi, which provides a wrapper for
most of the MPI functions, but since all my code will be in C, it
seems less sensible to make use of this, though I may be wrong.

Thank you for taking to the time to read this, and I very much
appreciate any advice, (especially) even if it is to say that my
proposed approach is entirely daft and I should do things completely
differently. Also, if you know of any examples of packages which do
what I?ve described above, I?d be very glad to know (it seems Sjava
does something like this?).

Best wishes,

Rajen Shah


From simon.urbanek at r-project.org  Sat Apr 28 15:35:51 2012
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Sat, 28 Apr 2012 09:35:51 -0400
Subject: [Rd] Creating a package which contains stand-alone C code
In-Reply-To: <CAFu5mM6TX6wmHyb-2c6EvmzKhk-jbnoMd2ordin8JuOGNc9rhg@mail.gmail.com>
References: <CAFu5mM6yd2sJ_sVx2b7p_By7EtniPs+JJG7hQ9iFh=g+fObrTA@mail.gmail.com>
	<CAFu5mM6TX6wmHyb-2c6EvmzKhk-jbnoMd2ordin8JuOGNc9rhg@mail.gmail.com>
Message-ID: <A9C4A85F-35D9-449B-9B1E-FD22E5559DD7@r-project.org>

On Apr 27, 2012, at 9:54 AM, Rajen Shah wrote:

> I would like to create an R package which uses some C code, which in
> turn uses MPI. At the moment I'm only interested in creating this
> package for UNIX-like systems. The way I envisage this working is for
> the R package to contain an R function which uses the system call to
> run the C code as a separate process (passing to the C code the
> location of a file of data). The C code can then do what it needs to
> do with the data, send its output to a file, and when it has finished
> R can read the output from the file.
> 
> My question is, is it possible to create an R package which contains C
> code which can then be called by an R function in that package using
> the system call? The obstacles seem to be (i) compiling this C code in
> the right way for it to be called by system, and (ii) giving the R
> function responsible for calling the C code (via system) the location
> of the executable.
> 
>> From my understanding of the R extensions manual (i) can be solved
> using careful configure script and Makevars file (though I don't know
> the details),

Simply add a target for your executable to Makevars.


> and (ii) would require me to provide an R script
> `src/install.libs.R?, which would need to copy the executable to the
> right place, and modify my R function which uses the system call so it
> knows where to look for the executable.
> 

Yes, you can have a look at Rserve (preferably the development version) for both of the above. It is not the perfect example (because it grew organically and is a bit more complex) but it does exactly that.


> The reason I?m interested in calling the C code in this peculiar way,
> rather than using the .C interface, for example, is that I?m worried
> about using MPI from within R. At least, my knowledge of both R and
> MPI is insufficient to be confident that calling MPI from within R
> will run smoothly. Also, this way I can debug the C program entirely
> on its own.
> 

Conceptually, you can achieve the same thing without another executable by forking and calling the main() function of your program -- that way you don't need another executable yet you can compile your code either as a stand-alone program (for testing) or as a package (for deployment):

SEXP call_main(SEXP args) {
   int argc = LENGTH(args), i;
   pid_t pid;
   char **argv = (char**) calloc(argc + 1, sizeof(char*));
   for (i = 0; i < argc; i++) argv[i] = CHAR(STRING_ELT(args, i));
   if ((pid = fork()) == 0) { main(argc, argv); exit(0); }
   return ScalarInteger(pid);
}

and PKG_CPPFLAGS=-Dmain=prog_main make sure you re-map main for the package in case it conflicts with R.

In general, you can do better and pass your data directly -- just define some interface in your program -- it will be much more efficient than going through files. Then your main() for the stand-alone program will read in the files and call that interface whereas R will call it directly.

Cheers,
Simon


> I realise there is an R package, Rmpi, which provides a wrapper for
> most of the MPI functions, but since all my code will be in C, it
> seems less sensible to make use of this, though I may be wrong.
> 
> Thank you for taking to the time to read this, and I very much
> appreciate any advice, (especially) even if it is to say that my
> proposed approach is entirely daft and I should do things completely
> differently. Also, if you know of any examples of packages which do
> what I?ve described above, I?d be very glad to know (it seems Sjava
> does something like this?).
> 
> Best wishes,
> 
> Rajen Shah
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 
> 


From ripley at stats.ox.ac.uk  Sat Apr 28 16:36:15 2012
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 28 Apr 2012 15:36:15 +0100
Subject: [Rd] Unable to install rggobi in R 2.15
In-Reply-To: <1335577655.10862.YahooMailNeo@web65413.mail.ac4.yahoo.com>
References: <1335577655.10862.YahooMailNeo@web65413.mail.ac4.yahoo.com>
Message-ID: <4F9C005F.8000303@stats.ox.ac.uk>

Do not cross-post.  This was answered on R-help.


On 28/04/2012 02:47, Indrajit Sengupta wrote:
> I am currently using R 2.15.0 with R Tools 2.15 (in Windows XP). I downloaded the source for RGgobi and extracted it to a folder.
>
> Then I tried compiling and installing with the following command and got an error message:
> --------------------------------------------------------------------------------------------
> D:\Work\tmp>R CMD INSTALL --build "D:\\DPF\\Rggobi\\rggobi"
> * installing to library 'C:/Program Files/R/R-2.15.0/library'
> * installing *source* package 'rggobi' ...
> ** libs
> cygwin warning:
>    MS-DOS style path detected: C:/PROGRA~1/R/R-215~1.0/etc/i386/Makeconf
>    Preferred POSIX equivalent is: /cygdrive/c/PROGRA~1/R/R-215~1.0/etc/i386/Makeconf
>    CYGWIN environment variable option "nodosfilewarning" turns off this warning.
>    Consult the user's guide for more details about POSIX paths:
>      http://cygwin.com/cygwin-ug-net/using.html#using-pathnames
> gcc  -I"C:/PROGRA~1/R/R-215~1.0/include" -DNDEBUG -D_R_=1 -DUSE_R=1 -mms-bitfields -I/include/gtk-2.0 -I/include/gdk-pixbuf-2.0 -I/../lib/gtk-2.0/include -
> I/include/atk-1.0 -I/include/cairo -I/include/pango-1.0 -I/include/glib-2.0 -I/../lib/glib-2.0/include -I/include/libxml2 -I/include -I/include/ggobi -IC:/
> PROGRA~1/R/R-215~1.0/include -I/include/libxml         -O3 -Wall  -std=gnu99 -mtune=core2 -c RSEval.c -o RSEval.o
> In file included from RSEval.c:6:0:
> RSGGobi.h:5:22: fatal error: GGobiAPI.h: No such file or directory
> compilation terminated.
> make: *** [RSEval.o] Error 1
> ERROR: compilation failed for package 'rggobi'
> * removing 'C:/Program Files/R/R-2.15.0/library/rggobi'
> --------------------------------------------------------------------------------------------
>
> Can anybody help in figuring out what is the problem? I have installed GGobi&  RGtk2 previously.
>
> Regards,
> Indrajit
> 	[[alternative HTML version deleted]]
>
>
>
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From phgrosjean at sciviews.org  Sun Apr 29 09:30:51 2012
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Sun, 29 Apr 2012 09:30:51 +0200
Subject: [Rd] A doubt about substitute() after delayedAssign()
Message-ID: <4F9CEE2B.4080507@sciviews.org>

Hello,

?delayedAssign presents substitute() as a way to look at the expression 
in the promise. However,

msg <- "old"
delayedAssign("x", msg)
msg <- "new!"
x #- new!
substitute(x) #- x (was 'msg' ?)

Here, we just got 'x'... shouldn't we got 'msg'?

Same result when the promise is not evaluated yet:

delayedAssign("x", msg)
substitute(x)

In a function, that works:

foo <- function (x = msg) substitute(x)
foo()

Did I misunderstood something? It seems to me that substitute() does not 
behaves as documented for promises created using delayedAssign().
Best,

Philippe
-- 
..............................................<?}))><........
  ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
  ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
  ) ) ) ) )   Mons University, Belgium
( ( ( ( (
..............................................................


From murdoch.duncan at gmail.com  Sun Apr 29 13:50:05 2012
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Sun, 29 Apr 2012 07:50:05 -0400
Subject: [Rd] A doubt about substitute() after delayedAssign()
In-Reply-To: <4F9CEE2B.4080507@sciviews.org>
References: <4F9CEE2B.4080507@sciviews.org>
Message-ID: <4F9D2AED.3020701@gmail.com>

On 12-04-29 3:30 AM, Philippe Grosjean wrote:
 > Hello,
 >
 > ?delayedAssign presents substitute() as a way to look at the expression
 > in the promise. However,
 >
 > msg<- "old"
 > delayedAssign("x", msg)
 > msg<- "new!"
 > x #- new!
 > substitute(x) #- x (was 'msg' ?)
 >
 > Here, we just got 'x'... shouldn't we got 'msg'?
 >
 > Same result when the promise is not evaluated yet:
 >
 > delayedAssign("x", msg)
 > substitute(x)
 >
 > In a function, that works:
 >
 > foo<- function (x = msg) substitute(x)
 > foo()
 >
 > Did I misunderstood something? It seems to me that substitute() does not
 > behaves as documented for promises created using delayedAssign().

I don't think this is well documented, but substitute() doesn't act the 
same when its "env" argument is the global environment.  So this works 
the way you'd expect:

e <- new.env()
msg <- "old"
delayedAssign("x", msg, assign=e)
msg <- "new"
e$x
substitute(x, e)

I forget what the motivation was for special-casing globalenv().

Duncan Murdoch


From hamdi_ines at hotmail.fr  Sun Apr 29 16:08:09 2012
From: hamdi_ines at hotmail.fr (nossa)
Date: Sun, 29 Apr 2012 07:08:09 -0700 (PDT)
Subject: [Rd] creating a package in R
Message-ID: <1335708489266-4596411.post@n4.nabble.com>

Please  give me the necessary links that permits me to create my own package
inR

--
View this message in context: http://r.789695.n4.nabble.com/creating-a-package-in-R-tp4596411p4596411.html
Sent from the R devel mailing list archive at Nabble.com.


From michael.weylandt at gmail.com  Sun Apr 29 16:28:42 2012
From: michael.weylandt at gmail.com (R. Michael Weylandt <michael.weylandt@gmail.com>)
Date: Sun, 29 Apr 2012 10:28:42 -0400
Subject: [Rd] creating a package in R
In-Reply-To: <1335708489266-4596411.post@n4.nabble.com>
References: <1335708489266-4596411.post@n4.nabble.com>
Message-ID: <587B04C0-FCA6-4E93-96F2-663E3547EF80@gmail.com>

http://cran.r-project.org/doc/manuals/R-exts.pdf

Ships with every distribution of R

Michael

On Apr 29, 2012, at 10:08 AM, nossa <hamdi_ines at hotmail.fr> wrote:

> Please  give me the necessary links that permits me to create my own package
> inR
> 
> --
> View this message in context: http://r.789695.n4.nabble.com/creating-a-package-in-R-tp4596411p4596411.html
> Sent from the R devel mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From jorismeys at gmail.com  Sun Apr 29 18:19:53 2012
From: jorismeys at gmail.com (Joris Meys)
Date: Sun, 29 Apr 2012 18:19:53 +0200
Subject: [Rd] creating a package in R
In-Reply-To: <587B04C0-FCA6-4E93-96F2-663E3547EF80@gmail.com>
References: <1335708489266-4596411.post@n4.nabble.com>
	<587B04C0-FCA6-4E93-96F2-663E3547EF80@gmail.com>
Message-ID: <CAO1zAVaQH9rBq7f5b=UGj_KjDHZY1S+tU_t-z=Lji4TxiAsFCw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20120429/87d96fb4/attachment.pl>

From moshersteven at gmail.com  Sun Apr 29 22:15:12 2012
From: moshersteven at gmail.com (steven mosher)
Date: Sun, 29 Apr 2012 13:15:12 -0700
Subject: [Rd] creating a package in R
In-Reply-To: <1335708489266-4596411.post@n4.nabble.com>
References: <1335708489266-4596411.post@n4.nabble.com>
Message-ID: <CAFFLneT1wYgmQ-DAyPKkmdzR0VBeF+AB2e2ibfuxY7GNQnbUhg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20120429/87c85119/attachment.pl>

From yanglei_cq at 126.com  Mon Apr 30 06:06:11 2012
From: yanglei_cq at 126.com (yangleicq)
Date: Sun, 29 Apr 2012 21:06:11 -0700 (PDT)
Subject: [Rd] How does .Fortran "dqrls" work?
In-Reply-To: <87aa1ybfp1.fsf@tajo.ucsd.edu>
References: <1335420021941-4588973.post@n4.nabble.com>
	<87aa1ybfp1.fsf@tajo.ucsd.edu>
Message-ID: <1335758771299-4597407.post@n4.nabble.com>

Thank you! now the problem is solved

--
View this message in context: http://r.789695.n4.nabble.com/How-does-Fortran-dqrls-work-tp4588973p4597407.html
Sent from the R devel mailing list archive at Nabble.com.


From rds37 at cam.ac.uk  Mon Apr 30 09:30:56 2012
From: rds37 at cam.ac.uk (Rajen Shah)
Date: Mon, 30 Apr 2012 08:30:56 +0100
Subject: [Rd] Creating a package which contains stand-alone C code
In-Reply-To: <A9C4A85F-35D9-449B-9B1E-FD22E5559DD7@r-project.org>
References: <CAFu5mM6yd2sJ_sVx2b7p_By7EtniPs+JJG7hQ9iFh=g+fObrTA@mail.gmail.com>
	<CAFu5mM6TX6wmHyb-2c6EvmzKhk-jbnoMd2ordin8JuOGNc9rhg@mail.gmail.com>
	<A9C4A85F-35D9-449B-9B1E-FD22E5559DD7@r-project.org>
Message-ID: <CAFu5mM4YSqoaC+XocZgigJC_-8WLa=RjZ18RRQx-kza1YC+D-w@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20120430/05c147e6/attachment.pl>

From jounihelske at gmail.com  Mon Apr 30 13:37:19 2012
From: jounihelske at gmail.com (Jouni Helske)
Date: Mon, 30 Apr 2012 14:37:19 +0300
Subject: [Rd] The constant part of the log-likelihood in StructTS
Message-ID: <CAGW7bkyFDeRzfUR91x79VrChvCf5py9ErYbs58eJ0fCOQnZ9Mw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20120430/b0c16ab7/attachment.pl>

From Sebastian.Meyer at ifspm.uzh.ch  Mon Apr 30 17:48:38 2012
From: Sebastian.Meyer at ifspm.uzh.ch (Sebastian Meyer)
Date: Mon, 30 Apr 2012 17:48:38 +0200
Subject: [Rd] R CMD check . segfault on re-building vignettes
Message-ID: <4F9EB456.4010206@ifspm.uzh.ch>

(Warning: There is some potential that this message is redundant.)

I think that i have spotted an issue with R CMD check that does not
trace back to a rare case, so it must have been reported already or i am
doing something stupid. However, to be sure and because I did not find
any comments on this while searching the R mailing lists, here is what i
did:
  R CMD check .
from within a package source directory which has some vignettes in
inst/doc. When the check finally comes to
"checking re-building of vignette PDFs ..."
i get the following error

 *** caught segfault ***
address 0x7fff6073c998, cause 'memory not mapped'

Traceback:
 1: file.copy(pkgdir, vd2, recursive = TRUE)
 2: run_vignettes(desc)
 3: check_pkg(pkgdir, pkgname, pkgoutdir, startdir, libdir, desc,
is_base_pkg, is_rec_pkg, thispkg_subdirs, extra_arch)
 4: tools:::.check_packages()
aborting ...
Segmentation fault (core dumped)

as one can see, the error stems from the call
file.copy(pkgdir, vd2, recursive = TRUE)
in the run_vignettes function defined in the tools:::.check_packages
function. this call will copy the contents of "pkgdir" (the source
directory) to the ..Rcheck/vign_test/ directory _recursively_, the
problem of which being that the "..Rcheck" directory is part of
"pkgdir". Thus we get into a loop of copying pkgdir into
pkgdir/..Rcheck/vign_test, which is interrupted at some point by a
segfault (at least on my x86_64 GNU/Linux machine running Ubuntu 12.04
and R 2.15.0; the OS might be crucial for this issue).

I can reproduce the error by e.g.:

  pkgdir <- tempdir()
  file.create(file.path(pkgdir,"DESCRIPTION"))    # not necessary
  vd2 <- file.path(pkgdir,"..Rcheck/vign_test")
  dir.create(vd2, recursive=TRUE)
  ### Better Do NOT run -> buffer overflow
  # file.copy(pkgdir, vd2, recursive=TRUE)
  ###

Can anybody confirm this problem? Of course, a workaround is to run R
CMD check from outside the source directory (which is probably the
preferred way).

Thanks in advance,
Sebastian


-- 
Sebastian Meyer
Division of Biostatistics
Institute of Social and Preventive Medicine
University of Zurich


From hb at biostat.ucsf.edu  Mon Apr 30 19:14:58 2012
From: hb at biostat.ucsf.edu (Henrik Bengtsson)
Date: Mon, 30 Apr 2012 10:14:58 -0700
Subject: [Rd] file.copy(src, src,
 recursive=TRUE) causes a segfault (Was: Re: R CMD check . segfault
 on re-building vignettes)
Message-ID: <CAFDcVCTGUCuPzBiADCdZHC0iezcmSsrmxsZmQ9OD-gUsLqmYLg@mail.gmail.com>

It appears that file.copy() does not protect against the case when one
copies one directory to a subdirectory of itself resulting in it
trying to create and endless recursive copy of itself.

REPRODUCIBLE EXAMPLE:
src <- "foo"
dir.create(src);
file.copy(src, src, recursive=FALSE) # ok
file.copy(src, src, recursive=TRUE) # PROBLEM
file.exists("foo/foo/foo/foo/foo/foo/foo/foo/") # TRUE
file.exists("foo/foo/foo/foo/foo/foo/foo/foo/foo/foo/foo/") # TRUE

I can reproduce this.

TROUBLESHOOTING:
On R version 2.15.0 (2012-03-30) [Platform: x86_64-pc-linux-gnu (64-bit)] I get

> file.copy(src, src, recursive=TRUE) # endless creation/loop
Error: segfault from C stack overflow

but return to a working prompt.  BTW, this follow up causes an R error:

> dirs <- list.files("foo", recursive=TRUE)
Error: C stack usage is too close to the limit

> dirs <- system("ls -R foo/ | tail -1", intern=TRUE)
> nchar(dirs)
[1] 2512
> str(strsplit(dirs, split="/"))
List of 1
 $ : chr [1:628] "foo" "foo" "foo" "foo" ...

I guess a possible cause for the segfault is what is hinted at by the
list.files() error; internally there is a write buffer overflow due to
a too long path.

On Windows, i.e. R version 2.15.0 Patched (2012-04-25 r59178)
[Platform: x86_64-pc-mingw32/x64 (64-bit)], I get:

> file.copy(src, src, recursive=TRUE)
[1] TRUE
Warning message:
In file.copy(src, src, recursive = TRUE) :
  problem creating directory
.\foo\foo\foo\foo\foo\foo\foo\foo\foo\foo\foo\foo\foo\foo\foo\foo\foo\foo\foo\foo\foo\foo\foo\foo\foo\foo\foo\foo\foo\foo\foo\foo\foo\foo\foo\foo\foo\foo\foo\foo\foo\foo\foo\foo\foo\foo\foo\foo\foo\foo\foo\foo\foo\foo\foo\foo:
No such file or directory

Here the maximum file path length of the OS is probably kicking in
before reaching the buffer overflow, cf.
http://aroma-project.org/howtos/UseLongFilenamesOnWindows

My $.02

/Henrik

On Mon, Apr 30, 2012 at 8:48 AM, Sebastian Meyer
<Sebastian.Meyer at ifspm.uzh.ch> wrote:
> (Warning: There is some potential that this message is redundant.)
>
> I think that i have spotted an issue with R CMD check that does not
> trace back to a rare case, so it must have been reported already or i am
> doing something stupid. However, to be sure and because I did not find
> any comments on this while searching the R mailing lists, here is what i
> did:
> ?R CMD check .
> from within a package source directory which has some vignettes in
> inst/doc. When the check finally comes to
> "checking re-building of vignette PDFs ..."
> i get the following error
>
> ?*** caught segfault ***
> address 0x7fff6073c998, cause 'memory not mapped'
>
> Traceback:
> ?1: file.copy(pkgdir, vd2, recursive = TRUE)
> ?2: run_vignettes(desc)
> ?3: check_pkg(pkgdir, pkgname, pkgoutdir, startdir, libdir, desc,
> is_base_pkg, is_rec_pkg, thispkg_subdirs, extra_arch)
> ?4: tools:::.check_packages()
> aborting ...
> Segmentation fault (core dumped)
>
> as one can see, the error stems from the call
> file.copy(pkgdir, vd2, recursive = TRUE)
> in the run_vignettes function defined in the tools:::.check_packages
> function. this call will copy the contents of "pkgdir" (the source
> directory) to the ..Rcheck/vign_test/ directory _recursively_, the
> problem of which being that the "..Rcheck" directory is part of
> "pkgdir". Thus we get into a loop of copying pkgdir into
> pkgdir/..Rcheck/vign_test, which is interrupted at some point by a
> segfault (at least on my x86_64 GNU/Linux machine running Ubuntu 12.04
> and R 2.15.0; the OS might be crucial for this issue).
>
> I can reproduce the error by e.g.:
>
> ?pkgdir <- tempdir()
> ?file.create(file.path(pkgdir,"DESCRIPTION")) ? ?# not necessary
> ?vd2 <- file.path(pkgdir,"..Rcheck/vign_test")
> ?dir.create(vd2, recursive=TRUE)
> ?### Better Do NOT run -> buffer overflow
> ?# file.copy(pkgdir, vd2, recursive=TRUE)
> ?###
>
> Can anybody confirm this problem? Of course, a workaround is to run R
> CMD check from outside the source directory (which is probably the
> preferred way).
>
> Thanks in advance,
> Sebastian
>
>
> --
> Sebastian Meyer
> Division of Biostatistics
> Institute of Social and Preventive Medicine
> University of Zurich
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


