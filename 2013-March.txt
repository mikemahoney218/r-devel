From simon.urbanek at r-project.org  Fri Mar  1 02:04:45 2013
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Thu, 28 Feb 2013 20:04:45 -0500
Subject: [Rd] conflict between rJava and data.table
In-Reply-To: <65A7F5D5-655A-4DCA-8A0F-F4ADD18DF0E7@lautloscrew.com>
References: <65A7F5D5-655A-4DCA-8A0F-F4ADD18DF0E7@lautloscrew.com>
Message-ID: <B086D600-A17F-42F4-9F7F-A313277DB16A@r-project.org>

On Feb 28, 2013, at 5:09 PM, Bunny wrote:

> Dear devel-listers, 
> 
> I found a conflct between rJava and data.table. Actually me questions is where to report it? 
> Should I rather send it directly to the package maintainers or post it on some bug tracker. 
> The problem is that data.table has a function called "J" and rJava uses the same quite intensively. 
> I used the  xlsx R package which depends on rJava to write .xls files and ran into an error. 
> 
> write.xls from this package uses the functions and returns an error depending on the sequence the packages
> were loaded. 
> 
> 
> Error in .jnew("org/apache/poi/xssf/usermodel/XSSFWorkbook") : 
>  java.lang.AbstractMethodError: java.lang.ClassLoader.loadClass(Ljava/lang/String;)Ljava/lang/Class;
> 
> 
> data.table::J
> rJava::J
> 
> I can work around this by loading and unloading packages, but I feel this should be addressed because 
> loading these two packages that both deal with tables of data does not seem that unlikely to me. 
> 

Can you elaborate on the details as of where this will be a problem? Packages should not be affected since they should be importing the namespaces from the packages they use, so the only problem would be in a package that uses both data.table and rJava --  and this is easily resolved in the namespace of such package. So there is no technical reason why you can't have multiple definitions of J - that's what namespaces are for.

The error you report is entirely unrelated to J -- at lest in isolation. If you have a reproducible example, please share it.

Cheers,
Simon




> best
> 
> matt
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 
> 


From mailinglist.honeypot at gmail.com  Fri Mar  1 02:49:10 2013
From: mailinglist.honeypot at gmail.com (Steve Lianoglou)
Date: Thu, 28 Feb 2013 20:49:10 -0500
Subject: [Rd] conflict between rJava and data.table
In-Reply-To: <65A7F5D5-655A-4DCA-8A0F-F4ADD18DF0E7@lautloscrew.com>
References: <65A7F5D5-655A-4DCA-8A0F-F4ADD18DF0E7@lautloscrew.com>
Message-ID: <CAHA9McM3DsxsREd-p-1AzS1-9wivXzLEyFCD68AH4TbF=6=R4w@mail.gmail.com>

Hi,

On Thu, Feb 28, 2013 at 5:09 PM, Bunny <bunny at lautloscrew.com> wrote:
> Dear devel-listers,
>
> I found a conflct between rJava and data.table. Actually me questions is where to report it?
> Should I rather send it directly to the package maintainers or post it on some bug tracker.
> The problem is that data.table has a function called "J" and rJava uses the same quite intensively.
[snip]

The development version of data.table no longer exports J from, but
once could still use J inside data.tabe[ ... ] calls.

I reckon using that would solve your problem. I'm not sure what
version of data.table you can get by installing from R-forge, but you
can either check out from subversion or download the latest source
tar-ball from R-forge and install from source ...

HTH,
-steve

--
Steve Lianoglou
Graduate Student: Computational Systems Biology
 | Memorial Sloan-Kettering Cancer Center
 | Weill Medical College of Cornell University
Contact Info: http://cbio.mskcc.org/~lianos/contact


From mailinglist.honeypot at gmail.com  Fri Mar  1 02:50:48 2013
From: mailinglist.honeypot at gmail.com (Steve Lianoglou)
Date: Thu, 28 Feb 2013 20:50:48 -0500
Subject: [Rd] conflict between rJava and data.table
In-Reply-To: <CAHA9McM3DsxsREd-p-1AzS1-9wivXzLEyFCD68AH4TbF=6=R4w@mail.gmail.com>
References: <65A7F5D5-655A-4DCA-8A0F-F4ADD18DF0E7@lautloscrew.com>
	<CAHA9McM3DsxsREd-p-1AzS1-9wivXzLEyFCD68AH4TbF=6=R4w@mail.gmail.com>
Message-ID: <CAHA9McPqyL91tJYSpShAJaGAqEfOfwj1a76P0ezAgcvkJ=nsqg@mail.gmail.com>

Ugh, sorry, I meant to say:

On Thu, Feb 28, 2013 at 8:49 PM, Steve Lianoglou
<mailinglist.honeypot at gmail.com> wrote:
[snip]
> The development version of data.table no longer exports J from, but
> once could still use J inside data.tabe[ ... ] calls.

The development version of data.table no longer exports J, so this
shouldn't happen. One could still use J from inside data.table[ ... ]
expression, though.

Hope that's a bit more clear.

-steve

> I reckon using that would solve your problem. I'm not sure what
> version of data.table you can get by installing from R-forge, but you
> can either check out from subversion or download the latest source
> tar-ball from R-forge and install from source ...
>
> HTH,
> -steve
>
> --
> Steve Lianoglou
> Graduate Student: Computational Systems Biology
>  | Memorial Sloan-Kettering Cancer Center
>  | Weill Medical College of Cornell University
> Contact Info: http://cbio.mskcc.org/~lianos/contact



-- 
Steve Lianoglou
Graduate Student: Computational Systems Biology
 | Memorial Sloan-Kettering Cancer Center
 | Weill Medical College of Cornell University
Contact Info: http://cbio.mskcc.org/~lianos/contact


From hkawakat at gmail.com  Fri Mar  1 12:27:17 2013
From: hkawakat at gmail.com (Hiroyuki Kawakatsu)
Date: Fri, 1 Mar 2013 11:27:17 +0000
Subject: [Rd] predict.loess() segfaults for large n?
Message-ID: <CADBEN2wB5cruNHRb7vpMFfWxNnRnQ=kJnNdH9gC+zxePGxXPFg@mail.gmail.com>

Hi,

I am segfaulting when using predict.loess() (checked with r62092).
I've traced the source with the help of valgrind (output pasted
below) and it appears that this is due to int overflow when
allocating an int work array in loess_workspace():

    liv = 50 + ((int)pow((double)2, (double)D) + 4) * nvmax + 2 * N;

where liv is an (global) int. For D=1 (one x variable), this
overflows at approx N = 4089 where N is the fitted sample size (not
prediction sample size).

I am aware that you are in the process of introducing long vectors
but a quick fix would be to error when predict.loess(..., se=TRUE)
and N is too large. (Ideally, one would use long int but does
fortran portably support long int?) The threshold N value may depend
on surface type (above is for surface=="interpolate").

The following sample code does not result in segfault but when run
with valgrind, it produces the warning about large range. (In the
code that segfaults N is about 77,000).

set.seed(1)
n = 5000      # n=4000 seems ok
x = rnorm(n)
y = x + rnorm(n)
yf = loess(y~x, span=0.75, control=loess.control(trace.hat="approximate"))
print( predict(yf, data.frame(x=1), se=TRUE) )

##---valgrid output with segfault (abridged):

> test4()
==30841== Warning: set address range perms: large range [0x3962a040,
0x5fb42608) (defined)
==30841== Warning: set address range perms: large range [0x5fb43040,
0xf8c8e130) (defined)
==30841== Invalid write of size 4
==30841==    at 0xCD719F0: ehg139_ (loessf.f:1444)
==30841==    by 0xCD72E0C: ehg131_ (loessf.f:467)
==30841==    by 0xCD73A5A: lowesb_ (loessf.f:1530)
==30841==    by 0xCD2C774: loess_ise (loessc.c:219)
==30841==    by 0x486C7F: do_dotCode (dotcode.c:1744)
==30841==    by 0x4AB040: bcEval (eval.c:4544)
==30841==    by 0x4B6B3F: Rf_eval (eval.c:498)
==30841==    by 0x4BAD87: Rf_applyClosure (eval.c:960)
==30841==    by 0x4B6D5E: Rf_eval (eval.c:611)
==30841==    by 0x4B7A1E: do_eval (eval.c:2193)
==30841==    by 0x4AB040: bcEval (eval.c:4544)
==30841==    by 0x4B6B3F: Rf_eval (eval.c:498)
==30841==  Address 0xf8cd4144 is not stack'd, malloc'd or (recently)
free'd
==30841==

 *** caught segfault ***
address 0xf8cd4144, cause 'memory not mapped'

Traceback:
 1: predLoess(y, x, newx, s, weights, pars$robust, pars$span,
pars$degree,     pars$normalize, pars$parametric, pars$drop.square,
pars$surface,     pars$cell, pars$family, kd, divisor, se = se)
 2: eval(expr, envir, enclos)
 3: eval(substitute(expr), data, enclos = parent.frame())
 4: with.default(object, predLoess(y, x, newx, s, weights,
pars$robust,     pars$span, pars$degree, pars$normalize,
pars$parametric,     pars$drop.square, pars$surface, pars$cell,
pars$family, kd,     divisor, se = se))
 5: with(object, predLoess(y, x, newx, s, weights, pars$robust,
pars$span,     pars$degree, pars$normalize, pars$parametric,
pars$drop.square,     pars$surface, pars$cell, pars$family, kd,
divisor, se = se))
 6: predict.loess(y2, data.frame(hours = xmin), se = TRUE)
 7: predict(y2, data.frame(hours = xmin), se = TRUE)
 8: test4()
aborting ...
==30841==


-- 
+---
| Hiroyuki Kawakatsu
| Business School, Dublin City University
| Dublin 9, Ireland. Tel +353 (0)1 700 7496


From mdowle at mdowle.plus.com  Fri Mar  1 14:03:10 2013
From: mdowle at mdowle.plus.com (Matthew Dowle)
Date: Fri, 01 Mar 2013 13:03:10 +0000
Subject: [Rd] conflict between rJava and data.table
Message-ID: <a0fcc30c57df7d1f0255dceff26c9dca@imap.plus.net>


Simon Urbanek wrote :
> Can you elaborate on the details as of where this will be a problem? 
> Packages
> should not be affected since they should be importing the namespaces 
> from the
> packages they use, so the only problem would be in a package that 
> uses both
> data.table and rJava --  and this is easily resolved in the namespace 
> of such
> package. So there is no technical reason why you can't have multiple
> definitions of J - that's what namespaces are for.

Right. It's users using J() in their own code, iiuc. rJava's manual 
says "J is
the high-level access to Java."  When they use J() on its own they 
probably
want the rJava one, but if data.table is higher they get that one.
They don't want to have to write out rJava::J(...).

It is not just rJava but package XLConnect, too. If there's a better 
way would
be interested but I didn't mind removing J from data.table.

Bunny/Matt,

To add to Steve's reply here's some background. This is well documented 
in NEWS
and Googling "data.table J rJava" and similar returns useful links to 
NEWS and
datatable-help (so you shouldn't have needed to post to r-devel).

 From 1.8.2 (Jul 2012) :

o  The J() alias is now deprecated outside DT[...], but will still work 
inside
    DT[...], as in DT[J(...)].
    J() is conflicting with function J() in package XLConnect (#1747)
    and rJava (#2045). For data.table to change is easier, with some 
efficiency
    advantages too. The next version of data.table will issue a warning 
from J()
    when used outside DT[...]. The version after will remove it. Only 
then will
    the conflict with rJava and XLConnect be resolved.
    Please use data.table() directly instead of J(), outside DT[...].

 From 1.8.4 (Nov 2012) :

o  J() now issues a warning (when used *outside* DT[...]) that using it
    outside DT[...] is deprecated. See item below in v1.8.2.
    Use data.table() directly instead of J(), outside DT[...]. Or, 
define
    an alias yourself. J() will continue to work *inside* DT[...] as 
documented.

 From 1.8.7 (soon to be on CRAN) :

o  The J() alias is now removed *outside* DT[...], but will still work 
inside DT[...];
    i.e., DT[J(...)] is fine. As warned in v1.8.2 (see below in this 
file) and deprecated
    with warning() in v1.8.6. This resolves the conflict with function 
J() in package
    XLConnect (#1747) and rJava (#2045).
    Please use data.table() directly instead of J(), outside DT[...].

Matthew


From simon.urbanek at r-project.org  Fri Mar  1 17:13:04 2013
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Fri, 1 Mar 2013 11:13:04 -0500
Subject: [Rd] conflict between rJava and data.table
In-Reply-To: <a0fcc30c57df7d1f0255dceff26c9dca@imap.plus.net>
References: <a0fcc30c57df7d1f0255dceff26c9dca@imap.plus.net>
Message-ID: <0B256578-A624-4D46-91AE-7683A84462EE@r-project.org>

On Mar 1, 2013, at 8:03 AM, Matthew Dowle wrote:

> 
> Simon Urbanek wrote :
>> Can you elaborate on the details as of where this will be a problem? Packages
>> should not be affected since they should be importing the namespaces from the
>> packages they use, so the only problem would be in a package that uses both
>> data.table and rJava --  and this is easily resolved in the namespace of such
>> package. So there is no technical reason why you can't have multiple
>> definitions of J - that's what namespaces are for.
> 
> Right. It's users using J() in their own code, iiuc. rJava's manual says "J is
> the high-level access to Java."  When they use J() on its own they probably
> want the rJava one, but if data.table is higher they get that one.
> They don't want to have to write out rJava::J(...).
> 
> It is not just rJava but package XLConnect, too. If there's a better way would
> be interested but I didn't mind removing J from data.table.
> 

For packages there is really no issue - if something breaks in XTConnect then the authors are probably importing the wrong function in their namespace (I still didn't see a reproducible example, though). The only difference is for interactive use so not having conflicting J() [if possible] would be actually useful there, since J() in rJava is primarily intended for interactive use.

Cheers,
Simon


> Bunny/Matt,
> 
> To add to Steve's reply here's some background. This is well documented in NEWS
> and Googling "data.table J rJava" and similar returns useful links to NEWS and
> datatable-help (so you shouldn't have needed to post to r-devel).
> 
> From 1.8.2 (Jul 2012) :
> 
> o  The J() alias is now deprecated outside DT[...], but will still work inside
>   DT[...], as in DT[J(...)].
>   J() is conflicting with function J() in package XLConnect (#1747)
>   and rJava (#2045). For data.table to change is easier, with some efficiency
>   advantages too. The next version of data.table will issue a warning from J()
>   when used outside DT[...]. The version after will remove it. Only then will
>   the conflict with rJava and XLConnect be resolved.
>   Please use data.table() directly instead of J(), outside DT[...].
> 
> From 1.8.4 (Nov 2012) :
> 
> o  J() now issues a warning (when used *outside* DT[...]) that using it
>   outside DT[...] is deprecated. See item below in v1.8.2.
>   Use data.table() directly instead of J(), outside DT[...]. Or, define
>   an alias yourself. J() will continue to work *inside* DT[...] as documented.
> 
> From 1.8.7 (soon to be on CRAN) :
> 
> o  The J() alias is now removed *outside* DT[...], but will still work inside DT[...];
>   i.e., DT[J(...)] is fine. As warned in v1.8.2 (see below in this file) and deprecated
>   with warning() in v1.8.6. This resolves the conflict with function J() in package
>   XLConnect (#1747) and rJava (#2045).
>   Please use data.table() directly instead of J(), outside DT[...].
> 
> Matthew
> 
> 
> 


From mdowle at mdowle.plus.com  Fri Mar  1 17:40:31 2013
From: mdowle at mdowle.plus.com (Matthew Dowle)
Date: Fri, 01 Mar 2013 16:40:31 +0000
Subject: [Rd] conflict between rJava and data.table
In-Reply-To: <0B256578-A624-4D46-91AE-7683A84462EE@r-project.org>
References: <a0fcc30c57df7d1f0255dceff26c9dca@imap.plus.net>
	<0B256578-A624-4D46-91AE-7683A84462EE@r-project.org>
Message-ID: <948a0ea9b58f7765e7082753479a2437@imap.plus.net>

On 01.03.2013 16:13, Simon Urbanek wrote:
> On Mar 1, 2013, at 8:03 AM, Matthew Dowle wrote:
>
>>
>> Simon Urbanek wrote :
>>> Can you elaborate on the details as of where this will be a 
>>> problem? Packages
>>> should not be affected since they should be importing the 
>>> namespaces from the
>>> packages they use, so the only problem would be in a package that 
>>> uses both
>>> data.table and rJava --  and this is easily resolved in the 
>>> namespace of such
>>> package. So there is no technical reason why you can't have 
>>> multiple
>>> definitions of J - that's what namespaces are for.
>>
>> Right. It's users using J() in their own code, iiuc. rJava's manual 
>> says "J is
>> the high-level access to Java."  When they use J() on its own they 
>> probably
>> want the rJava one, but if data.table is higher they get that one.
>> They don't want to have to write out rJava::J(...).
>>
>> It is not just rJava but package XLConnect, too. If there's a better 
>> way would
>> be interested but I didn't mind removing J from data.table.
>>
>
> For packages there is really no issue - if something breaks in
> XTConnect then the authors are probably importing the wrong function
> in their namespace (I still didn't see a reproducible example,
> though). The only difference is for interactive use so not having
> conflicting J() [if possible] would be actually useful there, since
> J() in rJava is primarily intended for interactive use.

Yes that's what I wrote above isn't it? i.e.

> It's users using J() in their own code, iiuc.
> "J is the high-level access to Java."

Not just interactive use (i.e. at the R prompt) but inside their 
functions and scripts, too.
Although, I don't know the rJava package at all. So why J() might be 
used for interactive
use but not in functions and scripts isn't clear to me.
Any use of J from example(J) will serve as a reproducible example; 
e.g.,

     library(rJava)          # load rJava first
     library(data.table)     # then data.table
     J("java.lang.Double")

There is no error or warning, but the user would be returned a 1 row 1 
column
data.table rather than something related to Java. Then the 
errors/warnings follow from there.

The user can either load the packages the other way around, or, use ::

     library(rJava)                  # load rJava first
     library(data.table)             # then data.table
     rJava::J("java.lang.Double")    # ok now


>
> Cheers,
> Simon
>
>
>> Bunny/Matt,
>>
>> To add to Steve's reply here's some background. This is well 
>> documented in NEWS
>> and Googling "data.table J rJava" and similar returns useful links 
>> to NEWS and
>> datatable-help (so you shouldn't have needed to post to r-devel).
>>
>> From 1.8.2 (Jul 2012) :
>>
>> o  The J() alias is now deprecated outside DT[...], but will still 
>> work inside
>>   DT[...], as in DT[J(...)].
>>   J() is conflicting with function J() in package XLConnect (#1747)
>>   and rJava (#2045). For data.table to change is easier, with some 
>> efficiency
>>   advantages too. The next version of data.table will issue a 
>> warning from J()
>>   when used outside DT[...]. The version after will remove it. Only 
>> then will
>>   the conflict with rJava and XLConnect be resolved.
>>   Please use data.table() directly instead of J(), outside DT[...].
>>
>> From 1.8.4 (Nov 2012) :
>>
>> o  J() now issues a warning (when used *outside* DT[...]) that using 
>> it
>>   outside DT[...] is deprecated. See item below in v1.8.2.
>>   Use data.table() directly instead of J(), outside DT[...]. Or, 
>> define
>>   an alias yourself. J() will continue to work *inside* DT[...] as 
>> documented.
>>
>> From 1.8.7 (soon to be on CRAN) :
>>
>> o  The J() alias is now removed *outside* DT[...], but will still 
>> work inside DT[...];
>>   i.e., DT[J(...)] is fine. As warned in v1.8.2 (see below in this 
>> file) and deprecated
>>   with warning() in v1.8.6. This resolves the conflict with function 
>> J() in package
>>   XLConnect (#1747) and rJava (#2045).
>>   Please use data.table() directly instead of J(), outside DT[...].
>>
>> Matthew
>>
>>
>>


From simon.urbanek at r-project.org  Fri Mar  1 21:19:36 2013
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Fri, 1 Mar 2013 15:19:36 -0500
Subject: [Rd] conflict between rJava and data.table
In-Reply-To: <948a0ea9b58f7765e7082753479a2437@imap.plus.net>
References: <a0fcc30c57df7d1f0255dceff26c9dca@imap.plus.net>
	<0B256578-A624-4D46-91AE-7683A84462EE@r-project.org>
	<948a0ea9b58f7765e7082753479a2437@imap.plus.net>
Message-ID: <A3216F1C-6209-4160-AB80-9691E2F1ECBF@r-project.org>


On Mar 1, 2013, at 11:40 AM, Matthew Dowle wrote:

> On 01.03.2013 16:13, Simon Urbanek wrote:
>> On Mar 1, 2013, at 8:03 AM, Matthew Dowle wrote:
>> 
>>> 
>>> Simon Urbanek wrote :
>>>> Can you elaborate on the details as of where this will be a problem? Packages
>>>> should not be affected since they should be importing the namespaces from the
>>>> packages they use, so the only problem would be in a package that uses both
>>>> data.table and rJava --  and this is easily resolved in the namespace of such
>>>> package. So there is no technical reason why you can't have multiple
>>>> definitions of J - that's what namespaces are for.
>>> 
>>> Right. It's users using J() in their own code, iiuc. rJava's manual says "J is
>>> the high-level access to Java."  When they use J() on its own they probably
>>> want the rJava one, but if data.table is higher they get that one.
>>> They don't want to have to write out rJava::J(...).
>>> 
>>> It is not just rJava but package XLConnect, too. If there's a better way would
>>> be interested but I didn't mind removing J from data.table.
>>> 
>> 
>> For packages there is really no issue - if something breaks in
>> XTConnect then the authors are probably importing the wrong function
>> in their namespace (I still didn't see a reproducible example,
>> though). The only difference is for interactive use so not having
>> conflicting J() [if possible] would be actually useful there, since
>> J() in rJava is primarily intended for interactive use.
> 
> Yes that's what I wrote above isn't it? i.e.
> 
>> It's users using J() in their own code, iiuc.
>> "J is the high-level access to Java."
> 
> Not just interactive use (i.e. at the R prompt) but inside their functions and scripts, too.
> Although, I don't know the rJava package at all. So why J() might be used for interactive
> use but not in functions and scripts isn't clear to me.
> Any use of J from example(J) will serve as a reproducible example; e.g.,
> 
>    library(rJava)          # load rJava first
>    library(data.table)     # then data.table
>    J("java.lang.Double")
> 
> There is no error or warning, but the user would be returned a 1 row 1 column
> data.table rather than something related to Java. Then the errors/warnings follow from there.
> 
> The user can either load the packages the other way around, or, use ::
> 
>    library(rJava)                  # load rJava first
>    library(data.table)             # then data.table
>    rJava::J("java.lang.Double")    # ok now
> 

Matt,

there are two entirely separate uses 

a) interactive use
b) use in packages

you are describing a) and as I said in the latter part above J() in rJava is meant for that so it would be useful to not have a conflict there.

However, in my first part of the e-mail I was referring to b) where there is no conflict, because packages define which package will a symbol come from, so the user search path plays no role. Today, all packages should be using imports so search path pollution should no longer be an issue, so the order in which the user attached packages to their search path won't affect the functionality of the packages (that's why namespaces are mandatory). Therefore, if XLConnect breaks (again, I don't know, I didn't see it) due to the order on the search path, it indicates there is a bug in the its namespace as it's apparently importing the wrong J - it should be importing it from rJava and not data.table. Is that more clear?

Cheers,
Simon





> 
>> 
>> Cheers,
>> Simon
>> 
>> 
>>> Bunny/Matt,
>>> 
>>> To add to Steve's reply here's some background. This is well documented in NEWS
>>> and Googling "data.table J rJava" and similar returns useful links to NEWS and
>>> datatable-help (so you shouldn't have needed to post to r-devel).
>>> 
>>> From 1.8.2 (Jul 2012) :
>>> 
>>> o  The J() alias is now deprecated outside DT[...], but will still work inside
>>>  DT[...], as in DT[J(...)].
>>>  J() is conflicting with function J() in package XLConnect (#1747)
>>>  and rJava (#2045). For data.table to change is easier, with some efficiency
>>>  advantages too. The next version of data.table will issue a warning from J()
>>>  when used outside DT[...]. The version after will remove it. Only then will
>>>  the conflict with rJava and XLConnect be resolved.
>>>  Please use data.table() directly instead of J(), outside DT[...].
>>> 
>>> From 1.8.4 (Nov 2012) :
>>> 
>>> o  J() now issues a warning (when used *outside* DT[...]) that using it
>>>  outside DT[...] is deprecated. See item below in v1.8.2.
>>>  Use data.table() directly instead of J(), outside DT[...]. Or, define
>>>  an alias yourself. J() will continue to work *inside* DT[...] as documented.
>>> 
>>> From 1.8.7 (soon to be on CRAN) :
>>> 
>>> o  The J() alias is now removed *outside* DT[...], but will still work inside DT[...];
>>>  i.e., DT[J(...)] is fine. As warned in v1.8.2 (see below in this file) and deprecated
>>>  with warning() in v1.8.6. This resolves the conflict with function J() in package
>>>  XLConnect (#1747) and rJava (#2045).
>>>  Please use data.table() directly instead of J(), outside DT[...].
>>> 
>>> Matthew
>>> 
>>> 
>>> 
> 
> 


From mdowle at mdowle.plus.com  Fri Mar  1 23:03:09 2013
From: mdowle at mdowle.plus.com (Matthew Dowle)
Date: Fri, 01 Mar 2013 22:03:09 +0000
Subject: [Rd] conflict between rJava and data.table
In-Reply-To: <A3216F1C-6209-4160-AB80-9691E2F1ECBF@r-project.org>
References: <a0fcc30c57df7d1f0255dceff26c9dca@imap.plus.net>
	<0B256578-A624-4D46-91AE-7683A84462EE@r-project.org>
	<948a0ea9b58f7765e7082753479a2437@imap.plus.net>
	<A3216F1C-6209-4160-AB80-9691E2F1ECBF@r-project.org>
Message-ID: <08fc70a15bb32661a6e921476de95dce@imap.plus.net>

On 01.03.2013 20:19, Simon Urbanek wrote:
> On Mar 1, 2013, at 11:40 AM, Matthew Dowle wrote:
>
>> On 01.03.2013 16:13, Simon Urbanek wrote:
>>> On Mar 1, 2013, at 8:03 AM, Matthew Dowle wrote:
>>>
>>>>
>>>> Simon Urbanek wrote :
>>>>> Can you elaborate on the details as of where this will be a 
>>>>> problem? Packages
>>>>> should not be affected since they should be importing the 
>>>>> namespaces from the
>>>>> packages they use, so the only problem would be in a package that 
>>>>> uses both
>>>>> data.table and rJava --  and this is easily resolved in the 
>>>>> namespace of such
>>>>> package. So there is no technical reason why you can't have 
>>>>> multiple
>>>>> definitions of J - that's what namespaces are for.
>>>>
>>>> Right. It's users using J() in their own code, iiuc. rJava's 
>>>> manual says "J is
>>>> the high-level access to Java."  When they use J() on its own they 
>>>> probably
>>>> want the rJava one, but if data.table is higher they get that one.
>>>> They don't want to have to write out rJava::J(...).
>>>>
>>>> It is not just rJava but package XLConnect, too. If there's a 
>>>> better way would
>>>> be interested but I didn't mind removing J from data.table.
>>>>
>>>
>>> For packages there is really no issue - if something breaks in
>>> XTConnect then the authors are probably importing the wrong 
>>> function
>>> in their namespace (I still didn't see a reproducible example,
>>> though). The only difference is for interactive use so not having
>>> conflicting J() [if possible] would be actually useful there, since
>>> J() in rJava is primarily intended for interactive use.
>>
>> Yes that's what I wrote above isn't it? i.e.
>>
>>> It's users using J() in their own code, iiuc.
>>> "J is the high-level access to Java."
>>
>> Not just interactive use (i.e. at the R prompt) but inside their 
>> functions and scripts, too.
>> Although, I don't know the rJava package at all. So why J() might be 
>> used for interactive
>> use but not in functions and scripts isn't clear to me.
>> Any use of J from example(J) will serve as a reproducible example; 
>> e.g.,
>>
>>    library(rJava)          # load rJava first
>>    library(data.table)     # then data.table
>>    J("java.lang.Double")
>>
>> There is no error or warning, but the user would be returned a 1 row 
>> 1 column
>> data.table rather than something related to Java. Then the 
>> errors/warnings follow from there.
>>
>> The user can either load the packages the other way around, or, use 
>> ::
>>
>>    library(rJava)                  # load rJava first
>>    library(data.table)             # then data.table
>>    rJava::J("java.lang.Double")    # ok now
>>
>
> Matt,
>
> there are two entirely separate uses
>
> a) interactive use
> b) use in packages
>
> you are describing a) and as I said in the latter part above J() in
> rJava is meant for that so it would be useful to not have a conflict
> there.

Yes (a) is the problem. Good, so I did the right thing in July 2012
by starting to deprecate J in data.table when this problem was first
reported.

> However, in my first part of the e-mail I was referring to b) where
> there is no conflict, because packages define which package will a
> symbol come from, so the user search path plays no role. Today, all
> packages should be using imports so search path pollution should no
> longer be an issue, so the order in which the user attached packages
> to their search path won't affect the functionality of the packages
> (that's why namespaces are mandatory). Therefore, if XLConnect breaks
> (again, I don't know, I didn't see it) due to the order on the search
> path, it indicates there is a bug in the its namespace as it's
> apparently importing the wrong J - it should be importing it from
> rJava and not data.table. Is that more clear?

Yes, thanks. (b) isn't a problem. rJava and XLConnect aren't breaking,
the users aren't reporting that. It's merely problem (a); e.g. where
end users of both rJava and data.table use J() in their own code.

>
> Cheers,
> Simon
>
>
>>
>>>
>>> Cheers,
>>> Simon
>>>
>>>
>>>> Bunny/Matt,
>>>>
>>>> To add to Steve's reply here's some background. This is well 
>>>> documented in NEWS
>>>> and Googling "data.table J rJava" and similar returns useful links 
>>>> to NEWS and
>>>> datatable-help (so you shouldn't have needed to post to r-devel).
>>>>
>>>> From 1.8.2 (Jul 2012) :
>>>>
>>>> o  The J() alias is now deprecated outside DT[...], but will still 
>>>> work inside
>>>>  DT[...], as in DT[J(...)].
>>>>  J() is conflicting with function J() in package XLConnect (#1747)
>>>>  and rJava (#2045). For data.table to change is easier, with some 
>>>> efficiency
>>>>  advantages too. The next version of data.table will issue a 
>>>> warning from J()
>>>>  when used outside DT[...]. The version after will remove it. Only 
>>>> then will
>>>>  the conflict with rJava and XLConnect be resolved.
>>>>  Please use data.table() directly instead of J(), outside DT[...].
>>>>
>>>> From 1.8.4 (Nov 2012) :
>>>>
>>>> o  J() now issues a warning (when used *outside* DT[...]) that 
>>>> using it
>>>>  outside DT[...] is deprecated. See item below in v1.8.2.
>>>>  Use data.table() directly instead of J(), outside DT[...]. Or, 
>>>> define
>>>>  an alias yourself. J() will continue to work *inside* DT[...] as 
>>>> documented.
>>>>
>>>> From 1.8.7 (soon to be on CRAN) :
>>>>
>>>> o  The J() alias is now removed *outside* DT[...], but will still 
>>>> work inside DT[...];
>>>>  i.e., DT[J(...)] is fine. As warned in v1.8.2 (see below in this 
>>>> file) and deprecated
>>>>  with warning() in v1.8.6. This resolves the conflict with 
>>>> function J() in package
>>>>  XLConnect (#1747) and rJava (#2045).
>>>>  Please use data.table() directly instead of J(), outside DT[...].
>>>>
>>>> Matthew
>>>>
>>>>
>>>>
>>
>>


From phaebz at gmail.com  Fri Mar  1 22:53:46 2013
From: phaebz at gmail.com (Michael Bach)
Date: Fri, 1 Mar 2013 22:53:46 +0100
Subject: [Rd] .Call interface: Use R SEXP as C mutable *char
Message-ID: <kgr816$432$1@ger.gmane.org>

Dear R Developers,


DISCLAIMER: I am new to package development in R and new to this list.

I am trying to do something along the lines of:

SEXP test_fun (SEXP filename) {

const char *inputfile = translateChar(STRING_ELT(filename, 0));

int abc = some_function(inputfile);

...

}

The code compiles fine, but I get a warning:
"passing argument of 'some_function' discards qualifiers from pointer 
target type"

I read up on my issue and found this posting:
https://stat.ethz.ch/pipermail/r-devel/2011-June/061221.html

I gather that the 'some_function' (which is a function from another 
library) takes just '*char' as argument type so the 'const' qualifier is 
discarded.

Of course I want my package to compile without warnings. All my other 
attempts led to similar 'discard' warnings (mainly initializations of 
helper variables).

What is the recommended approach here?

Best Regards,
Michael Bach


From simon.urbanek at r-project.org  Fri Mar  1 23:32:38 2013
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Fri, 1 Mar 2013 17:32:38 -0500
Subject: [Rd] .Call interface: Use R SEXP as C mutable *char
In-Reply-To: <kgr816$432$1@ger.gmane.org>
References: <kgr816$432$1@ger.gmane.org>
Message-ID: <F9DF50AC-DA88-47E8-9DAC-35FE6F0EDAC9@r-project.org>

Michael,

On Mar 1, 2013, at 4:53 PM, Michael Bach wrote:

> Dear R Developers,
> 
> 
> DISCLAIMER: I am new to package development in R and new to this list.
> 
> I am trying to do something along the lines of:
> 
> SEXP test_fun (SEXP filename) {
> 
> const char *inputfile = translateChar(STRING_ELT(filename, 0));
> 
> int abc = some_function(inputfile);
> 
> ...
> 
> }
> 
> The code compiles fine, but I get a warning:
> "passing argument of 'some_function' discards qualifiers from pointer target type"
> 
> I read up on my issue and found this posting:
> https://stat.ethz.ch/pipermail/r-devel/2011-June/061221.html
> 
> I gather that the 'some_function' (which is a function from another library) takes just '*char' as argument type so the 'const' qualifier is discarded.
> 
> Of course I want my package to compile without warnings. All my other attempts led to similar 'discard' warnings (mainly initializations of helper variables).
> 
> What is the recommended approach here?
> 

Well, it really depends on some_function. The issue here is that inputfile you get is immutable (aka read-only). However, the warning tells you that some_function() declares that it wants to modify its input, so you cannot pass an immutable object to it. So there are two options (rather just one, really ;)):

a) some_function() really means it, you have to create a copy - there are many ways to do it, this is just one of them, pick your best
static char buf[512];
if (strlen(inputfile) + 1 > sizeof(buf)) Rf_error("File name is too long");
strcpy(buf, inputfile);
int abc = some_function(buf);

b) some_function() doesn't really mean it - it's just a bug in the declaration and the author really meant
int some_function(const char *fn)
This is dangerous, because you have to know for sure that this is a bug that will be fixed. Meanwhile you can work around the bug with
int abc = some_function((char*) buf);
but that will remove all checking so if some_function() decides to actually modify the argument (which it legally can as it was telling you it will), you are in deep trouble, because memory is being corrupted affecting the whole R. So don't do this!

Cheers,
Simon


> Best Regards,
> Michael Bach
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 
> 


From phaebz at gmail.com  Sat Mar  2 19:37:10 2013
From: phaebz at gmail.com (Michael Bach)
Date: Sat, 2 Mar 2013 19:37:10 +0100
Subject: [Rd] .Call interface: Use R SEXP as C mutable *char
In-Reply-To: <F9DF50AC-DA88-47E8-9DAC-35FE6F0EDAC9@r-project.org>
References: <kgr816$432$1@ger.gmane.org>
	<F9DF50AC-DA88-47E8-9DAC-35FE6F0EDAC9@r-project.org>
Message-ID: <513246D6.8010509@gmail.com>

On 3/1/13 11:32 PM, Simon Urbanek wrote:
>> I am trying to do something along the lines of:
>>
>> SEXP test_fun (SEXP filename) {
>>
>> const char *inputfile = translateChar(STRING_ELT(filename, 0));
>>
>> int abc = some_function(inputfile);
>>
>> ...
>>
>> }
>>
>> The code compiles fine, but I get a warning:
>> "passing argument of 'some_function' discards qualifiers from pointer target type"
>>
>> I read up on my issue and found this posting:
>> https://stat.ethz.ch/pipermail/r-devel/2011-June/061221.html
>>
>> I gather that the 'some_function' (which is a function from another library) takes just '*char' as argument type so the 'const' qualifier is discarded.
>>
>> Of course I want my package to compile without warnings. All my other attempts led to similar 'discard' warnings (mainly initializations of helper variables).
>>
>> What is the recommended approach here?
>>
>
> Well, it really depends on some_function. The issue here is that inputfile you get is immutable (aka read-only). However, the warning tells you that some_function() declares that it wants to modify its input, so you cannot pass an immutable object to it. So there are two options (rather just one, really ;)):
>
> a) some_function() really means it, you have to create a copy - there are many ways to do it, this is just one of them, pick your best
> static char buf[512];
> if (strlen(inputfile) + 1 > sizeof(buf)) Rf_error("File name is too long");
> strcpy(buf, inputfile);
> int abc = some_function(buf);
>
> b) some_function() doesn't really mean it - it's just a bug in the declaration and the author really meant
> int some_function(const char *fn)
> This is dangerous, because you have to know for sure that this is a bug that will be fixed. Meanwhile you can work around the bug with
> int abc = some_function((char*) buf);
> but that will remove all checking so if some_function() decides to actually modify the argument (which it legally can as it was telling you it will), you are in deep trouble, because memory is being corrupted affecting the whole R. So don't do this!
>

I took The Only True Option b), i.e. a manual 'fix' and modified the .c 
and .h argument lists to 'const char *fn' after confirming that 
some_function() does indeed not change the argument. I also filed a bug 
and submitted a patch upstream.

Thanks Simon for clarifications!

Best Regards,
Michael


From phaebz at gmail.com  Sat Mar  2 19:37:10 2013
From: phaebz at gmail.com (Michael Bach)
Date: Sat, 2 Mar 2013 19:37:10 +0100
Subject: [Rd] .Call interface: Use R SEXP as C mutable *char
In-Reply-To: <F9DF50AC-DA88-47E8-9DAC-35FE6F0EDAC9@r-project.org>
References: <kgr816$432$1@ger.gmane.org>
	<F9DF50AC-DA88-47E8-9DAC-35FE6F0EDAC9@r-project.org>
Message-ID: <513246D6.8010509@gmail.com>

On 3/1/13 11:32 PM, Simon Urbanek wrote:
>> I am trying to do something along the lines of:
>>
>> SEXP test_fun (SEXP filename) {
>>
>> const char *inputfile = translateChar(STRING_ELT(filename, 0));
>>
>> int abc = some_function(inputfile);
>>
>> ...
>>
>> }
>>
>> The code compiles fine, but I get a warning:
>> "passing argument of 'some_function' discards qualifiers from pointer target type"
>>
>> I read up on my issue and found this posting:
>> https://stat.ethz.ch/pipermail/r-devel/2011-June/061221.html
>>
>> I gather that the 'some_function' (which is a function from another library) takes just '*char' as argument type so the 'const' qualifier is discarded.
>>
>> Of course I want my package to compile without warnings. All my other attempts led to similar 'discard' warnings (mainly initializations of helper variables).
>>
>> What is the recommended approach here?
>>
>
> Well, it really depends on some_function. The issue here is that inputfile you get is immutable (aka read-only). However, the warning tells you that some_function() declares that it wants to modify its input, so you cannot pass an immutable object to it. So there are two options (rather just one, really ;)):
>
> a) some_function() really means it, you have to create a copy - there are many ways to do it, this is just one of them, pick your best
> static char buf[512];
> if (strlen(inputfile) + 1 > sizeof(buf)) Rf_error("File name is too long");
> strcpy(buf, inputfile);
> int abc = some_function(buf);
>
> b) some_function() doesn't really mean it - it's just a bug in the declaration and the author really meant
> int some_function(const char *fn)
> This is dangerous, because you have to know for sure that this is a bug that will be fixed. Meanwhile you can work around the bug with
> int abc = some_function((char*) buf);
> but that will remove all checking so if some_function() decides to actually modify the argument (which it legally can as it was telling you it will), you are in deep trouble, because memory is being corrupted affecting the whole R. So don't do this!
>

I took The Only True Option b), i.e. a manual 'fix' and modified the .c 
and .h argument lists to 'const char *fn' after confirming that 
some_function() does indeed not change the argument. I also filed a bug 
and submitted a patch upstream.

Thanks Simon for clarifications!

Best Regards,
Michael


From mtmorgan at fhcrc.org  Sun Mar  3 19:11:00 2013
From: mtmorgan at fhcrc.org (Martin Morgan)
Date: Sun, 03 Mar 2013 10:11:00 -0800
Subject: [Rd] Missing PROTECT in mkPRIMSXP ?
Message-ID: <51339234.1030109@fhcrc.org>

The Bioconductor build for a package DirichletMultinomial on

   R Under development (unstable) (2013-02-26 r62077) -- "Unsuffered Consequences"

at

 
http://bioconductor.org/checkResults/devel/bioc-LATEST/DirichletMultinomial/george2-buildsrc.html

shows

* creating vignettes ... ERROR
...

Error: processing vignette ?DirichletMultinomial.Rnw? failed with diagnostics:
  chunk 21 (label = ROC-dmngroup)
Error in mapply(FUN = f, ..., SIMPLIFY = FALSE) :
   invalid primitive operation given for dispatch
Execution halted

I have not been able to reproduce this on other systems, but the basic approach 
to a simpler reproducible example is to install the DirichletMultinomial 
Bioconductor package and then

 > tools::buildVignettes(dir="DirichletMultinomial")

where the 'dir' argument points to the uncompressed tarball. There is C code in 
the package, but not reached by this code chunk. Attempts to simplify the code 
generally results in the error disappearing.

R -d valgrind -e "tools::buildVignettes(dir="DirichletMultinomial")"

does not reveal anything. The error does not occur with gctorture(TRUE); 
tools::buildVignettes(dir="DirichletMultinomial").



By setting a breakpoint at Rf_error and creating a back-trace

(gdb) bt
#0  Rf_error (
     format=0x7ffff7a7cef8 "invalid primitive operation given for dispatch")
     at /home/biocbuild/src/R-3.0.r62077/src/main/errors.c:741
#1  0x00007ffff793a7ed in R_possible_dispatch (call=0x164d5c0, op=0x4d772a0,
     args=0x53325d0, rho=0x5334428, promisedArgs=TRUE)
     at /home/biocbuild/src/R-3.0.r62077/src/main/objects.c:1414
#2  0x00007ffff7908463 in Rf_DispatchOrEval (call=0x164d5c0, op=0x4d772a0,
     generic=0x7ffff7a8b0ef "length", args=0x53325d0, rho=0x5334428,
     ans=0x7ffffffe45a0, dropmissing=0, argsevald=1)
     at /home/biocbuild/src/R-3.0.r62077/src/main/eval.c:2413
#3  0x00007ffff792c318 in do_mapply (call=0x164d5c0, op=<optimized out>,
     args=<optimized out>, rho=0x5334428)
     at /home/biocbuild/src/R-3.0.r62077/src/main/mapply.c:49
...

we get to

(gdb) l mapply.c:49
44                  /* Cache the .Primitive: unclear caching is worthwhile. */
45                  static SEXP length_op = NULL;
46                  if (length_op == NULL) length_op = R_Primitive("length");
47                  // DispatchOrEval() needs 'args' to be a pairlist
48                  SEXP ans, tmp2 = PROTECT(list1(tmp1));
49                  if (DispatchOrEval(call, length_op, "length", tmp2, rho, 
&ans, 0, 1))
50                      lengths[i] = (R_xlen_t) (TYPEOF(ans) == REALSXP ?
51                                               REAL(ans)[0] : asInteger(ans));
52                  UNPROTECT(1);
53              }

and it seems like length_op is somehow incorrect. Starting again with a 
breakpoint at mapply.c:46, and watching the memory location of length_op, once 
assigned, for a write at the address of length_op

(gdb) b mapply.c:46
(gdb) r
...
(gdb) p length_op
$11 = (SEXP) 0x4d772a0
(gdb) watch *0x4d772a0
(gdb) c

leads to

(gdb) c
Continuing.
Hardware watchpoint 4: *0x4d772a0

Old value = 8
New value = 0
Rf_allocSExp (t=<optimized out>)
     at /home/biocbuild/src/R-3.0.r62077/src/main/memory.c:2056
2056        TYPEOF(s) = t;

which is R writing a new SEXP to the length_op location, as though this location 
had been garbage collected.

Asking for R_Primitive("length") in mapply.c gets us to dstruct.c:60, where we 
end up at

     if (result == R_NilValue) {
	result = allocSExp(type);
	SET_PRIMOFFSET(result, offset);
     }
...
     return result;

which looks like the allocated SEXP is not added to the PrimCache vector, and 
hence available for garbage collection. Perhaps a patch is along the lines of

Index: dstruct.c
===================================================================
--- dstruct.c	(revision 62113)
+++ dstruct.c	(working copy)
@@ -59,6 +59,7 @@
      if (result == R_NilValue) {
  	result = allocSExp(type);
  	SET_PRIMOFFSET(result, offset);
+	SET_VECTOR_ELT(PrimCache, offset, result);
      }
      else if (TYPEOF(result) != type)
  	error("requested primitive type is not consistent with cached value");


Martin
-- 
Dr. Martin Morgan, PhD
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N.
PO Box 19024 Seattle, WA 98109


From luke-tierney at uiowa.edu  Sun Mar  3 19:55:48 2013
From: luke-tierney at uiowa.edu (luke-tierney at uiowa.edu)
Date: Sun, 3 Mar 2013 12:55:48 -0600
Subject: [Rd] Missing PROTECT in mkPRIMSXP ?
In-Reply-To: <51339234.1030109@fhcrc.org>
References: <51339234.1030109@fhcrc.org>
Message-ID: <alpine.DEB.2.02.1303031255340.3068@luke-Latitude>

Thanks -- fixed in R-devel and R_patched.

luke

On Sun, 3 Mar 2013, Martin Morgan wrote:

> The Bioconductor build for a package DirichletMultinomial on
>
>  R Under development (unstable) (2013-02-26 r62077) -- "Unsuffered 
> Consequences"
>
> at
>
>
> http://bioconductor.org/checkResults/devel/bioc-LATEST/DirichletMultinomial/george2-buildsrc.html
>
> shows
>
> * creating vignettes ... ERROR
> ...
>
> Error: processing vignette ?DirichletMultinomial.Rnw? failed with 
> diagnostics:
> chunk 21 (label = ROC-dmngroup)
> Error in mapply(FUN = f, ..., SIMPLIFY = FALSE) :
>  invalid primitive operation given for dispatch
> Execution halted
>
> I have not been able to reproduce this on other systems, but the basic 
> approach to a simpler reproducible example is to install the 
> DirichletMultinomial Bioconductor package and then
>
>> tools::buildVignettes(dir="DirichletMultinomial")
>
> where the 'dir' argument points to the uncompressed tarball. There is C code 
> in the package, but not reached by this code chunk. Attempts to simplify the 
> code generally results in the error disappearing.
>
> R -d valgrind -e "tools::buildVignettes(dir="DirichletMultinomial")"
>
> does not reveal anything. The error does not occur with gctorture(TRUE); 
> tools::buildVignettes(dir="DirichletMultinomial").
>
>
>
> By setting a breakpoint at Rf_error and creating a back-trace
>
> (gdb) bt
> #0  Rf_error (
>    format=0x7ffff7a7cef8 "invalid primitive operation given for dispatch")
>    at /home/biocbuild/src/R-3.0.r62077/src/main/errors.c:741
> #1  0x00007ffff793a7ed in R_possible_dispatch (call=0x164d5c0, op=0x4d772a0,
>    args=0x53325d0, rho=0x5334428, promisedArgs=TRUE)
>    at /home/biocbuild/src/R-3.0.r62077/src/main/objects.c:1414
> #2  0x00007ffff7908463 in Rf_DispatchOrEval (call=0x164d5c0, op=0x4d772a0,
>    generic=0x7ffff7a8b0ef "length", args=0x53325d0, rho=0x5334428,
>    ans=0x7ffffffe45a0, dropmissing=0, argsevald=1)
>    at /home/biocbuild/src/R-3.0.r62077/src/main/eval.c:2413
> #3  0x00007ffff792c318 in do_mapply (call=0x164d5c0, op=<optimized out>,
>    args=<optimized out>, rho=0x5334428)
>    at /home/biocbuild/src/R-3.0.r62077/src/main/mapply.c:49
> ...
>
> we get to
>
> (gdb) l mapply.c:49
> 44                  /* Cache the .Primitive: unclear caching is worthwhile. 
> */
> 45                  static SEXP length_op = NULL;
> 46                  if (length_op == NULL) length_op = R_Primitive("length");
> 47                  // DispatchOrEval() needs 'args' to be a pairlist
> 48                  SEXP ans, tmp2 = PROTECT(list1(tmp1));
> 49                  if (DispatchOrEval(call, length_op, "length", tmp2, rho, 
> &ans, 0, 1))
> 50                      lengths[i] = (R_xlen_t) (TYPEOF(ans) == REALSXP ?
> 51                                               REAL(ans)[0] : 
> asInteger(ans));
> 52                  UNPROTECT(1);
> 53              }
>
> and it seems like length_op is somehow incorrect. Starting again with a 
> breakpoint at mapply.c:46, and watching the memory location of length_op, 
> once assigned, for a write at the address of length_op
>
> (gdb) b mapply.c:46
> (gdb) r
> ...
> (gdb) p length_op
> $11 = (SEXP) 0x4d772a0
> (gdb) watch *0x4d772a0
> (gdb) c
>
> leads to
>
> (gdb) c
> Continuing.
> Hardware watchpoint 4: *0x4d772a0
>
> Old value = 8
> New value = 0
> Rf_allocSExp (t=<optimized out>)
>    at /home/biocbuild/src/R-3.0.r62077/src/main/memory.c:2056
> 2056        TYPEOF(s) = t;
>
> which is R writing a new SEXP to the length_op location, as though this 
> location had been garbage collected.
>
> Asking for R_Primitive("length") in mapply.c gets us to dstruct.c:60, where 
> we end up at
>
>    if (result == R_NilValue) {
> 	result = allocSExp(type);
> 	SET_PRIMOFFSET(result, offset);
>    }
> ...
>    return result;
>
> which looks like the allocated SEXP is not added to the PrimCache vector, and 
> hence available for garbage collection. Perhaps a patch is along the lines of
>
> Index: dstruct.c
> ===================================================================
> --- dstruct.c	(revision 62113)
> +++ dstruct.c	(working copy)
> @@ -59,6 +59,7 @@
>     if (result == R_NilValue) {
> 	result = allocSExp(type);
> 	SET_PRIMOFFSET(result, offset);
> +	SET_VECTOR_ELT(PrimCache, offset, result);
>     }
>     else if (TYPEOF(result) != type)
> 	error("requested primitive type is not consistent with cached 
> value");
>
>
> Martin
>

-- 
Luke Tierney
Chair, Statistics and Actuarial Science
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu

From pdalgd at gmail.com  Mon Mar  4 07:25:37 2013
From: pdalgd at gmail.com (peter dalgaard)
Date: Mon, 4 Mar 2013 07:25:37 +0100
Subject: [Rd] Missing PROTECT in mkPRIMSXP ?
In-Reply-To: <alpine.DEB.2.02.1303031255340.3068@luke-Latitude>
References: <51339234.1030109@fhcrc.org>
	<alpine.DEB.2.02.1303031255340.3068@luke-Latitude>
Message-ID: <774E7854-C373-4587-8AEE-13CE1335C263@gmail.com>


On Mar 3, 2013, at 19:55 , <luke-tierney at uiowa.edu> <luke-tierney at uiowa.edu> wrote:

> Thanks -- fixed in R-devel and R_patched.

However, notice that the change in R-patched is not likely to make it into a public release since 2.15.3 is officially final and we branch for 3.0.x on Wednesday. (It will be present in the 3.x.y series, of course.)

If this bug bites someone who for whatever reason wants to stay on 2.15.3, they'll have to dig the patch out from SVN.

Peter 

> 
> luke
> 
> On Sun, 3 Mar 2013, Martin Morgan wrote:
> 
>> The Bioconductor build for a package DirichletMultinomial on
>> 
>> R Under development (unstable) (2013-02-26 r62077) -- "Unsuffered Consequences"
>> 
>> at
>> 
>> 
>> http://bioconductor.org/checkResults/devel/bioc-LATEST/DirichletMultinomial/george2-buildsrc.html
>> 
>> shows
>> 
>> * creating vignettes ... ERROR
>> ...
>> 
>> Error: processing vignette ?DirichletMultinomial.Rnw? failed with diagnostics:
>> chunk 21 (label = ROC-dmngroup)
>> Error in mapply(FUN = f, ..., SIMPLIFY = FALSE) :
>> invalid primitive operation given for dispatch
>> Execution halted
>> 
>> I have not been able to reproduce this on other systems, but the basic approach to a simpler reproducible example is to install the DirichletMultinomial Bioconductor package and then
>> 
>>> tools::buildVignettes(dir="DirichletMultinomial")
>> 
>> where the 'dir' argument points to the uncompressed tarball. There is C code in the package, but not reached by this code chunk. Attempts to simplify the code generally results in the error disappearing.
>> 
>> R -d valgrind -e "tools::buildVignettes(dir="DirichletMultinomial")"
>> 
>> does not reveal anything. The error does not occur with gctorture(TRUE); tools::buildVignettes(dir="DirichletMultinomial").
>> 
>> 
>> 
>> By setting a breakpoint at Rf_error and creating a back-trace
>> 
>> (gdb) bt
>> #0  Rf_error (
>>   format=0x7ffff7a7cef8 "invalid primitive operation given for dispatch")
>>   at /home/biocbuild/src/R-3.0.r62077/src/main/errors.c:741
>> #1  0x00007ffff793a7ed in R_possible_dispatch (call=0x164d5c0, op=0x4d772a0,
>>   args=0x53325d0, rho=0x5334428, promisedArgs=TRUE)
>>   at /home/biocbuild/src/R-3.0.r62077/src/main/objects.c:1414
>> #2  0x00007ffff7908463 in Rf_DispatchOrEval (call=0x164d5c0, op=0x4d772a0,
>>   generic=0x7ffff7a8b0ef "length", args=0x53325d0, rho=0x5334428,
>>   ans=0x7ffffffe45a0, dropmissing=0, argsevald=1)
>>   at /home/biocbuild/src/R-3.0.r62077/src/main/eval.c:2413
>> #3  0x00007ffff792c318 in do_mapply (call=0x164d5c0, op=<optimized out>,
>>   args=<optimized out>, rho=0x5334428)
>>   at /home/biocbuild/src/R-3.0.r62077/src/main/mapply.c:49
>> ...
>> 
>> we get to
>> 
>> (gdb) l mapply.c:49
>> 44                  /* Cache the .Primitive: unclear caching is worthwhile. */
>> 45                  static SEXP length_op = NULL;
>> 46                  if (length_op == NULL) length_op = R_Primitive("length");
>> 47                  // DispatchOrEval() needs 'args' to be a pairlist
>> 48                  SEXP ans, tmp2 = PROTECT(list1(tmp1));
>> 49                  if (DispatchOrEval(call, length_op, "length", tmp2, rho, &ans, 0, 1))
>> 50                      lengths[i] = (R_xlen_t) (TYPEOF(ans) == REALSXP ?
>> 51                                               REAL(ans)[0] : asInteger(ans));
>> 52                  UNPROTECT(1);
>> 53              }
>> 
>> and it seems like length_op is somehow incorrect. Starting again with a breakpoint at mapply.c:46, and watching the memory location of length_op, once assigned, for a write at the address of length_op
>> 
>> (gdb) b mapply.c:46
>> (gdb) r
>> ...
>> (gdb) p length_op
>> $11 = (SEXP) 0x4d772a0
>> (gdb) watch *0x4d772a0
>> (gdb) c
>> 
>> leads to
>> 
>> (gdb) c
>> Continuing.
>> Hardware watchpoint 4: *0x4d772a0
>> 
>> Old value = 8
>> New value = 0
>> Rf_allocSExp (t=<optimized out>)
>>   at /home/biocbuild/src/R-3.0.r62077/src/main/memory.c:2056
>> 2056        TYPEOF(s) = t;
>> 
>> which is R writing a new SEXP to the length_op location, as though this location had been garbage collected.
>> 
>> Asking for R_Primitive("length") in mapply.c gets us to dstruct.c:60, where we end up at
>> 
>>   if (result == R_NilValue) {
>> 	result = allocSExp(type);
>> 	SET_PRIMOFFSET(result, offset);
>>   }
>> ...
>>   return result;
>> 
>> which looks like the allocated SEXP is not added to the PrimCache vector, and hence available for garbage collection. Perhaps a patch is along the lines of
>> 
>> Index: dstruct.c
>> ===================================================================
>> --- dstruct.c	(revision 62113)
>> +++ dstruct.c	(working copy)
>> @@ -59,6 +59,7 @@
>>    if (result == R_NilValue) {
>> 	result = allocSExp(type);
>> 	SET_PRIMOFFSET(result, offset);
>> +	SET_VECTOR_ELT(PrimCache, offset, result);
>>    }
>>    else if (TYPEOF(result) != type)
>> 	error("requested primitive type is not consistent with cached value");
>> 
>> 
>> Martin
>> 
> 
> -- 
> Luke Tierney
> Chair, Statistics and Actuarial Science
> Ralph E. Wareham Professor of Mathematical Sciences
> University of Iowa                  Phone:             319-335-3386
> Department of Statistics and        Fax:               319-335-3017
>   Actuarial Science
> 241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
> Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From vitaliy.feoktistov at gmail.com  Mon Mar  4 11:57:50 2013
From: vitaliy.feoktistov at gmail.com (Vitaliy FEOKTISTOV)
Date: Mon, 4 Mar 2013 11:57:50 +0100
Subject: [Rd] multi threaded execution of package
Message-ID: <CAGHrqz_nZhgrzyH2YNX13Owyer=dyn41jzkdMKpqqeMZUbqWPQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20130304/a7098cc9/attachment.pl>

From nicole.haenni at gmail.com  Sun Mar  3 18:22:25 2013
From: nicole.haenni at gmail.com (Nicole Haenni)
Date: Sun, 3 Mar 2013 18:22:25 +0100
Subject: [Rd] Survey for framework and library developers: "Information
 needs in software ecosystems"
In-Reply-To: <CAEM6MMxToG9ZBuh5LKwRWh-Kgevur00bO8FH+dQiHQQH3VVAVw@mail.gmail.com>
References: <CAEM6MMw1_-2NPu74f8=ju98LTV3ja6eAGr5Z5uP_5r+JfRhKEw@mail.gmail.com>
	<CAEM6MMy5t_+-HPgU6Gdapf=8t+V5FD=hXH47Kd139g0ceZxOjw@mail.gmail.com>
	<CAEM6MMw7Pq0mo4XKPbmFrwXzZCNpqYsq4E1V-+c_bodRFzdUVA@mail.gmail.com>
	<CAEM6MMzaz01nJ1SOmR-xU9aA4tavNaJz2PXxWU=VCCPQcF1tVg@mail.gmail.com>
	<CAEM6MMyOKkPY69u==J8qK1OD1tmqEHer5tavjqLA+6baEG3E0Q@mail.gmail.com>
	<CAEM6MMyOBsWimPFAD1S3N05h+AnCZ4SQMwitd2BG6DoU3f7Rkg@mail.gmail.com>
	<CAEM6MMzJYaUFgxV0d328KAmgp20q8yVs5SkHotoYUcbaQoKRLg@mail.gmail.com>
	<CAEM6MMz0p-buhxPD_3Q83JuoZHmjabia6osrpaPP-bHGTjqvdA@mail.gmail.com>
	<CAEM6MMzPHtqy_PorJu9Z6YafN7tsFfCr37LSvv56ymkrv43g3Q@mail.gmail.com>
	<CAEM6MMzAH2eJZcCbzHHMzmHam+SQsPJd38-pWZCzdVPuaupeAg@mail.gmail.com>
	<CAEM6MMxToG9ZBuh5LKwRWh-Kgevur00bO8FH+dQiHQQH3VVAVw@mail.gmail.com>
Message-ID: <CAEM6MMwe=pKv6-x_gdDM=kh=o62-9iE+hpRR+=OjBEG2OSh2vQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20130303/820335dd/attachment.pl>

From simon.urbanek at r-project.org  Mon Mar  4 15:25:07 2013
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Mon, 4 Mar 2013 09:25:07 -0500
Subject: [Rd] multi threaded execution of package
In-Reply-To: <CAGHrqz_nZhgrzyH2YNX13Owyer=dyn41jzkdMKpqqeMZUbqWPQ@mail.gmail.com>
References: <CAGHrqz_nZhgrzyH2YNX13Owyer=dyn41jzkdMKpqqeMZUbqWPQ@mail.gmail.com>
Message-ID: <B69DB70C-1C7B-47EB-AC62-A79AA68BBF63@r-project.org>

On Mar 4, 2013, at 5:57 AM, Vitaliy FEOKTISTOV wrote:

> Hello,
> 
> I'm creating a package for R. This package containes a fortran (wrapped C)
> *.so .
> This dynamic library is compiled to be multi-threaded (-parallel -openmp
> -threads .. options).
> 

Just the fact that it's compiled with such options doesn't mean it will actually run threaded, they just turn on support in case the code wants to use it, they don't actually parallelize anything on their own.


> When I call this library from R : dyn.load("mylib.so")
> the execution is one threaded !
> 
> Where could be a problem and how to correct it ?
> 

Hard to say for sure, but possibly in your code. Would you care to provide the details, such as your platform, the exact code you're using etc?
Note that there are many packages that have no issue with using OpenMP, for example.

Cheers,
Simon


From MEC at stowers.org  Mon Mar  4 22:13:18 2013
From: MEC at stowers.org (Cook, Malcolm)
Date: Mon, 4 Mar 2013 21:13:18 +0000
Subject: [Rd] enabling reproducible research & R package management &
 install.package.version & BiocLite
Message-ID: <D4772401B9D976478C0895769BE3E7920716E6@MBSRV02.sgc.loc>

Hi,

In support of reproducible research at my Institute, I seek an approach to re-creating the R environments in which an analysis has been conducted.

By which I mean, the exact version of R and the exact version of all packages used in a particular R session.

I am seeking comments/criticism of this as a goal, and of the following outline of an approach:

=== When all the steps to an workflow have been finalized ===
* re-run the workflow from beginning to end
* save the results of sessionInfo() into an RDS file named after the current date and time.

=== Later, when desirous of exactly recreating this analysis ===
* read the (old) sessionInfo() into an R session
* exit with failure if the running version of R doesn't match
* compare the old sessionInfo to the currently available installed libraries (i.e. using packageVersion)
* where there are discrepancies, install the required version of the package (without dependencies) into new library (named after the old sessionInfo RDS file)

Then the analyst should be able to put the new library into the front of .libPaths and run the analysis confident that the same version of the packages.

I have in that past used install-package-version.R  to revert to previous versions of R packages successfully (https://gist.github.com/1503736).  And there is a similar tool in Hadley Wickhams devtools.

But, I don't know if I need something special for (BioConductor) packages that have been installed using biocLite and seek advice here.

I do understand that the R environment is not sufficient to guarantee reproducibility.   Some of my colleagues have suggested saving a virtual machine with all your software/library/data installed. So, I am also in general interested in what other people are doing to this end.  But I am most interested in:

* is this a good idea
* is there a worked out solution
* does biocLite introduce special cases
* where do the dragons lurk

... and the like

Any tips?

Thanks,

~ Malcolm Cook
Stowers Institute / Computation Biology / Shilatifard Lab


From mailinglist.honeypot at gmail.com  Mon Mar  4 23:15:05 2013
From: mailinglist.honeypot at gmail.com (Steve Lianoglou)
Date: Mon, 4 Mar 2013 17:15:05 -0500
Subject: [Rd] [BioC] enabling reproducible research & R package
 management & install.package.version & BiocLite
In-Reply-To: <CAErFSogyfzMQ9UEK7e=Ukvgdg-NfxS60cPtppvvvN0MKnogyzw@mail.gmail.com>
References: <D4772401B9D976478C0895769BE3E7920716E6@MBSRV02.sgc.loc>
	<CAErFSogyfzMQ9UEK7e=Ukvgdg-NfxS60cPtppvvvN0MKnogyzw@mail.gmail.com>
Message-ID: <CAHA9McNqVxFAdBYMqJm=avjA8wsS0uVSGQ=6eviXU6iPzPVXVQ@mail.gmail.com>

On Mon, Mar 4, 2013 at 4:28 PM, Aaron Mackey <amackey at virginia.edu> wrote:
> On Mon, Mar 4, 2013 at 4:13 PM, Cook, Malcolm <MEC at stowers.org> wrote:
>
>> * where do the dragons lurk
>>
>
> webs of interconnected dynamically loaded libraries, identical versions of
> R compiled with different BLAS/LAPACK options, etc.  Go with the VM if you
> really, truly, want this level of exact reproducibility.

Sounds like the best bet -- maybe tools like vagrant might be useful here:

http://www.vagrantup.com

... or maybe they're overkill?

Haven't really checked it out myself too much, my impression is that
these tools (vagrant, chef, puppet) are built to handle such cases.

I'd imagine you'd probably need a location where you can grab the
precise (versioned) packages for the things you are specifying, but
...

-steve

-- 
Steve Lianoglou
Graduate Student: Computational Systems Biology
 | Memorial Sloan-Kettering Cancer Center
 | Weill Medical College of Cornell University
Contact Info: http://cbio.mskcc.org/~lianos/contact


From dtenenba at fhcrc.org  Mon Mar  4 23:28:40 2013
From: dtenenba at fhcrc.org (Dan Tenenbaum)
Date: Mon, 4 Mar 2013 14:28:40 -0800
Subject: [Rd] [BioC] enabling reproducible research & R package
 management & install.package.version & BiocLite
In-Reply-To: <CAHA9McNqVxFAdBYMqJm=avjA8wsS0uVSGQ=6eviXU6iPzPVXVQ@mail.gmail.com>
References: <D4772401B9D976478C0895769BE3E7920716E6@MBSRV02.sgc.loc>
	<CAErFSogyfzMQ9UEK7e=Ukvgdg-NfxS60cPtppvvvN0MKnogyzw@mail.gmail.com>
	<CAHA9McNqVxFAdBYMqJm=avjA8wsS0uVSGQ=6eviXU6iPzPVXVQ@mail.gmail.com>
Message-ID: <CAF42j23OD664yG7rgVMYzsEgges97=mn+DcQihne8CQUM0DmOQ@mail.gmail.com>

On Mon, Mar 4, 2013 at 2:15 PM, Steve Lianoglou
<mailinglist.honeypot at gmail.com> wrote:
> On Mon, Mar 4, 2013 at 4:28 PM, Aaron Mackey <amackey at virginia.edu> wrote:
>> On Mon, Mar 4, 2013 at 4:13 PM, Cook, Malcolm <MEC at stowers.org> wrote:
>>
>>> * where do the dragons lurk
>>>
>>
>> webs of interconnected dynamically loaded libraries, identical versions of
>> R compiled with different BLAS/LAPACK options, etc.  Go with the VM if you
>> really, truly, want this level of exact reproducibility.
>
> Sounds like the best bet -- maybe tools like vagrant might be useful here:
>
> http://www.vagrantup.com
>
> ... or maybe they're overkill?
>
> Haven't really checked it out myself too much, my impression is that
> these tools (vagrant, chef, puppet) are built to handle such cases.
>
> I'd imagine you'd probably need a location where you can grab the
> precise (versioned) packages for the things you are specifying, but

Right...and this is a bit tricky, because we don't keep old versions
around in our BioC software repositories.  They are available through
Subversion but with the sometimes additional overhead of setting up
build-time dependencies.

Dan



> ...
>
> -steve
>
> --
> Steve Lianoglou
> Graduate Student: Computational Systems Biology
>  | Memorial Sloan-Kettering Cancer Center
>  | Weill Medical College of Cornell University
> Contact Info: http://cbio.mskcc.org/~lianos/contact
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From xie at yihui.name  Tue Mar  5 00:04:25 2013
From: xie at yihui.name (Yihui Xie)
Date: Mon, 4 Mar 2013 17:04:25 -0600
Subject: [Rd] enabling reproducible research & R package management &
 install.package.version & BiocLite
In-Reply-To: <D4772401B9D976478C0895769BE3E7920716E6@MBSRV02.sgc.loc>
References: <D4772401B9D976478C0895769BE3E7920716E6@MBSRV02.sgc.loc>
Message-ID: <CANROs4cP8ZNJmFnK2_pb_9_ZctPPcouMt8-BOheN-Jyec76r=Q@mail.gmail.com>

Just my 2 cents: it may not be a good idea to restrict software
versions to gain reproducibility. To me, this kind of reproducibility
is "dead" reproducibility (what if the old software has a fatal bug?
do we want to reproduce the same **wrong** results?). Software
packages are continuously evolving, and our research should be adapted
as well. How to achieve this? I think this paper by Robert Gentleman
and Duncan Temple Lang has given a nice answer:
http://biostats.bepress.com/bioconductor/paper2/

With R 3.0.0 coming, it will be easy to achieve what they have
outlined because R 3.0 allows custom vignette builders. Basically,
your research paper can be built with 'R CMD build' and checked with
'R CMD check' if you provide an appropriate builder. An R package has
the great potential of becoming the ideal tool for reproducible
research due to its wonderful infrastructure: functions, datasets,
examples, unit tests, vignettes, dependency structure, and so on. With
the help of version control, you can easily spot the changes after you
upgrade the packages. With an R package, you can automate a lot of
things, e.g. install.packages() will take care of dependencies and R
CMD build can rebuild your paper.

Just like Bioc has a devel version, you can continuously check your
results in a devel version, so that you know what is going to break if
you upgrade to new versions of other packages. Is developing a
research paper too different with developing a software package? (in
the context of computing) Probably not.

Long live the reproducible research!

Regards,
Yihui
--
Yihui Xie <xieyihui at gmail.com>
Phone: 515-294-2465 Web: http://yihui.name
Department of Statistics, Iowa State University
2215 Snedecor Hall, Ames, IA


On Mon, Mar 4, 2013 at 3:13 PM, Cook, Malcolm <MEC at stowers.org> wrote:
> Hi,
>
> In support of reproducible research at my Institute, I seek an approach to re-creating the R environments in which an analysis has been conducted.
>
> By which I mean, the exact version of R and the exact version of all packages used in a particular R session.
>
> I am seeking comments/criticism of this as a goal, and of the following outline of an approach:
>
> === When all the steps to an workflow have been finalized ===
> * re-run the workflow from beginning to end
> * save the results of sessionInfo() into an RDS file named after the current date and time.
>
> === Later, when desirous of exactly recreating this analysis ===
> * read the (old) sessionInfo() into an R session
> * exit with failure if the running version of R doesn't match
> * compare the old sessionInfo to the currently available installed libraries (i.e. using packageVersion)
> * where there are discrepancies, install the required version of the package (without dependencies) into new library (named after the old sessionInfo RDS file)
>
> Then the analyst should be able to put the new library into the front of .libPaths and run the analysis confident that the same version of the packages.
>
> I have in that past used install-package-version.R  to revert to previous versions of R packages successfully (https://gist.github.com/1503736).  And there is a similar tool in Hadley Wickhams devtools.
>
> But, I don't know if I need something special for (BioConductor) packages that have been installed using biocLite and seek advice here.
>
> I do understand that the R environment is not sufficient to guarantee reproducibility.   Some of my colleagues have suggested saving a virtual machine with all your software/library/data installed. So, I am also in general interested in what other people are doing to this end.  But I am most interested in:
>
> * is this a good idea
> * is there a worked out solution
> * does biocLite introduce special cases
> * where do the dragons lurk
>
> ... and the like
>
> Any tips?
>
> Thanks,
>
> ~ Malcolm Cook
> Stowers Institute / Computation Biology / Shilatifard Lab
>


From oliver at first.in-berlin.de  Tue Mar  5 00:15:25 2013
From: oliver at first.in-berlin.de (oliver)
Date: Tue, 5 Mar 2013 00:15:25 +0100
Subject: [Rd] enabling reproducible research & R package management &
 install.package.version & BiocLite
In-Reply-To: <CANROs4cP8ZNJmFnK2_pb_9_ZctPPcouMt8-BOheN-Jyec76r=Q@mail.gmail.com>
References: <D4772401B9D976478C0895769BE3E7920716E6@MBSRV02.sgc.loc>
	<CANROs4cP8ZNJmFnK2_pb_9_ZctPPcouMt8-BOheN-Jyec76r=Q@mail.gmail.com>
Message-ID: <20130304231525.GA3062@siouxsie>

On Mon, Mar 04, 2013 at 05:04:25PM -0600, Yihui Xie wrote:
[...]
> With R 3.0.0 coming, it will be easy to achieve what they have
> outlined because R 3.0 allows custom vignette builders. Basically,
> your research paper can be built with 'R CMD build' and checked with
> 'R CMD check' if you provide an appropriate builder. An R package has
> the great potential of becoming the ideal tool for reproducible
> research due to its wonderful infrastructure: functions, datasets,
> examples, unit tests, vignettes, dependency structure, and so on. With
> the help of version control, you can easily spot the changes after you
> upgrade the packages.
[...]

A new major release number might be the right point to switch
from svn to git.

Branch-and-merge made easy :-)


Ciao,
   Oliver


From btyner at gmail.com  Tue Mar  5 02:45:20 2013
From: btyner at gmail.com (Benjamin Tyner)
Date: Mon, 04 Mar 2013 20:45:20 -0500
Subject: [Rd] crossprod(): g77 versus gfortran
Message-ID: <51354E30.7090802@gmail.com>

Hi

I've got two builds of R, one using g77 (version 3.4.6) and the other
using gfortran (version 4.1.2). The two builds are otherwise identical
as far as I can tell. The one which used g77 performs crossprod()s
roughly twice as fast as the gfortran one. I'm wondering if this rings a
bell with anyone, and if so, are you aware of any configure settings
which will improve the performance when using gfortran. This is on RHEL 5.

Regards
Ben


From ripley at stats.ox.ac.uk  Tue Mar  5 07:11:53 2013
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 05 Mar 2013 06:11:53 +0000
Subject: [Rd] crossprod(): g77 versus gfortran
In-Reply-To: <51354E30.7090802@gmail.com>
References: <51354E30.7090802@gmail.com>
Message-ID: <51358CA9.4030004@stats.ox.ac.uk>

On 05/03/2013 01:45, Benjamin Tyner wrote:
> Hi
>
> I've got two builds of R, one using g77 (version 3.4.6) and the other
> using gfortran (version 4.1.2). The two builds are otherwise identical
> as far as I can tell. The one which used g77 performs crossprod()s
> roughly twice as fast as the gfortran one. I'm wondering if this rings a
> bell with anyone, and if so, are you aware of any configure settings
> which will improve the performance when using gfortran. This is on RHEL 5.

Note that recent versions of R do not build with g77, and have 
performance improvements in linear algebra.  So please follow the 
posting guide and give up the 'at a minimum' information requested and a 
reproducible example.

Also, check what BLAS is in use: an optimized BLAS can make a lot of 
difference on such simple operations.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From amackey at virginia.edu  Mon Mar  4 22:28:48 2013
From: amackey at virginia.edu (Aaron Mackey)
Date: Mon, 4 Mar 2013 16:28:48 -0500
Subject: [Rd] [BioC] enabling reproducible research & R package
 management & install.package.version & BiocLite
In-Reply-To: <D4772401B9D976478C0895769BE3E7920716E6@MBSRV02.sgc.loc>
References: <D4772401B9D976478C0895769BE3E7920716E6@MBSRV02.sgc.loc>
Message-ID: <CAErFSogyfzMQ9UEK7e=Ukvgdg-NfxS60cPtppvvvN0MKnogyzw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20130304/ba1e5ca8/attachment.pl>

From marchywka at hotmail.com  Tue Mar  5 12:23:57 2013
From: marchywka at hotmail.com (Mike Marchywka)
Date: Tue, 5 Mar 2013 06:23:57 -0500
Subject: [Rd] [BioC] enabling reproducible research & R package
 management & install.package.version & BiocLite
In-Reply-To: <CAErFSogyfzMQ9UEK7e=Ukvgdg-NfxS60cPtppvvvN0MKnogyzw@mail.gmail.com>
References: <D4772401B9D976478C0895769BE3E7920716E6@MBSRV02.sgc.loc>,
	<CAErFSogyfzMQ9UEK7e=Ukvgdg-NfxS60cPtppvvvN0MKnogyzw@mail.gmail.com>
Message-ID: <BLU166-W14A5221668A7BDBAC726DEBEFB0@phx.gbl>


I hate to ask what go this thread started but it sounds like someone was counting on?
exact numeric reproducibility or was there a bug in a specific release? In actual?
fact, the best way to determine reproducibility is run the code in a variety of
packages. Alternatively, you can do everything in java and not assume?
that calculations commute or associate as the code is modified but it seems
pointless. Sensitivity determination would seem to lead to more reprodicible results
than trying to keep a specific set of code quirks.

I also seem to recall that FPU may have random lower order bits in some cases,
same code/data give different results. Alsways assume FP is stochastic and plan
on anlayzing the "noise."


----------------------------------------
> From: amackey at virginia.edu
> Date: Mon, 4 Mar 2013 16:28:48 -0500
> To: MEC at stowers.org
> CC: r-devel at r-project.org; bioconductor at r-project.org; r-discussion at listserv.stowers.org
> Subject: Re: [Rd] [BioC] enabling reproducible research & R package management & install.package.version & BiocLite
>
> On Mon, Mar 4, 2013 at 4:13 PM, Cook, Malcolm <MEC at stowers.org> wrote:
>
> > * where do the dragons lurk
> >
>
> webs of interconnected dynamically loaded libraries, identical versions of
> R compiled with different BLAS/LAPACK options, etc. Go with the VM if you
> really, truly, want this level of exact reproducibility.
>
> An alternative (and arguably more useful) strategy would be to cache
> results of each computational step, and report when results differ upon
> re-execution with identical inputs; if you cache sessionInfo along with
> each result, you can identify which package(s) changed, and begin to hunt
> down why the change occurred (possibly for the better); couple this with
> the concept of keeping both code *and* results in version control, then you
> can move forward with a (re)analysis without being crippled by out-of-date
> software.
>
> -Aaron
>
> --
> Aaron J. Mackey, PhD
> Assistant Professor
> Center for Public Health Genomics
> University of Virginia
> amackey at virginia.edu
> http://www.cphg.virginia.edu/mackey
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
 		 	   		  

From MEC at stowers.org  Tue Mar  5 15:36:12 2013
From: MEC at stowers.org (Cook, Malcolm)
Date: Tue, 5 Mar 2013 14:36:12 +0000
Subject: [Rd] [BioC] enabling reproducible research & R package
 management & install.package.version & BiocLite
In-Reply-To: <CAF42j23OD664yG7rgVMYzsEgges97=mn+DcQihne8CQUM0DmOQ@mail.gmail.com>
References: <D4772401B9D976478C0895769BE3E7920716E6@MBSRV02.sgc.loc>
	<CAErFSogyfzMQ9UEK7e=Ukvgdg-NfxS60cPtppvvvN0MKnogyzw@mail.gmail.com>
	<CAHA9McNqVxFAdBYMqJm=avjA8wsS0uVSGQ=6eviXU6iPzPVXVQ@mail.gmail.com>
	<CAF42j23OD664yG7rgVMYzsEgges97=mn+DcQihne8CQUM0DmOQ@mail.gmail.com>
Message-ID: <D4772401B9D976478C0895769BE3E792072B7C@MBSRV02.sgc.loc>

.>>> * where do the dragons lurk
 .>>>
 .>>
 .>> webs of interconnected dynamically loaded libraries, identical versions of
 .>> R compiled with different BLAS/LAPACK options, etc.  Go with the VM if you
 .>> really, truly, want this level of exact reproducibility.
 .>
 .> Sounds like the best bet -- maybe tools like vagrant might be useful here:
 .>
 .> http://www.vagrantup.com
 .>
 .> ... or maybe they're overkill?
 .>
 .> Haven't really checked it out myself too much, my impression is that
 .> these tools (vagrant, chef, puppet) are built to handle such cases.
 .>
 .> I'd imagine you'd probably need a location where you can grab the
 .> precise (versioned) packages for the things you are specifying, but
 .
 .Right...and this is a bit tricky, because we don't keep old versions
 .around in our BioC software repositories.  They are available through
 .Subversion but with the sometimes additional overhead of setting up
 .build-time dependencies.


So, even if I wanted to go where dragons lurked, it would not be possible to cobble a version of biocLite that installed specific versions of software.

Thus, I might rather consider an approach that at 'publish' time tarzips up a copy of the R package dependencies based on a config file defined from sessionInfo and caches it in the project directory.

Then when/if the project is revisited (and found to produce differnt results under current R enviRonment),  I can "simply" install an old R (oops, I guess I'd have to build it), and then un-tarzip the dependencies into the projects own R/Library which I would put on .libpaths.

Or, or?  

(My virtual machine advocating colleagues are snickering now, I am sure......)

Thanks for all your thoughts and advices....

--Malcolm

 .
 .
 .> ...
 .>
 .> -steve
 .>
 .> --
 .> Steve Lianoglou
 .> Graduate Student: Computational Systems Biology
 .>  | Memorial Sloan-Kettering Cancer Center
 .>  | Weill Medical College of Cornell University
 .> Contact Info: http://cbio.mskcc.org/~lianos/contact
 .>
 .> ______________________________________________
 .> R-devel at r-project.org mailing list
 .> https://stat.ethz.ch/mailman/listinfo/r-devel
 .
 ._______________________________________________
 .Bioconductor mailing list
 .Bioconductor at r-project.org
 .https://stat.ethz.ch/mailman/listinfo/bioconductor
 .Search the archives: http://news.gmane.org/gmane.science.biology.informatics.conductor


From MEC at stowers.org  Tue Mar  5 15:48:11 2013
From: MEC at stowers.org (Cook, Malcolm)
Date: Tue, 5 Mar 2013 14:48:11 +0000
Subject: [Rd] [BioC] enabling reproducible research & R package
 management & install.package.version & BiocLite
In-Reply-To: <BLU166-W14A5221668A7BDBAC726DEBEFB0@phx.gbl>
References: <D4772401B9D976478C0895769BE3E7920716E6@MBSRV02.sgc.loc>,
	<CAErFSogyfzMQ9UEK7e=Ukvgdg-NfxS60cPtppvvvN0MKnogyzw@mail.gmail.com>
	<BLU166-W14A5221668A7BDBAC726DEBEFB0@phx.gbl>
Message-ID: <D4772401B9D976478C0895769BE3E7920733A1@MBSRV02.sgc.loc>

All,

What got me started on this line of inquiry was my attempt at balancing the advantages of performing a periodic (daily or weekly) update to the 'release' version of locally installed R/Bioconductor packages on our institute-wide installation of R with the disadvantages of potentially changing the result of an analyst's workflow in mid-project.

I just got the "green light" to institute such periodic updates that I have been arguing is in our collective best interest.  In return,  I promised my best effort to provide a means for preserving or reverting to a working R library configuration.

Please note that the reproducibility I am most eager to provide is limited to reproducibility within the computing environment of our institute, which perhaps takes away some of the dragon's nests, though certainly not all.

There are technical issues of updating package installations on an NFS mount that might have files/libraries open on it from running R sessions.  I am interested in learning of approaches for minimizing/eliminating exposure to these issue as well.  The first/best approach seems to be to institute a 'black out' period when users should expect the installed library to change.   Perhaps there are improvements to this????

Best,

Malcolm


 .-----Original Message-----
 .From: Mike Marchywka [mailto:marchywka at hotmail.com]
 .Sent: Tuesday, March 05, 2013 5:24 AM
 .To: amackey at virginia.edu; Cook, Malcolm
 .Cc: r-devel at r-project.org; bioconductor at r-project.org; r-discussion at listserv.stowers.org
 .Subject: RE: [Rd] [BioC] enabling reproducible research & R package management & install.package.version & BiocLite
 .
 .
 .I hate to ask what go this thread started but it sounds like someone was counting on
 .exact numeric reproducibility or was there a bug in a specific release? In actual
 .fact, the best way to determine reproducibility is run the code in a variety of
 .packages. Alternatively, you can do everything in java and not assume
 .that calculations commute or associate as the code is modified but it seems
 .pointless. Sensitivity determination would seem to lead to more reprodicible results
 .than trying to keep a specific set of code quirks.
 .
 .I also seem to recall that FPU may have random lower order bits in some cases,
 .same code/data give different results. Alsways assume FP is stochastic and plan
 .on anlayzing the "noise."
 .
 .
 .----------------------------------------
 .> From: amackey at virginia.edu
 .> Date: Mon, 4 Mar 2013 16:28:48 -0500
 .> To: MEC at stowers.org
 .> CC: r-devel at r-project.org; bioconductor at r-project.org; r-discussion at listserv.stowers.org
 .> Subject: Re: [Rd] [BioC] enabling reproducible research & R package management & install.package.version & BiocLite
 .>
 .> On Mon, Mar 4, 2013 at 4:13 PM, Cook, Malcolm <MEC at stowers.org> wrote:
 .>
 .> > * where do the dragons lurk
 .> >
 .>
 .> webs of interconnected dynamically loaded libraries, identical versions of
 .> R compiled with different BLAS/LAPACK options, etc. Go with the VM if you
 .> really, truly, want this level of exact reproducibility.
 .>
 .> An alternative (and arguably more useful) strategy would be to cache
 .> results of each computational step, and report when results differ upon
 .> re-execution with identical inputs; if you cache sessionInfo along with
 .> each result, you can identify which package(s) changed, and begin to hunt
 .> down why the change occurred (possibly for the better); couple this with
 .> the concept of keeping both code *and* results in version control, then you
 .> can move forward with a (re)analysis without being crippled by out-of-date
 .> software.
 .>
 .> -Aaron
 .>
 .> --
 .> Aaron J. Mackey, PhD
 .> Assistant Professor
 .> Center for Public Health Genomics
 .> University of Virginia
 .> amackey at virginia.edu
 .> http://www.cphg.virginia.edu/mackey
 .>
 .> [[alternative HTML version deleted]]
 .>
 .> ______________________________________________
 .> R-devel at r-project.org mailing list
 .> https://stat.ethz.ch/mailman/listinfo/r-devel
 .


From jefferis at mrc-lmb.cam.ac.uk  Tue Mar  5 15:49:44 2013
From: jefferis at mrc-lmb.cam.ac.uk (Dr Gregory Jefferis)
Date: Tue, 05 Mar 2013 14:49:44 +0000
Subject: [Rd] [BioC] enabling reproducible research & R package
 management & install.package.version & BiocLite
In-Reply-To: <D4772401B9D976478C0895769BE3E792072B7C@MBSRV02.sgc.loc>
References: <D4772401B9D976478C0895769BE3E7920716E6@MBSRV02.sgc.loc>
	<CAErFSogyfzMQ9UEK7e=Ukvgdg-NfxS60cPtppvvvN0MKnogyzw@mail.gmail.com>
	<CAHA9McNqVxFAdBYMqJm=avjA8wsS0uVSGQ=6eviXU6iPzPVXVQ@mail.gmail.com>
	<CAF42j23OD664yG7rgVMYzsEgges97=mn+DcQihne8CQUM0DmOQ@mail.gmail.com>
	<D4772401B9D976478C0895769BE3E792072B7C@MBSRV02.sgc.loc>
Message-ID: <661EEA68-1C78-4117-8FD4-7B31EAB32E00@mrc-lmb.cam.ac.uk>

On 5 Mar 2013, at 14:36, Cook, Malcolm wrote:

> So, even if I wanted to go where dragons lurked, it would not be 
> possible to cobble a version of biocLite that installed specific 
> versions of software.
>
> Thus, I might rather consider an approach that at 'publish' time 
> tarzips up a copy of the R package dependencies based on a config file 
> defined from sessionInfo and caches it in the project directory.
>
> Then when/if the project is revisited (and found to produce differnt 
> results under current R enviRonment),  I can "simply" install an old R 
> (oops, I guess I'd have to build it), and then un-tarzip the 
> dependencies into the projects own R/Library which I would put on 
> .libpaths.

Sounds a little like this:

http://cran.r-project.org/web/packages/rbundler/index.html

(which I haven't tested). Best,

Greg.

--
PLEASE NOTE CHANGE OF CONTACT DETAILS FROM MON 4TH MARCH:

Gregory Jefferis, PhD                   Tel: 01223 267048
Division of Neurobiology
MRC Laboratory of Molecular Biology
Francis Crick Avenue
Cambridge Biomedical Campus
Cambridge, CB2 OQH, UK

http://www2.mrc-lmb.cam.ac.uk/group-leaders/h-to-m/g-jefferis
http://jefferislab.org
http://flybrain.stanford.edu


From geoffjentry at hexdump.org  Tue Mar  5 15:50:48 2013
From: geoffjentry at hexdump.org (Geoff Jentry)
Date: Tue, 5 Mar 2013 06:50:48 -0800 (PST)
Subject: [Rd] [BioC] enabling reproducible research & R package
 management & install.package.version & BiocLite
In-Reply-To: <D4772401B9D976478C0895769BE3E792072B7C@MBSRV02.sgc.loc>
References: <D4772401B9D976478C0895769BE3E7920716E6@MBSRV02.sgc.loc>
	<CAErFSogyfzMQ9UEK7e=Ukvgdg-NfxS60cPtppvvvN0MKnogyzw@mail.gmail.com>
	<CAHA9McNqVxFAdBYMqJm=avjA8wsS0uVSGQ=6eviXU6iPzPVXVQ@mail.gmail.com>
	<CAF42j23OD664yG7rgVMYzsEgges97=mn+DcQihne8CQUM0DmOQ@mail.gmail.com>
	<D4772401B9D976478C0895769BE3E792072B7C@MBSRV02.sgc.loc>
Message-ID: <Pine.LNX.4.64.1303050647190.22360@cardinals.dreamhost.com>

On Tue, 5 Mar 2013, Cook, Malcolm wrote:
> Thus, I might rather consider an approach that at 'publish' time tarzips 
> up a copy of the R package dependencies based on a config file defined 
> from sessionInfo and caches it in the project directory.

If you had a separate environment for every project, each with its own R 
installation and R installation lib.loc this becomes rather easy. For 
instance, something like this:

myProject/
    projectRInstallation/
       bin/
         R
       library/
         Biobase
         annotate
         .....
       ....
    projectData/
    projectCode/
    projectOutput/

The directory structure would likely be more complicated than that but 
something along those lines. This way all code, data *and* compute 
environment are always linked together.

-J


From MEC at stowers.org  Tue Mar  5 16:09:15 2013
From: MEC at stowers.org (Cook, Malcolm)
Date: Tue, 5 Mar 2013 15:09:15 +0000
Subject: [Rd] [BioC] enabling reproducible research & R package
 management & install.package.version & BiocLite
In-Reply-To: <661EEA68-1C78-4117-8FD4-7B31EAB32E00@mrc-lmb.cam.ac.uk>
References: <D4772401B9D976478C0895769BE3E7920716E6@MBSRV02.sgc.loc>
	<CAErFSogyfzMQ9UEK7e=Ukvgdg-NfxS60cPtppvvvN0MKnogyzw@mail.gmail.com>
	<CAHA9McNqVxFAdBYMqJm=avjA8wsS0uVSGQ=6eviXU6iPzPVXVQ@mail.gmail.com>
	<CAF42j23OD664yG7rgVMYzsEgges97=mn+DcQihne8CQUM0DmOQ@mail.gmail.com>
	<D4772401B9D976478C0895769BE3E792072B7C@MBSRV02.sgc.loc>
	<661EEA68-1C78-4117-8FD4-7B31EAB32E00@mrc-lmb.cam.ac.uk>
Message-ID: <D4772401B9D976478C0895769BE3E792073403@MBSRV02.sgc.loc>

.> So, even if I wanted to go where dragons lurked, it would not be
 .> possible to cobble a version of biocLite that installed specific
 .> versions of software.
 .>
 .> Thus, I might rather consider an approach that at 'publish' time
 .> tarzips up a copy of the R package dependencies based on a config file
 .> defined from sessionInfo and caches it in the project directory.
 .>
 .> Then when/if the project is revisited (and found to produce differnt
 .> results under current R enviRonment),  I can "simply" install an old R
 .> (oops, I guess I'd have to build it), and then un-tarzip the
 .> dependencies into the projects own R/Library which I would put on
 .> .libpaths.
 .
 .Sounds a little like this:
 .
 .http://cran.r-project.org/web/packages/rbundler/index.html
 .
 .(which I haven't tested). Best,
 .
 .Greg.

Looks interesting - thanks for the suggestion.

But, but.... my use case is one in which an analyst at my site depends upon the local library installation and only retrospectively, at some publishable event (like handing the results over the in-house customer/scientist), seeks to ensure the ability to return to that exact R library environment  later.  This tool, on the other hand, commits the user to keep a project specific "bundle" from the outset.  Another set of trade-offs.  I will have to synthesize the options I am learning.....

~ Malcolm 


From ripley at stats.ox.ac.uk  Tue Mar  5 16:19:30 2013
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 05 Mar 2013 15:19:30 +0000
Subject: [Rd] [BioC] enabling reproducible research & R package
 management & install.package.version & BiocLite
In-Reply-To: <D4772401B9D976478C0895769BE3E792073403@MBSRV02.sgc.loc>
References: <D4772401B9D976478C0895769BE3E7920716E6@MBSRV02.sgc.loc>
	<CAErFSogyfzMQ9UEK7e=Ukvgdg-NfxS60cPtppvvvN0MKnogyzw@mail.gmail.com>
	<CAHA9McNqVxFAdBYMqJm=avjA8wsS0uVSGQ=6eviXU6iPzPVXVQ@mail.gmail.com>
	<CAF42j23OD664yG7rgVMYzsEgges97=mn+DcQihne8CQUM0DmOQ@mail.gmail.com>
	<D4772401B9D976478C0895769BE3E792072B7C@MBSRV02.sgc.loc>
	<661EEA68-1C78-4117-8FD4-7B31EAB32E00@mrc-lmb.cam.ac.uk>
	<D4772401B9D976478C0895769BE3E792073403@MBSRV02.sgc.loc>
Message-ID: <51360D02.30701@stats.ox.ac.uk>

One comment: I have found numerical changes due to updates to the OS's 
compilers or runtime at least as often as I have been by changes in R or 
packages when trying to reproduce results from a year or two back.  That 
aspect is rarely mentioned in these discussions.

On 05/03/2013 15:09, Cook, Malcolm wrote:
> .> So, even if I wanted to go where dragons lurked, it would not be
>   .> possible to cobble a version of biocLite that installed specific
>   .> versions of software.
>   .>
>   .> Thus, I might rather consider an approach that at 'publish' time
>   .> tarzips up a copy of the R package dependencies based on a config file
>   .> defined from sessionInfo and caches it in the project directory.
>   .>
>   .> Then when/if the project is revisited (and found to produce differnt
>   .> results under current R enviRonment),  I can "simply" install an old R
>   .> (oops, I guess I'd have to build it), and then un-tarzip the
>   .> dependencies into the projects own R/Library which I would put on
>   .> .libpaths.
>   .
>   .Sounds a little like this:
>   .
>   .http://cran.r-project.org/web/packages/rbundler/index.html
>   .
>   .(which I haven't tested). Best,
>   .
>   .Greg.
>
> Looks interesting - thanks for the suggestion.
>
> But, but.... my use case is one in which an analyst at my site depends upon the local library installation and only retrospectively, at some publishable event (like handing the results over the in-house customer/scientist), seeks to ensure the ability to return to that exact R library environment  later.  This tool, on the other hand, commits the user to keep a project specific "bundle" from the outset.  Another set of trade-offs.  I will have to synthesize the options I am learning.....
>
> ~ Malcolm
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From marius.hofert at math.ethz.ch  Tue Mar  5 20:08:25 2013
From: marius.hofert at math.ethz.ch (Marius Hofert)
Date: Tue, 5 Mar 2013 20:08:25 +0100
Subject: [Rd] Patch for format.ftable()
Message-ID: <871ubtfyvq.fsf@sklar.v.cablecom.net>

Dear expeRts,

Please find attached the .diff for a bug fix in R-devel 62124. format.ftable()
fails to format ftable()s correctly which have no row.vars or no
col.vars. That should work with the patch (the example code below also runs
correctly for all the (new) 'method's).

Cheers,

Marius


--8<---------------cut here---------------start------------->8---
(ft1 <- ftable(Titanic, col.vars = 1:4))
(ft2 <- ftable(Titanic, row.vars = 1))
(ft3 <- ftable(Titanic, row.vars = 1:2))
(ft4 <- ftable(Titanic, row.vars = 1:3))
(ft5 <- ftable(Titanic, row.vars = 1:4))

## former version (R-devel 62124)
stats:::format.ftable(ft1) # fails
stats:::format.ftable(ft2)
stats:::format.ftable(ft3)
stats:::format.ftable(ft4)
stats:::format.ftable(ft5) # fails

## all work fine now (format.ftable.() is the patched version):
format.ftable.(ft1)
format.ftable.(ft1, method="row.compact")
format.ftable.(ft1, method="col.compact")
format.ftable.(ft1, method="compact")
format.ftable.(ft2)
format.ftable.(ft2, method="row.compact")
format.ftable.(ft2, method="col.compact")
format.ftable.(ft2, method="compact")
format.ftable.(ft3)
format.ftable.(ft3, method="row.compact")
format.ftable.(ft3, method="col.compact")
format.ftable.(ft3, method="compact")
format.ftable.(ft4)
format.ftable.(ft4, method="row.compact")
format.ftable.(ft4, method="col.compact")
format.ftable.(ft4, method="compact")
format.ftable.(ft5)
format.ftable.(ft5, method="row.compact")
format.ftable.(ft5, method="col.compact")
format.ftable.(ft5, method="compact")
--8<---------------cut here---------------end--------------->8---


-- 
ETH Zurich
Dr. Marius Hofert
RiskLab, Department of Mathematics
HG E 65.2
R?mistrasse 101
8092 Zurich
Switzerland

Phone +41 44 632 2423
http://www.math.ethz.ch/~hofertj
GPG key fingerprint 8EF4 5842 0EA2 5E1D 3D7F  0E34 AD4C 566E 655F 3F7C
-------------- next part --------------
A non-text attachment was scrubbed...
Name: mods.diff
Type: text/x-diff
Size: 1642 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20130305/a82a6452/attachment.bin>

From pgilbert902 at gmail.com  Tue Mar  5 23:34:09 2013
From: pgilbert902 at gmail.com (Paul Gilbert)
Date: Tue, 05 Mar 2013 17:34:09 -0500
Subject: [Rd] [BioC] enabling reproducible research & R package
 management & install.package.version & BiocLite
In-Reply-To: <D4772401B9D976478C0895769BE3E7920733A1@MBSRV02.sgc.loc>
References: <D4772401B9D976478C0895769BE3E7920716E6@MBSRV02.sgc.loc>,
	<CAErFSogyfzMQ9UEK7e=Ukvgdg-NfxS60cPtppvvvN0MKnogyzw@mail.gmail.com>
	<BLU166-W14A5221668A7BDBAC726DEBEFB0@phx.gbl>
	<D4772401B9D976478C0895769BE3E7920733A1@MBSRV02.sgc.loc>
Message-ID: <513672E1.6080105@gmail.com>

(More on the original question further below.)

On 13-03-05 09:48 AM, Cook, Malcolm wrote:
> All,
>
> What got me started on this line of inquiry was my attempt at
> balancing the advantages of performing a periodic (daily or weekly)
> update to the 'release' version of locally installed R/Bioconductor
> packages on our institute-wide installation of R with the
> disadvantages of potentially changing the result of an analyst's
> workflow in mid-project.

I have implemented a strategy to try to address this as follows:

1/ Install a new version of R when it is released, and packages in the R 
version's site-library with package versions as available at the time 
the R version is installed. Only upgrade these package versions in the 
case they are severely broken.

2/ Install the same packages in site-library-fresh and upgrade these 
package versions on a regular basis (e.g. daily).

3/ When a new version of R is released, freeze but do not remove the old 
R version, at least not for a fairly long time, and freeze 
site-library-fresh for the old version. Begin with the new version as in 
1/ and 2/. The old version remains available, so "reverting" is trivial.


The analysts are then responsible for choosing the R version they use, 
and the library they use. This means they do not have to change R and 
package version mid-project, but they can if they wish. I think the 
above two libraries will cover most cases, but it is possible that a few 
projects will need their own special library with a combination of 
package versions. In this case the user could create their own library, 
or you might prefer some more official mechanism.

The idea of the above strategy is to provide the stability one might 
want for an ongoing project, and the possibility of an upgraded package 
if necessary, but not encourage analysts to remain indefinitely with old 
versions (by say, putting new packages in an old R version library).

This strategy has been implemented in a set of make files in the project 
RoboAdmin available at http://automater.r-forge.r-project.org/. It can 
be done entirely automatically with a cron job. Constructive comments 
are always appreciated.

(IT departments sometimes think that there should be only one version of 
everything available, which they test and approve. So the initial 
reaction to this approach could be negative. I think they have not 
really thought about the advantages. They usually cannot test/approve an 
upgrade without user input, and timing is often extremely complicate 
because of ongoing user needs. This strategy is simply shifting 
responsibility and timing to the users, or user departments, that can 
actually do the testing and approving.)

Regarding NFS mounts, it is relatively robust. There can be occasional 
problems, especially for users that have a habit of keeping an R session 
open for days at a time and using site-library-fresh packages. In my 
experience this did not happen often enough to worry about a "blackout 
period".

Regarding the original question, I would like to think it could be 
possible to keep enough information to reproduce the exact environment, 
but I think for potentially sensitive numerical problems that is 
optimistic. As others have pointed out, results can depend not only on R 
and package versions, configuration, OS versions, and library and 
compiler versions, but also on the underlying hardware. You might have 
some hope using something like an Amazon core instance. (BTW, this 
problem is not specific to R.)

It is true that restricting to a fixed computing environment at your 
institution may ease things somewhat, but if you occasionally upgrade 
hardware or the OS then you will probably lose reproducibility.

An alternative that I recommend is that you produce a set of tests that 
confirm the results of any important project. These can be conveniently 
put in the tests/ directory of an R package, which is then maintained 
local, not on CRAN, and built/tested whenever a new R and packages are 
installed. (Tools for this are also available at the above indicated web 
site.) This approach means that you continue to reproduce the old 
results, or if not, discover differences/problems in the old or new 
version of R and/or packages that may be important to you. I have been 
successfully using a variant of this since about 1993, using R and 
package tests/ since they became available.

Paul

>
> I just got the "green light" to institute such periodic updates that
> I have been arguing is in our collective best interest.  In return,
> I promised my best effort to provide a means for preserving or
> reverting to a working R library configuration.
>
> Please note that the reproducibility I am most eager to provide is
> limited to reproducibility within the computing environment of our
> institute, which perhaps takes away some of the dragon's nests,
> though certainly not all.
>
> There are technical issues of updating package installations on an
> NFS mount that might have files/libraries open on it from running R
> sessions.  I am interested in learning of approaches for
> minimizing/eliminating exposure to these issue as well.  The
> first/best approach seems to be to institute a 'black out' period
> when users should expect the installed library to change.   Perhaps
> there are improvements to this????
>
> Best,
>
> Malcolm
>
>
> .-----Original Message----- .From: Mike Marchywka
> [mailto:marchywka at hotmail.com] .Sent: Tuesday, March 05, 2013 5:24
> AM .To: amackey at virginia.edu; Cook, Malcolm .Cc:
> r-devel at r-project.org; bioconductor at r-project.org;
> r-discussion at listserv.stowers.org .Subject: RE: [Rd] [BioC] enabling
> reproducible research & R package management &
> install.package.version & BiocLite . . .I hate to ask what go this
> thread started but it sounds like someone was counting on .exact
> numeric reproducibility or was there a bug in a specific release? In
> actual .fact, the best way to determine reproducibility is run the
> code in a variety of .packages. Alternatively, you can do everything
> in java and not assume .that calculations commute or associate as the
> code is modified but it seems .pointless. Sensitivity determination
> would seem to lead to more reprodicible results .than trying to keep
> a specific set of code quirks. . .I also seem to recall that FPU may
> have random lower order bits in some cases, .same code/data give
> different results. Alsways assume FP is stochastic and plan .on
> anlayzing the "noise." . . .----------------------------------------
> .> From: amackey at virginia.edu .> Date: Mon, 4 Mar 2013 16:28:48
> -0500 .> To: MEC at stowers.org .> CC: r-devel at r-project.org;
> bioconductor at r-project.org; r-discussion at listserv.stowers.org .>
> Subject: Re: [Rd] [BioC] enabling reproducible research & R package
> management & install.package.version & BiocLite .> .> On Mon, Mar 4,
> 2013 at 4:13 PM, Cook, Malcolm <MEC at stowers.org> wrote: .> .> > *
> where do the dragons lurk .> > .> .> webs of interconnected
> dynamically loaded libraries, identical versions of .> R compiled
> with different BLAS/LAPACK options, etc. Go with the VM if you .>
> really, truly, want this level of exact reproducibility. .> .> An
> alternative (and arguably more useful) strategy would be to cache .>
> results of each computational step, and report when results differ
> upon .> re-execution with identical inputs; if you cache sessionInfo
> along with .> each result, you can identify which package(s) changed,
> and begin to hunt .> down why the change occurred (possibly for the
> better); couple this with .> the concept of keeping both code *and*
> results in version control, then you .> can move forward with a
> (re)analysis without being crippled by out-of-date .> software. .> .>
> -Aaron .> .> -- .> Aaron J. Mackey, PhD .> Assistant Professor .>
> Center for Public Health Genomics .> University of Virginia .>
> amackey at virginia.edu .> http://www.cphg.virginia.edu/mackey .> .>
> [[alternative HTML version deleted]] .> .>
> ______________________________________________ .>
> R-devel at r-project.org mailing list .>
> https://stat.ethz.ch/mailman/listinfo/r-devel .
>
> ______________________________________________ R-devel at r-project.org
> mailing list https://stat.ethz.ch/mailman/listinfo/r-devel
>


From mailinglist.honeypot at gmail.com  Wed Mar  6 00:04:16 2013
From: mailinglist.honeypot at gmail.com (Steve Lianoglou)
Date: Tue, 5 Mar 2013 18:04:16 -0500
Subject: [Rd] [BioC] enabling reproducible research & R package
 management & install.package.version & BiocLite
In-Reply-To: <513672E1.6080105@gmail.com>
References: <D4772401B9D976478C0895769BE3E7920716E6@MBSRV02.sgc.loc>
	<CAErFSogyfzMQ9UEK7e=Ukvgdg-NfxS60cPtppvvvN0MKnogyzw@mail.gmail.com>
	<BLU166-W14A5221668A7BDBAC726DEBEFB0@phx.gbl>
	<D4772401B9D976478C0895769BE3E7920733A1@MBSRV02.sgc.loc>
	<513672E1.6080105@gmail.com>
Message-ID: <CAHA9McNpi0+EyZa9rqqZfRAYV8dj+NrdgSLZMnZUB0cFHqtfTQ@mail.gmail.com>

Hi Paul,

You outline some great suggestions!

I just wanted to point that in this case:

On Tue, Mar 5, 2013 at 5:34 PM, Paul Gilbert <pgilbert902 at gmail.com> wrote:
[snip]

> Regarding NFS mounts, it is relatively robust. There can be occasional
> problems, especially for users that have a habit of keeping an R session
> open for days at a time and using site-library-fresh packages. In my
> experience this did not happen often enough to worry about a "blackout
> period".

if users have a habit of working like this, they could also create an
R-library directory under their home directory, and put this library
path at the front of their .libPaths() so the continually updated
"fresh" stuff won't affect them.

Just wanted to point that out as I really like your general approach
you've outlined, and just wanted to point out that there's an easy
work around in case someone else tries to institute such a regime but
is getting friction due to that point in particular.

Good stuff, though .. thanks for sharing that!

-steve

-- 
Steve Lianoglou
Graduate Student: Computational Systems Biology
 | Memorial Sloan-Kettering Cancer Center
 | Weill Medical College of Cornell University
Contact Info: http://cbio.mskcc.org/~lianos/contact


From MEC at stowers.org  Wed Mar  6 00:08:01 2013
From: MEC at stowers.org (Cook, Malcolm)
Date: Tue, 5 Mar 2013 23:08:01 +0000
Subject: [Rd] [BioC] enabling reproducible research & R package
 management & install.package.version & BiocLite
In-Reply-To: <513672E1.6080105@gmail.com>
References: <D4772401B9D976478C0895769BE3E7920716E6@MBSRV02.sgc.loc>,
	<CAErFSogyfzMQ9UEK7e=Ukvgdg-NfxS60cPtppvvvN0MKnogyzw@mail.gmail.com>
	<BLU166-W14A5221668A7BDBAC726DEBEFB0@phx.gbl>
	<D4772401B9D976478C0895769BE3E7920733A1@MBSRV02.sgc.loc>
	<513672E1.6080105@gmail.com>
Message-ID: <D4772401B9D976478C0895769BE3E7920747D3@MBSRV02.sgc.loc>

Paul,

I think your balanced and reasoned approach addresses all my current concerns.  Nice!  I will likely adopt your methods.  Let me ruminate.  Thanks for this.

~ Malcolm

 .-----Original Message-----
 .From: Paul Gilbert [mailto:pgilbert902 at gmail.com]
 .Sent: Tuesday, March 05, 2013 4:34 PM
 .To: Cook, Malcolm
 .Cc: 'r-devel at r-project.org'; 'bioconductor at r-project.org'; 'r-discussion at listserv.stowers.org'
 .Subject: Re: [Rd] [BioC] enabling reproducible research & R package management & install.package.version & BiocLite
 .
 .(More on the original question further below.)
 .
 .On 13-03-05 09:48 AM, Cook, Malcolm wrote:
 .> All,
 .>
 .> What got me started on this line of inquiry was my attempt at
 .> balancing the advantages of performing a periodic (daily or weekly)
 .> update to the 'release' version of locally installed R/Bioconductor
 .> packages on our institute-wide installation of R with the
 .> disadvantages of potentially changing the result of an analyst's
 .> workflow in mid-project.
 .
 .I have implemented a strategy to try to address this as follows:
 .
 .1/ Install a new version of R when it is released, and packages in the R
 .version's site-library with package versions as available at the time
 .the R version is installed. Only upgrade these package versions in the
 .case they are severely broken.
 .
 .2/ Install the same packages in site-library-fresh and upgrade these
 .package versions on a regular basis (e.g. daily).
 .
 .3/ When a new version of R is released, freeze but do not remove the old
 .R version, at least not for a fairly long time, and freeze
 .site-library-fresh for the old version. Begin with the new version as in
 .1/ and 2/. The old version remains available, so "reverting" is trivial.
 .
 .
 .The analysts are then responsible for choosing the R version they use,
 .and the library they use. This means they do not have to change R and
 .package version mid-project, but they can if they wish. I think the
 .above two libraries will cover most cases, but it is possible that a few
 .projects will need their own special library with a combination of
 .package versions. In this case the user could create their own library,
 .or you might prefer some more official mechanism.
 .
 .The idea of the above strategy is to provide the stability one might
 .want for an ongoing project, and the possibility of an upgraded package
 .if necessary, but not encourage analysts to remain indefinitely with old
 .versions (by say, putting new packages in an old R version library).
 .
 .This strategy has been implemented in a set of make files in the project
 .RoboAdmin available at http://automater.r-forge.r-project.org/. It can
 .be done entirely automatically with a cron job. Constructive comments
 .are always appreciated.
 .
 .(IT departments sometimes think that there should be only one version of
 .everything available, which they test and approve. So the initial
 .reaction to this approach could be negative. I think they have not
 .really thought about the advantages. They usually cannot test/approve an
 .upgrade without user input, and timing is often extremely complicate
 .because of ongoing user needs. This strategy is simply shifting
 .responsibility and timing to the users, or user departments, that can
 .actually do the testing and approving.)
 .
 .Regarding NFS mounts, it is relatively robust. There can be occasional
 .problems, especially for users that have a habit of keeping an R session
 .open for days at a time and using site-library-fresh packages. In my
 .experience this did not happen often enough to worry about a "blackout
 .period".
 .
 .Regarding the original question, I would like to think it could be
 .possible to keep enough information to reproduce the exact environment,
 .but I think for potentially sensitive numerical problems that is
 .optimistic. As others have pointed out, results can depend not only on R
 .and package versions, configuration, OS versions, and library and
 .compiler versions, but also on the underlying hardware. You might have
 .some hope using something like an Amazon core instance. (BTW, this
 .problem is not specific to R.)
 .
 .It is true that restricting to a fixed computing environment at your
 .institution may ease things somewhat, but if you occasionally upgrade
 .hardware or the OS then you will probably lose reproducibility.
 .
 .An alternative that I recommend is that you produce a set of tests that
 .confirm the results of any important project. These can be conveniently
 .put in the tests/ directory of an R package, which is then maintained
 .local, not on CRAN, and built/tested whenever a new R and packages are
 .installed. (Tools for this are also available at the above indicated web
 .site.) This approach means that you continue to reproduce the old
 .results, or if not, discover differences/problems in the old or new
 .version of R and/or packages that may be important to you. I have been
 .successfully using a variant of this since about 1993, using R and
 .package tests/ since they became available.
 .
 .Paul
 .
 .>
 .> I just got the "green light" to institute such periodic updates that
 .> I have been arguing is in our collective best interest.  In return,
 .> I promised my best effort to provide a means for preserving or
 .> reverting to a working R library configuration.
 .>
 .> Please note that the reproducibility I am most eager to provide is
 .> limited to reproducibility within the computing environment of our
 .> institute, which perhaps takes away some of the dragon's nests,
 .> though certainly not all.
 .>
 .> There are technical issues of updating package installations on an
 .> NFS mount that might have files/libraries open on it from running R
 .> sessions.  I am interested in learning of approaches for
 .> minimizing/eliminating exposure to these issue as well.  The
 .> first/best approach seems to be to institute a 'black out' period
 .> when users should expect the installed library to change.   Perhaps
 .> there are improvements to this????
 .>
 .> Best,
 .>
 .> Malcolm
 .>
 .>
 .> .-----Original Message----- .From: Mike Marchywka
 .> [mailto:marchywka at hotmail.com] .Sent: Tuesday, March 05, 2013 5:24
 .> AM .To: amackey at virginia.edu; Cook, Malcolm .Cc:
 .> r-devel at r-project.org; bioconductor at r-project.org;
 .> r-discussion at listserv.stowers.org .Subject: RE: [Rd] [BioC] enabling
 .> reproducible research & R package management &
 .> install.package.version & BiocLite . . .I hate to ask what go this
 .> thread started but it sounds like someone was counting on .exact
 .> numeric reproducibility or was there a bug in a specific release? In
 .> actual .fact, the best way to determine reproducibility is run the
 .> code in a variety of .packages. Alternatively, you can do everything
 .> in java and not assume .that calculations commute or associate as the
 .> code is modified but it seems .pointless. Sensitivity determination
 .> would seem to lead to more reprodicible results .than trying to keep
 .> a specific set of code quirks. . .I also seem to recall that FPU may
 .> have random lower order bits in some cases, .same code/data give
 .> different results. Alsways assume FP is stochastic and plan .on
 .> anlayzing the "noise." . . .----------------------------------------
 .> .> From: amackey at virginia.edu .> Date: Mon, 4 Mar 2013 16:28:48
 .> -0500 .> To: MEC at stowers.org .> CC: r-devel at r-project.org;
 .> bioconductor at r-project.org; r-discussion at listserv.stowers.org .>
 .> Subject: Re: [Rd] [BioC] enabling reproducible research & R package
 .> management & install.package.version & BiocLite .> .> On Mon, Mar 4,
 .> 2013 at 4:13 PM, Cook, Malcolm <MEC at stowers.org> wrote: .> .> > *
 .> where do the dragons lurk .> > .> .> webs of interconnected
 .> dynamically loaded libraries, identical versions of .> R compiled
 .> with different BLAS/LAPACK options, etc. Go with the VM if you .>
 .> really, truly, want this level of exact reproducibility. .> .> An
 .> alternative (and arguably more useful) strategy would be to cache .>
 .> results of each computational step, and report when results differ
 .> upon .> re-execution with identical inputs; if you cache sessionInfo
 .> along with .> each result, you can identify which package(s) changed,
 .> and begin to hunt .> down why the change occurred (possibly for the
 .> better); couple this with .> the concept of keeping both code *and*
 .> results in version control, then you .> can move forward with a
 .> (re)analysis without being crippled by out-of-date .> software. .> .>
 .> -Aaron .> .> -- .> Aaron J. Mackey, PhD .> Assistant Professor .>
 .> Center for Public Health Genomics .> University of Virginia .>
 .> amackey at virginia.edu .> http://www.cphg.virginia.edu/mackey .> .>
 .> [[alternative HTML version deleted]] .> .>
 .> ______________________________________________ .>
 .> R-devel at r-project.org mailing list .>
 .> https://stat.ethz.ch/mailman/listinfo/r-devel .
 .>
 .> ______________________________________________ R-devel at r-project.org
 .> mailing list https://stat.ethz.ch/mailman/listinfo/r-devel
 .>


From btyner at gmail.com  Wed Mar  6 04:38:11 2013
From: btyner at gmail.com (Benjamin Tyner)
Date: Tue, 05 Mar 2013 22:38:11 -0500
Subject: [Rd] crossprod(): g77 versus gfortran
In-Reply-To: <51358CA9.4030004@stats.ox.ac.uk>
References: <51358CA9.4030004@stats.ox.ac.uk>
Message-ID: <5136BA23.6010605@gmail.com>

Thank you Brian. So it sounds like appendix B.6 of

   http://cran.r-project.org/doc/manuals/R-admin.html

should be updated to reflect that g77 is no longer supported; I will
notify CRAN at R-project.org of this unless you suggest a different recipient.

In any case, I tried again but using gfortran version 4.4.0, and now the
timings are back to what they were under g77.

As for BLAS, was using the one that ships with R, as wanted to keep
things simple for benchmarking purposes.

Thanks again.

> On 05/03/2013 01:45, Benjamin Tyner wrote:
> >/ Hi
> />/
> />/ I've got two builds of R, one using g77 (version 3.4.6) and the other
> />/ using gfortran (version 4.1.2). The two builds are otherwise identical
> />/ as far as I can tell. The one which used g77 performs crossprod()s
> />/ roughly twice as fast as the gfortran one. I'm wondering if this rings a
> />/ bell with anyone, and if so, are you aware of any configure settings
> />/ which will improve the performance when using gfortran. This is on RHEL 5.
> /
> Note that recent versions of R do not build with g77, and have 
> performance improvements in linear algebra.  So please follow the 
> posting guide and give up the 'at a minimum' information requested and a 
> reproducible example.
>
> Also, check what BLAS is in use: an optimized BLAS can make a lot of 
> difference on such simple operations.
>
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk <https://stat.ethz.ch/mailman/listinfo/r-devel>
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/ <http://www.stats.ox.ac.uk/%7Eripley/>
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From simon.urbanek at r-project.org  Wed Mar  6 05:46:05 2013
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Tue, 5 Mar 2013 23:46:05 -0500
Subject: [Rd] crossprod(): g77 versus gfortran
In-Reply-To: <5136BA23.6010605@gmail.com>
References: <51358CA9.4030004@stats.ox.ac.uk> <5136BA23.6010605@gmail.com>
Message-ID: <6B13B887-BA0A-4CDC-8377-96A7BD140228@r-project.org>


On Mar 5, 2013, at 10:38 PM, Benjamin Tyner wrote:

> Thank you Brian. So it sounds like appendix B.6 of
> 
>   http://cran.r-project.org/doc/manuals/R-admin.html
> 
> should be updated to reflect that g77 is no longer supported; I will
> notify CRAN at R-project.org of this unless you suggest a different recipient.
> 

I think you misunderstood - you can use g77 but you will also need F90-capable Fortran - typically gfortran - unless you have other source for LAPACK and that's what B.6 says explicitly.

Cheers,
Simon




> In any case, I tried again but using gfortran version 4.4.0, and now the
> timings are back to what they were under g77.
> 
> As for BLAS, was using the one that ships with R, as wanted to keep
> things simple for benchmarking purposes.
> 
> Thanks again.
> 
>> On 05/03/2013 01:45, Benjamin Tyner wrote:
>>> / Hi
>> />/
>> />/ I've got two builds of R, one using g77 (version 3.4.6) and the other
>> />/ using gfortran (version 4.1.2). The two builds are otherwise identical
>> />/ as far as I can tell. The one which used g77 performs crossprod()s
>> />/ roughly twice as fast as the gfortran one. I'm wondering if this rings a
>> />/ bell with anyone, and if so, are you aware of any configure settings
>> />/ which will improve the performance when using gfortran. This is on RHEL 5.
>> /
>> Note that recent versions of R do not build with g77, and have 
>> performance improvements in linear algebra.  So please follow the 
>> posting guide and give up the 'at a minimum' information requested and a 
>> reproducible example.
>> 
>> Also, check what BLAS is in use: an optimized BLAS can make a lot of 
>> difference on such simple operations.
>> 
>> 
>> -- 
>> Brian D. Ripley,                  ripley at stats.ox.ac.uk <https://stat.ethz.ch/mailman/listinfo/r-devel>
>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/ <http://www.stats.ox.ac.uk/%7Eripley/>
>> University of Oxford,             Tel:  +44 1865 272861 (self)
>> 1 South Parks Road,                     +44 1865 272866 (PA)
>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From thomas.friedrichsmeier at ruhr-uni-bochum.de  Wed Mar  6 13:12:00 2013
From: thomas.friedrichsmeier at ruhr-uni-bochum.de (Thomas Friedrichsmeier)
Date: 6 Mar 2013 13:12:00 +0100
Subject: [Rd] Printing warning messages around R_tryEval
Message-ID: <201303061312.05899.thomas.friedrichsmeier@ruhr-uni-bochum.de>

Hi!

In RKWard we use R_tryEval() at a number places to run R commands. In order to 
make sure that any generated warnings become visible close to the problem, we 
were following up (most of) these calls with Rf_PrintWarnings().

Rf_PrintWarnings() was never available in the headers (as far as I know), and 
in r61771 the symbol has been hidden from libR.so. So this solution is no 
longer available. Could you give advice on the best way to print warnings 
before returning to the top level?

Some available options, I am aware of:
1. Call R function warnings(). However, this will always print the latest 
warning, even if it was generate by some earlier command. I would need a way 
to print new warnings, only.

2. Use options(warn=1) where applicable. However, in some cases, collecting 
warnings until some procedure is complete, and printing them then, would be 
preferrable.

3. I see there is an internal call printDeferredWarnings(), which seems to be 
almost exactly what I want. However, using an .Internal() does not look like a 
terribly stable solution, either. Also, having direct access to a similar 
function from the C-API would be very comfortable.

Thanks!
Thomas
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 198 bytes
Desc: This is a digitally signed message part.
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20130306/af4f758a/attachment.bin>

From nashjc at uottawa.ca  Wed Mar  6 14:35:29 2013
From: nashjc at uottawa.ca (Prof J C Nash (U30A))
Date: Wed, 06 Mar 2013 08:35:29 -0500
Subject: [Rd] Lessons from LibreOffice project
In-Reply-To: <bcaec52999bf9722c904d739c637@google.com>
References: <bcaec52999bf9722c904d739c637@google.com>
Message-ID: <51374621.3040106@uottawa.ca>

The message below came to me from the Getting Open Source Logic INto 
Government list. I'm passing it on to the devel list as the infoworld 
article may have some ideas of relevance to the R project, mainly 
concerning build and test issues and tracking changes in the code base. 
While the LibreOffice project is very different from R, there may 
nevertheless be some tips we can borrow in the best open source 
tradition. FWIW, the article is quite short.

John Nash

-------- Original Message --------
Subject: [OTT-GOSLING] Interesting article about LibreOffice project
Date: Wed, 06 Mar 2013 04:10:51 +0000
From: gabriel.cossette at gmail.com
Reply-To: GOSLING members in Ottawa 
<ottawa-gosling at list.goslingcommunity.org>
To: ottawa-gosling at list.goslingcommunity.org

Hi everyone!

Here's a very interesting article about how the LibreOffice project is
evolving:

What you can learn from the monster LibreOffice project
http://www.infoworld.com/print/212908

Have a great day!


Gabriel Cossette
Coordonnateur de communaut?s sur les logiciels libres | Open Source
Software Communities Coordinator
Technologies de l'information | Information Technology
Services partag?s Canada | Parcs Canada
Shared Services Canada | Parks Canada
3, passage du Chien-d'Or, Qu?bec (Qu?bec)  G1R 4V7
gabriel.cossette at spc-ssc.gc.ca
T?l. | Tel.: 418 649-8163
Cell. | Cell.: 418 254-8558
Gouvernement du Canada | Government of Canada
_______________________________________________
Ottawa-gosling mailing list
Ottawa-gosling at list.goslingcommunity.org
http://list.goslingcommunity.org/mailman/listinfo/ottawa-gosling


From ripley at stats.ox.ac.uk  Wed Mar  6 16:20:23 2013
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 06 Mar 2013 15:20:23 +0000
Subject: [Rd] predict.loess() segfaults for large n?
In-Reply-To: <CADBEN2wB5cruNHRb7vpMFfWxNnRnQ=kJnNdH9gC+zxePGxXPFg@mail.gmail.com>
References: <CADBEN2wB5cruNHRb7vpMFfWxNnRnQ=kJnNdH9gC+zxePGxXPFg@mail.gmail.com>
Message-ID: <51375EB7.4000606@stats.ox.ac.uk>

Thanks.

This is in the netlib loess code: the size is used in Fortran (and an 
INTEGER) so we cannot increase it.  I've added a test and thrown an 
error if the dimension is too large.


On 01/03/2013 11:27, Hiroyuki Kawakatsu wrote:
> Hi,
>
> I am segfaulting when using predict.loess() (checked with r62092).
> I've traced the source with the help of valgrind (output pasted
> below) and it appears that this is due to int overflow when
> allocating an int work array in loess_workspace():
>
>      liv = 50 + ((int)pow((double)2, (double)D) + 4) * nvmax + 2 * N;
>
> where liv is an (global) int. For D=1 (one x variable), this
> overflows at approx N = 4089 where N is the fitted sample size (not
> prediction sample size).
>
> I am aware that you are in the process of introducing long vectors
> but a quick fix would be to error when predict.loess(..., se=TRUE)
> and N is too large. (Ideally, one would use long int but does
> fortran portably support long int?) The threshold N value may depend
> on surface type (above is for surface=="interpolate").
>
> The following sample code does not result in segfault but when run
> with valgrind, it produces the warning about large range. (In the
> code that segfaults N is about 77,000).
>
> set.seed(1)
> n = 5000      # n=4000 seems ok
> x = rnorm(n)
> y = x + rnorm(n)
> yf = loess(y~x, span=0.75, control=loess.control(trace.hat="approximate"))
> print( predict(yf, data.frame(x=1), se=TRUE) )
>
> ##---valgrid output with segfault (abridged):
>
>> test4()
> ==30841== Warning: set address range perms: large range [0x3962a040,
> 0x5fb42608) (defined)
> ==30841== Warning: set address range perms: large range [0x5fb43040,
> 0xf8c8e130) (defined)
> ==30841== Invalid write of size 4
> ==30841==    at 0xCD719F0: ehg139_ (loessf.f:1444)
> ==30841==    by 0xCD72E0C: ehg131_ (loessf.f:467)
> ==30841==    by 0xCD73A5A: lowesb_ (loessf.f:1530)
> ==30841==    by 0xCD2C774: loess_ise (loessc.c:219)
> ==30841==    by 0x486C7F: do_dotCode (dotcode.c:1744)
> ==30841==    by 0x4AB040: bcEval (eval.c:4544)
> ==30841==    by 0x4B6B3F: Rf_eval (eval.c:498)
> ==30841==    by 0x4BAD87: Rf_applyClosure (eval.c:960)
> ==30841==    by 0x4B6D5E: Rf_eval (eval.c:611)
> ==30841==    by 0x4B7A1E: do_eval (eval.c:2193)
> ==30841==    by 0x4AB040: bcEval (eval.c:4544)
> ==30841==    by 0x4B6B3F: Rf_eval (eval.c:498)
> ==30841==  Address 0xf8cd4144 is not stack'd, malloc'd or (recently)
> free'd
> ==30841==
>
>   *** caught segfault ***
> address 0xf8cd4144, cause 'memory not mapped'
>
> Traceback:
>   1: predLoess(y, x, newx, s, weights, pars$robust, pars$span,
> pars$degree,     pars$normalize, pars$parametric, pars$drop.square,
> pars$surface,     pars$cell, pars$family, kd, divisor, se = se)
>   2: eval(expr, envir, enclos)
>   3: eval(substitute(expr), data, enclos = parent.frame())
>   4: with.default(object, predLoess(y, x, newx, s, weights,
> pars$robust,     pars$span, pars$degree, pars$normalize,
> pars$parametric,     pars$drop.square, pars$surface, pars$cell,
> pars$family, kd,     divisor, se = se))
>   5: with(object, predLoess(y, x, newx, s, weights, pars$robust,
> pars$span,     pars$degree, pars$normalize, pars$parametric,
> pars$drop.square,     pars$surface, pars$cell, pars$family, kd,
> divisor, se = se))
>   6: predict.loess(y2, data.frame(hours = xmin), se = TRUE)
>   7: predict(y2, data.frame(hours = xmin), se = TRUE)
>   8: test4()
> aborting ...
> ==30841==
>
>


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From MEC at stowers.org  Wed Mar  6 19:17:52 2013
From: MEC at stowers.org (Cook, Malcolm)
Date: Wed, 6 Mar 2013 18:17:52 +0000
Subject: [Rd] [BioC] enabling reproducible research & R package
 management & install.package.version & BiocLite
In-Reply-To: <86BFEB1DFA6CB3448DB8AB1FC52F405915F069@ummscsmbx06.ad.umassmed.edu>
References: <D4772401B9D976478C0895769BE3E7920716E6@MBSRV02.sgc.loc>,
	<CAErFSogyfzMQ9UEK7e=Ukvgdg-NfxS60cPtppvvvN0MKnogyzw@mail.gmail.com>
	<BLU166-W14A5221668A7BDBAC726DEBEFB0@phx.gbl>
	<D4772401B9D976478C0895769BE3E7920733A1@MBSRV02.sgc.loc>
	<513672E1.6080105@gmail.com>
	<D4772401B9D976478C0895769BE3E7920747D3@MBSRV02.sgc.loc>
	<86BFEB1DFA6CB3448DB8AB1FC52F405915F069@ummscsmbx06.ad.umassmed.edu>
Message-ID: <D4772401B9D976478C0895769BE3E7920767B0@MBSRV02.sgc.loc>

Thanks David, I've looked into them both a bit, and I don't think the provide an approach for R (or Perl, for that matter) library management, which is the wicket I'm trying to get less sticky now.

They could be useful to manage the various installations of version of R and analysis files (we're talking allot of NextGenSequencing, so, bowtie, tophat, and friends) quite nicely similarly in service of an approach to enabling reproducible results.

THanks for you thoughts, and, if you know of others similar to dotkit/modules I'd be keen to here of them.

~Malcolm


 .-----Original Message-----
 .From: Lapointe, David [mailto:David.Lapointe at umassmed.edu]
 .Sent: Wednesday, March 06, 2013 7:46 AM
 .To: Cook, Malcolm; 'Paul Gilbert'
 .Cc: 'r-devel at r-project.org'; 'bioconductor at r-project.org'; 'r-discussion at listserv.stowers.org'
 .Subject: RE: [BioC] [Rd] enabling reproducible research & R package management & install.package.version & BiocLite
 .
 .There are utilities ( e.g. dotkit, and modules) which facilitate version management, basically creating on the fly PATH and env setups, if
 .you are comfortable keeping all that around.
 .
 .David
 .
 .-----Original Message-----
 .From: bioconductor-bounces at r-project.org [mailto:bioconductor-bounces at r-project.org] On Behalf Of Cook, Malcolm
 .Sent: Tuesday, March 05, 2013 6:08 PM
 .To: 'Paul Gilbert'
 .Cc: 'r-devel at r-project.org'; 'bioconductor at r-project.org'; 'r-discussion at listserv.stowers.org'
 .Subject: Re: [BioC] [Rd] enabling reproducible research & R package management & install.package.version & BiocLite
 .
 .Paul,
 .
 .I think your balanced and reasoned approach addresses all my current concerns.  Nice!  I will likely adopt your methods.  Let me
 .ruminate.  Thanks for this.
 .
 .~ Malcolm
 .
 . .-----Original Message-----
 . .From: Paul Gilbert [mailto:pgilbert902 at gmail.com]
 . .Sent: Tuesday, March 05, 2013 4:34 PM
 . .To: Cook, Malcolm
 . .Cc: 'r-devel at r-project.org'; 'bioconductor at r-project.org'; 'r-discussion at listserv.stowers.org'
 . .Subject: Re: [Rd] [BioC] enabling reproducible research & R package management & install.package.version & BiocLite  .
 . .(More on the original question further below.)  .
 . .On 13-03-05 09:48 AM, Cook, Malcolm wrote:
 . .> All,
 . .>
 . .> What got me started on this line of inquiry was my attempt at  .> balancing the advantages of performing a periodic (daily or
 .weekly)  .> update to the 'release' version of locally installed R/Bioconductor  .> packages on our institute-wide installation of R with
 .the  .> disadvantages of potentially changing the result of an analyst's  .> workflow in mid-project.
 . .
 . .I have implemented a strategy to try to address this as follows:
 . .
 . .1/ Install a new version of R when it is released, and packages in the R  .version's site-library with package versions as available at the
 .time  .the R version is installed. Only upgrade these package versions in the  .case they are severely broken.
 . .
 . .2/ Install the same packages in site-library-fresh and upgrade these  .package versions on a regular basis (e.g. daily).
 . .
 . .3/ When a new version of R is released, freeze but do not remove the old  .R version, at least not for a fairly long time, and freeze
 ..site-library-fresh for the old version. Begin with the new version as in  .1/ and 2/. The old version remains available, so "reverting" is
 .trivial.
 . .
 . .
 . .The analysts are then responsible for choosing the R version they use,  .and the library they use. This means they do not have to
 .change R and  .package version mid-project, but they can if they wish. I think the  .above two libraries will cover most cases, but it is
 .possible that a few  .projects will need their own special library with a combination of  .package versions. In this case the user could
 .create their own library,  .or you might prefer some more official mechanism.
 . .
 . .The idea of the above strategy is to provide the stability one might  .want for an ongoing project, and the possibility of an upgraded
 .package  .if necessary, but not encourage analysts to remain indefinitely with old  .versions (by say, putting new packages in an old R
 .version library).
 . .
 . .This strategy has been implemented in a set of make files in the project  .RoboAdmin available at http://automater.r-forge.r-
 .project.org/. It can  .be done entirely automatically with a cron job. Constructive comments  .are always appreciated.
 . .
 . .(IT departments sometimes think that there should be only one version of  .everything available, which they test and approve. So
 .the initial  .reaction to this approach could be negative. I think they have not  .really thought about the advantages. They usually
 .cannot test/approve an  .upgrade without user input, and timing is often extremely complicate  .because of ongoing user needs. This
 .strategy is simply shifting  .responsibility and timing to the users, or user departments, that can  .actually do the testing and
 .approving.)  .
 . .Regarding NFS mounts, it is relatively robust. There can be occasional  .problems, especially for users that have a habit of keeping an
 .R session  .open for days at a time and using site-library-fresh packages. In my  .experience this did not happen often enough to worry
 .about a "blackout  .period".
 . .
 . .Regarding the original question, I would like to think it could be  .possible to keep enough information to reproduce the exact
 .environment,  .but I think for potentially sensitive numerical problems that is  .optimistic. As others have pointed out, results can
 .depend not only on R  .and package versions, configuration, OS versions, and library and  .compiler versions, but also on the
 .underlying hardware. You might have  .some hope using something like an Amazon core instance. (BTW, this  .problem is not specific
 .to R.)  .
 . .It is true that restricting to a fixed computing environment at your  .institution may ease things somewhat, but if you occasionally
 .upgrade  .hardware or the OS then you will probably lose reproducibility.
 . .
 . .An alternative that I recommend is that you produce a set of tests that  .confirm the results of any important project. These can be
 .conveniently  .put in the tests/ directory of an R package, which is then maintained  .local, not on CRAN, and built/tested whenever a
 .new R and packages are  .installed. (Tools for this are also available at the above indicated web
 . .site.) This approach means that you continue to reproduce the old  .results, or if not, discover differences/problems in the old or new
 ..version of R and/or packages that may be important to you. I have been  .successfully using a variant of this since about 1993, using R
 .and  .package tests/ since they became available.
 . .
 . .Paul
 . .
 . .>
 . .> I just got the "green light" to institute such periodic updates that  .> I have been arguing is in our collective best interest.  In return,
 ..> I promised my best effort to provide a means for preserving or  .> reverting to a working R library configuration.
 . .>
 . .> Please note that the reproducibility I am most eager to provide is  .> limited to reproducibility within the computing environment of
 .our  .> institute, which perhaps takes away some of the dragon's nests,  .> though certainly not all.
 . .>
 . .> There are technical issues of updating package installations on an  .> NFS mount that might have files/libraries open on it from
 .running R  .> sessions.  I am interested in learning of approaches for  .> minimizing/eliminating exposure to these issue as well.  The  .>
 .first/best approach seems to be to institute a 'black out' period
 . .> when users should expect the installed library to change.   Perhaps
 . .> there are improvements to this????
 . .>
 . .> Best,
 . .>
 . .> Malcolm
 . .>
 . .>
 . .> .-----Original Message----- .From: Mike Marchywka  .> [mailto:marchywka at hotmail.com] .Sent: Tuesday, March 05, 2013 5:24  .>
 .AM .To: amackey at virginia.edu; Cook, Malcolm .Cc:
 . .> r-devel at r-project.org; bioconductor at r-project.org;  .> r-discussion at listserv.stowers.org .Subject: RE: [Rd] [BioC] enabling  .>
 .reproducible research & R package management &  .> install.package.version & BiocLite . . .I hate to ask what go this  .> thread started
 .but it sounds like someone was counting on .exact  .> numeric reproducibility or was there a bug in a specific release? In  .> actual
 ..fact, the best way to determine reproducibility is run the  .> code in a variety of .packages. Alternatively, you can do everything  .> in
 .java and not assume .that calculations commute or associate as the  .> code is modified but it seems .pointless. Sensitivity
 .determination  .> would seem to lead to more reprodicible results .than trying to keep  .> a specific set of code quirks. . .I also seem to
 .recall that FPU may  .> have random lower order bits in some cases, .same code/data give  .> different results. Alsways assume FP is
 .stochastic and plan .on  .> anlayzing the "noise." . . .----------------------------------------
 . .> .> From: amackey at virginia.edu .> Date: Mon, 4 Mar 2013 16:28:48  .> -0500 .> To: MEC at stowers.org .> CC: r-devel at r-project.org;
 ..> bioconductor at r-project.org; r-discussion at listserv.stowers.org .>  .> Subject: Re: [Rd] [BioC] enabling reproducible research & R
 .package  .> management & install.package.version & BiocLite .> .> On Mon, Mar 4,  .> 2013 at 4:13 PM, Cook, Malcolm
 .<MEC at stowers.org> wrote: .> .> > *  .> where do the dragons lurk .> > .> .> webs of interconnected  .> dynamically loaded libraries,
 .identical versions of .> R compiled  .> with different BLAS/LAPACK options, etc. Go with the VM if you .>  .> really, truly, want this level
 .of exact reproducibility. .> .> An  .> alternative (and arguably more useful) strategy would be to cache .>  .> results of each
 .computational step, and report when results differ  .> upon .> re-execution with identical inputs; if you cache sessionInfo  .> along
 .with .> each result, you can identify which package(s) changed,  .> and begin to hunt .> down why the change occurred (possibly for
 .the  .> better); couple this with .> the concept of keeping both code *and*  .> results in version control, then you .> can move forward
 .with a  .> (re)analysis without being crippled by out-of-date .> software. .> .>  .> -Aaron .> .> -- .> Aaron J. Mackey, PhD .> Assistant
 .Professor .>  .> Center for Public Health Genomics .> University of Virginia .>  .> amackey at virginia.edu .>
 .http://www.cphg.virginia.edu/mackey .> .>  .> [[alternative HTML version deleted]] .> .>  .>
 .______________________________________________ .>  .> R-devel at r-project.org mailing list .>  .>
 .https://stat.ethz.ch/mailman/listinfo/r-devel .
 . .>
 . .> ______________________________________________ R-devel at r-project.org  .> mailing list
 .https://stat.ethz.ch/mailman/listinfo/r-devel
 . .>
 .
 ._______________________________________________
 .Bioconductor mailing list
 .Bioconductor at r-project.org
 .https://stat.ethz.ch/mailman/listinfo/bioconductor
 .Search the archives: http://news.gmane.org/gmane.science.biology.informatics.conductor


From tim.triche at gmail.com  Wed Mar  6 19:30:37 2013
From: tim.triche at gmail.com (Tim Triche, Jr.)
Date: Wed, 6 Mar 2013 10:30:37 -0800
Subject: [Rd] [BioC] enabling reproducible research & R package
 management & install.package.version & BiocLite
In-Reply-To: <D4772401B9D976478C0895769BE3E7920767B0@MBSRV02.sgc.loc>
References: <D4772401B9D976478C0895769BE3E7920716E6@MBSRV02.sgc.loc>
	<CAErFSogyfzMQ9UEK7e=Ukvgdg-NfxS60cPtppvvvN0MKnogyzw@mail.gmail.com>
	<BLU166-W14A5221668A7BDBAC726DEBEFB0@phx.gbl>
	<D4772401B9D976478C0895769BE3E7920733A1@MBSRV02.sgc.loc>
	<513672E1.6080105@gmail.com>
	<D4772401B9D976478C0895769BE3E7920747D3@MBSRV02.sgc.loc>
	<86BFEB1DFA6CB3448DB8AB1FC52F405915F069@ummscsmbx06.ad.umassmed.edu>
	<D4772401B9D976478C0895769BE3E7920767B0@MBSRV02.sgc.loc>
Message-ID: <CAC+N9BXwzO-cMc8x74xoyR0R0LZbrpxRyAjXOVycVsS-MusYJw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20130306/f191d562/attachment.pl>

From alexbbrown at gmail.com  Wed Mar  6 19:24:01 2013
From: alexbbrown at gmail.com (Alex Brown)
Date: Wed, 6 Mar 2013 10:24:01 -0800
Subject: [Rd] do_fileinfo / file.info test for file IS directory during
 package load pointlessly stresses NIS by getting username / group info
Message-ID: <CAOu2b4a38i+hREfvtSFWLOx8WK7d-2ZqhwACkU6tmWfp8TFLBQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20130306/8722edba/attachment.pl>

From David.Lapointe at umassmed.edu  Wed Mar  6 14:46:06 2013
From: David.Lapointe at umassmed.edu (Lapointe, David)
Date: Wed, 6 Mar 2013 13:46:06 +0000
Subject: [Rd] [BioC] enabling reproducible research & R package
 management & install.package.version & BiocLite
In-Reply-To: <D4772401B9D976478C0895769BE3E7920747D3@MBSRV02.sgc.loc>
References: <D4772401B9D976478C0895769BE3E7920716E6@MBSRV02.sgc.loc>,
	<CAErFSogyfzMQ9UEK7e=Ukvgdg-NfxS60cPtppvvvN0MKnogyzw@mail.gmail.com>
	<BLU166-W14A5221668A7BDBAC726DEBEFB0@phx.gbl>
	<D4772401B9D976478C0895769BE3E7920733A1@MBSRV02.sgc.loc>
	<513672E1.6080105@gmail.com>
	<D4772401B9D976478C0895769BE3E7920747D3@MBSRV02.sgc.loc>
Message-ID: <86BFEB1DFA6CB3448DB8AB1FC52F405915F069@ummscsmbx06.ad.umassmed.edu>

There are utilities ( e.g. dotkit, and modules) which facilitate version management, basically creating on the fly PATH and env setups, if you are comfortable keeping all that around. 

David

-----Original Message-----
From: bioconductor-bounces at r-project.org [mailto:bioconductor-bounces at r-project.org] On Behalf Of Cook, Malcolm
Sent: Tuesday, March 05, 2013 6:08 PM
To: 'Paul Gilbert'
Cc: 'r-devel at r-project.org'; 'bioconductor at r-project.org'; 'r-discussion at listserv.stowers.org'
Subject: Re: [BioC] [Rd] enabling reproducible research & R package management & install.package.version & BiocLite

Paul,

I think your balanced and reasoned approach addresses all my current concerns.  Nice!  I will likely adopt your methods.  Let me ruminate.  Thanks for this.

~ Malcolm

 .-----Original Message-----
 .From: Paul Gilbert [mailto:pgilbert902 at gmail.com]
 .Sent: Tuesday, March 05, 2013 4:34 PM
 .To: Cook, Malcolm
 .Cc: 'r-devel at r-project.org'; 'bioconductor at r-project.org'; 'r-discussion at listserv.stowers.org'
 .Subject: Re: [Rd] [BioC] enabling reproducible research & R package management & install.package.version & BiocLite  .
 .(More on the original question further below.)  .
 .On 13-03-05 09:48 AM, Cook, Malcolm wrote:
 .> All,
 .>
 .> What got me started on this line of inquiry was my attempt at  .> balancing the advantages of performing a periodic (daily or weekly)  .> update to the 'release' version of locally installed R/Bioconductor  .> packages on our institute-wide installation of R with the  .> disadvantages of potentially changing the result of an analyst's  .> workflow in mid-project.
 .
 .I have implemented a strategy to try to address this as follows:
 .
 .1/ Install a new version of R when it is released, and packages in the R  .version's site-library with package versions as available at the time  .the R version is installed. Only upgrade these package versions in the  .case they are severely broken.
 .
 .2/ Install the same packages in site-library-fresh and upgrade these  .package versions on a regular basis (e.g. daily).
 .
 .3/ When a new version of R is released, freeze but do not remove the old  .R version, at least not for a fairly long time, and freeze  .site-library-fresh for the old version. Begin with the new version as in  .1/ and 2/. The old version remains available, so "reverting" is trivial.
 .
 .
 .The analysts are then responsible for choosing the R version they use,  .and the library they use. This means they do not have to change R and  .package version mid-project, but they can if they wish. I think the  .above two libraries will cover most cases, but it is possible that a few  .projects will need their own special library with a combination of  .package versions. In this case the user could create their own library,  .or you might prefer some more official mechanism.
 .
 .The idea of the above strategy is to provide the stability one might  .want for an ongoing project, and the possibility of an upgraded package  .if necessary, but not encourage analysts to remain indefinitely with old  .versions (by say, putting new packages in an old R version library).
 .
 .This strategy has been implemented in a set of make files in the project  .RoboAdmin available at http://automater.r-forge.r-project.org/. It can  .be done entirely automatically with a cron job. Constructive comments  .are always appreciated.
 .
 .(IT departments sometimes think that there should be only one version of  .everything available, which they test and approve. So the initial  .reaction to this approach could be negative. I think they have not  .really thought about the advantages. They usually cannot test/approve an  .upgrade without user input, and timing is often extremely complicate  .because of ongoing user needs. This strategy is simply shifting  .responsibility and timing to the users, or user departments, that can  .actually do the testing and approving.)  .
 .Regarding NFS mounts, it is relatively robust. There can be occasional  .problems, especially for users that have a habit of keeping an R session  .open for days at a time and using site-library-fresh packages. In my  .experience this did not happen often enough to worry about a "blackout  .period".
 .
 .Regarding the original question, I would like to think it could be  .possible to keep enough information to reproduce the exact environment,  .but I think for potentially sensitive numerical problems that is  .optimistic. As others have pointed out, results can depend not only on R  .and package versions, configuration, OS versions, and library and  .compiler versions, but also on the underlying hardware. You might have  .some hope using something like an Amazon core instance. (BTW, this  .problem is not specific to R.)  .
 .It is true that restricting to a fixed computing environment at your  .institution may ease things somewhat, but if you occasionally upgrade  .hardware or the OS then you will probably lose reproducibility.
 .
 .An alternative that I recommend is that you produce a set of tests that  .confirm the results of any important project. These can be conveniently  .put in the tests/ directory of an R package, which is then maintained  .local, not on CRAN, and built/tested whenever a new R and packages are  .installed. (Tools for this are also available at the above indicated web
 .site.) This approach means that you continue to reproduce the old  .results, or if not, discover differences/problems in the old or new  .version of R and/or packages that may be important to you. I have been  .successfully using a variant of this since about 1993, using R and  .package tests/ since they became available.
 .
 .Paul
 .
 .>
 .> I just got the "green light" to institute such periodic updates that  .> I have been arguing is in our collective best interest.  In return,  .> I promised my best effort to provide a means for preserving or  .> reverting to a working R library configuration.
 .>
 .> Please note that the reproducibility I am most eager to provide is  .> limited to reproducibility within the computing environment of our  .> institute, which perhaps takes away some of the dragon's nests,  .> though certainly not all.
 .>
 .> There are technical issues of updating package installations on an  .> NFS mount that might have files/libraries open on it from running R  .> sessions.  I am interested in learning of approaches for  .> minimizing/eliminating exposure to these issue as well.  The  .> first/best approach seems to be to institute a 'black out' period
 .> when users should expect the installed library to change.   Perhaps
 .> there are improvements to this????
 .>
 .> Best,
 .>
 .> Malcolm
 .>
 .>
 .> .-----Original Message----- .From: Mike Marchywka  .> [mailto:marchywka at hotmail.com] .Sent: Tuesday, March 05, 2013 5:24  .> AM .To: amackey at virginia.edu; Cook, Malcolm .Cc:
 .> r-devel at r-project.org; bioconductor at r-project.org;  .> r-discussion at listserv.stowers.org .Subject: RE: [Rd] [BioC] enabling  .> reproducible research & R package management &  .> install.package.version & BiocLite . . .I hate to ask what go this  .> thread started but it sounds like someone was counting on .exact  .> numeric reproducibility or was there a bug in a specific release? In  .> actual .fact, the best way to determine reproducibility is run the  .> code in a variety of .packages. Alternatively, you can do everything  .> in java and not assume .that calculations commute or associate as the  .> code is modified but it seems .pointless. Sensitivity determination  .> would seem to lead to more reprodicible results .than trying to keep  .> a specific set of code quirks. . .I also seem to recall that FPU may  .> have random lower order bits in some cases, .same code/data give  .> different results. Alsways assume FP is stochastic and plan .on  .> anlayzing the "noise." . . .----------------------------------------
 .> .> From: amackey at virginia.edu .> Date: Mon, 4 Mar 2013 16:28:48  .> -0500 .> To: MEC at stowers.org .> CC: r-devel at r-project.org;  .> bioconductor at r-project.org; r-discussion at listserv.stowers.org .>  .> Subject: Re: [Rd] [BioC] enabling reproducible research & R package  .> management & install.package.version & BiocLite .> .> On Mon, Mar 4,  .> 2013 at 4:13 PM, Cook, Malcolm <MEC at stowers.org> wrote: .> .> > *  .> where do the dragons lurk .> > .> .> webs of interconnected  .> dynamically loaded libraries, identical versions of .> R compiled  .> with different BLAS/LAPACK options, etc. Go with the VM if you .>  .> really, truly, want this level of exact reproducibility. .> .> An  .> alternative (and arguably more useful) strategy would be to cache .>  .> results of each computational step, and report when results differ  .> upon .> re-execution with identical inputs; if you cache sessionInfo  .> along with .> each result, you can identify which package(s) changed,  .> and begin to hunt .> down why the change occurred (possibly for the  .> better); couple this with .> the concept of keeping both code *and*  .> results in version control, then you .> can move forward with a  .> (re)analysis without being crippled by out-of-date .> software. .> .>  .> -Aaron .> .> -- .> Aaron J. Mackey, PhD .> Assistant Professor .>  .> Center for Public Health Genomics .> University of Virginia .>  .> amackey at virginia.edu .> http://www.cphg.virginia.edu/mackey .> .>  .> [[alternative HTML version deleted]] .> .>  .> ______________________________________________ .>  .> R-devel at r-project.org mailing list .>  .> https://stat.ethz.ch/mailman/listinfo/r-devel .
 .>
 .> ______________________________________________ R-devel at r-project.org  .> mailing list https://stat.ethz.ch/mailman/listinfo/r-devel
 .>

_______________________________________________
Bioconductor mailing list
Bioconductor at r-project.org
https://stat.ethz.ch/mailman/listinfo/bioconductor
Search the archives: http://news.gmane.org/gmane.science.biology.informatics.conductor


From saleumd at yahoo.com  Wed Mar  6 18:42:23 2013
From: saleumd at yahoo.com (qwumd)
Date: Wed, 6 Mar 2013 09:42:23 -0800 (PST)
Subject: [Rd] lapply coerce data.frame to a list
In-Reply-To: <1348264356.46280.YahooMailNeo@web163802.mail.gq1.yahoo.com>
References: <1348264356.46280.YahooMailNeo@web163802.mail.gq1.yahoo.com>
Message-ID: <1362591743.1827.YahooMailNeo@web163803.mail.gq1.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20130306/d2423af2/attachment.pl>

From lgautier at gmail.com  Thu Mar  7 15:04:42 2013
From: lgautier at gmail.com (Laurent Gautier)
Date: Thu, 07 Mar 2013 15:04:42 +0100
Subject: [Rd] Printing warning messages around R_tryEval
In-Reply-To: <mailman.27.1362654009.12396.r-devel@r-project.org>
References: <mailman.27.1362654009.12396.r-devel@r-project.org>
Message-ID: <51389E7A.1010704@gmail.com>

We are having a similar issue with rpy2 and R-devel.
I would also vote for having back a C-level solution to the problem.

I cannot find an explicit explanation for the change in the SVN logs,
and traced the change to rev 61771:

------------------------------------------------------------------------
r61771 | ripley | 2013-01-29 12:09:45 +0100 (Tue, 29 Jan 2013) | 1 line

more hiding
------------------------------------------------------------------------

Could this mean that the change could reverted and Rf_PrintWarnings
just be added to the C API ?

Laurent


On 2013-03-07 12:00, r-devel-request at r-project.org wrote:
>
> Hi!
>
> In RKWard we use R_tryEval() at a number places to run R commands. In order to
> make sure that any generated warnings become visible close to the problem, we
> were following up (most of) these calls with Rf_PrintWarnings().
>
> Rf_PrintWarnings() was never available in the headers (as far as I know), and
> in r61771 the symbol has been hidden from libR.so. So this solution is no
> longer available. Could you give advice on the best way to print warnings
> before returning to the top level?
>
> Some available options, I am aware of:
> 1. Call R function warnings(). However, this will always print the latest
> warning, even if it was generate by some earlier command. I would need a way
> to print new warnings, only.
>
> 2. Use options(warn=1) where applicable. However, in some cases, collecting
> warnings until some procedure is complete, and printing them then, would be
> preferrable.
>
> 3. I see there is an internal call printDeferredWarnings(), which seems to be
> almost exactly what I want. However, using an .Internal() does not look like a
> terribly stable solution, either. Also, having direct access to a similar
> function from the C-API would be very comfortable.
>
> Thanks!
> Thomas


From nalimilan at club.fr  Fri Mar  8 14:50:50 2013
From: nalimilan at club.fr (Milan Bouchet-Valat)
Date: Fri, 08 Mar 2013 14:50:50 +0100
Subject: [Rd] Small quirks in summary.(g)lm docs
In-Reply-To: <1361291525.2067.109.camel@milan.ined.fr>
References: <1361291525.2067.109.camel@milan.ined.fr>
Message-ID: <1362750650.28193.23.camel@milan>

Hi!

Anybody on this patch?

Le mardi 19 f?vrier 2013 ? 17:32 +0100, Milan Bouchet-Valat a ?crit :
> Hi!
> 
> In R 3.0.0 from current SVN, ?summary.lm says:
> > Value [...]
> > df degrees of freedom, a 3-vector (p, n-p, p*), the last
> >    being the number of non-aliased coefficients.
> 
> ?summary.glm says:
> > df a 3-vector of the rank of the model and the number of residual 
> >    degrees of freedom, plus number of non-aliased coefficients.
> 
> It seems to me that the description is reversed: p is the number of
> non-aliased coefficients, and p* the total number of coefficients. I do
> not have reference books off-hand to check how it is intended to work,
> but see this example:
> 
> ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
> trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
> group <- gl(2,10,20, labels=c("Ctl","Trt"))
> weight <- c(ctl, trt)
> lm.D9 <- lm(weight ~ group + I(group != "Ctl"))
> lm.D9
> 
> Call:
> lm(formula = weight ~ group + I(group != "Ctl"))
> 
> Coefficients:
>           (Intercept)               groupTrt  I(group != "Ctl")TRUE  
>                 5.032                 -0.371                     NA  
> 
> summary(lm.D9)$df
> [1]  2 18  3
> 
> sum(!summary(lm.D9)$aliased)
> [1] 2
> 
> 
> The same is true with glm().
> 
> 
> Also, ?summary.lm seems to miss a mention that is present
> in ?summary.glm:
> > Aliased coefficients are omitted in the returned object but (as from
> > R 1.8.0) restored by the print method.
> 
> This is apparently true of summary.lm too:
> 
> summary(lm.D9)
> 
> Call:
> lm(formula = weight ~ group + I(group != "Ctl"))
> 
> Residuals:
>     Min      1Q  Median      3Q     Max 
> -1.0710 -0.4938  0.0685  0.2462  1.3690 
> 
> Coefficients: (1 not defined because of singularities)
>                       Estimate Std. Error t value Pr(>|t|)    
> (Intercept)             5.0320     0.2202  22.850 9.55e-15 ***
> groupTrt               -0.3710     0.3114  -1.191    0.249    
> I(group != "Ctl")TRUE       NA         NA      NA       NA    
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 
> 
> Residual standard error: 0.6964 on 18 degrees of freedom
> Multiple R-squared: 0.07308,	Adjusted R-squared: 0.02158 
> F-statistic: 1.419 on 1 and 18 DF,  p-value: 0.249 
> 
> summary(lm.D9)$coefficients
>             Estimate Std. Error  t value     Pr(>|t|)
> (Intercept)    5.032  0.2202177 22.85012 9.547128e-15
> groupTrt      -0.371  0.3114349 -1.19126 2.490232e-01
> 
> 
> Attached is a patch that applies these changes, if I'm not mistaken (and
> my English can be improved...).
> 
> 
> Regards
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From krivitsky at stat.psu.edu  Sat Mar  9 01:42:04 2013
From: krivitsky at stat.psu.edu (Pavel N. Krivitsky)
Date: Fri, 08 Mar 2013 19:42:04 -0500
Subject: [Rd] Submitting packages with weak circular dependencies to CRAN.
Message-ID: <1362789724.31926.414.camel@krivitsky.heinz.win.cmu.edu>

Hello, R-devel,

I am planning to submit two packages, A and B, to CRAN. Package B uses
an API exported by package A, while package A uses package B to test the
API in question. Thus, package B Depends on, and Enhances, A, and A
Suggests B.

Could I get some guidance on submitting them to CRAN? A cannot be
checked without B, but B cannot be installed without A.

                    Thanks in advance,
                    Pavel Krivitsky


From oliver at first.in-berlin.de  Sat Mar  9 01:49:20 2013
From: oliver at first.in-berlin.de (oliver)
Date: Sat, 9 Mar 2013 01:49:20 +0100
Subject: [Rd] Submitting packages with weak circular dependencies to
 CRAN.
In-Reply-To: <1362789724.31926.414.camel@krivitsky.heinz.win.cmu.edu>
References: <1362789724.31926.414.camel@krivitsky.heinz.win.cmu.edu>
Message-ID: <20130309004920.GA9395@siouxsie>

Hello,

On Fri, Mar 08, 2013 at 07:42:04PM -0500, Pavel N. Krivitsky wrote:
> Hello, R-devel,
> 
> I am planning to submit two packages, A and B, to CRAN. Package B uses
> an API exported by package A, while package A uses package B to test the
> API in question. Thus, package B Depends on, and Enhances, A, and A
> Suggests B.
[...]

Why not putting common stuff of A and B into C and then let A and B depend on C?

Ciao,
   Oliver


From hpages at fhcrc.org  Sat Mar  9 10:02:52 2013
From: hpages at fhcrc.org (=?ISO-8859-1?Q?Herv=E9_Pag=E8s?=)
Date: Sat, 09 Mar 2013 01:02:52 -0800
Subject: [Rd] Submitting packages with weak circular dependencies to
	CRAN.
In-Reply-To: <1362789724.31926.414.camel@krivitsky.heinz.win.cmu.edu>
References: <1362789724.31926.414.camel@krivitsky.heinz.win.cmu.edu>
Message-ID: <513AFABC.7030203@fhcrc.org>

Hi Pavel,

On 03/08/2013 04:42 PM, Pavel N. Krivitsky wrote:
> Hello, R-devel,
>
> I am planning to submit two packages, A and B, to CRAN. Package B uses
> an API exported by package A, while package A uses package B to test the
> API in question. Thus, package B Depends on, and Enhances, A, and A
> Suggests B.
>
> Could I get some guidance on submitting them to CRAN? A cannot be
> checked without B, but B cannot be installed without A.

The CRAN people will probably have a better solution to offer but the
following 3-step procedure should work, at least in theory:

- First submit a degraded version of A i.e. a version that doesn't
   suggest B and where all the tests using B are turned off.

- Then submit B.

- Then submit the full version of A with a higher version.

Or, merge A and B in a single package...

Cheers,
H.


>
>                      Thanks in advance,
>                      Pavel Krivitsky
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fhcrc.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From ligges at statistik.tu-dortmund.de  Sat Mar  9 16:43:45 2013
From: ligges at statistik.tu-dortmund.de (Uwe Ligges)
Date: Sat, 09 Mar 2013 16:43:45 +0100
Subject: [Rd] Submitting packages with weak circular dependencies to
	CRAN.
In-Reply-To: <513AFABC.7030203@fhcrc.org>
References: <1362789724.31926.414.camel@krivitsky.heinz.win.cmu.edu>
	<513AFABC.7030203@fhcrc.org>
Message-ID: <513B58B1.4070705@statistik.tu-dortmund.de>



On 09.03.2013 10:02, Herv? Pag?s wrote:
> Hi Pavel,
>
> On 03/08/2013 04:42 PM, Pavel N. Krivitsky wrote:
>> Hello, R-devel,
>>
>> I am planning to submit two packages, A and B, to CRAN. Package B uses
>> an API exported by package A, while package A uses package B to test the
>> API in question. Thus, package B Depends on, and Enhances, A, and A
>> Suggests B.
>>
>> Could I get some guidance on submitting them to CRAN? A cannot be
>> checked without B, but B cannot be installed without A.
>
> The CRAN people will probably have a better solution to offer but the
> following 3-step procedure should work, at least in theory:
>
> - First submit a degraded version of A i.e. a version that doesn't
>    suggest B and where all the tests using B are turned off.
>
> - Then submit B.
>
> - Then submit the full version of A with a higher version.
>
> Or, merge A and B in a single package...


...  or ask the CRAN people directly. They will tell you to go ahead, 
submit A and mention in the submission mail that the Suggested B which 
Depends on A will be uploaded once A has been accepted.

Best,
Uwe Ligges


>
> Cheers,
> H.
>
>
>>
>>                      Thanks in advance,
>>                      Pavel Krivitsky
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>


From kevin.hendricks at sympatico.ca  Sat Mar  9 21:25:44 2013
From: kevin.hendricks at sympatico.ca (Kevin Hendricks)
Date: Sat, 9 Mar 2013 15:25:44 -0500
Subject: [Rd] question on why Rigroup package moved to Archive on CRAN
Message-ID: <BLU0-SMTP1663AF8838B6E599BDF19787E70@phx.gbl>

Hi,

Who should I ask about my package Rigroup_0.83 being moved to Archive status on CRAN and no longer available via install.package?  I have no problems with the move if this was simply because of low demand.  However, if there was a build issue with the newest releases that caused problems, I would be happy to address it.  I'll just ask my students to install it from my own locally hosted source archive.

Thanks,

Kevin

From hb at biostat.ucsf.edu  Sat Mar  9 22:00:42 2013
From: hb at biostat.ucsf.edu (Henrik Bengtsson)
Date: Sat, 9 Mar 2013 13:00:42 -0800
Subject: [Rd] question on why Rigroup package moved to Archive on CRAN
In-Reply-To: <BLU0-SMTP1663AF8838B6E599BDF19787E70@phx.gbl>
References: <BLU0-SMTP1663AF8838B6E599BDF19787E70@phx.gbl>
Message-ID: <CAFDcVCSpBzh0hm0_C=SfYTVekrZ7iSckuDg5wFwd5f2N0bGbSA@mail.gmail.com>

On Sat, Mar 9, 2013 at 12:25 PM, Kevin Hendricks
<kevin.hendricks at sympatico.ca> wrote:
> Hi,
>
> Who should I ask about my package Rigroup_0.83 being moved to Archive status on CRAN and no longer available via install.package?  I have no problems with the move if this was simply because of low demand.  However, if there was a build issue with the newest releases that caused problems, I would be happy to address it.  I'll just ask my students to install it from my own locally hosted source archive.

Most likely yourself; from the 'CRAN Repository Policy'
[http://cran.r-project.org/web/packages/policies.html]:

"Packages will not normally be removed from CRAN: however, they may be
archived, including at the maintainer's request.

Packages for which R CMD check gives an ?ERROR? when a new R x.y.0
version is released will be archived (or in exceptional circumstances
updated by the CRAN team) unless the maintainer has set a firm
deadline for an upcoming update (and keeps to it).

Maintainers will be asked to update packages which show any warnings
or significant notes, especially at around the time of a new x.y.0
release. Packages which are not updated are liable to be archived."

/Henrik

>
> Thanks,
>
> Kevin
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From xie at yihui.name  Sat Mar  9 22:32:21 2013
From: xie at yihui.name (Yihui Xie)
Date: Sat, 9 Mar 2013 15:32:21 -0600
Subject: [Rd] Why cannot `Rscript -e` accept an empty line?
Message-ID: <CANROs4c10oLkEmroi34oWTH=0zujNWgFxwLAGaa_rMemimPHZQ@mail.gmail.com>

See the example below (under Ubuntu):

$ Rscript -e '1' -e '2'
[1] 1
[1] 2
$ Rscript -e '1' -e '' -e '2'
ERROR: option '-e' requires an argument
$ uname -a
Linux xie 3.5.0-25-generic #39-Ubuntu SMP Mon Feb 25 18:26:58 UTC 2013
x86_64 x86_64 x86_64 GNU/Linux

Similar problem under Windows:

Rscript -e "1" -e "" -e "2"
[1] 1
Error: object 'e' not found
Execution halted

I can certainly save the code in a script and run Rscript foo.R, but
I'm curious why Rscript stops when the -e argument is an empty string.

Regards,
Yihui
--
Yihui Xie <xieyihui at gmail.com>
Phone: 515-294-2465 Web: http://yihui.name
Department of Statistics, Iowa State University
2215 Snedecor Hall, Ames, IA


From kevin.hendricks at sympatico.ca  Sat Mar  9 22:38:39 2013
From: kevin.hendricks at sympatico.ca (Kevin Hendricks)
Date: Sat, 9 Mar 2013 16:38:39 -0500
Subject: [Rd] question on why Rigroup package moved to Archive on CRAN
In-Reply-To: <CAFDcVCSpBzh0hm0_C=SfYTVekrZ7iSckuDg5wFwd5f2N0bGbSA@mail.gmail.com>
References: <BLU0-SMTP1663AF8838B6E599BDF19787E70@phx.gbl>
	<CAFDcVCSpBzh0hm0_C=SfYTVekrZ7iSckuDg5wFwd5f2N0bGbSA@mail.gmail.com>
Message-ID: <BLU0-SMTP30278138E54E7526966E2187E70@phx.gbl>

Hi, 
Thanks for that info.  It works/builds without error or warnings on my RedHat Enterprise 6  with R 2.15.2 version which I use but must be broken somehow on later versions.  I will see if I can find the autobuild warnings error report someplace.

Kevin


Sent from my iPad

On 2013-03-09, at 4:00 PM, Henrik Bengtsson <hb at biostat.ucsf.edu> wrote:

> On Sat, Mar 9, 2013 at 12:25 PM, Kevin Hendricks
> <kevin.hendricks at sympatico.ca> wrote:
>> Hi,
>> 
>> Who should I ask about my package Rigroup_0.83 being moved to Archive status on CRAN and no longer available via install.package?  I have no problems with the move if this was simply because of low demand.  However, if there was a build issue with the newest releases that caused problems, I would be happy to address it.  I'll just ask my students to install it from my own locally hosted source archive.
> 
> Most likely yourself; from the 'CRAN Repository Policy'
> [http://cran.r-project.org/web/packages/policies.html]:
> 
> "Packages will not normally be removed from CRAN: however, they may be
> archived, including at the maintainer's request.
> 
> Packages for which R CMD check gives an ?ERROR? when a new R x.y.0
> version is released will be archived (or in exceptional circumstances
> updated by the CRAN team) unless the maintainer has set a firm
> deadline for an upcoming update (and keeps to it).
> 
> Maintainers will be asked to update packages which show any warnings
> or significant notes, especially at around the time of a new x.y.0
> release. Packages which are not updated are liable to be archived."
> 
> /Henrik
> 
>> 
>> Thanks,
>> 
>> Kevin
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
> 


From kevin.hendricks at sympatico.ca  Sat Mar  9 23:17:45 2013
From: kevin.hendricks at sympatico.ca (Kevin Hendricks)
Date: Sat, 9 Mar 2013 17:17:45 -0500
Subject: [Rd] question on why Rigroup package moved to Archive on CRAN
In-Reply-To: <BLU0-SMTP30278138E54E7526966E2187E70@phx.gbl>
References: <BLU0-SMTP1663AF8838B6E599BDF19787E70@phx.gbl>
	<CAFDcVCSpBzh0hm0_C=SfYTVekrZ7iSckuDg5wFwd5f2N0bGbSA@mail.gmail.com>
	<BLU0-SMTP30278138E54E7526966E2187E70@phx.gbl>
Message-ID: <BLU0-SMTP472512ED1388C9E8DDB39B87E70@phx.gbl>

Hi,
One last quick question ... does anyone archive older CRAN package check summaries?  I searched the web but could not find any.  My package was archived in February of this year so any package check summary from earlier than that should help me figure out what is broken on MacOSX or Windows.  
Thanks,
Kevin


Sent from my iPad

On 2013-03-09, at 4:38 PM, Kevin Hendricks <kevin.hendricks at sympatico.ca> wrote:

> Hi, 
> Thanks for that info.  It works/builds without error or warnings on my RedHat Enterprise 6  with R 2.15.2 version which I use but must be broken somehow on later versions.  I will see if I can find the autobuild warnings error report someplace.
> 
> Kevin
> 
> 
> Sent from my iPad
> 
> On 2013-03-09, at 4:00 PM, Henrik Bengtsson <hb at biostat.ucsf.edu> wrote:
> 
>> On Sat, Mar 9, 2013 at 12:25 PM, Kevin Hendricks
>> <kevin.hendricks at sympatico.ca> wrote:
>>> Hi,
>>> 
>>> Who should I ask about my package Rigroup_0.83 being moved to Archive status on CRAN and no longer available via install.package?  I have no problems with the move if this was simply because of low demand.  However, if there was a build issue with the newest releases that caused problems, I would be happy to address it.  I'll just ask my students to install it from my own locally hosted source archive.
>> 
>> Most likely yourself; from the 'CRAN Repository Policy'
>> [http://cran.r-project.org/web/packages/policies.html]:
>> 
>> "Packages will not normally be removed from CRAN: however, they may be
>> archived, including at the maintainer's request.
>> 
>> Packages for which R CMD check gives an ?ERROR? when a new R x.y.0
>> version is released will be archived (or in exceptional circumstances
>> updated by the CRAN team) unless the maintainer has set a firm
>> deadline for an upcoming update (and keeps to it).
>> 
>> Maintainers will be asked to update packages which show any warnings
>> or significant notes, especially at around the time of a new x.y.0
>> release. Packages which are not updated are liable to be archived."
>> 
>> /Henrik
>> 
>>> 
>>> Thanks,
>>> 
>>> Kevin
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From dtenenba at fhcrc.org  Sat Mar  9 23:52:48 2013
From: dtenenba at fhcrc.org (Dan Tenenbaum)
Date: Sat, 9 Mar 2013 14:52:48 -0800
Subject: [Rd] question on why Rigroup package moved to Archive on CRAN
In-Reply-To: <BLU0-SMTP472512ED1388C9E8DDB39B87E70@phx.gbl>
References: <BLU0-SMTP1663AF8838B6E599BDF19787E70@phx.gbl>
	<CAFDcVCSpBzh0hm0_C=SfYTVekrZ7iSckuDg5wFwd5f2N0bGbSA@mail.gmail.com>
	<BLU0-SMTP30278138E54E7526966E2187E70@phx.gbl>
	<BLU0-SMTP472512ED1388C9E8DDB39B87E70@phx.gbl>
Message-ID: <CAF42j23teS7eq9Qj-CqqOTyUC-GKxvy5OLoRwG0Rkcw9VOf52w@mail.gmail.com>

On Sat, Mar 9, 2013 at 2:17 PM, Kevin Hendricks
<kevin.hendricks at sympatico.ca> wrote:
> Hi,
> One last quick question ... does anyone archive older CRAN package check summaries?  I searched the web but could not find any.  My package was archived in February of this year so any package check summary from earlier than that should help me figure out what is broken on MacOSX or Windows.

FWIW, it fails check on my Mac as follows:

* checking examples ... ERROR
Running examples in 'Rigroup-Ex.R' failed
The error most likely occurred in:

> ### Name: igroupAlls
> ### Title: calculates logical All for small integer groups of logical
> ###   vectors
> ### Aliases: igroupAlls
> ### Keywords: utilities
>
> ### ** Examples
>
> y <- rnorm(100)
> i <- rep(1:25,4)
> x <- (y > 1.0)
> alls <- igroupAlls(x,i)
Error in .External("igroupFuns", x, i, R_IGFMINS, na.rm, PACKAGE = "Rigroup") :
  Incorrect number of arguments (4), expecting 1 for 'igroupFuns'
Calls: igroupAlls -> .External
Execution halted


Dan


> Thanks,
> Kevin
>
>
> Sent from my iPad
>
> On 2013-03-09, at 4:38 PM, Kevin Hendricks <kevin.hendricks at sympatico.ca> wrote:
>
>> Hi,
>> Thanks for that info.  It works/builds without error or warnings on my RedHat Enterprise 6  with R 2.15.2 version which I use but must be broken somehow on later versions.  I will see if I can find the autobuild warnings error report someplace.
>>
>> Kevin
>>
>>
>> Sent from my iPad
>>
>> On 2013-03-09, at 4:00 PM, Henrik Bengtsson <hb at biostat.ucsf.edu> wrote:
>>
>>> On Sat, Mar 9, 2013 at 12:25 PM, Kevin Hendricks
>>> <kevin.hendricks at sympatico.ca> wrote:
>>>> Hi,
>>>>
>>>> Who should I ask about my package Rigroup_0.83 being moved to Archive status on CRAN and no longer available via install.package?  I have no problems with the move if this was simply because of low demand.  However, if there was a build issue with the newest releases that caused problems, I would be happy to address it.  I'll just ask my students to install it from my own locally hosted source archive.
>>>
>>> Most likely yourself; from the 'CRAN Repository Policy'
>>> [http://cran.r-project.org/web/packages/policies.html]:
>>>
>>> "Packages will not normally be removed from CRAN: however, they may be
>>> archived, including at the maintainer's request.
>>>
>>> Packages for which R CMD check gives an ?ERROR? when a new R x.y.0
>>> version is released will be archived (or in exceptional circumstances
>>> updated by the CRAN team) unless the maintainer has set a firm
>>> deadline for an upcoming update (and keeps to it).
>>>
>>> Maintainers will be asked to update packages which show any warnings
>>> or significant notes, especially at around the time of a new x.y.0
>>> release. Packages which are not updated are liable to be archived."
>>>
>>> /Henrik
>>>
>>>>
>>>> Thanks,
>>>>
>>>> Kevin
>>>> ______________________________________________
>>>> R-devel at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From b.rowlingson at lancaster.ac.uk  Sun Mar 10 01:26:16 2013
From: b.rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Sun, 10 Mar 2013 00:26:16 +0000
Subject: [Rd] Why cannot `Rscript -e` accept an empty line?
In-Reply-To: <a5ece76a00fe4140a55e967d550d9100@EX-0-HT0.lancs.local>
References: <a5ece76a00fe4140a55e967d550d9100@EX-0-HT0.lancs.local>
Message-ID: <CANVKczNe5=qbGpD6cDF5N1NetzWNYe=M7FrdgF=0G=V80dBRsg@mail.gmail.com>

On Sat, Mar 9, 2013 at 9:32 PM, Yihui Xie <xie at yihui.name> wrote:
> See the example below (under Ubuntu):
>
> $ Rscript -e '1' -e '2'
> [1] 1
> [1] 2
> $ Rscript -e '1' -e '' -e '2'
> ERROR: option '-e' requires an argument
> $ uname -a
> Linux xie 3.5.0-25-generic #39-Ubuntu SMP Mon Feb 25 18:26:58 UTC 2013
> x86_64 x86_64 x86_64 GNU/Linux
>
> Similar problem under Windows:
>
> Rscript -e "1" -e "" -e "2"
> [1] 1
> Error: object 'e' not found
> Execution halted
>
> I can certainly save the code in a script and run Rscript foo.R, but
> I'm curious why Rscript stops when the -e argument is an empty string.

 There's some over-zealous argument parsing in the main R startup
shell script. You didn't try negative numbers, did you?

$ Rscript  -e "-1"
ERROR: option '-e' requires an argument
$ Rscript  -e "+1"
[1] 1

The R script strips anything starting with a - from the arg after -e
and throws that error if there's nothing left. The relevant lines are:

    -e)
      if test -n "`echo ${2} | ${SED} 's/^-.*//'`"; then
    a=`echo "${2}" | ${SED} -e 's/ /~+~/g'`; shift
      else
    error "option '${1}' requires an argument"
      fi

- now at least that's on Unix. The Windows problem looks like its
doing something else, and mashing some quotes up and passing 'e' as an
expression to one of the other -e options.

 Not sure what the fix might be, except maybe for the user to always
wrap their expressions in curly brackets...

Barry


From kevin.hendricks at sympatico.ca  Sun Mar 10 01:31:19 2013
From: kevin.hendricks at sympatico.ca (Kevin Hendricks)
Date: Sat, 9 Mar 2013 19:31:19 -0500
Subject: [Rd] question on why Rigroup package moved to Archive on CRAN
In-Reply-To: <CAF42j23teS7eq9Qj-CqqOTyUC-GKxvy5OLoRwG0Rkcw9VOf52w@mail.gmail.com>
References: <BLU0-SMTP1663AF8838B6E599BDF19787E70@phx.gbl>
	<CAFDcVCSpBzh0hm0_C=SfYTVekrZ7iSckuDg5wFwd5f2N0bGbSA@mail.gmail.com>
	<BLU0-SMTP30278138E54E7526966E2187E70@phx.gbl>
	<BLU0-SMTP472512ED1388C9E8DDB39B87E70@phx.gbl>
	<CAF42j23teS7eq9Qj-CqqOTyUC-GKxvy5OLoRwG0Rkcw9VOf52w@mail.gmail.com>
Message-ID: <BLU0-SMTP4049508119FC883031BA5387E00@phx.gbl>

Hi Dan,
Thank you!  I dig up a Mac at work and recreate this.

Kevin


On 2013-03-09, at 5:52 PM, Dan Tenenbaum <dtenenba at fhcrc.org> wrote:

> On Sat, Mar 9, 2013 at 2:17 PM, Kevin Hendricks
> <kevin.hendricks at sympatico.ca> wrote:
>> Hi,
>> One last quick question ... does anyone archive older CRAN package check summaries?  I searched the web but could not find any.  My package was archived in February of this year so any package check summary from earlier than that should help me figure out what is broken on MacOSX or Windows.
> 
> FWIW, it fails check on my Mac as follows:
> 
> * checking examples ... ERROR
> Running examples in 'Rigroup-Ex.R' failed
> The error most likely occurred in:
> 
>> ### Name: igroupAlls
>> ### Title: calculates logical All for small integer groups of logical
>> ###   vectors
>> ### Aliases: igroupAlls
>> ### Keywords: utilities
>> 
>> ### ** Examples
>> 
>> y <- rnorm(100)
>> i <- rep(1:25,4)
>> x <- (y > 1.0)
>> alls <- igroupAlls(x,i)
> Error in .External("igroupFuns", x, i, R_IGFMINS, na.rm, PACKAGE = "Rigroup") :
>  Incorrect number of arguments (4), expecting 1 for 'igroupFuns'
> Calls: igroupAlls -> .External
> Execution halted
> 
> 
> Dan
> 
> 
>> Thanks,
>> Kevin
>> 
>> 
>> Sent from my iPad
>> 
>> On 2013-03-09, at 4:38 PM, Kevin Hendricks <kevin.hendricks at sympatico.ca> wrote:
>> 
>>> Hi,
>>> Thanks for that info.  It works/builds without error or warnings on my RedHat Enterprise 6  with R 2.15.2 version which I use but must be broken somehow on later versions.  I will see if I can find the autobuild warnings error report someplace.
>>> 
>>> Kevin
>>> 
>>> 
>>> Sent from my iPad
>>> 
>>> On 2013-03-09, at 4:00 PM, Henrik Bengtsson <hb at biostat.ucsf.edu> wrote:
>>> 
>>>> On Sat, Mar 9, 2013 at 12:25 PM, Kevin Hendricks
>>>> <kevin.hendricks at sympatico.ca> wrote:
>>>>> Hi,
>>>>> 
>>>>> Who should I ask about my package Rigroup_0.83 being moved to Archive status on CRAN and no longer available via install.package?  I have no problems with the move if this was simply because of low demand.  However, if there was a build issue with the newest releases that caused problems, I would be happy to address it.  I'll just ask my students to install it from my own locally hosted source archive.
>>>> 
>>>> Most likely yourself; from the 'CRAN Repository Policy'
>>>> [http://cran.r-project.org/web/packages/policies.html]:
>>>> 
>>>> "Packages will not normally be removed from CRAN: however, they may be
>>>> archived, including at the maintainer's request.
>>>> 
>>>> Packages for which R CMD check gives an ?ERROR? when a new R x.y.0
>>>> version is released will be archived (or in exceptional circumstances
>>>> updated by the CRAN team) unless the maintainer has set a firm
>>>> deadline for an upcoming update (and keeps to it).
>>>> 
>>>> Maintainers will be asked to update packages which show any warnings
>>>> or significant notes, especially at around the time of a new x.y.0
>>>> release. Packages which are not updated are liable to be archived."
>>>> 
>>>> /Henrik
>>>> 
>>>>> 
>>>>> Thanks,
>>>>> 
>>>>> Kevin
>>>>> ______________________________________________
>>>>> R-devel at r-project.org mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>> 
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>> 
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
> 


From kevin.hendricks at sympatico.ca  Sun Mar 10 02:43:36 2013
From: kevin.hendricks at sympatico.ca (Kevin Hendricks)
Date: Sat, 9 Mar 2013 20:43:36 -0500
Subject: [Rd] question on why Rigroup package moved to Archive on CRAN
In-Reply-To: <CAF42j23sooTLrLo_+werPDh25MCrMdxe7GrySi8EG=osOCH_wg@mail.gmail.com>
References: <BLU0-SMTP1663AF8838B6E599BDF19787E70@phx.gbl>
	<CAFDcVCSpBzh0hm0_C=SfYTVekrZ7iSckuDg5wFwd5f2N0bGbSA@mail.gmail.com>
	<BLU0-SMTP30278138E54E7526966E2187E70@phx.gbl>
	<BLU0-SMTP472512ED1388C9E8DDB39B87E70@phx.gbl>
	<CAF42j23teS7eq9Qj-CqqOTyUC-GKxvy5OLoRwG0Rkcw9VOf52w@mail.gmail.com>
	<BLU0-SMTP32DDDC2FAFD538C33A193287E00@phx.gbl>
	<CAF42j23sooTLrLo_+werPDh25MCrMdxe7GrySi8EG=osOCH_wg@mail.gmail.com>
Message-ID: <BLU0-SMTP89E4EB1F1CB967D93FF4CC87E00@phx.gbl>

Hi Dan,

In case this catches anyone else ... 

FWIW, I found the issue ...  in my Rinit.c, my package uses the .External call which actually takes one SEXP which points to a "varargs-like" list.

Under 2.15.X and earlier, I thought the proper entry for an .External call was as below since it only does take one pointer as an argument:

#include "Rigroup.h"

/* Automate using sed or something. */
#if _MSC_VER >= 1000
__declspec(dllexport)
#endif

   static const R_ExternalMethodDef R_ExtDef[] = {
	{"igroupFuns", (DL_FUNC)&igroupFuns, 1},
	{NULL, NULL, 0},
   };

void R_init_Rigroup(DllInfo *info)
{
 R_registerRoutines(info,NULL,NULL,NULL,R_ExtDef);
}


But now according to the latest online docs on building your own package it says:

"For routines with a variable number of arguments invoked viathe .External interface, one specifies -1 for the number of arguments which tells R not to check the actual number passed. Note that the number of arguments passed to .External are not currently checked but they will be in R 3.0.0."

So I need to change my Rinit.c to change the "1" to a "-1" and that error should go away.

Thanks again for all your help with this.  I will update my package and resubmit it once version 3.0 gets released and I get a chance to verify that this does in fact fix the problem.

Kevin


From ligges at statistik.tu-dortmund.de  Sun Mar 10 16:18:30 2013
From: ligges at statistik.tu-dortmund.de (Uwe Ligges)
Date: Sun, 10 Mar 2013 16:18:30 +0100
Subject: [Rd] question on why Rigroup package moved to Archive on CRAN
In-Reply-To: <BLU0-SMTP89E4EB1F1CB967D93FF4CC87E00@phx.gbl>
References: <BLU0-SMTP1663AF8838B6E599BDF19787E70@phx.gbl>
	<CAFDcVCSpBzh0hm0_C=SfYTVekrZ7iSckuDg5wFwd5f2N0bGbSA@mail.gmail.com>
	<BLU0-SMTP30278138E54E7526966E2187E70@phx.gbl>
	<BLU0-SMTP472512ED1388C9E8DDB39B87E70@phx.gbl>
	<CAF42j23teS7eq9Qj-CqqOTyUC-GKxvy5OLoRwG0Rkcw9VOf52w@mail.gmail.com>
	<BLU0-SMTP32DDDC2FAFD538C33A193287E00@phx.gbl>
	<CAF42j23sooTLrLo_+werPDh25MCrMdxe7GrySi8EG=osOCH_wg@mail.gmail.com>
	<BLU0-SMTP89E4EB1F1CB967D93FF4CC87E00@phx.gbl>
Message-ID: <513CA446.8040005@statistik.tu-dortmund.de>

I wonder why you do not ask on CRAN at ...? List members here cannot know 
the answer. And we typically do not discuss such matters in public.

I wonder why you do not read the e-mail message you get from the CRAN team?

Please see the message with subject line "Registering .External entry 
points" you got on January 20. You never answered nor fixed the package, 
hence the package has been archived.

Best,
Uwe Ligges




On 10.03.2013 02:43, Kevin Hendricks wrote:
> Hi Dan,
>
> In case this catches anyone else ...
>
> FWIW, I found the issue ...  in my Rinit.c, my package uses the .External call which actually takes one SEXP which points to a "varargs-like" list.
>
> Under 2.15.X and earlier, I thought the proper entry for an .External call was as below since it only does take one pointer as an argument:
>
> #include "Rigroup.h"
>
> /* Automate using sed or something. */
> #if _MSC_VER >= 1000
> __declspec(dllexport)
> #endif
>
>     static const R_ExternalMethodDef R_ExtDef[] = {
> 	{"igroupFuns", (DL_FUNC)&igroupFuns, 1},
> 	{NULL, NULL, 0},
>     };
>
> void R_init_Rigroup(DllInfo *info)
> {
>   R_registerRoutines(info,NULL,NULL,NULL,R_ExtDef);
> }
>
>
> But now according to the latest online docs on building your own package it says:
>
> "For routines with a variable number of arguments invoked viathe .External interface, one specifies -1 for the number of arguments which tells R not to check the actual number passed. Note that the number of arguments passed to .External are not currently checked but they will be in R 3.0.0."
>
> So I need to change my Rinit.c to change the "1" to a "-1" and that error should go away.
>
> Thanks again for all your help with this.  I will update my package and resubmit it once version 3.0 gets released and I get a chance to verify that this does in fact fix the problem.
>
> Kevin
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From jari.oksanen at oulu.fi  Sun Mar 10 16:51:14 2013
From: jari.oksanen at oulu.fi (Jari Oksanen)
Date: Sun, 10 Mar 2013 15:51:14 +0000
Subject: [Rd] question on why Rigroup package moved to Archive on CRAN
In-Reply-To: <513CA446.8040005@statistik.tu-dortmund.de>
References: <BLU0-SMTP1663AF8838B6E599BDF19787E70@phx.gbl>
	<CAFDcVCSpBzh0hm0_C=SfYTVekrZ7iSckuDg5wFwd5f2N0bGbSA@mail.gmail.com>
	<BLU0-SMTP30278138E54E7526966E2187E70@phx.gbl>
	<BLU0-SMTP472512ED1388C9E8DDB39B87E70@phx.gbl>
	<CAF42j23teS7eq9Qj-CqqOTyUC-GKxvy5OLoRwG0Rkcw9VOf52w@mail.gmail.com>
	<BLU0-SMTP32DDDC2FAFD538C33A193287E00@phx.gbl>
	<CAF42j23sooTLrLo_+werPDh25MCrMdxe7GrySi8EG=osOCH_wg@mail.gmail.com>
	<BLU0-SMTP89E4EB1F1CB967D93FF4CC87E00@phx.gbl>
	<513CA446.8040005@statistik.tu-dortmund.de>
Message-ID: <A99D2247-240A-4AD3-B54E-47E144D47C56@oulu.fi>

"What we've got here is failure to communicate. Some men you just can't reach. So you get what we had here last week, which is the way he wants it. Well, he gets it. I don't like it any more than you men." (from "Cool hand Luke" -- but whose fault?)

Cheers, Jari Oksanen

On 10/03/2013, at 17:18 PM, Uwe Ligges wrote:

> I wonder why you do not ask on CRAN at ...? List members here cannot know the answer. And we typically do not discuss such matters in public.
> 
> I wonder why you do not read the e-mail message you get from the CRAN team?
> 
> Please see the message with subject line "Registering .External entry points" you got on January 20. You never answered nor fixed the package, hence the package has been archived.
> 
> Best,
> Uwe Ligges
> 
> 
> 
> 
> On 10.03.2013 02:43, Kevin Hendricks wrote:
>> Hi Dan,
>> 
>> In case this catches anyone else ...
>> 
>> FWIW, I found the issue ...  in my Rinit.c, my package uses the .External call which actually takes one SEXP which points to a "varargs-like" list.
>> 
>> Under 2.15.X and earlier, I thought the proper entry for an .External call was as below since it only does take one pointer as an argument:
>> 
>> #include "Rigroup.h"
>> 
>> /* Automate using sed or something. */
>> #if _MSC_VER >= 1000
>> __declspec(dllexport)
>> #endif
>> 
>>    static const R_ExternalMethodDef R_ExtDef[] = {
>> 	{"igroupFuns", (DL_FUNC)&igroupFuns, 1},
>> 	{NULL, NULL, 0},
>>    };
>> 
>> void R_init_Rigroup(DllInfo *info)
>> {
>>  R_registerRoutines(info,NULL,NULL,NULL,R_ExtDef);
>> }
>> 
>> 
>> But now according to the latest online docs on building your own package it says:
>> 
>> "For routines with a variable number of arguments invoked viathe .External interface, one specifies -1 for the number of arguments which tells R not to check the actual number passed. Note that the number of arguments passed to .External are not currently checked but they will be in R 3.0.0."
>> 
>> So I need to change my Rinit.c to change the "1" to a "-1" and that error should go away.
>> 
>> Thanks again for all your help with this.  I will update my package and resubmit it once version 3.0 gets released and I get a chance to verify that this does in fact fix the problem.
>> 
>> Kevin
>> 
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>> 
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
Jari Oksanen, Dept Biology, Univ Oulu, 90014 Finland
jari.oksanen at oulu.fi, Ph. +358 400 408593, http://cc.oulu.fi/~jarioksa


From kevin.hendricks at sympatico.ca  Sun Mar 10 16:54:10 2013
From: kevin.hendricks at sympatico.ca (Kevin Hendricks)
Date: Sun, 10 Mar 2013 11:54:10 -0400
Subject: [Rd] question on why Rigroup package moved to Archive on CRAN
In-Reply-To: <513CA446.8040005@statistik.tu-dortmund.de>
References: <BLU0-SMTP1663AF8838B6E599BDF19787E70@phx.gbl>
	<CAFDcVCSpBzh0hm0_C=SfYTVekrZ7iSckuDg5wFwd5f2N0bGbSA@mail.gmail.com>
	<BLU0-SMTP30278138E54E7526966E2187E70@phx.gbl>
	<BLU0-SMTP472512ED1388C9E8DDB39B87E70@phx.gbl>
	<CAF42j23teS7eq9Qj-CqqOTyUC-GKxvy5OLoRwG0Rkcw9VOf52w@mail.gmail.com>
	<BLU0-SMTP32DDDC2FAFD538C33A193287E00@phx.gbl>
	<CAF42j23sooTLrLo_+werPDh25MCrMdxe7GrySi8EG=osOCH_wg@mail.gmail.com>
	<BLU0-SMTP89E4EB1F1CB967D93FF4CC87E00@phx.gbl>
	<513CA446.8040005@statistik.tu-dortmund.de>
Message-ID: <BLU0-SMTP891BCC4E88285CE2DC5B1087E00@phx.gbl>

Sorry if you considered this a waste of bandwidth.  I did not know CRAN had its own mailing list.  The reason I never responded to any mail is that I never received any message in January  (I searched my inbox for it and found nothing).   It probably was stripped out by the sympatico mail server as spam.    

The reason I asked here was that my package seemed to pass all tests on the current version and I was honestly confused as to why it was removed.   In addition, I thought others in a similar situation may benefit.   Years ago people here were quite helpful when I first designed the package (and from the responses many still are).  My question was not meant as an attack on you or your processes, it was honest confusion as to why it was removed.
 
Kevin

On 2013-03-10, at 11:18 AM, Uwe Ligges <ligges at statistik.tu-dortmund.de> wrote:

> I wonder why you do not ask on CRAN at ...? List members here cannot know the answer. And we typically do not discuss such matters in public.
> 
> I wonder why you do not read the e-mail message you get from the CRAN team?
> 
> Please see the message with subject line "Registering .External entry points" you got on January 20. You never answered nor fixed the package, hence the package has been archived.
> 
> Best,
> Uwe Ligges
> 
> 
> 
> 
> On 10.03.2013 02:43, Kevin Hendricks wrote:
>> Hi Dan,
>> 
>> In case this catches anyone else ...
>> 
>> FWIW, I found the issue ...  in my Rinit.c, my package uses the .External call which actually takes one SEXP which points to a "varargs-like" list.
>> 
>> Under 2.15.X and earlier, I thought the proper entry for an .External call was as below since it only does take one pointer as an argument:
>> 
>> #include "Rigroup.h"
>> 
>> /* Automate using sed or something. */
>> #if _MSC_VER >= 1000
>> __declspec(dllexport)
>> #endif
>> 
>>    static const R_ExternalMethodDef R_ExtDef[] = {
>> 	{"igroupFuns", (DL_FUNC)&igroupFuns, 1},
>> 	{NULL, NULL, 0},
>>    };
>> 
>> void R_init_Rigroup(DllInfo *info)
>> {
>>  R_registerRoutines(info,NULL,NULL,NULL,R_ExtDef);
>> }
>> 
>> 
>> But now according to the latest online docs on building your own package it says:
>> 
>> "For routines with a variable number of arguments invoked viathe .External interface, one specifies -1 for the number of arguments which tells R not to check the actual number passed. Note that the number of arguments passed to .External are not currently checked but they will be in R 3.0.0."
>> 
>> So I need to change my Rinit.c to change the "1" to a "-1" and that error should go away.
>> 
>> Thanks again for all your help with this.  I will update my package and resubmit it once version 3.0 gets released and I get a chance to verify that this does in fact fix the problem.
>> 
>> Kevin
>> 
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>> 
> 


From ligges at statistik.tu-dortmund.de  Sun Mar 10 17:00:22 2013
From: ligges at statistik.tu-dortmund.de (Uwe Ligges)
Date: Sun, 10 Mar 2013 17:00:22 +0100
Subject: [Rd] question on why Rigroup package moved to Archive on CRAN
In-Reply-To: <BLU0-SMTP891BCC4E88285CE2DC5B1087E00@phx.gbl>
References: <BLU0-SMTP1663AF8838B6E599BDF19787E70@phx.gbl>
	<CAFDcVCSpBzh0hm0_C=SfYTVekrZ7iSckuDg5wFwd5f2N0bGbSA@mail.gmail.com>
	<BLU0-SMTP30278138E54E7526966E2187E70@phx.gbl>
	<BLU0-SMTP472512ED1388C9E8DDB39B87E70@phx.gbl>
	<CAF42j23teS7eq9Qj-CqqOTyUC-GKxvy5OLoRwG0Rkcw9VOf52w@mail.gmail.com>
	<BLU0-SMTP32DDDC2FAFD538C33A193287E00@phx.gbl>
	<CAF42j23sooTLrLo_+werPDh25MCrMdxe7GrySi8EG=osOCH_wg@mail.gmail.com>
	<BLU0-SMTP89E4EB1F1CB967D93FF4CC87E00@phx.gbl>
	<513CA446.8040005@statistik.tu-dortmund.de>
	<BLU0-SMTP891BCC4E88285CE2DC5B1087E00@phx.gbl>
Message-ID: <513CAE16.2050109@statistik.tu-dortmund.de>

OK, the original message will be resend privately in a minute.

Best,
Uwe Ligges




On 10.03.2013 16:54, Kevin Hendricks wrote:
> Sorry if you considered this a waste of bandwidth.  I did not know CRAN had its own mailing list.  The reason I never responded to any mail is that I never received any message in January  (I searched my inbox for it and found nothing).   It probably was stripped out by the sympatico mail server as spam.
>
> The reason I asked here was that my package seemed to pass all tests on the current version and I was honestly confused as to why it was removed.   In addition, I thought others in a similar situation may benefit.   Years ago people here were quite helpful when I first designed the package (and from the responses many still are).  My question was not meant as an attack on you or your processes, it was honest confusion as to why it was removed.
>
> Kevin
>
> On 2013-03-10, at 11:18 AM, Uwe Ligges <ligges at statistik.tu-dortmund.de> wrote:
>
>> I wonder why you do not ask on CRAN at ...? List members here cannot know the answer. And we typically do not discuss such matters in public.
>>
>> I wonder why you do not read the e-mail message you get from the CRAN team?
>>
>> Please see the message with subject line "Registering .External entry points" you got on January 20. You never answered nor fixed the package, hence the package has been archived.
>>
>> Best,
>> Uwe Ligges
>>
>>
>>
>>
>> On 10.03.2013 02:43, Kevin Hendricks wrote:
>>> Hi Dan,
>>>
>>> In case this catches anyone else ...
>>>
>>> FWIW, I found the issue ...  in my Rinit.c, my package uses the .External call which actually takes one SEXP which points to a "varargs-like" list.
>>>
>>> Under 2.15.X and earlier, I thought the proper entry for an .External call was as below since it only does take one pointer as an argument:
>>>
>>> #include "Rigroup.h"
>>>
>>> /* Automate using sed or something. */
>>> #if _MSC_VER >= 1000
>>> __declspec(dllexport)
>>> #endif
>>>
>>>     static const R_ExternalMethodDef R_ExtDef[] = {
>>> 	{"igroupFuns", (DL_FUNC)&igroupFuns, 1},
>>> 	{NULL, NULL, 0},
>>>     };
>>>
>>> void R_init_Rigroup(DllInfo *info)
>>> {
>>>   R_registerRoutines(info,NULL,NULL,NULL,R_ExtDef);
>>> }
>>>
>>>
>>> But now according to the latest online docs on building your own package it says:
>>>
>>> "For routines with a variable number of arguments invoked viathe .External interface, one specifies -1 for the number of arguments which tells R not to check the actual number passed. Note that the number of arguments passed to .External are not currently checked but they will be in R 3.0.0."
>>>
>>> So I need to change my Rinit.c to change the "1" to a "-1" and that error should go away.
>>>
>>> Thanks again for all your help with this.  I will update my package and resubmit it once version 3.0 gets released and I get a chance to verify that this does in fact fix the problem.
>>>
>>> Kevin
>>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>
>>
>


From madjakarta at gmail.com  Mon Mar 11 10:06:18 2013
From: madjakarta at gmail.com (=?EUC-KR?B?wPzI8b/4?=)
Date: Mon, 11 Mar 2013 18:06:18 +0900
Subject: [Rd] "undefined symbol" when `R CMD check'.
Message-ID: <CAKN9kqUP6eAy6-fq0Dag8SGqECfWgAKSb17xyPH-LotJD=uvXQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20130311/bb468302/attachment.pl>

From edd at debian.org  Mon Mar 11 13:11:25 2013
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon, 11 Mar 2013 07:11:25 -0500
Subject: [Rd] "undefined symbol" when `R CMD check'.
In-Reply-To: <CAKN9kqUP6eAy6-fq0Dag8SGqECfWgAKSb17xyPH-LotJD=uvXQ@mail.gmail.com>
References: <CAKN9kqUP6eAy6-fq0Dag8SGqECfWgAKSb17xyPH-LotJD=uvXQ@mail.gmail.com>
Message-ID: <20797.51693.655539.67552@max.nulle.part>


On 11 March 2013 at 18:06, ????????? wrote:
| Hi! All.
| 
| I want to make R package with "http://code.google.com/p/uchardet/" library.
| 
| But I encountered error.
| 
| Executable file with -lchardet works well but shared library didn't work
| with dyn.load() with Rcpp.

This is the wrong list. We support Rcpp on the rcpp-devel list at R-Forge.
Subscribe before posting, or use something like gmane.org to post to it.
 
| Can any one give some tips?
| 
| 
| * installing *source* package ?Ruchardet? ..
| ** libs
| g++ -I/usr/share/R/include -DNDEBUG -I/usr/local/include -I/usr/include
| -I"/home/gogamza/R/x86_64-pc-linux-gnu-library/2.15/Rcpp/include"   -fpic
| -O2 -pipe -g  -c getCharEncoding.cpp -o getCharEncoding.o
| g++ -shared -o Ruchardet.so getCharEncoding.o
| -L/home/gogamza/R/x86_64-pc-linux-gnu-library/2.15/Rcpp/lib -lRcpp
| -Wl,-rpath,/home/gogamza/R/x86_64-  pc-linux-gnu-library/2.15/Rcpp/lib
| -L/usr/local/lib -L/usr/lib/ -lstdc++  -luchardet -L/usr/lib/R/lib -lR
| installing to /home/gogamza/work/Ruchardet.Rcheck/Ruchardet/libs
| ** R
| ** preparing package for lazy loading
| ** help
| *** installing help indices
| ** building package indices
| ** testing if installed package can be loaded
| Error in dyn.load(file, DLLpath = DLLpath, ...) :
|   unable to load shared object
| '/home/gogamza/work/Ruchardet.Rcheck/Ruchardet/libs/Ruchardet.so':
|   /home/gogamza/work/Ruchardet.Rcheck/Ruchardet/libs/Ruchardet.so:
| undefined symbol: uchardet_get_charset

That looks correct in principle. But your link step failed. Maybe the name of
the library is not libuchardet, maybe it was not found, maybe it does not
have the 'uchardet_get_charset' symbol you used, or maybe you used it with
the wrong argument signature.

We cannot tell as your example was not complete and hence not reproducible.

Dirk

| Error: loading failed
| Execution halted
| ERROR: loading failed
| * removing ?/home/gogamza/work/Ruchardet.Rcheck/Ruchardet
| 
| 
| 
| My sessionInfo().
| 
| R > sessionInfo()R version 2.15.3 (2013-03-01)
| Platform: x86_64-pc-linux-gnu (64-bit)
| 
| locale:
|  [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C         LC_TIME=C
| LC_COLLATE=C
|  [5] LC_MONETARY=C        LC_MESSAGES=C        LC_PAPER=C
| LC_NAME=C
|  [9] LC_ADDRESS=C         LC_TELEPHONE=C       LC_MEASUREMENT=C
| LC_IDENTIFICATION=C
| 
| attached base packages:
| [1] stats     graphics  grDevices utils     datasets  methods   base
| 
| other attached packages:
| [1] Rcpp_0.10.2
| 
| loaded via a namespace (and not attached):
| [1] tools_2.15.3
| 
| Thanks.
| 
| Heewon
| 
| 	[[alternative HTML version deleted]]
| 
| 
| ----------------------------------------------------------------------
| ______________________________________________
| R-devel at r-project.org mailing list
| https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
Dirk Eddelbuettel | edd at debian.org | http://dirk.eddelbuettel.com  


From khoran at cs.ucr.edu  Mon Mar 11 22:20:24 2013
From: khoran at cs.ucr.edu (Kevin Horan)
Date: Mon, 11 Mar 2013 14:20:24 -0700
Subject: [Rd] compiling C code using headers from another R package
Message-ID: <513E4A98.9080503@cs.ucr.edu>


I am developing an R package, eiR,  which depends on another C library, 
GNU scientific library (GSL). In order to make life easier for the user, 
it would be nice to not have this as an external dependency, thus I 
would like to wrap this library in another R package, say GSLR for 
example. Thus far I know how to do this. The C code in eiR requires the 
.so library and the header files from GSL in order to compile. So the 
idea is that eiR would depend on GSLR, then GSLR gets compiled and 
installed first, then, while eiR is installing, it should be able to 
make use of the GSL library and header files while compiling. So my 
question is, how do I know where the GSL library and header files, 
packaged in GSLR, would live so I can point the compiler at them? I know 
how to find the installed directory of an R package from within R, but 
is there way to find that out using just Makevars or a Makefile? I'm 
open to suggestions about a better way organize all of this as well. I 
like the idea of keeping the GSL code separate so that it can be 
updated/changed independently from eiR though.
     I'm also aware of the gsl R library on CRAN, however, this just 
wraps GSL in R functions, but I need to use the GSL C functions in other 
C code in eiR.

Thanks.

Kevin


From radford at cs.toronto.edu  Tue Mar 12 11:59:07 2013
From: radford at cs.toronto.edu (Radford Neal)
Date: Tue, 12 Mar 2013 06:59:07 -0400
Subject: [Rd] Bugs due to naive copying of list elements
Message-ID: <20130312105907.GA7498@cs.toronto.edu>

Several bugs are present in R-2.15.3 and R-alpha due to
naive copying of list elements.

The bug below is due to naive copying in subset.c (with
similar bugs for matrices and arrays):

a<-list(c(1,2),c(3,4),c(5,6))
b<-a[2:3]
a[[2]][2]<-9
print(b[[1]][2])

Naive copying in mapply.c leads to the following bug:

X<-1+1
f<-function(a,b) X
A<-mapply(f,c(1,2,3),c(4,5,6),SIMPLIFY=FALSE)
print(A)
X[1]<-99
print(A)

Similar bugs exist in eapply, rapply, and vapply.


From simon.urbanek at r-project.org  Tue Mar 12 13:26:24 2013
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Tue, 12 Mar 2013 08:26:24 -0400
Subject: [Rd] compiling C code using headers from another R package
In-Reply-To: <513E4A98.9080503@cs.ucr.edu>
References: <513E4A98.9080503@cs.ucr.edu>
Message-ID: <B54A5D4C-2578-4739-BEB5-6B17FA6A9856@r-project.org>

Kevin,

On Mar 11, 2013, at 5:20 PM, Kevin Horan wrote:

> 
> I am developing an R package, eiR,  which depends on another C library, GNU scientific library (GSL). In order to make life easier for the user, it would be nice to not have this as an external dependency, thus I would like to wrap this library in another R package, say GSLR for example. Thus far I know how to do this. The C code in eiR requires the .so library and the header files from GSL in order to compile. So the idea is that eiR would depend on GSLR, then GSLR gets compiled and installed first, then, while eiR is installing, it should be able to make use of the GSL library and header files while compiling. So my question is, how do I know where the GSL library and header files, packaged in GSLR, would live so I can point the compiler at them? I know how to find the installed directory of an R package from within R, but is there way to find that out using just Makevars or a Makefile? I'm open to suggestions about a better way organize all of this as well. I like the idea of keeping the GSL code separate so that it can be updated/changed independently from eiR though.

Have a look at Rcpp.


>    I'm also aware of the gsl R library on CRAN, however, this just wraps GSL in R functions, but I need to use the GSL C functions in other C code in eiR.
> 

Why is what you are proposing any better than simply using GSL in eiR? You will still need the GSL external dependency for GSLR and you are only adding a lot of complexity by linking into another package's external directory (you cannot use libs) which is in itself very tricky (you'll have to deal with both static and shared version, multi-arch setups, possible relocation etc.). It won't make it any easier on the user, rather to the contrary as there will be more things to break. The only reason Rcpp goes into such length to do this is because it has no choice (the Rcpp library has to use the same libR so cannot be used as external dependency) - I would certainly not recommend it for something as trivial as providing GSL.

Cheers,
Simon


> Thanks.
> 
> Kevin
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 
> 


From kasperdanielhansen at gmail.com  Tue Mar 12 16:00:12 2013
From: kasperdanielhansen at gmail.com (Kasper Daniel Hansen)
Date: Tue, 12 Mar 2013 11:00:12 -0400
Subject: [Rd] wishlist: ObjectFiles
Message-ID: <CAC2h7uvCyG+XNH9xB1pi1DdiBDk8NGtRtgsVex--5WvnOVecXA@mail.gmail.com>

It is possible to list binaries in BinaryFiles and thereby excluded
them from R CMD check (although they are disallowed by CRAN).

I am interested in the same functionality, but for object files.
Background: in Rgraphviz, we (I) include pre-compiled object files for
use on Windows, because generating these object files requires a full
unix environment, and not just the pieces exposed in Rtools.  I am
interested in excluding these files from R CMD check's check of
whether "stray" object files are included.  I imagine this could be
useful for other packages.

Best,
Kasper D Hansen


From edd at debian.org  Tue Mar 12 16:14:39 2013
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue, 12 Mar 2013 10:14:39 -0500
Subject: [Rd] compiling C code using headers from another R package
In-Reply-To: <B54A5D4C-2578-4739-BEB5-6B17FA6A9856@r-project.org>
References: <513E4A98.9080503@cs.ucr.edu>
	<B54A5D4C-2578-4739-BEB5-6B17FA6A9856@r-project.org>
Message-ID: <20799.18015.205188.372286@max.nulle.part>


On 12 March 2013 at 08:26, Simon Urbanek wrote:
| Kevin,
| 
| On Mar 11, 2013, at 5:20 PM, Kevin Horan wrote:
| 
| > 
| > I am developing an R package, eiR,  which depends on another C library, GNU scientific library (GSL). In order to make life easier for the user, it would be nice to not have this as an external dependency, thus I would like to wrap this library in another R package, say GSLR for example. Thus far I know how to do this. The C code in eiR requires the .so library and the header files from GSL in order to compile. So the idea is that eiR would depend on GSLR, then GSLR gets compiled and installed first, then, while eiR is installing, it should be able to make use of the GSL library and header files while compiling. So my question is, how do I know where the GSL library and header files, packaged in GSLR, would live so I can point the compiler at them? I know how to find the installed directory of an R package from within R, but is there way to find that out using just Makevars or a Makefile? I'm open to suggestions about a better way organize all of this as well. I like the i!
|  dea of keeping the GSL code separate so that it can be updated/changed independently from eiR though.
| 
| Have a look at Rcpp.

And particularly the RcppGSL package which even includes a complete working
example calling a single GSL function, a vector norm.

Users still need an external GSL to link (on Unix and OS X, om Windows CRAN
has this covered as I recall). 
 
Dirk

| >    I'm also aware of the gsl R library on CRAN, however, this just wraps GSL in R functions, but I need to use the GSL C functions in other C code in eiR.
| > 
| 
| Why is what you are proposing any better than simply using GSL in eiR? You will still need the GSL external dependency for GSLR and you are only adding a lot of complexity by linking into another package's external directory (you cannot use libs) which is in itself very tricky (you'll have to deal with both static and shared version, multi-arch setups, possible relocation etc.). It won't make it any easier on the user, rather to the contrary as there will be more things to break. The only reason Rcpp goes into such length to do this is because it has no choice (the Rcpp library has to use the same libR so cannot be used as external dependency) - I would certainly not recommend it for something as trivial as providing GSL.
| 
| Cheers,
| Simon
| 
| 
| > Thanks.
| > 
| > Kevin
| > 
| > ______________________________________________
| > R-devel at r-project.org mailing list
| > https://stat.ethz.ch/mailman/listinfo/r-devel
| > 
| > 
| 
| ______________________________________________
| R-devel at r-project.org mailing list
| https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
Dirk Eddelbuettel | edd at debian.org | http://dirk.eddelbuettel.com


From simon.urbanek at r-project.org  Tue Mar 12 17:55:32 2013
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Tue, 12 Mar 2013 12:55:32 -0400
Subject: [Rd] compiling C code using headers from another R package
In-Reply-To: <513F582B.9090601@cs.ucr.edu>
References: <513E4A98.9080503@cs.ucr.edu>
	<B54A5D4C-2578-4739-BEB5-6B17FA6A9856@r-project.org>
	<513F582B.9090601@cs.ucr.edu>
Message-ID: <E43810D3-2AEC-4F07-B61A-03923D6D2FC3@r-project.org>


On Mar 12, 2013, at 12:30 PM, Kevin Horan wrote:

> 
>    Thanks for your input. To clarify, I don't need to use any part of GSL in my R code, nor do I wish to make any part of it accessible to users of eiR. I need it to compile other C/C++ code (LSH KIT), which I did not write, that will itself be used in eiR.
>    My goal is allow the user to install eiR without also having to install GSL before hand.

If your package is on CRAN they won't need to as we are providing Mac and Windows binaries. Linux can get the binaries form their distro, so the dependencies are installed automatically.


> The target audience is people in bioinformatics who may not how to install something like GSL. It seems like what I was suggesting is not such a good idea, if it will be hard to reliably find the header files from another R package. I could also push all of GSL into eiR, but as GSL has over 5000 files, this makes the package very large ( >22 MB) and  slow to compile. Both of which are a problem when submitting a package to bioconductor. It may very well be that leaving GSL as an external dependency to eiR is really the best and easiest way, but I just wanted to see if there was any way to make it easier for the user.

Can you clarify what you mean by "user"? The vast majority of R users use binaries, so all this is irrelevant to them as they don't need to install GSL at all.

Cheers,
Simon


> So, any other suggestions about how this could be accomplished?  Thanks.
> 
> Kevin
> 
> On 03/12/2013 05:26 AM, Simon Urbanek wrote:
>> Kevin,
>> 
>> On Mar 11, 2013, at 5:20 PM, Kevin Horan wrote:
>> 
>>> I am developing an R package, eiR,  which depends on another C library, GNU scientific library (GSL). In order to make life easier for the user, it would be nice to not have this as an external dependency, thus I would like to wrap this library in another R package, say GSLR for example. Thus far I know how to do this. The C code in eiR requires the .so library and the header files from GSL in order to compile. So the idea is that eiR would depend on GSLR, then GSLR gets compiled and installed first, then, while eiR is installing, it should be able to make use of the GSL library and header files while compiling. So my question is, how do I know where the GSL library and header files, packaged in GSLR, would live so I can point the compiler at them? I know how to find the installed directory of an R package from within R, but is there way to find that out using just Makevars or a Makefile? I'm open to suggestions about a better way organize all of this as well. I like the idea of keeping the GSL code separate so that it can be updated/changed independently from eiR though.
>> Have a look at Rcpp.
>> 
>> 
>>>    I'm also aware of the gsl R library on CRAN, however, this just wraps GSL in R functions, but I need to use the GSL C functions in other C code in eiR.
>>> 
>> Why is what you are proposing any better than simply using GSL in eiR? You will still need the GSL external dependency for GSLR and you are only adding a lot of complexity by linking into another package's external directory (you cannot use libs) which is in itself very tricky (you'll have to deal with both static and shared version, multi-arch setups, possible relocation etc.). It won't make it any easier on the user, rather to the contrary as there will be more things to break. The only reason Rcpp goes into such length to do this is because it has no choice (the Rcpp library has to use the same libR so cannot be used as external dependency) - I would certainly not recommend it for something as trivial as providing GSL.
>> 
>> Cheers,
>> Simon
>> 
>> 
>>> Thanks.
>>> 
>>> Kevin
>>> 
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>> 
>>> 
> 
> 


From dwinsemius at comcast.net  Tue Mar 12 18:28:44 2013
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 12 Mar 2013 10:28:44 -0700
Subject: [Rd] Bugs due to naive copying of list elements
In-Reply-To: <20130312105907.GA7498@cs.toronto.edu>
References: <20130312105907.GA7498@cs.toronto.edu>
Message-ID: <71C77A69-728D-47B8-A331-BC2111C8EA3C@comcast.net>


On Mar 12, 2013, at 3:59 AM, Radford Neal wrote:

> Several bugs are present in R-2.15.3 and R-alpha due to
> naive copying of list elements.
> 
> The bug below is due to naive copying in subset.c (with
> similar bugs for matrices and arrays):
> 
> a<-list(c(1,2),c(3,4),c(5,6))
> b<-a[2:3]
> a[[2]][2]<-9
> print(b[[1]][2])

This is an example of lazy evaluation, right?

> 
> Naive copying in mapply.c leads to the following bug:
> 
> X<-1+1
> f<-function(a,b) X
> A<-mapply(f,c(1,2,3),c(4,5,6),SIMPLIFY=FALSE)
> print(A)
> X[1]<-99
> print(A)
> 

Is this a bug in mapply()? or in print()? I thought 'print' should evaluate its argument and "force" the promise to be executed. Or does it just return the same promise as was passed to it? Compare:

X<-1+1
f<-function(a,b) X
A<-mapply(f,c(1,2,3),c(4,5,6),SIMPLIFY=FALSE)
print(A); str(A)
X[1]<-99
print(A)

Could someone could comment on what 'force' actually does. I am unclear why force(A) in the code above in the pace of str(A) did not have the effect I expected, whereas str(b) or str(A) did have that effect.

> a<-list(c(1,2),c(3,4),c(5,6))
> b<-a[2:3]; force(b)
[[1]]
[1] 3 4

[[2]]
[1] 5 6

> a[[2]][2]<-9
> print(b[[1]][2])
[1] 9

#----------
> X<-1+1
> f<-function(a,b) X
> A<-mapply(f,c(1,2,3),c(4,5,6),SIMPLIFY=FALSE)
> print(A); force(A)
[[1]]
[1] 2

[[2]]
[1] 2

[[3]]
[1] 2

[[1]]
[1] 2

[[2]]
[1] 2

[[3]]
[1] 2

> X[1]<-99
> print(A)
[[1]]
[1] 99

[[2]]
[1] 99

[[3]]
[1] 99


> Similar bugs exist in eapply, rapply, and vapply.
> 

-- 
David Winsemius
Alameda, CA, USA


From hpages at fhcrc.org  Tue Mar 12 19:01:22 2013
From: hpages at fhcrc.org (=?ISO-8859-1?Q?Herv=E9_Pag=E8s?=)
Date: Tue, 12 Mar 2013 11:01:22 -0700
Subject: [Rd] compiling C code using headers from another R package
In-Reply-To: <E43810D3-2AEC-4F07-B61A-03923D6D2FC3@r-project.org>
References: <513E4A98.9080503@cs.ucr.edu>
	<B54A5D4C-2578-4739-BEB5-6B17FA6A9856@r-project.org>
	<513F582B.9090601@cs.ucr.edu>
	<E43810D3-2AEC-4F07-B61A-03923D6D2FC3@r-project.org>
Message-ID: <513F6D72.3040304@fhcrc.org>

Hi,

On 03/12/2013 09:55 AM, Simon Urbanek wrote:
>
> On Mar 12, 2013, at 12:30 PM, Kevin Horan wrote:
>
>>
>>     Thanks for your input. To clarify, I don't need to use any part of GSL in my R code, nor do I wish to make any part of it accessible to users of eiR. I need it to compile other C/C++ code (LSH KIT), which I did not write, that will itself be used in eiR.
>>     My goal is allow the user to install eiR without also having to install GSL before hand.
>
> If your package is on CRAN they won't need to as we are providing Mac and Windows binaries.

I think that at least on Windows, the user would still need to have the
GSL installed on his/her machine.


> Linux can get the binaries form their distro, so the dependencies are installed automatically.
>
>
>> The target audience is people in bioinformatics who may not how to install something like GSL. It seems like what I was suggesting is not such a good idea, if it will be hard to reliably find the header files from another R package. I could also push all of GSL into eiR, but as GSL has over 5000 files, this makes the package very large ( >22 MB) and  slow to compile. Both of which are a problem when submitting a package to bioconductor. It may very well be that leaving GSL as an external dependency to eiR is really the best and easiest way, but I just wanted to see if there was any way to make it easier for the user.
>
> Can you clarify what you mean by "user"? The vast majority of R users use binaries, so all this is irrelevant to them as they don't need to install GSL at all.

FWIW we currently have 7 or 8 Bioconductor packages that require the
GSL as an external system lib. That's because even if we provide Windows
binaries for those packages, those binaries are dynamically linked.
Is there a way to build those binaries that would avoid that dependency?

Anyway, to answer Kevin's original question:

   how do I know where the GSL library and header files, packaged
   in GSLR, would live so I can point the compiler at them?

Use the LinkingTo field.

Cheers,
H.

>
> Cheers,
> Simon
>
>
>> So, any other suggestions about how this could be accomplished?  Thanks.
>>
>> Kevin
>>
>> On 03/12/2013 05:26 AM, Simon Urbanek wrote:
>>> Kevin,
>>>
>>> On Mar 11, 2013, at 5:20 PM, Kevin Horan wrote:
>>>
>>>> I am developing an R package, eiR,  which depends on another C library, GNU scientific library (GSL). In order to make life easier for the user, it would be nice to not have this as an external dependency, thus I would like to wrap this library in another R package, say GSLR for example. Thus far I know how to do this. The C code in eiR requires the .so library and the header files from GSL in order to compile. So the idea is that eiR would depend on GSLR, then GSLR gets compiled and installed first, then, while eiR is installing, it should be able to make use of the GSL library and header files while compiling. So my question is, how do I know where the GSL library and header files, packaged in GSLR, would live so I can point the compiler at them? I know how to find the installed directory of an R package from within R, but is there way to find that out using just Makevars or a Makefile? I'm open to suggestions about a better way organize all of this as well. I like the
 !
>    idea of keeping the GSL code separate so that it can be updated/changed independently from eiR though.
>>> Have a look at Rcpp.
>>>
>>>
>>>>     I'm also aware of the gsl R library on CRAN, however, this just wraps GSL in R functions, but I need to use the GSL C functions in other C code in eiR.
>>>>
>>> Why is what you are proposing any better than simply using GSL in eiR? You will still need the GSL external dependency for GSLR and you are only adding a lot of complexity by linking into another package's external directory (you cannot use libs) which is in itself very tricky (you'll have to deal with both static and shared version, multi-arch setups, possible relocation etc.). It won't make it any easier on the user, rather to the contrary as there will be more things to break. The only reason Rcpp goes into such length to do this is because it has no choice (the Rcpp library has to use the same libR so cannot be used as external dependency) - I would certainly not recommend it for something as trivial as providing GSL.
>>>
>>> Cheers,
>>> Simon
>>>
>>>
>>>> Thanks.
>>>>
>>>> Kevin
>>>>
>>>> ______________________________________________
>>>> R-devel at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>
>>>>
>>
>>
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fhcrc.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From simon.urbanek at r-project.org  Tue Mar 12 19:09:18 2013
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Tue, 12 Mar 2013 14:09:18 -0400
Subject: [Rd] compiling C code using headers from another R package
In-Reply-To: <513F6D72.3040304@fhcrc.org>
References: <513E4A98.9080503@cs.ucr.edu>
	<B54A5D4C-2578-4739-BEB5-6B17FA6A9856@r-project.org>
	<513F582B.9090601@cs.ucr.edu>
	<E43810D3-2AEC-4F07-B61A-03923D6D2FC3@r-project.org>
	<513F6D72.3040304@fhcrc.org>
Message-ID: <5195A4FB-F14C-484C-A732-C98758D0D737@r-project.org>


On Mar 12, 2013, at 2:01 PM, Herv? Pag?s wrote:

> Hi,
> 
> On 03/12/2013 09:55 AM, Simon Urbanek wrote:
>> 
>> On Mar 12, 2013, at 12:30 PM, Kevin Horan wrote:
>> 
>>> 
>>>    Thanks for your input. To clarify, I don't need to use any part of GSL in my R code, nor do I wish to make any part of it accessible to users of eiR. I need it to compile other C/C++ code (LSH KIT), which I did not write, that will itself be used in eiR.
>>>    My goal is allow the user to install eiR without also having to install GSL before hand.
>> 
>> If your package is on CRAN they won't need to as we are providing Mac and Windows binaries.
> 
> I think that at least on Windows, the user would still need to have the
> GSL installed on his/her machine.
> 

Why?


> 
>> Linux can get the binaries form their distro, so the dependencies are installed automatically.
>> 
>> 
>>> The target audience is people in bioinformatics who may not how to install something like GSL. It seems like what I was suggesting is not such a good idea, if it will be hard to reliably find the header files from another R package. I could also push all of GSL into eiR, but as GSL has over 5000 files, this makes the package very large ( >22 MB) and  slow to compile. Both of which are a problem when submitting a package to bioconductor. It may very well be that leaving GSL as an external dependency to eiR is really the best and easiest way, but I just wanted to see if there was any way to make it easier for the user.
>> 
>> Can you clarify what you mean by "user"? The vast majority of R users use binaries, so all this is irrelevant to them as they don't need to install GSL at all.
> 
> FWIW we currently have 7 or 8 Bioconductor packages that require the
> GSL as an external system lib. That's because even if we provide Windows
> binaries for those packages, those binaries are dynamically linked.
> Is there a way to build those binaries that would avoid that dependency?
> 

Yes, use static libgsl.


> Anyway, to answer Kevin's original question:
> 
>  how do I know where the GSL library and header files, packaged
>  in GSLR, would live so I can point the compiler at them?
> 
> Use the LinkingTo field.
> 

No, you're not linking to another package, your'e linking to a *library*. LinkingTo uses R's own mechanism for symbol detection in another *package*. I know, the name is a bit misleading but those are two different things.

Cheers,
Simon


> Cheers,
> H.
> 
>> 
>> Cheers,
>> Simon
>> 
>> 
>>> So, any other suggestions about how this could be accomplished?  Thanks.
>>> 
>>> Kevin
>>> 
>>> On 03/12/2013 05:26 AM, Simon Urbanek wrote:
>>>> Kevin,
>>>> 
>>>> On Mar 11, 2013, at 5:20 PM, Kevin Horan wrote:
>>>> 
>>>>> I am developing an R package, eiR,  which depends on another C library, GNU scientific library (GSL). In order to make life easier for the user, it would be nice to not have this as an external dependency, thus I would like to wrap this library in another R package, say GSLR for example. Thus far I know how to do this. The C code in eiR requires the .so library and the header files from GSL in order to compile. So the idea is that eiR would depend on GSLR, then GSLR gets compiled and installed first, then, while eiR is installing, it should be able to make use of the GSL library and header files while compiling. So my question is, how do I know where the GSL library and header files, packaged in GSLR, would live so I can point the compiler at them? I know how to find the installed directory of an R package from within R, but is there way to find that out using just Makevars or a Makefile? I'm open to suggestions about a better way organize all of this as well. I like the
> !
>>   idea of keeping the GSL code separate so that it can be updated/changed independently from eiR though.
>>>> Have a look at Rcpp.
>>>> 
>>>> 
>>>>>    I'm also aware of the gsl R library on CRAN, however, this just wraps GSL in R functions, but I need to use the GSL C functions in other C code in eiR.
>>>>> 
>>>> Why is what you are proposing any better than simply using GSL in eiR? You will still need the GSL external dependency for GSLR and you are only adding a lot of complexity by linking into another package's external directory (you cannot use libs) which is in itself very tricky (you'll have to deal with both static and shared version, multi-arch setups, possible relocation etc.). It won't make it any easier on the user, rather to the contrary as there will be more things to break. The only reason Rcpp goes into such length to do this is because it has no choice (the Rcpp library has to use the same libR so cannot be used as external dependency) - I would certainly not recommend it for something as trivial as providing GSL.
>>>> 
>>>> Cheers,
>>>> Simon
>>>> 
>>>> 
>>>>> Thanks.
>>>>> 
>>>>> Kevin
>>>>> 
>>>>> ______________________________________________
>>>>> R-devel at r-project.org mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>> 
>>>>> 
>>> 
>>> 
>> 
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>> 
> 
> -- 
> Herv? Pag?s
> 
> Program in Computational Biology
> Division of Public Health Sciences
> Fred Hutchinson Cancer Research Center
> 1100 Fairview Ave. N, M1-B514
> P.O. Box 19024
> Seattle, WA 98109-1024
> 
> E-mail: hpages at fhcrc.org
> Phone:  (206) 667-5791
> Fax:    (206) 667-1319
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 
> 


From edd at debian.org  Tue Mar 12 19:14:15 2013
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue, 12 Mar 2013 13:14:15 -0500
Subject: [Rd] compiling C code using headers from another R package
In-Reply-To: <513F6D72.3040304@fhcrc.org>
References: <513E4A98.9080503@cs.ucr.edu>
	<B54A5D4C-2578-4739-BEB5-6B17FA6A9856@r-project.org>
	<513F582B.9090601@cs.ucr.edu>
	<E43810D3-2AEC-4F07-B61A-03923D6D2FC3@r-project.org>
	<513F6D72.3040304@fhcrc.org>
Message-ID: <20799.28791.574452.229966@max.nulle.part>


On 12 March 2013 at 11:01, Herv? Pag?s wrote:
| Anyway, to answer Kevin's original question:
| 
|    how do I know where the GSL library and header files, packaged
|    in GSLR, would live so I can point the compiler at them?
| 
| Use the LinkingTo field.

Nope. Only covers the case of include files [1]

Dirk

[1] Unless you also register every single C function which is clearly
impractical with a library as large as the GSL. See '5.4 Registering native
routines' and '5.8 Linking to other packages' in the usual "Writing R
Extensions" manual.

-- 
Dirk Eddelbuettel | edd at debian.org | http://dirk.eddelbuettel.com  


From smckinney at bccrc.ca  Tue Mar 12 19:40:27 2013
From: smckinney at bccrc.ca (Steven McKinney)
Date: Tue, 12 Mar 2013 11:40:27 -0700
Subject: [Rd] Bugs due to naive copying of list elements
In-Reply-To: <71C77A69-728D-47B8-A331-BC2111C8EA3C@comcast.net>
References: <20130312105907.GA7498@cs.toronto.edu>,
	<71C77A69-728D-47B8-A331-BC2111C8EA3C@comcast.net>
Message-ID: <DCE81E14EB74504B971DAD4D2DB0356B0CB926ADA0@crcmail4.BCCRC.CA>

Whereas

> a <- list(c(1,2),c(3,4),c(5,6))
> b <<- a[2:3]
> a[[2]][2] <- 9
> b
[[1]]
[1] 3 4

[[2]]
[1] 5 6

> 

Examples such as this leave me in a cold sweat - where did I miss the documentation describing
Radford's case?  Can anyone point to the documentation that describes Radford's outcome?
(It looks very lisp-like.  Is it linked to R's origins in lisp?)

I get the same outcome in R-2.11.1 so it is nothing new.  I can only hope I have not
set up such an effect in analysis scripts I've used to generate reproducible publication output.

Steven McKinney

________________________________________
From: r-devel-bounces at r-project.org [r-devel-bounces at r-project.org] On Behalf Of David Winsemius [dwinsemius at comcast.net]
Sent: March 12, 2013 10:28 AM
To: Radford Neal
Cc: r-devel at r-project.org
Subject: Re: [Rd] Bugs due to naive copying of list elements

On Mar 12, 2013, at 3:59 AM, Radford Neal wrote:

> Several bugs are present in R-2.15.3 and R-alpha due to
> naive copying of list elements.
>
> The bug below is due to naive copying in subset.c (with
> similar bugs for matrices and arrays):
>
> a<-list(c(1,2),c(3,4),c(5,6))
> b<-a[2:3]
> a[[2]][2]<-9
> print(b[[1]][2])

This is an example of lazy evaluation, right?

>
> Naive copying in mapply.c leads to the following bug:
>
> X<-1+1
> f<-function(a,b) X
> A<-mapply(f,c(1,2,3),c(4,5,6),SIMPLIFY=FALSE)
> print(A)
> X[1]<-99
> print(A)
>

Is this a bug in mapply()? or in print()? I thought 'print' should evaluate its argument and "force" the promise to be executed. Or does it just return the same promise as was passed to it? Compare:

X<-1+1
f<-function(a,b) X
A<-mapply(f,c(1,2,3),c(4,5,6),SIMPLIFY=FALSE)
print(A); str(A)
X[1]<-99
print(A)

Could someone could comment on what 'force' actually does. I am unclear why force(A) in the code above in the pace of str(A) did not have the effect I expected, whereas str(b) or str(A) did have that effect.

> a<-list(c(1,2),c(3,4),c(5,6))
> b<-a[2:3]; force(b)
[[1]]
[1] 3 4

[[2]]
[1] 5 6

> a[[2]][2]<-9
> print(b[[1]][2])
[1] 9

#----------
> X<-1+1
> f<-function(a,b) X
> A<-mapply(f,c(1,2,3),c(4,5,6),SIMPLIFY=FALSE)
> print(A); force(A)
[[1]]
[1] 2

[[2]]
[1] 2

[[3]]
[1] 2

[[1]]
[1] 2

[[2]]
[1] 2

[[3]]
[1] 2

> X[1]<-99
> print(A)
[[1]]
[1] 99

[[2]]
[1] 99

[[3]]
[1] 99


> Similar bugs exist in eapply, rapply, and vapply.
>

--
David Winsemius
Alameda, CA, USA

______________________________________________
R-devel at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-devel


From hpages at fhcrc.org  Tue Mar 12 19:48:35 2013
From: hpages at fhcrc.org (=?ISO-8859-1?Q?Herv=E9_Pag=E8s?=)
Date: Tue, 12 Mar 2013 11:48:35 -0700
Subject: [Rd] compiling C code using headers from another R package
In-Reply-To: <5195A4FB-F14C-484C-A732-C98758D0D737@r-project.org>
References: <513E4A98.9080503@cs.ucr.edu>
	<B54A5D4C-2578-4739-BEB5-6B17FA6A9856@r-project.org>
	<513F582B.9090601@cs.ucr.edu>
	<E43810D3-2AEC-4F07-B61A-03923D6D2FC3@r-project.org>
	<513F6D72.3040304@fhcrc.org>
	<5195A4FB-F14C-484C-A732-C98758D0D737@r-project.org>
Message-ID: <513F7883.7060701@fhcrc.org>

On 03/12/2013 11:09 AM, Simon Urbanek wrote:
>
> On Mar 12, 2013, at 2:01 PM, Herv? Pag?s wrote:
>
>> Hi,
>>
>> On 03/12/2013 09:55 AM, Simon Urbanek wrote:
>>>
>>> On Mar 12, 2013, at 12:30 PM, Kevin Horan wrote:
>>>
>>>>
>>>>     Thanks for your input. To clarify, I don't need to use any part of GSL in my R code, nor do I wish to make any part of it accessible to users of eiR. I need it to compile other C/C++ code (LSH KIT), which I did not write, that will itself be used in eiR.
>>>>     My goal is allow the user to install eiR without also having to install GSL before hand.
>>>
>>> If your package is on CRAN they won't need to as we are providing Mac and Windows binaries.
>>
>> I think that at least on Windows, the user would still need to have the
>> GSL installed on his/her machine.
>>
>
> Why?
>
>
>>
>>> Linux can get the binaries form their distro, so the dependencies are installed automatically.
>>>
>>>
>>>> The target audience is people in bioinformatics who may not how to install something like GSL. It seems like what I was suggesting is not such a good idea, if it will be hard to reliably find the header files from another R package. I could also push all of GSL into eiR, but as GSL has over 5000 files, this makes the package very large ( >22 MB) and  slow to compile. Both of which are a problem when submitting a package to bioconductor. It may very well be that leaving GSL as an external dependency to eiR is really the best and easiest way, but I just wanted to see if there was any way to make it easier for the user.
>>>
>>> Can you clarify what you mean by "user"? The vast majority of R users use binaries, so all this is irrelevant to them as they don't need to install GSL at all.
>>
>> FWIW we currently have 7 or 8 Bioconductor packages that require the
>> GSL as an external system lib. That's because even if we provide Windows
>> binaries for those packages, those binaries are dynamically linked.
>> Is there a way to build those binaries that would avoid that dependency?
>>
>
> Yes, use static libgsl.

Ah yes, of course. Thanks for the reminder. Just checked with Dan and
turns out that we are using that libgsl.a too (made by Brian D. Ripley)
on our build system. Some Bioconductor packages have README or INSTALL
files that still mention that the Windows user needs to install the GSL
but that doesn't seem to be the case so we'll make sure this information
gets updated.

Also the SystemRequirements field in the DESCRIPTION file can be a
little bit confusing as it suggests that everybody requires the stuff
listed here when it actually depends whether the user is installing the
binary or not and how the binary was made.

>
>
>> Anyway, to answer Kevin's original question:
>>
>>   how do I know where the GSL library and header files, packaged
>>   in GSLR, would live so I can point the compiler at them?
>>
>> Use the LinkingTo field.
>>
>
> No, you're not linking to another package, your'e linking to a *library*. LinkingTo uses R's own mechanism for symbol detection in another *package*. I know, the name is a bit misleading but those are two different things.

I understand that but that's what Kevin wants right, i.e. linking
to another package. Yes there is the extra difficulty to register all
the C functions in the GSL API (as pointed out by Dirk) but that's
another story.

Thanks,
H.

>
> Cheers,
> Simon
>
>
>> Cheers,
>> H.
>>
>>>
>>> Cheers,
>>> Simon
>>>
>>>
>>>> So, any other suggestions about how this could be accomplished?  Thanks.
>>>>
>>>> Kevin
>>>>
>>>> On 03/12/2013 05:26 AM, Simon Urbanek wrote:
>>>>> Kevin,
>>>>>
>>>>> On Mar 11, 2013, at 5:20 PM, Kevin Horan wrote:
>>>>>
>>>>>> I am developing an R package, eiR,  which depends on another C library, GNU scientific library (GSL). In order to make life easier for the user, it would be nice to not have this as an external dependency, thus I would like to wrap this library in another R package, say GSLR for example. Thus far I know how to do this. The C code in eiR requires the .so library and the header files from GSL in order to compile. So the idea is that eiR would depend on GSLR, then GSLR gets compiled and installed first, then, while eiR is installing, it should be able to make use of the GSL library and header files while compiling. So my question is, how do I know where the GSL library and header files, packaged in GSLR, would live so I can point the compiler at them? I know how to find the installed directory of an R package from within R, but is there way to find that out using just Makevars or a Makefile? I'm open to suggestions about a better way organize all of this as well. I like t
 he
>> !
>>>    idea of keeping the GSL code separate so that it can be updated/changed independently from eiR though.
>>>>> Have a look at Rcpp.
>>>>>
>>>>>
>>>>>>     I'm also aware of the gsl R library on CRAN, however, this just wraps GSL in R functions, but I need to use the GSL C functions in other C code in eiR.
>>>>>>
>>>>> Why is what you are proposing any better than simply using GSL in eiR? You will still need the GSL external dependency for GSLR and you are only adding a lot of complexity by linking into another package's external directory (you cannot use libs) which is in itself very tricky (you'll have to deal with both static and shared version, multi-arch setups, possible relocation etc.). It won't make it any easier on the user, rather to the contrary as there will be more things to break. The only reason Rcpp goes into such length to do this is because it has no choice (the Rcpp library has to use the same libR so cannot be used as external dependency) - I would certainly not recommend it for something as trivial as providing GSL.
>>>>>
>>>>> Cheers,
>>>>> Simon
>>>>>
>>>>>
>>>>>> Thanks.
>>>>>>
>>>>>> Kevin
>>>>>>
>>>>>> ______________________________________________
>>>>>> R-devel at r-project.org mailing list
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>>>
>>>>>>
>>>>
>>>>
>>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>
>>
>> --
>> Herv? Pag?s
>>
>> Program in Computational Biology
>> Division of Public Health Sciences
>> Fred Hutchinson Cancer Research Center
>> 1100 Fairview Ave. N, M1-B514
>> P.O. Box 19024
>> Seattle, WA 98109-1024
>>
>> E-mail: hpages at fhcrc.org
>> Phone:  (206) 667-5791
>> Fax:    (206) 667-1319
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>>
>

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fhcrc.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From simon.urbanek at r-project.org  Tue Mar 12 19:56:53 2013
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Tue, 12 Mar 2013 14:56:53 -0400
Subject: [Rd] compiling C code using headers from another R package
In-Reply-To: <513F7883.7060701@fhcrc.org>
References: <513E4A98.9080503@cs.ucr.edu>
	<B54A5D4C-2578-4739-BEB5-6B17FA6A9856@r-project.org>
	<513F582B.9090601@cs.ucr.edu>
	<E43810D3-2AEC-4F07-B61A-03923D6D2FC3@r-project.org>
	<513F6D72.3040304@fhcrc.org>
	<5195A4FB-F14C-484C-A732-C98758D0D737@r-project.org>
	<513F7883.7060701@fhcrc.org>
Message-ID: <CEA0FBC0-908A-4ED4-99AC-83C6BACB61F6@r-project.org>


On Mar 12, 2013, at 2:48 PM, Herv? Pag?s wrote:

> On 03/12/2013 11:09 AM, Simon Urbanek wrote:
>> 
>> On Mar 12, 2013, at 2:01 PM, Herv? Pag?s wrote:
>> 
>>> Hi,
>>> 
>>> On 03/12/2013 09:55 AM, Simon Urbanek wrote:
>>>> 
>>>> On Mar 12, 2013, at 12:30 PM, Kevin Horan wrote:
>>>> 
>>>>> 
>>>>>    Thanks for your input. To clarify, I don't need to use any part of GSL in my R code, nor do I wish to make any part of it accessible to users of eiR. I need it to compile other C/C++ code (LSH KIT), which I did not write, that will itself be used in eiR.
>>>>>    My goal is allow the user to install eiR without also having to install GSL before hand.
>>>> 
>>>> If your package is on CRAN they won't need to as we are providing Mac and Windows binaries.
>>> 
>>> I think that at least on Windows, the user would still need to have the
>>> GSL installed on his/her machine.
>>> 
>> 
>> Why?
>> 
>> 
>>> 
>>>> Linux can get the binaries form their distro, so the dependencies are installed automatically.
>>>> 
>>>> 
>>>>> The target audience is people in bioinformatics who may not how to install something like GSL. It seems like what I was suggesting is not such a good idea, if it will be hard to reliably find the header files from another R package. I could also push all of GSL into eiR, but as GSL has over 5000 files, this makes the package very large ( >22 MB) and  slow to compile. Both of which are a problem when submitting a package to bioconductor. It may very well be that leaving GSL as an external dependency to eiR is really the best and easiest way, but I just wanted to see if there was any way to make it easier for the user.
>>>> 
>>>> Can you clarify what you mean by "user"? The vast majority of R users use binaries, so all this is irrelevant to them as they don't need to install GSL at all.
>>> 
>>> FWIW we currently have 7 or 8 Bioconductor packages that require the
>>> GSL as an external system lib. That's because even if we provide Windows
>>> binaries for those packages, those binaries are dynamically linked.
>>> Is there a way to build those binaries that would avoid that dependency?
>>> 
>> 
>> Yes, use static libgsl.
> 
> Ah yes, of course. Thanks for the reminder. Just checked with Dan and
> turns out that we are using that libgsl.a too (made by Brian D. Ripley)
> on our build system. Some Bioconductor packages have README or INSTALL
> files that still mention that the Windows user needs to install the GSL
> but that doesn't seem to be the case so we'll make sure this information
> gets updated.
> 
> Also the SystemRequirements field in the DESCRIPTION file can be a
> little bit confusing as it suggests that everybody requires the stuff
> listed here when it actually depends whether the user is installing the
> binary or not and how the binary was made.
> 
>> 
>> 
>>> Anyway, to answer Kevin's original question:
>>> 
>>>  how do I know where the GSL library and header files, packaged
>>>  in GSLR, would live so I can point the compiler at them?
>>> 
>>> Use the LinkingTo field.
>>> 
>> 
>> No, you're not linking to another package, your'e linking to a *library*. LinkingTo uses R's own mechanism for symbol detection in another *package*. I know, the name is a bit misleading but those are two different things.
> 
> I understand that but that's what Kevin wants right, i.e. linking
> to another package.

No (you cannot link to another package - that's the misnomer), he wants to link to a library provided by another package (see his e-mail, he's asking about how to locate the GSL library supplied with RGSL, not to RGSL itself).


> Yes there is the extra difficulty to register all the C functions in the GSL API (as pointed out by Dirk) but that's another story.
> 

That's not another story - that's very much part of the story why using LinkingTo is not a good idea.

Cheers,
Simon



> Thanks,
> H.
> 
>> 
>> Cheers,
>> Simon
>> 
>> 
>>> Cheers,
>>> H.
>>> 
>>>> 
>>>> Cheers,
>>>> Simon
>>>> 
>>>> 
>>>>> So, any other suggestions about how this could be accomplished?  Thanks.
>>>>> 
>>>>> Kevin
>>>>> 
>>>>> On 03/12/2013 05:26 AM, Simon Urbanek wrote:
>>>>>> Kevin,
>>>>>> 
>>>>>> On Mar 11, 2013, at 5:20 PM, Kevin Horan wrote:
>>>>>> 
>>>>>>> I am developing an R package, eiR,  which depends on another C library, GNU scientific library (GSL). In order to make life easier for the user, it would be nice to not have this as an external dependency, thus I would like to wrap this library in another R package, say GSLR for example. Thus far I know how to do this. The C code in eiR requires the .so library and the header files from GSL in order to compile. So the idea is that eiR would depend on GSLR, then GSLR gets compiled and installed first, then, while eiR is installing, it should be able to make use of the GSL library and header files while compiling. So my question is, how do I know where the GSL library and header files, packaged in GSLR, would live so I can point the compiler at them? I know how to find the installed directory of an R package from within R, but is there way to find that out using just Makevars or a Makefile? I'm open to suggestions about a better way organize all of this as well. I like t
> he
>>> !
>>>>   idea of keeping the GSL code separate so that it can be updated/changed independently from eiR though.
>>>>>> Have a look at Rcpp.
>>>>>> 
>>>>>> 
>>>>>>>    I'm also aware of the gsl R library on CRAN, however, this just wraps GSL in R functions, but I need to use the GSL C functions in other C code in eiR.
>>>>>>> 
>>>>>> Why is what you are proposing any better than simply using GSL in eiR? You will still need the GSL external dependency for GSLR and you are only adding a lot of complexity by linking into another package's external directory (you cannot use libs) which is in itself very tricky (you'll have to deal with both static and shared version, multi-arch setups, possible relocation etc.). It won't make it any easier on the user, rather to the contrary as there will be more things to break. The only reason Rcpp goes into such length to do this is because it has no choice (the Rcpp library has to use the same libR so cannot be used as external dependency) - I would certainly not recommend it for something as trivial as providing GSL.
>>>>>> 
>>>>>> Cheers,
>>>>>> Simon
>>>>>> 
>>>>>> 
>>>>>>> Thanks.
>>>>>>> 
>>>>>>> Kevin
>>>>>>> 
>>>>>>> ______________________________________________
>>>>>>> R-devel at r-project.org mailing list
>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>>>> 
>>>>>>> 
>>>>> 
>>>>> 
>>>> 
>>>> ______________________________________________
>>>> R-devel at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>> 
>>> 
>>> --
>>> Herv? Pag?s
>>> 
>>> Program in Computational Biology
>>> Division of Public Health Sciences
>>> Fred Hutchinson Cancer Research Center
>>> 1100 Fairview Ave. N, M1-B514
>>> P.O. Box 19024
>>> Seattle, WA 98109-1024
>>> 
>>> E-mail: hpages at fhcrc.org
>>> Phone:  (206) 667-5791
>>> Fax:    (206) 667-1319
>>> 
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>> 
>>> 
>> 
> 
> -- 
> Herv? Pag?s
> 
> Program in Computational Biology
> Division of Public Health Sciences
> Fred Hutchinson Cancer Research Center
> 1100 Fairview Ave. N, M1-B514
> P.O. Box 19024
> Seattle, WA 98109-1024
> 
> E-mail: hpages at fhcrc.org
> Phone:  (206) 667-5791
> Fax:    (206) 667-1319
> 
> 


From wdunlap at tibco.com  Tue Mar 12 20:05:12 2013
From: wdunlap at tibco.com (William Dunlap)
Date: Tue, 12 Mar 2013 19:05:12 +0000
Subject: [Rd] duplicate export entries in NAMESPACE
Message-ID: <E66794E69CFDE04D9A70842786030B931B91337E@PA-MBX04.na.tibco.com>

Circa 80 CRAN and core-R packages have duplicate export entries in their NAMESPACE files.  E.g.,
  bit 1.1.9 : c("as.bit", "as.bitwhich", "as.which", "physical", "virtual")
  forecast 4.1 : "forecast.lm" 
  graphics 2.15.3 : "barplot" 
  mcmc 0.9.1 : "morph" 
  RCurl 1.95.3 : "curlOptions" 
  utils 2.15.3 : "RweaveLatexOptions"
Would it be helpful for 'check' to alert package writers to this?

I made the list using f():
  f <- function () 
  {
     for(pkg in installed.packages()[,"Package"]) {
        try( {
            exports <- parseNamespaceFile(pkg, R.home("library"))$exports
            if (any(dup <- duplicated(exports))) {
                cat(pkg, format(packageVersion(pkg)), ":", deparse(exports[dup]), "\n")
            }
        }, silent = TRUE)
     }
  }
I suppose it should also check for duplicates in S3method component, etc.

Bill Dunlap
Spotfire, TIBCO Software
wdunlap tibco.com


From hpages at fhcrc.org  Tue Mar 12 20:35:20 2013
From: hpages at fhcrc.org (=?ISO-8859-1?Q?Herv=E9_Pag=E8s?=)
Date: Tue, 12 Mar 2013 12:35:20 -0700
Subject: [Rd] compiling C code using headers from another R package
In-Reply-To: <CEA0FBC0-908A-4ED4-99AC-83C6BACB61F6@r-project.org>
References: <513E4A98.9080503@cs.ucr.edu>
	<B54A5D4C-2578-4739-BEB5-6B17FA6A9856@r-project.org>
	<513F582B.9090601@cs.ucr.edu>
	<E43810D3-2AEC-4F07-B61A-03923D6D2FC3@r-project.org>
	<513F6D72.3040304@fhcrc.org>
	<5195A4FB-F14C-484C-A732-C98758D0D737@r-project.org>
	<513F7883.7060701@fhcrc.org>
	<CEA0FBC0-908A-4ED4-99AC-83C6BACB61F6@r-project.org>
Message-ID: <513F8378.4020800@fhcrc.org>



On 03/12/2013 11:56 AM, Simon Urbanek wrote:
>
> On Mar 12, 2013, at 2:48 PM, Herv? Pag?s wrote:
>
>> On 03/12/2013 11:09 AM, Simon Urbanek wrote:
>>>
>>> On Mar 12, 2013, at 2:01 PM, Herv? Pag?s wrote:
>>>
>>>> Hi,
>>>>
>>>> On 03/12/2013 09:55 AM, Simon Urbanek wrote:
>>>>>
>>>>> On Mar 12, 2013, at 12:30 PM, Kevin Horan wrote:
>>>>>
>>>>>>
>>>>>>     Thanks for your input. To clarify, I don't need to use any part of GSL in my R code, nor do I wish to make any part of it accessible to users of eiR. I need it to compile other C/C++ code (LSH KIT), which I did not write, that will itself be used in eiR.
>>>>>>     My goal is allow the user to install eiR without also having to install GSL before hand.
>>>>>
>>>>> If your package is on CRAN they won't need to as we are providing Mac and Windows binaries.
>>>>
>>>> I think that at least on Windows, the user would still need to have the
>>>> GSL installed on his/her machine.
>>>>
>>>
>>> Why?
>>>
>>>
>>>>
>>>>> Linux can get the binaries form their distro, so the dependencies are installed automatically.
>>>>>
>>>>>
>>>>>> The target audience is people in bioinformatics who may not how to install something like GSL. It seems like what I was suggesting is not such a good idea, if it will be hard to reliably find the header files from another R package. I could also push all of GSL into eiR, but as GSL has over 5000 files, this makes the package very large ( >22 MB) and  slow to compile. Both of which are a problem when submitting a package to bioconductor. It may very well be that leaving GSL as an external dependency to eiR is really the best and easiest way, but I just wanted to see if there was any way to make it easier for the user.
>>>>>
>>>>> Can you clarify what you mean by "user"? The vast majority of R users use binaries, so all this is irrelevant to them as they don't need to install GSL at all.
>>>>
>>>> FWIW we currently have 7 or 8 Bioconductor packages that require the
>>>> GSL as an external system lib. That's because even if we provide Windows
>>>> binaries for those packages, those binaries are dynamically linked.
>>>> Is there a way to build those binaries that would avoid that dependency?
>>>>
>>>
>>> Yes, use static libgsl.
>>
>> Ah yes, of course. Thanks for the reminder. Just checked with Dan and
>> turns out that we are using that libgsl.a too (made by Brian D. Ripley)
>> on our build system. Some Bioconductor packages have README or INSTALL
>> files that still mention that the Windows user needs to install the GSL
>> but that doesn't seem to be the case so we'll make sure this information
>> gets updated.
>>
>> Also the SystemRequirements field in the DESCRIPTION file can be a
>> little bit confusing as it suggests that everybody requires the stuff
>> listed here when it actually depends whether the user is installing the
>> binary or not and how the binary was made.
>>
>>>
>>>
>>>> Anyway, to answer Kevin's original question:
>>>>
>>>>   how do I know where the GSL library and header files, packaged
>>>>   in GSLR, would live so I can point the compiler at them?
>>>>
>>>> Use the LinkingTo field.
>>>>
>>>
>>> No, you're not linking to another package, your'e linking to a *library*. LinkingTo uses R's own mechanism for symbol detection in another *package*. I know, the name is a bit misleading but those are two different things.
>>
>> I understand that but that's what Kevin wants right, i.e. linking
>> to another package.
>
> No (you cannot link to another package - that's the misnomer), he wants to link to a library provided by another package (see his e-mail, he's asking about how to locate the GSL library supplied with RGSL, not to RGSL itself).

Maybe I misunderstand what the OP wants to do, and the only way to know
would be for him to clarify, but IIUC the RGSL package would only
contain the GSL library. At least that's how I understand it (and
that's what makes sense to me). So the RGSL package would contain
only 1 shared object (or 1 shared object per sub-arch): the GSLR.so
file (extension may vary). So I'm not sure what's the difference between
linking to the "GSL library supplied with RGSL" and linking to
"RGSL itself"?

Very confusing to me. Thanks for your time and sorry if I'm missing
something obvious.

H.

>
>
>> Yes there is the extra difficulty to register all the C functions in the GSL API (as pointed out by Dirk) but that's another story.
>>
>
> That's not another story - that's very much part of the story why using LinkingTo is not a good idea.
>
> Cheers,
> Simon
>
>
>
>> Thanks,
>> H.
>>
>>>
>>> Cheers,
>>> Simon
>>>
>>>
>>>> Cheers,
>>>> H.
>>>>
>>>>>
>>>>> Cheers,
>>>>> Simon
>>>>>
>>>>>
>>>>>> So, any other suggestions about how this could be accomplished?  Thanks.
>>>>>>
>>>>>> Kevin
>>>>>>
>>>>>> On 03/12/2013 05:26 AM, Simon Urbanek wrote:
>>>>>>> Kevin,
>>>>>>>
>>>>>>> On Mar 11, 2013, at 5:20 PM, Kevin Horan wrote:
>>>>>>>
>>>>>>>> I am developing an R package, eiR,  which depends on another C library, GNU scientific library (GSL). In order to make life easier for the user, it would be nice to not have this as an external dependency, thus I would like to wrap this library in another R package, say GSLR for example. Thus far I know how to do this. The C code in eiR requires the .so library and the header files from GSL in order to compile. So the idea is that eiR would depend on GSLR, then GSLR gets compiled and installed first, then, while eiR is installing, it should be able to make use of the GSL library and header files while compiling. So my question is, how do I know where the GSL library and header files, packaged in GSLR, would live so I can point the compiler at them? I know how to find the installed directory of an R package from within R, but is there way to find that out using just Makevars or a Makefile? I'm open to suggestions about a better way organize all of this as well. I like
  t
>> he
>>>> !
>>>>>    idea of keeping the GSL code separate so that it can be updated/changed independently from eiR though.
>>>>>>> Have a look at Rcpp.
>>>>>>>
>>>>>>>
>>>>>>>>     I'm also aware of the gsl R library on CRAN, however, this just wraps GSL in R functions, but I need to use the GSL C functions in other C code in eiR.
>>>>>>>>
>>>>>>> Why is what you are proposing any better than simply using GSL in eiR? You will still need the GSL external dependency for GSLR and you are only adding a lot of complexity by linking into another package's external directory (you cannot use libs) which is in itself very tricky (you'll have to deal with both static and shared version, multi-arch setups, possible relocation etc.). It won't make it any easier on the user, rather to the contrary as there will be more things to break. The only reason Rcpp goes into such length to do this is because it has no choice (the Rcpp library has to use the same libR so cannot be used as external dependency) - I would certainly not recommend it for something as trivial as providing GSL.
>>>>>>>
>>>>>>> Cheers,
>>>>>>> Simon
>>>>>>>
>>>>>>>
>>>>>>>> Thanks.
>>>>>>>>
>>>>>>>> Kevin
>>>>>>>>
>>>>>>>> ______________________________________________
>>>>>>>> R-devel at r-project.org mailing list
>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>>>>>
>>>>>>>>
>>>>>>
>>>>>>
>>>>>
>>>>> ______________________________________________
>>>>> R-devel at r-project.org mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>>
>>>>
>>>> --
>>>> Herv? Pag?s
>>>>
>>>> Program in Computational Biology
>>>> Division of Public Health Sciences
>>>> Fred Hutchinson Cancer Research Center
>>>> 1100 Fairview Ave. N, M1-B514
>>>> P.O. Box 19024
>>>> Seattle, WA 98109-1024
>>>>
>>>> E-mail: hpages at fhcrc.org
>>>> Phone:  (206) 667-5791
>>>> Fax:    (206) 667-1319
>>>>
>>>> ______________________________________________
>>>> R-devel at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>
>>>>
>>>
>>
>> --
>> Herv? Pag?s
>>
>> Program in Computational Biology
>> Division of Public Health Sciences
>> Fred Hutchinson Cancer Research Center
>> 1100 Fairview Ave. N, M1-B514
>> P.O. Box 19024
>> Seattle, WA 98109-1024
>>
>> E-mail: hpages at fhcrc.org
>> Phone:  (206) 667-5791
>> Fax:    (206) 667-1319
>>
>>
>

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fhcrc.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From kevin.hendricks at sympatico.ca  Tue Mar 12 20:38:39 2013
From: kevin.hendricks at sympatico.ca (kevin.hendricks at sympatico.ca)
Date: Tue, 12 Mar 2013 19:38:39 +0000
Subject: [Rd] Bugs due to naive copying of list elements
In-Reply-To: <DCE81E14EB74504B971DAD4D2DB0356B0CB926ADA0@crcmail4.BCCRC.CA>
References: <20130312105907.GA7498@cs.toronto.edu>, ,
	<71C77A69-728D-47B8-A331-BC2111C8EA3C@comcast.net>,
	<DCE81E14EB74504B971DAD4D2DB0356B0CB926ADA0@crcmail4.BCCRC.CA>
Message-ID: <BAY164-W58E3C7788E933C38CB94EB87E20@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20130312/318362b3/attachment.pl>

From simon.urbanek at r-project.org  Tue Mar 12 20:53:26 2013
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Tue, 12 Mar 2013 15:53:26 -0400
Subject: [Rd] compiling C code using headers from another R package
In-Reply-To: <513F8378.4020800@fhcrc.org>
References: <513E4A98.9080503@cs.ucr.edu>
	<B54A5D4C-2578-4739-BEB5-6B17FA6A9856@r-project.org>
	<513F582B.9090601@cs.ucr.edu>
	<E43810D3-2AEC-4F07-B61A-03923D6D2FC3@r-project.org>
	<513F6D72.3040304@fhcrc.org>
	<5195A4FB-F14C-484C-A732-C98758D0D737@r-project.org>
	<513F7883.7060701@fhcrc.org>
	<CEA0FBC0-908A-4ED4-99AC-83C6BACB61F6@r-project.org>
	<513F8378.4020800@fhcrc.org>
Message-ID: <C427ED87-4C8E-455F-83FF-9A7FEF4BFAFC@r-project.org>


On Mar 12, 2013, at 3:35 PM, Herv? Pag?s wrote:

> 
> 
> On 03/12/2013 11:56 AM, Simon Urbanek wrote:
>> 
>> On Mar 12, 2013, at 2:48 PM, Herv? Pag?s wrote:
>> 
>>> On 03/12/2013 11:09 AM, Simon Urbanek wrote:
>>>> 
>>>> On Mar 12, 2013, at 2:01 PM, Herv? Pag?s wrote:
>>>> 
>>>>> Hi,
>>>>> 
>>>>> On 03/12/2013 09:55 AM, Simon Urbanek wrote:
>>>>>> 
>>>>>> On Mar 12, 2013, at 12:30 PM, Kevin Horan wrote:
>>>>>> 
>>>>>>> 
>>>>>>>    Thanks for your input. To clarify, I don't need to use any part of GSL in my R code, nor do I wish to make any part of it accessible to users of eiR. I need it to compile other C/C++ code (LSH KIT), which I did not write, that will itself be used in eiR.
>>>>>>>    My goal is allow the user to install eiR without also having to install GSL before hand.
>>>>>> 
>>>>>> If your package is on CRAN they won't need to as we are providing Mac and Windows binaries.
>>>>> 
>>>>> I think that at least on Windows, the user would still need to have the
>>>>> GSL installed on his/her machine.
>>>>> 
>>>> 
>>>> Why?
>>>> 
>>>> 
>>>>> 
>>>>>> Linux can get the binaries form their distro, so the dependencies are installed automatically.
>>>>>> 
>>>>>> 
>>>>>>> The target audience is people in bioinformatics who may not how to install something like GSL. It seems like what I was suggesting is not such a good idea, if it will be hard to reliably find the header files from another R package. I could also push all of GSL into eiR, but as GSL has over 5000 files, this makes the package very large ( >22 MB) and  slow to compile. Both of which are a problem when submitting a package to bioconductor. It may very well be that leaving GSL as an external dependency to eiR is really the best and easiest way, but I just wanted to see if there was any way to make it easier for the user.
>>>>>> 
>>>>>> Can you clarify what you mean by "user"? The vast majority of R users use binaries, so all this is irrelevant to them as they don't need to install GSL at all.
>>>>> 
>>>>> FWIW we currently have 7 or 8 Bioconductor packages that require the
>>>>> GSL as an external system lib. That's because even if we provide Windows
>>>>> binaries for those packages, those binaries are dynamically linked.
>>>>> Is there a way to build those binaries that would avoid that dependency?
>>>>> 
>>>> 
>>>> Yes, use static libgsl.
>>> 
>>> Ah yes, of course. Thanks for the reminder. Just checked with Dan and
>>> turns out that we are using that libgsl.a too (made by Brian D. Ripley)
>>> on our build system. Some Bioconductor packages have README or INSTALL
>>> files that still mention that the Windows user needs to install the GSL
>>> but that doesn't seem to be the case so we'll make sure this information
>>> gets updated.
>>> 
>>> Also the SystemRequirements field in the DESCRIPTION file can be a
>>> little bit confusing as it suggests that everybody requires the stuff
>>> listed here when it actually depends whether the user is installing the
>>> binary or not and how the binary was made.
>>> 
>>>> 
>>>> 
>>>>> Anyway, to answer Kevin's original question:
>>>>> 
>>>>>  how do I know where the GSL library and header files, packaged
>>>>>  in GSLR, would live so I can point the compiler at them?
>>>>> 
>>>>> Use the LinkingTo field.
>>>>> 
>>>> 
>>>> No, you're not linking to another package, your'e linking to a *library*. LinkingTo uses R's own mechanism for symbol detection in another *package*. I know, the name is a bit misleading but those are two different things.
>>> 
>>> I understand that but that's what Kevin wants right, i.e. linking
>>> to another package.
>> 
>> No (you cannot link to another package - that's the misnomer), he wants to link to a library provided by another package (see his e-mail, he's asking about how to locate the GSL library supplied with RGSL, not to RGSL itself).
> 
> Maybe I misunderstand what the OP wants to do, and the only way to know
> would be for him to clarify, but IIUC the RGSL package would only
> contain the GSL library. At least that's how I understand it (and
> that's what makes sense to me). So the RGSL package would contain
> only 1 shared object (or 1 shared object per sub-arch): the GSLR.so
> file (extension may vary). So I'm not sure what's the difference between
> linking to the "GSL library supplied with RGSL" and linking to
> "RGSL itself"?
> 

GSL library inside RGSL is libgsl.* (where * is a, dylib, so, dll depending on the type and OS). This is *not* the same as RGSL.so/dll which would be the package shared object compiled by R for the package. There is a big difference: the shared objects that R creates for packages cannot be linked to, they are meant to be used with dlopen()/dlsym() - which is what R uses to load symbols entry points. That is also the reason why using LinkingTo: requires explicit exposure of symbols by the package providing the symbols as well as explicit loading of the symbols by the package that uses them. This is entirely different than linking to a library - in the latter case the linker (not R) establishes the connection between the symbols in the library and references - and in case of a static library they get copied into the binary that is being created (which is why you don't need it anymore after linking).

Cheers,
Simon


> Very confusing to me. Thanks for your time and sorry if I'm missing something obvious.
> 
> H.
> 
>> 
>> 
>>> Yes there is the extra difficulty to register all the C functions in the GSL API (as pointed out by Dirk) but that's another story.
>>> 
>> 
>> That's not another story - that's very much part of the story why using LinkingTo is not a good idea.
>> 
>> Cheers,
>> Simon
>> 
>> 
>> 
>>> Thanks,
>>> H.
>>> 
>>>> 
>>>> Cheers,
>>>> Simon
>>>> 
>>>> 
>>>>> Cheers,
>>>>> H.
>>>>> 
>>>>>> 
>>>>>> Cheers,
>>>>>> Simon
>>>>>> 
>>>>>> 
>>>>>>> So, any other suggestions about how this could be accomplished?  Thanks.
>>>>>>> 
>>>>>>> Kevin
>>>>>>> 
>>>>>>> On 03/12/2013 05:26 AM, Simon Urbanek wrote:
>>>>>>>> Kevin,
>>>>>>>> 
>>>>>>>> On Mar 11, 2013, at 5:20 PM, Kevin Horan wrote:
>>>>>>>> 
>>>>>>>>> I am developing an R package, eiR,  which depends on another C library, GNU scientific library (GSL). In order to make life easier for the user, it would be nice to not have this as an external dependency, thus I would like to wrap this library in another R package, say GSLR for example. Thus far I know how to do this. The C code in eiR requires the .so library and the header files from GSL in order to compile. So the idea is that eiR would depend on GSLR, then GSLR gets compiled and installed first, then, while eiR is installing, it should be able to make use of the GSL library and header files while compiling. So my question is, how do I know where the GSL library and header files, packaged in GSLR, would live so I can point the compiler at them? I know how to find the installed directory of an R package from within R, but is there way to find that out using just Makevars or a Makefile? I'm open to suggestions about a better way organize all of this as well. I like
> t
>>> he
>>>>> !
>>>>>>   idea of keeping the GSL code separate so that it can be updated/changed independently from eiR though.
>>>>>>>> Have a look at Rcpp.
>>>>>>>> 
>>>>>>>> 
>>>>>>>>>    I'm also aware of the gsl R library on CRAN, however, this just wraps GSL in R functions, but I need to use the GSL C functions in other C code in eiR.
>>>>>>>>> 
>>>>>>>> Why is what you are proposing any better than simply using GSL in eiR? You will still need the GSL external dependency for GSLR and you are only adding a lot of complexity by linking into another package's external directory (you cannot use libs) which is in itself very tricky (you'll have to deal with both static and shared version, multi-arch setups, possible relocation etc.). It won't make it any easier on the user, rather to the contrary as there will be more things to break. The only reason Rcpp goes into such length to do this is because it has no choice (the Rcpp library has to use the same libR so cannot be used as external dependency) - I would certainly not recommend it for something as trivial as providing GSL.
>>>>>>>> 
>>>>>>>> Cheers,
>>>>>>>> Simon
>>>>>>>> 
>>>>>>>> 
>>>>>>>>> Thanks.
>>>>>>>>> 
>>>>>>>>> Kevin
>>>>>>>>> 
>>>>>>>>> ______________________________________________
>>>>>>>>> R-devel at r-project.org mailing list
>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>>>>>> 
>>>>>>>>> 
>>>>>>> 
>>>>>>> 
>>>>>> 
>>>>>> ______________________________________________
>>>>>> R-devel at r-project.org mailing list
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>>> 
>>>>> 
>>>>> --
>>>>> Herv? Pag?s
>>>>> 
>>>>> Program in Computational Biology
>>>>> Division of Public Health Sciences
>>>>> Fred Hutchinson Cancer Research Center
>>>>> 1100 Fairview Ave. N, M1-B514
>>>>> P.O. Box 19024
>>>>> Seattle, WA 98109-1024
>>>>> 
>>>>> E-mail: hpages at fhcrc.org
>>>>> Phone:  (206) 667-5791
>>>>> Fax:    (206) 667-1319
>>>>> 
>>>>> ______________________________________________
>>>>> R-devel at r-project.org mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>> 
>>>>> 
>>>> 
>>> 
>>> --
>>> Herv? Pag?s
>>> 
>>> Program in Computational Biology
>>> Division of Public Health Sciences
>>> Fred Hutchinson Cancer Research Center
>>> 1100 Fairview Ave. N, M1-B514
>>> P.O. Box 19024
>>> Seattle, WA 98109-1024
>>> 
>>> E-mail: hpages at fhcrc.org
>>> Phone:  (206) 667-5791
>>> Fax:    (206) 667-1319
>>> 
>>> 
>> 
> 
> -- 
> Herv? Pag?s
> 
> Program in Computational Biology
> Division of Public Health Sciences
> Fred Hutchinson Cancer Research Center
> 1100 Fairview Ave. N, M1-B514
> P.O. Box 19024
> Seattle, WA 98109-1024
> 
> E-mail: hpages at fhcrc.org
> Phone:  (206) 667-5791
> Fax:    (206) 667-1319
> 
> 


From luke-tierney at uiowa.edu  Tue Mar 12 21:41:43 2013
From: luke-tierney at uiowa.edu (luke-tierney at uiowa.edu)
Date: Tue, 12 Mar 2013 15:41:43 -0500
Subject: [Rd] Bugs due to naive copying of list elements
In-Reply-To: <20130312105907.GA7498@cs.toronto.edu>
References: <20130312105907.GA7498@cs.toronto.edu>
Message-ID: <alpine.DEB.2.02.1303121540510.2076@luke-Latitude>

Thanks for the report. Fixed in r62220 on trunk, r62221 on
R-3-0-branch, and r62222 on R-2-15-branch.

Best,

luke

On Tue, 12 Mar 2013, Radford Neal wrote:

> Several bugs are present in R-2.15.3 and R-alpha due to
> naive copying of list elements.
>
> The bug below is due to naive copying in subset.c (with
> similar bugs for matrices and arrays):
>
> a<-list(c(1,2),c(3,4),c(5,6))
> b<-a[2:3]
> a[[2]][2]<-9
> print(b[[1]][2])
>
> Naive copying in mapply.c leads to the following bug:
>
> X<-1+1
> f<-function(a,b) X
> A<-mapply(f,c(1,2,3),c(4,5,6),SIMPLIFY=FALSE)
> print(A)
> X[1]<-99
> print(A)
>
> Similar bugs exist in eapply, rapply, and vapply.
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

-- 
Luke Tierney
Chair, Statistics and Actuarial Science
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu


From hpages at fhcrc.org  Tue Mar 12 21:56:34 2013
From: hpages at fhcrc.org (=?ISO-8859-1?Q?Herv=E9_Pag=E8s?=)
Date: Tue, 12 Mar 2013 13:56:34 -0700
Subject: [Rd] compiling C code using headers from another R package
In-Reply-To: <C427ED87-4C8E-455F-83FF-9A7FEF4BFAFC@r-project.org>
References: <513E4A98.9080503@cs.ucr.edu>
	<B54A5D4C-2578-4739-BEB5-6B17FA6A9856@r-project.org>
	<513F582B.9090601@cs.ucr.edu>
	<E43810D3-2AEC-4F07-B61A-03923D6D2FC3@r-project.org>
	<513F6D72.3040304@fhcrc.org>
	<5195A4FB-F14C-484C-A732-C98758D0D737@r-project.org>
	<513F7883.7060701@fhcrc.org>
	<CEA0FBC0-908A-4ED4-99AC-83C6BACB61F6@r-project.org>
	<513F8378.4020800@fhcrc.org>
	<C427ED87-4C8E-455F-83FF-9A7FEF4BFAFC@r-project.org>
Message-ID: <513F9682.2080503@fhcrc.org>



On 03/12/2013 12:53 PM, Simon Urbanek wrote:
>
> On Mar 12, 2013, at 3:35 PM, Herv? Pag?s wrote:
>
>>
>>
>> On 03/12/2013 11:56 AM, Simon Urbanek wrote:
>>>
>>> On Mar 12, 2013, at 2:48 PM, Herv? Pag?s wrote:
>>>
>>>> On 03/12/2013 11:09 AM, Simon Urbanek wrote:
>>>>>
>>>>> On Mar 12, 2013, at 2:01 PM, Herv? Pag?s wrote:
>>>>>
>>>>>> Hi,
>>>>>>
>>>>>> On 03/12/2013 09:55 AM, Simon Urbanek wrote:
>>>>>>>
>>>>>>> On Mar 12, 2013, at 12:30 PM, Kevin Horan wrote:
>>>>>>>
>>>>>>>>
>>>>>>>>     Thanks for your input. To clarify, I don't need to use any part of GSL in my R code, nor do I wish to make any part of it accessible to users of eiR. I need it to compile other C/C++ code (LSH KIT), which I did not write, that will itself be used in eiR.
>>>>>>>>     My goal is allow the user to install eiR without also having to install GSL before hand.
>>>>>>>
>>>>>>> If your package is on CRAN they won't need to as we are providing Mac and Windows binaries.
>>>>>>
>>>>>> I think that at least on Windows, the user would still need to have the
>>>>>> GSL installed on his/her machine.
>>>>>>
>>>>>
>>>>> Why?
>>>>>
>>>>>
>>>>>>
>>>>>>> Linux can get the binaries form their distro, so the dependencies are installed automatically.
>>>>>>>
>>>>>>>
>>>>>>>> The target audience is people in bioinformatics who may not how to install something like GSL. It seems like what I was suggesting is not such a good idea, if it will be hard to reliably find the header files from another R package. I could also push all of GSL into eiR, but as GSL has over 5000 files, this makes the package very large ( >22 MB) and  slow to compile. Both of which are a problem when submitting a package to bioconductor. It may very well be that leaving GSL as an external dependency to eiR is really the best and easiest way, but I just wanted to see if there was any way to make it easier for the user.
>>>>>>>
>>>>>>> Can you clarify what you mean by "user"? The vast majority of R users use binaries, so all this is irrelevant to them as they don't need to install GSL at all.
>>>>>>
>>>>>> FWIW we currently have 7 or 8 Bioconductor packages that require the
>>>>>> GSL as an external system lib. That's because even if we provide Windows
>>>>>> binaries for those packages, those binaries are dynamically linked.
>>>>>> Is there a way to build those binaries that would avoid that dependency?
>>>>>>
>>>>>
>>>>> Yes, use static libgsl.
>>>>
>>>> Ah yes, of course. Thanks for the reminder. Just checked with Dan and
>>>> turns out that we are using that libgsl.a too (made by Brian D. Ripley)
>>>> on our build system. Some Bioconductor packages have README or INSTALL
>>>> files that still mention that the Windows user needs to install the GSL
>>>> but that doesn't seem to be the case so we'll make sure this information
>>>> gets updated.
>>>>
>>>> Also the SystemRequirements field in the DESCRIPTION file can be a
>>>> little bit confusing as it suggests that everybody requires the stuff
>>>> listed here when it actually depends whether the user is installing the
>>>> binary or not and how the binary was made.
>>>>
>>>>>
>>>>>
>>>>>> Anyway, to answer Kevin's original question:
>>>>>>
>>>>>>   how do I know where the GSL library and header files, packaged
>>>>>>   in GSLR, would live so I can point the compiler at them?
>>>>>>
>>>>>> Use the LinkingTo field.
>>>>>>
>>>>>
>>>>> No, you're not linking to another package, your'e linking to a *library*. LinkingTo uses R's own mechanism for symbol detection in another *package*. I know, the name is a bit misleading but those are two different things.
>>>>
>>>> I understand that but that's what Kevin wants right, i.e. linking
>>>> to another package.
>>>
>>> No (you cannot link to another package - that's the misnomer), he wants to link to a library provided by another package (see his e-mail, he's asking about how to locate the GSL library supplied with RGSL, not to RGSL itself).
>>
>> Maybe I misunderstand what the OP wants to do, and the only way to know
>> would be for him to clarify, but IIUC the RGSL package would only
>> contain the GSL library. At least that's how I understand it (and
>> that's what makes sense to me). So the RGSL package would contain
>> only 1 shared object (or 1 shared object per sub-arch): the GSLR.so
>> file (extension may vary). So I'm not sure what's the difference between
>> linking to the "GSL library supplied with RGSL" and linking to
>> "RGSL itself"?
>>
>
> GSL library inside RGSL is libgsl.* (where * is a, dylib, so, dll depending on the type and OS). This is *not* the same as RGSL.so/dll which would be the package shared object compiled by R for the package. There is a big difference: the shared objects that R creates for packages cannot be linked to, they are meant to be used with dlopen()/dlsym() - which is what R uses to load symbols entry points. That is also the reason why using LinkingTo: requires explicit exposure of symbols by the package providing the symbols as well as explicit loading of the symbols by the package that uses them. This is entirely different than linking to a library - in the latter case the linker (not R) establishes the connection between the symbols in the library and references - and in case of a static library they get copied into the binary that is being created (which is why you don't need it anymore after linking).

OK, thanks for the clarification. I agree with you that putting
libgsl.* inside RGSL sounds like a complicated solution and that
LinkingTo wouldn't work.

FWIW I was suggesting the use of LinkingTo with a setup where RGSL
contains the GSL source code under src/ and the header files
under inst/include/. Plus an extra file under src/ for registering
the GSL API. As mentioned earlier, this is probably not the best
way to go, but that *should* work.

The reason I'm interested in clarifying this is that we are facing
a similar situation with other libraries (e.g. the BOOST library)
used by some Bioconductor packages. Right now, each Bioconductor
package includes its own version of the BOOST source code, which
is of course less than optimal. Ideally we'd want to wrap the BOOST
source (or a subset of it, it's huge!) in something like an rBOOST
package and use a setup similar to what I describe above for RGSL
(i.e. using LinkingTo). Are there better ways? Is there something
like an RcppBOOST package? Sounds like, like for the GSL, it would
be better to install the static BOOST libs on the build machine and
have client packages link against that (but that also means more
complexity in the client packages since they need a configure script).

Thanks,
H.


>
> Cheers,
> Simon
>
>
>> Very confusing to me. Thanks for your time and sorry if I'm missing something obvious.
>>
>> H.
>>
>>>
>>>
>>>> Yes there is the extra difficulty to register all the C functions in the GSL API (as pointed out by Dirk) but that's another story.
>>>>
>>>
>>> That's not another story - that's very much part of the story why using LinkingTo is not a good idea.
>>>
>>> Cheers,
>>> Simon
>>>
>>>
>>>
>>>> Thanks,
>>>> H.
>>>>
>>>>>
>>>>> Cheers,
>>>>> Simon
>>>>>
>>>>>
>>>>>> Cheers,
>>>>>> H.
>>>>>>
>>>>>>>
>>>>>>> Cheers,
>>>>>>> Simon
>>>>>>>
>>>>>>>
>>>>>>>> So, any other suggestions about how this could be accomplished?  Thanks.
>>>>>>>>
>>>>>>>> Kevin
>>>>>>>>
>>>>>>>> On 03/12/2013 05:26 AM, Simon Urbanek wrote:
>>>>>>>>> Kevin,
>>>>>>>>>
>>>>>>>>> On Mar 11, 2013, at 5:20 PM, Kevin Horan wrote:
>>>>>>>>>
>>>>>>>>>> I am developing an R package, eiR,  which depends on another C library, GNU scientific library (GSL). In order to make life easier for the user, it would be nice to not have this as an external dependency, thus I would like to wrap this library in another R package, say GSLR for example. Thus far I know how to do this. The C code in eiR requires the .so library and the header files from GSL in order to compile. So the idea is that eiR would depend on GSLR, then GSLR gets compiled and installed first, then, while eiR is installing, it should be able to make use of the GSL library and header files while compiling. So my question is, how do I know where the GSL library and header files, packaged in GSLR, would live so I can point the compiler at them? I know how to find the installed directory of an R package from within R, but is there way to find that out using just Makevars or a Makefile? I'm open to suggestions about a better way organize all of this as well. I li
 ke
>> t
>>>> he
>>>>>> !
>>>>>>>    idea of keeping the GSL code separate so that it can be updated/changed independently from eiR though.
>>>>>>>>> Have a look at Rcpp.
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>>     I'm also aware of the gsl R library on CRAN, however, this just wraps GSL in R functions, but I need to use the GSL C functions in other C code in eiR.
>>>>>>>>>>
>>>>>>>>> Why is what you are proposing any better than simply using GSL in eiR? You will still need the GSL external dependency for GSLR and you are only adding a lot of complexity by linking into another package's external directory (you cannot use libs) which is in itself very tricky (you'll have to deal with both static and shared version, multi-arch setups, possible relocation etc.). It won't make it any easier on the user, rather to the contrary as there will be more things to break. The only reason Rcpp goes into such length to do this is because it has no choice (the Rcpp library has to use the same libR so cannot be used as external dependency) - I would certainly not recommend it for something as trivial as providing GSL.
>>>>>>>>>
>>>>>>>>> Cheers,
>>>>>>>>> Simon
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>> Thanks.
>>>>>>>>>>
>>>>>>>>>> Kevin
>>>>>>>>>>
>>>>>>>>>> ______________________________________________
>>>>>>>>>> R-devel at r-project.org mailing list
>>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>
>>>>>>> ______________________________________________
>>>>>>> R-devel at r-project.org mailing list
>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>>>>
>>>>>>
>>>>>> --
>>>>>> Herv? Pag?s
>>>>>>
>>>>>> Program in Computational Biology
>>>>>> Division of Public Health Sciences
>>>>>> Fred Hutchinson Cancer Research Center
>>>>>> 1100 Fairview Ave. N, M1-B514
>>>>>> P.O. Box 19024
>>>>>> Seattle, WA 98109-1024
>>>>>>
>>>>>> E-mail: hpages at fhcrc.org
>>>>>> Phone:  (206) 667-5791
>>>>>> Fax:    (206) 667-1319
>>>>>>
>>>>>> ______________________________________________
>>>>>> R-devel at r-project.org mailing list
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>>>
>>>>>>
>>>>>
>>>>
>>>> --
>>>> Herv? Pag?s
>>>>
>>>> Program in Computational Biology
>>>> Division of Public Health Sciences
>>>> Fred Hutchinson Cancer Research Center
>>>> 1100 Fairview Ave. N, M1-B514
>>>> P.O. Box 19024
>>>> Seattle, WA 98109-1024
>>>>
>>>> E-mail: hpages at fhcrc.org
>>>> Phone:  (206) 667-5791
>>>> Fax:    (206) 667-1319
>>>>
>>>>
>>>
>>
>> --
>> Herv? Pag?s
>>
>> Program in Computational Biology
>> Division of Public Health Sciences
>> Fred Hutchinson Cancer Research Center
>> 1100 Fairview Ave. N, M1-B514
>> P.O. Box 19024
>> Seattle, WA 98109-1024
>>
>> E-mail: hpages at fhcrc.org
>> Phone:  (206) 667-5791
>> Fax:    (206) 667-1319
>>
>>
>

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fhcrc.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From edd at debian.org  Tue Mar 12 22:14:20 2013
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue, 12 Mar 2013 16:14:20 -0500
Subject: [Rd] compiling C code using headers from another R package
In-Reply-To: <513F9682.2080503@fhcrc.org>
References: <513E4A98.9080503@cs.ucr.edu>
	<B54A5D4C-2578-4739-BEB5-6B17FA6A9856@r-project.org>
	<513F582B.9090601@cs.ucr.edu>
	<E43810D3-2AEC-4F07-B61A-03923D6D2FC3@r-project.org>
	<513F6D72.3040304@fhcrc.org>
	<5195A4FB-F14C-484C-A732-C98758D0D737@r-project.org>
	<513F7883.7060701@fhcrc.org>
	<CEA0FBC0-908A-4ED4-99AC-83C6BACB61F6@r-project.org>
	<513F8378.4020800@fhcrc.org>
	<C427ED87-4C8E-455F-83FF-9A7FEF4BFAFC@r-project.org>
	<513F9682.2080503@fhcrc.org>
Message-ID: <20799.39596.152428.12460@max.nulle.part>


On 12 March 2013 at 13:56, Herv? Pag?s wrote:
| The reason I'm interested in clarifying this is that we are facing
| a similar situation with other libraries (e.g. the BOOST library)
| used by some Bioconductor packages. Right now, each Bioconductor
| package includes its own version of the BOOST source code, which
| is of course less than optimal. Ideally we'd want to wrap the BOOST
| source (or a subset of it, it's huge!) in something like an rBOOST
| package and use a setup similar to what I describe above for RGSL
| (i.e. using LinkingTo). Are there better ways? Is there something
| like an RcppBOOST package? Sounds like, like for the GSL, it would
| be better to install the static BOOST libs on the build machine and
| have client packages link against that (but that also means more
| complexity in the client packages since they need a configure script).

Are you aware of the BH package on r-forge and now in CRAN?  

It provides Boost __headers__ which is enough for purely template-based
packages.  The bigmemory packages already uses it, and we are collecting
a few 'issue tickets' at the R-Forge page to see which other (CRAN) packages
could use which other Boost packages.  Again, this works for _template_ based
Boost library and excludes eg Boost regex which needs linking.  So this may
not help you if not to also link to Boost Graph.

Dirk

-- 
Dirk Eddelbuettel | edd at debian.org | http://dirk.eddelbuettel.com  


From simon.urbanek at r-project.org  Tue Mar 12 22:15:05 2013
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Tue, 12 Mar 2013 17:15:05 -0400
Subject: [Rd] compiling C code using headers from another R package
In-Reply-To: <513F9682.2080503@fhcrc.org>
References: <513E4A98.9080503@cs.ucr.edu>
	<B54A5D4C-2578-4739-BEB5-6B17FA6A9856@r-project.org>
	<513F582B.9090601@cs.ucr.edu>
	<E43810D3-2AEC-4F07-B61A-03923D6D2FC3@r-project.org>
	<513F6D72.3040304@fhcrc.org>
	<5195A4FB-F14C-484C-A732-C98758D0D737@r-project.org>
	<513F7883.7060701@fhcrc.org>
	<CEA0FBC0-908A-4ED4-99AC-83C6BACB61F6@r-project.org>
	<513F8378.4020800@fhcrc.org>
	<C427ED87-4C8E-455F-83FF-9A7FEF4BFAFC@r-project.org>
	<513F9682.2080503@fhcrc.org>
Message-ID: <248D0D15-A270-4396-8326-DC4F61CAF0AD@r-project.org>


On Mar 12, 2013, at 4:56 PM, Herv? Pag?s wrote:

> 
> 
> On 03/12/2013 12:53 PM, Simon Urbanek wrote:
>> 
>> On Mar 12, 2013, at 3:35 PM, Herv? Pag?s wrote:
>> 
>>> 
>>> 
>>> On 03/12/2013 11:56 AM, Simon Urbanek wrote:
>>>> 
>>>> On Mar 12, 2013, at 2:48 PM, Herv? Pag?s wrote:
>>>> 
>>>>> On 03/12/2013 11:09 AM, Simon Urbanek wrote:
>>>>>> 
>>>>>> On Mar 12, 2013, at 2:01 PM, Herv? Pag?s wrote:
>>>>>> 
>>>>>>> Hi,
>>>>>>> 
>>>>>>> On 03/12/2013 09:55 AM, Simon Urbanek wrote:
>>>>>>>> 
>>>>>>>> On Mar 12, 2013, at 12:30 PM, Kevin Horan wrote:
>>>>>>>> 
>>>>>>>>> 
>>>>>>>>>    Thanks for your input. To clarify, I don't need to use any part of GSL in my R code, nor do I wish to make any part of it accessible to users of eiR. I need it to compile other C/C++ code (LSH KIT), which I did not write, that will itself be used in eiR.
>>>>>>>>>    My goal is allow the user to install eiR without also having to install GSL before hand.
>>>>>>>> 
>>>>>>>> If your package is on CRAN they won't need to as we are providing Mac and Windows binaries.
>>>>>>> 
>>>>>>> I think that at least on Windows, the user would still need to have the
>>>>>>> GSL installed on his/her machine.
>>>>>>> 
>>>>>> 
>>>>>> Why?
>>>>>> 
>>>>>> 
>>>>>>> 
>>>>>>>> Linux can get the binaries form their distro, so the dependencies are installed automatically.
>>>>>>>> 
>>>>>>>> 
>>>>>>>>> The target audience is people in bioinformatics who may not how to install something like GSL. It seems like what I was suggesting is not such a good idea, if it will be hard to reliably find the header files from another R package. I could also push all of GSL into eiR, but as GSL has over 5000 files, this makes the package very large ( >22 MB) and  slow to compile. Both of which are a problem when submitting a package to bioconductor. It may very well be that leaving GSL as an external dependency to eiR is really the best and easiest way, but I just wanted to see if there was any way to make it easier for the user.
>>>>>>>> 
>>>>>>>> Can you clarify what you mean by "user"? The vast majority of R users use binaries, so all this is irrelevant to them as they don't need to install GSL at all.
>>>>>>> 
>>>>>>> FWIW we currently have 7 or 8 Bioconductor packages that require the
>>>>>>> GSL as an external system lib. That's because even if we provide Windows
>>>>>>> binaries for those packages, those binaries are dynamically linked.
>>>>>>> Is there a way to build those binaries that would avoid that dependency?
>>>>>>> 
>>>>>> 
>>>>>> Yes, use static libgsl.
>>>>> 
>>>>> Ah yes, of course. Thanks for the reminder. Just checked with Dan and
>>>>> turns out that we are using that libgsl.a too (made by Brian D. Ripley)
>>>>> on our build system. Some Bioconductor packages have README or INSTALL
>>>>> files that still mention that the Windows user needs to install the GSL
>>>>> but that doesn't seem to be the case so we'll make sure this information
>>>>> gets updated.
>>>>> 
>>>>> Also the SystemRequirements field in the DESCRIPTION file can be a
>>>>> little bit confusing as it suggests that everybody requires the stuff
>>>>> listed here when it actually depends whether the user is installing the
>>>>> binary or not and how the binary was made.
>>>>> 
>>>>>> 
>>>>>> 
>>>>>>> Anyway, to answer Kevin's original question:
>>>>>>> 
>>>>>>>  how do I know where the GSL library and header files, packaged
>>>>>>>  in GSLR, would live so I can point the compiler at them?
>>>>>>> 
>>>>>>> Use the LinkingTo field.
>>>>>>> 
>>>>>> 
>>>>>> No, you're not linking to another package, your'e linking to a *library*. LinkingTo uses R's own mechanism for symbol detection in another *package*. I know, the name is a bit misleading but those are two different things.
>>>>> 
>>>>> I understand that but that's what Kevin wants right, i.e. linking
>>>>> to another package.
>>>> 
>>>> No (you cannot link to another package - that's the misnomer), he wants to link to a library provided by another package (see his e-mail, he's asking about how to locate the GSL library supplied with RGSL, not to RGSL itself).
>>> 
>>> Maybe I misunderstand what the OP wants to do, and the only way to know
>>> would be for him to clarify, but IIUC the RGSL package would only
>>> contain the GSL library. At least that's how I understand it (and
>>> that's what makes sense to me). So the RGSL package would contain
>>> only 1 shared object (or 1 shared object per sub-arch): the GSLR.so
>>> file (extension may vary). So I'm not sure what's the difference between
>>> linking to the "GSL library supplied with RGSL" and linking to
>>> "RGSL itself"?
>>> 
>> 
>> GSL library inside RGSL is libgsl.* (where * is a, dylib, so, dll depending on the type and OS). This is *not* the same as RGSL.so/dll which would be the package shared object compiled by R for the package. There is a big difference: the shared objects that R creates for packages cannot be linked to, they are meant to be used with dlopen()/dlsym() - which is what R uses to load symbols entry points. That is also the reason why using LinkingTo: requires explicit exposure of symbols by the package providing the symbols as well as explicit loading of the symbols by the package that uses them. This is entirely different than linking to a library - in the latter case the linker (not R) establishes the connection between the symbols in the library and references - and in case of a static library they get copied into the binary that is being created (which is why you don't need it anymore after linking).
> 
> OK, thanks for the clarification. I agree with you that putting
> libgsl.* inside RGSL sounds like a complicated solution and that
> LinkingTo wouldn't work.
> 
> FWIW I was suggesting the use of LinkingTo with a setup where RGSL
> contains the GSL source code under src/ and the header files
> under inst/include/. Plus an extra file under src/ for registering
> the GSL API. As mentioned earlier, this is probably not the best
> way to go, but that *should* work.
> 

In theory, yes, but it's less efficient and requires you to get the declarations of the imported symbols right. I'd argue that it's really error prone unless you have some auto-generator for the necessary R API code. This is really intended for R package exposing a few calls, not for re-mapping calls of other libraries through a package.  It doesn't mean you can't do it, but I wouldn't want to maintain such a package :).


> The reason I'm interested in clarifying this is that we are facing
> a similar situation with other libraries (e.g. the BOOST library)
> used by some Bioconductor packages. Right now, each Bioconductor
> package includes its own version of the BOOST source code, which
> is of course less than optimal. Ideally we'd want to wrap the BOOST
> source (or a subset of it, it's huge!) in something like an rBOOST
> package and use a setup similar to what I describe above for RGSL
> (i.e. using LinkingTo). Are there better ways? Is there something
> like an RcppBOOST package? Sounds like, like for the GSL, it would
> be better to install the static BOOST libs on the build machine and
> have client packages link against that

That's what we do on CRAN (at least I do for Mac binaries - I didn't check Windows).

Note that the packages can still use internal version of BOOST regardless. Actually, with boost I'd argue that's not a bad idea, because that way the package knows that it's using a version that works (since there are version compatibility issues with Boost).


> (but that also means more
> complexity in the client packages since they need a configure script).
> 

They don't necessarily - only if there are some special options they want to enable/disable depending on some run-time checks.

Cheers,
Simon



> Thanks,
> H.
> 
> 
>> 
>> Cheers,
>> Simon
>> 
>> 
>>> Very confusing to me. Thanks for your time and sorry if I'm missing something obvious.
>>> 
>>> H.
>>> 
>>>> 
>>>> 
>>>>> Yes there is the extra difficulty to register all the C functions in the GSL API (as pointed out by Dirk) but that's another story.
>>>>> 
>>>> 
>>>> That's not another story - that's very much part of the story why using LinkingTo is not a good idea.
>>>> 
>>>> Cheers,
>>>> Simon
>>>> 
>>>> 
>>>> 
>>>>> Thanks,
>>>>> H.
>>>>> 
>>>>>> 
>>>>>> Cheers,
>>>>>> Simon
>>>>>> 
>>>>>> 
>>>>>>> Cheers,
>>>>>>> H.
>>>>>>> 
>>>>>>>> 
>>>>>>>> Cheers,
>>>>>>>> Simon
>>>>>>>> 
>>>>>>>> 
>>>>>>>>> So, any other suggestions about how this could be accomplished?  Thanks.
>>>>>>>>> 
>>>>>>>>> Kevin
>>>>>>>>> 
>>>>>>>>> On 03/12/2013 05:26 AM, Simon Urbanek wrote:
>>>>>>>>>> Kevin,
>>>>>>>>>> 
>>>>>>>>>> On Mar 11, 2013, at 5:20 PM, Kevin Horan wrote:
>>>>>>>>>> 
>>>>>>>>>>> I am developing an R package, eiR,  which depends on another C library, GNU scientific library (GSL). In order to make life easier for the user, it would be nice to not have this as an external dependency, thus I would like to wrap this library in another R package, say GSLR for example. Thus far I know how to do this. The C code in eiR requires the .so library and the header files from GSL in order to compile. So the idea is that eiR would depend on GSLR, then GSLR gets compiled and installed first, then, while eiR is installing, it should be able to make use of the GSL library and header files while compiling. So my question is, how do I know where the GSL library and header files, packaged in GSLR, would live so I can point the compiler at them? I know how to find the installed directory of an R package from within R, but is there way to find that out using just Makevars or a Makefile? I'm open to suggestions about a better way organize all of this as well. I li
> ke
>>> t
>>>>> he
>>>>>>> !
>>>>>>>>   idea of keeping the GSL code separate so that it can be updated/changed independently from eiR though.
>>>>>>>>>> Have a look at Rcpp.
>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>>>    I'm also aware of the gsl R library on CRAN, however, this just wraps GSL in R functions, but I need to use the GSL C functions in other C code in eiR.
>>>>>>>>>>> 
>>>>>>>>>> Why is what you are proposing any better than simply using GSL in eiR? You will still need the GSL external dependency for GSLR and you are only adding a lot of complexity by linking into another package's external directory (you cannot use libs) which is in itself very tricky (you'll have to deal with both static and shared version, multi-arch setups, possible relocation etc.). It won't make it any easier on the user, rather to the contrary as there will be more things to break. The only reason Rcpp goes into such length to do this is because it has no choice (the Rcpp library has to use the same libR so cannot be used as external dependency) - I would certainly not recommend it for something as trivial as providing GSL.
>>>>>>>>>> 
>>>>>>>>>> Cheers,
>>>>>>>>>> Simon
>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>>> Thanks.
>>>>>>>>>>> 
>>>>>>>>>>> Kevin
>>>>>>>>>>> 
>>>>>>>>>>> ______________________________________________
>>>>>>>>>>> R-devel at r-project.org mailing list
>>>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>>>>>>>> 
>>>>>>>>>>> 
>>>>>>>>> 
>>>>>>>>> 
>>>>>>>> 
>>>>>>>> ______________________________________________
>>>>>>>> R-devel at r-project.org mailing list
>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>>>>> 
>>>>>>> 
>>>>>>> --
>>>>>>> Herv? Pag?s
>>>>>>> 
>>>>>>> Program in Computational Biology
>>>>>>> Division of Public Health Sciences
>>>>>>> Fred Hutchinson Cancer Research Center
>>>>>>> 1100 Fairview Ave. N, M1-B514
>>>>>>> P.O. Box 19024
>>>>>>> Seattle, WA 98109-1024
>>>>>>> 
>>>>>>> E-mail: hpages at fhcrc.org
>>>>>>> Phone:  (206) 667-5791
>>>>>>> Fax:    (206) 667-1319
>>>>>>> 
>>>>>>> ______________________________________________
>>>>>>> R-devel at r-project.org mailing list
>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>>>> 
>>>>>>> 
>>>>>> 
>>>>> 
>>>>> --
>>>>> Herv? Pag?s
>>>>> 
>>>>> Program in Computational Biology
>>>>> Division of Public Health Sciences
>>>>> Fred Hutchinson Cancer Research Center
>>>>> 1100 Fairview Ave. N, M1-B514
>>>>> P.O. Box 19024
>>>>> Seattle, WA 98109-1024
>>>>> 
>>>>> E-mail: hpages at fhcrc.org
>>>>> Phone:  (206) 667-5791
>>>>> Fax:    (206) 667-1319
>>>>> 
>>>>> 
>>>> 
>>> 
>>> --
>>> Herv? Pag?s
>>> 
>>> Program in Computational Biology
>>> Division of Public Health Sciences
>>> Fred Hutchinson Cancer Research Center
>>> 1100 Fairview Ave. N, M1-B514
>>> P.O. Box 19024
>>> Seattle, WA 98109-1024
>>> 
>>> E-mail: hpages at fhcrc.org
>>> Phone:  (206) 667-5791
>>> Fax:    (206) 667-1319
>>> 
>>> 
>> 
> 
> -- 
> Herv? Pag?s
> 
> Program in Computational Biology
> Division of Public Health Sciences
> Fred Hutchinson Cancer Research Center
> 1100 Fairview Ave. N, M1-B514
> P.O. Box 19024
> Seattle, WA 98109-1024
> 
> E-mail: hpages at fhcrc.org
> Phone:  (206) 667-5791
> Fax:    (206) 667-1319
> 
> 


From khoran at cs.ucr.edu  Tue Mar 12 17:30:35 2013
From: khoran at cs.ucr.edu (Kevin Horan)
Date: Tue, 12 Mar 2013 09:30:35 -0700
Subject: [Rd]  compiling C code using headers from another R package
In-Reply-To: <B54A5D4C-2578-4739-BEB5-6B17FA6A9856@r-project.org>
References: <513E4A98.9080503@cs.ucr.edu>
	<B54A5D4C-2578-4739-BEB5-6B17FA6A9856@r-project.org>
Message-ID: <513F582B.9090601@cs.ucr.edu>


     Thanks for your input. To clarify, I don't need to use any part of 
GSL in my R code, nor do I wish to make any part of it accessible to 
users of eiR. I need it to compile other C/C++ code (LSH KIT), which I 
did not write, that will itself be used in eiR.
     My goal is allow the user to install eiR without also having to 
install GSL before hand. The target audience is people in bioinformatics 
who may not how to install something like GSL. It seems like what I was 
suggesting is not such a good idea, if it will be hard to reliably find 
the header files from another R package. I could also push all of GSL 
into eiR, but as GSL has over 5000 files, this makes the package very 
large ( >22 MB) and  slow to compile. Both of which are a problem when 
submitting a package to bioconductor. It may very well be that leaving 
GSL as an external dependency to eiR is really the best and easiest way, 
but I just wanted to see if there was any way to make it easier for the 
user. So, any other suggestions about how this could be accomplished?  
Thanks.

Kevin

On 03/12/2013 05:26 AM, Simon Urbanek wrote:
> Kevin,
>
> On Mar 11, 2013, at 5:20 PM, Kevin Horan wrote:
>
>> I am developing an R package, eiR,  which depends on another C library, GNU scientific library (GSL). In order to make life easier for the user, it would be nice to not have this as an external dependency, thus I would like to wrap this library in another R package, say GSLR for example. Thus far I know how to do this. The C code in eiR requires the .so library and the header files from GSL in order to compile. So the idea is that eiR would depend on GSLR, then GSLR gets compiled and installed first, then, while eiR is installing, it should be able to make use of the GSL library and header files while compiling. So my question is, how do I know where the GSL library and header files, packaged in GSLR, would live so I can point the compiler at them? I know how to find the installed directory of an R package from within R, but is there way to find that out using just Makevars or a Makefile? I'm open to suggestions about a better way organize all of this as well. I like the idea of keeping the GSL code separate so that it can be updated/changed independently from eiR though.
> Have a look at Rcpp.
>
>
>>     I'm also aware of the gsl R library on CRAN, however, this just wraps GSL in R functions, but I need to use the GSL C functions in other C code in eiR.
>>
> Why is what you are proposing any better than simply using GSL in eiR? You will still need the GSL external dependency for GSLR and you are only adding a lot of complexity by linking into another package's external directory (you cannot use libs) which is in itself very tricky (you'll have to deal with both static and shared version, multi-arch setups, possible relocation etc.). It won't make it any easier on the user, rather to the contrary as there will be more things to break. The only reason Rcpp goes into such length to do this is because it has no choice (the Rcpp library has to use the same libR so cannot be used as external dependency) - I would certainly not recommend it for something as trivial as providing GSL.
>
> Cheers,
> Simon
>
>
>> Thanks.
>>
>> Kevin
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>>


From maressyl at gmail.com  Tue Mar 12 17:44:52 2013
From: maressyl at gmail.com (Sylvain Mareschal)
Date: Tue, 12 Mar 2013 17:44:52 +0100
Subject: [Rd] Request for more flexibility in heatmap() width / height ratio
Message-ID: <CAMug1mTpDZKnogHuN8OFaqPrRePgdDJmKZAC+22oH92J52MWSw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20130312/ae1395a2/attachment.pl>

From khoran at cs.ucr.edu  Tue Mar 12 18:03:44 2013
From: khoran at cs.ucr.edu (Kevin Horan)
Date: Tue, 12 Mar 2013 10:03:44 -0700
Subject: [Rd] compiling C code using headers from another R package
In-Reply-To: <E43810D3-2AEC-4F07-B61A-03923D6D2FC3@r-project.org>
References: <513E4A98.9080503@cs.ucr.edu>
	<B54A5D4C-2578-4739-BEB5-6B17FA6A9856@r-project.org>
	<513F582B.9090601@cs.ucr.edu>
	<E43810D3-2AEC-4F07-B61A-03923D6D2FC3@r-project.org>
Message-ID: <513F5FF0.9070806@cs.ucr.edu>



On 03/12/2013 09:55 AM, Simon Urbanek wrote:
> On Mar 12, 2013, at 12:30 PM, Kevin Horan wrote:
>
>>     Thanks for your input. To clarify, I don't need to use any part of GSL in my R code, nor do I wish to make any part of it accessible to users of eiR. I need it to compile other C/C++ code (LSH KIT), which I did not write, that will itself be used in eiR.
>>     My goal is allow the user to install eiR without also having to install GSL before hand.
> If your package is on CRAN they won't need to as we are providing Mac and Windows binaries. Linux can get the binaries form their distro, so the dependencies are installed automatically.
It will (hopefully) be hosted on BioConductor. I suppose they would do 
the same thing? I didn't realize this was the normal case.
>
>
>> The target audience is people in bioinformatics who may not how to install something like GSL. It seems like what I was suggesting is not such a good idea, if it will be hard to reliably find the header files from another R package. I could also push all of GSL into eiR, but as GSL has over 5000 files, this makes the package very large (>22 MB) and  slow to compile. Both of which are a problem when submitting a package to bioconductor. It may very well be that leaving GSL as an external dependency to eiR is really the best and easiest way, but I just wanted to see if there was any way to make it easier for the user.
> Can you clarify what you mean by "user"? The vast majority of R users use binaries, so all this is irrelevant to them as they don't need to install GSL at all.
I just mean biologists installing and using the package. I didn't 
realize they are always distributed in binary form, guess I've had my 
head stuck in the linux world for too long :).
>
> Cheers,
> Simon
>
>
>> So, any other suggestions about how this could be accomplished?  Thanks.
>>
>> Kevin
>>
>> On 03/12/2013 05:26 AM, Simon Urbanek wrote:
>>> Kevin,
>>>
>>> On Mar 11, 2013, at 5:20 PM, Kevin Horan wrote:
>>>
>>>> I am developing an R package, eiR,  which depends on another C library, GNU scientific library (GSL). In order to make life easier for the user, it would be nice to not have this as an external dependency, thus I would like to wrap this library in another R package, say GSLR for example. Thus far I know how to do this. The C code in eiR requires the .so library and the header files from GSL in order to compile. So the idea is that eiR would depend on GSLR, then GSLR gets compiled and installed first, then, while eiR is installing, it should be able to make use of the GSL library and header files while compiling. So my question is, how do I know where the GSL library and header files, packaged in GSLR, would live so I can point the compiler at them? I know how to find the installed directory of an R package from within R, but is there way to find that out using just Makevars or a Makefile? I'm open to suggestions about a better way organize all of this as well. I like the idea of keeping the GSL code separate so that it can be updated/changed independently from eiR though.
>>> Have a look at Rcpp.
>>>
>>>
>>>>     I'm also aware of the gsl R library on CRAN, however, this just wraps GSL in R functions, but I need to use the GSL C functions in other C code in eiR.
>>>>
>>> Why is what you are proposing any better than simply using GSL in eiR? You will still need the GSL external dependency for GSLR and you are only adding a lot of complexity by linking into another package's external directory (you cannot use libs) which is in itself very tricky (you'll have to deal with both static and shared version, multi-arch setups, possible relocation etc.). It won't make it any easier on the user, rather to the contrary as there will be more things to break. The only reason Rcpp goes into such length to do this is because it has no choice (the Rcpp library has to use the same libR so cannot be used as external dependency) - I would certainly not recommend it for something as trivial as providing GSL.
>>>
>>> Cheers,
>>> Simon
>>>
>>>
>>>> Thanks.
>>>>
>>>> Kevin
>>>>
>>>> ______________________________________________
>>>> R-devel at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>
>>>>
>>


From dkesh at qcue.com  Tue Mar 12 22:34:04 2013
From: dkesh at qcue.com (Dan Keshet)
Date: Tue, 12 Mar 2013 17:34:04 -0400
Subject: [Rd] techniques for mocking
Message-ID: <CANhnsHNAxMZuqqk1HDtn0XBtiZbp76oiwrHTfO8em_JKjx5JBQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20130312/240d561f/attachment.pl>

From hpages at fhcrc.org  Tue Mar 12 22:53:52 2013
From: hpages at fhcrc.org (=?ISO-8859-1?Q?Herv=E9_Pag=E8s?=)
Date: Tue, 12 Mar 2013 14:53:52 -0700
Subject: [Rd] compiling C code using headers from another R package
In-Reply-To: <20799.39596.152428.12460@max.nulle.part>
References: <513E4A98.9080503@cs.ucr.edu>
	<B54A5D4C-2578-4739-BEB5-6B17FA6A9856@r-project.org>
	<513F582B.9090601@cs.ucr.edu>
	<E43810D3-2AEC-4F07-B61A-03923D6D2FC3@r-project.org>
	<513F6D72.3040304@fhcrc.org>
	<5195A4FB-F14C-484C-A732-C98758D0D737@r-project.org>
	<513F7883.7060701@fhcrc.org>
	<CEA0FBC0-908A-4ED4-99AC-83C6BACB61F6@r-project.org>
	<513F8378.4020800@fhcrc.org>
	<C427ED87-4C8E-455F-83FF-9A7FEF4BFAFC@r-project.org>
	<513F9682.2080503@fhcrc.org>
	<20799.39596.152428.12460@max.nulle.part>
Message-ID: <513FA3F0.4080409@fhcrc.org>

On 03/12/2013 02:14 PM, Dirk Eddelbuettel wrote:
>
> On 12 March 2013 at 13:56, Herv? Pag?s wrote:
> | The reason I'm interested in clarifying this is that we are facing
> | a similar situation with other libraries (e.g. the BOOST library)
> | used by some Bioconductor packages. Right now, each Bioconductor
> | package includes its own version of the BOOST source code, which
> | is of course less than optimal. Ideally we'd want to wrap the BOOST
> | source (or a subset of it, it's huge!) in something like an rBOOST
> | package and use a setup similar to what I describe above for RGSL
> | (i.e. using LinkingTo). Are there better ways? Is there something
> | like an RcppBOOST package? Sounds like, like for the GSL, it would
> | be better to install the static BOOST libs on the build machine and
> | have client packages link against that (but that also means more
> | complexity in the client packages since they need a configure script).
>
> Are you aware of the BH package on r-forge and now in CRAN?
>
> It provides Boost __headers__ which is enough for purely template-based
> packages.  The bigmemory packages already uses it, and we are collecting
> a few 'issue tickets' at the R-Forge page to see which other (CRAN) packages
> could use which other Boost packages.  Again, this works for _template_ based
> Boost library and excludes eg Boost regex which needs linking.  So this may
> not help you if not to also link to Boost Graph.

That's great to know, thanks! We'll need to check which BioC packages
are purely template-based. Using BH for those certainly sounds like
a better way to go.

H.

>
> Dirk
>

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fhcrc.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From hpages at fhcrc.org  Tue Mar 12 23:25:25 2013
From: hpages at fhcrc.org (=?ISO-8859-1?Q?Herv=E9_Pag=E8s?=)
Date: Tue, 12 Mar 2013 15:25:25 -0700
Subject: [Rd] compiling C code using headers from another R package
In-Reply-To: <248D0D15-A270-4396-8326-DC4F61CAF0AD@r-project.org>
References: <513E4A98.9080503@cs.ucr.edu>
	<B54A5D4C-2578-4739-BEB5-6B17FA6A9856@r-project.org>
	<513F582B.9090601@cs.ucr.edu>
	<E43810D3-2AEC-4F07-B61A-03923D6D2FC3@r-project.org>
	<513F6D72.3040304@fhcrc.org>
	<5195A4FB-F14C-484C-A732-C98758D0D737@r-project.org>
	<513F7883.7060701@fhcrc.org>
	<CEA0FBC0-908A-4ED4-99AC-83C6BACB61F6@r-project.org>
	<513F8378.4020800@fhcrc.org>
	<C427ED87-4C8E-455F-83FF-9A7FEF4BFAFC@r-project.org>
	<513F9682.2080503@fhcrc.org>
	<248D0D15-A270-4396-8326-DC4F61CAF0AD@r-project.org>
Message-ID: <513FAB55.6080106@fhcrc.org>

On 03/12/2013 02:15 PM, Simon Urbanek wrote:
>
> On Mar 12, 2013, at 4:56 PM, Herv? Pag?s wrote:
>
>>
>>
>> On 03/12/2013 12:53 PM, Simon Urbanek wrote:
>>>
>>> On Mar 12, 2013, at 3:35 PM, Herv? Pag?s wrote:
>>>
>>>>
>>>>
>>>> On 03/12/2013 11:56 AM, Simon Urbanek wrote:
>>>>>
>>>>> On Mar 12, 2013, at 2:48 PM, Herv? Pag?s wrote:
>>>>>
>>>>>> On 03/12/2013 11:09 AM, Simon Urbanek wrote:
>>>>>>>
>>>>>>> On Mar 12, 2013, at 2:01 PM, Herv? Pag?s wrote:
>>>>>>>
>>>>>>>> Hi,
>>>>>>>>
>>>>>>>> On 03/12/2013 09:55 AM, Simon Urbanek wrote:
>>>>>>>>>
>>>>>>>>> On Mar 12, 2013, at 12:30 PM, Kevin Horan wrote:
>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>     Thanks for your input. To clarify, I don't need to use any part of GSL in my R code, nor do I wish to make any part of it accessible to users of eiR. I need it to compile other C/C++ code (LSH KIT), which I did not write, that will itself be used in eiR.
>>>>>>>>>>     My goal is allow the user to install eiR without also having to install GSL before hand.
>>>>>>>>>
>>>>>>>>> If your package is on CRAN they won't need to as we are providing Mac and Windows binaries.
>>>>>>>>
>>>>>>>> I think that at least on Windows, the user would still need to have the
>>>>>>>> GSL installed on his/her machine.
>>>>>>>>
>>>>>>>
>>>>>>> Why?
>>>>>>>
>>>>>>>
>>>>>>>>
>>>>>>>>> Linux can get the binaries form their distro, so the dependencies are installed automatically.
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>> The target audience is people in bioinformatics who may not how to install something like GSL. It seems like what I was suggesting is not such a good idea, if it will be hard to reliably find the header files from another R package. I could also push all of GSL into eiR, but as GSL has over 5000 files, this makes the package very large ( >22 MB) and  slow to compile. Both of which are a problem when submitting a package to bioconductor. It may very well be that leaving GSL as an external dependency to eiR is really the best and easiest way, but I just wanted to see if there was any way to make it easier for the user.
>>>>>>>>>
>>>>>>>>> Can you clarify what you mean by "user"? The vast majority of R users use binaries, so all this is irrelevant to them as they don't need to install GSL at all.
>>>>>>>>
>>>>>>>> FWIW we currently have 7 or 8 Bioconductor packages that require the
>>>>>>>> GSL as an external system lib. That's because even if we provide Windows
>>>>>>>> binaries for those packages, those binaries are dynamically linked.
>>>>>>>> Is there a way to build those binaries that would avoid that dependency?
>>>>>>>>
>>>>>>>
>>>>>>> Yes, use static libgsl.
>>>>>>
>>>>>> Ah yes, of course. Thanks for the reminder. Just checked with Dan and
>>>>>> turns out that we are using that libgsl.a too (made by Brian D. Ripley)
>>>>>> on our build system. Some Bioconductor packages have README or INSTALL
>>>>>> files that still mention that the Windows user needs to install the GSL
>>>>>> but that doesn't seem to be the case so we'll make sure this information
>>>>>> gets updated.
>>>>>>
>>>>>> Also the SystemRequirements field in the DESCRIPTION file can be a
>>>>>> little bit confusing as it suggests that everybody requires the stuff
>>>>>> listed here when it actually depends whether the user is installing the
>>>>>> binary or not and how the binary was made.
>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>> Anyway, to answer Kevin's original question:
>>>>>>>>
>>>>>>>>   how do I know where the GSL library and header files, packaged
>>>>>>>>   in GSLR, would live so I can point the compiler at them?
>>>>>>>>
>>>>>>>> Use the LinkingTo field.
>>>>>>>>
>>>>>>>
>>>>>>> No, you're not linking to another package, your'e linking to a *library*. LinkingTo uses R's own mechanism for symbol detection in another *package*. I know, the name is a bit misleading but those are two different things.
>>>>>>
>>>>>> I understand that but that's what Kevin wants right, i.e. linking
>>>>>> to another package.
>>>>>
>>>>> No (you cannot link to another package - that's the misnomer), he wants to link to a library provided by another package (see his e-mail, he's asking about how to locate the GSL library supplied with RGSL, not to RGSL itself).
>>>>
>>>> Maybe I misunderstand what the OP wants to do, and the only way to know
>>>> would be for him to clarify, but IIUC the RGSL package would only
>>>> contain the GSL library. At least that's how I understand it (and
>>>> that's what makes sense to me). So the RGSL package would contain
>>>> only 1 shared object (or 1 shared object per sub-arch): the GSLR.so
>>>> file (extension may vary). So I'm not sure what's the difference between
>>>> linking to the "GSL library supplied with RGSL" and linking to
>>>> "RGSL itself"?
>>>>
>>>
>>> GSL library inside RGSL is libgsl.* (where * is a, dylib, so, dll depending on the type and OS). This is *not* the same as RGSL.so/dll which would be the package shared object compiled by R for the package. There is a big difference: the shared objects that R creates for packages cannot be linked to, they are meant to be used with dlopen()/dlsym() - which is what R uses to load symbols entry points. That is also the reason why using LinkingTo: requires explicit exposure of symbols by the package providing the symbols as well as explicit loading of the symbols by the package that uses them. This is entirely different than linking to a library - in the latter case the linker (not R) establishes the connection between the symbols in the library and references - and in case of a static library they get copied into the binary that is being created (which is why you don't need it anymore after linking).
>>
>> OK, thanks for the clarification. I agree with you that putting
>> libgsl.* inside RGSL sounds like a complicated solution and that
>> LinkingTo wouldn't work.
>>
>> FWIW I was suggesting the use of LinkingTo with a setup where RGSL
>> contains the GSL source code under src/ and the header files
>> under inst/include/. Plus an extra file under src/ for registering
>> the GSL API. As mentioned earlier, this is probably not the best
>> way to go, but that *should* work.
>>
>
> In theory, yes, but it's less efficient and requires you to get the declarations of the imported symbols right. I'd argue that it's really error prone unless you have some auto-generator for the necessary R API code. This is really intended for R package exposing a few calls, not for re-mapping calls of other libraries through a package.  It doesn't mean you can't do it, but I wouldn't want to maintain such a package :).

Yes with a script for auto generating the necessary R API code.
I've done this by hand for a package that exposes 132 symbols
and I know how painful it is. I would certainly not do this by hand
for a beast like GSL or BOOST ;-)

>
>
>> The reason I'm interested in clarifying this is that we are facing
>> a similar situation with other libraries (e.g. the BOOST library)
>> used by some Bioconductor packages. Right now, each Bioconductor
>> package includes its own version of the BOOST source code, which
>> is of course less than optimal. Ideally we'd want to wrap the BOOST
>> source (or a subset of it, it's huge!) in something like an rBOOST
>> package and use a setup similar to what I describe above for RGSL
>> (i.e. using LinkingTo). Are there better ways? Is there something
>> like an RcppBOOST package? Sounds like, like for the GSL, it would
>> be better to install the static BOOST libs on the build machine and
>> have client packages link against that
>
> That's what we do on CRAN (at least I do for Mac binaries - I didn't check Windows).

Good to know. Sounds like we should check the availability of the
BOOST static libs for Windows before we take a decision.

>
> Note that the packages can still use internal version of BOOST regardless.

That's what they do right now. They take a long time to compile. And the
Linux user has to recompile the entire thing every time there is a new
version of the package, even if it's only a comma that was added in the
man page for a function that has nothing to do with BOOST. And this for
every package that uses an internal version of BOOST.

> Actually, with boost I'd argue that's not a bad idea, because that way the package knows that it's using a version that works (since there are version compatibility issues with Boost).

Sounds like an argument maybe in favor of going for the rBOOST solution
over having the BOOST static libs on the build machine. That way the
client package knows exactly what to expect: they take it or they leave
it (i.e. use their own internal BOOST). We've already seen the tricky
situation where 2 BioC packages require 2 different versions of the
same external library. A real pain from a build system maintenance
perspective.

>
>
>> (but that also means more
>> complexity in the client packages since they need a configure script).
>>
>
> They don't necessarily - only if there are some special options they want to enable/disable depending on some run-time checks.

I see.

Thanks,
H.

>
> Cheers,
> Simon
>
>
>
>> Thanks,
>> H.
>>
>>
>>>
>>> Cheers,
>>> Simon
>>>
>>>
>>>> Very confusing to me. Thanks for your time and sorry if I'm missing something obvious.
>>>>
>>>> H.
>>>>
>>>>>
>>>>>
>>>>>> Yes there is the extra difficulty to register all the C functions in the GSL API (as pointed out by Dirk) but that's another story.
>>>>>>
>>>>>
>>>>> That's not another story - that's very much part of the story why using LinkingTo is not a good idea.
>>>>>
>>>>> Cheers,
>>>>> Simon
>>>>>
>>>>>
>>>>>
>>>>>> Thanks,
>>>>>> H.
>>>>>>
>>>>>>>
>>>>>>> Cheers,
>>>>>>> Simon
>>>>>>>
>>>>>>>
>>>>>>>> Cheers,
>>>>>>>> H.
>>>>>>>>
>>>>>>>>>
>>>>>>>>> Cheers,
>>>>>>>>> Simon
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>> So, any other suggestions about how this could be accomplished?  Thanks.
>>>>>>>>>>
>>>>>>>>>> Kevin
>>>>>>>>>>
>>>>>>>>>> On 03/12/2013 05:26 AM, Simon Urbanek wrote:
>>>>>>>>>>> Kevin,
>>>>>>>>>>>
>>>>>>>>>>> On Mar 11, 2013, at 5:20 PM, Kevin Horan wrote:
>>>>>>>>>>>
>>>>>>>>>>>> I am developing an R package, eiR,  which depends on another C library, GNU scientific library (GSL). In order to make life easier for the user, it would be nice to not have this as an external dependency, thus I would like to wrap this library in another R package, say GSLR for example. Thus far I know how to do this. The C code in eiR requires the .so library and the header files from GSL in order to compile. So the idea is that eiR would depend on GSLR, then GSLR gets compiled and installed first, then, while eiR is installing, it should be able to make use of the GSL library and header files while compiling. So my question is, how do I know where the GSL library and header files, packaged in GSLR, would live so I can point the compiler at them? I know how to find the installed directory of an R package from within R, but is there way to find that out using just Makevars or a Makefile? I'm open to suggestions about a better way organize all of this as well. I 
 li
>> ke
>>>> t
>>>>>> he
>>>>>>>> !
>>>>>>>>>    idea of keeping the GSL code separate so that it can be updated/changed independently from eiR though.
>>>>>>>>>>> Have a look at Rcpp.
>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>>>     I'm also aware of the gsl R library on CRAN, however, this just wraps GSL in R functions, but I need to use the GSL C functions in other C code in eiR.
>>>>>>>>>>>>
>>>>>>>>>>> Why is what you are proposing any better than simply using GSL in eiR? You will still need the GSL external dependency for GSLR and you are only adding a lot of complexity by linking into another package's external directory (you cannot use libs) which is in itself very tricky (you'll have to deal with both static and shared version, multi-arch setups, possible relocation etc.). It won't make it any easier on the user, rather to the contrary as there will be more things to break. The only reason Rcpp goes into such length to do this is because it has no choice (the Rcpp library has to use the same libR so cannot be used as external dependency) - I would certainly not recommend it for something as trivial as providing GSL.
>>>>>>>>>>>
>>>>>>>>>>> Cheers,
>>>>>>>>>>> Simon
>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>>> Thanks.
>>>>>>>>>>>>
>>>>>>>>>>>> Kevin
>>>>>>>>>>>>
>>>>>>>>>>>> ______________________________________________
>>>>>>>>>>>> R-devel at r-project.org mailing list
>>>>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>>>>>>>>>
>>>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>
>>>>>>>>> ______________________________________________
>>>>>>>>> R-devel at r-project.org mailing list
>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>>>>>>
>>>>>>>>
>>>>>>>> --
>>>>>>>> Herv? Pag?s
>>>>>>>>
>>>>>>>> Program in Computational Biology
>>>>>>>> Division of Public Health Sciences
>>>>>>>> Fred Hutchinson Cancer Research Center
>>>>>>>> 1100 Fairview Ave. N, M1-B514
>>>>>>>> P.O. Box 19024
>>>>>>>> Seattle, WA 98109-1024
>>>>>>>>
>>>>>>>> E-mail: hpages at fhcrc.org
>>>>>>>> Phone:  (206) 667-5791
>>>>>>>> Fax:    (206) 667-1319
>>>>>>>>
>>>>>>>> ______________________________________________
>>>>>>>> R-devel at r-project.org mailing list
>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>>>>>
>>>>>>>>
>>>>>>>
>>>>>>
>>>>>> --
>>>>>> Herv? Pag?s
>>>>>>
>>>>>> Program in Computational Biology
>>>>>> Division of Public Health Sciences
>>>>>> Fred Hutchinson Cancer Research Center
>>>>>> 1100 Fairview Ave. N, M1-B514
>>>>>> P.O. Box 19024
>>>>>> Seattle, WA 98109-1024
>>>>>>
>>>>>> E-mail: hpages at fhcrc.org
>>>>>> Phone:  (206) 667-5791
>>>>>> Fax:    (206) 667-1319
>>>>>>
>>>>>>
>>>>>
>>>>
>>>> --
>>>> Herv? Pag?s
>>>>
>>>> Program in Computational Biology
>>>> Division of Public Health Sciences
>>>> Fred Hutchinson Cancer Research Center
>>>> 1100 Fairview Ave. N, M1-B514
>>>> P.O. Box 19024
>>>> Seattle, WA 98109-1024
>>>>
>>>> E-mail: hpages at fhcrc.org
>>>> Phone:  (206) 667-5791
>>>> Fax:    (206) 667-1319
>>>>
>>>>
>>>
>>
>> --
>> Herv? Pag?s
>>
>> Program in Computational Biology
>> Division of Public Health Sciences
>> Fred Hutchinson Cancer Research Center
>> 1100 Fairview Ave. N, M1-B514
>> P.O. Box 19024
>> Seattle, WA 98109-1024
>>
>> E-mail: hpages at fhcrc.org
>> Phone:  (206) 667-5791
>> Fax:    (206) 667-1319
>>
>>
>

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fhcrc.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From ripley at stats.ox.ac.uk  Wed Mar 13 11:26:18 2013
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 13 Mar 2013 10:26:18 +0000
Subject: [Rd] Request for more flexibility in heatmap() width / height
 ratio
In-Reply-To: <CAMug1mTpDZKnogHuN8OFaqPrRePgdDJmKZAC+22oH92J52MWSw@mail.gmail.com>
References: <CAMug1mTpDZKnogHuN8OFaqPrRePgdDJmKZAC+22oH92J52MWSw@mail.gmail.com>
Message-ID: <5140544A.6010508@stats.ox.ac.uk>

On 12/03/2013 16:44, Sylvain Mareschal wrote:
> Hi R users,
>
> I'm a quite extensive user of the heatmap() function, and as many others
> i'm frequently frustrated by its fixed square width / height ratio. Here is
> a typical example :
>
>       dev.new()
>       heatmap(matrix(rnorm(1200), nrow=10))
>
> I have a non square matrix to plot with heatmap, as there are many more
> columns than rows, row labels are larger than necessary but column labels
> can't be read (this is quite common in transcriptomics, where there are
> many more genes than samples studied). With a classical plot, this could be
> fixed using a non square device area :
>
>       dev.new(width=10, height=5)
>       heatmap(matrix(rnorm(1200), nrow=10))
>
> But as can be seen, heatmap don't make profit of it and leave empty room on
> each side. To get a readable figure, the user is requested to use a very
> large square device area (export to file becomes mandatory when the screen
> is not large enough), and thus to accommodate with very high rows and tiny
> columns.
>
> This flaw led several useRs to develop alternative functions in external
> packages :
> http://bioinformatics.mdanderson.org/Software/OOMPA/ClassDiscovery/html/aspectHeatmap.html
> http://hosho.ees.hokudai.ac.jp/~kubo/Rdoc/library/gplots/html/heatmap.2.html
>
> But after a quick look at the "stats" package sources, this seems quite
> easy to fix, without breaking backward compatibility :
>
>
> [ src/library/stats/R/dendrogram.R ]
>
> - line 689 : add "width=1" and "height=1" as arguments to heatmap()
> - line 768 : lwid <- c(if(doRdend) 1 else 0.05, 4)
>                  lwid <- c(if(doRdend) 1 else 0.05, width*4)
> - line 769 : lhei <- c((if(doCdend) 1 else 0.05) + if(!is.null(main)) 0.2
> else 0, 4)
>                  lhei <- c((if(doCdend) 1 else 0.05) + if(!is.null(main))
> 0.2 else 0, height*4)
>
>
> As the default values are 1, default behavior does not change and lead to
> square ratio. Changing "width" from 1 to 2 would lead to a figure with
> image() call roughly twice wider than high, and so on.
>
> Smarter default values could also be computed from the matrix size, e.g. :
> width = log(dim(x)[2] / dim(x)[1])
> height = 1
>
> As can be seen the two arguments could be replaced by a single "ratio"
> argument, as the function is using relative layout values ("width = 0.5 /
> height = 1" and "width = 1 / height = 2" have the same effect).
>
>
> So, is there a chance to see this behavior in a future R version or not ?

Yes.  Submit it as a wishlist item at bugs.r-project.org with patches to 
both the R sources and the help page (patches should be against the 
development version of R).  An example would be needed, as part of the 
report or the enhanced help page.

>
>
> Regards,
> Sylvain Mareschal
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From renaud at mancala.cbio.uct.ac.za  Wed Mar 13 12:10:44 2013
From: renaud at mancala.cbio.uct.ac.za (Renaud Gaujoux)
Date: Wed, 13 Mar 2013 13:10:44 +0200
Subject: [Rd] removing union class
Message-ID: <CAHavPHES4xxZ6+wUXBG54ED8Y3OcOyxrCRUEMhJaB2hsepG3Xw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20130313/4f09ce11/attachment.pl>

From therneau at mayo.edu  Wed Mar 13 12:54:15 2013
From: therneau at mayo.edu (Terry Therneau)
Date: Wed, 13 Mar 2013 06:54:15 -0500
Subject: [Rd] R-devel Digest, Vol 121, Issue 13
In-Reply-To: <mailman.27.1363172409.19992.r-devel@r-project.org>
References: <mailman.27.1363172409.19992.r-devel@r-project.org>
Message-ID: <514068E7.9080405@mayo.edu>

I think it would be a good idea.  Several versions of the survival package had a duplicate 
line in the S3methods, and were missing a line that should have been there, due to a 
cut/paste error.
    Terry T.

On 03/13/2013 06:00 AM, r-devel-request at r-project.org wrote:
> Circa 80 CRAN and core-R packages have duplicate export entries in their NAMESPACE files.  E.g.,
>    bit 1.1.9 : c("as.bit", "as.bitwhich", "as.which", "physical", "virtual")
>    forecast 4.1 : "forecast.lm"
>    graphics 2.15.3 : "barplot"
>    mcmc 0.9.1 : "morph"
>    RCurl 1.95.3 : "curlOptions"
>    utils 2.15.3 : "RweaveLatexOptions"
> Would it be helpful for 'check' to alert package writers to this?
>
> I made the list using f():
>    f<- function ()
>    {
>       for(pkg in installed.packages()[,"Package"]) {
>          try( {
>              exports<- parseNamespaceFile(pkg, R.home("library"))$exports
>              if (any(dup<- duplicated(exports))) {
>                  cat(pkg, format(packageVersion(pkg)), ":", deparse(exports[dup]), "\n")
>              }
>          }, silent = TRUE)
>       }
>    }
> I suppose it should also check for duplicates in S3method component, etc.
>
> Bill Dunlap
> Spotfire, TIBCO Software
> wdunlap tibco.com
>


From lukaslehnert at googlemail.com  Wed Mar 13 15:31:10 2013
From: lukaslehnert at googlemail.com (Lukas Lehnert)
Date: Wed, 13 Mar 2013 15:31:10 +0100
Subject: [Rd] Registering method for "t.test"
Message-ID: <1731977.T1ph8suAQ2@kobold>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20130313/b164555d/attachment.pl>

From dtenenba at fhcrc.org  Wed Mar 13 18:17:04 2013
From: dtenenba at fhcrc.org (Dan Tenenbaum)
Date: Wed, 13 Mar 2013 10:17:04 -0700
Subject: [Rd] Failed to locate 'weave' output file / vignette product does
 not have a known filename extension
Message-ID: <CAF42j20rv=oXrCQ7=C=bp8cPQKOUnXjzg8yuE_MQaMzoN04g8g@mail.gmail.com>

Hello,

I'm seeing three different vignette-related errors with recent
versions of R-3.0.0 alpha.

First, with the package BitSeq
(http://bioconductor.org/packages/2.12/bioc/html/BitSeq.html), I get
the following when trying to build the package:

Error: processing vignette ?BitSeq.Rnw' failed with diagnostics:
Failed to locate the ?weave? output file (by engine ?utils::Sweave?)
for vignette with name ?BitSeq?. The following files exists in
directory ?.?: ?data-C0.est?, ?data-C1.est?, ?data-c0b0.prob?,
?data-c0b0.rpkm?, ?data-c0b0.sam?, ?data-c0b0.thetaMeans?,
?data-c0b1.rpkm?, ?data-c1b0.rpkm?, ?data-c1b1.rpkm?, ?data.estVar?,
?data.means?, ?data.par?, ?data.pplr?, ?data.tr?, ?ensSelect1.fasta?,
?ensSelect1.tr?, ?parameters1.txt?
Execution halted

My guess is it's looking for BitSeq.tex, but it's not clear where it's
looking; the package is pretty straightforward, inst/doc contains
BitSeq.Rnw and nothing else (no Makefile).

Next, the package ppiData whose source is here:
https://hedgehog.fhcrc.org/bioc-data/trunk/experiment/pkgs/ppiData
(login: readonly password: readonly)

As you can see, there is no .tex file in the source of the package,
but after running R CMD build on it, the resulting .tar.gz file
contains
ppiData/inst/doc/ppiData.tex
Then the package fails check (excerpt of ppiData.Rcheck/00install.out):

Error in vignette_type(Outfile) :
  Vignette product ?ppiData.tex? does not have a known filename extension (?NA?)
ERROR: installing vignettes failed
* removing ?/home/biocbuild/bbs-2.12-data-experiment/meat/ppiData.Rcheck/ppiData?

This package does not have a vignette Makefile either, so I'm not sure
why the tex file is included in the .tar.gz.

These two errors are with:

> sessionInfo()
R version 3.0.0 alpha (2013-03-09 r62188)
Platform: x86_64-unknown-linux-gnu (64-bit)

locale:
 [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
 [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
 [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8
 [7] LC_PAPER=C                 LC_NAME=C
 [9] LC_ADDRESS=C               LC_TELEPHONE=C
[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base


I also get another error on windows only ( it builds successfully on
Mac and Linux) when trying to build bumphunter
http://bioconductor.org/packages/2.12/bioc/html/bumphunter.html

Warning: running command
'"C:\PROGRA?2\MIKTEX?1.9\miktex\bin\texi2dvi.exe" --quiet --pdf
"bumphunter.tex"  -I
"E:/biocbld/bbs-2.12-bioc/R/share/texmf/tex/latex" -I
"E:/biocbld/bbs-2.12-bioc/R/share/texmf/bibtex/bst"' had status 1
Error in find_vignette_product(name, by = "texi2pdf", engine = engine) :
  Failed to locate the 'texi2pdf' output file (by engine
'utils::Sweave') for vignette with name 'bumphunter'. The following
files exists in directory '.': 'bumphunter-clusterplot.pdf',
'bumphunter-plotSegments.pdf', 'bumphunter.Rnw', 'bumphunter.bib',
'bumphunter.tex'
Calls: <Anonymous> -> find_vignette_product
Execution halted

I guess it's looking for bumphunter.pdf.

This is with:

> sessionInfo()
R version 3.0.0 alpha (2013-03-09 r62188)
Platform: i386-w64-mingw32/i386 (32-bit)

locale:
[1] LC_COLLATE=English_United States.1252
[2] LC_CTYPE=English_United States.1252
[3] LC_MONETARY=English_United States.1252
[4] LC_NUMERIC=C
[5] LC_TIME=English_United States.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

thanks,
Dan


From mtmorgan at fhcrc.org  Wed Mar 13 20:43:59 2013
From: mtmorgan at fhcrc.org (Martin Morgan)
Date: Wed, 13 Mar 2013 12:43:59 -0700
Subject: [Rd] S4 generic not exported correctly / incorrect dispatch?
Message-ID: <5140D6FF.4020708@fhcrc.org>

In this post

   https://stat.ethz.ch/pipermail/bioc-devel/2013-March/004152.html

a package author reports that S4 dispatch fails. I can reproduce this with a 
PkgA (attached; 'intervals' is a relatively light-weight CRAN package) that has 
DESCRIPTION with

Depends: intervals
Imports: graphics

NAMESPACE:

importFrom(graphics, "plot")
export("plot")
exportMethods("plot")

R/tmp.R

setClass("A")
setMethod("plot", "A", function(x, y, ...) {})


and then

 > library(PkgA)
Loading required package: intervals
 > plot
function (x, y, ...)
UseMethod("plot")
<environment: namespace:graphics>

notice that 'plot' is reported as an S3 generic, but should be an S4 generic.

Removing Depends: intervals or changing to importsFrom(intervals, "plot") 
recovers S4 export

 > library(PkgA)
Loading required package: intervals
 > plot
standardGeneric for "plot" defined from package "graphics"

function (x, y, ...)
standardGeneric("plot")
<environment: 0x60aea90>
Methods may be defined for arguments: x, y
Use  showMethods("plot")  for currently available ones.


The 'intervals' package Depends: on methods but nothing else. It defines S3 and 
S4 methods on plot, creating an implicit S4 generic in the process. It's 
NAMESPACE has

S3method( "plot", "Intervals" )
S3method( "plot", "Intervals_full" )
exportMethods("plot")

and we have

 > library(intervals)
 > plot
standardGeneric for "plot" defined from package "graphics"

function (x, y, ...)
standardGeneric("plot")
<environment: 0x68cdc78>
Methods may be defined for arguments: x, y
Use  showMethods("plot")  for currently available ones.

I think everyone is playing by the rules, and that plot should be reported as an 
S4 generic in girafe / PkgA?

 > sessionInfo()
R Under development (unstable) (2013-03-13 r62241)
Platform: x86_64-unknown-linux-gnu (64-bit)

locale:
  [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
  [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
  [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8
  [7] LC_PAPER=C                 LC_NAME=C
  [9] LC_ADDRESS=C               LC_TELEPHONE=C
[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] PkgA_1.2.3       intervals_0.14.0

This is also seen in

 > sessionInfo()
R version 3.0.0 alpha (2013-03-13 r62244)
Platform: x86_64-unknown-linux-gnu (64-bit)


Martin
-- 
Computational Biology / Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N.
PO Box 19024 Seattle, WA 98109

Location: Arnold Building M1 B861
Phone: (206) 667-2793
-------------- next part --------------
A non-text attachment was scrubbed...
Name: PkgA_1.2.3.tar.gz
Type: application/x-gzip
Size: 729 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20130313/56aa2aff/attachment.gz>

From hb at biostat.ucsf.edu  Wed Mar 13 21:16:21 2013
From: hb at biostat.ucsf.edu (Henrik Bengtsson)
Date: Wed, 13 Mar 2013 13:16:21 -0700
Subject: [Rd] Failed to locate 'weave' output file / vignette product
 does not have a known filename extension
In-Reply-To: <CAF42j20rv=oXrCQ7=C=bp8cPQKOUnXjzg8yuE_MQaMzoN04g8g@mail.gmail.com>
References: <CAF42j20rv=oXrCQ7=C=bp8cPQKOUnXjzg8yuE_MQaMzoN04g8g@mail.gmail.com>
Message-ID: <CAFDcVCS+1hW7EGgU4D6K=eRrpV9p4XY6V5+r_kNPSG9uZM=RSw@mail.gmail.com>

Hi,

I contributed code to R r62130 which may be responsible for that (not
sure).  I'll investigate as soon as got the time.

/Henrik

On Wed, Mar 13, 2013 at 10:17 AM, Dan Tenenbaum <dtenenba at fhcrc.org> wrote:
> Hello,
>
> I'm seeing three different vignette-related errors with recent
> versions of R-3.0.0 alpha.
>
> First, with the package BitSeq
> (http://bioconductor.org/packages/2.12/bioc/html/BitSeq.html), I get
> the following when trying to build the package:
>
> Error: processing vignette ?BitSeq.Rnw' failed with diagnostics:
> Failed to locate the ?weave? output file (by engine ?utils::Sweave?)
> for vignette with name ?BitSeq?. The following files exists in
> directory ?.?: ?data-C0.est?, ?data-C1.est?, ?data-c0b0.prob?,
> ?data-c0b0.rpkm?, ?data-c0b0.sam?, ?data-c0b0.thetaMeans?,
> ?data-c0b1.rpkm?, ?data-c1b0.rpkm?, ?data-c1b1.rpkm?, ?data.estVar?,
> ?data.means?, ?data.par?, ?data.pplr?, ?data.tr?, ?ensSelect1.fasta?,
> ?ensSelect1.tr?, ?parameters1.txt?
> Execution halted
>
> My guess is it's looking for BitSeq.tex, but it's not clear where it's
> looking; the package is pretty straightforward, inst/doc contains
> BitSeq.Rnw and nothing else (no Makefile).
>
> Next, the package ppiData whose source is here:
> https://hedgehog.fhcrc.org/bioc-data/trunk/experiment/pkgs/ppiData
> (login: readonly password: readonly)
>
> As you can see, there is no .tex file in the source of the package,
> but after running R CMD build on it, the resulting .tar.gz file
> contains
> ppiData/inst/doc/ppiData.tex
> Then the package fails check (excerpt of ppiData.Rcheck/00install.out):
>
> Error in vignette_type(Outfile) :
>   Vignette product ?ppiData.tex? does not have a known filename extension (?NA?)
> ERROR: installing vignettes failed
> * removing ?/home/biocbuild/bbs-2.12-data-experiment/meat/ppiData.Rcheck/ppiData?
>
> This package does not have a vignette Makefile either, so I'm not sure
> why the tex file is included in the .tar.gz.
>
> These two errors are with:
>
>> sessionInfo()
> R version 3.0.0 alpha (2013-03-09 r62188)
> Platform: x86_64-unknown-linux-gnu (64-bit)
>
> locale:
>  [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
>  [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
>  [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8
>  [7] LC_PAPER=C                 LC_NAME=C
>  [9] LC_ADDRESS=C               LC_TELEPHONE=C
> [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
>
> I also get another error on windows only ( it builds successfully on
> Mac and Linux) when trying to build bumphunter
> http://bioconductor.org/packages/2.12/bioc/html/bumphunter.html
>
> Warning: running command
> '"C:\PROGRA?2\MIKTEX?1.9\miktex\bin\texi2dvi.exe" --quiet --pdf
> "bumphunter.tex"  -I
> "E:/biocbld/bbs-2.12-bioc/R/share/texmf/tex/latex" -I
> "E:/biocbld/bbs-2.12-bioc/R/share/texmf/bibtex/bst"' had status 1
> Error in find_vignette_product(name, by = "texi2pdf", engine = engine) :
>   Failed to locate the 'texi2pdf' output file (by engine
> 'utils::Sweave') for vignette with name 'bumphunter'. The following
> files exists in directory '.': 'bumphunter-clusterplot.pdf',
> 'bumphunter-plotSegments.pdf', 'bumphunter.Rnw', 'bumphunter.bib',
> 'bumphunter.tex'
> Calls: <Anonymous> -> find_vignette_product
> Execution halted
>
> I guess it's looking for bumphunter.pdf.
>
> This is with:
>
>> sessionInfo()
> R version 3.0.0 alpha (2013-03-09 r62188)
> Platform: i386-w64-mingw32/i386 (32-bit)
>
> locale:
> [1] LC_COLLATE=English_United States.1252
> [2] LC_CTYPE=English_United States.1252
> [3] LC_MONETARY=English_United States.1252
> [4] LC_NUMERIC=C
> [5] LC_TIME=English_United States.1252
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> thanks,
> Dan
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From nicebread at gmx.net  Thu Mar 14 09:34:29 2013
From: nicebread at gmx.net (=?iso-8859-1?Q?Felix_Sch=F6nbrodt?=)
Date: Thu, 14 Mar 2013 09:34:29 +0100
Subject: [Rd] Snippets from other packages/ License
Message-ID: <249519AD-6ABB-4253-90C3-12C95CC468F3@gmx.net>

Hello,

I want to use a function from another package (which is GPL>=3), about 20 lines of code, in my own package.
I somewhat hesitate to depend on the entire package just for this single function, but of course I want to credit the original authors.

What would be the best way to do that? Where should I put that credit? Or should I proceed completely different?

Best,
Felix

From manoj.g at isim.net.in  Thu Mar 14 10:51:40 2013
From: manoj.g at isim.net.in (Manoj G)
Date: Thu, 14 Mar 2013 15:21:40 +0530
Subject: [Rd] Viewing Compelete Decision Tree in R - R.2.15.2 - Wndows7 32bit
Message-ID: <CAG5xmxYmcZkdnT85VkUmcdRGy0gRq4PD5OUEC9Us=tFQK7es3Q@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20130314/2560c041/attachment.pl>

From michael.weylandt at gmail.com  Thu Mar 14 12:10:07 2013
From: michael.weylandt at gmail.com (R. Michael Weylandt)
Date: Thu, 14 Mar 2013 11:10:07 +0000
Subject: [Rd] Viewing Compelete Decision Tree in R - R.2.15.2 - Wndows7
	32bit
In-Reply-To: <CAG5xmxYmcZkdnT85VkUmcdRGy0gRq4PD5OUEC9Us=tFQK7es3Q@mail.gmail.com>
References: <CAG5xmxYmcZkdnT85VkUmcdRGy0gRq4PD5OUEC9Us=tFQK7es3Q@mail.gmail.com>
Message-ID: <CAAmySGMAJ6pdrt6jqUTEuAn9HucU2ECJQpMzsCOXYT814XRFyw@mail.gmail.com>

Not an R-devel question.

Wait for a response for your cross post to R-help.

MW

On Thu, Mar 14, 2013 at 9:51 AM, Manoj G <manoj.g at isim.net.in> wrote:
> I tried drawing some decision trees. Since the number of levels are more in
> the tree, the plot result for the decision tree is  not clear and
> conjusted.  If i save it as pdf or png also, the image is same.
>
> So how can i view the complete clear plot of decision tree?
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From brian at braverock.com  Thu Mar 14 12:40:32 2013
From: brian at braverock.com (Brian G. Peterson)
Date: Thu, 14 Mar 2013 06:40:32 -0500
Subject: [Rd] Snippets from other packages/ License
In-Reply-To: <249519AD-6ABB-4253-90C3-12C95CC468F3@gmx.net>
References: <249519AD-6ABB-4253-90C3-12C95CC468F3@gmx.net>
Message-ID: <5141B730.6030705@braverock.com>

On 03/14/2013 03:34 AM, Felix Sch?nbrodt wrote:
> I want to use a function from another package (which is GPL>=3),
> about 20 lines of code, in my own package. I somewhat hesitate to
> depend on the entire package just for this single function, but of
> course I want to credit the original authors.
>
> What would be the best way to do that? Where should I put that
> credit? Or should I proceed completely different?

You could use 'Imports' if you will always use that code for your package.

You could use 'Suggests' with appropriate tests to see if it is
available if you only need it occasionally, or for only one function.

If you wish to fork the original code and include the code directly in
your package, then your package will also need to be GPL>=3, you will 
need to list the authors of that code as Contributors in your 
DESCRIPTION file, and I would strongly recommend that you place this 
code in a separate .R file in your package, with a separate Copyright 
and License comment block in that separate file stating the origins and 
license for this code snippet.

That should satisfy both the spirit and the letter of the GPL.

Cheers,

Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From h.wickham at gmail.com  Thu Mar 14 16:52:00 2013
From: h.wickham at gmail.com (Hadley Wickham)
Date: Thu, 14 Mar 2013 10:52:00 -0500
Subject: [Rd] Snippets from other packages/ License
In-Reply-To: <5141B730.6030705@braverock.com>
References: <249519AD-6ABB-4253-90C3-12C95CC468F3@gmx.net>
	<5141B730.6030705@braverock.com>
Message-ID: <CABdHhvFeDGac+CY1-hftx=mbn0xOtyPqxVJ8Pg6o_147PbPZ+w@mail.gmail.com>

> If you wish to fork the original code and include the code directly in
> your package, then your package will also need to be GPL>=3, you will need
> to list the authors of that code as Contributors in your DESCRIPTION file,
> and I would strongly recommend that you place this code in a separate .R
> file in your package, with a separate Copyright and License comment block in
> that separate file stating the origins and license for this code snippet.

Another option is to email the original author and ask if they'd be
willing to license the code to you under whatever license your package
currently uses.

Hadley

-- 
Chief Scientist, RStudio
http://had.co.nz/


From tlumley at uw.edu  Fri Mar 15 01:04:29 2013
From: tlumley at uw.edu (Thomas Lumley)
Date: Fri, 15 Mar 2013 13:04:29 +1300
Subject: [Rd] Registering method for "t.test"
In-Reply-To: <1731977.T1ph8suAQ2@kobold>
References: <1731977.T1ph8suAQ2@kobold>
Message-ID: <CAJ55+dJzbpyTrpOfc+Ldyek-_rFRs9A1cmAtB=JwwxiDYbvibg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20130315/b68856b8/attachment.pl>

From nicebread at gmx.net  Fri Mar 15 08:41:41 2013
From: nicebread at gmx.net (=?windows-1252?Q?Felix_Sch=F6nbrodt?=)
Date: Fri, 15 Mar 2013 08:41:41 +0100
Subject: [Rd] Snippets from other packages/ License
In-Reply-To: <CABdHhvFeDGac+CY1-hftx=mbn0xOtyPqxVJ8Pg6o_147PbPZ+w@mail.gmail.com>
References: <249519AD-6ABB-4253-90C3-12C95CC468F3@gmx.net>
	<5141B730.6030705@braverock.com>
	<CABdHhvFeDGac+CY1-hftx=mbn0xOtyPqxVJ8Pg6o_147PbPZ+w@mail.gmail.com>
Message-ID: <1F06EEAC-A6A6-4626-8278-1BCD36F6DFF5@gmx.net>

Thanks Brian and Hadley,

these all seem to be reasonable approaches! 
(For the current package I think I'll depend on the other one, but it's good to know how to proceed for future situations ?).

Felix

Am 14.03.2013 um 16:52 schrieb Hadley Wickham <h.wickham at gmail.com>:

>> If you wish to fork the original code and include the code directly in
>> your package, then your package will also need to be GPL>=3, you will need
>> to list the authors of that code as Contributors in your DESCRIPTION file,
>> and I would strongly recommend that you place this code in a separate .R
>> file in your package, with a separate Copyright and License comment block in
>> that separate file stating the origins and license for this code snippet.
> 
> Another option is to email the original author and ask if they'd be
> willing to license the code to you under whatever license your package
> currently uses.
> 
> Hadley
> 
> -- 
> Chief Scientist, RStudio
> http://had.co.nz/
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From maechler at stat.math.ethz.ch  Fri Mar 15 11:39:18 2013
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 15 Mar 2013 11:39:18 +0100
Subject: [Rd] removing union class
In-Reply-To: <CAHavPHES4xxZ6+wUXBG54ED8Y3OcOyxrCRUEMhJaB2hsepG3Xw@mail.gmail.com>
References: <CAHavPHES4xxZ6+wUXBG54ED8Y3OcOyxrCRUEMhJaB2hsepG3Xw@mail.gmail.com>
Message-ID: <20802.64086.443586.813352@stat.math.ethz.ch>

>>>>> Renaud Gaujoux <renaud at mancala.cbio.uct.ac.za>
>>>>>     on Wed, 13 Mar 2013 13:10:44 +0200 writes:

    > Hi,
    > I get the following error when trying to remove a union class:

    >> setClassUnion('a', c('matrix', 'numeric'))
    >> removeClass('a')
    >> sessionInfo()
    > R version 2.15.3 (2013-03-01)
    > Platform: i686-pc-linux-gnu (32-bit)

    > locale:
    > [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
    > [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
    > [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8
    > [7] LC_PAPER=C                 LC_NAME=C
    > [9] LC_ADDRESS=C               LC_TELEPHONE=C
    > [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C

    > attached base packages:
    > [1] stats     graphics  grDevices utils     datasets  methods   base


    > Is this normal? Am I doing something wrong?

Well,... you forgot to show the error (and the traceback) :

  > setClassUnion('a', c('matrix', 'numeric'))
  > removeClass('a')
  Error in .getClassFromCache(Class, where) : node stack overflow

  > traceback()
  1888: .getClassFromCache(Class, where)
  1887: getClass(cl)
  1886: checkAtAssignment("classRepresentation", "contains", "list")
  1885: .deleteSuperClass(cdef, superclass)
  1884: .removeSuperClass(subclass, superclass)
  ..........
  ..........
  3: .deleteSuperClass(cdef, superclass)
  2: .removeSuperClass(what, Class)
  1: removeClass("a")
  > 

So that looks indeed like a bug in R's  removeClass() or its
helper functions.
Note that this problem is somewhat dependent on the use of the 
infamous "matrix" class {not properly defined as a class in S3,
as it may or may not have dimnames, and then tried to be made S4
compatible "as well as possible" in the methods package, see

  getClass("matrix")

Of course, I could not have thought of a realistic situation
where this was a problem, but then you exemplify one :

    > Hadley, this is problematic for devtools, because load_all tries to cleanup
    > S4 classes when an error occurs when loading a development package and
    > crashes with no hint on the original error.

    > Thank you.

    > Bests,
    > Renaud

Thank you for the report... although it would have been great if
it had come earlier, as we are approaching feature freeze for
3.0.0 very rapidly.

Martin Maechler, ETH Zurich


From b.rowlingson at lancaster.ac.uk  Fri Mar 15 11:43:06 2013
From: b.rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 15 Mar 2013 10:43:06 +0000
Subject: [Rd] Snippets from other packages/ License
In-Reply-To: <a48847374c644c08923d8fea217d36d4@EX-0-HT0.lancs.local>
References: <a48847374c644c08923d8fea217d36d4@EX-0-HT0.lancs.local>
Message-ID: <CANVKczMj+Ps_sQXe3O98xbYV_Eco-MSWRfDZnLDHQPq+wgQiDA@mail.gmail.com>

On Thu, Mar 14, 2013 at 8:34 AM, Felix Sch?nbrodt <nicebread at gmx.net> wrote:
> Hello,
>
> I want to use a function from another package (which is GPL>=3), about 20 lines of code, in my own package.
> I somewhat hesitate to depend on the entire package just for this single function, but of course I want to credit the original authors.
>
> What would be the best way to do that? Where should I put that credit? Or should I proceed completely different?

 Why so shy about the name and location of this function?

 I think I've seen cases like this before, where a package has a handy
little function that has use outside the context of the package. In
that case I think the best thing is often for that function to be
taken out of that package completely, and put in a new package,
because it isn't so tightly coupled to the aims of the package.

 Obviously this is overkill for one function, but its possible there's
a few functions, or that they are handy enough to belong somewhere
else, such as the plotrix package if its a little plotting function,
or in one of Hadley's packages if it has an underscore in the name.

Barry


From maechler at stat.math.ethz.ch  Fri Mar 15 11:52:50 2013
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 15 Mar 2013 11:52:50 +0100
Subject: [Rd] removing union class
In-Reply-To: <20802.64086.443586.813352@stat.math.ethz.ch>
References: <CAHavPHES4xxZ6+wUXBG54ED8Y3OcOyxrCRUEMhJaB2hsepG3Xw@mail.gmail.com>
	<20802.64086.443586.813352@stat.math.ethz.ch>
Message-ID: <20802.64898.70553.300100@stat.math.ethz.ch>

>>>>> Martin Maechler <maechler at stat.math.ethz.ch>
>>>>>     on Fri, 15 Mar 2013 11:39:18 +0100 writes:

>>>>> Renaud Gaujoux <renaud at mancala.cbio.uct.ac.za>
>>>>>     on Wed, 13 Mar 2013 13:10:44 +0200 writes:

    >> Hi,
    >> I get the following error when trying to remove a union class:

    >>> setClassUnion('a', c('matrix', 'numeric'))
    >>> removeClass('a')
    >>> sessionInfo()
    >> R version 2.15.3 (2013-03-01)
    >> Platform: i686-pc-linux-gnu (32-bit)

    >> locale:
    >> [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
    >> [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
    >> [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8
    >> [7] LC_PAPER=C                 LC_NAME=C
    >> [9] LC_ADDRESS=C               LC_TELEPHONE=C
    >> [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C

    >> attached base packages:
    >> [1] stats     graphics  grDevices utils     datasets  methods   base


    >> Is this normal? Am I doing something wrong?

    > Well,... you forgot to show the error (and the traceback) :

    >> setClassUnion('a', c('matrix', 'numeric'))
    >> removeClass('a')
    > Error in .getClassFromCache(Class, where) : node stack overflow

    >> traceback()
    > 1888: .getClassFromCache(Class, where)
    > 1887: getClass(cl)
    > 1886: checkAtAssignment("classRepresentation", "contains", "list")
    > 1885: .deleteSuperClass(cdef, superclass)
    > 1884: .removeSuperClass(subclass, superclass)
    > ..........
    > ..........
    > 3: .deleteSuperClass(cdef, superclass)
    > 2: .removeSuperClass(what, Class)
    > 1: removeClass("a")
    >> 

    > So that looks indeed like a bug in R's  removeClass() or its
    > helper functions.

or in the definition of  "matrix" and "array"  as   S4-like
classes.

The infinite recursion happens because 
    "matrix" has "array" as subclass
and "array"  has "matrix" ....
which by logic and usual set theory semantic would entail that
"array" and "matrix" must be the same class... 
:-)

Well have to see how to resolve this as cleanly as possible.
Martin

    > Note that this problem is somewhat dependent on the use of the 
    > infamous "matrix" class {not properly defined as a class in S3,
    > as it may or may not have dimnames, and then tried to be made S4
    > compatible "as well as possible" in the methods package, see

    > getClass("matrix")

    > Of course, I could not have thought of a realistic situation
    > where this was a problem, but then you exemplify one :

    >> Hadley, this is problematic for devtools, because load_all tries to cleanup
    >> S4 classes when an error occurs when loading a development package and
    >> crashes with no hint on the original error.

    >> Thank you.

    >> Bests,
    >> Renaud

    > Thank you for the report... although it would have been great if
    > it had come earlier, as we are approaching feature freeze for
    > 3.0.0 very rapidly.

    > Martin Maechler, ETH Zurich

    > ______________________________________________
    > R-devel at r-project.org mailing list
    > https://stat.ethz.ch/mailman/listinfo/r-devel


From maechler at stat.math.ethz.ch  Fri Mar 15 12:42:25 2013
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 15 Mar 2013 12:42:25 +0100
Subject: [Rd] S4 generic not exported correctly / incorrect dispatch?
In-Reply-To: <5140D6FF.4020708@fhcrc.org>
References: <5140D6FF.4020708@fhcrc.org>
Message-ID: <20803.2337.525773.878602@stat.math.ethz.ch>

>>>>> Martin Morgan <mtmorgan at fhcrc.org>
>>>>>     on Wed, 13 Mar 2013 12:43:59 -0700 writes:

    > In this post
    > https://stat.ethz.ch/pipermail/bioc-devel/2013-March/004152.html

    > a package author reports that S4 dispatch fails. I can reproduce this with a 
    > PkgA (attached; 'intervals' is a relatively light-weight CRAN package) that has 
    > DESCRIPTION with

    > Depends: intervals
    > Imports: graphics

    > NAMESPACE:

    > importFrom(graphics, "plot")
    > export("plot")
    > exportMethods("plot")

    > R/tmp.R

    > setClass("A")
    > setMethod("plot", "A", function(x, y, ...) {})


    > and then

    >> library(PkgA)
    > Loading required package: intervals
    >> plot
    > function (x, y, ...)
    > UseMethod("plot")
    > <environment: namespace:graphics>

    > notice that 'plot' is reported as an S3 generic, but should be an S4 generic.

    > Removing Depends: intervals or changing to importsFrom(intervals, "plot") 
    > recovers S4 export

Indeed.

What happens is the following:

Because of the 'Depends: intervals',
the setMethod("plot", A", ...) 
is *NOT* implicitly creating a new S4 plot() generic,
but rather sees the existing S4 plot generic in 'intervals' and
attaches its method there.
But as "you" fail to import plot from intervals, that is not
"seen" because it is masked by the S3 generic plot which you do
explicitly import from graphics.

If you (well the 'girafe' package author) really want to define
and export both S3 and S4 generics for plot, you should
ensure that you either import an S4 generic (from 'intervals', e.g.),
*or* that you explicitly create one yourself, using
setGeneric("plot", ..)  to be in your own name space.
[But of course, the latter is not really what you'd want, I think].

Why are you opposed to   
    importsFrom(intervals, "plot") 
?

I agree that it is ``asymmetric'' that you must think about how
to get plot as S4 generic,  *because* you depend on a package
that defines plot as S4 generic,
whereas that package does not have to do the same.

I think we have to live with this infelicity of 
interaction of namespace and S4 designs.

Martin

    >> library(PkgA)
    > Loading required package: intervals
    >> plot
    > standardGeneric for "plot" defined from package "graphics"

    > function (x, y, ...)
    > standardGeneric("plot")
    > <environment: 0x60aea90>
    > Methods may be defined for arguments: x, y
    > Use  showMethods("plot")  for currently available ones.


    > The 'intervals' package Depends: on methods but nothing else. It defines S3 and 
    > S4 methods on plot, creating an implicit S4 generic in the process. It's 
    > NAMESPACE has

    > S3method( "plot", "Intervals" )
    > S3method( "plot", "Intervals_full" )
    > exportMethods("plot")

    > and we have

    >> library(intervals)
    >> plot
    > standardGeneric for "plot" defined from package "graphics"

    > function (x, y, ...)
    > standardGeneric("plot")
    > <environment: 0x68cdc78>
    > Methods may be defined for arguments: x, y
    > Use  showMethods("plot")  for currently available ones.

    > I think everyone is playing by the rules, and that plot should be reported as an 
    > S4 generic in girafe / PkgA?

    >> sessionInfo()
    > R Under development (unstable) (2013-03-13 r62241)
    > Platform: x86_64-unknown-linux-gnu (64-bit)

    > locale:
    > [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
    > [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
    > [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8
    > [7] LC_PAPER=C                 LC_NAME=C
    > [9] LC_ADDRESS=C               LC_TELEPHONE=C
    > [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C

    > attached base packages:
    > [1] stats     graphics  grDevices utils     datasets  methods   base

    > other attached packages:
    > [1] PkgA_1.2.3       intervals_0.14.0

    > This is also seen in

    >> sessionInfo()
    > R version 3.0.0 alpha (2013-03-13 r62244)
    > Platform: x86_64-unknown-linux-gnu (64-bit)


    > Martin
    > -- 
    > Computational Biology / Fred Hutchinson Cancer Research Center
    > 1100 Fairview Ave. N.
    > PO Box 19024 Seattle, WA 98109

    > Location: Arnold Building M1 B861
    > Phone: (206) 667-2793
    > x[DELETED ATTACHMENT external: PkgA_1.2.3.tar.gz, application/x-gzip]
    > ______________________________________________
    > R-devel at r-project.org mailing list
    > https://stat.ethz.ch/mailman/listinfo/r-devel


From lukaslehnert at googlemail.com  Fri Mar 15 13:07:35 2013
From: lukaslehnert at googlemail.com (Lukas Lehnert)
Date: Fri, 15 Mar 2013 13:07:35 +0100
Subject: [Rd] Registering method for "t.test"
In-Reply-To: <CAJ55+dJzbpyTrpOfc+Ldyek-_rFRs9A1cmAtB=JwwxiDYbvibg@mail.gmail.com>
References: <1731977.T1ph8suAQ2@kobold>
	<CAJ55+dJzbpyTrpOfc+Ldyek-_rFRs9A1cmAtB=JwwxiDYbvibg@mail.gmail.com>
Message-ID: <2077173.ZiMAUUv6iA@kobold>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20130315/54f94331/attachment.pl>

From mtmorgan at fhcrc.org  Fri Mar 15 14:54:26 2013
From: mtmorgan at fhcrc.org (Martin Morgan)
Date: Fri, 15 Mar 2013 06:54:26 -0700
Subject: [Rd] S4 generic not exported correctly / incorrect dispatch?
In-Reply-To: <20803.2337.525773.878602@stat.math.ethz.ch>
References: <5140D6FF.4020708@fhcrc.org>
	<20803.2337.525773.878602@stat.math.ethz.ch>
Message-ID: <51432812.6050701@fhcrc.org>

On 03/15/2013 04:42 AM, Martin Maechler wrote:
>>>>>> Martin Morgan <mtmorgan at fhcrc.org>
>>>>>>      on Wed, 13 Mar 2013 12:43:59 -0700 writes:
>
>      > In this post
>      > https://stat.ethz.ch/pipermail/bioc-devel/2013-March/004152.html
>
>      > a package author reports that S4 dispatch fails. I can reproduce this with a
>      > PkgA (attached; 'intervals' is a relatively light-weight CRAN package) that has
>      > DESCRIPTION with
>
>      > Depends: intervals
>      > Imports: graphics
>
>      > NAMESPACE:
>
>      > importFrom(graphics, "plot")
>      > export("plot")
>      > exportMethods("plot")
>
>      > R/tmp.R
>
>      > setClass("A")
>      > setMethod("plot", "A", function(x, y, ...) {})
>
>
>      > and then
>
>      >> library(PkgA)
>      > Loading required package: intervals
>      >> plot
>      > function (x, y, ...)
>      > UseMethod("plot")
>      > <environment: namespace:graphics>
>
>      > notice that 'plot' is reported as an S3 generic, but should be an S4 generic.
>
>      > Removing Depends: intervals or changing to importsFrom(intervals, "plot")
>      > recovers S4 export
>
> Indeed.
>
> What happens is the following:
>
> Because of the 'Depends: intervals',
> the setMethod("plot", A", ...)
> is *NOT* implicitly creating a new S4 plot() generic,
> but rather sees the existing S4 plot generic in 'intervals' and
> attaches its method there.
> But as "you" fail to import plot from intervals, that is not
> "seen" because it is masked by the S3 generic plot which you do
> explicitly import from graphics.

Thanks for your help and insight, Martin.

There _was_ a change to intervals, overlooked in the other various moving parts, 
with exportMethods(plot) added to the NAMESPACE.

So setMethod is finding and adding to the generic on the search path, but 
exporting the (not implicitly promoted) S3 generic imported from graphics.

The 'surprise' is that plot on the search path wins out over plot in the name space.

I think (?) I would have been 'surprised' later when PkgA's exportMethods(plot) 
did not merge the methods on the implicit generic in PkgA with the implicit 
generic on the search path, especially if PkgA had not also export(plot).

>
> If you (well the 'girafe' package author) really want to define
> and export both S3 and S4 generics for plot, you should
> ensure that you either import an S4 generic (from 'intervals', e.g.),
> *or* that you explicitly create one yourself, using
> setGeneric("plot", ..)  to be in your own name space.
> [But of course, the latter is not really what you'd want, I think].
>
> Why are you opposed to
>      importsFrom(intervals, "plot")
> ?

Conceptually, I'm implementing a method orthogonal to what other packages are doing.

Practically, I'd like my name space to insulate me from what other packages do.

Thanks a lot for your help, I understand what is going on.

Martin

>
> I agree that it is ``asymmetric'' that you must think about how
> to get plot as S4 generic,  *because* you depend on a package
> that defines plot as S4 generic,
> whereas that package does not have to do the same.
>
> I think we have to live with this infelicity of
> interaction of namespace and S4 designs.
>
> Martin
>
>      >> library(PkgA)
>      > Loading required package: intervals
>      >> plot
>      > standardGeneric for "plot" defined from package "graphics"
>
>      > function (x, y, ...)
>      > standardGeneric("plot")
>      > <environment: 0x60aea90>
>      > Methods may be defined for arguments: x, y
>      > Use  showMethods("plot")  for currently available ones.
>
>
>      > The 'intervals' package Depends: on methods but nothing else. It defines S3 and
>      > S4 methods on plot, creating an implicit S4 generic in the process. It's
>      > NAMESPACE has
>
>      > S3method( "plot", "Intervals" )
>      > S3method( "plot", "Intervals_full" )
>      > exportMethods("plot")
>
>      > and we have
>
>      >> library(intervals)
>      >> plot
>      > standardGeneric for "plot" defined from package "graphics"
>
>      > function (x, y, ...)
>      > standardGeneric("plot")
>      > <environment: 0x68cdc78>
>      > Methods may be defined for arguments: x, y
>      > Use  showMethods("plot")  for currently available ones.
>
>      > I think everyone is playing by the rules, and that plot should be reported as an
>      > S4 generic in girafe / PkgA?
>
>      >> sessionInfo()
>      > R Under development (unstable) (2013-03-13 r62241)
>      > Platform: x86_64-unknown-linux-gnu (64-bit)
>
>      > locale:
>      > [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
>      > [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
>      > [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8
>      > [7] LC_PAPER=C                 LC_NAME=C
>      > [9] LC_ADDRESS=C               LC_TELEPHONE=C
>      > [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C
>
>      > attached base packages:
>      > [1] stats     graphics  grDevices utils     datasets  methods   base
>
>      > other attached packages:
>      > [1] PkgA_1.2.3       intervals_0.14.0
>
>      > This is also seen in
>
>      >> sessionInfo()
>      > R version 3.0.0 alpha (2013-03-13 r62244)
>      > Platform: x86_64-unknown-linux-gnu (64-bit)
>
>
>      > Martin
>      > --
>      > Computational Biology / Fred Hutchinson Cancer Research Center
>      > 1100 Fairview Ave. N.
>      > PO Box 19024 Seattle, WA 98109
>
>      > Location: Arnold Building M1 B861
>      > Phone: (206) 667-2793
>      > x[DELETED ATTACHMENT external: PkgA_1.2.3.tar.gz, application/x-gzip]
>      > ______________________________________________
>      > R-devel at r-project.org mailing list
>      > https://stat.ethz.ch/mailman/listinfo/r-devel
>


-- 
Computational Biology / Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N.
PO Box 19024 Seattle, WA 98109

Location: Arnold Building M1 B861
Phone: (206) 667-2793


From hb at biostat.ucsf.edu  Fri Mar 15 18:09:10 2013
From: hb at biostat.ucsf.edu (Henrik Bengtsson)
Date: Fri, 15 Mar 2013 10:09:10 -0700
Subject: [Rd] Failed to locate 'weave' output file / vignette product
 does not have a known filename extension
In-Reply-To: <CAFDcVCS+1hW7EGgU4D6K=eRrpV9p4XY6V5+r_kNPSG9uZM=RSw@mail.gmail.com>
References: <CAF42j20rv=oXrCQ7=C=bp8cPQKOUnXjzg8yuE_MQaMzoN04g8g@mail.gmail.com>
	<CAFDcVCS+1hW7EGgU4D6K=eRrpV9p4XY6V5+r_kNPSG9uZM=RSw@mail.gmail.com>
Message-ID: <CAFDcVCScw2JkxruOg43GkHxWSbmwMkQwyOxSLE7a9h2GaJqMnQ@mail.gmail.com>

Hi.

I can reproduce 2 out of the 3 cases on R version 3.0.0 alpha
(2013-03-12 r62224) [Platform: x86_64-w64-mingw32/x64 (64-bit)].  The
error does not appear to be in the R vignette machinery but rather in
the vignettes themselves (or the packages they're using).  Comments
below.


On Wed, Mar 13, 2013 at 1:16 PM, Henrik Bengtsson <hb at biostat.ucsf.edu> wrote:
> Hi,
>
> I contributed code to R r62130 which may be responsible for that (not
> sure).  I'll investigate as soon as got the time.
>
> /Henrik
>
> On Wed, Mar 13, 2013 at 10:17 AM, Dan Tenenbaum <dtenenba at fhcrc.org> wrote:
>> Hello,
>>
>> I'm seeing three different vignette-related errors with recent
>> versions of R-3.0.0 alpha.
>>


#############################################
# Case #1: BitSeq
#############################################

>> First, with the package BitSeq
>> (http://bioconductor.org/packages/2.12/bioc/html/BitSeq.html), I get
>> the following when trying to build the package:
>>
>> Error: processing vignette ?BitSeq.Rnw' failed with diagnostics:
>> Failed to locate the ?weave? output file (by engine ?utils::Sweave?)
>> for vignette with name ?BitSeq?. The following files exists in
>> directory ?.?: ?data-C0.est?, ?data-C1.est?, ?data-c0b0.prob?,
>> ?data-c0b0.rpkm?, ?data-c0b0.sam?, ?data-c0b0.thetaMeans?,
>> ?data-c0b1.rpkm?, ?data-c1b0.rpkm?, ?data-c1b1.rpkm?, ?data.estVar?,
>> ?data.means?, ?data.par?, ?data.pplr?, ?data.tr?, ?ensSelect1.fasta?,
>> ?ensSelect1.tr?, ?parameters1.txt?
>> Execution halted
>>
>> My guess is it's looking for BitSeq.tex, but it's not clear where it's
>> looking; the package is pretty straightforward, inst/doc contains
>> BitSeq.Rnw and nothing else (no Makefile).
>>

I failed to reproduce this when running the vignette manually;

> setwd("BitSeq/inst/doc/")
> Sweave("BitSeq.Rnw")
> tools:::find_vignette_product("BitSeq", by="weave")
[1] "BitSeq.tex"


However, when I look for the files listed in your error message I
don't find them in ".", but I do find them in
system.file("extdata",package="BitSeq").  Inspecting 'BitSeq.Rnw', I
find

# move to directory with the data
setwd(system.file("extdata",package="BitSeq"));

and at the very end

# restore the original working directory
setwd(old_directory);

My guess is that in your case there's an error occurring during Sweave
resulting in the current directory being incorrect upon return.  This
causes the tools:::find_vignette_product() assertion following
immediately after to look in the incorrect directory.

[ It should be easy to update the R vignette mechanism such that this
check for the vignette output is done in the proper directory
regardless whether the vignette changes the current working directory
or not. ]



#############################################
# Case #2: ppiData
#############################################

>> Next, the package ppiData whose source is here:
>> https://hedgehog.fhcrc.org/bioc-data/trunk/experiment/pkgs/ppiData
>> (login: readonly password: readonly)
>>
>> As you can see, there is no .tex file in the source of the package,
>> but after running R CMD build on it, the resulting .tar.gz file
>> contains
>> ppiData/inst/doc/ppiData.tex
>> Then the package fails check (excerpt of ppiData.Rcheck/00install.out):
>>
>> Error in vignette_type(Outfile) :
>>   Vignette product ?ppiData.tex? does not have a known filename extension (?NA?)
>> ERROR: installing vignettes failed
>> * removing ?/home/biocbuild/bbs-2.12-data-experiment/meat/ppiData.Rcheck/ppiData?
>>
>> This package does not have a vignette Makefile either, so I'm not sure
>> why the tex file is included in the .tar.gz.

Building the vignette manually, I get:

> setwd("ppiData/inst/doc/")
> Sweave("ppiData.Rnw")
Writing to file ppiData.tex
Processing code chunks with options ...
[...]
Loading required package: lattice
 2 : echo keep.source term verbatim (label = collectData, ppiData.Rnw:79)

Error:  chunk 2 (label = collectData)
Error in as.list.default(org.Sc.sgdCOMMON2ORF) :
  no method for coercing this S4 class to a vector

Enter a frame number, or 0 to exit

1: Sweave("ppiData.Rnw")
2: driver$runcode(drobj, chunk, chunkopts)
3: RweaveTryStop(err, options)

Selection:

So, that appears to be the root of the problem.

The "Error in vignette_type(Outfile) : Vignette product 'ppiData.tex'
does not have a known filename extension ('NA')" occurs after R has
temporarily built the package and then tries to install it.  It fails
to pick up the above Sweave() error during the build process because
Sweave() leaves an (incomplete) 'ppiData.tex' file behind the R
vignette machinery believes it was successful and keeps going.

[ I don't know enough of the inner details of Sweave() and how it's
exception handling works, but hopefully it's possible to detect the
Sweave error and give a more informative error message earlier on. ]


>>
>> These two errors are with:
>>
>>> sessionInfo()
>> R version 3.0.0 alpha (2013-03-09 r62188)
>> Platform: x86_64-unknown-linux-gnu (64-bit)
>>
>> locale:
>>  [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
>>  [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
>>  [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8
>>  [7] LC_PAPER=C                 LC_NAME=C
>>  [9] LC_ADDRESS=C               LC_TELEPHONE=C
>> [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C
>>
>> attached base packages:
>> [1] stats     graphics  grDevices utils     datasets  methods   base
>>
>>


#############################################
# Case #3: bumphunter
#############################################

>> I also get another error on windows only ( it builds successfully on
>> Mac and Linux) when trying to build bumphunter
>> http://bioconductor.org/packages/2.12/bioc/html/bumphunter.html
>>
>> Warning: running command
>> '"C:\PROGRA?2\MIKTEX?1.9\miktex\bin\texi2dvi.exe" --quiet --pdf
>> "bumphunter.tex"  -I
>> "E:/biocbld/bbs-2.12-bioc/R/share/texmf/tex/latex" -I
>> "E:/biocbld/bbs-2.12-bioc/R/share/texmf/bibtex/bst"' had status 1
>> Error in find_vignette_product(name, by = "texi2pdf", engine = engine) :
>>   Failed to locate the 'texi2pdf' output file (by engine
>> 'utils::Sweave') for vignette with name 'bumphunter'. The following
>> files exists in directory '.': 'bumphunter-clusterplot.pdf',
>> 'bumphunter-plotSegments.pdf', 'bumphunter.Rnw', 'bumphunter.bib',
>> 'bumphunter.tex'
>> Calls: <Anonymous> -> find_vignette_product
>> Execution halted
>>
>> I guess it's looking for bumphunter.pdf.

Yes.  The error message occurs as part of an assertion test
immediately after running texi2pdf() on the Sweave generated
'bumphunter.tex' file.  So, the error message is correct and it occurs
as soon as possible.  [ I'll see if we could also pick up and report
on the (below) warning message produced my 'texi2pdf', because that
would help the troubleshooting on the user end. ]

I can reproduce this on Windows (session details above), also when
building the vignette manually:

> setwd("bumphunter/vignettes/")
> Sweave("bumphunter.Rnw")  # successful
[...]
> tools::texi2pdf("bumphunter.tex")
Warning message:
running command '"C:\PROGRA~2\MIKTEX~1.9\miktex\bin\texi2dvi.exe"
--quiet --pdf "bumphunter.tex"  -I
"C:/PROGRA~1/R/R-3.0.0alpha/share/texmf/tex/latex" -I
"C:/PROGRA~1/R/R-3.0.0alpha/share/texmf/bibtex/bst"' had status 1

It turns out that the 'bumphunter.tex' file has an active file lock
that prevents texi2dvi.exe from compiling the file, e.g.

fileLocked <- function(file) {
  fileT <- sprintf("%s.tmp", file)
  ok <- file.rename(file, fileT)
  file.rename(fileT[ok], file[ok])
  names(ok) <- file
  !ok
}

> files <- list.files()
> files <- files[file_test("-f", files)]
> fileLocked(files)
 bumphunter-clusterplot.pdf bumphunter-plotSegments.pdf
                      FALSE                       FALSE
             bumphunter.Rnw              bumphunter.tex
                      FALSE                        TRUE


So, there is something that holds on to this file.  My guess it has to
do with the parallel computations via socket connections, e.g.

> showConnections()
  description       class      mode  text     isopen   can read
5 "<-hb-x201:11372" "sockconn" "a+b" "binary" "opened" "yes"
6 "<-hb-x201:11372" "sockconn" "a+b" "binary" "opened" "yes"
7 "<-hb-x201:11372" "sockconn" "a+b" "binary" "opened" "yes"
  can write
5 "yes"
6 "yes"
7 "yes"

Sure enough, doing:

> closeAllConnections()
> tools::texi2pdf("bumphunter.tex")

Now texi2dvi runs and it produces a PDF (though there is some bibtex
error, but that's a different story).


Hope this helps

/Henrik

>>
>> This is with:
>>
>>> sessionInfo()
>> R version 3.0.0 alpha (2013-03-09 r62188)
>> Platform: i386-w64-mingw32/i386 (32-bit)
>>
>> locale:
>> [1] LC_COLLATE=English_United States.1252
>> [2] LC_CTYPE=English_United States.1252
>> [3] LC_MONETARY=English_United States.1252
>> [4] LC_NUMERIC=C
>> [5] LC_TIME=English_United States.1252
>>
>> attached base packages:
>> [1] stats     graphics  grDevices utils     datasets  methods   base
>>
>> thanks,
>> Dan
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel


From therneau at mayo.edu  Fri Mar 15 18:47:01 2013
From: therneau at mayo.edu (Terry Therneau)
Date: Fri, 15 Mar 2013 12:47:01 -0500
Subject: [Rd] numerics from a factor
Message-ID: <51435E95.304@mayo.edu>

A problem has been pointed out by a French user of the survival package and I'm looking 
for a pointer.

 > options(OutDec= ",")
 > fit <- survfit(Surv(1:6 /2) ~ 1)
 > fit$time
[1] NA  1 NA  2 NA  3

A year or two ago some test cases that broke survfit were presented to me. The heart of 
the problem was numbers that were almost identical, where table(x) and unique(x) gave 
different counts of distinct values.
The solution was to use "ftime <- factor(time)" at the top of the code, and do all the 
calulations using the integer levels of the factor as the unique time points.  At the very 
end the numeric component "time" of the result is created using 
as.numeric(levels(ftime)).  It's this last line that breaks.

I could set the OutDec option within survfit and reset when I leave using on.exit.  Any 
other simple solutions?  Any other ways I could get caught by this issue?

Terry Therneau


From hintak_leung at yahoo.co.uk  Sat Mar 16 00:29:48 2013
From: hintak_leung at yahoo.co.uk (Hin-Tak Leung)
Date: Fri, 15 Mar 2013 23:29:48 +0000 (GMT)
Subject: [Rd] the case of building R snapshot without svn nor network
	connection.
Message-ID: <1363390188.35233.YahooMailClassic@web172303.mail.ir2.yahoo.com>

The decision to actively discourage non-subsersion usage of snapshot build is already made (r62183). So I am just here to register a differing opinion.

- it is not about subversion vs other-version-control-tools. There are two parts of R's dev build process which requires an active network connection - tools/rsync-recommended and capturing `svn info` into R's headers. The former can be overridden with "./configure --with-recommended-packages=no". A few changes had been made in the last 6 months to make the latter mandatory. It used to be optional.

--- there are genuine needs for testing snapshots in a non-networked minimalist environment - e.g. banks or telecom industries - where one wants a "standardised host" build, and often it means an "outdated host"; or in a virtual machine environment - which are non-networked for security reasons, and also do not have tools beyond necessary for compiling and building. This is quite a common scenario.

--- AFAIK, 6 months ago, a snapshot copied to an non-networked host will build with "./configure --with-recommended-packages=no". Of course copying those recommended package tar balls across would be nicer. This is sadly no longer the case.

- dependent on `svn info` and sitting on top of a svn checkout possibly also affects cross-compiling. But then, it is not clear whether it is still possible to cross-compile R, since quite a few changes have been made to remove the capability of cross-compiling R for windows on unix hosts in the last few years. 

- testing dev snapshots is about trying things and fixing bugs before release. Making it difficult for non-core people to "try", seem to go against that ethos. If that's the case, maybe the repository could be closed off to anonymous check outs altogether, just to make it clear that testing snapshots before releases or even close to release, is not welcomed. Just a thought.

- although the official repository of the "main" linux kernel branch is git-based, there are mercurial mirrors; AFAIK the digital video broadcasting hardware support of the linux kernel is officially in mercurial repositories, but have git mirrors; likewise much of Xorg's is in mercurial but have git mirrors. I haven't heard of any much bigger public projects than R where some actively discourage others.

- To be honest r62183 itself is probably a good reason to move away from server-side repositories to distributed repositories (hg/git/arch/bzr). Local enhancements - i.e. an in-house fork - some of which are never going upstream, such as a local revert of r62183 (or a local revert of any other upstream commits) is one reason why some have distributed repositories.

Lastly, a minor grip. The current head of the HK government is probably sometimes called "HK Leung", just as some might call a certain old lady "UK Windsor" and a certain black person "US Obama".


From simon.urbanek at r-project.org  Sat Mar 16 01:56:28 2013
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Fri, 15 Mar 2013 20:56:28 -0400
Subject: [Rd] the case of building R snapshot without svn nor network
	connection.
In-Reply-To: <1363390188.35233.YahooMailClassic@web172303.mail.ir2.yahoo.com>
References: <1363390188.35233.YahooMailClassic@web172303.mail.ir2.yahoo.com>
Message-ID: <D9FE2744-83B0-4B8D-B59D-358B3B38B93A@r-project.org>

On Mar 15, 2013, at 7:29 PM, Hin-Tak Leung wrote:

> The decision to actively discourage non-subsersion usage of snapshot build is already made (r62183). So I am just here to register a differing opinion.
> 
> - it is not about subversion vs other-version-control-tools. There are two parts of R's dev build process which requires an active network connection - tools/rsync-recommended and capturing `svn info` into R's headers.

That is a false statement - svn info doesn't require any network connection.

Cheers,
Simon



> The former can be overridden with "./configure --with-recommended-packages=no". A few changes had been made in the last 6 months to make the latter mandatory. It used to be optional.
> 
> --- there are genuine needs for testing snapshots in a non-networked minimalist environment - e.g. banks or telecom industries - where one wants a "standardised host" build, and often it means an "outdated host"; or in a virtual machine environment - which are non-networked for security reasons, and also do not have tools beyond necessary for compiling and building. This is quite a common scenario.
> 
> --- AFAIK, 6 months ago, a snapshot copied to an non-networked host will build with "./configure --with-recommended-packages=no". Of course copying those recommended package tar balls across would be nicer. This is sadly no longer the case.
> 
> - dependent on `svn info` and sitting on top of a svn checkout possibly also affects cross-compiling. But then, it is not clear whether it is still possible to cross-compile R, since quite a few changes have been made to remove the capability of cross-compiling R for windows on unix hosts in the last few years. 
> 
> - testing dev snapshots is about trying things and fixing bugs before release. Making it difficult for non-core people to "try", seem to go against that ethos. If that's the case, maybe the repository could be closed off to anonymous check outs altogether, just to make it clear that testing snapshots before releases or even close to release, is not welcomed. Just a thought.
> 
> - although the official repository of the "main" linux kernel branch is git-based, there are mercurial mirrors; AFAIK the digital video broadcasting hardware support of the linux kernel is officially in mercurial repositories, but have git mirrors; likewise much of Xorg's is in mercurial but have git mirrors. I haven't heard of any much bigger public projects than R where some actively discourage others.
> 
> - To be honest r62183 itself is probably a good reason to move away from server-side repositories to distributed repositories (hg/git/arch/bzr). Local enhancements - i.e. an in-house fork - some of which are never going upstream, such as a local revert of r62183 (or a local revert of any other upstream commits) is one reason why some have distributed repositories.
> 
> Lastly, a minor grip. The current head of the HK government is probably sometimes called "HK Leung", just as some might call a certain old lady "UK Windsor" and a certain black person "US Obama".
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 
> 


From htl10 at users.sourceforge.net  Sat Mar 16 02:50:04 2013
From: htl10 at users.sourceforge.net (Hin-Tak Leung)
Date: Sat, 16 Mar 2013 01:50:04 +0000 (GMT)
Subject: [Rd] the case of building R snapshot without svn nor network
	connection.
In-Reply-To: <1363390188.35233.YahooMailClassic@web172303.mail.ir2.yahoo.com>
Message-ID: <1363398604.5295.YahooMailClassic@web172302.mail.ir2.yahoo.com>

I'll quantify the first part - R is perhaps the only public software project hosted on a subversion repository for which the result of 'svn export ...' does not build. Not only that it does not build, but make it a feature that it does not build.

Very few other projects actively try to go in that direction.

--- On Fri, 15/3/13, Hin-Tak Leung <hintak_leung at yahoo.co.uk> wrote:

> The decision to actively discourage
> non-subsersion usage of snapshot build is already made
> (r62183). So I am just here to register a differing
> opinion.
> 
> - it is not about subversion vs other-version-control-tools.
> There are two parts of R's dev build process which requires
> an active network connection - tools/rsync-recommended and
> capturing `svn info` into R's headers. The former can be
> overridden with "./configure
> --with-recommended-packages=no". A few changes had been made
> in the last 6 months to make the latter mandatory. It used
> to be optional.
> 
> --- there are genuine needs for testing snapshots in a
> non-networked minimalist environment - e.g. banks or telecom
> industries - where one wants a "standardised host" build,
> and often it means an "outdated host"; or in a virtual
> machine environment - which are non-networked for security
> reasons, and also do not have tools beyond necessary for
> compiling and building. This is quite a common scenario.
> 
> --- AFAIK, 6 months ago, a snapshot copied to an
> non-networked host will build with "./configure
> --with-recommended-packages=no". Of course copying those
> recommended package tar balls across would be nicer. This is
> sadly no longer the case.
> 
> - dependent on `svn info` and sitting on top of a svn
> checkout possibly also affects cross-compiling. But then, it
> is not clear whether it is still possible to cross-compile
> R, since quite a few changes have been made to remove the
> capability of cross-compiling R for windows on unix hosts in
> the last few years. 
> 
> - testing dev snapshots is about trying things and fixing
> bugs before release. Making it difficult for non-core people
> to "try", seem to go against that ethos. If that's the case,
> maybe the repository could be closed off to anonymous check
> outs altogether, just to make it clear that testing
> snapshots before releases or even close to release, is not
> welcomed. Just a thought.
> 
> - although the official repository of the "main" linux
> kernel branch is git-based, there are mercurial mirrors;
> AFAIK the digital video broadcasting hardware support of the
> linux kernel is officially in mercurial repositories, but
> have git mirrors; likewise much of Xorg's is in mercurial
> but have git mirrors. I haven't heard of any much bigger
> public projects than R where some actively discourage
> others.
> 
> - To be honest r62183 itself is probably a good reason to
> move away from server-side repositories to distributed
> repositories (hg/git/arch/bzr). Local enhancements - i.e. an
> in-house fork - some of which are never going upstream, such
> as a local revert of r62183 (or a local revert of any other
> upstream commits) is one reason why some have distributed
> repositories.
> 
> Lastly, a minor grip. The current head of the HK government
> is probably sometimes called "HK Leung", just as some might
> call a certain old lady "UK Windsor" and a certain black
> person "US Obama".
>


From pdalgd at gmail.com  Sat Mar 16 12:36:21 2013
From: pdalgd at gmail.com (peter dalgaard)
Date: Sat, 16 Mar 2013 12:36:21 +0100
Subject: [Rd] the case of building R snapshot without svn nor network
	connection.
In-Reply-To: <1363398604.5295.YahooMailClassic@web172302.mail.ir2.yahoo.com>
References: <1363398604.5295.YahooMailClassic@web172302.mail.ir2.yahoo.com>
Message-ID: <9280B9F7-14D5-47F7-87B6-EA299885B8B8@gmail.com>


On Mar 16, 2013, at 02:50 , Hin-Tak Leung wrote:

> I'll quantify the first part - R is perhaps the only public software project hosted on a subversion repository for which the result of 'svn export ...' does not build. Not only that it does not build, but make it a feature that it does not build.
> 
> Very few other projects actively try to go in that direction.

It is not a design goal in any project that I know of, that svn exports should be equivalent to distribution tarballs. In simple projects, that might be the case, but requiring a "make dist" step is quite common before the final shipment. 

An actual design goal has been to ensure that snapshot builds carry an SVN revision number so that we can detect whether issues are reported on versions prior to a known fix. This is done via an "svn info" command because that was the path of least resistance (you can't really e.g. maintain the SVN-REVISION file in svn because it would need to change on every commit). There's no particular intention to hardwire SVN as the source code management tool, but as it happens to be what we use, that tiny subsystem of the build system has to be specific to SVN.

The generic point is that you are given access to a working tool that is internal to the core R developers. We are not putting restrictions on what you do with that access, but if you want to play the game by other rules than we do, you need to take the consequences. If things don't work and you start complaining about them being "broken", steps may be taken to make it clearer who broke them.

As Jari Oksanen put it recently:

"I think the rule is that you can do anything as long as you don't complain. If you want to complain, you must follow the instructions."

Peter D.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From groemping at bht-berlin.de  Sat Mar 16 13:04:07 2013
From: groemping at bht-berlin.de (=?ISO-8859-1?Q?Ulrike_Gr=F6mping?=)
Date: Sat, 16 Mar 2013 13:04:07 +0100
Subject: [Rd] numerics from a factor
In-Reply-To: <mailman.27.1363431608.4225.r-devel@r-project.org>
References: <mailman.27.1363431608.4225.r-devel@r-project.org>
Message-ID: <51445FB7.9090503@bht-berlin.de>

Hi Terry,

you can use type.convert instead of as.numeric for numbers with decimals:

type.convert(levels(factor(1:6/2)), dec=unlist(options("OutDec")))

Best,
Ulrike


Am 16.03.2013 12:00, schrieb r-devel-request at r-project.org:
> Date: Fri, 15 Mar 2013 12:47:01 -0500
> From: Terry Therneau<therneau at mayo.edu>
> To:"r-devel at r-project.org"  <r-devel at r-project.org>
> Subject: [Rd] numerics from a factor
> Message-ID:<51435E95.304 at mayo.edu>
> Content-Type: text/plain; charset=ISO-8859-1; format=flowed
>
> A problem has been pointed out by a French user of the survival package and I'm looking
> for a pointer.
>
>   > options(OutDec= ",")
>   > fit <- survfit(Surv(1:6 /2) ~ 1)
>   > fit$time
> [1] NA  1 NA  2 NA  3
>
> A year or two ago some test cases that broke survfit were presented to me. The heart of
> the problem was numbers that were almost identical, where table(x) and unique(x) gave
> different counts of distinct values.
> The solution was to use "ftime <- factor(time)" at the top of the code, and do all the
> calulations using the integer levels of the factor as the unique time points.  At the very
> end the numeric component "time" of the result is created using
> as.numeric(levels(ftime)).  It's this last line that breaks.
>
> I could set the OutDec option within survfit and reset when I leave using on.exit.  Any
> other simple solutions?  Any other ways I could get caught by this issue?
>
> Terry Therneau


From htl10 at users.sourceforge.net  Sat Mar 16 13:24:18 2013
From: htl10 at users.sourceforge.net (Hin-Tak Leung)
Date: Sat, 16 Mar 2013 12:24:18 +0000 (GMT)
Subject: [Rd] the case of building R snapshot without svn nor network
	connection.
In-Reply-To: <9280B9F7-14D5-47F7-87B6-EA299885B8B8@gmail.com>
Message-ID: <1363436658.23413.YahooMailClassic@web172304.mail.ir2.yahoo.com>

--- On Sat, 16/3/13, peter dalgaard <pdalgd at gmail.com> wrote:

> On Mar 16, 2013, at 02:50 , Hin-Tak Leung wrote:
> 
> > I'll quantify the first part - R is perhaps the only
> public software project hosted on a subversion repository
> for which the result of 'svn export ...' does not build. Not
> only that it does not build, but make it a feature that it
> does not build.
> > 
> > Very few other projects actively try to go in that
> direction.
> 
> It is not a design goal in any project that I know of, that
> svn exports should be equivalent to distribution tarballs.
> In simple projects, that might be the case, but requiring a
> "make dist" step is quite common before the final shipment.

Yes and no. Many projects much larger than R have a "make dist" target. However, I don't know of any where they make it a feature and a design goal and actively go forward that a 'make dist' tar ball differs substantially in functionality from a snapshot close to the release revision, and also actively make sure that a snapshot does not work.

Try name one.

Even if you can name one other such project, is that honestly good practice to emulate?

> An actual design goal has been to ensure that snapshot
> builds carry an SVN revision number so that we can detect
> whether issues are reported on versions prior to a known
> fix. This is done via an "svn info" command because that was
> the path of least resistance (you can't really e.g. maintain
> the SVN-REVISION file in svn because it would need to change
> on every commit). There's no particular intention to
> hardwire SVN as the source code management tool, but as it
> happens to be what we use, that tiny subsystem of the build
> system has to be specific to SVN.

As is evident, dependence on svn is already hardwired. Look at the issue leading up to this discussion: there were (are, since there isn't a new release yet) code in Matrix, and also elsewhere from a cursory grep, where code paths are conditional on specific version commit numbers, and do different things before/after specific svn revision numbers. 

> The generic point is that you are given access to a working
> tool that is internal to the core R developers. We are not
> putting restrictions on what you do with that access, but if
> you want to play the game by other rules than we do, you
> need to take the consequences. If things don't work and you
> start complaining about them being "broken", steps may be
> taken to make it clearer who broke them.

There is a difference between "unsupported" ("I don't want to spend time hearing about issues arising from going off the beaten path"), versus 'actively discourage going off "beaten" path'. 

Where, of course, "beaten path" is defined by a small group of people, as is "design goal". (it is a feature to break).

> As Jari Oksanen put it recently:
> 
> "I think the rule is that you can do anything as long as you
> don't complain. If you want to complain, you must follow the
> instructions."
> 
> Peter D.
> 
> -- 
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Email: pd.mes at cbs.dk?
> Priv: PDalgd at gmail.com
> 
> 
> 
> 
> 
> 
> 
> 
>


From edd at debian.org  Sat Mar 16 14:09:55 2013
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sat, 16 Mar 2013 08:09:55 -0500
Subject: [Rd] the case of building R snapshot without svn nor
	network	connection.
In-Reply-To: <1363436658.23413.YahooMailClassic@web172304.mail.ir2.yahoo.com>
References: <9280B9F7-14D5-47F7-87B6-EA299885B8B8@gmail.com>
	<1363436658.23413.YahooMailClassic@web172304.mail.ir2.yahoo.com>
Message-ID: <20804.28451.124452.280677@max.nulle.part>


Can you please stop this?  You made your point, repeatedly.  Nobody has come
to your side.  

And you *do* have easy alternatives:

   i)  git repo,  see https://github.com/wch/r-source    

   ii) nighly tarballs,  see ftp://ftp.stat.math.ethz.ch/Software/R

Whatever your particular circumstances are (which you never detailed in a way
that made me understand what your issue is), one of these should work.

As I said before, 'svn up && make distclean && ~/bin/makeRdevel' works just
fine for me with the configure choice I prefer hardwired in the script.  I
never sync to r-recommended and *honestly* have no idea what your issue is.
I just rebuilt two days ago without an issue:

   edd at max:~$ ~/bin/R-devel.sh --version|head -1
   R Under development (unstable) (2013-03-15 r62262) -- "Unsuffered Consequences"


Dirk

-- 
Dirk Eddelbuettel | edd at debian.org | http://dirk.eddelbuettel.com


From htl10 at users.sourceforge.net  Sat Mar 16 15:13:42 2013
From: htl10 at users.sourceforge.net (Hin-Tak Leung)
Date: Sat, 16 Mar 2013 14:13:42 +0000 (GMT)
Subject: [Rd] the case of building R snapshot without svn nor network
	connection.
In-Reply-To: <1363398604.5295.YahooMailClassic@web172302.mail.ir2.yahoo.com>
Message-ID: <1363443222.94887.YahooMailClassic@web172302.mail.ir2.yahoo.com>

Network access is *not* a given, nor is the privilege of installing arbitrary "uncertified" and "non-essential" tools - whatever the meaning of "uncertified" and "non-essential" are, those being defined, as is "design goal", etc, by some small committee.

It is a very common scenario, e.g. banks & telecom, some part of public/government service and health care. This does not seem to sink in without repeating.

--- On Sat, 16/3/13, Hin-Tak Leung <htl10 at users.sourceforge.net> wrote:

> I'll quantify the first part - R is
> perhaps the only public software project hosted on a
> subversion repository for which the result of 'svn export
> ...' does not build. Not only that it does not build, but
> make it a feature that it does not build.
> 
> Very few other projects actively try to go in that
> direction.
> 
> --- On Fri, 15/3/13, Hin-Tak Leung <hintak_leung at yahoo.co.uk>
> wrote:
> 
> > The decision to actively discourage
> > non-subsersion usage of snapshot build is already made
> > (r62183). So I am just here to register a differing
> > opinion.
> > 
> > - it is not about subversion vs
> other-version-control-tools.
> > There are two parts of R's dev build process which
> requires
> > an active network connection - tools/rsync-recommended
> and
> > capturing `svn info` into R's headers. The former can
> be
> > overridden with "./configure
> > --with-recommended-packages=no". A few changes had been
> made
> > in the last 6 months to make the latter mandatory. It
> used
> > to be optional.
> > 
> > --- there are genuine needs for testing snapshots in a
> > non-networked minimalist environment - e.g. banks or
> telecom
> > industries - where one wants a "standardised host"
> build,
> > and often it means an "outdated host"; or in a virtual
> > machine environment - which are non-networked for
> security
> > reasons, and also do not have tools beyond necessary
> for
> > compiling and building. This is quite a common
> scenario.
> > 
> > --- AFAIK, 6 months ago, a snapshot copied to an
> > non-networked host will build with "./configure
> > --with-recommended-packages=no". Of course copying
> those
> > recommended package tar balls across would be nicer.
> This is
> > sadly no longer the case.
> > 
> > - dependent on `svn info` and sitting on top of a svn
> > checkout possibly also affects cross-compiling. But
> then, it
> > is not clear whether it is still possible to
> cross-compile
> > R, since quite a few changes have been made to remove
> the
> > capability of cross-compiling R for windows on unix
> hosts in
> > the last few years. 
> > 
> > - testing dev snapshots is about trying things and
> fixing
> > bugs before release. Making it difficult for non-core
> people
> > to "try", seem to go against that ethos. If that's the
> case,
> > maybe the repository could be closed off to anonymous
> check
> > outs altogether, just to make it clear that testing
> > snapshots before releases or even close to release, is
> not
> > welcomed. Just a thought.
> > 
> > - although the official repository of the "main" linux
> > kernel branch is git-based, there are mercurial
> mirrors;
> > AFAIK the digital video broadcasting hardware support
> of the
> > linux kernel is officially in mercurial repositories,
> but
> > have git mirrors; likewise much of Xorg's is in
> mercurial
> > but have git mirrors. I haven't heard of any much
> bigger
> > public projects than R where some actively discourage
> > others.
> > 
> > - To be honest r62183 itself is probably a good reason
> to
> > move away from server-side repositories to distributed
> > repositories (hg/git/arch/bzr). Local enhancements -
> i.e. an
> > in-house fork - some of which are never going upstream,
> such
> > as a local revert of r62183 (or a local revert of any
> other
> > upstream commits) is one reason why some have
> distributed
> > repositories.
> > 
> > Lastly, a minor grip. The current head of the HK
> government
> > is probably sometimes called "HK Leung", just as some
> might
> > call a certain old lady "UK Windsor" and a certain
> black
> > person "US Obama".
> >
>


From simon.urbanek at r-project.org  Sat Mar 16 15:39:42 2013
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Sat, 16 Mar 2013 10:39:42 -0400
Subject: [Rd] the case of building R snapshot without svn nor network
	connection.
In-Reply-To: <1363443222.94887.YahooMailClassic@web172302.mail.ir2.yahoo.com>
References: <1363443222.94887.YahooMailClassic@web172302.mail.ir2.yahoo.com>
Message-ID: <2F0444D6-24AC-44B0-8413-8F41A2B5D983@r-project.org>

On Mar 16, 2013, at 10:13 AM, Hin-Tak Leung wrote:

> Network access is *not* a given, nor is the privilege of installing arbitrary "uncertified" and "non-essential" tools - whatever the meaning of "uncertified" and "non-essential" are, those being defined, as is "design goal", etc, by some small committee.
> 
> It is a very common scenario, e.g. banks & telecom, some part of public/government service and health care. This does not seem to sink in without repeating.
> 

Network access is not needed to build R - apparently that fact did not seem to sink in, either. This entire thread is based on false assumptions and as such the only place for it is /dev/null


> --- On Sat, 16/3/13, Hin-Tak Leung <htl10 at users.sourceforge.net> wrote:
> 
>> I'll quantify the first part - R is
>> perhaps the only public software project hosted on a
>> subversion repository for which the result of 'svn export
>> ...' does not build. Not only that it does not build, but
>> make it a feature that it does not build.
>> 
>> Very few other projects actively try to go in that
>> direction.
>> 
>> --- On Fri, 15/3/13, Hin-Tak Leung <hintak_leung at yahoo.co.uk>
>> wrote:
>> 
>>> The decision to actively discourage
>>> non-subsersion usage of snapshot build is already made
>>> (r62183). So I am just here to register a differing
>>> opinion.
>>> 
>>> - it is not about subversion vs
>> other-version-control-tools.
>>> There are two parts of R's dev build process which
>> requires
>>> an active network connection - tools/rsync-recommended
>> and
>>> capturing `svn info` into R's headers. The former can
>> be
>>> overridden with "./configure
>>> --with-recommended-packages=no". A few changes had been
>> made
>>> in the last 6 months to make the latter mandatory. It
>> used
>>> to be optional.
>>> 
>>> --- there are genuine needs for testing snapshots in a
>>> non-networked minimalist environment - e.g. banks or
>> telecom
>>> industries - where one wants a "standardised host"
>> build,
>>> and often it means an "outdated host"; or in a virtual
>>> machine environment - which are non-networked for
>> security
>>> reasons, and also do not have tools beyond necessary
>> for
>>> compiling and building. This is quite a common
>> scenario.
>>> 
>>> --- AFAIK, 6 months ago, a snapshot copied to an
>>> non-networked host will build with "./configure
>>> --with-recommended-packages=no". Of course copying
>> those
>>> recommended package tar balls across would be nicer.
>> This is
>>> sadly no longer the case.
>>> 
>>> - dependent on `svn info` and sitting on top of a svn
>>> checkout possibly also affects cross-compiling. But
>> then, it
>>> is not clear whether it is still possible to
>> cross-compile
>>> R, since quite a few changes have been made to remove
>> the
>>> capability of cross-compiling R for windows on unix
>> hosts in
>>> the last few years. 
>>> 
>>> - testing dev snapshots is about trying things and
>> fixing
>>> bugs before release. Making it difficult for non-core
>> people
>>> to "try", seem to go against that ethos. If that's the
>> case,
>>> maybe the repository could be closed off to anonymous
>> check
>>> outs altogether, just to make it clear that testing
>>> snapshots before releases or even close to release, is
>> not
>>> welcomed. Just a thought.
>>> 
>>> - although the official repository of the "main" linux
>>> kernel branch is git-based, there are mercurial
>> mirrors;
>>> AFAIK the digital video broadcasting hardware support
>> of the
>>> linux kernel is officially in mercurial repositories,
>> but
>>> have git mirrors; likewise much of Xorg's is in
>> mercurial
>>> but have git mirrors. I haven't heard of any much
>> bigger
>>> public projects than R where some actively discourage
>>> others.
>>> 
>>> - To be honest r62183 itself is probably a good reason
>> to
>>> move away from server-side repositories to distributed
>>> repositories (hg/git/arch/bzr). Local enhancements -
>> i.e. an
>>> in-house fork - some of which are never going upstream,
>> such
>>> as a local revert of r62183 (or a local revert of any
>> other
>>> upstream commits) is one reason why some have
>> distributed
>>> repositories.
>>> 
>>> Lastly, a minor grip. The current head of the HK
>> government
>>> is probably sometimes called "HK Leung", just as some
>> might
>>> call a certain old lady "UK Windsor" and a certain
>> black
>>> person "US Obama".
>>> 
>> 
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 
> 


From xie at yihui.name  Sat Mar 16 18:29:41 2013
From: xie at yihui.name (Yihui Xie)
Date: Sat, 16 Mar 2013 12:29:41 -0500
Subject: [Rd] the case of building R snapshot without svn nor network
	connection.
In-Reply-To: <2F0444D6-24AC-44B0-8413-8F41A2B5D983@r-project.org>
References: <1363443222.94887.YahooMailClassic@web172302.mail.ir2.yahoo.com>
	<2F0444D6-24AC-44B0-8413-8F41A2B5D983@r-project.org>
Message-ID: <CANROs4f7dQxTqSS3AZrYdpc+0BgPYr0xqCTks1XhshB0k5iR3A@mail.gmail.com>

"the only place for it is /dev/null" --> hi Achim, I bet this is a
nice fortune candidate :)

Regards,
Yihui
--
Yihui Xie <xieyihui at gmail.com>
Phone: 515-294-2465 Web: http://yihui.name
Department of Statistics, Iowa State University
2215 Snedecor Hall, Ames, IA


On Sat, Mar 16, 2013 at 9:39 AM, Simon Urbanek
<simon.urbanek at r-project.org> wrote:
> On Mar 16, 2013, at 10:13 AM, Hin-Tak Leung wrote:
>
>> Network access is *not* a given, nor is the privilege of installing arbitrary "uncertified" and "non-essential" tools - whatever the meaning of "uncertified" and "non-essential" are, those being defined, as is "design goal", etc, by some small committee.
>>
>> It is a very common scenario, e.g. banks & telecom, some part of public/government service and health care. This does not seem to sink in without repeating.
>>
>
> Network access is not needed to build R - apparently that fact did not seem to sink in, either. This entire thread is based on false assumptions and as such the only place for it is /dev/null
>


From lgautier at gmail.com  Sat Mar 16 19:17:58 2013
From: lgautier at gmail.com (Laurent Gautier)
Date: Sat, 16 Mar 2013 11:17:58 -0700
Subject: [Rd] Mirroring R on a DVCS
In-Reply-To: <mailman.27.1363431608.4225.r-devel@r-project.org>
References: <mailman.27.1363431608.4225.r-devel@r-project.org>
Message-ID: <5144B756.60909@gmail.com>

Hi,

I have been looking at mirroring the SVN R repository into a DVCS 
(Mercurial, Git, bzr, etc...).

The idea would be to have that as an always up-to-date untouched master 
(a mirror) but having it in a DVCS would make forks easier to make and 
maintain, and possibly ease up the process of having patches included 
upstream.

Would anyone else be interested ?


Laurent


From peter.meilstrup at gmail.com  Sat Mar 16 19:28:11 2013
From: peter.meilstrup at gmail.com (Peter Meilstrup)
Date: Sat, 16 Mar 2013 11:28:11 -0700
Subject: [Rd] Mirroring R on a DVCS
In-Reply-To: <5144B756.60909@gmail.com>
References: <mailman.27.1363431608.4225.r-devel@r-project.org>
	<5144B756.60909@gmail.com>
Message-ID: <0D87478D-9F5C-473A-838F-01EBD742AF08@gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20130316/b8ca846f/attachment.pl>

From lgautier at gmail.com  Sat Mar 16 19:44:57 2013
From: lgautier at gmail.com (Laurent Gautier)
Date: Sat, 16 Mar 2013 11:44:57 -0700
Subject: [Rd] Mirroring R on a DVCS
In-Reply-To: <0D87478D-9F5C-473A-838F-01EBD742AF08@gmail.com>
References: <mailman.27.1363431608.4225.r-devel@r-project.org>
	<5144B756.60909@gmail.com>
	<0D87478D-9F5C-473A-838F-01EBD742AF08@gmail.com>
Message-ID: <5144BDA9.8060505@gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20130316/b26741bb/attachment.pl>

From winstonchang1 at gmail.com  Sat Mar 16 21:38:39 2013
From: winstonchang1 at gmail.com (Winston Chang)
Date: Sat, 16 Mar 2013 15:38:39 -0500
Subject: [Rd] Mirroring R on a DVCS
In-Reply-To: <5144BDA9.8060505@gmail.com>
References: <mailman.27.1363431608.4225.r-devel@r-project.org>
	<5144B756.60909@gmail.com>
	<0D87478D-9F5C-473A-838F-01EBD742AF08@gmail.com>
	<5144BDA9.8060505@gmail.com>
Message-ID: <CAFOpNVHjmGDYR9SnF_7Bo+gFNfMP=-e0DV0X5KOFvBeAujZpiA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20130316/0c5e5524/attachment.pl>

From mtmorgan at fhcrc.org  Sat Mar 16 23:29:47 2013
From: mtmorgan at fhcrc.org (Martin Morgan)
Date: Sat, 16 Mar 2013 15:29:47 -0700
Subject: [Rd] configure.ac clock_gettime (and R_CHECK_FUNCS)  incorrect?
Message-ID: <5144F25B.2030904@fhcrc.org>

When R is configured with CFLAGS=-O2, clock_gettime is included

$ nm src/main/libR.a | grep clock_gettime
                  U clock_gettime

whereas when configured with -O0 it is not

$ nm src/main/libR.a | grep clock_gettime
$

Similarly when built as a shared library the linker flags include or not -lrt, 
and ldd indicates dependency on librt or not.

This thread

   http://lists.r-forge.r-project.org/pipermail/rcpp-devel/2012-December/005022.html

lead to the conclusion that the test for clock_gettime from configure.ac:1838 is 
flawed, because

R_CHECK_FUNCS([clock_gettime timespec_get], [#include <time.h>])
if test "${ac_cv_have_decl_clock_gettime}" = "yes"; then
AC_CHECK_LIB(rt, clock_gettime)
fi

generates test code, from config.log, which looks like

| int
| main ()
| {
| #ifndef clock_gettime
|   char *p = (char *) clock_gettime;
| #endif
|
|   ;
|   return 0;
| }

when compiled with CFLAGS=-O2, the assignment to p is optimized out and the test 
succeeds whether clock_gettime is present or not; the autoconf macro may add 
-lrt to the compile flags.

When compiled with CFLAGS=-O0, the test fails and autoconf does not try to add 
-lrt, even though that might be (is) necessary.

It seems that R_CHECK_FUNCS (R_CHECK_DECLS) in general and the logic of this 
specific test need revisiting?

$ bin/R --version
R Under development (unstable) (2013-03-16 r62282) -- "Unsuffered Consequences"
...

on

$ gcc --version
gcc (Ubuntu/Linaro 4.6.3-1ubuntu5) 4.6.3
Copyright (C) 2011 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

or

$ clang --version
Ubuntu clang version 3.0-6ubuntu3 (tags/RELEASE_30/final) (based on LLVM 3.0)
Target: x86_64-pc-linux-gnu
Thread model: posix

Martin
-- 
Computational Biology / Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N.
PO Box 19024 Seattle, WA 98109

Location: Arnold Building M1 B861
Phone: (206) 667-2793


From renaud at mancala.cbio.uct.ac.za  Sun Mar 17 09:38:44 2013
From: renaud at mancala.cbio.uct.ac.za (Renaud Gaujoux)
Date: Sun, 17 Mar 2013 10:38:44 +0200
Subject: [Rd] removing union class
In-Reply-To: <20802.64898.70553.300100@stat.math.ethz.ch>
References: <CAHavPHES4xxZ6+wUXBG54ED8Y3OcOyxrCRUEMhJaB2hsepG3Xw@mail.gmail.com>
	<20802.64086.443586.813352@stat.math.ethz.ch>
	<20802.64898.70553.300100@stat.math.ethz.ch>
Message-ID: <CAHavPHGHnLEZK6oO9t0A6pCocizo1Anqk37EbzrVH5V1ThjqYA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20130317/948dc2d5/attachment.pl>

From renaud at mancala.cbio.uct.ac.za  Tue Mar 19 10:03:36 2013
From: renaud at mancala.cbio.uct.ac.za (Renaud Gaujoux)
Date: Tue, 19 Mar 2013 11:03:36 +0200
Subject: [Rd] source, sys.source and error line numbers
Message-ID: <CAHavPHHLoyDJodDj6aOmKfAnTDuvYjOcNF_YsGYkJp7w3DOXMw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20130319/1015c6da/attachment.pl>

From r.d.morey at rug.nl  Tue Mar 19 15:41:21 2013
From: r.d.morey at rug.nl (Richard D. Morey)
Date: Tue, 19 Mar 2013 15:41:21 +0100
Subject: [Rd] preparing for R 3.0.0
Message-ID: <6D0731B187284CFA85522F8A2CDBC847@rug.nl>

I am preparing my package for the release of R 3.0.0 and thinking about the suggestion (from Duncan Murdoch) to maintain two versions of the package temporarily. Since my package contains knitr-compiled vignettes, I need to restrict it to 3.0.0 going forward. I'd like to submit a final version for 2.x that will not be updated but compiles the vignettes in the old manner; and a version for 3.x. My thought is that they will have the same version number, with the only difference being how the vignette is compiled and the DESCRIPTION files.

Is it possible that they have the same version number; or, should I send two updates simultaneously with successive version numbers, flagging one as restricted to R >3.0.0?

Thanks,
Richard


From Wayne.W.Jones at shell.com  Mon Mar 18 12:10:38 2013
From: Wayne.W.Jones at shell.com (Wayne.W.Jones at shell.com)
Date: Mon, 18 Mar 2013 11:10:38 +0000
Subject: [Rd]  Windows R-3.0.0 and Tcl/tkrplot issue
Message-ID: <823FB8AD8FD2F44A92284630A4AADF7E1F96C1D9@seacmw-s-53401.europe.shell.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20130318/a96c0b77/attachment.pl>

From ligges at statistik.tu-dortmund.de  Tue Mar 19 16:07:31 2013
From: ligges at statistik.tu-dortmund.de (Uwe Ligges)
Date: Tue, 19 Mar 2013 16:07:31 +0100
Subject: [Rd] Windows R-3.0.0 and Tcl/tkrplot issue
In-Reply-To: <823FB8AD8FD2F44A92284630A4AADF7E1F96C1D9@seacmw-s-53401.europe.shell.com>
References: <823FB8AD8FD2F44A92284630A4AADF7E1F96C1D9@seacmw-s-53401.europe.shell.com>
Message-ID: <51487F33.7020404@statistik.tu-dortmund.de>

This has been fixed already, as far as I know,
Uwe


On 18.03.2013 12:10, Wayne.W.Jones at shell.com wrote:
> Greetings R Developers,
>
> I've been testing the alpha release of R-3.0.0 and I noticed that the plotting functionality in package tkrplot was not working correctly.
> Further diagnosis found the issue to be related to the command '.Tcl("image create Rplot plotname")' from package "tkrplot".
>
> The following example is  taken from here:  http://tkrplot.sourcearchive.com/documentation/0.0.14/tcltkimg_8c_source.html
>
>> library(tkrplot)
>> library(tcltk)
>> my.tkdev <- function() win.metafile(width=4,height=4)
>> my.tkdev()
>> plot(1:20)
>> .Tcl("image create Rplot fred")
> Error in structure(.External(.C_dotTcl, ...), class = "tclObj") :
>    [tcl] can't get device image.
>
> This example works fine in R-2.15.3.  I've emailed the package maintainer Luke Tierney but unfortunately he doesn't have the ability to build and test packages on windows.
> Can anyone elucidate what tcltk changes have been made in moving to R-3.0.0?
>
> Details of installation are as follows:
>
>> version
> platform       i386-w64-mingw32
> arch           i386
> os             mingw32
> system         i386, mingw32
> status         alpha
> major          3
> minor          0.0
> year           2013
> month          03
> day            16
> svn rev        62282
> language       R
> version.string R version 3.0.0 alpha (2013-03-16 r62282)
> nickname       Unsuffered Consequences
>>
>
> Thanks,
>
> Wayne
>
> ----------------------------------------------------------------------------------------
> Wayne Jones
> Statistics & Chemometrics
> Shell Global Solutions (UK)
> Shell Technology Centre Thornton
> P.O. Box 1, Chester CH1 3SH, United Kingdom
> Tel: +44 (0) 151 373 5977 Mobile: +44 (0) 7896 536026
> Email: Wayne.W.Jones at shell.com<mailto:Wayne.W.Jones at shell.com>
> Intranet: Statistics and Chemometrics - Shell Wiki<http://sww.wiki.shell.com/wiki/index.php/Statistics_and_Chemometrics>
> Internet: Shell Global Solutions<http://www.shell.com/home/content/globalsolutions>
>
> Shell Global Solutions (UK) is a division of Shell Research Limited which has its Registered Office at Shell Centre, London SE1 7NA and is registered in England & Wales with No.539964.
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From ligges at statistik.tu-dortmund.de  Tue Mar 19 16:09:38 2013
From: ligges at statistik.tu-dortmund.de (Uwe Ligges)
Date: Tue, 19 Mar 2013 16:09:38 +0100
Subject: [Rd] preparing for R 3.0.0
In-Reply-To: <6D0731B187284CFA85522F8A2CDBC847@rug.nl>
References: <6D0731B187284CFA85522F8A2CDBC847@rug.nl>
Message-ID: <51487FB2.7040502@statistik.tu-dortmund.de>



On 19.03.2013 15:41, Richard D. Morey wrote:
> I am preparing my package for the release of R 3.0.0 and thinking about the suggestion (from Duncan Murdoch) to maintain two versions of the package temporarily. Since my package contains knitr-compiled vignettes, I need to restrict it to 3.0.0 going forward. I'd like to submit a final version for 2.x that will not be updated but compiles the vignettes in the old manner; and a version for 3.x. My thought is that they will have the same version number, with the only difference being how the vignette is compiled and the DESCRIPTION files.
>
> Is it possible that they have the same version number; or, should I send two updates simultaneously with successive version numbers, flagging one as restricted to R >3.0.0?



The one for R >= 3.0.0 should have a larger version number and goes into 
a separate part of the repository (and moved to the main part once 
R-3.0.0 is released), the other one should have a dependency on R < 3.0.0.

Best,
Uwe Ligges

> Thanks,
> Richard
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From maechler at stat.math.ethz.ch  Tue Mar 19 17:16:42 2013
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 19 Mar 2013 17:16:42 +0100
Subject: [Rd] removing union class
In-Reply-To: <CAHavPHGHnLEZK6oO9t0A6pCocizo1Anqk37EbzrVH5V1ThjqYA@mail.gmail.com>
References: <CAHavPHES4xxZ6+wUXBG54ED8Y3OcOyxrCRUEMhJaB2hsepG3Xw@mail.gmail.com>
	<20802.64086.443586.813352@stat.math.ethz.ch>
	<20802.64898.70553.300100@stat.math.ethz.ch>
	<CAHavPHGHnLEZK6oO9t0A6pCocizo1Anqk37EbzrVH5V1ThjqYA@mail.gmail.com>
Message-ID: <20808.36714.521516.852157@stat.math.ethz.ch>

>>>>> "RG" == Renaud Gaujoux <renaud at mancala.cbio.uct.ac.za>
>>>>>     on Sun, 17 Mar 2013 10:38:44 +0200 writes:

    RG> Late report is better than never isn't it? :)
    >> > Well,... you forgot to show the error (and the
    >> traceback) :
    >> 

    RG> Apologies, I usually include them (and sessionInfo ...).


    >> 
    >> > Note that this problem is somewhat dependent on the use
    >> of the > infamous "matrix" class {not properly defined as
    >> a class in S3, > as it may or may not have dimnames, and
    >> then tried to be made S4 > compatible "as well as
    >> possible" in the methods package, see
    >> 

    RG> Ok. What I want to do is to define a class in my
    RG> namespace that gather some matrix-like classes so that I
    RG> can define a set of common functions for them.
    RG> Initially I want matrix and ExpressionSet objects, and
    RG> possibly add array objects later.  Union classes seem to
    RG> be exactly the way to go, but maybe there is an
    RG> alternative?

There's really *no* problem with class unions.
They work fine and nicely... we also use them in the Matrix
package (part of the R distro).

The problem you've reported is only a propblem of removeClass().

Martin



    >> 
    >> > Of course, I could not have thought of a realistic
    >> situation > where this was a problem, but then you
    >> exemplify one :
    >> 
    >> >> Hadley, this is problematic for devtools, because
    >> load_all tries to cleanup >> S4 classes when an error
    >> occurs when loading a development package and >> crashes
    >> with no hint on the original error.
    >> 
    >> 
    RG> I guess a quick fix for devtools, would be to wrap the
    RG> cleanup procedure into a try or tryCatch (Hadley?).


    RG> Thanks, Renaud


From maechler at stat.math.ethz.ch  Tue Mar 19 17:25:54 2013
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 19 Mar 2013 17:25:54 +0100
Subject: [Rd] the case of building R snapshot without svn nor
	network	connection.
In-Reply-To: <2F0444D6-24AC-44B0-8413-8F41A2B5D983@r-project.org>
References: <1363443222.94887.YahooMailClassic@web172302.mail.ir2.yahoo.com>
	<2F0444D6-24AC-44B0-8413-8F41A2B5D983@r-project.org>
Message-ID: <20808.37266.948912.217540@stat.math.ethz.ch>

>>>>> Simon Urbanek <simon.urbanek at r-project.org>
>>>>>     on Sat, 16 Mar 2013 10:39:42 -0400 writes:

    > On Mar 16, 2013, at 10:13 AM, Hin-Tak Leung wrote:
    >> Network access is *not* a given, nor is the privilege of
    >> installing arbitrary "uncertified" and "non-essential"
    >> tools - whatever the meaning of "uncertified" and
    >> "non-essential" are, those being defined, as is "design
    >> goal", etc, by some small committee.
    >> 
    >> It is a very common scenario, e.g. banks & telecom, some
    >> part of public/government service and health care. This
    >> does not seem to sink in without repeating.
    >> 

    > Network access is not needed to build R - apparently that
    > fact did not seem to sink in, either. This entire thread
    > is based on false assumptions and as such the only place
    > for it is /dev/null

Indeed!

Hin-Tak, do you really think that we, R core, would cripple
ourselves in such a way ??
I bet that almost all of us build R (from svn) without internet
connection many times a year.

But we do follow the build presciptions (*) which you have taken
enormous lengths *not* to go along with.

Martin

---
(*) builddir != srcdir


    >> --- On Sat, 16/3/13, Hin-Tak Leung
    >> <htl10 at users.sourceforge.net> wrote:
    >> 
    >>> I'll quantify the first part - R is perhaps the only
    >>> public software project hosted on a subversion
    >>> repository for which the result of 'svn export ...' does
    >>> not build. Not only that it does not build, but make it
    >>> a feature that it does not build.
    >>> 
    >>> Very few other projects actively try to go in that
    >>> direction.
    >>> 
    >>> --- On Fri, 15/3/13, Hin-Tak Leung
    >>> <hintak_leung at yahoo.co.uk> wrote:
    >>> 
    >>>> The decision to actively discourage non-subsersion
    >>>> usage of snapshot build is already made (r62183). So I
    >>>> am just here to register a differing opinion.
    >>>> 
    >>>> - it is not about subversion vs
    >>> other-version-control-tools.
    >>>> There are two parts of R's dev build process which
    >>> requires
    >>>> an active network connection - tools/rsync-recommended
    >>> and
    >>>> capturing `svn info` into R's headers. The former can
    >>> be
    >>>> overridden with "./configure
    >>>> --with-recommended-packages=no". A few changes had been
    >>> made
    >>>> in the last 6 months to make the latter mandatory. It
    >>> used
    >>>> to be optional.
    >>>> 
    >>>> --- there are genuine needs for testing snapshots in a
    >>>> non-networked minimalist environment - e.g. banks or
    >>> telecom
    >>>> industries - where one wants a "standardised host"
    >>> build,
    >>>> and often it means an "outdated host"; or in a virtual
    >>>> machine environment - which are non-networked for
    >>> security
    >>>> reasons, and also do not have tools beyond necessary
    >>> for
    >>>> compiling and building. This is quite a common
    >>> scenario.
    >>>> 
    >>>> --- AFAIK, 6 months ago, a snapshot copied to an
    >>>> non-networked host will build with "./configure
    >>>> --with-recommended-packages=no". Of course copying
    >>> those
    >>>> recommended package tar balls across would be nicer.
    >>> This is
    >>>> sadly no longer the case.
    >>>> 
    >>>> - dependent on `svn info` and sitting on top of a svn
    >>>> checkout possibly also affects cross-compiling. But
    >>> then, it
    >>>> is not clear whether it is still possible to
    >>> cross-compile
    >>>> R, since quite a few changes have been made to remove
    >>> the
    >>>> capability of cross-compiling R for windows on unix
    >>> hosts in
    >>>> the last few years.
    >>>> 
    >>>> - testing dev snapshots is about trying things and
    >>> fixing
    >>>> bugs before release. Making it difficult for non-core
    >>> people
    >>>> to "try", seem to go against that ethos. If that's the
    >>> case,
    >>>> maybe the repository could be closed off to anonymous
    >>> check
    >>>> outs altogether, just to make it clear that testing
    >>>> snapshots before releases or even close to release, is
    >>> not
    >>>> welcomed. Just a thought.
    >>>> 
    >>>> - although the official repository of the "main" linux
    >>>> kernel branch is git-based, there are mercurial
    >>> mirrors;
    >>>> AFAIK the digital video broadcasting hardware support
    >>> of the
    >>>> linux kernel is officially in mercurial repositories,
    >>> but
    >>>> have git mirrors; likewise much of Xorg's is in
    >>> mercurial
    >>>> but have git mirrors. I haven't heard of any much
    >>> bigger
    >>>> public projects than R where some actively discourage
    >>>> others.
    >>>> 
    >>>> - To be honest r62183 itself is probably a good reason
    >>> to
    >>>> move away from server-side repositories to distributed
    >>>> repositories (hg/git/arch/bzr). Local enhancements -
    >>> i.e. an
    >>>> in-house fork - some of which are never going upstream,
    >>> such
    >>>> as a local revert of r62183 (or a local revert of any
    >>> other
    >>>> upstream commits) is one reason why some have
    >>> distributed
    >>>> repositories.
    >>>> 
    >>>> Lastly, a minor grip. The current head of the HK
    >>> government
    >>>> is probably sometimes called "HK Leung", just as some
    >>> might
    >>>> call a certain old lady "UK Windsor" and a certain
    >>> black
    >>>> person "US Obama".
    >>>> 
    >>> 
    >> 
    >> ______________________________________________
    >> R-devel at r-project.org mailing list
    >> https://stat.ethz.ch/mailman/listinfo/r-devel
    >> 
    >> 

    > ______________________________________________
    > R-devel at r-project.org mailing list
    > https://stat.ethz.ch/mailman/listinfo/r-devel


From maechler at stat.math.ethz.ch  Tue Mar 19 17:30:45 2013
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 19 Mar 2013 17:30:45 +0100
Subject: [Rd] numerics from a factor
In-Reply-To: <51445FB7.9090503@bht-berlin.de>
References: <mailman.27.1363431608.4225.r-devel@r-project.org>
	<51445FB7.9090503@bht-berlin.de>
Message-ID: <20808.37557.715432.547904@stat.math.ethz.ch>

>>>>> Ulrike Gr?mping <groemping at bht-berlin.de>
>>>>>     on Sat, 16 Mar 2013 13:04:07 +0100 writes:

    > Hi Terry, you can use type.convert instead of as.numeric
    > for numbers with decimals:

    > type.convert(levels(factor(1:6/2)),  dec=unlist(options("OutDec")))

    > Best, Ulrike

a late and minor remark: If you use the above, a slightly more expressive
preferred way is 

  type.convert(levels(factor(1:6/2)),  dec = getOption("OutDec"))

Martin


    > Am 16.03.2013 12:00, schrieb r-devel-request at r-project.org:

   [...]

    >> A problem has been pointed out by a French user of the survival package and I'm looking
    >> for a pointer.
    >> 
    >> > options(OutDec= ",")
    >> > fit <- survfit(Surv(1:6 /2) ~ 1)
    >> > fit$time
    >> [1] NA  1 NA  2 NA  3
    >> 
    >> A year or two ago some test cases that broke survfit were presented to me. The heart of
    >> the problem was numbers that were almost identical, where table(x) and unique(x) gave
    >> different counts of distinct values.
    >> The solution was to use "ftime <- factor(time)" at the top of the code, and do all the
    >> calulations using the integer levels of the factor as the unique time points.  At the very
    >> end the numeric component "time" of the result is created using
    >> as.numeric(levels(ftime)).  It's this last line that breaks.
    >> 
    >> I could set the OutDec option within survfit and reset when I leave using on.exit.  Any
    >> other simple solutions?  Any other ways I could get caught by this issue?
    >> 
    >> Terry Therneau


From therneau at mayo.edu  Tue Mar 19 18:01:57 2013
From: therneau at mayo.edu (Terry Therneau)
Date: Tue, 19 Mar 2013 12:01:57 -0500
Subject: [Rd] numerics from a factor
In-Reply-To: <20808.37557.715432.547904@stat.math.ethz.ch>
References: <mailman.27.1363431608.4225.r-devel@r-project.org>
	<51445FB7.9090503@bht-berlin.de>
	<20808.37557.715432.547904@stat.math.ethz.ch>
Message-ID: <51489A05.6070408@mayo.edu>

Thanks Martin.  I had already changed the second argument to getOption.

By the way, per an off list comment from Brian R the bug I was addressing won't affect 
anyone using R as shipped; the default decimal separator is "." whatever the region.  It 
only bit those who set the OutDec option themselves.

Terry T.


On 03/19/2013 11:30 AM, Martin Maechler wrote:
>      >  Hi Terry, you can use type.convert instead of as.numeric
>      >  for numbers with decimals:
>
>      >  type.convert(levels(factor(1:6/2)),  dec=unlist(options("OutDec")))
>
>      >  Best, Ulrike
>
> a late and minor remark: If you use the above, a slightly more expressive
> preferred way is
>
>    type.convert(levels(factor(1:6/2)),  dec = getOption("OutDec"))
>
> Martin


From htl10 at users.sourceforge.net  Tue Mar 19 18:44:13 2013
From: htl10 at users.sourceforge.net (Hin-Tak Leung)
Date: Tue, 19 Mar 2013 17:44:13 +0000 (GMT)
Subject: [Rd] the case of building R snapshot without svn nor network
	connection.
In-Reply-To: <1363443222.94887.YahooMailClassic@web172302.mail.ir2.yahoo.com>
Message-ID: <1363715053.67097.YahooMailClassic@web172304.mail.ir2.yahoo.com>

--- On Sat, 16/3/13, Hin-Tak Leung <htl10 at users.sourceforge.net> wrote:

> Network access is *not* a given, nor
> is the privilege of installing arbitrary "uncertified" and
> "non-essential" tools - whatever the meaning of
> "uncertified" and "non-essential" are, those being defined,
> as is "design goal", etc, by some small committee.
> 
> It is a very common scenario, e.g. banks & telecom, some
> part of public/government service and health care. This does
> not seem to sink in without repeating.

Oh, and some might call using a tool beyond its initial purpose and context an enhancement, rather than 'misuse'. Quite a lot of words, like 'certified', 'essential', have no meaning outside the small group of people who use and misuse them, as is 'misuse'.

> --- On Sat, 16/3/13, Hin-Tak Leung <htl10 at users.sourceforge.net>
> wrote:
> 
> > I'll quantify the first part - R is
> > perhaps the only public software project hosted on a
> > subversion repository for which the result of 'svn
> export
> > ...' does not build. Not only that it does not build,
> but
> > make it a feature that it does not build.
> >
> > Very few other projects actively try to go in that
> > direction.
> > 
> > --- On Fri, 15/3/13, Hin-Tak Leung <hintak_leung at yahoo.co.uk>
> > wrote:
> > 
> > > The decision to actively discourage
> > > non-subsersion usage of snapshot build is already
> made
> > > (r62183). So I am just here to register a
> differing
> > > opinion.
> > > 
> > > - it is not about subversion vs
> > other-version-control-tools.
> > > There are two parts of R's dev build process
> which
> > requires
> > > an active network connection -
> tools/rsync-recommended
> > and
> > > capturing `svn info` into R's headers. The former
> can
> > be
> > > overridden with "./configure
> > > --with-recommended-packages=no". A few changes had
> been
> > made
> > > in the last 6 months to make the latter mandatory.
> It
> > used
> > > to be optional.
> > > 
> > > --- there are genuine needs for testing snapshots
> in a
> > > non-networked minimalist environment - e.g. banks
> or
> > telecom
> > > industries - where one wants a "standardised
> host"
> > build,
> > > and often it means an "outdated host"; or in a
> virtual
> > > machine environment - which are non-networked for
> > security
> > > reasons, and also do not have tools beyond
> necessary
> > for
> > > compiling and building. This is quite a common
> > scenario.
> > > 
> > > --- AFAIK, 6 months ago, a snapshot copied to an
> > > non-networked host will build with "./configure
> > > --with-recommended-packages=no". Of course
> copying
> > those
> > > recommended package tar balls across would be
> nicer.
> > This is
> > > sadly no longer the case.
> > > 
> > > - dependent on `svn info` and sitting on top of a
> svn
> > > checkout possibly also affects cross-compiling.
> But
> > then, it
> > > is not clear whether it is still possible to
> > cross-compile
> > > R, since quite a few changes have been made to
> remove
> > the
> > > capability of cross-compiling R for windows on
> unix
> > hosts in
> > > the last few years. 
> > > 
> > > - testing dev snapshots is about trying things
> and
> > fixing
> > > bugs before release. Making it difficult for
> non-core
> > people
> > > to "try", seem to go against that ethos. If that's
> the
> > case,
> > > maybe the repository could be closed off to
> anonymous
> > check
> > > outs altogether, just to make it clear that
> testing
> > > snapshots before releases or even close to
> release, is
> > not
> > > welcomed. Just a thought.
> > > 
> > > - although the official repository of the "main"
> linux
> > > kernel branch is git-based, there are mercurial
> > mirrors;
> > > AFAIK the digital video broadcasting hardware
> support
> > of the
> > > linux kernel is officially in mercurial
> repositories,
> > but
> > > have git mirrors; likewise much of Xorg's is in
> > mercurial
> > > but have git mirrors. I haven't heard of any much
> > bigger
> > > public projects than R where some actively
> discourage
> > > others.
> > > 
> > > - To be honest r62183 itself is probably a good
> reason
> > to
> > > move away from server-side repositories to
> distributed
> > > repositories (hg/git/arch/bzr). Local enhancements
> -
> > i.e. an
> > > in-house fork - some of which are never going
> upstream,
> > such
> > > as a local revert of r62183 (or a local revert of
> any
> > other
> > > upstream commits) is one reason why some have
> > distributed
> > > repositories.
> > > 
> > > Lastly, a minor grip. The current head of the HK
> > government
> > > is probably sometimes called "HK Leung", just as
> some
> > might
> > > call a certain old lady "UK Windsor" and a
> certain
> > black
> > > person "US Obama".
> > >
> >
>


From renaud at mancala.cbio.uct.ac.za  Wed Mar 20 07:16:55 2013
From: renaud at mancala.cbio.uct.ac.za (Renaud Gaujoux)
Date: Wed, 20 Mar 2013 08:16:55 +0200
Subject: [Rd] removing union class
In-Reply-To: <20808.36714.521516.852157@stat.math.ethz.ch>
References: <CAHavPHES4xxZ6+wUXBG54ED8Y3OcOyxrCRUEMhJaB2hsepG3Xw@mail.gmail.com>
	<20802.64086.443586.813352@stat.math.ethz.ch>
	<20802.64898.70553.300100@stat.math.ethz.ch>
	<CAHavPHGHnLEZK6oO9t0A6pCocizo1Anqk37EbzrVH5V1ThjqYA@mail.gmail.com>
	<20808.36714.521516.852157@stat.math.ethz.ch>
Message-ID: <CAHavPHEL_M1Rh4jwRBDrGj+Kjk7dmxKFNByODy9M97hd+ZudKQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20130320/2a66b98e/attachment.pl>

From pburns at pburns.seanet.com  Wed Mar 20 12:31:31 2013
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Wed, 20 Mar 2013 11:31:31 +0000
Subject: [Rd] fortune?
Message-ID: <51499E13.1060701@pburns.seanet.com>

Brian Ripley:

If things are not readily available in R it is always good to pause and 
reflect if there might be a good reason.

In the R-help thread: How to get the t-stat for arima()?

Pat

-- 
Patrick Burns
pburns at pburns.seanet.com
twitter: @burnsstat @portfolioprobe
http://www.portfolioprobe.com/blog
http://www.burns-stat.com
(home of:
  'Impatient R'
  'The R Inferno'
  'Tao Te Programming')


From pdalgd at gmail.com  Wed Mar 20 13:28:10 2013
From: pdalgd at gmail.com (peter dalgaard)
Date: Wed, 20 Mar 2013 13:28:10 +0100
Subject: [Rd] Deprecating partial matching in $.data.frame
Message-ID: <A0DE2631-213D-4DB2-A952-5DBFA7A7F14C@gmail.com>

Allowing partial matching on $-extraction has always been a source of accidents. Recently, someone who shall remain nameless tried names(mydata) <- "d^2" followed by mydata$d^2. 

As variables in a data frame are generally considered similar to variables in, say, the global environment, it seems strange that foo$bar can give you the content of foo$bartender.

In R-devel (i.e., *not* R-3.0.0 beta, but 3.1.0-to-be) partial matches now gives a warning. 

Of course, it is inevitable that lazy programmers will have been using code like

> anova(fit1)$P
[1] 0.0008866369           NA
Warning message:
In `$.data.frame`(anova(fit1), P) : Name partially matched in data frame

and now get the warning during package checks. This can always be removed by spelling out the column name, as in

> anova(fit1)$`Pr(>F)`
[1] 0.0008866369           NA

or by explicitly specifying a partial match with

> anova(fit1)[["P", exact=FALSE]]
[1] 0.0008866369           NA


-- 
Peter Dalgaard, Professor
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From mdowle at mdowle.plus.com  Wed Mar 20 13:56:35 2013
From: mdowle at mdowle.plus.com (Matthew Dowle)
Date: Wed, 20 Mar 2013 12:56:35 +0000
Subject: [Rd] double in summary.c : isum
Message-ID: <5595fbfb430053bff583ff078ed607ef@imap.plus.net>


Hi,

Please consider the following :

> x = as.integer(2^30-1)
[1] 1073741823
> sum(c(rep(x, 10000000), rep(-x,9999999)))
[1] 1073741824

Tested on 2.15.2 and a recent R-devel (r62132).

I'm wondering if s in isum could be LDOUBLE instead of double, like 
rsum, to fix this edge case?

https://svn.r-project.org/R/trunk/src/main/summary.c

Thanks,
Matthew


From h.wickham at gmail.com  Wed Mar 20 16:23:34 2013
From: h.wickham at gmail.com (Hadley Wickham)
Date: Wed, 20 Mar 2013 10:23:34 -0500
Subject: [Rd] Deprecating partial matching in $.data.frame
In-Reply-To: <A0DE2631-213D-4DB2-A952-5DBFA7A7F14C@gmail.com>
References: <A0DE2631-213D-4DB2-A952-5DBFA7A7F14C@gmail.com>
Message-ID: <CABdHhvEb6cg2Jtgmoor-H8-YHpoEK=RJSnDH6kN7kE46yxmF6g@mail.gmail.com>

On Wed, Mar 20, 2013 at 7:28 AM, peter dalgaard <pdalgd at gmail.com> wrote:
> Allowing partial matching on $-extraction has always been a source of accidents. Recently, someone who shall remain nameless tried names(mydata) <- "d^2" followed by mydata$d^2.
>
> As variables in a data frame are generally considered similar to variables in, say, the global environment, it seems strange that foo$bar can give you the content of foo$bartender.
>
> In R-devel (i.e., *not* R-3.0.0 beta, but 3.1.0-to-be) partial matches now gives a warning.

Just for data frames, or also for lists?

I think this is a fantastic change, but I do worry a little that it is
going to generate warnings for a _lot_ of existing code.

Hadley

-- 
Chief Scientist, RStudio
http://had.co.nz/


From wdunlap at tibco.com  Wed Mar 20 16:59:12 2013
From: wdunlap at tibco.com (William Dunlap)
Date: Wed, 20 Mar 2013 15:59:12 +0000
Subject: [Rd] Deprecating partial matching in $.data.frame
In-Reply-To: <A0DE2631-213D-4DB2-A952-5DBFA7A7F14C@gmail.com>
References: <A0DE2631-213D-4DB2-A952-5DBFA7A7F14C@gmail.com>
Message-ID: <E66794E69CFDE04D9A70842786030B931B918849@PA-MBX04.na.tibco.com>

Will you be doing the same for attribute names?

  > options(prompt=with(version, paste0(language,"-",major,".",minor,"> ")))
  R-2.15.3> x <- structure(17, AnAttr="an attribute", Abcd="a b c d")
  R-2.15.3> attr(x, "A")
  NULL
  R-2.15.3> attr(x, "An")
  [1] "an attribute"
  R-2.15.3> attr(x, "Ab")
  [1] "a b c d"

How will you deal with the common idiom of using is.null(x$n)
to see if x has a compnent named "n"?  One would not want
a warning if x had a component called "nn".

Bill Dunlap
Spotfire, TIBCO Software
wdunlap tibco.com


> -----Original Message-----
> From: r-devel-bounces at r-project.org [mailto:r-devel-bounces at r-project.org] On Behalf
> Of peter dalgaard
> Sent: Wednesday, March 20, 2013 5:28 AM
> To: r-devel at r-project.org
> Subject: [Rd] Deprecating partial matching in $.data.frame
> 
> Allowing partial matching on $-extraction has always been a source of accidents.
> Recently, someone who shall remain nameless tried names(mydata) <- "d^2" followed by
> mydata$d^2.
> 
> As variables in a data frame are generally considered similar to variables in, say, the
> global environment, it seems strange that foo$bar can give you the content of
> foo$bartender.
> 
> In R-devel (i.e., *not* R-3.0.0 beta, but 3.1.0-to-be) partial matches now gives a warning.
> 
> Of course, it is inevitable that lazy programmers will have been using code like
> 
> > anova(fit1)$P
> [1] 0.0008866369           NA
> Warning message:
> In `$.data.frame`(anova(fit1), P) : Name partially matched in data frame
> 
> and now get the warning during package checks. This can always be removed by spelling
> out the column name, as in
> 
> > anova(fit1)$`Pr(>F)`
> [1] 0.0008866369           NA
> 
> or by explicitly specifying a partial match with
> 
> > anova(fit1)[["P", exact=FALSE]]
> [1] 0.0008866369           NA
> 
> 
> --
> Peter Dalgaard, Professor
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From pdalgd at gmail.com  Wed Mar 20 17:16:26 2013
From: pdalgd at gmail.com (peter dalgaard)
Date: Wed, 20 Mar 2013 17:16:26 +0100
Subject: [Rd] Deprecating partial matching in $.data.frame
In-Reply-To: <CABdHhvEb6cg2Jtgmoor-H8-YHpoEK=RJSnDH6kN7kE46yxmF6g@mail.gmail.com>
References: <A0DE2631-213D-4DB2-A952-5DBFA7A7F14C@gmail.com>
	<CABdHhvEb6cg2Jtgmoor-H8-YHpoEK=RJSnDH6kN7kE46yxmF6g@mail.gmail.com>
Message-ID: <615CDFB0-1E10-4F84-A920-849BDC806A70@gmail.com>


On Mar 20, 2013, at 16:23 , Hadley Wickham wrote:

> On Wed, Mar 20, 2013 at 7:28 AM, peter dalgaard <pdalgd at gmail.com> wrote:
>> Allowing partial matching on $-extraction has always been a source of accidents. Recently, someone who shall remain nameless tried names(mydata) <- "d^2" followed by mydata$d^2.
>> 
>> As variables in a data frame are generally considered similar to variables in, say, the global environment, it seems strange that foo$bar can give you the content of foo$bartender.
>> 
>> In R-devel (i.e., *not* R-3.0.0 beta, but 3.1.0-to-be) partial matches now gives a warning.
> 
> Just for data frames, or also for lists?

Just for data frames, at least for now. For lists, there are just too many uses of chisq.test()$exp etc. (I nearly wrote t.test()$p, but that doesn't actually work!)

> 
> I think this is a fantastic change, but I do worry a little that it is
> going to generate warnings for a _lot_ of existing code.

We'll see about that, but I expect it not to be all that bad. In general purpose code, you need to have a situation where the data frame has known column names, and the one that you want is sufficiently awkward to type.  The p-value column in anova is about the only realistic scenario that I can come up with. The ones in, e.g., summary.lm are in a matrix, not a data frame.  

> 
> Hadley
> 
> -- 
> Chief Scientist, RStudio
> http://had.co.nz/

-- 
Peter Dalgaard, Professor
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From pdalgd at gmail.com  Wed Mar 20 17:26:58 2013
From: pdalgd at gmail.com (peter dalgaard)
Date: Wed, 20 Mar 2013 17:26:58 +0100
Subject: [Rd] Deprecating partial matching in $.data.frame
In-Reply-To: <E66794E69CFDE04D9A70842786030B931B918849@PA-MBX04.na.tibco.com>
References: <A0DE2631-213D-4DB2-A952-5DBFA7A7F14C@gmail.com>
	<E66794E69CFDE04D9A70842786030B931B918849@PA-MBX04.na.tibco.com>
Message-ID: <A3631670-5E8D-48DC-9868-8C9EE8852FE6@gmail.com>


On Mar 20, 2013, at 16:59 , William Dunlap wrote:

> Will you be doing the same for attribute names?

Not at this point. 

> 
>> options(prompt=with(version, paste0(language,"-",major,".",minor,"> ")))
>  R-2.15.3> x <- structure(17, AnAttr="an attribute", Abcd="a b c d")
>  R-2.15.3> attr(x, "A")
>  NULL
>  R-2.15.3> attr(x, "An")
>  [1] "an attribute"
>  R-2.15.3> attr(x, "Ab")
>  [1] "a b c d"
> 
> How will you deal with the common idiom of using is.null(x$n)
> to see if x has a compnent named "n"?  One would not want
> a warning if x had a component called "nn".

Why not? If you were looking for x$n, you're getting the wrong answer.


> 
> Bill Dunlap
> Spotfire, TIBCO Software
> wdunlap tibco.com
> 
> 
>> -----Original Message-----
>> From: r-devel-bounces at r-project.org [mailto:r-devel-bounces at r-project.org] On Behalf
>> Of peter dalgaard
>> Sent: Wednesday, March 20, 2013 5:28 AM
>> To: r-devel at r-project.org
>> Subject: [Rd] Deprecating partial matching in $.data.frame
>> 
>> Allowing partial matching on $-extraction has always been a source of accidents.
>> Recently, someone who shall remain nameless tried names(mydata) <- "d^2" followed by
>> mydata$d^2.
>> 
>> As variables in a data frame are generally considered similar to variables in, say, the
>> global environment, it seems strange that foo$bar can give you the content of
>> foo$bartender.
>> 
>> In R-devel (i.e., *not* R-3.0.0 beta, but 3.1.0-to-be) partial matches now gives a warning.
>> 
>> Of course, it is inevitable that lazy programmers will have been using code like
>> 
>>> anova(fit1)$P
>> [1] 0.0008866369           NA
>> Warning message:
>> In `$.data.frame`(anova(fit1), P) : Name partially matched in data frame
>> 
>> and now get the warning during package checks. This can always be removed by spelling
>> out the column name, as in
>> 
>>> anova(fit1)$`Pr(>F)`
>> [1] 0.0008866369           NA
>> 
>> or by explicitly specifying a partial match with
>> 
>>> anova(fit1)[["P", exact=FALSE]]
>> [1] 0.0008866369           NA
>> 
>> 
>> --
>> Peter Dalgaard, Professor
>> Center for Statistics, Copenhagen Business School
>> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
>> Phone: (+45)38153501
>> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>> 
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
Peter Dalgaard, Professor
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From karl.forner at gmail.com  Wed Mar 20 17:41:04 2013
From: karl.forner at gmail.com (Karl Forner)
Date: Wed, 20 Mar 2013 17:41:04 +0100
Subject: [Rd] How to avoid using gridextra via Depends instead of Imports in
 a package ?
Message-ID: <CAMd4_Adf9zJTN8qXgDzqbDGwKd9Tb2-JXZEH9g=k2xaaz9khzA@mail.gmail.com>

Hello,

I really need some insight on a problem we encountered using grid,
lattice and gridExtra.

I tried to reduce the problem, so the plot make no sense.

we have a package: gridextrabug

with:

DESCRIPTION
------------------
Package: gridextrabug
Title: gridextrabug
Version: 0.1
Author: toto
Maintainer: toto <karl.forner at quartzbio.com>
Description: gridextrabug
Imports:
    grid,
    gridExtra,
    lattice,
    latticeExtra,
    reshape,
Depends:
    R (>= 2.15),
    methods
Suggests:
    testthat,
    devtools
License: GPL (>= 3)
Collate:
    'zzz.R'
    'plotFDR.R'

R/plotFDR.R
----------------
plot_fdr <- function(dt,qvalue_col,pvalue_col, zoom_x=NULL, zoom_y=NULL,
		fdrLimit=0,overview_plot=FALSE,...)
{
		
	frm <- as.formula(paste(qvalue_col,"~ rank(",pvalue_col,")"))
	plt <- xyplot( frm ,
			data=dt,
			abline=list(h=fdrLimit,lty="dashed"),
			pch=16,cex=1,
			type="p",
			panel=panelinplot2,
			subscripts= TRUE,

        )
	
	return(plt)
}

panelinplot2 <- function(x,y,subscripts,cex,type,...){

    panel.xyplot(x,y,subscripts=subscripts,
        ylim=c(0,1),
        type=type,
        cex=cex,...)
    pltoverview <- xyplot(y~x,xlab=NULL,
        ylab=NULL,
        type="l",
        par.settings=qb_theme_nopadding(),
        scales=list(draw=FALSE),
        cex=0.6,...)
    gr <- grob(p=pltoverview, ..., cl="lattice")


    grid.draw(gr) # <-----------------------------------------------
problematic call
}

NAMESPACE
------------------
export(panelinplot2)
export(plot_fdr)
importFrom(grid,gpar)
importFrom(grid,grid.draw)
importFrom(grid,grid.rect)
importFrom(grid,grid.text)
importFrom(grid,grob)
importFrom(grid,popViewport)
importFrom(grid,pushViewport)
importFrom(grid,unit)
importFrom(grid,viewport)
importFrom(gridExtra,drawDetails.lattice)
importFrom(lattice,ltext)
importFrom(lattice,panel.segments)
importFrom(lattice,panel.xyplot)
importFrom(lattice,stripplot)
importFrom(lattice,xyplot)
importFrom(latticeExtra,as.layer)
importFrom(latticeExtra,layer)
importFrom(reshape,sort_df)

Then if you execute this script:

without_extra.R
------------------
library(gridextrabug)
p <- seq(10^-10,1,0.001)
p <- p[sample(1:length(p))]
q <- p.adjust(p, "BH")
df <- data.frame(p,q)


plt <-	plot_fdr(df,qvalue_col= "q", pvalue_col="p",
    zoom_x=c(0,20),
    fdrLimit=0.6,
    overview_plot=TRUE)
X11()
print(plt)

you will not have the second plot corresponding the call to panelinplot2


If you execute this one:

with_extra.R
------------------
library(gridextrabug)
p <- seq(10^-10,1,0.001)
p <- p[sample(1:length(p))]
q <- p.adjust(p, "BH")
df <- data.frame(p,q)


plt <-	plot_fdr(df,qvalue_col= "q", pvalue_col="p",
    zoom_x=c(0,20),
    fdrLimit=0.6,
    overview_plot=TRUE)
X11()

library(gridExtra)
print(plt)

you will have the second plot.


>From what I understood, the last line of panelinplot2(), "
grid.draw(x)", dispatches to  grid:::grid.draw.grob(), which in turn
calls grid:::drawGrob(), which calls grid::drawDetails() which is a S3
generic.
The gridExtra package defines the method drawDetails.lattice().
When the package is loaded in the search() path,  the "grid.draw(x)"
call dispatches to gridExtra:::drawDetails.lattice().

We would rather avoid messing with the search path, which is a best
practice if I'm not mistaken, so we tried hard to solve it using
Imports.
But I came to realize that the problem was in the grid namespace, not
in our package namespace.

I tested it with the following work-around:
parent.env(parent.env(getNamespace('grid'))) <- getNamespace('gridExtra')

which works.

So my questions are:
  * did we miss something obvious ?
  * what is the proper way to handle this situation ?


Thanks in advance for your wisdom.

Karl Forner


From nalimilan at club.fr  Wed Mar 20 17:54:49 2013
From: nalimilan at club.fr (Milan Bouchet-Valat)
Date: Wed, 20 Mar 2013 17:54:49 +0100
Subject: [Rd] Deprecating partial matching in $.data.frame
In-Reply-To: <615CDFB0-1E10-4F84-A920-849BDC806A70@gmail.com>
References: <A0DE2631-213D-4DB2-A952-5DBFA7A7F14C@gmail.com>
	<CABdHhvEb6cg2Jtgmoor-H8-YHpoEK=RJSnDH6kN7kE46yxmF6g@mail.gmail.com>
	<615CDFB0-1E10-4F84-A920-849BDC806A70@gmail.com>
Message-ID: <1363798489.15442.43.camel@milan>

Le mercredi 20 mars 2013 ? 17:16 +0100, peter dalgaard a ?crit :
> On Mar 20, 2013, at 16:23 , Hadley Wickham wrote:
> 
> > On Wed, Mar 20, 2013 at 7:28 AM, peter dalgaard <pdalgd at gmail.com>
> wrote:
> >> Allowing partial matching on $-extraction has always been a source
> of accidents. Recently, someone who shall remain nameless tried
> names(mydata) <- "d^2" followed by mydata$d^2.
> >> 
> >> As variables in a data frame are generally considered similar to
> variables in, say, the global environment, it seems strange that foo
> $bar can give you the content of foo$bartender.
> >> 
> >> In R-devel (i.e., *not* R-3.0.0 beta, but 3.1.0-to-be) partial
> matches now gives a warning.
> > 
> > Just for data frames, or also for lists?
> 
> Just for data frames, at least for now. For lists, there are just too
> many uses of chisq.test()$exp etc. (I nearly wrote t.test()$p, but
> that doesn't actually work!)
I also think this is a very good idea, but special-casing data frames is
going to create some confusion in that already complex area. Wouldn't it
make more sense to aim at fixing both lists and data frames in the same
R release?

In a first phase, R CMD check could report errors when partial matching
is detected, but normal R use would not warn: this would leave some time
for package maintainers to fix their code (I guess R CMD check could
enable the warnings as an option while running package tests if
detecting them from static code parsing is not possible). Then, in a
second phase, warnings would be enabled by default for lists and data
frames.


My two cents

> > 
> > I think this is a fantastic change, but I do worry a little that it is
> > going to generate warnings for a _lot_ of existing code.
> 
> We'll see about that, but I expect it not to be all that bad. In
> general purpose code, you need to have a situation where the data
> frame has known column names, and the one that you want is
> sufficiently awkward to type.  The p-value column in anova is about
> the only realistic scenario that I can come up with. The ones in,
> e.g., summary.lm are in a matrix, not a data frame.  
> 
> > 
> > Hadley
> > 
> > -- 
> > Chief Scientist, RStudio
> > http://had.co.nz/
>


From h.wickham at gmail.com  Wed Mar 20 17:58:17 2013
From: h.wickham at gmail.com (Hadley Wickham)
Date: Wed, 20 Mar 2013 11:58:17 -0500
Subject: [Rd] Deprecating partial matching in $.data.frame
In-Reply-To: <A3631670-5E8D-48DC-9868-8C9EE8852FE6@gmail.com>
References: <A0DE2631-213D-4DB2-A952-5DBFA7A7F14C@gmail.com>
	<E66794E69CFDE04D9A70842786030B931B918849@PA-MBX04.na.tibco.com>
	<A3631670-5E8D-48DC-9868-8C9EE8852FE6@gmail.com>
Message-ID: <CABdHhvHdHnOEyS+MKmwKTQpYp=zsfB84QDND-LRo2Sf4DXrRXA@mail.gmail.com>

On Wed, Mar 20, 2013 at 11:26 AM, peter dalgaard <pdalgd at gmail.com> wrote:
>
> On Mar 20, 2013, at 16:59 , William Dunlap wrote:
>
>> Will you be doing the same for attribute names?
>
> Not at this point.

It would be really nice to have consistent behaviour across argument
names, attributes, lists and data frames, at least for R CMD check.

Hadley

-- 
Chief Scientist, RStudio
http://had.co.nz/


From jason at revolutionanalytics.com  Wed Mar 20 20:32:52 2013
From: jason at revolutionanalytics.com (Jason Wood)
Date: Wed, 20 Mar 2013 12:32:52 -0700
Subject: [Rd] Character Encoding: Why are valid Windows-1252 characters
 encoded as invalid ISO-8859-1 characters?
Message-ID: <CAC=9Yd=RGFQhzzgx+h7wzzB5TxK2Gdyrq7cDR381QtiW98nG8Q@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20130320/a708f408/attachment.pl>

From michael.weylandt at gmail.com  Wed Mar 20 20:59:54 2013
From: michael.weylandt at gmail.com (R. Michael Weylandt)
Date: Wed, 20 Mar 2013 19:59:54 +0000
Subject: [Rd] Typo-let in NEWS.Rd
Message-ID: <CAAmySGNNcWndUS_vWcR6s0=aQQQ5ZyTyNx6SK2S-jWXbhwNgKw@mail.gmail.com>

Applicable against current trunk and (I believe) R-3.0.0 branch.

Michael


Index: doc/NEWS.Rd
===================================================================
--- doc/NEWS.Rd	(revision 62340)
+++ doc/NEWS.Rd	(working copy)
@@ -183,7 +183,7 @@
       through as bytes inputs invalid in the current locale.

       \item Specifying both \code{rate} and \code{scale} to
-      \code{[dpar]gamma} is a warning (if they are essentially the same
+      \code{[dpqr]gamma} is a warning (if they are essentially the same
       value) or an error.

       \item \code{merge()} works in more cases where the data frames


From h.wickham at gmail.com  Wed Mar 20 21:12:52 2013
From: h.wickham at gmail.com (Hadley Wickham)
Date: Wed, 20 Mar 2013 15:12:52 -0500
Subject: [Rd] Structure not deparsed correctly when printing calls
Message-ID: <CABdHhvGpmK1Z9sKPte=pxcybH=4Oa7+59hzGNfPMyTFY6XJU_A@mail.gmail.com>

z <- substitute(f(x), list(x = data.frame(y = 1)))

z
# f(list(y = 1))

str(z)
# language f(structure(list(y = 1), .Names = "y", row.names = c(NA,
-1L), class = "data.frame"))

dput(z)
# f(structure(list(y = 1), .Names = "y", row.names = c(NA, -1L), class
= "data.frame"))

Hadley

-- 
Chief Scientist, RStudio
http://had.co.nz/


From r.m.krug at gmail.com  Thu Mar 21 09:25:50 2013
From: r.m.krug at gmail.com (Rainer M Krug)
Date: Thu, 21 Mar 2013 09:25:50 +0100
Subject: [Rd] Deprecating partial matching in $.data.frame
In-Reply-To: <CABdHhvHdHnOEyS+MKmwKTQpYp=zsfB84QDND-LRo2Sf4DXrRXA@mail.gmail.com>
References: <A0DE2631-213D-4DB2-A952-5DBFA7A7F14C@gmail.com>
	<E66794E69CFDE04D9A70842786030B931B918849@PA-MBX04.na.tibco.com>
	<A3631670-5E8D-48DC-9868-8C9EE8852FE6@gmail.com>
	<CABdHhvHdHnOEyS+MKmwKTQpYp=zsfB84QDND-LRo2Sf4DXrRXA@mail.gmail.com>
Message-ID: <514AC40E.2080808@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 20/03/13 17:58, Hadley Wickham wrote:
> On Wed, Mar 20, 2013 at 11:26 AM, peter dalgaard <pdalgd at gmail.com> wrote:
>> 
>> On Mar 20, 2013, at 16:59 , William Dunlap wrote:
>> 
>>> Will you be doing the same for attribute names?
>> 
>> Not at this point.
> 
> It would be really nice to have consistent behaviour across argument names, attributes, lists 
> and data frames, at least for R CMD check.

I agree with Hadley that consistency is quite important. This is especially true for data.frames
and lists, as this concerns the data itself, and not names or attributes of the data.

I would very much like to see at least at the level of R CMD check warnings for *all* partial
matching so that they can be ironed out before in the next stage warnings are give to the user (as
mentioned by Milan).

I was bitten at least once by a bug, which cost me quite some time to figure out, caused by
partial completion and would very much like to see it go (or at least have the option to show
warnings if it occurs).

Cheers,

Rainer



> 
> Hadley
> 


- -- 
Rainer M. Krug, PhD (Conservation Ecology, SUN), MSc (Conservation Biology, UCT), Dipl. Phys.
(Germany)

Centre of Excellence for Invasion Biology
Stellenbosch University
South Africa

Tel :       +33 - (0)9 53 10 27 44
Cell:       +33 - (0)6 85 62 59 98
Fax :       +33 - (0)9 58 10 27 44

Fax (D):    +49 - (0)3 21 21 25 22 44

email:      Rainer at krugs.de

Skype:      RMkrug
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.11 (GNU/Linux)
Comment: Using GnuPG with Thunderbird - http://www.enigmail.net/

iQEcBAEBAgAGBQJRSsQOAAoJENvXNx4PUvmC7zkH/Rp0yFMmgQD9D2Z2EpWm5vGR
T0ojk8WKCeqoGY4IKpCPP0rSKJqPI0HxjdAplOclFSdfBaCDrHdALLaxzqJWG6TJ
346A/lAgdgbJWNTTWMXiXcq2vqDKAvoOVhZ/A1YDo7CzjZsgpcBPzmUZREFNSDKu
TeFNM29GgLIaQ2JqV6wRPQee/j36+iLpcCfACTdsXs0H/kRkcogV96g75OTGsxJr
9pZRzOQpH0fv9DsdLGkOCO1twZ+XtWOKSCmTTcOJ97wBWcYk80jrwJObKFG7qMz7
VVoz38hWjgLKj9RRKSLtEtIfUhNogvT5bayPO3ZBD1jDx8qRfm8BtNV+ofEvnd0=
=akLx
-----END PGP SIGNATURE-----


From therneau at mayo.edu  Thu Mar 21 14:51:46 2013
From: therneau at mayo.edu (Terry Therneau)
Date: Thu, 21 Mar 2013 08:51:46 -0500
Subject: [Rd] R-devel Digest, Vol 121, Issue 20
In-Reply-To: <mailman.37.1363863608.9408.r-devel@r-project.org>
References: <mailman.37.1363863608.9408.r-devel@r-project.org>
Message-ID: <514B1072.4000003@mayo.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20130321/4733c879/attachment.pl>

From pdalgd at gmail.com  Thu Mar 21 14:52:01 2013
From: pdalgd at gmail.com (peter dalgaard)
Date: Thu, 21 Mar 2013 14:52:01 +0100
Subject: [Rd] Deprecating partial matching in $.data.frame
In-Reply-To: <514AC40E.2080808@gmail.com>
References: <A0DE2631-213D-4DB2-A952-5DBFA7A7F14C@gmail.com>
	<E66794E69CFDE04D9A70842786030B931B918849@PA-MBX04.na.tibco.com>
	<A3631670-5E8D-48DC-9868-8C9EE8852FE6@gmail.com>
	<CABdHhvHdHnOEyS+MKmwKTQpYp=zsfB84QDND-LRo2Sf4DXrRXA@mail.gmail.com>
	<514AC40E.2080808@gmail.com>
Message-ID: <9966F1D2-A672-4017-B59D-C05BC86DB603@gmail.com>


On Mar 21, 2013, at 09:25 , Rainer M Krug wrote:

> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
> 
> On 20/03/13 17:58, Hadley Wickham wrote:
>> On Wed, Mar 20, 2013 at 11:26 AM, peter dalgaard <pdalgd at gmail.com> wrote:
>>> 
>>> On Mar 20, 2013, at 16:59 , William Dunlap wrote:
>>> 
>>>> Will you be doing the same for attribute names?
>>> 
>>> Not at this point.
>> 
>> It would be really nice to have consistent behaviour across argument names, attributes, lists 
>> and data frames, at least for R CMD check.
> 
> I agree with Hadley that consistency is quite important. This is especially true for data.frames
> and lists, as this concerns the data itself, and not names or attributes of the data.

Well, maybe consistency is important, but partial matching never worked for $-extraction in environments, so the current change could be considered mainly a nudge of data frames in the direction of environments. After all, both can be thought of as collections of named objects.

General lists are a somewhat different issue. They often, formally or informally, represent classed objects with a defined set of names, typically obtained as return values from functions. Since the names are known, people will have used the expedient of abbreviating them. This can happen with data frames as well, but less commonly, since it is in general unsafe to rely on column names being uniquely defined by any particular prefix.

I.e., deprecating partial matching for lists opens a rather larger can of worms, and might require more extensive code revisions. Also, the performance hit of a runtime check for partial matching might be more important for lists than it is for data frames. It could be worth it to implement an R CMD check warning as you suggest, but perhaps not just now. 

-Peter

-- 
Peter Dalgaard, Professor
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From nalimilan at club.fr  Thu Mar 21 14:59:44 2013
From: nalimilan at club.fr (Milan Bouchet-Valat)
Date: Thu, 21 Mar 2013 14:59:44 +0100
Subject: [Rd] R-devel Digest, Vol 121, Issue 20
In-Reply-To: <514B1072.4000003@mayo.edu>
References: <mailman.37.1363863608.9408.r-devel@r-project.org>
	<514B1072.4000003@mayo.edu>
Message-ID: <1363874384.609.7.camel@milan>

Le jeudi 21 mars 2013 ? 08:51 -0500, Terry Therneau a ?crit :
> I am not in favor of the change, which is a choice of rigor over usability.
> 
> When I am developing code or functions I agree with this, and I view any warnings from R 
> CMD check about shortened arguments as positive feedback.
> 
> But 90% of my usage of R is day to day data analysis, interactive, at the keyboard.  A lot 
> of data sets that come to me have long variable names.  What this change will mean to me 
> is that I'll either spend a lot of time cursing at these annoying frickin warnings, or 
> have to take the time to make new data sets (several times a week) that have abbreviated 
> names.  Of course when I come back to that project 8 weeks later I'll need to recreate the 
> short-long mapping in my head...
> 
> Let's think about WHY abbreviated names were allowed in the first place.  Usability is 
> worth a lot more than purity to the actual users of a package.  S had the rare advantage 
> of serious users just down the hall from the developers, which I hypothesise is one of the 
> true foundation for it's success.  (What user would have invented the hiding aspect S4 
> classes --- "let's put the results of the fit into a steel box so they can't see the 
> parts" --- which is actually touted as a virtue?  I cringe every time a new method I need 
> is sewed up this way.)
> 
> "C is a remarkable language.  ... The success of C is due to a number of factors, none of 
> them key, but all of them important. Perhaps the most significant of all is that C was 
> developed by real practioners of programming and was designed for practical day-to-day 
> use, not for show or for demonstration. Like any well-designed tool, it falls easily to 
> the hand and feels good to use. Instead of providing constraints, checks and rigorous 
> boundaries, it concentrates on providing you with power and on not getting in your way."
>     Preface to "The C Book", M Banahan et al
I would think that the ability to hit the Tab key to trigger name
completion in your R GUI makes partial matching almost useless. The
avantage of interactive completion in the GUI is that you immediately
see the result of the partial matching. So you get the best of both
worlds: no need to type long variable names in full, but no traps when a
match is not what you would expect.

Doesn't this suit your use case?


Regards

> Terry Therneau
> 
> 
> On 03/21/2013 06:00 AM, r-devel-request at r-project.org wrote:
> > Allowing partial matching on $-extraction has always been a source of accidents. 
> > Recently, someone who shall remain nameless tried names(mydata) <- "d^2" followed by 
> > mydata$d^2. As variables in a data frame are generally considered similar to variables 
> > in, say, the global environment, it seems strange that foo$bar can give you the content 
> > of foo$bartender. In R-devel (i.e., *not* R-3.0.0 beta, but 3.1.0-to-be) partial matches 
> > now gives a warning. Of course, it is inevitable that lazy programmers will have been 
> > using code like
> >> >  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From therneau at mayo.edu  Thu Mar 21 15:15:48 2013
From: therneau at mayo.edu (Terry Therneau)
Date: Thu, 21 Mar 2013 09:15:48 -0500
Subject: [Rd] Re Deprecating partial matching in $.data.frame
In-Reply-To: <mailman.37.1363863608.9408.r-devel@r-project.org>
References: <mailman.37.1363863608.9408.r-devel@r-project.org>
Message-ID: <514B1614.2050309@mayo.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20130321/92933100/attachment.pl>

From therneau at mayo.edu  Thu Mar 21 15:48:39 2013
From: therneau at mayo.edu (Terry Therneau)
Date: Thu, 21 Mar 2013 09:48:39 -0500
Subject: [Rd] Depreciating partial matching
In-Reply-To: <1363874384.609.7.camel@milan>
References: <mailman.37.1363863608.9408.r-devel@r-project.org>
	<514B1072.4000003@mayo.edu> <1363874384.609.7.camel@milan>
Message-ID: <514B1DC7.3060706@mayo.edu>

Note: My apolgies for the "Subject" in the original post

On 03/21/2013 08:59 AM, Milan Bouchet-Valat wrote:
> Le jeudi 21 mars 2013 ? 08:51 -0500, Terry Therneau a ?crit :
>> I am not in favor of the change, which is a choice of rigor over usability.
>>
>> When I am developing code or functions I agree with this, and I view any warnings from R
>> CMD check about shortened arguments as positive feedback.
>>
>> But 90% of my usage of R is day to day data analysis, interactive, at the keyboard.  A lot
>> of data sets that come to me have long variable names.  What this change will mean to me
>> is that I'll either spend a lot of time cursing at these annoying frickin warnings, or
>> have to take the time to make new data sets (several times a week) that have abbreviated
>> names.  Of course when I come back to that project 8 weeks later I'll need to recreate the
>> short-long mapping in my head...
>>
>> Let's think about WHY abbreviated names were allowed in the first place.  Usability is
>> worth a lot more than purity to the actual users of a package.  S had the rare advantage
>> of serious users just down the hall from the developers, which I hypothesise is one of the
>> true foundation for it's success.  (What user would have invented the hiding aspect S4
>> classes --- "let's put the results of the fit into a steel box so they can't see the
>> parts" --- which is actually touted as a virtue?  I cringe every time a new method I need
>> is sewed up this way.)
>>
>> "C is a remarkable language.  ... The success of C is due to a number of factors, none of
>> them key, but all of them important. Perhaps the most significant of all is that C was
>> developed by real practioners of programming and was designed for practical day-to-day
>> use, not for show or for demonstration. Like any well-designed tool, it falls easily to
>> the hand and feels good to use. Instead of providing constraints, checks and rigorous
>> boundaries, it concentrates on providing you with power and on not getting in your way."
>>      Preface to "The C Book", M Banahan et al
> I would think that the ability to hit the Tab key to trigger name
> completion in your R GUI makes partial matching almost useless. The
> avantage of interactive completion in the GUI is that you immediately
> see the result of the partial matching. So you get the best of both
> worlds: no need to type long variable names in full, but no traps when a
> match is not what you would expect.
>
> Doesn't this suit your use case?
Good point.  This works well at the command line.  However, not when interacting between 
emacs and R in the way I do.  For reproducability I use and emacs file that is being 
corrected and massaged with chunks submitted to R; at the end I have a clean record of how 
the result was obtained.

>
>
> Regards
>
>> Terry Therneau
>>
>>
>> On 03/21/2013 06:00 AM, r-devel-request at r-project.org wrote:
>>> Allowing partial matching on $-extraction has always been a source of accidents.
>>> Recently, someone who shall remain nameless tried names(mydata)<- "d^2" followed by
>>> mydata$d^2. As variables in a data frame are generally considered similar to variables
>>> in, say, the global environment, it seems strange that foo$bar can give you the content
>>> of foo$bartender. In R-devel (i.e., *not* R-3.0.0 beta, but 3.1.0-to-be) partial matches
>>> now gives a warning. Of course, it is inevitable that lazy programmers will have been
>>> using code like
>>>>>
>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From simon.urbanek at r-project.org  Thu Mar 21 16:00:41 2013
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Thu, 21 Mar 2013 11:00:41 -0400
Subject: [Rd] Depreciating partial matching
In-Reply-To: <514B1DC7.3060706@mayo.edu>
References: <mailman.37.1363863608.9408.r-devel@r-project.org>
	<514B1072.4000003@mayo.edu> <1363874384.609.7.camel@milan>
	<514B1DC7.3060706@mayo.edu>
Message-ID: <30F7521C-5D05-4259-9472-606AF1B36D45@r-project.org>


On Mar 21, 2013, at 10:48 AM, Terry Therneau wrote:

> Note: My apolgies for the "Subject" in the original post
> 
> On 03/21/2013 08:59 AM, Milan Bouchet-Valat wrote:
>> Le jeudi 21 mars 2013 ? 08:51 -0500, Terry Therneau a ?crit :
>>> I am not in favor of the change, which is a choice of rigor over usability.
>>> 
>>> When I am developing code or functions I agree with this, and I view any warnings from R
>>> CMD check about shortened arguments as positive feedback.
>>> 
>>> But 90% of my usage of R is day to day data analysis, interactive, at the keyboard.  A lot
>>> of data sets that come to me have long variable names.  What this change will mean to me
>>> is that I'll either spend a lot of time cursing at these annoying frickin warnings, or
>>> have to take the time to make new data sets (several times a week) that have abbreviated
>>> names.  Of course when I come back to that project 8 weeks later I'll need to recreate the
>>> short-long mapping in my head...
>>> 
>>> Let's think about WHY abbreviated names were allowed in the first place.  Usability is
>>> worth a lot more than purity to the actual users of a package.  S had the rare advantage
>>> of serious users just down the hall from the developers, which I hypothesise is one of the
>>> true foundation for it's success.  (What user would have invented the hiding aspect S4
>>> classes --- "let's put the results of the fit into a steel box so they can't see the
>>> parts" --- which is actually touted as a virtue?  I cringe every time a new method I need
>>> is sewed up this way.)
>>> 
>>> "C is a remarkable language.  ... The success of C is due to a number of factors, none of
>>> them key, but all of them important. Perhaps the most significant of all is that C was
>>> developed by real practioners of programming and was designed for practical day-to-day
>>> use, not for show or for demonstration. Like any well-designed tool, it falls easily to
>>> the hand and feels good to use. Instead of providing constraints, checks and rigorous
>>> boundaries, it concentrates on providing you with power and on not getting in your way."
>>>     Preface to "The C Book", M Banahan et al
>> I would think that the ability to hit the Tab key to trigger name
>> completion in your R GUI makes partial matching almost useless. The
>> avantage of interactive completion in the GUI is that you immediately
>> see the result of the partial matching. So you get the best of both
>> worlds: no need to type long variable names in full, but no traps when a
>> match is not what you would expect.
>> 
>> Doesn't this suit your use case?
> Good point.  This works well at the command line.  However, not when interacting between emacs and R in the way I do.  For reproducability I use and emacs file that is being corrected and massaged with chunks submitted to R; at the end I have a clean record of how the result was obtained.
> 

If this is really true (that ESS doesn't complete in R files) then this seems more like a bug (or wish?) report for ESS - other editors correctly support code completion in R documents - after all this is a feature of R, so they don't need to re-invent the wheel.

Cheers,
Simon



>> 
>> 
>> Regards
>> 
>>> Terry Therneau
>>> 
>>> 
>>> On 03/21/2013 06:00 AM, r-devel-request at r-project.org wrote:
>>>> Allowing partial matching on $-extraction has always been a source of accidents.
>>>> Recently, someone who shall remain nameless tried names(mydata)<- "d^2" followed by
>>>> mydata$d^2. As variables in a data frame are generally considered similar to variables
>>>> in, say, the global environment, it seems strange that foo$bar can give you the content
>>>> of foo$bartender. In R-devel (i.e., *not* R-3.0.0 beta, but 3.1.0-to-be) partial matches
>>>> now gives a warning. Of course, it is inevitable that lazy programmers will have been
>>>> using code like
>>>>>> 
>>> 
>>> 	[[alternative HTML version deleted]]
>>> 
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>> 
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From therneau at mayo.edu  Thu Mar 21 16:34:05 2013
From: therneau at mayo.edu (Terry Therneau)
Date: Thu, 21 Mar 2013 10:34:05 -0500
Subject: [Rd] Depreciating partial matching
In-Reply-To: <30F7521C-5D05-4259-9472-606AF1B36D45@r-project.org>
References: <mailman.37.1363863608.9408.r-devel@r-project.org>
	<514B1072.4000003@mayo.edu> <1363874384.609.7.camel@milan>
	<514B1DC7.3060706@mayo.edu>
	<30F7521C-5D05-4259-9472-606AF1B36D45@r-project.org>
Message-ID: <514B286D.7020902@mayo.edu>



On 03/21/2013 10:00 AM, Simon Urbanek wrote:
>>> I would think that the ability to hit the Tab key to trigger name
>>> >>  completion in your R GUI makes partial matching almost useless. The
>>> >>  avantage of interactive completion in the GUI is that you immediately
>>> >>  see the result of the partial matching. So you get the best of both
>>> >>  worlds: no need to type long variable names in full, but no traps when a
>>> >>  match is not what you would expect.
>>> >>  
>>> >>  Doesn't this suit your use case?
>> >  Good point.  This works well at the command line.  However, not when interacting between emacs and R in the way I do.  For reproducability I use and emacs file that is being corrected and massaged with chunks submitted to R; at the end I have a clean record of how the result was obtained.
>> >  
> If this is really true (that ESS doesn't complete in R files) then this seems more like a bug (or wish?) report for ESS - other editors correctly support code completion in R documents - after all this is a feature of R, so they don't need to re-invent the wheel.
>
> Cheers,
> Simon
If you are running the R process inside ESS then there is matching -- it is R.  Doing 
this, keeping a log file, and then post-hoc cleaning up all the cruft from that file is 
one way to keep documentation.  But since for my analyses the number of models/plots/etc 
that turn out to be detours or dead ends on the way to a solution is larger than the 
worthwhile part (typos alone are lots larger) I prefer to keep the file(s) as their own 
buffers and submit bits of them to an R process either by cut-paste to a separate window 
or ess-submit to an inferior process.  Emacs can't do name completion in that case.  Nor 
could it do so in an Sweave file, unless you were to keep a live R process in hand to 
pre-test chunks as you wrote them.  (One could reasonably argue that when one gets the 
Sweave stage the names should be expanded.)

To summarize: my own interactive mix of emacs/R may be unusual.  For pure interactive 
folks completion does most of the work.  I hadn't tried the newest ESS 
interactive-within-emacs till today, it's slick as well.  The number of people howling 
will be less than my original thought, though not zero.

   Still, this change could cause a lot of grief for saved R scripts.  In our group the 
code + data directory is archived whenever a medical paper is submitted (close to 
500/year), and it is very common to pull one back as is 1-4 years later for further 
exploration.  A very small subset of those are in a legal context where exact 
reproducability is paramount.


From dtenenba at fhcrc.org  Thu Mar 21 17:52:44 2013
From: dtenenba at fhcrc.org (Dan Tenenbaum)
Date: Thu, 21 Mar 2013 09:52:44 -0700
Subject: [Rd] missing space in R version specifier makes PACKAGES file
 unreadable by install.packages()
Message-ID: <CAF42j21Hm4Vp=mB9wSgUeh=Ydf1KD4mkkVsYBq2jCoJJ3dZW0w@mail.gmail.com>

Hi,

After updating to R-3.0 beta r62328, I get the following:

> install.packages("Biobase", type="source", repos="http://george2/BBS/2.12/bioc")
Error in do.call(op, list(v_c, v_t[[op]])) :
  could not find function "R (>=2.15.1)"

The problem can be fixed by adding a space after >= in the offending
package's DESCRIPTION file and re-generating the PACKAGES file with
tools:::write_PACKAGES().

However, this worked OK in r62077. I'm not sure if >=2.15.1 (without
the space) is valid syntax, but I wonder if R can be a bit more
forgiving, as this issue derailed our daily builds.

> sessionInfo()
R version 3.0.0 alpha (2013-03-18 r62312)
Platform: x86_64-apple-darwin10.8.0 (64-bit)

locale:
[1] C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

loaded via a namespace (and not attached):
[1] tools_3.0.0


Thanks,
Dan


From dtenenba at fhcrc.org  Thu Mar 21 17:59:08 2013
From: dtenenba at fhcrc.org (Dan Tenenbaum)
Date: Thu, 21 Mar 2013 09:59:08 -0700
Subject: [Rd] missing space in R version specifier makes PACKAGES file
 unreadable by install.packages()
In-Reply-To: <CAF42j21Hm4Vp=mB9wSgUeh=Ydf1KD4mkkVsYBq2jCoJJ3dZW0w@mail.gmail.com>
References: <CAF42j21Hm4Vp=mB9wSgUeh=Ydf1KD4mkkVsYBq2jCoJJ3dZW0w@mail.gmail.com>
Message-ID: <CAF42j22Sn66_hW5eQ+0Xc0VKFUs_zBN0rx3VqAKvuv9tagHS+w@mail.gmail.com>

On Thu, Mar 21, 2013 at 9:52 AM, Dan Tenenbaum <dtenenba at fhcrc.org> wrote:
> Hi,
>
> After updating to R-3.0 beta r62328, I get the following:
>
>> install.packages("Biobase", type="source", repos="http://george2/BBS/2.12/bioc")
> Error in do.call(op, list(v_c, v_t[[op]])) :
>   could not find function "R (>=2.15.1)"
>
> The problem can be fixed by adding a space after >= in the offending
> package's DESCRIPTION file and re-generating the PACKAGES file with
> tools:::write_PACKAGES().
>
> However, this worked OK in r62077.

Beg pardon, this did not work in r62077 either.

However I still think it's worth reporting.
Dan


> I'm not sure if >=2.15.1 (without
> the space) is valid syntax, but I wonder if R can be a bit more
> forgiving, as this issue derailed our daily builds.
>
>> sessionInfo()
> R version 3.0.0 alpha (2013-03-18 r62312)
> Platform: x86_64-apple-darwin10.8.0 (64-bit)
>
> locale:
> [1] C
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> loaded via a namespace (and not attached):
> [1] tools_3.0.0
>
>
> Thanks,
> Dan


From murdoch.duncan at gmail.com  Thu Mar 21 18:28:55 2013
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Thu, 21 Mar 2013 13:28:55 -0400
Subject: [Rd] missing space in R version specifier makes PACKAGES file
 unreadable by install.packages()
In-Reply-To: <CAF42j22Sn66_hW5eQ+0Xc0VKFUs_zBN0rx3VqAKvuv9tagHS+w@mail.gmail.com>
References: <CAF42j21Hm4Vp=mB9wSgUeh=Ydf1KD4mkkVsYBq2jCoJJ3dZW0w@mail.gmail.com>
	<CAF42j22Sn66_hW5eQ+0Xc0VKFUs_zBN0rx3VqAKvuv9tagHS+w@mail.gmail.com>
Message-ID: <514B4357.9030008@gmail.com>

On 21/03/2013 12:59 PM, Dan Tenenbaum wrote:
> On Thu, Mar 21, 2013 at 9:52 AM, Dan Tenenbaum <dtenenba at fhcrc.org> wrote:
> > Hi,
> >
> > After updating to R-3.0 beta r62328, I get the following:
> >
> >> install.packages("Biobase", type="source", repos="http://george2/BBS/2.12/bioc")
> > Error in do.call(op, list(v_c, v_t[[op]])) :
> >   could not find function "R (>=2.15.1)"
> >
> > The problem can be fixed by adding a space after >= in the offending
> > package's DESCRIPTION file and re-generating the PACKAGES file with
> > tools:::write_PACKAGES().
> >
> > However, this worked OK in r62077.
>
> Beg pardon, this did not work in r62077 either.
>
> However I still think it's worth reporting.

I don't think this has changed recently.  The manual currently says "The 
comment should contain a comparison operator, whitespace and a valid 
version number."  In 2.15.0 it was less explicit, but the example 
contained a space.  It may be that some code path worked before more or 
less by accident, because in December changes were made to the parsing 
to avoid a tricky reentrancy problem, but I don't think it has ever been 
documented to work without whitespace.

I can't think of the reason white space was required, but the code does 
explicitly look for it, so I'd be reluctant to change it.

Duncan Murdoch

> Dan
>
>
> > I'm not sure if >=2.15.1 (without
> > the space) is valid syntax, but I wonder if R can be a bit more
> > forgiving, as this issue derailed our daily builds.
> >
> >> sessionInfo()
> > R version 3.0.0 alpha (2013-03-18 r62312)
> > Platform: x86_64-apple-darwin10.8.0 (64-bit)
> >
> > locale:
> > [1] C
> >
> > attached base packages:
> > [1] stats     graphics  grDevices utils     datasets  methods   base
> >
> > loaded via a namespace (and not attached):
> > [1] tools_3.0.0
> >
> >
> > Thanks,
> > Dan
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From dtenenba at fhcrc.org  Thu Mar 21 18:30:10 2013
From: dtenenba at fhcrc.org (Dan Tenenbaum)
Date: Thu, 21 Mar 2013 10:30:10 -0700
Subject: [Rd] missing space in R version specifier makes PACKAGES file
 unreadable by install.packages()
In-Reply-To: <514B4357.9030008@gmail.com>
References: <CAF42j21Hm4Vp=mB9wSgUeh=Ydf1KD4mkkVsYBq2jCoJJ3dZW0w@mail.gmail.com>
	<CAF42j22Sn66_hW5eQ+0Xc0VKFUs_zBN0rx3VqAKvuv9tagHS+w@mail.gmail.com>
	<514B4357.9030008@gmail.com>
Message-ID: <CAF42j21viRX0QhfbCMTGpven+XfGkJZNLDtKth4AU3RtKYQuGQ@mail.gmail.com>

On Thu, Mar 21, 2013 at 10:28 AM, Duncan Murdoch
<murdoch.duncan at gmail.com> wrote:
> On 21/03/2013 12:59 PM, Dan Tenenbaum wrote:
>>
>> On Thu, Mar 21, 2013 at 9:52 AM, Dan Tenenbaum <dtenenba at fhcrc.org> wrote:
>> > Hi,
>> >
>> > After updating to R-3.0 beta r62328, I get the following:
>> >
>> >> install.packages("Biobase", type="source",
>> >> repos="http://george2/BBS/2.12/bioc")
>> > Error in do.call(op, list(v_c, v_t[[op]])) :
>> >   could not find function "R (>=2.15.1)"
>> >
>> > The problem can be fixed by adding a space after >= in the offending
>> > package's DESCRIPTION file and re-generating the PACKAGES file with
>> > tools:::write_PACKAGES().
>> >
>> > However, this worked OK in r62077.
>>
>> Beg pardon, this did not work in r62077 either.
>>
>> However I still think it's worth reporting.
>
>
> I don't think this has changed recently.  The manual currently says "The
> comment should contain a comparison operator, whitespace and a valid version
> number."  In 2.15.0 it was less explicit, but the example contained a space.
> It may be that some code path worked before more or less by accident,
> because in December changes were made to the parsing to avoid a tricky
> reentrancy problem, but I don't think it has ever been documented to work
> without whitespace.
>
> I can't think of the reason white space was required, but the code does
> explicitly look for it, so I'd be reluctant to change it.
>

You're right. It was not R that changed, but something on our side. R
CMD build will not build a package with such a malformed version
specifier, so it normally would not be an issue. Please disregard and
sorry for the noise.

Dan


> Duncan Murdoch
>
>> Dan
>>
>>
>> > I'm not sure if >=2.15.1 (without
>> > the space) is valid syntax, but I wonder if R can be a bit more
>> > forgiving, as this issue derailed our daily builds.
>> >
>> >> sessionInfo()
>> > R version 3.0.0 alpha (2013-03-18 r62312)
>> > Platform: x86_64-apple-darwin10.8.0 (64-bit)
>> >
>> > locale:
>> > [1] C
>> >
>> > attached base packages:
>> > [1] stats     graphics  grDevices utils     datasets  methods   base
>> >
>> > loaded via a namespace (and not attached):
>> > [1] tools_3.0.0
>> >
>> >
>> > Thanks,
>> > Dan
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>
>


From bbolker at gmail.com  Thu Mar 21 22:46:57 2013
From: bbolker at gmail.com (Ben Bolker)
Date: Thu, 21 Mar 2013 21:46:57 +0000
Subject: [Rd] Depreciating partial matching
References: <mailman.37.1363863608.9408.r-devel@r-project.org>
	<514B1072.4000003@mayo.edu> <1363874384.609.7.camel@milan>
	<514B1DC7.3060706@mayo.edu>
	<30F7521C-5D05-4259-9472-606AF1B36D45@r-project.org>
	<514B286D.7020902@mayo.edu>
Message-ID: <loom.20130321T224159-677@post.gmane.org>

Terry Therneau <therneau <at> mayo.edu> writes:

>  To summarize: my own interactive mix of emacs/R may be unusual.
> For pure interactive folks completion does most of the work.  I
> hadn't tried the newest ESS interactive-within-emacs till today,
> it's slick as well.  The number of people howling will be less than
> my original thought, though not zero.

  For what it's worth I have a similar working style, although
I could probably train myself/adjust my working style if it came
to it.
 
>    Still, this change could cause a lot of grief for saved R
> scripts.  In our group the code + data directory is archived
> whenever a medical paper is submitted (close to 500/year), and it is
> very common to pull one back as is 1-4 years later for further
> exploration.  A very small subset of those are in a legal context
> where exact reproducability is paramount.

  Just to nitpick a bit -- as I think has been pointed out
before, if you *really* need exact reproducibility then you probably
need to be doing something more elaborate (e.g. saving complete
R+package installations/virtual machines).

  But I agree on the general point. (I don't envy R-core these
decisions -- but given how conservative they are generally, it
seems that they would be willing to consider being very careful
with this change as well.)


From andreas.leha at med.uni-goettingen.de  Thu Mar 21 23:32:39 2013
From: andreas.leha at med.uni-goettingen.de (Andreas Leha)
Date: Thu, 21 Mar 2013 23:32:39 +0100
Subject: [Rd] Depreciating partial matching
References: <mailman.37.1363863608.9408.r-devel@r-project.org>
	<514B1072.4000003@mayo.edu> <1363874384.609.7.camel@milan>
	<514B1DC7.3060706@mayo.edu>
	<30F7521C-5D05-4259-9472-606AF1B36D45@r-project.org>
	<514B286D.7020902@mayo.edu>
Message-ID: <87obec75a0.fsf@med.uni-goettingen.de>

Terry Therneau <therneau at mayo.edu> writes:

> On 03/21/2013 10:00 AM, Simon Urbanek wrote:
>>>> I would think that the ability to hit the Tab key to trigger name
>>>> >>  completion in your R GUI makes partial matching almost useless. The
>>>> >>  avantage of interactive completion in the GUI is that you immediately
>>>> >>  see the result of the partial matching. So you get the best of both
>>>> >>  worlds: no need to type long variable names in full, but no traps when a
>>>> >>  match is not what you would expect.
>>>> >>  >>  Doesn't this suit your use case?
>>> >  Good point.  This works well at the command line.  However, not
>>> > when interacting between emacs and R in the way I do.  For
>>> > reproducability I use and emacs file that is being corrected and
>>> > massaged with chunks submitted to R; at the end I have a clean
>>> > record of how the result was obtained.
>>> >  
>> If this is really true (that ESS doesn't complete in R files) then
>> this seems more like a bug (or wish?) report for ESS - other editors
>> correctly support code completion in R documents - after all this is
>> a feature of R, so they don't need to re-invent the wheel.
>>
>> Cheers,
>> Simon
> If you are running the R process inside ESS then there is matching --
> it is R.  Doing this, keeping a log file, and then post-hoc cleaning
> up all the cruft from that file is one way to keep documentation.  But
> since for my analyses the number of models/plots/etc that turn out to
> be detours or dead ends on the way to a solution is larger than the
> worthwhile part (typos alone are lots larger) I prefer to keep the
> file(s) as their own buffers and submit bits of them to an R process
> either by cut-paste to a separate window or ess-submit to an inferior
> process.  Emacs can't do name completion in that case.  Nor could it
> do so in an Sweave file, unless you were to keep a live R process in
> hand to pre-test chunks as you wrote them.  (One could reasonably
> argue that when one gets the Sweave stage the names should be
> expanded.)

Not true (any more?).

,----
| ESS[S] is the mode for editing S language files. This mode handles:
| 
| [...]
|  -  completion of object names and file names. 
`----
from
http://ess.r-project.org/Manual/ess.html#ESS_0028S_0029_002d_002dEditing-files 


ESS can do completion of functions, object names, and also
list/data.frame elements in *both* the inferior R buffer as well as in
the .R file.

[...]

- Andreas


From smckinney at bccrc.ca  Fri Mar 22 02:08:51 2013
From: smckinney at bccrc.ca (Steven McKinney)
Date: Thu, 21 Mar 2013 18:08:51 -0700
Subject: [Rd] Depreciating partial matching
In-Reply-To: <87obec75a0.fsf@med.uni-goettingen.de>
References: <mailman.37.1363863608.9408.r-devel@r-project.org>
	<514B1072.4000003@mayo.edu> <1363874384.609.7.camel@milan>
	<514B1DC7.3060706@mayo.edu>
	<30F7521C-5D05-4259-9472-606AF1B36D45@r-project.org>
	<514B286D.7020902@mayo.edu> <87obec75a0.fsf@med.uni-goettingen.de>
Message-ID: <DCE81E14EB74504B971DAD4D2DB0356B0CB926ADC9@crcmail4.BCCRC.CA>


> -----Original Message-----
> From: r-devel-bounces at r-project.org [mailto:r-devel-bounces at r-project.org]
> On Behalf Of Andreas Leha
> Sent: March-21-13 3:33 PM
> To: r-devel at stat.math.ethz.ch
> Subject: Re: [Rd] Depreciating partial matching
> 
> Terry Therneau <therneau at mayo.edu> writes:
> 
> > On 03/21/2013 10:00 AM, Simon Urbanek wrote:
> >>>> I would think that the ability to hit the Tab key to trigger name
> >>>> >>  completion in your R GUI makes partial matching almost useless.
> The
> >>>> >>  avantage of interactive completion in the GUI is that you
> immediately
> >>>> >>  see the result of the partial matching. So you get the best of
> both
> >>>> >>  worlds: no need to type long variable names in full, but no traps
> when a
> >>>> >>  match is not what you would expect.
> >>>> >>  >>  Doesn't this suit your use case?
> >>> >  Good point.  This works well at the command line.  However, not
> >>> > when interacting between emacs and R in the way I do.  For
> >>> > reproducability I use and emacs file that is being corrected and
> >>> > massaged with chunks submitted to R; at the end I have a clean
> >>> > record of how the result was obtained.
> >>> >
> >> If this is really true (that ESS doesn't complete in R files) then
> >> this seems more like a bug (or wish?) report for ESS - other editors
> >> correctly support code completion in R documents - after all this is
> >> a feature of R, so they don't need to re-invent the wheel.
> >>
> >> Cheers,
> >> Simon
> > If you are running the R process inside ESS then there is matching --
> > it is R.  Doing this, keeping a log file, and then post-hoc cleaning
> > up all the cruft from that file is one way to keep documentation.  But
> > since for my analyses the number of models/plots/etc that turn out to
> > be detours or dead ends on the way to a solution is larger than the
> > worthwhile part (typos alone are lots larger) I prefer to keep the
> > file(s) as their own buffers and submit bits of them to an R process
> > either by cut-paste to a separate window or ess-submit to an inferior
> > process.  Emacs can't do name completion in that case.  Nor could it
> > do so in an Sweave file, unless you were to keep a live R process in
> > hand to pre-test chunks as you wrote them.  (One could reasonably
> > argue that when one gets the Sweave stage the names should be
> > expanded.)
> 
> Not true (any more?).
> 
> ,----
> | ESS[S] is the mode for editing S language files. This mode handles:
> |
> | [...]
> |  -  completion of object names and file names.
> `----
> from
> http://ess.r-project.org/Manual/ess.html#ESS_0028S_0029_002d_002dEditing-
> files
> 
> 
> ESS can do completion of functions, object names, and also
> list/data.frame elements in *both* the inferior R buffer as well as in
> the .R file.
> 
> [...]

Looking forward to that!  But it is not working at present.
The link above points to the development version ess-13 which
I downloaded using svn export (I got version 13.03, running
on a Mac with emacs 24.3).  

Completion of object names works in the inferior ESS buffer 
but not yet in an ESS buffer ( .R file).

Steven McKinney


> 
> - Andreas
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From peter.meilstrup at gmail.com  Fri Mar 22 02:53:08 2013
From: peter.meilstrup at gmail.com (Peter Meilstrup)
Date: Thu, 21 Mar 2013 18:53:08 -0700
Subject: [Rd] Depreciating partial matching
In-Reply-To: <DCE81E14EB74504B971DAD4D2DB0356B0CB926ADC9@crcmail4.BCCRC.CA>
References: <mailman.37.1363863608.9408.r-devel@r-project.org>
	<514B1072.4000003@mayo.edu> <1363874384.609.7.camel@milan>
	<514B1DC7.3060706@mayo.edu>
	<30F7521C-5D05-4259-9472-606AF1B36D45@r-project.org>
	<514B286D.7020902@mayo.edu> <87obec75a0.fsf@med.uni-goettingen.de>
	<DCE81E14EB74504B971DAD4D2DB0356B0CB926ADC9@crcmail4.BCCRC.CA>
Message-ID: <CAJoaRhbBd_XuVj1dO=M9ztgTbQwNcuGJKMwfkUUCXWzksa0DUg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20130321/47910762/attachment.pl>

From hpages at fhcrc.org  Fri Mar 22 05:57:58 2013
From: hpages at fhcrc.org (=?ISO-8859-1?Q?Herv=E9_Pag=E8s?=)
Date: Thu, 21 Mar 2013 21:57:58 -0700
Subject: [Rd] Deprecating partial matching in $.data.frame
In-Reply-To: <9966F1D2-A672-4017-B59D-C05BC86DB603@gmail.com>
References: <A0DE2631-213D-4DB2-A952-5DBFA7A7F14C@gmail.com>
	<E66794E69CFDE04D9A70842786030B931B918849@PA-MBX04.na.tibco.com>
	<A3631670-5E8D-48DC-9868-8C9EE8852FE6@gmail.com>
	<CABdHhvHdHnOEyS+MKmwKTQpYp=zsfB84QDND-LRo2Sf4DXrRXA@mail.gmail.com>
	<514AC40E.2080808@gmail.com>
	<9966F1D2-A672-4017-B59D-C05BC86DB603@gmail.com>
Message-ID: <514BE4D6.105@fhcrc.org>

Hi,

Maybe a compromise would be to just issue a warning without
deprecating? That way people who want to do anova(fit1)$P can
still do it. When working interactively, it's certainly convenient
(serious code however should probably stay away from partial matching).

And so you keep the semantic consistent with lists because yes,
consistency is important. data.frame inherits from list so any
operation that works on a list is expected to work on a data.frame,
preferably the same way (otherwise it will always be a BIG surprise
to the user/programmer). For example if I have to maintain someone
else code and see something like:

     bar <- x$bar

and I know that 'x' is a list that contains atomic vectors of the
same length, I could have some good reasons to want to use a
data.frame instead of a list. And I would assume it's safe to
modify the code by adding the following line earlier in it:

    x <- as.data.frame(x)

But with the proposed change to $.data.frame, I cannot make this
kind of assumption anymore...

My two cents

H.


On 03/21/2013 06:52 AM, peter dalgaard wrote:
>
> On Mar 21, 2013, at 09:25 , Rainer M Krug wrote:
>
>> -----BEGIN PGP SIGNED MESSAGE-----
>> Hash: SHA1
>>
>> On 20/03/13 17:58, Hadley Wickham wrote:
>>> On Wed, Mar 20, 2013 at 11:26 AM, peter dalgaard <pdalgd at gmail.com> wrote:
>>>>
>>>> On Mar 20, 2013, at 16:59 , William Dunlap wrote:
>>>>
>>>>> Will you be doing the same for attribute names?
>>>>
>>>> Not at this point.
>>>
>>> It would be really nice to have consistent behaviour across argument names, attributes, lists
>>> and data frames, at least for R CMD check.
>>
>> I agree with Hadley that consistency is quite important. This is especially true for data.frames
>> and lists, as this concerns the data itself, and not names or attributes of the data.
>
> Well, maybe consistency is important, but partial matching never worked for $-extraction in environments, so the current change could be considered mainly a nudge of data frames in the direction of environments. After all, both can be thought of as collections of named objects.
>
> General lists are a somewhat different issue. They often, formally or informally, represent classed objects with a defined set of names, typically obtained as return values from functions. Since the names are known, people will have used the expedient of abbreviating them. This can happen with data frames as well, but less commonly, since it is in general unsafe to rely on column names being uniquely defined by any particular prefix.
>
> I.e., deprecating partial matching for lists opens a rather larger can of worms, and might require more extensive code revisions. Also, the performance hit of a runtime check for partial matching might be more important for lists than it is for data frames. It could be worth it to implement an R CMD check warning as you suggest, but perhaps not just now.
>
> -Peter
>

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fhcrc.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From pdalgd at gmail.com  Fri Mar 22 09:31:57 2013
From: pdalgd at gmail.com (peter dalgaard)
Date: Fri, 22 Mar 2013 09:31:57 +0100
Subject: [Rd] Deprecating partial matching in $.data.frame
In-Reply-To: <514BE4D6.105@fhcrc.org>
References: <A0DE2631-213D-4DB2-A952-5DBFA7A7F14C@gmail.com>
	<E66794E69CFDE04D9A70842786030B931B918849@PA-MBX04.na.tibco.com>
	<A3631670-5E8D-48DC-9868-8C9EE8852FE6@gmail.com>
	<CABdHhvHdHnOEyS+MKmwKTQpYp=zsfB84QDND-LRo2Sf4DXrRXA@mail.gmail.com>
	<514AC40E.2080808@gmail.com>
	<9966F1D2-A672-4017-B59D-C05BC86DB603@gmail.com>
	<514BE4D6.105@fhcrc.org>
Message-ID: <A60359C0-87E8-430A-B76A-1D800A5A00CB@gmail.com>


On Mar 22, 2013, at 05:57 , Herv? Pag?s wrote:

> Hi,
> 
> Maybe a compromise would be to just issue a warning without
> deprecating? That way people who want to do anova(fit1)$P can
> still do it. When working interactively, it's certainly convenient
> (serious code however should probably stay away from partial matching).

That's what it does. Issuing a warning when users do X is pretty much equivalent to deprecating X.

> 
> And so you keep the semantic consistent with lists because yes,
> consistency is important. data.frame inherits from list so any
> operation that works on a list is expected to work on a data.frame,
> preferably the same way (otherwise it will always be a BIG surprise
> to the user/programmer). For example if I have to maintain someone
> else code and see something like:
> 
>    bar <- x$bar
> 
> and I know that 'x' is a list that contains atomic vectors of the
> same length, I could have some good reasons to want to use a
> data.frame instead of a list. And I would assume it's safe to
> modify the code by adding the following line earlier in it:
> 
>   x <- as.data.frame(x)
> 
> But with the proposed change to $.data.frame, I cannot make this
> kind of assumption anymore...


No, but it's only a real problem if the component is not actually called "bar". You could make the same point for environments, but they never allowed partial matching:

> e <- as.environment(list(barbaric=666))
> e$bar
NULL
> e$barbaric
[1] 666




-- 
Peter Dalgaard, Professor
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From therneau at mayo.edu  Fri Mar 22 13:30:05 2013
From: therneau at mayo.edu (Terry Therneau)
Date: Fri, 22 Mar 2013 07:30:05 -0500
Subject: [Rd] ess completion
In-Reply-To: <mailman.27.1363950008.2536.r-devel@r-project.org>
References: <mailman.27.1363950008.2536.r-devel@r-project.org>
Message-ID: <514C4ECD.5010001@mayo.edu>

The thread is strange to me as well, since completion is logically impossible for my 
Sweave files.
     - an emacs buffer is open working on an .Rnw file
     - there is no copy of R running anywhere on the machine
     - the code chunk in the .Rnw file refers to a R data object saved somewhere else
Of course it cannot do "name completion" on a bit of code I'm writing, lacking omniscient 
powers.  Emacs is good but not that good :-)

The ESS manual section 12.1 states that for .R files it has "completion of object names 
and file names".  I'm puzzled by exactly what this means, since it is logically impossible 
(without a running R session) to do this in general.
Linking an .Rnw file to an inferior R process doesn't make sense to me.

However, I think it's time to move this sub-thread off of R-devel.  Respond to me 
privately about the answer or the proper list for this discussion.

Terry T

On 03/22/2013 06:00 AM, r-devel-request at r-project.org wrote:
> This thread is strange for me to read as I've been getting completion of
> object names, function arguments names, and whatnot in ESS buffers for as
> long as I can have been using it. And I'm only on ESS 12.09.
>
> Perhaps you need to set `ess-use-R-completion` to non-nil. Or check the
> value of `completion-at-point-functions`. Mine is '(ess-roxy-tag-completion
> ess-filename-completion ess-object-completion t)
>
> Peter


From r.m.krug at gmail.com  Fri Mar 22 14:39:14 2013
From: r.m.krug at gmail.com (Rainer M Krug)
Date: Fri, 22 Mar 2013 14:39:14 +0100
Subject: [Rd] ess completion
In-Reply-To: <514C4ECD.5010001@mayo.edu>
References: <mailman.27.1363950008.2536.r-devel@r-project.org>
	<514C4ECD.5010001@mayo.edu>
Message-ID: <514C5F02.1030704@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 22/03/13 13:30, Terry Therneau wrote:
> The thread is strange to me as well, since completion is logically impossible for my Sweave
> files. - an emacs buffer is open working on an .Rnw file - there is no copy of R running
> anywhere on the machine - the code chunk in the .Rnw file refers to a R data object saved
> somewhere else Of course it cannot do "name completion" on a bit of code I'm writing, lacking
> omniscient powers. Emacs is good but not that good :-)
> 
> The ESS manual section 12.1 states that for .R files it has "completion of object names and
> file names".  I'm puzzled by exactly what this means, since it is logically impossible (without
> a running R session) to do this in general. Linking an .Rnw file to an inferior R process
> doesn't make sense to me.
> 
> However, I think it's time to move this sub-thread off of R-devel.  Respond to me privately
> about the answer or the proper list for this discussion.

If anybody wants to continue this ESS discussion:

The proper list is the ESS mailing list: https://stat.ethz.ch/mailman/listinfo/ess-help

See you there,

Rainer

> 
> Terry T
> 
> On 03/22/2013 06:00 AM, r-devel-request at r-project.org wrote:
>> This thread is strange for me to read as I've been getting completion of object names,
>> function arguments names, and whatnot in ESS buffers for as long as I can have been using it.
>> And I'm only on ESS 12.09.
>> 
>> Perhaps you need to set `ess-use-R-completion` to non-nil. Or check the value of
>> `completion-at-point-functions`. Mine is '(ess-roxy-tag-completion ess-filename-completion
>> ess-object-completion t)
>> 
>> Peter
> 
> ______________________________________________ R-devel at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-devel


- -- 
Rainer M. Krug, PhD (Conservation Ecology, SUN), MSc (Conservation Biology, UCT), Dipl. Phys.
(Germany)

Centre of Excellence for Invasion Biology
Stellenbosch University
South Africa

Tel :       +33 - (0)9 53 10 27 44
Cell:       +33 - (0)6 85 62 59 98
Fax :       +33 - (0)9 58 10 27 44

Fax (D):    +49 - (0)3 21 21 25 22 44

email:      Rainer at krugs.de

Skype:      RMkrug
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.11 (GNU/Linux)
Comment: Using GnuPG with Thunderbird - http://www.enigmail.net/

iQEcBAEBAgAGBQJRTF8CAAoJENvXNx4PUvmCrSQH/inP6c5JT+YEJYDEGOkLQOiA
8GLGzgCdO+iekF1EMrKVvcPjim14gRu+y2HcryxMO44w1gIutcY2wpq6m7OYvCXI
BV40TjKut6nLzopx0IWVr0vX+mA5IybKiziup+lSS86gb5E8QNRVlBFIw3Xp2TXP
rMlS6kkNiQR6y94lwdnUJTy/FerqsV9sQNnNUNipTPt9coL4qISyl1TDoDImrtgM
kCG/kSoUnazzldH39qHgfEJb4rhgCLISNFKbPmtCuCbKh+iF0bHJF4rlEXrSkiRu
GsmfV1Ln3hWmYijtOygHUlmeSiiVTZVVtfXA5oXrW/GXVvDCYEkSTDuu3JbIj7o=
=DJwz
-----END PGP SIGNATURE-----


From hpages at fhcrc.org  Fri Mar 22 16:43:21 2013
From: hpages at fhcrc.org (=?ISO-8859-1?Q?Herv=E9_Pag=E8s?=)
Date: Fri, 22 Mar 2013 08:43:21 -0700
Subject: [Rd] Deprecating partial matching in $.data.frame
In-Reply-To: <A60359C0-87E8-430A-B76A-1D800A5A00CB@gmail.com>
References: <A0DE2631-213D-4DB2-A952-5DBFA7A7F14C@gmail.com>
	<E66794E69CFDE04D9A70842786030B931B918849@PA-MBX04.na.tibco.com>
	<A3631670-5E8D-48DC-9868-8C9EE8852FE6@gmail.com>
	<CABdHhvHdHnOEyS+MKmwKTQpYp=zsfB84QDND-LRo2Sf4DXrRXA@mail.gmail.com>
	<514AC40E.2080808@gmail.com>
	<9966F1D2-A672-4017-B59D-C05BC86DB603@gmail.com>
	<514BE4D6.105@fhcrc.org>
	<A60359C0-87E8-430A-B76A-1D800A5A00CB@gmail.com>
Message-ID: <514C7C19.3040306@fhcrc.org>

Hi,

On 03/22/2013 01:31 AM, peter dalgaard wrote:
>
> On Mar 22, 2013, at 05:57 , Herv? Pag?s wrote:
>
>> Hi,
>>
>> Maybe a compromise would be to just issue a warning without
>> deprecating? That way people who want to do anova(fit1)$P can
>> still do it. When working interactively, it's certainly convenient
>> (serious code however should probably stay away from partial matching).
>
> That's what it does. Issuing a warning when users do X is pretty much equivalent to deprecating X.

For now yes. But you won't keep it deprecated forever right?

>
>>
>> And so you keep the semantic consistent with lists because yes,
>> consistency is important. data.frame inherits from list so any
>> operation that works on a list is expected to work on a data.frame,
>> preferably the same way (otherwise it will always be a BIG surprise
>> to the user/programmer). For example if I have to maintain someone
>> else code and see something like:
>>
>>     bar <- x$bar
>>
>> and I know that 'x' is a list that contains atomic vectors of the
>> same length, I could have some good reasons to want to use a
>> data.frame instead of a list. And I would assume it's safe to
>> modify the code by adding the following line earlier in it:
>>
>>    x <- as.data.frame(x)
>>
>> But with the proposed change to $.data.frame, I cannot make this
>> kind of assumption anymore...
>
>
> No, but it's only a real problem if the component is not actually called "bar". You could make the same point for environments, but they never allowed partial matching:

A data.fame is a list, not an environment. It would be silly for
me as a programmer to assume that replacing environment 'x' by
data.frame 'x' won't break the code.

H.

>
>> e <- as.environment(list(barbaric=666))
>> e$bar
> NULL
>> e$barbaric
> [1] 666
>
>
>
>

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fhcrc.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From amredd at gmail.com  Fri Mar 22 17:44:29 2013
From: amredd at gmail.com (Andrew Redd)
Date: Fri, 22 Mar 2013 10:44:29 -0600
Subject: [Rd] Description depends line for windows only
Message-ID: <CAK_CNyYER1eH_ZFuQAJ=4kXTMD_4am1F=rzrM9Q-Dd1epQVmJA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20130322/5538af82/attachment.pl>

From dtenenba at fhcrc.org  Fri Mar 22 17:47:53 2013
From: dtenenba at fhcrc.org (Dan Tenenbaum)
Date: Fri, 22 Mar 2013 09:47:53 -0700
Subject: [Rd] Description depends line for windows only
In-Reply-To: <CAK_CNyYER1eH_ZFuQAJ=4kXTMD_4am1F=rzrM9Q-Dd1epQVmJA@mail.gmail.com>
References: <CAK_CNyYER1eH_ZFuQAJ=4kXTMD_4am1F=rzrM9Q-Dd1epQVmJA@mail.gmail.com>
Message-ID: <CAF42j23Z88u0zpkC7J=AGNecLK8Ekk76x9zbZbAY9=2vSikSaw@mail.gmail.com>

On Fri, Mar 22, 2013 at 9:44 AM, Andrew Redd <amredd at gmail.com> wrote:
> I am developing a package that is only applicable on windows, is there a
> line I can put in the depends field of the DESCRIPTION file to tell that
> this should only be build for windows?
>

OS_type: windows


See
 RShowDoc("R-exts")
Section 1.1.1 The DESCRIPTION file, search for OS_type.

Dan



> Are there any other protocols that I should follow?
>
> Thanks,
>
> --
> Andrew
> May the Open Source be with you.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From edd at debian.org  Fri Mar 22 18:01:08 2013
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 22 Mar 2013 12:01:08 -0500
Subject: [Rd] Description depends line for windows only
In-Reply-To: <CAK_CNyYER1eH_ZFuQAJ=4kXTMD_4am1F=rzrM9Q-Dd1epQVmJA@mail.gmail.com>
References: <CAK_CNyYER1eH_ZFuQAJ=4kXTMD_4am1F=rzrM9Q-Dd1epQVmJA@mail.gmail.com>
Message-ID: <20812.36436.851687.48981@max.nulle.part>


On 22 March 2013 at 10:44, Andrew Redd wrote:
| I am developing a package that is only applicable on windows, is there a
| line I can put in the depends field of the DESCRIPTION file to tell that
| this should only be build for windows?

There is a different field for this. From Section 1.1.1 of you-know-what:

     The `OS_type' field specifies the OS(es) for which the package is
  intended.  If present, it should be one of `unix' or `windows', and
  indicates that the package can only be installed on a platform with
  `.Platform$OS.type' having that value.

Dirk

-- 
Dirk Eddelbuettel | edd at debian.org | http://dirk.eddelbuettel.com


From HodgessE at uhd.edu  Fri Mar 22 18:53:56 2013
From: HodgessE at uhd.edu (Hodgess, Erin)
Date: Fri, 22 Mar 2013 17:53:56 +0000
Subject: [Rd] read.pnm question in R-beta
Message-ID: <FF9DB805FC41CC4E95825A50F680630212944215@columbia.uhd.campus>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20130322/acf6c4c9/attachment.pl>

From pdalgd at gmail.com  Fri Mar 22 20:09:32 2013
From: pdalgd at gmail.com (peter dalgaard)
Date: Fri, 22 Mar 2013 20:09:32 +0100
Subject: [Rd] read.pnm question in R-beta
In-Reply-To: <FF9DB805FC41CC4E95825A50F680630212944215@columbia.uhd.campus>
References: <FF9DB805FC41CC4E95825A50F680630212944215@columbia.uhd.campus>
Message-ID: <FB1F4807-ABCF-40FC-B9C2-9449EB8C61B6@gmail.com>


On Mar 22, 2013, at 18:53 , Hodgess, Erin wrote:

> In R-beta (Masked Marvel), when I do the example from the read.pnm help file, this is what happens:
> 
> x <- read.pnm(system.file("pictures/logo.pgm",package="pixmap")[1])
> Warning message:
> 
> In rep(cellres, length=2): x is NULL so the result will be NULL
> In R-2.15.3, it's all right.

It's just a warning. The change is in NEWS. It's up to the pixmap package maintainer to get rid of the warning.

-pd

> 
> Thanks,
> Erin
> 
> Erin M. Hodgess, Ph.D.
> Associate Professor
> Department of Computer and Mathematical Sciences
> University of Houston - Downtown
> mailto: hodgesse at uhd.edu<mailto:hodgesse at uhd.edu>
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From ripley at stats.ox.ac.uk  Fri Mar 22 20:10:51 2013
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 22 Mar 2013 19:10:51 +0000
Subject: [Rd] read.pnm question in R-beta
In-Reply-To: <FF9DB805FC41CC4E95825A50F680630212944215@columbia.uhd.campus>
References: <FF9DB805FC41CC4E95825A50F680630212944215@columbia.uhd.campus>
Message-ID: <514CACBB.3060603@stats.ox.ac.uk>

On 22/03/2013 17:53, Hodgess, Erin wrote:
> In R-beta (Masked Marvel), when I do the example from the read.pnm help file, this is what happens:
>
> x <- read.pnm(system.file("pictures/logo.pgm",package="pixmap")[1])
> Warning message:
>
> In rep(cellres, length=2): x is NULL so the result will be NULL
> In R-2.15.3, it's all right.

That is an error in a package (and no, it was not all right: how can 
NULL ever be replicated to length 2?).  I presume package pixmap, but 
you did not tell us.

After all these years you should have read the posting guide and know to 
send such comments to the package maintainer.

>
> Thanks,
> Erin
>
> Erin M. Hodgess, Ph.D.
> Associate Professor
> Department of Computer and Mathematical Sciences
> University of Houston - Downtown
> mailto: hodgesse at uhd.edu<mailto:hodgesse at uhd.edu>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From joshmobrien at gmail.com  Fri Mar 22 20:43:05 2013
From: joshmobrien at gmail.com (Josh O'Brien)
Date: Fri, 22 Mar 2013 12:43:05 -0700
Subject: [Rd] Why does typeof() modify an object's "named" field?
Message-ID: <CAOwKfPSc8X8qNCVmieCLLnaLMo2k+GmH8dHHUmrYt+1AdPDcEg@mail.gmail.com>

Hello,

Doing typeof() on an object appears to reset the "named" field in its
sxpinfo header to 2, which can change the way that subsequent
subassignment operations are carried out:


X <- 1:5e7
.Internal(inspect(X))
# @4eeb0008 13 INTSXP g0c7 [NAM(1)] (len=50000000, tl=0) 1,2,3,4,5,...
system.time(X[1] <- 9L)
#    user  system elapsed
#       0       0       0


typeof(X)

.Internal(inspect(X))
# @4eeb0008 13 INTSXP g1c7 [MARK,NAM(2)] (len=50000000, tl=0) 9,2,3,4,5,...
system.time(X[2] <- 9L)
#    user  system elapsed
#    0.16    0.08    0.23

Some other functions that query the nature of an object (e.g. class(),
length(), attributes()) do not modify the object's "named" field. Is
there a reason that typeof() should?


(Possibly of interest is this somewhat related thread on Stack
Overflow: http://stackoverflow.com/questions/15559387/operator-in-rstudio-and-r/15559956#15559956
).


From bhh at xs4all.nl  Fri Mar 22 21:03:11 2013
From: bhh at xs4all.nl (Berend Hasselman)
Date: Fri, 22 Mar 2013 21:03:11 +0100
Subject: [Rd] R CMD check in R-3.0.0 gives warnings
Message-ID: <74BF18D6-268F-4E9D-9FEE-41782B54CE72@xs4all.nl>


I am running R CMD check on my package nleqslv with R-3.0.0 beta (2013-03-33 r62364) on Mac OS X 10.6.8

In contrast with R-2.15.3 R CMD check now issues a note for 

- Mercurial version control files .hgignore, .hgtags and directory .hg. These are however included in .hidden_file_exclusions.
- typical Mac Finder files .DS_Store

All of these are ignored when executing R CMD check --as-cran.

I can't remove the .hg* files and directories. I could remove the .DS_Store files to avoid the note.

The note is harmless but annoying and strikes me as slightly overzealous.

Berend

From edd at debian.org  Fri Mar 22 21:26:27 2013
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 22 Mar 2013 15:26:27 -0500
Subject: [Rd] R CMD check in R-3.0.0 gives warnings
In-Reply-To: <74BF18D6-268F-4E9D-9FEE-41782B54CE72@xs4all.nl>
References: <74BF18D6-268F-4E9D-9FEE-41782B54CE72@xs4all.nl>
Message-ID: <20812.48755.904094.959054@max.nulle.part>


On 22 March 2013 at 21:03, Berend Hasselman wrote:
| I am running R CMD check on my package nleqslv with R-3.0.0 beta (2013-03-33 r62364) on Mac OS X 10.6.8
| 
| In contrast with R-2.15.3 R CMD check now issues a note for 
| 
| - Mercurial version control files .hgignore, .hgtags and directory .hg. These are however included in .hidden_file_exclusions.
| - typical Mac Finder files .DS_Store
| 
| All of these are ignored when executing R CMD check --as-cran.
| 
| I can't remove the .hg* files and directories. I could remove the .DS_Store files to avoid the note.

A few years ago the "Right Way To Do Things" changed.  You are really
supposed to do 'R CMD check' off the tar.gz, and you *do* have a toggle to
exclude what goes into the tar.gz:   .Rbuildignore

| The note is harmless but annoying and strikes me as slightly overzealous.

So are are many other warnings. 

But in the long run I am happy they are there so that eg my system does not
get polluted with your .hg and .DS_Store files which are of zero use to me.

Warnings are your friends. Be nice to them.

Dirk


-- 
Dirk Eddelbuettel | edd at debian.org | http://dirk.eddelbuettel.com


From bhh at xs4all.nl  Fri Mar 22 21:40:36 2013
From: bhh at xs4all.nl (Berend Hasselman)
Date: Fri, 22 Mar 2013 21:40:36 +0100
Subject: [Rd] R CMD check in R-3.0.0 gives warnings
In-Reply-To: <20812.48755.904094.959054@max.nulle.part>
References: <74BF18D6-268F-4E9D-9FEE-41782B54CE72@xs4all.nl>
	<20812.48755.904094.959054@max.nulle.part>
Message-ID: <A699D8A3-8DFE-46A2-B245-88C808F87DE7@xs4all.nl>


On 22-03-2013, at 21:26, Dirk Eddelbuettel <edd at debian.org> wrote:

> 
> On 22 March 2013 at 21:03, Berend Hasselman wrote:
> | I am running R CMD check on my package nleqslv with R-3.0.0 beta (2013-03-33 r62364) on Mac OS X 10.6.8
> | 
> | In contrast with R-2.15.3 R CMD check now issues a note for 
> | 
> | - Mercurial version control files .hgignore, .hgtags and directory .hg. These are however included in .hidden_file_exclusions.
> | - typical Mac Finder files .DS_Store
> | 
> | All of these are ignored when executing R CMD check --as-cran.
> | 
> | I can't remove the .hg* files and directories. I could remove the .DS_Store files to avoid the note.
> 
> A few years ago the "Right Way To Do Things" changed.  You are really
> supposed to do 'R CMD check' off the tar.gz, and you *do* have a toggle to
> exclude what goes into the tar.gz:   .Rbuildignore
> 

which I have. And making a .tar.gz will ignore the version control files. So why not ignore them when doing a check on a (allowed) package directory?

From  R CMD check --help
Usage: R CMD check [options] pkgs

Check R packages from package sources, which can be directories or
package 'tar' archives with extension '.tar.gz', '.tar.bz2',
'.tar.xz' or '.tgz'.

So a directory is allowed.

> | The note is harmless but annoying and strikes me as slightly overzealous.
> 
> So are are many other warnings. 
> 

It's a note and not a warning.

> But in the long run I am happy they are there so that eg my system does not
> get polluted with your .hg and .DS_Store files which are of zero use to me.
> 

My .hg and .DS_Store files will not pollute your system.

Berend

> Warnings are your friends. Be nice to them.
> 
> Dirk
> 
> 
> -- 
> Dirk Eddelbuettel | edd at debian.org | http://dirk.eddelbuettel.com  


From murdoch.duncan at gmail.com  Fri Mar 22 22:47:53 2013
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Fri, 22 Mar 2013 17:47:53 -0400
Subject: [Rd] R CMD check in R-3.0.0 gives warnings
In-Reply-To: <A699D8A3-8DFE-46A2-B245-88C808F87DE7@xs4all.nl>
References: <74BF18D6-268F-4E9D-9FEE-41782B54CE72@xs4all.nl>
	<20812.48755.904094.959054@max.nulle.part>
	<A699D8A3-8DFE-46A2-B245-88C808F87DE7@xs4all.nl>
Message-ID: <514CD189.3070801@gmail.com>

On 13-03-22 4:40 PM, Berend Hasselman wrote:
>
> On 22-03-2013, at 21:26, Dirk Eddelbuettel <edd at debian.org> wrote:
>
>>
>> On 22 March 2013 at 21:03, Berend Hasselman wrote:
>> | I am running R CMD check on my package nleqslv with R-3.0.0 beta (2013-03-33 r62364) on Mac OS X 10.6.8
>> |
>> | In contrast with R-2.15.3 R CMD check now issues a note for
>> |
>> | - Mercurial version control files .hgignore, .hgtags and directory .hg. These are however included in .hidden_file_exclusions.
>> | - typical Mac Finder files .DS_Store
>> |
>> | All of these are ignored when executing R CMD check --as-cran.
>> |
>> | I can't remove the .hg* files and directories. I could remove the .DS_Store files to avoid the note.
>>
>> A few years ago the "Right Way To Do Things" changed.  You are really
>> supposed to do 'R CMD check' off the tar.gz, and you *do* have a toggle to
>> exclude what goes into the tar.gz:   .Rbuildignore
>>
>
> which I have. And making a .tar.gz will ignore the version control files. So why not ignore them when doing a check on a (allowed) package directory?

Because that makes the code messier.  A directory is treated as if it 
was just extracted from the tarball.  If the code had to distinguish two 
different ways of getting to that state it would be harder to maintain.

Duncan Murdoch




>
>  From  R CMD check --help
> Usage: R CMD check [options] pkgs
>
> Check R packages from package sources, which can be directories or
> package 'tar' archives with extension '.tar.gz', '.tar.bz2',
> '.tar.xz' or '.tgz'.
>
> So a directory is allowed.
>
>> | The note is harmless but annoying and strikes me as slightly overzealous.
>>
>> So are are many other warnings.
>>
>
> It's a note and not a warning.
>
>> But in the long run I am happy they are there so that eg my system does not
>> get polluted with your .hg and .DS_Store files which are of zero use to me.
>>
>
> My .hg and .DS_Store files will not pollute your system.
>
> Berend
>
>> Warnings are your friends. Be nice to them.
>>
>> Dirk
>>
>>
>> --
>> Dirk Eddelbuettel | edd at debian.org | http://dirk.eddelbuettel.com
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From michael.weylandt at gmail.com  Sat Mar 23 10:43:37 2013
From: michael.weylandt at gmail.com (R. Michael Weylandt)
Date: Sat, 23 Mar 2013 09:43:37 +0000
Subject: [Rd] Why does typeof() modify an object's "named" field?
In-Reply-To: <CAOwKfPSc8X8qNCVmieCLLnaLMo2k+GmH8dHHUmrYt+1AdPDcEg@mail.gmail.com>
References: <CAOwKfPSc8X8qNCVmieCLLnaLMo2k+GmH8dHHUmrYt+1AdPDcEg@mail.gmail.com>
Message-ID: <CAAmySGP8EqbKgJv6bnkwkMwAWP5cicuSNpR_cePJYz6=4cFqAw@mail.gmail.com>

On Fri, Mar 22, 2013 at 7:43 PM, Josh O'Brien <joshmobrien at gmail.com> wrote:
> Some other functions that query the nature of an object (e.g. class(),
> length(), attributes()) do not modify the object's "named" field. Is
> there a reason that typeof() should?
>

Because it's not implemented as a primitive and the closure used in
setting up the internal call [1] bumps up the NAMED field:

compare

x <- 1:3; y <- 1:4;  z <- 1:5

.Internal(inspect(x))
typeof(x) # Closure
.Internal(inspect(x))

.Internal(inspect(y))
class(y) # Primitive
.Internal(inspect(y))

.Internal(inspect(z))
.Internal(typeof(z))
.Internal(inspect(z))

giving

> x <- 1:3; y <- 1:4;  z <- 1:5
>
> .Internal(inspect(x))
@7886c78 13 INTSXP g0c2 [NAM(1)] (len=3, tl=0) 1,2,3
> typeof(x) # Closure
[1] "integer"
> .Internal(inspect(x))
@7886c78 13 INTSXP g0c2 [NAM(2)] (len=3, tl=0) 1,2,3
>
> .Internal(inspect(y))
@7886cf0 13 INTSXP g0c2 [NAM(1)] (len=4, tl=0) 1,2,3,4
> class(y) # Primitive
[1] "integer"
> .Internal(inspect(y))
@7886cf0 13 INTSXP g0c2 [NAM(1)] (len=4, tl=0) 1,2,3,4
>
> .Internal(inspect(z))
@7a8e9d8 13 INTSXP g0c3 [NAM(1)] (len=5, tl=0) 1,2,3,4,5
> .Internal(typeof(z))
[1] "integer"
> .Internal(inspect(z))
@7a8e9d8 13 INTSXP g0c3 [NAM(1)] (len=5, tl=0) 1,2,3,4,5

Michael

[1] I'm not super certain about the fact it's the closure vs argument
passing (or if the distinction even makes sense) but I know that this
behavior is part of the reason primitives are important.


From Achim.Zeileis at uibk.ac.at  Sat Mar 23 11:12:41 2013
From: Achim.Zeileis at uibk.ac.at (Achim Zeileis)
Date: Sat, 23 Mar 2013 11:12:41 +0100 (CET)
Subject: [Rd] fortune?
In-Reply-To: <51499E13.1060701@pburns.seanet.com>
References: <51499E13.1060701@pburns.seanet.com>
Message-ID: <alpine.DEB.2.02.1303231111270.24104@paninaro.uibk.ac.at>

On Wed, 20 Mar 2013, Patrick Burns wrote:

> Brian Ripley:
>
> If things are not readily available in R it is always good to pause and 
> reflect if there might be a good reason.
>
> In the R-help thread: How to get the t-stat for arima()?

Thanks for the nice quote, added to the devel version on R-Forge now.
Z

> Pat
>
> -- 
> Patrick Burns
> pburns at pburns.seanet.com
> twitter: @burnsstat @portfolioprobe
> http://www.portfolioprobe.com/blog
> http://www.burns-stat.com
> (home of:
> 'Impatient R'
> 'The R Inferno'
> 'Tao Te Programming')
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From ripley at stats.ox.ac.uk  Sat Mar 23 13:01:13 2013
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 23 Mar 2013 12:01:13 +0000
Subject: [Rd] double in summary.c : isum
In-Reply-To: <5595fbfb430053bff583ff078ed607ef@imap.plus.net>
References: <5595fbfb430053bff583ff078ed607ef@imap.plus.net>
Message-ID: <514D9989.1060909@stats.ox.ac.uk>

On 20/03/2013 12:56, Matthew Dowle wrote:
>
> Hi,
>
> Please consider the following :
>
>> x = as.integer(2^30-1)
> [1] 1073741823
>> sum(c(rep(x, 10000000), rep(-x,9999999)))
> [1] 1073741824
>
> Tested on 2.15.2 and a recent R-devel (r62132).
>
> I'm wondering if s in isum could be LDOUBLE instead of double, like
> rsum, to fix this edge case?

No, because there is no guarantee that LDOUBLE differs from double (and 
platform on which it does not).

Users really need to take responsibility for the numerical stability of 
calcuations they attempt.  Expecting to sum 20 million large numbers 
exactly is unrealistic.

There are cases where 64-bit integer accumulators would be beneficial, 
and this is one.  Unfortunately C11 does not require them but some 
optional moves in that direction are planned.

>
> https://svn.r-project.org/R/trunk/src/main/summary.c
>
> Thanks,
> Matthew
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From mdowle at mdowle.plus.com  Sat Mar 23 15:20:15 2013
From: mdowle at mdowle.plus.com (Matthew Dowle)
Date: Sat, 23 Mar 2013 14:20:15 +0000
Subject: [Rd] double in summary.c : isum
In-Reply-To: <514D9989.1060909@stats.ox.ac.uk>
References: <5595fbfb430053bff583ff078ed607ef@imap.plus.net>
	<514D9989.1060909@stats.ox.ac.uk>
Message-ID: <cfe3cd9e7092cbca7c5901558d605d62@imap.plus.net>

On 23.03.2013 12:01, Prof Brian Ripley wrote:
> On 20/03/2013 12:56, Matthew Dowle wrote:
>>
>> Hi,
>>
>> Please consider the following :
>>
>>> x = as.integer(2^30-1)
>> [1] 1073741823
>>> sum(c(rep(x, 10000000), rep(-x,9999999)))
>> [1] 1073741824
>>
>> Tested on 2.15.2 and a recent R-devel (r62132).
>>
>> I'm wondering if s in isum could be LDOUBLE instead of double, like
>> rsum, to fix this edge case?
>
> No, because there is no guarantee that LDOUBLE differs from double
> (and platform on which it does not).

That's a reason for not using LDOUBLE at all isn't it? Yet src/main/*.c 
has 19 lines using LDOUBLE e.g. arithmetic.c and cum.c as well as 
summary.c.

I'd assumed LDOUBLE was being used by R to benefit from long double (or 
equivalent) on platforms that support it (which is all modern Unix, Mac 
and Windows as far as I know). I do realise that the edge case wouldn't 
be fixed on platforms where LDOUBLE is defined as double.

What have I misunderstood?

>
> Users really need to take responsibility for the numerical stability
> of calcuations they attempt.  Expecting to sum 20 million large
> numbers exactly is unrealistic.

Trying to take responsibility, but you said no. Changing from double to 
LDOUBLE would mean that something that wasn't realistic, was then 
realistic (on platforms that support long double).

And it would bring open source R into line with TERR, which gets the 
answer right, on 64bit Windows at least. But I'm not sure I should be as 
confident in TERR as I am in open source R because I can't see its 
source code.


> There are cases where 64-bit integer accumulators would be
> beneficial, and this is one.  Unfortunately C11 does not require them
> but some optional moves in that direction are planned.
>
>>
>> https://svn.r-project.org/R/trunk/src/main/summary.c
>>
>> Thanks,
>> Matthew
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel


From groemping at beuth-hochschule.de  Sat Mar 23 18:12:54 2013
From: groemping at beuth-hochschule.de (=?ISO-8859-15?Q?Ulrike_Gr=F6mping?=)
Date: Sat, 23 Mar 2013 18:12:54 +0100
Subject: [Rd] sysdata.rda vs. rda files in data directory
Message-ID: <514DE296.2040605@beuth-hochschule.de>

Dear developeRs,

my package FrF2.catlg128 holds large catalogues and is supposed to gain 
additional ones. All the catalogues are intended for the user.
So far, the catalogues were stored in the data directory, and LazyData 
was "no". I understand that this is not considered wise any more (if it 
ever was), so that I want to change to LazyData "yes" with the next 
release (which will also get some additional catalogues).

I have tried out using separate data files in the data directory (like 
before) and one sysdata.rda file in the R directory (exporting all 
catalogues from the namespace); there is a large difference in the 
installed sizes between those two ways: the approach with sysdata.rda 
uses only about half the size of the separate data files approach (5.6 
Mb vs 11.7Mb).

As I would like to be able to query the available data in the package 
via data(package="FrF2.catlg128")) even before the package is loaded,  I 
want to have a data directory with a datalist file in there. This 
appears to be compatible with using a sysdata.rda file in the R 
directory. (From a tidyness point of view, I would prefer the data file 
to sit in the data directory as well; however, that about doubles the 
installed size again (11.4 vs 5.6Mb) even if I use just the one 
sysdata.rda file.)

Regarding the installed package size, the best option is obviously one 
sysdata.rda file in the R directory, but I want the datalist file for 
the reason given above. A data directory without data files throws a 
warning, so that I have to include a dummy data file (and documentation 
for it) for allowing me to have a datalist file.
Finally my questions: Is there a better way to achieve what I am looking 
for? And if not: is there any reason against combining a sysdata.rda 
file in the R directory with a datalist file (that lists the data from 
the sysdata.rda file) in the data directory, be it policy-wise or 
perhaps in terms of memory usage within an R session?

Best regards,
Ulrike


From lgautier at gmail.com  Sun Mar 24 15:33:34 2013
From: lgautier at gmail.com (Laurent Gautier)
Date: Sun, 24 Mar 2013 15:33:34 +0100
Subject: [Rd] Mirroring R on a DVCS
In-Reply-To: <CAFOpNVHjmGDYR9SnF_7Bo+gFNfMP=-e0DV0X5KOFvBeAujZpiA@mail.gmail.com>
References: <mailman.27.1363431608.4225.r-devel@r-project.org>
	<5144B756.60909@gmail.com>
	<0D87478D-9F5C-473A-838F-01EBD742AF08@gmail.com>
	<5144BDA9.8060505@gmail.com>
	<CAFOpNVHjmGDYR9SnF_7Bo+gFNfMP=-e0DV0X5KOFvBeAujZpiA@mail.gmail.com>
Message-ID: <514F0EBE.3090302@gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20130324/fcc6701f/attachment.pl>

From murdoch.duncan at gmail.com  Sun Mar 24 16:01:58 2013
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Sun, 24 Mar 2013 11:01:58 -0400
Subject: [Rd] double in summary.c : isum
In-Reply-To: <cfe3cd9e7092cbca7c5901558d605d62@imap.plus.net>
References: <5595fbfb430053bff583ff078ed607ef@imap.plus.net>
	<514D9989.1060909@stats.ox.ac.uk>
	<cfe3cd9e7092cbca7c5901558d605d62@imap.plus.net>
Message-ID: <514F1566.4030004@gmail.com>

On 13-03-23 10:20 AM, Matthew Dowle wrote:
> On 23.03.2013 12:01, Prof Brian Ripley wrote:
>> On 20/03/2013 12:56, Matthew Dowle wrote:
>>>
>>> Hi,
>>>
>>> Please consider the following :
>>>
>>>> x = as.integer(2^30-1)
>>> [1] 1073741823
>>>> sum(c(rep(x, 10000000), rep(-x,9999999)))
>>> [1] 1073741824
>>>
>>> Tested on 2.15.2 and a recent R-devel (r62132).
>>>
>>> I'm wondering if s in isum could be LDOUBLE instead of double, like
>>> rsum, to fix this edge case?
>>
>> No, because there is no guarantee that LDOUBLE differs from double
>> (and platform on which it does not).
>
> That's a reason for not using LDOUBLE at all isn't it? Yet src/main/*.c
> has 19 lines using LDOUBLE e.g. arithmetic.c and cum.c as well as
> summary.c.
>
> I'd assumed LDOUBLE was being used by R to benefit from long double (or
> equivalent) on platforms that support it (which is all modern Unix, Mac
> and Windows as far as I know). I do realise that the edge case wouldn't
> be fixed on platforms where LDOUBLE is defined as double.

I think the problem is that there are two opposing targets in R:  we 
want things to be as accurate as possible, and we want them to be 
consistent across platforms. Sometimes one goal wins, sometimes the 
other.  Inconsistencies across platforms give false positives in tests 
that tend to make us miss true bugs.  Some people think we should never 
use LDOUBLE because of that.  In other cases, the extra accuracy is so 
helpful that it's worth it.  So I think you'd need to argue that the 
case you found is something where the benefit outweighs the costs. 
Since almost all integer sums are done exactly with the current code, is 
it really worth introducing inconsistencies in the rare inexact cases?

Duncan Murdoch


>
> What have I misunderstood?
>
>>
>> Users really need to take responsibility for the numerical stability
>> of calcuations they attempt.  Expecting to sum 20 million large
>> numbers exactly is unrealistic.
>
> Trying to take responsibility, but you said no. Changing from double to
> LDOUBLE would mean that something that wasn't realistic, was then
> realistic (on platforms that support long double).
>
> And it would bring open source R into line with TERR, which gets the
> answer right, on 64bit Windows at least. But I'm not sure I should be as
> confident in TERR as I am in open source R because I can't see its
> source code.
>
>
>> There are cases where 64-bit integer accumulators would be
>> beneficial, and this is one.  Unfortunately C11 does not require them
>> but some optional moves in that direction are planned.
>>
>>>
>>> https://svn.r-project.org/R/trunk/src/main/summary.c
>>>
>>> Thanks,
>>> Matthew
>>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From pauljohn32 at gmail.com  Mon Mar 25 03:21:40 2013
From: pauljohn32 at gmail.com (Paul Johnson)
Date: Sun, 24 Mar 2013 21:21:40 -0500
Subject: [Rd] ifelse can't return a list? Please explain (R-2.15.3)
Message-ID: <CAErODj9_7ioo2JvVR7m2fyUn3r2wzzfHh9gUVwFuH45NpOeJnQ@mail.gmail.com>

I hope you are doing well.

For me, this was an unexpected problem. I've hoped for quite a few
wrong things today, but I'm only asking you about this one. Why does

ifelse(1, list(a, b, c), list(x, y, z))

return a list with only a, not list(a, b, c) as I hoped.  I wish it
would either
cause an error or return the whole list, not just the first thing.

Working example:

> x <- 1
> y <- 2
> z <- 3
> a <- 4
> b <- 5
> c <- 6
> list(x,y,z)
[[1]]
[1] 1

[[2]]
[1] 2

[[3]]
[1] 3

> list(a,b,c)
[[1]]
[1] 4

[[2]]
[1] 5

[[3]]
[1] 6

> ifelse(1, list(a,b,c), list(x,y,z))
[[1]]
[1] 4

> ifelse(0, list(a,b,c), list(x,y,z))
[[1]]
[1] 1


> sessionInfo()
R version 2.15.3 (2013-03-01)
Platform: x86_64-pc-linux-gnu (64-bit)

locale:
 [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
 [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
 [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8
 [7] LC_PAPER=C                 LC_NAME=C
 [9] LC_ADDRESS=C               LC_TELEPHONE=C
[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] rockchalk_1.5.5.10 car_2.0-16         nnet_7.3-5         MASS_7.3-23

loaded via a namespace (and not attached):
[1] compiler_2.15.3 tools_2.15.3


I realize I can code around this, but I'm just curious about why
ifelse hates me so much :(

> if (1) myvar <- list(a, b, c) else myvar <- list(x, y, z)
> myvar
[[1]]
[1] 4

[[2]]
[1] 5

[[3]]
[1] 6

> myvar <- if (1) list(a, b, c) else list(x, y, z)
> myvar
[[1]]
[1] 4

[[2]]
[1] 5

[[3]]
[1] 6


-- 
Paul E. Johnson
Professor, Political Science      Assoc. Director
1541 Lilac Lane, Room 504      Center for Research Methods
University of Kansas                 University of Kansas
http://pj.freefaculty.org               http://quant.ku.edu


From jwiley.psych at gmail.com  Mon Mar 25 03:32:37 2013
From: jwiley.psych at gmail.com (Joshua Wiley)
Date: Sun, 24 Mar 2013 19:32:37 -0700
Subject: [Rd] ifelse can't return a list? Please explain (R-2.15.3)
In-Reply-To: <CAErODj9_7ioo2JvVR7m2fyUn3r2wzzfHh9gUVwFuH45NpOeJnQ@mail.gmail.com>
References: <CAErODj9_7ioo2JvVR7m2fyUn3r2wzzfHh9gUVwFuH45NpOeJnQ@mail.gmail.com>
Message-ID: <CANz9Z_J3GxeuYkvALUazbm+XS0NV9TBFxG0M8yG5B8LBRbWDKQ@mail.gmail.com>

Hi Paul,

Wonder why this is an R devel thing?

ifelse is vectorized----there should be logical conditions matching
the length of the output.

ifelse(c(1, 1, 1), list(a=2, b=3, c=4), list(d=1, e=2, f=3))

otherwise it is truncated.  Also note that both results have to be
valid, because both are typically evaluated.

Cheers,

Josh

On Sun, Mar 24, 2013 at 7:21 PM, Paul Johnson <pauljohn32 at gmail.com> wrote:
> I hope you are doing well.
>
> For me, this was an unexpected problem. I've hoped for quite a few
> wrong things today, but I'm only asking you about this one. Why does
>
> ifelse(1, list(a, b, c), list(x, y, z))
>
> return a list with only a, not list(a, b, c) as I hoped.  I wish it
> would either
> cause an error or return the whole list, not just the first thing.
>
> Working example:
>
>> x <- 1
>> y <- 2
>> z <- 3
>> a <- 4
>> b <- 5
>> c <- 6
>> list(x,y,z)
> [[1]]
> [1] 1
>
> [[2]]
> [1] 2
>
> [[3]]
> [1] 3
>
>> list(a,b,c)
> [[1]]
> [1] 4
>
> [[2]]
> [1] 5
>
> [[3]]
> [1] 6
>
>> ifelse(1, list(a,b,c), list(x,y,z))
> [[1]]
> [1] 4
>
>> ifelse(0, list(a,b,c), list(x,y,z))
> [[1]]
> [1] 1
>
>
>> sessionInfo()
> R version 2.15.3 (2013-03-01)
> Platform: x86_64-pc-linux-gnu (64-bit)
>
> locale:
>  [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
>  [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
>  [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8
>  [7] LC_PAPER=C                 LC_NAME=C
>  [9] LC_ADDRESS=C               LC_TELEPHONE=C
> [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> other attached packages:
> [1] rockchalk_1.5.5.10 car_2.0-16         nnet_7.3-5         MASS_7.3-23
>
> loaded via a namespace (and not attached):
> [1] compiler_2.15.3 tools_2.15.3
>
>
> I realize I can code around this, but I'm just curious about why
> ifelse hates me so much :(
>
>> if (1) myvar <- list(a, b, c) else myvar <- list(x, y, z)
>> myvar
> [[1]]
> [1] 4
>
> [[2]]
> [1] 5
>
> [[3]]
> [1] 6
>
>> myvar <- if (1) list(a, b, c) else list(x, y, z)
>> myvar
> [[1]]
> [1] 4
>
> [[2]]
> [1] 5
>
> [[3]]
> [1] 6
>
>
> --
> Paul E. Johnson
> Professor, Political Science      Assoc. Director
> 1541 Lilac Lane, Room 504      Center for Research Methods
> University of Kansas                 University of Kansas
> http://pj.freefaculty.org               http://quant.ku.edu
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel



-- 
Joshua Wiley
Ph.D. Student, Health Psychology
University of California, Los Angeles
http://joshuawiley.com/
Senior Analyst - Elkhart Group Ltd.
http://elkhartgroup.com


From winstonchang1 at gmail.com  Mon Mar 25 05:48:21 2013
From: winstonchang1 at gmail.com (Winston Chang)
Date: Sun, 24 Mar 2013 23:48:21 -0500
Subject: [Rd] Mirroring R on a DVCS
In-Reply-To: <514F0EBE.3090302@gmail.com>
References: <mailman.27.1363431608.4225.r-devel@r-project.org>
	<5144B756.60909@gmail.com>
	<0D87478D-9F5C-473A-838F-01EBD742AF08@gmail.com>
	<5144BDA9.8060505@gmail.com>
	<CAFOpNVHjmGDYR9SnF_7Bo+gFNfMP=-e0DV0X5KOFvBeAujZpiA@mail.gmail.com>
	<514F0EBE.3090302@gmail.com>
Message-ID: <CAFOpNVEsSRpy=1jrJkZmGu_dmYaesn0GzZ8AHrBx6Kwh6hE0Qg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20130324/dddb506b/attachment.pl>

From ripley at stats.ox.ac.uk  Mon Mar 25 10:20:46 2013
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 25 Mar 2013 09:20:46 +0000
Subject: [Rd] double in summary.c : isum
In-Reply-To: <514F1566.4030004@gmail.com>
References: <5595fbfb430053bff583ff078ed607ef@imap.plus.net>
	<514D9989.1060909@stats.ox.ac.uk>
	<cfe3cd9e7092cbca7c5901558d605d62@imap.plus.net>
	<514F1566.4030004@gmail.com>
Message-ID: <515016EE.7010601@stats.ox.ac.uk>

On 24/03/2013 15:01, Duncan Murdoch wrote:
> On 13-03-23 10:20 AM, Matthew Dowle wrote:
>> On 23.03.2013 12:01, Prof Brian Ripley wrote:
>>> On 20/03/2013 12:56, Matthew Dowle wrote:
>>>>
>>>> Hi,
>>>>
>>>> Please consider the following :
>>>>
>>>>> x = as.integer(2^30-1)
>>>> [1] 1073741823
>>>>> sum(c(rep(x, 10000000), rep(-x,9999999)))
>>>> [1] 1073741824
>>>>
>>>> Tested on 2.15.2 and a recent R-devel (r62132).
>>>>
>>>> I'm wondering if s in isum could be LDOUBLE instead of double, like
>>>> rsum, to fix this edge case?
>>>
>>> No, because there is no guarantee that LDOUBLE differs from double
>>> (and platform on which it does not).
>>
>> That's a reason for not using LDOUBLE at all isn't it? Yet src/main/*.c
>> has 19 lines using LDOUBLE e.g. arithmetic.c and cum.c as well as
>> summary.c.
>>
>> I'd assumed LDOUBLE was being used by R to benefit from long double (or
>> equivalent) on platforms that support it (which is all modern Unix, Mac
>> and Windows as far as I know). I do realise that the edge case wouldn't

Actually, you don't know.  Really only on almost all Intel ix86: most 
other current CPUs do not have it in hardware.  C99/C11 require long 
double, but does not require the accuracy that you are thinking of and 
it can be implemented in software.

Note that even on ix86 this is something that can be switched on or off 
in the CPU: last time I looked (years ago) it was off by default in 
Microsoft compilers.

All C99 requires is that long double is at least as precise as double. 
C11 recommends in ?F.2

Recommended practice
2 The long double type should match an IEC 60559 extended format.

Notice the 'an': there are two such formats, and both are in use on R 
platforms.  But then some OS/compiler suppliers have never paid any heed 
to ISO standards.

>> be fixed on platforms where LDOUBLE is defined as double.
>
> I think the problem is that there are two opposing targets in R:  we
> want things to be as accurate as possible, and we want them to be
> consistent across platforms. Sometimes one goal wins, sometimes the
> other.  Inconsistencies across platforms give false positives in tests
> that tend to make us miss true bugs.  Some people think we should never
> use LDOUBLE because of that.  In other cases, the extra accuracy is so
> helpful that it's worth it.  So I think you'd need to argue that the
> case you found is something where the benefit outweighs the costs. Since
> almost all integer sums are done exactly with the current code, is it
> really worth introducing inconsistencies in the rare inexact cases?

But as I said lower down, a 64-bit integer accumulator would be helpful, 
C99/C11 requires one at least that large and it is implemented in 
hardware on all known R platforms.  So there is a way to do this pretty 
consistently across platforms.


>
> Duncan Murdoch
>
>
>>
>> What have I misunderstood?
>>
>>>
>>> Users really need to take responsibility for the numerical stability
>>> of calcuations they attempt.  Expecting to sum 20 million large
>>> numbers exactly is unrealistic.
>>
>> Trying to take responsibility, but you said no. Changing from double to
>> LDOUBLE would mean that something that wasn't realistic, was then
>> realistic (on platforms that support long double).
>>
>> And it would bring open source R into line with TERR, which gets the
>> answer right, on 64bit Windows at least. But I'm not sure I should be as
>> confident in TERR as I am in open source R because I can't see its
>> source code.
>>
>>
>>> There are cases where 64-bit integer accumulators would be
>>> beneficial, and this is one.  Unfortunately C11 does not require them
>>> but some optional moves in that direction are planned.
>>>
>>>>
>>>> https://svn.r-project.org/R/trunk/src/main/summary.c
>>>>
>>>> Thanks,
>>>> Matthew
>>>>
>>>> ______________________________________________
>>>> R-devel at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From pburns at pburns.seanet.com  Mon Mar 25 12:16:13 2013
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Mon, 25 Mar 2013 11:16:13 +0000
Subject: [Rd] ifelse can't return a list? Please explain (R-2.15.3)
In-Reply-To: <CAErODj9_7ioo2JvVR7m2fyUn3r2wzzfHh9gUVwFuH45NpOeJnQ@mail.gmail.com>
References: <CAErODj9_7ioo2JvVR7m2fyUn3r2wzzfHh9gUVwFuH45NpOeJnQ@mail.gmail.com>
Message-ID: <515031FD.9020104@pburns.seanet.com>

When you what you hope for turns out to
be wrong, then have a look at 'The R Inferno'.

http://www.burns-stat.com/documents/books/the-r-inferno/

It does talk about 'ifelse'.

Pat

On 25/03/2013 02:21, Paul Johnson wrote:
> I hope you are doing well.
>
> For me, this was an unexpected problem. I've hoped for quite a few
> wrong things today, but I'm only asking you about this one. Why does
>
> ifelse(1, list(a, b, c), list(x, y, z))
>
> return a list with only a, not list(a, b, c) as I hoped.  I wish it
> would either
> cause an error or return the whole list, not just the first thing.
>
> Working example:
>
>> x <- 1
>> y <- 2
>> z <- 3
>> a <- 4
>> b <- 5
>> c <- 6
>> list(x,y,z)
> [[1]]
> [1] 1
>
> [[2]]
> [1] 2
>
> [[3]]
> [1] 3
>
>> list(a,b,c)
> [[1]]
> [1] 4
>
> [[2]]
> [1] 5
>
> [[3]]
> [1] 6
>
>> ifelse(1, list(a,b,c), list(x,y,z))
> [[1]]
> [1] 4
>
>> ifelse(0, list(a,b,c), list(x,y,z))
> [[1]]
> [1] 1
>
>
>> sessionInfo()
> R version 2.15.3 (2013-03-01)
> Platform: x86_64-pc-linux-gnu (64-bit)
>
> locale:
>   [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
>   [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
>   [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8
>   [7] LC_PAPER=C                 LC_NAME=C
>   [9] LC_ADDRESS=C               LC_TELEPHONE=C
> [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> other attached packages:
> [1] rockchalk_1.5.5.10 car_2.0-16         nnet_7.3-5         MASS_7.3-23
>
> loaded via a namespace (and not attached):
> [1] compiler_2.15.3 tools_2.15.3
>
>
> I realize I can code around this, but I'm just curious about why
> ifelse hates me so much :(
>
>> if (1) myvar <- list(a, b, c) else myvar <- list(x, y, z)
>> myvar
> [[1]]
> [1] 4
>
> [[2]]
> [1] 5
>
> [[3]]
> [1] 6
>
>> myvar <- if (1) list(a, b, c) else list(x, y, z)
>> myvar
> [[1]]
> [1] 4
>
> [[2]]
> [1] 5
>
> [[3]]
> [1] 6
>
>

-- 
Patrick Burns
pburns at pburns.seanet.com
twitter: @burnsstat @portfolioprobe
http://www.portfolioprobe.com/blog
http://www.burns-stat.com
(home of:
  'Impatient R'
  'The R Inferno'
  'Tao Te Programming')


From mdowle at mdowle.plus.com  Mon Mar 25 12:27:38 2013
From: mdowle at mdowle.plus.com (Matthew Dowle)
Date: Mon, 25 Mar 2013 11:27:38 +0000
Subject: [Rd] double in summary.c : isum
In-Reply-To: <515016EE.7010601@stats.ox.ac.uk>
References: <5595fbfb430053bff583ff078ed607ef@imap.plus.net>
	<514D9989.1060909@stats.ox.ac.uk>
	<cfe3cd9e7092cbca7c5901558d605d62@imap.plus.net>
	<514F1566.4030004@gmail.com> <515016EE.7010601@stats.ox.ac.uk>
Message-ID: <370ad6b8bd29f8bf1fcec2cb4d27f625@imap.plus.net>

On 25.03.2013 09:20, Prof Brian Ripley wrote:
> On 24/03/2013 15:01, Duncan Murdoch wrote:
>> On 13-03-23 10:20 AM, Matthew Dowle wrote:
>>> On 23.03.2013 12:01, Prof Brian Ripley wrote:
>>>> On 20/03/2013 12:56, Matthew Dowle wrote:
>>>>>
>>>>> Hi,
>>>>>
>>>>> Please consider the following :
>>>>>
>>>>>> x = as.integer(2^30-1)
>>>>> [1] 1073741823
>>>>>> sum(c(rep(x, 10000000), rep(-x,9999999)))
>>>>> [1] 1073741824
>>>>>
>>>>> Tested on 2.15.2 and a recent R-devel (r62132).
>>>>>
>>>>> I'm wondering if s in isum could be LDOUBLE instead of double, 
>>>>> like
>>>>> rsum, to fix this edge case?
>>>>
>>>> No, because there is no guarantee that LDOUBLE differs from double
>>>> (and platform on which it does not).
>>>
>>> That's a reason for not using LDOUBLE at all isn't it? Yet 
>>> src/main/*.c
>>> has 19 lines using LDOUBLE e.g. arithmetic.c and cum.c as well as
>>> summary.c.
>>>
>>> I'd assumed LDOUBLE was being used by R to benefit from long double 
>>> (or
>>> equivalent) on platforms that support it (which is all modern Unix, 
>>> Mac
>>> and Windows as far as I know). I do realise that the edge case 
>>> wouldn't
>
> Actually, you don't know.  Really only on almost all Intel ix86: most
> other current CPUs do not have it in hardware.  C99/C11 require long
> double, but does not require the accuracy that you are thinking of 
> and
> it can be implemented in software.

This is very interesting, thanks. Which of the CRAN machines don't 
support LDOUBLE with higher accuracy than double, either in hardware or 
software?  Yes I had assumed that all CRAN machines would do. It would 
be useful to know for something else I'm working on as well.

>>> be fixed on platforms where LDOUBLE is defined as double.
>>
>> I think the problem is that there are two opposing targets in R:  we
>> want things to be as accurate as possible, and we want them to be
>> consistent across platforms. Sometimes one goal wins, sometimes the
>> other.  Inconsistencies across platforms give false positives in 
>> tests
>> that tend to make us miss true bugs.  Some people think we should 
>> never
>> use LDOUBLE because of that.  In other cases, the extra accuracy is 
>> so
>> helpful that it's worth it.  So I think you'd need to argue that the
>> case you found is something where the benefit outweighs the costs. 
>> Since
>> almost all integer sums are done exactly with the current code, is 
>> it
>> really worth introducing inconsistencies in the rare inexact cases?
>
> But as I said lower down, a 64-bit integer accumulator would be
> helpful, C99/C11 requires one at least that large and it is
> implemented in hardware on all known R platforms.  So there is a way
> to do this pretty consistently across platforms.

That sounds much better. Is it just a matter of changing s to be 
declared as uint64_t?

>>
>> Duncan Murdoch
>>
>>
>>>
>>> What have I misunderstood?
>>>
>>>>
>>>> Users really need to take responsibility for the numerical 
>>>> stability
>>>> of calcuations they attempt.  Expecting to sum 20 million large
>>>> numbers exactly is unrealistic.
>>>
>>> Trying to take responsibility, but you said no. Changing from 
>>> double to
>>> LDOUBLE would mean that something that wasn't realistic, was then
>>> realistic (on platforms that support long double).
>>>
>>> And it would bring open source R into line with TERR, which gets 
>>> the
>>> answer right, on 64bit Windows at least. But I'm not sure I should 
>>> be as
>>> confident in TERR as I am in open source R because I can't see its
>>> source code.
>>>
>>>
>>>> There are cases where 64-bit integer accumulators would be
>>>> beneficial, and this is one.  Unfortunately C11 does not require 
>>>> them
>>>> but some optional moves in that direction are planned.
>>>>
>>>>>
>>>>> https://svn.r-project.org/R/trunk/src/main/summary.c
>>>>>
>>>>> Thanks,
>>>>> Matthew
>>>>>
>>>>> ______________________________________________
>>>>> R-devel at r-project.org mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>
>>


From mdowle at mdowle.plus.com  Mon Mar 25 12:31:37 2013
From: mdowle at mdowle.plus.com (Matthew Dowle)
Date: Mon, 25 Mar 2013 11:31:37 +0000
Subject: [Rd] double in summary.c : isum
In-Reply-To: <370ad6b8bd29f8bf1fcec2cb4d27f625@imap.plus.net>
References: <5595fbfb430053bff583ff078ed607ef@imap.plus.net>
	<514D9989.1060909@stats.ox.ac.uk>
	<cfe3cd9e7092cbca7c5901558d605d62@imap.plus.net>
	<514F1566.4030004@gmail.com> <515016EE.7010601@stats.ox.ac.uk>
	<370ad6b8bd29f8bf1fcec2cb4d27f625@imap.plus.net>
Message-ID: <ccc479e4403dfa6a6618cda79b73d9b1@imap.plus.net>

On 25.03.2013 11:27, Matthew Dowle wrote:
> On 25.03.2013 09:20, Prof Brian Ripley wrote:
>> On 24/03/2013 15:01, Duncan Murdoch wrote:
>>> On 13-03-23 10:20 AM, Matthew Dowle wrote:
>>>> On 23.03.2013 12:01, Prof Brian Ripley wrote:
>>>>> On 20/03/2013 12:56, Matthew Dowle wrote:
>>>>>>
>>>>>> Hi,
>>>>>>
>>>>>> Please consider the following :
>>>>>>
>>>>>>> x = as.integer(2^30-1)
>>>>>> [1] 1073741823
>>>>>>> sum(c(rep(x, 10000000), rep(-x,9999999)))
>>>>>> [1] 1073741824
>>>>>>
>>>>>> Tested on 2.15.2 and a recent R-devel (r62132).
>>>>>>
>>>>>> I'm wondering if s in isum could be LDOUBLE instead of double, 
>>>>>> like
>>>>>> rsum, to fix this edge case?
>>>>>
>>>>> No, because there is no guarantee that LDOUBLE differs from 
>>>>> double
>>>>> (and platform on which it does not).
>>>>
>>>> That's a reason for not using LDOUBLE at all isn't it? Yet 
>>>> src/main/*.c
>>>> has 19 lines using LDOUBLE e.g. arithmetic.c and cum.c as well as
>>>> summary.c.
>>>>
>>>> I'd assumed LDOUBLE was being used by R to benefit from long 
>>>> double (or
>>>> equivalent) on platforms that support it (which is all modern 
>>>> Unix, Mac
>>>> and Windows as far as I know). I do realise that the edge case 
>>>> wouldn't
>>
>> Actually, you don't know.  Really only on almost all Intel ix86: 
>> most
>> other current CPUs do not have it in hardware.  C99/C11 require long
>> double, but does not require the accuracy that you are thinking of 
>> and
>> it can be implemented in software.
>
> This is very interesting, thanks. Which of the CRAN machines don't
> support LDOUBLE with higher accuracy than double, either in hardware
> or software?  Yes I had assumed that all CRAN machines would do. It
> would be useful to know for something else I'm working on as well.
>
>>>> be fixed on platforms where LDOUBLE is defined as double.
>>>
>>> I think the problem is that there are two opposing targets in R:  
>>> we
>>> want things to be as accurate as possible, and we want them to be
>>> consistent across platforms. Sometimes one goal wins, sometimes the
>>> other.  Inconsistencies across platforms give false positives in 
>>> tests
>>> that tend to make us miss true bugs.  Some people think we should 
>>> never
>>> use LDOUBLE because of that.  In other cases, the extra accuracy is 
>>> so
>>> helpful that it's worth it.  So I think you'd need to argue that 
>>> the
>>> case you found is something where the benefit outweighs the costs. 
>>> Since
>>> almost all integer sums are done exactly with the current code, is 
>>> it
>>> really worth introducing inconsistencies in the rare inexact cases?
>>
>> But as I said lower down, a 64-bit integer accumulator would be
>> helpful, C99/C11 requires one at least that large and it is
>> implemented in hardware on all known R platforms.  So there is a way
>> to do this pretty consistently across platforms.
>
> That sounds much better. Is it just a matter of changing s to be
> declared as uint64_t?

Typo. I meant int64_t.

>
>>>
>>> Duncan Murdoch
>>>
>>>
>>>>
>>>> What have I misunderstood?
>>>>
>>>>>
>>>>> Users really need to take responsibility for the numerical 
>>>>> stability
>>>>> of calcuations they attempt.  Expecting to sum 20 million large
>>>>> numbers exactly is unrealistic.
>>>>
>>>> Trying to take responsibility, but you said no. Changing from 
>>>> double to
>>>> LDOUBLE would mean that something that wasn't realistic, was then
>>>> realistic (on platforms that support long double).
>>>>
>>>> And it would bring open source R into line with TERR, which gets 
>>>> the
>>>> answer right, on 64bit Windows at least. But I'm not sure I should 
>>>> be as
>>>> confident in TERR as I am in open source R because I can't see its
>>>> source code.
>>>>
>>>>
>>>>> There are cases where 64-bit integer accumulators would be
>>>>> beneficial, and this is one.  Unfortunately C11 does not require 
>>>>> them
>>>>> but some optional moves in that direction are planned.
>>>>>
>>>>>>
>>>>>> https://svn.r-project.org/R/trunk/src/main/summary.c
>>>>>>
>>>>>> Thanks,
>>>>>> Matthew
>>>>>>
>>>>>> ______________________________________________
>>>>>> R-devel at r-project.org mailing list
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>
>>>> ______________________________________________
>>>> R-devel at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>
>>>


From mdowle at mdowle.plus.com  Mon Mar 25 12:42:08 2013
From: mdowle at mdowle.plus.com (Matthew Dowle)
Date: Mon, 25 Mar 2013 11:42:08 +0000
Subject: [Rd] double in summary.c : isum
In-Reply-To: <ccc479e4403dfa6a6618cda79b73d9b1@imap.plus.net>
References: <5595fbfb430053bff583ff078ed607ef@imap.plus.net>
	<514D9989.1060909@stats.ox.ac.uk>
	<cfe3cd9e7092cbca7c5901558d605d62@imap.plus.net>
	<514F1566.4030004@gmail.com> <515016EE.7010601@stats.ox.ac.uk>
	<370ad6b8bd29f8bf1fcec2cb4d27f625@imap.plus.net>
	<ccc479e4403dfa6a6618cda79b73d9b1@imap.plus.net>
Message-ID: <4ff35e5321adc2d18cceca1e9fbad325@imap.plus.net>

On 25.03.2013 11:31, Matthew Dowle wrote:
> On 25.03.2013 11:27, Matthew Dowle wrote:
>> On 25.03.2013 09:20, Prof Brian Ripley wrote:
>>> On 24/03/2013 15:01, Duncan Murdoch wrote:
>>>> On 13-03-23 10:20 AM, Matthew Dowle wrote:
>>>>> On 23.03.2013 12:01, Prof Brian Ripley wrote:
>>>>>> On 20/03/2013 12:56, Matthew Dowle wrote:
>>>>>>>
>>>>>>> Hi,
>>>>>>>
>>>>>>> Please consider the following :
>>>>>>>
>>>>>>>> x = as.integer(2^30-1)
>>>>>>> [1] 1073741823
>>>>>>>> sum(c(rep(x, 10000000), rep(-x,9999999)))
>>>>>>> [1] 1073741824
>>>>>>>
>>>>>>> Tested on 2.15.2 and a recent R-devel (r62132).
>>>>>>>
>>>>>>> I'm wondering if s in isum could be LDOUBLE instead of double, 
>>>>>>> like
>>>>>>> rsum, to fix this edge case?
>>>>>>
>>>>>> No, because there is no guarantee that LDOUBLE differs from 
>>>>>> double
>>>>>> (and platform on which it does not).
>>>>>
>>>>> That's a reason for not using LDOUBLE at all isn't it? Yet 
>>>>> src/main/*.c
>>>>> has 19 lines using LDOUBLE e.g. arithmetic.c and cum.c as well as
>>>>> summary.c.
>>>>>
>>>>> I'd assumed LDOUBLE was being used by R to benefit from long 
>>>>> double (or
>>>>> equivalent) on platforms that support it (which is all modern 
>>>>> Unix, Mac
>>>>> and Windows as far as I know). I do realise that the edge case 
>>>>> wouldn't
>>>
>>> Actually, you don't know.  Really only on almost all Intel ix86: 
>>> most
>>> other current CPUs do not have it in hardware.  C99/C11 require 
>>> long
>>> double, but does not require the accuracy that you are thinking of 
>>> and
>>> it can be implemented in software.
>>
>> This is very interesting, thanks. Which of the CRAN machines don't
>> support LDOUBLE with higher accuracy than double, either in hardware
>> or software?  Yes I had assumed that all CRAN machines would do. It
>> would be useful to know for something else I'm working on as well.
>>
>>>>> be fixed on platforms where LDOUBLE is defined as double.
>>>>
>>>> I think the problem is that there are two opposing targets in R:  
>>>> we
>>>> want things to be as accurate as possible, and we want them to be
>>>> consistent across platforms. Sometimes one goal wins, sometimes 
>>>> the
>>>> other.  Inconsistencies across platforms give false positives in 
>>>> tests
>>>> that tend to make us miss true bugs.  Some people think we should 
>>>> never
>>>> use LDOUBLE because of that.  In other cases, the extra accuracy 
>>>> is so
>>>> helpful that it's worth it.  So I think you'd need to argue that 
>>>> the
>>>> case you found is something where the benefit outweighs the costs. 
>>>> Since
>>>> almost all integer sums are done exactly with the current code, is 
>>>> it
>>>> really worth introducing inconsistencies in the rare inexact 
>>>> cases?
>>>
>>> But as I said lower down, a 64-bit integer accumulator would be
>>> helpful, C99/C11 requires one at least that large and it is
>>> implemented in hardware on all known R platforms.  So there is a 
>>> way
>>> to do this pretty consistently across platforms.
>>
>> That sounds much better. Is it just a matter of changing s to be
>> declared as uint64_t?
>
> Typo. I meant int64_t.

But even 64-bit integer might under or overflow. Which is one of the 
reasons for accumulating in double (or LDOUBLE) isn't it? To save a
test for over/underflow on each iteration.

>
>>
>>>>
>>>> Duncan Murdoch
>>>>
>>>>
>>>>>
>>>>> What have I misunderstood?
>>>>>
>>>>>>
>>>>>> Users really need to take responsibility for the numerical 
>>>>>> stability
>>>>>> of calcuations they attempt.  Expecting to sum 20 million large
>>>>>> numbers exactly is unrealistic.
>>>>>
>>>>> Trying to take responsibility, but you said no. Changing from 
>>>>> double to
>>>>> LDOUBLE would mean that something that wasn't realistic, was then
>>>>> realistic (on platforms that support long double).
>>>>>
>>>>> And it would bring open source R into line with TERR, which gets 
>>>>> the
>>>>> answer right, on 64bit Windows at least. But I'm not sure I 
>>>>> should be as
>>>>> confident in TERR as I am in open source R because I can't see 
>>>>> its
>>>>> source code.
>>>>>
>>>>>
>>>>>> There are cases where 64-bit integer accumulators would be
>>>>>> beneficial, and this is one.  Unfortunately C11 does not require 
>>>>>> them
>>>>>> but some optional moves in that direction are planned.
>>>>>>
>>>>>>>
>>>>>>> https://svn.r-project.org/R/trunk/src/main/summary.c
>>>>>>>
>>>>>>> Thanks,
>>>>>>> Matthew
>>>>>>>
>>>>>>> ______________________________________________
>>>>>>> R-devel at r-project.org mailing list
>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>>
>>>>> ______________________________________________
>>>>> R-devel at r-project.org mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>>
>>>>


From las65 at buffalo.edu  Mon Mar 25 17:27:31 2013
From: las65 at buffalo.edu (Lori Shepherd)
Date: Mon, 25 Mar 2013 12:27:31 -0400
Subject: [Rd] Rd.sty not found
Message-ID: <1364228851.13888.18.camel@biostat12>

I'm trying to build and check a new package. But keep getting the
warning: 

* checking PDF version of manual ... WARNING
LaTeX errors when creating PDF version.
This typically indicates Rd problems.


When I go into the package.Rcheck directory, 
package-manual.pdf is created.  
however if I do a pdflatex on package.tex I get the following: 


This is pdfTeX, Version 3.1415926-2.4-1.40.13 (TeX Live 2012)
 restricted \write18 enabled.
entering extended mode
(./dbEmpLikeNorm-manual.tex
LaTeX2e <2011/06/27>
Babel <v3.8m> and hyphenation patterns for english, dumylang,
nohyphenation, ge
rman-x-2012-05-30, ngerman-x-2012-05-30, afrikaans, ancientgreek,
ibycus, arabi
c, armenian, basque, bulgarian, catalan, pinyin, coptic, croatian,
czech, danis
h, dutch, ukenglish, usenglishmax, esperanto, estonian, ethiopic, farsi,
finnis
h, french, friulan, galician, german, ngerman, swissgerman, monogreek,
greek, h
ungarian, icelandic, assamese, bengali, gujarati, hindi, kannada,
malayalam, ma
rathi, oriya, panjabi, tamil, telugu, indonesian, interlingua, irish,
italian, 
kurmanji, latin, latvian, lithuanian, mongolian, mongolianlmc, bokmal,
nynorsk,
 polish, portuguese, romanian, romansh, russian, sanskrit, serbian,
serbianc, s
lovak, slovenian, spanish, swedish, turkish, turkmen, ukrainian,
uppersorbian, 
welsh, loaded.

(/usr/local/texlive/2012/texmf-dist/tex/latex/base/book.cls
Document Class: book 2007/10/19 v1.4h Standard LaTeX document class
(/usr/local/texlive/2012/texmf-dist/tex/latex/base/bk10.clo))

! LaTeX Error: File `Rd.sty' not found.

Type X to quit or <RETURN> to proceed,
or enter new name. (Default extension: sty)

Enter file name: 
! Emergency stop.
<read *> 
         
l.4 \usepackage
               {makeidx}^^M
!  ==> Fatal error occurred, no output PDF file produced!



How do I fix this? I have never had this problem when building or
checking packages before? 


Also of note: I notice that the pacakge-Ex.R and package-Ex.Rout run
without error  but the package-Ex.pdf is empty and there is no
associated package-Ex.tex to look at. 

Lori


From murdoch.duncan at gmail.com  Mon Mar 25 18:05:28 2013
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Mon, 25 Mar 2013 13:05:28 -0400
Subject: [Rd] Rd.sty not found
In-Reply-To: <1364228851.13888.18.camel@biostat12>
References: <1364228851.13888.18.camel@biostat12>
Message-ID: <515083D8.5020200@gmail.com>

On 25/03/2013 12:27 PM, Lori Shepherd wrote:
> I'm trying to build and check a new package. But keep getting the
> warning:
>
> * checking PDF version of manual ... WARNING
> LaTeX errors when creating PDF version.
> This typically indicates Rd problems.
>
>
> When I go into the package.Rcheck directory,
> package-manual.pdf is created.
> however if I do a pdflatex on package.tex I get the following:
>
>
> This is pdfTeX, Version 3.1415926-2.4-1.40.13 (TeX Live 2012)
>   restricted \write18 enabled.
> entering extended mode
> (./dbEmpLikeNorm-manual.tex
> LaTeX2e <2011/06/27>
> Babel <v3.8m> and hyphenation patterns for english, dumylang,
> nohyphenation, ge
> rman-x-2012-05-30, ngerman-x-2012-05-30, afrikaans, ancientgreek,
> ibycus, arabi
> c, armenian, basque, bulgarian, catalan, pinyin, coptic, croatian,
> czech, danis
> h, dutch, ukenglish, usenglishmax, esperanto, estonian, ethiopic, farsi,
> finnis
> h, french, friulan, galician, german, ngerman, swissgerman, monogreek,
> greek, h
> ungarian, icelandic, assamese, bengali, gujarati, hindi, kannada,
> malayalam, ma
> rathi, oriya, panjabi, tamil, telugu, indonesian, interlingua, irish,
> italian,
> kurmanji, latin, latvian, lithuanian, mongolian, mongolianlmc, bokmal,
> nynorsk,
>   polish, portuguese, romanian, romansh, russian, sanskrit, serbian,
> serbianc, s
> lovak, slovenian, spanish, swedish, turkish, turkmen, ukrainian,
> uppersorbian,
> welsh, loaded.
>
> (/usr/local/texlive/2012/texmf-dist/tex/latex/base/book.cls
> Document Class: book 2007/10/19 v1.4h Standard LaTeX document class
> (/usr/local/texlive/2012/texmf-dist/tex/latex/base/bk10.clo))
>
> ! LaTeX Error: File `Rd.sty' not found.
>
> Type X to quit or <RETURN> to proceed,
> or enter new name. (Default extension: sty)
>
> Enter file name:
> ! Emergency stop.
> <read *>
>           
> l.4 \usepackage
>                 {makeidx}^^M
> !  ==> Fatal error occurred, no output PDF file produced!
>
>
>
> How do I fix this? I have never had this problem when building or
> checking packages before?

Rd.sty is a style file provided by R to handle some of the macros used 
in help pages.  You can have R tell pdflatex where to find it by using

R CMD texify --pdf package.tex

Alternatively, if your package is in directory "package", you can do

R CMD Rd2pdf package

and it will produce the .tex and texify it all in one step.   I believe 
both will leave the log file for you to examine if there's an error, but 
you can force that using the "--no-clean" command line option.

Duncan Murdoch

>
>
> Also of note: I notice that the pacakge-Ex.R and package-Ex.Rout run
> without error  but the package-Ex.pdf is empty and there is no
> associated package-Ex.tex to look at.


From khoran at cs.ucr.edu  Mon Mar 25 20:50:35 2013
From: khoran at cs.ucr.edu (Kevin Horan)
Date: Mon, 25 Mar 2013 12:50:35 -0700
Subject: [Rd] using openbabel plugins in R
Message-ID: <5150AA8B.404@cs.ucr.edu>

I posted this in openbabel-devel but didn't get much help, so hopefully 
someone here can help. I don't think its too openbabel specific.

I would like to make use of open babel from within the R language.
Initially I just need to do some format conversions, but may expand the
usage to other parts of OpenBabel as well. I am familiar with embedding
C/C++ code in R, but I'm having some trouble with the plugin mechanism
of OpenBabel in this case. The  problem is that the formats are not
available when I run the OpenBabel code from within R. So, for example,
if I search for the SDF format like so:
      OBFormat *format = conv.FindFormat("SDF");
I always get back a 0 value. The same chunk of code executed outside of
R, as a normal stand-alone program, works fine. So does anyone know how
I can ensure that the formats get loaded? Thanks.
      One other thing to mention, someone might suggest linking against a
static version of openbabel which includes all the plugins. I would like
to avoid that if possible since this needs to work in an R package that
will be distributed across platforms, so it would be hard to ask people
to compile a special, static, version of openbabel just to compile this
R package. Since it needs to work on windows, mac and linux, it would be
nice  if I can make use of any existing installed shared obenbabel
libraries. If it turns out it can't be done, then I'll go down that
path. Thanks.

      Here is an example of the problem:

test program (obtest2.cc):

     #include <iostream>
     #include <openbabel/obconversion.h>
     #include <R.h>
     #include <Rinternals.h>

     extern "C" {   SEXP test(); }

     int main(){
         test();
     }
     SEXP  test()
     {
         OpenBabel::OBConversion conv;
         OpenBabel::OBFormat *format = conv.FindFormat("SDF");   //    
search for SDF format
         std::cout<<"format: "<<format<<std::endl;        // print out 
search result, either 0   or an address
         return R_NilValue;
     }


compile:
      g++  -I/usr/include/openbabel-2.0 -I/usr/share/R/include -fpic -c 
obtest2.cc -o obtest2.o
      g++ -o obtest2 obtest2.o -fpic  -lopenbabel 
-lR                       # Executable
      g++ -shared -o libobtest2.so obtest2.o -fpic  -lopenbabel -lR  # R 
library

Run executable:
      $ ./obtest2
      format: 0x7f1858275d20  #found some result, this is what I expect

Run in R:
      R>dyn.load("libobtest2.so")
      R>.Call("test")
          format: 0              # the format was not found, so 0 was
returned
          NULL


After some more experimentation, I have discovered I can get it to work 
in the following way, but I think it is a bit impractical. If I compile 
the shared library as:
     g++ -shared -o libobtest2.so obtest2.o -fpic 
/usr/lib/openbabel/2.2.3/mdlformat.so -lopenbabel -lR
so the name of one of the plugins is specified. Then, in R I run:

     R>dyn.load("/usr/lib/openbabel/2.2.3/mdlformat.so")
     R>dyn.load("libobtest2.so")
     R>.Call("test")
         format: 0x7fe114c96d20
         NULL
So then it works. But this requires that I know the full path to every 
plugin when the code is compiled and when the library is loaded. Is 
there a practical way to do this, say, if this were part of an R 
package? I have also tried compiling the shared library as before, 
without the plugin, and then just loading the plugin with dyn.load but 
this does not work. It seems like it should though, does anyone know why 
it doesn't? Conversely, if you compile with the plugin specified, but 
don't load it with dyn.load it seg faults.
     The way it works normally in OpenBabel is that each plugin is its 
own shared library and then they get loaded at run time with the dlopen 
function (on linux at least). I have verified that this code is still 
being executed when called from within R, but it doesn't work for some 
reason.
     Also, swig does not help.

Thanks.

Kevin


From edd at debian.org  Tue Mar 26 14:54:55 2013
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue, 26 Mar 2013 08:54:55 -0500
Subject: [Rd] using openbabel plugins in R
In-Reply-To: <5150AA8B.404@cs.ucr.edu>
References: <5150AA8B.404@cs.ucr.edu>
Message-ID: <20817.43183.487840.575952@max.nulle.part>


On 25 March 2013 at 12:50, Kevin Horan wrote:
| I posted this in openbabel-devel but didn't get much help, so hopefully 
| someone here can help. I don't think its too openbabel specific.
| 
| I would like to make use of open babel from within the R language.
| Initially I just need to do some format conversions, but may expand the
| usage to other parts of OpenBabel as well. I am familiar with embedding
| C/C++ code in R, but I'm having some trouble with the plugin mechanism
| of OpenBabel in this case. The  problem is that the formats are not
| available when I run the OpenBabel code from within R. So, for example,
| if I search for the SDF format like so:
|       OBFormat *format = conv.FindFormat("SDF");

[...]

|      The way it works normally in OpenBabel is that each plugin is its 
| own shared library and then they get loaded at run time with the dlopen 
| function (on linux at least). I have verified that this code is still 
| being executed when called from within R, but it doesn't work for some 
| reason.

I would try to start from the smallest possible working examples.  R itself
uses dlopen (see eg $RHOME/modules/ for the files it loads), and so does
OpenBabel. Maybe some wires get crossed. You may well have to dig down with
the debugger and see what assumptions / environment variables / ... are valid
or invalid between the working and non-working case.

Dirk

-- 
Dirk Eddelbuettel | edd at debian.org | http://dirk.eddelbuettel.com


From maechler at stat.math.ethz.ch  Tue Mar 26 15:01:02 2013
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 26 Mar 2013 15:01:02 +0100
Subject: [Rd] removing union class
In-Reply-To: <20808.36714.521516.852157@stat.math.ethz.ch>
References: <CAHavPHES4xxZ6+wUXBG54ED8Y3OcOyxrCRUEMhJaB2hsepG3Xw@mail.gmail.com>
	<20802.64086.443586.813352@stat.math.ethz.ch>
	<20802.64898.70553.300100@stat.math.ethz.ch>
	<CAHavPHGHnLEZK6oO9t0A6pCocizo1Anqk37EbzrVH5V1ThjqYA@mail.gmail.com>
	<20808.36714.521516.852157@stat.math.ethz.ch>
Message-ID: <20817.43550.606343.292431@stat.math.ethz.ch>

>>>>> "MM" == Martin Maechler <maechler at stat.math.ethz.ch>
>>>>>     on Tue, 19 Mar 2013 17:16:42 +0100 writes:

>>>>> "RG" == Renaud Gaujoux <renaud at mancala.cbio.uct.ac.za>
>>>>>     on Sun, 17 Mar 2013 10:38:44 +0200 writes:

    RG> Late report is better than never isn't it? :)
    >>> > Well,... you forgot to show the error (and the
    >>> traceback) :
    >>> 

    RG> Apologies, I usually include them (and sessionInfo ...).


    >>> 
    >>> > Note that this problem is somewhat dependent on the
    >>> use of the > infamous "matrix" class {not properly
    >>> defined as a class in S3, > as it may or may not have
    >>> dimnames, and then tried to be made S4 > compatible "as
    >>> well as possible" in the methods package, see
    >>> 

    RG> Ok. What I want to do is to define a class in my
    RG> namespace that gather some matrix-like classes so that I
    RG> can define a set of common functions for them.
    RG> Initially I want matrix and ExpressionSet objects, and
    RG> possibly add array objects later.  Union classes seem to
    RG> be exactly the way to go, but maybe there is an
    RG> alternative?

    MM> There's really *no* problem with class unions.  They
    MM> work fine and nicely... we also use them in the Matrix
    MM> package (part of the R distro).

    MM> The problem you've reported is only a propblem of
    MM> removeClass().

I've committed a patch for this to R-devel yesterday, and to 
R 3.0.0_beta, a few minutes ago.

The solution for now has been to cut one of the ties between
"matrix" and "array" in ``class space''.

Martin


    >>> > Of course, I could not have thought of a realistic
    >>> situation > where this was a problem, but then you
    >>> exemplify one :
    >>> 
    >>> >> Hadley, this is problematic for devtools, because
    >>> load_all tries to cleanup >> S4 classes when an error
    >>> occurs when loading a development package and >> crashes
    >>> with no hint on the original error.
    >>> 
    >>> 
    RG> I guess a quick fix for devtools, would be to wrap the
    RG> cleanup procedure into a try or tryCatch (Hadley?).


    RG> Thanks, Renaud


From mdowle at mdowle.plus.com  Tue Mar 26 15:49:54 2013
From: mdowle at mdowle.plus.com (Matthew Dowle)
Date: Tue, 26 Mar 2013 14:49:54 +0000
Subject: [Rd] C API entry point to currentTime()
Message-ID: <e0e1f8874fca4a3ecb16712fe2043119@imap.plus.net>


Hi,

I used to use currentTime() (from /src/main/datetime.c) to time various 
sections of data.table C code in wall clock time in sub-second accuracy 
(type double), consistently across platforms. The consistency across 
platforms is a really nice feature of currentTime(). But currentTime() 
isn't part of R's API so I changed to clock() in order to pass R3 
checks. This is nicer in many ways but I'd still like to time elapsed 
wall clock time as well, since some of the operations are i/o bound.

Does R provide a C entry point to currentTime() (or equivalent) 
suitable for use by packages?  I searched r-devel archive and the 
manuals but may well have missed it.

Thanks, Matthew


From josh.m.ulrich at gmail.com  Tue Mar 26 17:06:01 2013
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Tue, 26 Mar 2013 11:06:01 -0500
Subject: [Rd] as.Date.POSIXct
Message-ID: <CAPPM_gSGmsqoGEgZN4t6aUWGt+fSKFeXUUvXu29AAXM2WOyBSw@mail.gmail.com>

Would it make sense for as.Date.POSIXct to not assume tz="UTC" if the
POSIXct object has a valid tzone attribute?  Current behavior may be
confusing in certain cases, for example:

> (d <- structure(1090450800, tzone="Europe/Berlin",
+ class=c("POSIXct","POSIXt")))
[1] "2004-07-22 01:00:00 CEST"
> as.Date(d)
[1] "2004-07-21"
> as.Date(as.POSIXlt(d))
[1] "2004-07-22"
> as.Date(d, tz=attr(d,'tzone'))
[1] "2004-07-22"

Best,
--
Joshua Ulrich  |  about.me/joshuaulrich
FOSS Trading  |  www.fosstrading.com

R/Finance 2013: Applied Finance with R  | www.RinFinance.com


From h.wickham at gmail.com  Tue Mar 26 23:45:07 2013
From: h.wickham at gmail.com (Hadley Wickham)
Date: Tue, 26 Mar 2013 17:45:07 -0500
Subject: [Rd] source, sys.source and error line numbers
In-Reply-To: <CAHavPHHLoyDJodDj6aOmKfAnTDuvYjOcNF_YsGYkJp7w3DOXMw@mail.gmail.com>
References: <CAHavPHHLoyDJodDj6aOmKfAnTDuvYjOcNF_YsGYkJp7w3DOXMw@mail.gmail.com>
Message-ID: <CABdHhvGuxJdc6Jgt-nhG5SGDG6m_Tkv0CRXL6VG+Wei-C4=0_w@mail.gmail.com>

It turns out the reason for this is pretty simple:

sys.source does:
for (i in exprs) eval(i, envir)

where source basically does
n <- length(exprs)
for (i in seq_len(n)) eval(expr[i], envir)

so the problem is presumably related to the way that for strips attributes.

Hadley

On Tue, Mar 19, 2013 at 4:03 AM, Renaud Gaujoux
<renaud at mancala.cbio.uct.ac.za> wrote:
> Hi,
>
> is there a way to retrieve the line number of where en error occurred when
> sourcing a file in a tryCatch statement? Is it stored somewhere accessible?
> It is not found in the error object.
>
> Consider the following code/output and note the difference in the traceback
> between source (has line number) and sys.source (has no line number).
>
> Thank you,
> Renaud
>
>
> ########
> # code
> ########
> codefile <- tempfile()
> write("# some comment
> # some good lines
> a <- 1
> # a bad line
> stop('an error')
> # another good line
> b <- 2
> ", file=codefile)
>
> # with source() the line number is displayed
> source(codefile)
> traceback()
> tryCatch(source(codefile), error= function(e){ str(e) })
>
> # with sys.source() the line number is _not_ displayed
> e <- new.env()
> sys.source(codefile, e)
> traceback()
>
> sessionInfo()
>
> #####
> # output
> #####
>
>> codefile <- tempfile()
>> write("# some comment
> + # some good lines
> + a <- 1
> + # a bad line
> + stop('an error')
> + # another good line
> + b <- 2
> + ", file=codefile)
>> # with source() the line number is displayed
>> source(codefile)
> Error in eval(expr, envir, enclos) : an error
>> traceback()
> 5: stop("an error") at file46641af8754#5
> 4: eval(expr, envir, enclos)
> 3: eval(ei, envir)
> 2: withVisible(eval(ei, envir))
> 1: source(codefile)
>> tryCatch(source(codefile), error= function(e){ str(e) })
> List of 2
>  $ message: chr "an error"
>  $ call   : language eval(expr, envir, enclos)
>  - attr(*, "class")= chr [1:3] "simpleError" "error" "condition"
>>
>> # with sys.source() the line number is _not_ displayed
>> e <- new.env()
>> sys.source(codefile, e)
> Error in eval(expr, envir, enclos) : an error
>> traceback()
> 4: stop("an error")
> 3: eval(expr, envir, enclos)
> 2: eval(i, envir)
> 1: sys.source(codefile, e)
>>
>> sessionInfo()
> R version 2.15.3 (2013-03-01)
> Platform: i686-pc-linux-gnu (32-bit)
>
> locale:
>  [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
>  [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
>  [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8
>  [7] LC_PAPER=C                 LC_NAME=C
>  [9] LC_ADDRESS=C               LC_TELEPHONE=C
> [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel



-- 
Chief Scientist, RStudio
http://had.co.nz/


From murdoch.duncan at gmail.com  Wed Mar 27 01:12:54 2013
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Tue, 26 Mar 2013 20:12:54 -0400
Subject: [Rd] source, sys.source and error line numbers
In-Reply-To: <CABdHhvGuxJdc6Jgt-nhG5SGDG6m_Tkv0CRXL6VG+Wei-C4=0_w@mail.gmail.com>
References: <CAHavPHHLoyDJodDj6aOmKfAnTDuvYjOcNF_YsGYkJp7w3DOXMw@mail.gmail.com>
	<CABdHhvGuxJdc6Jgt-nhG5SGDG6m_Tkv0CRXL6VG+Wei-C4=0_w@mail.gmail.com>
Message-ID: <51523986.9020503@gmail.com>

On 13-03-26 6:45 PM, Hadley Wickham wrote:
> It turns out the reason for this is pretty simple:
>
> sys.source does:
> for (i in exprs) eval(i, envir)
>
> where source basically does
> n <- length(exprs)
> for (i in seq_len(n)) eval(expr[i], envir)
>
> so the problem is presumably related to the way that for strips attributes.

That's part of it, but there are also different defaults for 
keep.source.  It needs to be TRUE or the error location won't be known.

I'll fix the difference you noticed after 3.0.0 is released; I think it 
is not serious enough to slip in at this point.

Duncan Murdoch

>
> Hadley
>
> On Tue, Mar 19, 2013 at 4:03 AM, Renaud Gaujoux
> <renaud at mancala.cbio.uct.ac.za> wrote:
>> Hi,
>>
>> is there a way to retrieve the line number of where en error occurred when
>> sourcing a file in a tryCatch statement? Is it stored somewhere accessible?
>> It is not found in the error object.
>>
>> Consider the following code/output and note the difference in the traceback
>> between source (has line number) and sys.source (has no line number).
>>
>> Thank you,
>> Renaud
>>
>>
>> ########
>> # code
>> ########
>> codefile <- tempfile()
>> write("# some comment
>> # some good lines
>> a <- 1
>> # a bad line
>> stop('an error')
>> # another good line
>> b <- 2
>> ", file=codefile)
>>
>> # with source() the line number is displayed
>> source(codefile)
>> traceback()
>> tryCatch(source(codefile), error= function(e){ str(e) })
>>
>> # with sys.source() the line number is _not_ displayed
>> e <- new.env()
>> sys.source(codefile, e)
>> traceback()
>>
>> sessionInfo()
>>
>> #####
>> # output
>> #####
>>
>>> codefile <- tempfile()
>>> write("# some comment
>> + # some good lines
>> + a <- 1
>> + # a bad line
>> + stop('an error')
>> + # another good line
>> + b <- 2
>> + ", file=codefile)
>>> # with source() the line number is displayed
>>> source(codefile)
>> Error in eval(expr, envir, enclos) : an error
>>> traceback()
>> 5: stop("an error") at file46641af8754#5
>> 4: eval(expr, envir, enclos)
>> 3: eval(ei, envir)
>> 2: withVisible(eval(ei, envir))
>> 1: source(codefile)
>>> tryCatch(source(codefile), error= function(e){ str(e) })
>> List of 2
>>   $ message: chr "an error"
>>   $ call   : language eval(expr, envir, enclos)
>>   - attr(*, "class")= chr [1:3] "simpleError" "error" "condition"
>>>
>>> # with sys.source() the line number is _not_ displayed
>>> e <- new.env()
>>> sys.source(codefile, e)
>> Error in eval(expr, envir, enclos) : an error
>>> traceback()
>> 4: stop("an error")
>> 3: eval(expr, envir, enclos)
>> 2: eval(i, envir)
>> 1: sys.source(codefile, e)
>>>
>>> sessionInfo()
>> R version 2.15.3 (2013-03-01)
>> Platform: i686-pc-linux-gnu (32-bit)
>>
>> locale:
>>   [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
>>   [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
>>   [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8
>>   [7] LC_PAPER=C                 LC_NAME=C
>>   [9] LC_ADDRESS=C               LC_TELEPHONE=C
>> [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C
>>
>> attached base packages:
>> [1] stats     graphics  grDevices utils     datasets  methods   base
>>
>>          [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>
>
>


From dwinsemius at comcast.net  Wed Mar 27 05:07:19 2013
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 26 Mar 2013 21:07:19 -0700
Subject: [Rd] mean.data.frame: R 3.0.0 help page wrong?
Message-ID: <64E3A5BD-9070-4982-B548-69F18480C064@comcast.net>


The help page for mean still says there is a method for data.frame although this has been deprecated for several versions and in R 3.0.0 beta I get:

 mean(data.frame(x=rnorm(10), y=rnorm(10))  )
[1] NA
Warning message:
In mean.default(data.frame(x = rnorm(10), y = rnorm(10))) :
  argument is not numeric or logical: returning NA

I read in news():

o   mean() for data frames and sd() for data frames and matrices are
	defunct.


Shouldn't the help page be amended?
-- 
David Winsemius
Alameda, CA, USA


From renaud at mancala.cbio.uct.ac.za  Wed Mar 27 05:24:29 2013
From: renaud at mancala.cbio.uct.ac.za (Renaud Gaujoux)
Date: Wed, 27 Mar 2013 06:24:29 +0200
Subject: [Rd] source, sys.source and error line numbers
In-Reply-To: <51523986.9020503@gmail.com>
References: <CAHavPHHLoyDJodDj6aOmKfAnTDuvYjOcNF_YsGYkJp7w3DOXMw@mail.gmail.com>
	<CABdHhvGuxJdc6Jgt-nhG5SGDG6m_Tkv0CRXL6VG+Wei-C4=0_w@mail.gmail.com>
	<51523986.9020503@gmail.com>
Message-ID: <CAHavPHEaEHgortWMkH3O_wJcudk5hjJnD9O=x0a5L05-id2Yug@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20130327/ce68e4e1/attachment.pl>

From khoran at cs.ucr.edu  Wed Mar 27 18:02:01 2013
From: khoran at cs.ucr.edu (Kevin Horan)
Date: Wed, 27 Mar 2013 10:02:01 -0700
Subject: [Rd] using openbabel plugins in R
In-Reply-To: <20817.43183.487840.575952@max.nulle.part>
References: <5150AA8B.404@cs.ucr.edu>
	<20817.43183.487840.575952@max.nulle.part>
Message-ID: <51532609.9040503@cs.ucr.edu>

After some more testing I have found that it actually does work if I 
compile without the plugin library but load it with dyn.load. I'm not 
sure why this wasn't working before. It only works though if the plugin 
library is loaded before libobtest2.so (the open babel main lib basically).
     So, to clarify, the following works now:

g++ -shared -o libobtest2.so obtest2.o -fpic -lopenbabel -lR

R>dyn.load("/usr/lib/openbabel/2.2.3/mdlformat.so")
R>dyn.load("libobtest2.so")
R>.Call("test")
   format: 0x7fe114c96d20  #this is the correct result
   NULL


     But now I have a chicken and egg problem. The plugin libraries are 
not stored in a standard directory, but open babel provides a function 
to list their paths. So I need to load the open babel library to fetch 
the plugin paths, then I can load the plugins, but, oops, too late, the 
open babel library is already loaded so loading the plugins now doesn't 
work. I tried using dyn.unload("libobtest2.so") but it didn't work. It 
seems like I'd have to compile a small executable program that uses 
openbabel to fetch the plugin paths, then run it as an external program 
from within R, then load the plugins, then load the open babel lib.
     Does it make any sense that the order in which these are loaded 
affects things? Is there a way to load the plugin lib later and still 
have  it work? If the order does have to be maintained, any better ideas 
how to accomplish this? Thanks.
     Also, here is the dlopen command that openbabel uses:
         dlopen(lib_name.c_str(), RTLD_LAZY | RTLD_GLOBAL)


Kevin


On 03/26/2013 06:54 AM, Dirk Eddelbuettel wrote:
> On 25 March 2013 at 12:50, Kevin Horan wrote:
> | I posted this in openbabel-devel but didn't get much help, so hopefully
> | someone here can help. I don't think its too openbabel specific.
> |
> | I would like to make use of open babel from within the R language.
> | Initially I just need to do some format conversions, but may expand the
> | usage to other parts of OpenBabel as well. I am familiar with embedding
> | C/C++ code in R, but I'm having some trouble with the plugin mechanism
> | of OpenBabel in this case. The  problem is that the formats are not
> | available when I run the OpenBabel code from within R. So, for example,
> | if I search for the SDF format like so:
> |       OBFormat *format = conv.FindFormat("SDF");
>
> [...]
>
> |      The way it works normally in OpenBabel is that each plugin is its
> | own shared library and then they get loaded at run time with the dlopen
> | function (on linux at least). I have verified that this code is still
> | being executed when called from within R, but it doesn't work for some
> | reason.
>
> I would try to start from the smallest possible working examples.  R itself
> uses dlopen (see eg $RHOME/modules/ for the files it loads), and so does
> OpenBabel. Maybe some wires get crossed. You may well have to dig down with
> the debugger and see what assumptions / environment variables / ... are valid
> or invalid between the working and non-working case.
>
> Dirk
>


From ligges at statistik.tu-dortmund.de  Wed Mar 27 18:15:37 2013
From: ligges at statistik.tu-dortmund.de (Uwe Ligges)
Date: Wed, 27 Mar 2013 18:15:37 +0100
Subject: [Rd] mean.data.frame: R 3.0.0 help page wrong?
In-Reply-To: <64E3A5BD-9070-4982-B548-69F18480C064@comcast.net>
References: <64E3A5BD-9070-4982-B548-69F18480C064@comcast.net>
Message-ID: <51532939.8000005@statistik.tu-dortmund.de>



On 27.03.2013 05:07, David Winsemius wrote:
>
> The help page for mean still says there is a method for data.frame although this has been deprecated for several versions and in R 3.0.0 beta I get:
>
>   mean(data.frame(x=rnorm(10), y=rnorm(10))  )
> [1] NA
> Warning message:
> In mean.default(data.frame(x = rnorm(10), y = rnorm(10))) :
>    argument is not numeric or logical: returning NA
>
> I read in news():
>
> o   mean() for data frames and sd() for data frames and matrices are
> 	defunct.
>
>
> Shouldn't the help page be amended?

Has been amended this morning by Martin Maechler, although I do not see 
a reply.

Best,
Uwe


From edd at debian.org  Wed Mar 27 18:28:03 2013
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 27 Mar 2013 12:28:03 -0500
Subject: [Rd] using openbabel plugins in R
In-Reply-To: <51532609.9040503@cs.ucr.edu>
References: <5150AA8B.404@cs.ucr.edu>
	<20817.43183.487840.575952@max.nulle.part>
	<51532609.9040503@cs.ucr.edu>
Message-ID: <20819.11299.272331.532730@max.nulle.part>


On 27 March 2013 at 10:02, Kevin Horan wrote:
| After some more testing I have found that it actually does work if I 
| compile without the plugin library but load it with dyn.load. I'm not 
| sure why this wasn't working before. It only works though if the plugin 
| library is loaded before libobtest2.so (the open babel main lib basically).
|      So, to clarify, the following works now:
| 
| g++ -shared -o libobtest2.so obtest2.o -fpic -lopenbabel -lR
| 
| R>dyn.load("/usr/lib/openbabel/2.2.3/mdlformat.so")
| R>dyn.load("libobtest2.so")
| R>.Call("test")
|    format: 0x7fe114c96d20  #this is the correct result
|    NULL
| 
| 
|      But now I have a chicken and egg problem. The plugin libraries are 
| not stored in a standard directory, but open babel provides a function 
| to list their paths. So I need to load the open babel library to fetch 
| the plugin paths, then I can load the plugins, but, oops, too late, the 
| open babel library is already loaded so loading the plugins now doesn't 

Can use something like pkg-config to query the path?  Eg R offers this
(beyond its own "R CMD config ..." interface:

    edd at max:~$ pkg-config --libs-only-L libR
    -L/usr/lib/R/lib  
    edd at max:~$ 

| work. I tried using dyn.unload("libobtest2.so") but it didn't work. It 
| seems like I'd have to compile a small executable program that uses 
| openbabel to fetch the plugin paths, then run it as an external program 
| from within R, then load the plugins, then load the open babel lib.

Yup. And you could do the test / probing (if that is the last resort) at the
configure test.

|      Does it make any sense that the order in which these are loaded 
| affects things? Is there a way to load the plugin lib later and still 
| have  it work? If the order does have to be maintained, any better ideas 
| how to accomplish this? Thanks.
|      Also, here is the dlopen command that openbabel uses:
|          dlopen(lib_name.c_str(), RTLD_LAZY | RTLD_GLOBAL)

That rings a bell. We once had what I think was that very same issue with
Rmpi as the OpenMPI libraries have there symbols split over several shared
libraries.  But that was many many years ago and I have forgotten what we did
then ...

Dirk

 
| Kevin
| 
| 
| On 03/26/2013 06:54 AM, Dirk Eddelbuettel wrote:
| > On 25 March 2013 at 12:50, Kevin Horan wrote:
| > | I posted this in openbabel-devel but didn't get much help, so hopefully
| > | someone here can help. I don't think its too openbabel specific.
| > |
| > | I would like to make use of open babel from within the R language.
| > | Initially I just need to do some format conversions, but may expand the
| > | usage to other parts of OpenBabel as well. I am familiar with embedding
| > | C/C++ code in R, but I'm having some trouble with the plugin mechanism
| > | of OpenBabel in this case. The  problem is that the formats are not
| > | available when I run the OpenBabel code from within R. So, for example,
| > | if I search for the SDF format like so:
| > |       OBFormat *format = conv.FindFormat("SDF");
| >
| > [...]
| >
| > |      The way it works normally in OpenBabel is that each plugin is its
| > | own shared library and then they get loaded at run time with the dlopen
| > | function (on linux at least). I have verified that this code is still
| > | being executed when called from within R, but it doesn't work for some
| > | reason.
| >
| > I would try to start from the smallest possible working examples.  R itself
| > uses dlopen (see eg $RHOME/modules/ for the files it loads), and so does
| > OpenBabel. Maybe some wires get crossed. You may well have to dig down with
| > the debugger and see what assumptions / environment variables / ... are valid
| > or invalid between the working and non-working case.
| >
| > Dirk
| >
| 
| ______________________________________________
| R-devel at r-project.org mailing list
| https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
Dirk Eddelbuettel | edd at debian.org | http://dirk.eddelbuettel.com


From simon.urbanek at r-project.org  Wed Mar 27 19:20:36 2013
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Wed, 27 Mar 2013 14:20:36 -0400
Subject: [Rd] using openbabel plugins in R
In-Reply-To: <51532609.9040503@cs.ucr.edu>
References: <5150AA8B.404@cs.ucr.edu>
	<20817.43183.487840.575952@max.nulle.part>
	<51532609.9040503@cs.ucr.edu>
Message-ID: <2EE7294F-CA83-4398-A164-E4211FF48E99@r-project.org>


On Mar 27, 2013, at 1:02 PM, Kevin Horan wrote:

> After some more testing I have found that it actually does work if I compile without the plugin library but load it with dyn.load. I'm not sure why this wasn't working before. It only works though if the plugin library is loaded before libobtest2.so (the open babel main lib basically).
>    So, to clarify, the following works now:
> 
> g++ -shared -o libobtest2.so obtest2.o -fpic -lopenbabel -lR
> 
> R>dyn.load("/usr/lib/openbabel/2.2.3/mdlformat.so")
> R>dyn.load("libobtest2.so")
> R>.Call("test")
>  format: 0x7fe114c96d20  #this is the correct result
>  NULL
> 
> 
>    But now I have a chicken and egg problem.

Run the egg in a separate R process (i.e. use system() to call another R process which loads libobtest2.so and calls the API to get the path). Then load the modules followed by the .so. 

Note that all this is inherently fragile and probably not portable (e.g. it seems to assume flat namespaces). Another way (only slightly less fragile) is to link libobtest2.so against the modules directly.

The real issue seems in openbabel - there is really no reason why it shouldn't be loading the modules. I didn't look at it, but it could be that it is simply trying to detect things in the wrong namespace and thus mis-detecting something in R as its own. It's obviously a bad design in openbabel as it's polluting the global namespace, but that's another story... (Linux users won't notice as Linux doesn't support two-level namespaces AFAIK).

Cheers,
Simon


> The plugin libraries are not stored in a standard directory, but open babel provides a function to list their paths. So I need to load the open babel library to fetch the plugin paths, then I can load the plugins, but, oops, too late, the open babel library is already loaded so loading the plugins now doesn't work. I tried using dyn.unload("libobtest2.so") but it didn't work. It seems like I'd have to compile a small executable program that uses openbabel to fetch the plugin paths, then run it as an external program from within R, then load the plugins, then load the open babel lib.
>    Does it make any sense that the order in which these are loaded affects things? Is there a way to load the plugin lib later and still have  it work? If the order does have to be maintained, any better ideas how to accomplish this? Thanks.
>    Also, here is the dlopen command that openbabel uses:
>        dlopen(lib_name.c_str(), RTLD_LAZY | RTLD_GLOBAL)
> 
> 
> Kevin
> 
> 
> On 03/26/2013 06:54 AM, Dirk Eddelbuettel wrote:
>> On 25 March 2013 at 12:50, Kevin Horan wrote:
>> | I posted this in openbabel-devel but didn't get much help, so hopefully
>> | someone here can help. I don't think its too openbabel specific.
>> |
>> | I would like to make use of open babel from within the R language.
>> | Initially I just need to do some format conversions, but may expand the
>> | usage to other parts of OpenBabel as well. I am familiar with embedding
>> | C/C++ code in R, but I'm having some trouble with the plugin mechanism
>> | of OpenBabel in this case. The  problem is that the formats are not
>> | available when I run the OpenBabel code from within R. So, for example,
>> | if I search for the SDF format like so:
>> |       OBFormat *format = conv.FindFormat("SDF");
>> 
>> [...]
>> 
>> |      The way it works normally in OpenBabel is that each plugin is its
>> | own shared library and then they get loaded at run time with the dlopen
>> | function (on linux at least). I have verified that this code is still
>> | being executed when called from within R, but it doesn't work for some
>> | reason.
>> 
>> I would try to start from the smallest possible working examples.  R itself
>> uses dlopen (see eg $RHOME/modules/ for the files it loads), and so does
>> OpenBabel. Maybe some wires get crossed. You may well have to dig down with
>> the debugger and see what assumptions / environment variables / ... are valid
>> or invalid between the working and non-working case.
>> 
>> Dirk
>> 
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 
> 


From khoran at cs.ucr.edu  Wed Mar 27 19:35:43 2013
From: khoran at cs.ucr.edu (Kevin Horan)
Date: Wed, 27 Mar 2013 11:35:43 -0700
Subject: [Rd] using openbabel plugins in R
In-Reply-To: <2EE7294F-CA83-4398-A164-E4211FF48E99@r-project.org>
References: <5150AA8B.404@cs.ucr.edu>
	<20817.43183.487840.575952@max.nulle.part>
	<51532609.9040503@cs.ucr.edu>
	<2EE7294F-CA83-4398-A164-E4211FF48E99@r-project.org>
Message-ID: <51533BFF.5000007@cs.ucr.edu>


Thanks for all the suggestions. I have discovered that the problem is 
fixed in openbabel 2.3.x. I had actually been testing with open babel 
2.3.2 (as well as 2.2.3), but I was running it from the build directory 
since I didn't want to install it on the system as I was only testing 
with it. Because of this, 2.3.2 failed in the same way as 2.2.3 (the 
version actually installed), but for a different reason (It couldn't 
find the modules directory ). Actually installing 2.3.2 fixed the problem.

Kevin

On 03/27/2013 11:20 AM, Simon Urbanek wrote:
> On Mar 27, 2013, at 1:02 PM, Kevin Horan wrote:
>
>> After some more testing I have found that it actually does work if I compile without the plugin library but load it with dyn.load. I'm not sure why this wasn't working before. It only works though if the plugin library is loaded before libobtest2.so (the open babel main lib basically).
>>     So, to clarify, the following works now:
>>
>> g++ -shared -o libobtest2.so obtest2.o -fpic -lopenbabel -lR
>>
>> R>dyn.load("/usr/lib/openbabel/2.2.3/mdlformat.so")
>> R>dyn.load("libobtest2.so")
>> R>.Call("test")
>>   format: 0x7fe114c96d20  #this is the correct result
>>   NULL
>>
>>
>>     But now I have a chicken and egg problem.
> Run the egg in a separate R process (i.e. use system() to call another R process which loads libobtest2.so and calls the API to get the path). Then load the modules followed by the .so.
>
> Note that all this is inherently fragile and probably not portable (e.g. it seems to assume flat namespaces). Another way (only slightly less fragile) is to link libobtest2.so against the modules directly.
>
> The real issue seems in openbabel - there is really no reason why it shouldn't be loading the modules. I didn't look at it, but it could be that it is simply trying to detect things in the wrong namespace and thus mis-detecting something in R as its own. It's obviously a bad design in openbabel as it's polluting the global namespace, but that's another story... (Linux users won't notice as Linux doesn't support two-level namespaces AFAIK).
>
> Cheers,
> Simon
>
>
>> The plugin libraries are not stored in a standard directory, but open babel provides a function to list their paths. So I need to load the open babel library to fetch the plugin paths, then I can load the plugins, but, oops, too late, the open babel library is already loaded so loading the plugins now doesn't work. I tried using dyn.unload("libobtest2.so") but it didn't work. It seems like I'd have to compile a small executable program that uses openbabel to fetch the plugin paths, then run it as an external program from within R, then load the plugins, then load the open babel lib.
>>     Does it make any sense that the order in which these are loaded affects things? Is there a way to load the plugin lib later and still have  it work? If the order does have to be maintained, any better ideas how to accomplish this? Thanks.
>>     Also, here is the dlopen command that openbabel uses:
>>         dlopen(lib_name.c_str(), RTLD_LAZY | RTLD_GLOBAL)
>>
>>
>> Kevin
>>
>>
>> On 03/26/2013 06:54 AM, Dirk Eddelbuettel wrote:
>>> On 25 March 2013 at 12:50, Kevin Horan wrote:
>>> | I posted this in openbabel-devel but didn't get much help, so hopefully
>>> | someone here can help. I don't think its too openbabel specific.
>>> |
>>> | I would like to make use of open babel from within the R language.
>>> | Initially I just need to do some format conversions, but may expand the
>>> | usage to other parts of OpenBabel as well. I am familiar with embedding
>>> | C/C++ code in R, but I'm having some trouble with the plugin mechanism
>>> | of OpenBabel in this case. The  problem is that the formats are not
>>> | available when I run the OpenBabel code from within R. So, for example,
>>> | if I search for the SDF format like so:
>>> |       OBFormat *format = conv.FindFormat("SDF");
>>>
>>> [...]
>>>
>>> |      The way it works normally in OpenBabel is that each plugin is its
>>> | own shared library and then they get loaded at run time with the dlopen
>>> | function (on linux at least). I have verified that this code is still
>>> | being executed when called from within R, but it doesn't work for some
>>> | reason.
>>>
>>> I would try to start from the smallest possible working examples.  R itself
>>> uses dlopen (see eg $RHOME/modules/ for the files it loads), and so does
>>> OpenBabel. Maybe some wires get crossed. You may well have to dig down with
>>> the debugger and see what assumptions / environment variables / ... are valid
>>> or invalid between the working and non-working case.
>>>
>>> Dirk
>>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>>


From dtenenba at fhcrc.org  Wed Mar 27 21:37:03 2013
From: dtenenba at fhcrc.org (Dan Tenenbaum)
Date: Wed, 27 Mar 2013 13:37:03 -0700
Subject: [Rd] broken link to binary of R-3.0 RC
Message-ID: <CAF42j22G1f3CSKU8PjgoGnZGgn1m-wW2Y56NY3Bgze6uFsFbYg@mail.gmail.com>

Hi,

>From http://cran.r-project.org/bin/windows/base/rtest.html
I clicked on:

Download R-3.0.0 RC build for Windows

and got Object not found for

http://cran.r-project.org/bin/windows/base/R-3.0.0rc-win.exe

Thanks,
Dan


From groemping at bht-berlin.de  Wed Mar 27 22:13:40 2013
From: groemping at bht-berlin.de (=?UTF-8?Q?Ulrike_Gr=C3=B6mping?=)
Date: Wed, 27 Mar 2013 14:13:40 -0700 (PDT)
Subject: [Rd] sysdata.rda vs. rda files in data directory
In-Reply-To: <514DE296.2040605@beuth-hochschule.de>
References: <514DE296.2040605@beuth-hochschule.de>
Message-ID: <1364418820352-4662661.post@n4.nabble.com>

Dear all, 

for everybody's benefit, here is a brief summary of what I learnt from Uwe
Ligges. There is a convincing reason against combining a data directory with
a datalist file only (the way for retrieving a list of the data available
without the package loaded) with a sysdata.rda file in the R directory (the
only way for achieving a small installed size): the command data(filename)
will not work on the names listed in the datalist file (I thought
otherwise), which would be sort of odd and unexpected. Therefore, I will go
for the small installed size (=sysdata.rda file) without a data directory.

Best, Ulrike



--
View this message in context: http://r.789695.n4.nabble.com/sysdata-rda-vs-rda-files-in-data-directory-tp4662302p4662661.html
Sent from the R devel mailing list archive at Nabble.com.


From h.wickham at gmail.com  Thu Mar 28 19:40:03 2013
From: h.wickham at gmail.com (Hadley Wickham)
Date: Thu, 28 Mar 2013 13:40:03 -0500
Subject: [Rd] Ref classes initFields has incorrect environment?
Message-ID: <CABdHhvFOcTbX0YnQ8Uiz6C_QdWX_hu9kuA7gD8XgDLg+voEzrQ@mail.gmail.com>

As suggested by the following code:

A <- setRefClass("A", fields = list(x = "logical"),
  methods = list(initialize = function(...) {
    x <<- FALSE
    initFields(...)
}))
A$new()
# Works as expected

quote <- as.character
A$new()
# Error in function (value)  :
#  invalid replacement for field ?x?, should be from class ?logical?
or a subclass (was class ?character?)

I get the same error in:
* R version 2.15.2 (2012-10-26)
* R version 2.15.3 (2013-03-01)
* R Under development (unstable) (2013-03-28 r62432)

Hadley

-- 
Chief Scientist, RStudio
http://had.co.nz/


From jmc at r-project.org  Fri Mar 29 19:30:36 2013
From: jmc at r-project.org (John Chambers)
Date: Fri, 29 Mar 2013 11:30:36 -0700
Subject: [Rd] Redefining quote() with reference classes (was: Ref
	classes initFields has incorrect environment?)
In-Reply-To: <CABdHhvFOcTbX0YnQ8Uiz6C_QdWX_hu9kuA7gD8XgDLg+voEzrQ@mail.gmail.com>
References: <CABdHhvFOcTbX0YnQ8Uiz6C_QdWX_hu9kuA7gD8XgDLg+voEzrQ@mail.gmail.com>
Message-ID: <3FB0FB82-F3E3-4911-93B9-D2729246AC94@r-project.org>

Nothing to do with initFields.  If you trace your redefined quote(), it's called from the <<- assignment of x.

The "x" element in the environment for the reference class object is implemented as an active binding in order to enforce the class when assigning the field.

Effectively that makes the assignment behave like a reference class method, and so ends up doing as() and getting back to the malware version of quote().

The fix will be to  have a more bullet proof (and perhaps more efficient) version of the active binding generated for fields.  A better design anyway, but this will take a little re-organization because the current default binding function is generated by some code manipulation.

Meanwhile the workaround is: Don't do what the revised subject heading says.

John


On Mar 28, 2013, at 11:40 AM, Hadley Wickham <h.wickham at gmail.com> wrote:

> As suggested by the following code:
> 
> A <- setRefClass("A", fields = list(x = "logical"),
>  methods = list(initialize = function(...) {
>    x <<- FALSE
>    initFields(...)
> }))
> A$new()
> # Works as expected
> 
> quote <- as.character
> A$new()
> # Error in function (value)  :
> #  invalid replacement for field ?x?, should be from class ?logical?
> or a subclass (was class ?character?)
> 
> I get the same error in:
> * R version 2.15.2 (2012-10-26)
> * R version 2.15.3 (2013-03-01)
> * R Under development (unstable) (2013-03-28 r62432)
> 
> Hadley
> 
> -- 
> Chief Scientist, RStudio
> http://had.co.nz/
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From htl10 at users.sourceforge.net  Sat Mar 30 09:25:37 2013
From: htl10 at users.sourceforge.net (Hin-Tak Leung)
Date: Sat, 30 Mar 2013 08:25:37 +0000 (GMT)
Subject: [Rd] R/Sweave/cairo/freetype bug fix.
Message-ID: <1364631937.97083.YahooMailClassic@web172306.mail.ir2.yahoo.com>

The problem was first seen with R/Sweave (#c0) then reproduced directly with cairo (#c10) and was eventually traced to freetype. The 5-part bug fix:
610ee58e07090ead529849b2a454bb6c503b4995
da11e5e7647b668dee46fd0418ea5ecbc33ae3b2
e1a2ac1900f2f16ec48fb4840a6b7965a8373c2b
869fb8c49ddf292d6daf4826172a308973d3e11f
d56e544d653b09c657911629557ffc5277a503e3
was committed to freetype in January and will form the next release (2.4.12). They were back ported to 2.4.11
https://bugzilla.redhat.com/show_bug.cgi?id=891457#c35
and the redhat people had further back-ported it to 2.4.10 for fedora 18/19 (#c51).

The freetype people had reproduced the problem with a latin font, so this affects most people, unlike what the initial report (#c0) suggests.

Since freetype is part of X11, most unix/linux users would be understandably nervous about breaking X (see #c45 for screenshot of broken gnome terminal!) and should wait up to a year before the new and not-yet-released 2.4.12 becomes an official upgrade; or contact their favourite unix vendors and/or Apple for upgrades. AFAIK, current up-to-date linux distributions ships the rather older 2.4.10, with the exception of fedora 18/19 (#c51). Mac OS X 10.5 ships freetype 2.3.5 as part of X11; I haven't bother looking up later Mac OS X's.

The official R binaries for windows and mac OS X are compiled against static libraries of cairo 1.10.2 (over 2 years old), and cairo 1.11.2 and freetype 2.4.4 respectively, and are firmly in the "do not work correctly" category.

The long and short of the story is that R/Sweave uses a feature of cairo which wasn't implemented before cairo 1.11.2 (#c13, Jan 2011), which in turn depends on a feature of freetype that has been around since 2005 but did not anticipate cairo's usage. It is commendable that the freetype people did not refer to cairo's usage as "misuse" but took the patience to address the problem, unlike some group's style.

It has been an interesting few months returning to freetype after about 17 years, I think.

Here is how to look up what version of freetype - libfreetype.so.x.y.z for most unix platforms, and /usr/X11/lib/libfreetype.x.y.z.dylib on Mac OS X:

(excerpt from docs/VERSION.DLL)

     version    x.y.z   date of release
     2.4.11     6.10.0  Dec 2012
     2.4.10     6.9.0   June 2012
     2.4.9      6.8.1   March 2012
...
     2.4.4      6.6.2   Nov 2010  (official R mac binaries)
...
     2.3.5      6.3.16  July 2007 (Mac OS X 10.5)



From pdalgd at gmail.com  Sat Mar 30 13:47:07 2013
From: pdalgd at gmail.com (peter dalgaard)
Date: Sat, 30 Mar 2013 13:47:07 +0100
Subject: [Rd] R/Sweave/cairo/freetype bug fix.
In-Reply-To: <1364631937.97083.YahooMailClassic@web172306.mail.ir2.yahoo.com>
References: <1364631937.97083.YahooMailClassic@web172306.mail.ir2.yahoo.com>
Message-ID: <2383FD01-6A11-4BFB-9301-506018631079@gmail.com>

Huh?

This is utterly incomprehensible without reading the redhat bugzilla, and even after reading, I'm not sure what the issue is. Something with bold Chinese fonts in X11, but maybe also affecting Latin fonts, ....?

Please explain yourself.

-pd

On Mar 30, 2013, at 09:25 , Hin-Tak Leung wrote:

> The problem was first seen with R/Sweave (#c0) then reproduced directly with cairo (#c10) and was eventually traced to freetype. The 5-part bug fix:
> 610ee58e07090ead529849b2a454bb6c503b4995
> da11e5e7647b668dee46fd0418ea5ecbc33ae3b2
> e1a2ac1900f2f16ec48fb4840a6b7965a8373c2b
> 869fb8c49ddf292d6daf4826172a308973d3e11f
> d56e544d653b09c657911629557ffc5277a503e3
> was committed to freetype in January and will form the next release (2.4.12). They were back ported to 2.4.11
> https://bugzilla.redhat.com/show_bug.cgi?id=891457#c35
> and the redhat people had further back-ported it to 2.4.10 for fedora 18/19 (#c51).
> 
> The freetype people had reproduced the problem with a latin font, so this affects most people, unlike what the initial report (#c0) suggests.
> 
> Since freetype is part of X11, most unix/linux users would be understandably nervous about breaking X (see #c45 for screenshot of broken gnome terminal!) and should wait up to a year before the new and not-yet-released 2.4.12 becomes an official upgrade; or contact their favourite unix vendors and/or Apple for upgrades. AFAIK, current up-to-date linux distributions ships the rather older 2.4.10, with the exception of fedora 18/19 (#c51). Mac OS X 10.5 ships freetype 2.3.5 as part of X11; I haven't bother looking up later Mac OS X's.
> 
> The official R binaries for windows and mac OS X are compiled against static libraries of cairo 1.10.2 (over 2 years old), and cairo 1.11.2 and freetype 2.4.4 respectively, and are firmly in the "do not work correctly" category.
> 
> The long and short of the story is that R/Sweave uses a feature of cairo which wasn't implemented before cairo 1.11.2 (#c13, Jan 2011), which in turn depends on a feature of freetype that has been around since 2005 but did not anticipate cairo's usage. It is commendable that the freetype people did not refer to cairo's usage as "misuse" but took the patience to address the problem, unlike some group's style.
> 
> It has been an interesting few months returning to freetype after about 17 years, I think.
> 
> Here is how to look up what version of freetype - libfreetype.so.x.y.z for most unix platforms, and /usr/X11/lib/libfreetype.x.y.z.dylib on Mac OS X:
> 
> (excerpt from docs/VERSION.DLL)
> 
>     version    x.y.z   date of release
>     2.4.11     6.10.0  Dec 2012
>     2.4.10     6.9.0   June 2012
>     2.4.9      6.8.1   March 2012
> ...
>     2.4.4      6.6.2   Nov 2010  (official R mac binaries)
> ...
>     2.3.5      6.3.16  July 2007 (Mac OS X 10.5)
> 
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From htl10 at users.sourceforge.net  Sat Mar 30 14:24:48 2013
From: htl10 at users.sourceforge.net (Hin-Tak Leung)
Date: Sat, 30 Mar 2013 13:24:48 +0000 (GMT)
Subject: [Rd] R/Sweave/cairo/freetype bug fix.
In-Reply-To: <2383FD01-6A11-4BFB-9301-506018631079@gmail.com>
Message-ID: <1364649888.23992.YahooMailClassic@web172306.mail.ir2.yahoo.com>

Perhaps that's too much details. There is (will be) a new freetype because of cairo's unanticipated usage (which R uses, among other cairo users). Most people should upgrade or request an upgrade eventually, when they are comfortable.

--- On Sat, 30/3/13, peter dalgaard <pdalgd at gmail.com> wrote:

> Huh?
> 
> This is utterly incomprehensible without reading the redhat
> bugzilla, and even after reading, I'm not sure what the
> issue is. Something with bold Chinese fonts in X11, but
> maybe also affecting Latin fonts, ....?
> 
> Please explain yourself.
> 
> -pd
> 
> On Mar 30, 2013, at 09:25 , Hin-Tak Leung wrote:
> 
> > The problem was first seen with R/Sweave (#c0) then
> reproduced directly with cairo (#c10) and was eventually
> traced to freetype. The 5-part bug fix:
> > 610ee58e07090ead529849b2a454bb6c503b4995
> > da11e5e7647b668dee46fd0418ea5ecbc33ae3b2
> > e1a2ac1900f2f16ec48fb4840a6b7965a8373c2b
> > 869fb8c49ddf292d6daf4826172a308973d3e11f
> > d56e544d653b09c657911629557ffc5277a503e3
> > was committed to freetype in January and will form the
> next release (2.4.12). They were back ported to 2.4.11
> > https://bugzilla.redhat.com/show_bug.cgi?id=891457#c35
> > and the redhat people had further back-ported it to
> 2.4.10 for fedora 18/19 (#c51).
> > 
> > The freetype people had reproduced the problem with a
> latin font, so this affects most people, unlike what the
> initial report (#c0) suggests.
> > 
> > Since freetype is part of X11, most unix/linux users
> would be understandably nervous about breaking X (see #c45
> for screenshot of broken gnome terminal!) and should wait up
> to a year before the new and not-yet-released 2.4.12 becomes
> an official upgrade; or contact their favourite unix vendors
> and/or Apple for upgrades. AFAIK, current up-to-date linux
> distributions ships the rather older 2.4.10, with the
> exception of fedora 18/19 (#c51). Mac OS X 10.5 ships
> freetype 2.3.5 as part of X11; I haven't bother looking up
> later Mac OS X's.
> > 
> > The official R binaries for windows and mac OS X are
> compiled against static libraries of cairo 1.10.2 (over 2
> years old), and cairo 1.11.2 and freetype 2.4.4
> respectively, and are firmly in the "do not work correctly"
> category.
> > 
> > The long and short of the story is that R/Sweave uses a
> feature of cairo which wasn't implemented before cairo
> 1.11.2 (#c13, Jan 2011), which in turn depends on a feature
> of freetype that has been around since 2005 but did not
> anticipate cairo's usage. It is commendable that the
> freetype people did not refer to cairo's usage as "misuse"
> but took the patience to address the problem, unlike some
> group's style.
> > 
> > It has been an interesting few months returning to
> freetype after about 17 years, I think.
> > 
> > Here is how to look up what version of freetype -
> libfreetype.so.x.y.z for most unix platforms, and
> /usr/X11/lib/libfreetype.x.y.z.dylib on Mac OS X:
> > 
> > (excerpt from docs/VERSION.DLL)
> > 
> >? ???version? ?
> x.y.z???date of release
> >? ???2.4.11?
> ???6.10.0? Dec 2012
> >? ???2.4.10?
> ???6.9.0???June 2012
> >? ???2.4.9? ? ?
> 6.8.1???March 2012
> > ...
> >? ???2.4.4? ? ?
> 6.6.2???Nov 2010? (official R mac
> binaries)
> > ...
> >? ???2.3.5? ? ?
> 6.3.16? July 2007 (Mac OS X 10.5)
> > 
> > 
> > ______________________________________________
> > R-devel at r-project.org
> mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
> 
> -- 
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Email: pd.mes at cbs.dk?
> Priv: PDalgd at gmail.com
> 
> 
> 
> 
> 
> 
> 
> 
>


From simon.urbanek at r-project.org  Sat Mar 30 19:54:22 2013
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Sat, 30 Mar 2013 14:54:22 -0400
Subject: [Rd] R/Sweave/cairo/freetype bug fix.
In-Reply-To: <1364649888.23992.YahooMailClassic@web172306.mail.ir2.yahoo.com>
References: <1364649888.23992.YahooMailClassic@web172306.mail.ir2.yahoo.com>
Message-ID: <AE4F2665-6E21-43ED-990C-4516BBA95798@r-project.org>

On Mar 30, 2013, at 9:24 AM, Hin-Tak Leung wrote:

> Perhaps that's too much details. There is (will be) a new freetype because of cairo's unanticipated usage (which R uses, among other cairo users). Most people should upgrade or request an upgrade eventually, when they are comfortable.
> 

Which versions are affected? R binary for OS X uses freetype 2.4.11 (and cairo 1.12.14) so I just need to know if there is an action item.

Thanks,
SImon



> --- On Sat, 30/3/13, peter dalgaard <pdalgd at gmail.com> wrote:
> 
>> Huh?
>> 
>> This is utterly incomprehensible without reading the redhat
>> bugzilla, and even after reading, I'm not sure what the
>> issue is. Something with bold Chinese fonts in X11, but
>> maybe also affecting Latin fonts, ....?
>> 
>> Please explain yourself.
>> 
>> -pd
>> 
>> On Mar 30, 2013, at 09:25 , Hin-Tak Leung wrote:
>> 
>>> The problem was first seen with R/Sweave (#c0) then
>> reproduced directly with cairo (#c10) and was eventually
>> traced to freetype. The 5-part bug fix:
>>> 610ee58e07090ead529849b2a454bb6c503b4995
>>> da11e5e7647b668dee46fd0418ea5ecbc33ae3b2
>>> e1a2ac1900f2f16ec48fb4840a6b7965a8373c2b
>>> 869fb8c49ddf292d6daf4826172a308973d3e11f
>>> d56e544d653b09c657911629557ffc5277a503e3
>>> was committed to freetype in January and will form the
>> next release (2.4.12). They were back ported to 2.4.11
>>> https://bugzilla.redhat.com/show_bug.cgi?id=891457#c35
>>> and the redhat people had further back-ported it to
>> 2.4.10 for fedora 18/19 (#c51).
>>> 
>>> The freetype people had reproduced the problem with a
>> latin font, so this affects most people, unlike what the
>> initial report (#c0) suggests.
>>> 
>>> Since freetype is part of X11, most unix/linux users
>> would be understandably nervous about breaking X (see #c45
>> for screenshot of broken gnome terminal!) and should wait up
>> to a year before the new and not-yet-released 2.4.12 becomes
>> an official upgrade; or contact their favourite unix vendors
>> and/or Apple for upgrades. AFAIK, current up-to-date linux
>> distributions ships the rather older 2.4.10, with the
>> exception of fedora 18/19 (#c51). Mac OS X 10.5 ships
>> freetype 2.3.5 as part of X11; I haven't bother looking up
>> later Mac OS X's.
>>> 
>>> The official R binaries for windows and mac OS X are
>> compiled against static libraries of cairo 1.10.2 (over 2
>> years old), and cairo 1.11.2 and freetype 2.4.4
>> respectively, and are firmly in the "do not work correctly"
>> category.
>>> 
>>> The long and short of the story is that R/Sweave uses a
>> feature of cairo which wasn't implemented before cairo
>> 1.11.2 (#c13, Jan 2011), which in turn depends on a feature
>> of freetype that has been around since 2005 but did not
>> anticipate cairo's usage. It is commendable that the
>> freetype people did not refer to cairo's usage as "misuse"
>> but took the patience to address the problem, unlike some
>> group's style.
>>> 
>>> It has been an interesting few months returning to
>> freetype after about 17 years, I think.
>>> 
>>> Here is how to look up what version of freetype -
>> libfreetype.so.x.y.z for most unix platforms, and
>> /usr/X11/lib/libfreetype.x.y.z.dylib on Mac OS X:
>>> 
>>> (excerpt from docs/VERSION.DLL)
>>> 
>>>      version   
>> x.y.z   date of release
>>>      2.4.11 
>>    6.10.0  Dec 2012
>>>      2.4.10 
>>    6.9.0   June 2012
>>>      2.4.9     
>> 6.8.1   March 2012
>>> ...
>>>      2.4.4     
>> 6.6.2   Nov 2010  (official R mac
>> binaries)
>>> ...
>>>      2.3.5     
>> 6.3.16  July 2007 (Mac OS X 10.5)
>>> 
>>> 
>>> ______________________________________________
>>> R-devel at r-project.org
>> mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>> 
>> -- 
>> Peter Dalgaard, Professor,
>> Center for Statistics, Copenhagen Business School
>> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
>> Phone: (+45)38153501
>> Email: pd.mes at cbs.dk 
>> Priv: PDalgd at gmail.com
>> 
>> 
>> 
>> 
>> 
>> 
>> 
>> 
>> 
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 
> 


From htl10 at users.sourceforge.net  Sat Mar 30 20:03:44 2013
From: htl10 at users.sourceforge.net (Hin-Tak Leung)
Date: Sat, 30 Mar 2013 19:03:44 +0000 (GMT)
Subject: [Rd] R/Sweave/cairo/freetype bug fix.
Message-ID: <1364670224.26868.BPMail_high_noncarrier@web172302.mail.ir2.yahoo.com>


"... was committed to freetype in January and will form the next release (2.4.12)". 



------------------------------
On Sat, Mar 30, 2013 18:54 GMT Simon Urbanek wrote:

>On Mar 30, 2013, at 9:24 AM, Hin-Tak Leung wrote:
>
>> Perhaps that's too much details. There is (will be) a new freetype because of cairo's unanticipated usage (which R uses, among other cairo users). Most people should upgrade or request an upgrade eventually, when they are comfortable.
>> 
>
>Which versions are affected? R binary for OS X uses freetype 2.4.11 (and cairo 1.12.14) so I just need to know if there is an action item.
>
>Thanks,
>SImon
>
>
>
>> --- On Sat, 30/3/13, peter dalgaard <pdalgd at gmail.com> wrote:
>> 
>> Huh?
>> 
>> This is utterly incomprehensible without reading the redhat
>> bugzilla, and even after reading, I'm not sure what the
>> issue is. Something with bold Chinese fonts in X11, but
>> maybe also affecting Latin fonts, ....?
>> 
>> Please explain yourself.
>> 
>> -pd
>> 
>> On Mar 30, 2013, at 09:25 , Hin-Tak Leung wrote:
>> 
>>> The problem was first seen with R/Sweave (#c0) then
>> reproduced directly with cairo (#c10) and was eventually
>> traced to freetype. The 5-part bug fix:
>>> 610ee58e07090ead529849b2a454bb6c503b4995
>>> da11e5e7647b668dee46fd0418ea5ecbc33ae3b2
>>> e1a2ac1900f2f16ec48fb4840a6b7965a8373c2b
>>> 869fb8c49ddf292d6daf4826172a308973d3e11f
>>> d56e544d653b09c657911629557ffc5277a503e3
>>> was committed to freetype in January and will form the
>> next release (2.4.12). They were back ported to 2.4.11
>>> https://bugzilla.redhat.com/show_bug.cgi?id=891457#c35
>>> and the redhat people had further back-ported it to
>> 2.4.10 for fedora 18/19 (#c51).
>>> 
>>> The freetype people had reproduced the problem with a
>> latin font, so this affects most people, unlike what the
>> initial report (#c0) suggests.
>>> 
>>> Since freetype is part of X11, most unix/linux users
>> would be understandably nervous about breaking X (see #c45
>> for screenshot of broken gnome terminal!) and should wait up
>> to a year before the new and not-yet-released 2.4.12 becomes
>> an official upgrade; or contact their favourite unix vendors
>> and/or Apple for upgrades. AFAIK, current up-to-date linux
>> distributions ships the rather older 2.4.10, with the
>> exception of fedora 18/19 (#c51). Mac OS X 10.5 ships
>> freetype 2.3.5 as part of X11; I haven't bother looking up
>> later Mac OS X's.
>>> 
>>> The official R binaries for windows and mac OS X are
>> compiled against static libraries of cairo 1.10.2 (over 2
>> years old), and cairo 1.11.2 and freetype 2.4.4
>> respectively, and are firmly in the "do not work correctly"
>> category.
>>> 
>>> The long and short of the story is that R/Sweave uses a
>> feature of cairo which wasn't implemented before cairo
>> 1.11.2 (#c13, Jan 2011), which in turn depends on a feature
>> of freetype that has been around since 2005 but did not
>> anticipate cairo's usage. It is commendable that the
>> freetype people did not refer to cairo's usage as "misuse"
>> but took the patience to address the problem, unlike some
>> group's style.
>>> 
>>> It has been an interesting few months returning to
>> freetype after about 17 years, I think.
>>> 
>>> Here is how to look up what version of freetype -
>> libfreetype.so.x.y.z for most unix platforms, and
>> /usr/X11/lib/libfreetype.x.y.z.dylib on Mac OS X:
>>> 
>>> (excerpt from docs/VERSION.DLL)
>>> 
>>>      version   
>> x.y.z   date of release
>>>      2.4.11 
>>    6.10.0  Dec 2012
>>>      2.4.10 
>>    6.9.0   June 2012
>>>      2.4.9     
>> 6.8.1   March 2012
>>> ...
>>>      2.4.4     
>> 6.6.2   Nov 2010  (official R mac
>> binaries)
>>> ...
>>>      2.3.5     
>> 6.3.16  July 2007 (Mac OS X 10.5)
>>> 
>>> 
>>> ______________________________________________
>>> R-devel at r-project.org
>> mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>> 
>> -- 
>> Peter Dalgaard, Professor,
>> Center for Statistics, Copenhagen Business School
>> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
>> Phone: (+45)38153501
>> Email: pd.mes at cbs.dk 
>> Priv: PDalgd at gmail.com
>> 
>> 
>> 
>> 
>> 
>> 
>> 
>> 
>> 
>> 
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>> 
>> 
>


From jouni.helske at jyu.fi  Sun Mar 31 22:56:03 2013
From: jouni.helske at jyu.fi (Helske Jouni)
Date: Sun, 31 Mar 2013 20:56:03 +0000
Subject: [Rd] terms.formula(y ~ (x + fn(z = NA)) - fn(z = NA),
 simplify=TRUE) does not simplify
Message-ID: <7FC11B33B8C53E4EB9510C0BAA73EAE32309A39C@mbs3.ad.jyu.fi>

Dear all,

I'm trying to use update.formula for removing a function in formula, but when the function has argument set as NA, this doesn't seem to work. I was able to track the issue to terms.formula function (.Internal(terms.formula(..))), but couldn't figure out anything useful from corresponding C code in src/main/model.c. Here's an example of the issue:

> rm(list=ls())
> tmp<-terms.formula(y ~ (x + fn(z = NA)) - fn(z = NA),simplify=TRUE)
> tmp
y ~ x + fn(z = NA)
attr(,"variables")
list(y, x, fn(z = NA), fn(z = NA))
attr(,"factors")
           x fn(z = NA)
y          0          0
x          1          0
fn(z = NA) 0          1
fn(z = NA) 0          0
attr(,"term.labels")
[1] "x"          "fn(z = NA)"
attr(,"order")
[1] 1 1
attr(,"intercept")
[1] 1
attr(,"response")
[1] 1
attr(,".Environment")
<environment: R_GlobalEnv>
> formula(tmp)
y ~ x + fn(z = NA)

with fn(z=a) it works fine, and also with NA_character_ (but not with NA_real_):

> rm(list=ls())
> tmp<-terms.formula(y ~ (x + fn(z = NA_character_)) - fn(z = NA_character_),simplify=TRUE)
> tmp
y ~ x
attr(,"variables")
list(y, x, fn(z = NA_character_))
attr(,"factors")
           x
y          0
x          1
fn(z = NA) 0
attr(,"term.labels")
[1] "x"
attr(,"order")
[1] 1
attr(,"intercept")
[1] 1
attr(,"response")
[1] 1
attr(,".Environment")
<environment: R_GlobalEnv>
> formula(tmp)
y ~ x

Is this what is supposed to happen?

Best regards,

Jouni Helske

