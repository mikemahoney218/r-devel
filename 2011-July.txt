From stephen.ellison at lgcgroup.com  Fri Jul  1 16:24:43 2011
From: stephen.ellison at lgcgroup.com (S Ellison)
Date: Fri, 1 Jul 2011 07:24:43 -0700 (PDT)
Subject: [Rd] Unexpected email address change - again...
In-Reply-To: <98B156BB22D11342A931E823798D434853E9718185@GOLD.corp.lgc-group.com>
References: <98B156BB22D11342A931E823798D434853E9718185@GOLD.corp.lgc-group.com>
Message-ID: <1309530283011-3638311.post@n4.nabble.com>

Further apologies to the list, but emails are still not getting to folk. 
Duncan, you should have had a diff from me yesterday - if not, they've
fouled it up again...



--
View this message in context: http://r.789695.n4.nabble.com/Unexpected-email-address-change-and-maybe-a-missing-manual-patch-tp3621766p3638311.html
Sent from the R devel mailing list archive at Nabble.com.


From ken.sensei at gmail.com  Fri Jul  1 22:13:03 2011
From: ken.sensei at gmail.com (Mohit Dayal)
Date: Sat, 2 Jul 2011 01:43:03 +0530
Subject: [Rd] AS Algorithms
Message-ID: <BANLkTin4Q9sChdnn=ddP+5XViOg3C1+2Xg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20110702/bec7618f/attachment.pl>

From murdoch.duncan at gmail.com  Sat Jul  2 12:02:12 2011
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Sat, 02 Jul 2011 06:02:12 -0400
Subject: [Rd] AS Algorithms
In-Reply-To: <BANLkTin4Q9sChdnn=ddP+5XViOg3C1+2Xg@mail.gmail.com>
References: <BANLkTin4Q9sChdnn=ddP+5XViOg3C1+2Xg@mail.gmail.com>
Message-ID: <4E0EECA4.4060809@gmail.com>

On 11-07-01 4:13 PM, Mohit Dayal wrote:
> Dear R-programmers,
>
> I would like to use one of the AS Algorithms that used to be published in
> the journal Applied Statistics of the Royal Statistical Society (Series C).
> FORTRAN code based on these are available on the Statlib website at
>
> http://lib.stat.cmu.edu/modules.php?op=modload&name=PostWrap&file=index&page=apstat/
>
> with the disclaimer,
>
> *"The Royal Statistical Society holds the copyright to these routines, but
> has given its permission for their distribution provided that no fee is
> charged."*
> *
> *
> *Does this make these programs compatible with an open source license like
> GPL-2 ? More specifically, can one safely include them in an R package?*

I think that is not GPL-2 compatible.  If I'm right, they could still be 
included in an R package if a more permissive license is used on the 
whole package.  I don't know if CRAN will allow you to put one license 
on some parts of the code, and another on those files.

> *
> *
> *BTW I am looking at AS 133 : Finding the global maximum or minimum of a
> function of 1 variable.

I don't know AS 133, but R does have other optimizers, and you may be 
able to use them instead.

Duncan Murdoch

> *
>
> Regards,
> Mohit Dayal
> Researcher
> Applied Statistics&  Computing Lab
> ISB
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From pdalgd at gmail.com  Sat Jul  2 12:27:06 2011
From: pdalgd at gmail.com (peter dalgaard)
Date: Sat, 2 Jul 2011 12:27:06 +0200
Subject: [Rd] AS Algorithms
In-Reply-To: <4E0EECA4.4060809@gmail.com>
References: <BANLkTin4Q9sChdnn=ddP+5XViOg3C1+2Xg@mail.gmail.com>
	<4E0EECA4.4060809@gmail.com>
Message-ID: <489E7040-7122-4FF1-8FB5-9CDD945C3945@gmail.com>


On Jul 2, 2011, at 12:02 , Duncan Murdoch wrote:

> On 11-07-01 4:13 PM, Mohit Dayal wrote:
>> Dear R-programmers,
>> 
>> I would like to use one of the AS Algorithms that used to be published in
>> the journal Applied Statistics of the Royal Statistical Society (Series C).
>> FORTRAN code based on these are available on the Statlib website at
>> 
>> http://lib.stat.cmu.edu/modules.php?op=modload&name=PostWrap&file=index&page=apstat/
>> 
>> with the disclaimer,
>> 
>> *"The Royal Statistical Society holds the copyright to these routines, but
>> has given its permission for their distribution provided that no fee is
>> charged."*
>> *
>> *
>> *Does this make these programs compatible with an open source license like
>> GPL-2 ? More specifically, can one safely include them in an R package?*
> 
> I think that is not GPL-2 compatible.  If I'm right, they could still be included in an R package if a more permissive license is used on the whole package.  I don't know if CRAN will allow you to put one license on some parts of the code, and another on those files.

As for AS algorithms actually _in_ R, we have a statement from the RSS to the effect that it is OK to use them in R since R is free software. (I forget the exact wording.)

I would expect that to carry over to other GPL software, but it could be prudent to ask the RSS explicitly.

-pd

-- 
Peter Dalgaard
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From nashjc at uottawa.ca  Sat Jul  2 20:11:53 2011
From: nashjc at uottawa.ca (John C Nash)
Date: Sat, 02 Jul 2011 14:11:53 -0400
Subject: [Rd] R-devel Digest, Vol 101, Issue 2
In-Reply-To: <mailman.19.1309600805.23060.r-devel@r-project.org>
References: <mailman.19.1309600805.23060.r-devel@r-project.org>
Message-ID: <4E0F5F69.30108@uottawa.ca>

I've been finding that the "loose ends" in many of these older codes cause more trouble
than it is worth in their use. When I encounter them, I've attempted to re-program the
algorithm in R. A lot of the Fortran code is because of the software structure the author
used and nothing to do with the job to be done.

If you can prepare an R function for this, you'd be doing the R community a favour.  You
may also find that a judicious combination of optimize() and grid search gets the task
done satisfactorily.

Best,

JN


> Date: Sat, 2 Jul 2011 01:43:03 +0530
> From: Mohit Dayal <ken.sensei at gmail.com>
> To: R-devel at r-project.org
> Subject: [Rd] AS Algorithms
> 
> I would like to use one of the AS Algorithms that used to be published in
> the journal Applied Statistics of the Royal Statistical Society (Series C).
> FORTRAN code based on these are available on the Statlib website at
> 
> http://lib.stat.cmu.edu/modules.php?op=modload&name=PostWrap&file=index&page=apstat/

[snip]
> 
> *BTW I am looking at AS 133 : Finding the global maximum or minimum of a
> function of 1 variable.
> *
>


From ivo.welch at gmail.com  Sat Jul  2 20:23:01 2011
From: ivo.welch at gmail.com (ivo welch)
Date: Sat, 2 Jul 2011 11:23:01 -0700
Subject: [Rd] speeding up perception
Message-ID: <CAPr7RtU44GO-7_m02-ek+ps3YcR_qr3m7vV8BSvhAYJ=F_bnbA@mail.gmail.com>

Dear R developers: R is supposed to be slow for iterative
calculations.  actually, it isn't.  matrix operations are fast.  it is
data frame operations that are slow.

R <- 1000
C <- 1000

example <- function(m) {
  cat("rows: "); cat(system.time( for (r in 1:R) m[r,20] <-
sqrt(abs(m[r,20])) + rnorm(1) ), "\n")
  cat("columns: "); cat(system.time(for (c in 1:C) m[20,c] <-
sqrt(abs(m[20,c])) + rnorm(1)), "\n")
  if (is.data.frame(m)) { cat("df: columns as names: ");
cat(system.time(for (c in 1:C) m[[c]][20] <- sqrt(abs(m[[c]][20])) +
rnorm(1)), "\n") }
}

cat("\n**** Now as matrix\n")
example( matrix( rnorm(C*R), nrow=R ) )

cat("\n**** Now as data frame\n")
example( as.data.frame( matrix( rnorm(C*R), nrow=R ) ) )


When m is a data frame, the operation is about 300 times slower than
when m is a matrix.    The program is basically accessing 1000
numbers.  When m is a data frame, the speed of R is about 20 accesses
per seconds on a Mac Pro.  This is pretty pathetic.

I do not know the R internals, so the following is pure speculation.
I understand that an index calculation is faster than a vector lookup
for arbitrary size objects, but it seems as if R relies on search to
find its element.  maybe there isn't even a basic vector lookup table.
 a vector lookup table should be possible at least along the dimension
of consecutive storage.  another possible improvement would be to add
an operation that adds an attribute to the data frame that contains a
full index table to the object for quick lookup.  (if the index table
is there, it could be used.  otherwise, R could simply use the
existing internal mechanism.)

I think faster data frame access would significantly improve the
impression that R makes on novices.  just my 5 cents.

/iaw
----
Ivo Welch (ivo.welch at gmail.com)
http://www.ivo-welch.info/
J. Fred Weston Professor of Finance
Anderson School at UCLA, C519


From jmc at r-project.org  Sat Jul  2 22:43:16 2011
From: jmc at r-project.org (John Chambers)
Date: Sat, 02 Jul 2011 13:43:16 -0700
Subject: [Rd] Ref Classes: bug with using '.self' within initialize
	methods?
In-Reply-To: <4E0B5488.4030604@googlemail.com>
References: <4E0B5488.4030604@googlemail.com>
Message-ID: <4E0F82E4.3070805@r-project.org>

I don't have anything to suggest on your specific example but perhaps 
these two notes are relevant.

1. As is mentioned in the documentation, it's generally a bad idea to 
write S4 initialize() methods for reference classes, rather than 
reference class methods for $initialize():
   "a reference method is recommended rather than a method for the S4 
generic function initialize(), because some special initialization is 
required for reference objects _before_ the initialization of fields."



2. In a simple example, there is no problem using .self in a 
$initialize() method.

######
ss <- setRefClass("ss", fields = c("a", "b", "c"),
     methods = list(
         initialize = function(...) {
             callSuper(...)
             .self$b <- .self$a
         },
         check = function()
              .self$c <- .self$a
     ))

s1 <- ss$new(a=1)
s1$check()
stopifnot(identical(s1$a, 1), identical(s1$a, s1$b),
           identical(s1$a, s1$c))
#######

On 6/29/11 9:36 AM, Janko Thyson wrote:
> Dear list,
>
> I'm wondering if the following error I'm getting is a small bug in the
> Reference Class paradigm or if it makes perfect sense.
>
> When you write an explicit initialize method for a Ref Class, can you
> then make use of '.self' WITHIN this initialize method just as you would
> once an object of the class has actually been initialized?
> Because it seems to me that you can not.
>
> Below is an example that shows that calling '.self$someInitFoo()' within
> the initialize method for 'MyClass' does not work (see section "METHODS"
> in example below). Instead I have to go with
> 'someInitFooRefInner(.self=.Object, ...)' (see section "UPDATED METHOD"
> in example below). Yet, this is only possible because there actually IS
> such a method (I try to stick to the recommendations at ?setRefClass
> where it says: "Reference methods should be kept simple; if they need to
> do some specialized *R* computation, that computation should use a
> separate *R* function that is called from the reference method")
>
> The same problem occurs when, say 'someInitFoo()' calls yet another Ref
> Class method (as is the case in the example below with a call to
> '.self$someFoo()').
>
> Is this a desired behavior?
>
> Thanks for any clarifying comments!
> Janko
>
> ##### CODE EXAMPLE #####
>
> # CLASSES
> setRefClass(
>       Class="MyVirtual",
>       contains=c("VIRTUAL"),
>       methods=list(
>           initialize=function(...){
>               callSuper(...)
>               return(.self)
>           },
>           someInitFoo=function(flds, ...){
>               someInitFooRefInner(
>                   .self=.self,
>                   flds=flds
>               )
>           }
>       )
> )
> GENERATOR<- setRefClass(
>       Class="MyClass",
>       contains=c("MyVirtual"),
>       fields=list(
>           A="character",
>           B="numeric"
>       ),
>       methods=list(
>           someFoo=function(...){
>               someFooRefInner(.self=.self, ...)
>           }
>       )
> )
> # /
>
> # GENERICS
> setGeneric(name="someInitFooRefInner",
>       def=function(.self, ...) standardGeneric("someInitFooRefInner"),
>       signature=c(".self")
> )
> setGeneric(name="someFooRefInner",
>       def=function(.self, ...) standardGeneric("someFooRefInner"),
>       signature=c(".self")
> )
> # /
>
> # METHODS
> setMethod(
>       f="someInitFooRefInner",
>       signature=signature(.self="MyVirtual"),
>       definition=function(.self, flds, ...){
>           print("Trying to call '.self$someFoo()")
>           try(.self$someFoo())
>           print("Trying to call 'someFooRefInner(.self=.self)")
>           try(someFooRefInner(.self=.self))
>           return(flds)
>       }
> )
> setMethod(
>       f="someFooRefInner",
>       signature=signature(.self="MyVirtual"),
>       definition=function(.self, ...){
>           print("hello world!")
>       }
> )
> setMethod(
>       f="initialize",
>       signature=signature(.Object="MyVirtual"),
>       definition=function(.Object, GENERATOR=NULL, ...){
>           # MESSAGE
>           if(class(.Object) == "MyVirtual"){
>               cat(paste("initializing object of class '", class(.Object),
> "'",
>                   sep=""), sep="\n")
>           } else {
>               cat(paste("initializig object of class'", class(.Object),
>                   "' inheriting from class 'MyVirtual'", sep=""), sep="\n")
>           }
>           # /
>           # GET GENERATOR OBJECT
>           if(is.null(GENERATOR)){
>               GENERATOR<- getRefClass(class(.Object))
>           }
>           flds<- names(GENERATOR$fields())
>           .Object$someInitFoo(
>               flds=flds,
>               ...
>           )
>           return(.Object)
>       }
> )
> # /
>
> x<- GENERATOR$new()
>
> # UPDATED METHOD
> setMethod(
>       f="initialize",
>       signature=signature(.Object="MyVirtual"),
>       definition=function(.Object, GENERATOR=NULL, ...){
>           # MESSAGE
>           if(class(.Object) == "MyVirtual"){
>               cat(paste("initializing object of class '", class(.Object),
> "'",
>                       sep=""), sep="\n")
>           } else {
>               cat(paste("initializig object of class'", class(.Object),
>                       "' inheriting from class 'MyVirtual'", sep=""),
> sep="\n")
>           }
>           # /
>           # GET GENERATOR OBJECT
>           if(is.null(GENERATOR)){
>               GENERATOR<- getRefClass(class(.Object))
>           }
>           flds<- names(GENERATOR$fields())
>           someInitFooRefInner(.self=.Object, flds=flds, ...)
>           return(.Object)
>       }
> )
> # /
>
> x<- GENERATOR$new()
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From simon.urbanek at r-project.org  Sun Jul  3 06:30:26 2011
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Sun, 3 Jul 2011 00:30:26 -0400
Subject: [Rd] speeding up perception
In-Reply-To: <CAPr7RtU44GO-7_m02-ek+ps3YcR_qr3m7vV8BSvhAYJ=F_bnbA@mail.gmail.com>
References: <CAPr7RtU44GO-7_m02-ek+ps3YcR_qr3m7vV8BSvhAYJ=F_bnbA@mail.gmail.com>
Message-ID: <26F2E4D9-2105-4195-8DC4-10CD86A081E7@r-project.org>

This is just a quick, incomplete response, but the main misconception is really the use of data.frames. If you don't use the elaborate mechanics of data frames that involve the management of row names, then they are definitely the wrong tool to use, because most of the overhead is exactly to manage to row names and you pay a substantial penalty for that. Just drop that one feature and you get timings similar to a matrix:

> m=matrix(rnorm(C*R), nrow=R)
> example(m)
rows: 0.015 0 0.015 0 0 
columns: 0.01 0 0.01 0 0 

> l=as.list(as.data.frame( matrix( rnorm(C*R), nrow=R ) ))
> example(l)
rows: 0.015 0 0.016 0 0 
columns: 0.012 0 0.011 0 0 

(with example modified to use m[[y]][x] instad of m[x,y])

I would not be surprised that many people use data.frames for the convenience of the matrix subsetting/subassignement operators and don't really care about the row names and for all those uses data.frames are the wrong tool. (Just look at `[.data.frame` and `[<-.data.frame`).

As a side note, it's a bit pointless to compare the performance to matrices as they imposes much more rigorous structure (all columns have the same type) - if you use data frames in such special (rare) cases, it's really your fault ;). So the bottom line is to educate users to not use data frames where not needed and/or provide alternatives (and there may be some things coming up, too).

And as I said, this is just a quick note, so carry on and comment on the original question ;).

Cheers,
Simon


On Jul 2, 2011, at 2:23 PM, ivo welch wrote:

> Dear R developers: R is supposed to be slow for iterative
> calculations.  actually, it isn't.  matrix operations are fast.  it is
> data frame operations that are slow.
> 
> R <- 1000
> C <- 1000
> 
> example <- function(m) {
>  cat("rows: "); cat(system.time( for (r in 1:R) m[r,20] <-
> sqrt(abs(m[r,20])) + rnorm(1) ), "\n")
>  cat("columns: "); cat(system.time(for (c in 1:C) m[20,c] <-
> sqrt(abs(m[20,c])) + rnorm(1)), "\n")
>  if (is.data.frame(m)) { cat("df: columns as names: ");
> cat(system.time(for (c in 1:C) m[[c]][20] <- sqrt(abs(m[[c]][20])) +
> rnorm(1)), "\n") }
> }
> 
> cat("\n**** Now as matrix\n")
> example( matrix( rnorm(C*R), nrow=R ) )
> 
> cat("\n**** Now as data frame\n")
> example( as.data.frame( matrix( rnorm(C*R), nrow=R ) ) )
> 
> 
> When m is a data frame, the operation is about 300 times slower than
> when m is a matrix.    The program is basically accessing 1000
> numbers.  When m is a data frame, the speed of R is about 20 accesses
> per seconds on a Mac Pro.  This is pretty pathetic.
> 
> I do not know the R internals, so the following is pure speculation.
> I understand that an index calculation is faster than a vector lookup
> for arbitrary size objects, but it seems as if R relies on search to
> find its element.  maybe there isn't even a basic vector lookup table.
> a vector lookup table should be possible at least along the dimension
> of consecutive storage.  another possible improvement would be to add
> an operation that adds an attribute to the data frame that contains a
> full index table to the object for quick lookup.  (if the index table
> is there, it could be used.  otherwise, R could simply use the
> existing internal mechanism.)
> 
> I think faster data frame access would significantly improve the
> impression that R makes on novices.  just my 5 cents.
> 
> /iaw
> ----
> Ivo Welch (ivo.welch at gmail.com)
> http://www.ivo-welch.info/
> J. Fred Weston Professor of Finance
> Anderson School at UCLA, C519
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 
> 


From rainmansr at gmail.com  Sun Jul  3 14:13:03 2011
From: rainmansr at gmail.com (Robert Stojnic)
Date: Sun, 03 Jul 2011 13:13:03 +0100
Subject: [Rd] speeding up perception
In-Reply-To: <26F2E4D9-2105-4195-8DC4-10CD86A081E7@r-project.org>
References: <CAPr7RtU44GO-7_m02-ek+ps3YcR_qr3m7vV8BSvhAYJ=F_bnbA@mail.gmail.com>
	<26F2E4D9-2105-4195-8DC4-10CD86A081E7@r-project.org>
Message-ID: <4E105CCF.8090306@gmail.com>


Hi Simon,

On 03/07/11 05:30, Simon Urbanek wrote:
> This is just a quick, incomplete response, but the main misconception is really the use of data.frames. If you don't use the elaborate mechanics of data frames that involve the management of row names, then they are definitely the wrong tool to use, because most of the overhead is exactly to manage to row names and you pay a substantial penalty for that. Just drop that one feature and you get timings similar to a matrix:

I tried to find some documentation on why there needs to be extra row 
names handling when one is just assigning values into the column of a 
data frame, but couldn't find any. For a while I stared at the code of 
`[<-.data.frame` but couldn't figure out it myself. Can you please 
summarise what exactly is going one when one does m[1, 1] <- 1 where m 
is a data frame?

I found that the performance is significantly different with different 
number of columns. For instance

# reassign first column to 1
example <- function(m){
     for(i in 1:1000)
         m[i,1] <- 1
}

m <- as.data.frame(matrix(0, ncol=2, nrow=1000))
system.time( example(m) )

    user  system elapsed
   0.164   0.000   0.163

m <- as.data.frame(matrix(0, ncol=1000, nrow=1000))
system.time( example(m) )

    user  system elapsed
  34.634   0.004  34.765

When m is a matrix, both run well under 0.1s.

Increasing the number of rows (but not the number of iterations) leads 
to some increase in time, but not as drastic when increasing column 
number. Using m[[y]][x] in this case doesn't help either.

example2 <- function(m){
     for(i in 1:1000)
         m[[1]][i] <- 1
}

m <- as.data.frame(matrix(0, ncol=1000, nrow=1000))
system.time( example2(m) )

    user  system elapsed
  36.007   0.148  36.233


r.


From simon.urbanek at r-project.org  Sun Jul  3 16:26:03 2011
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Sun, 3 Jul 2011 10:26:03 -0400
Subject: [Rd] speeding up perception
In-Reply-To: <4E105CCF.8090306@gmail.com>
References: <CAPr7RtU44GO-7_m02-ek+ps3YcR_qr3m7vV8BSvhAYJ=F_bnbA@mail.gmail.com>
	<26F2E4D9-2105-4195-8DC4-10CD86A081E7@r-project.org>
	<4E105CCF.8090306@gmail.com>
Message-ID: <459E3053-481C-4A86-BEB2-946E7B8854C6@r-project.org>

Robert,

it's not the handling of row names per se that causes the slowdown, but my point was that if what you need is just matrix-like structure with different column types, you may want to use lists instead and for equal column types you're better of with a matrix.

But to address your point, one of the reasons for subassignments on data frames being slow is that they need extra copies of the data frame for method dispatch. Data frames are lists of column vectors, so the penalty is worse with increasing number of columns. Rows play no significant (additional) role, because those are simply operations on the column vectors (they need to be copied on modification in any case).

In practice it would not matter as much unless the users do stupid things like the example loop. In that case the list holding the columns is copied twice for every single value of i which is deadly. Obviously the sensible thing to do m[1:1000,1] <- 1 does not have that issue.

So to illustrate part of the data.frame penalty effect consider simply falling back to lists in the assignment:

> example2 <- function(m){
+    for(i in 1:1000)
+        m[[1]][i] <- 1
+ }
> m <- as.data.frame(matrix(0, ncol=1000, nrow=1000))
> system.time( example2(m) )
   user  system elapsed 
 44.359  13.608  58.011 

> ### using a list is very fast as illustrated before:
> m <- as.list(as.data.frame(matrix(0, ncol=1000, nrow=1000)))
> system.time( example2(m) )
   user  system elapsed 
   0.01    0.00    0.01 

> ### now try to fall back to a list for each iteration (part of what the data frames have to do):
> example3 <- function(m){
+    for(i in 1:1000) {
+        oc <- class(m)
+        class(m) <- NULL
+        m[[1]][i] <- 1
+        class(m) <- oc
+    }
+ }
> system.time( example3(m) )
   user  system elapsed 
 19.080   2.251  21.335 

So just the simple fact that you unclass and re-class the object gives you half of the penalty that data.frames incur even if you're dealing with a list. Add the additional logic that data frames have to go through and you have the full picture.

So, as I was saying earlier, if you want to loop subassignments over many elements: don't do that in the first place, but if you do, use lists or matrices, NOT data frames.

Cheers,
Simon


On Jul 3, 2011, at 8:13 AM, Robert Stojnic wrote:

> 
> Hi Simon,
> 
> On 03/07/11 05:30, Simon Urbanek wrote:
>> This is just a quick, incomplete response, but the main misconception is really the use of data.frames. If you don't use the elaborate mechanics of data frames that involve the management of row names, then they are definitely the wrong tool to use, because most of the overhead is exactly to manage to row names and you pay a substantial penalty for that. Just drop that one feature and you get timings similar to a matrix:
> 
> I tried to find some documentation on why there needs to be extra row names handling when one is just assigning values into the column of a data frame, but couldn't find any. For a while I stared at the code of `[<-.data.frame` but couldn't figure out it myself. Can you please summarise what exactly is going one when one does m[1, 1] <- 1 where m is a data frame?
> 
> I found that the performance is significantly different with different number of columns. For instance
> 
> # reassign first column to 1
> example <- function(m){
>    for(i in 1:1000)
>        m[i,1] <- 1
> }
> 
> m <- as.data.frame(matrix(0, ncol=2, nrow=1000))
> system.time( example(m) )
> 
>   user  system elapsed
>  0.164   0.000   0.163
> 
> m <- as.data.frame(matrix(0, ncol=1000, nrow=1000))
> system.time( example(m) )
> 
>   user  system elapsed
> 34.634   0.004  34.765
> 
> When m is a matrix, both run well under 0.1s.
> 
> Increasing the number of rows (but not the number of iterations) leads to some increase in time, but not as drastic when increasing column number. Using m[[y]][x] in this case doesn't help either.
> 
> example2 <- function(m){
>    for(i in 1:1000)
>        m[[1]][i] <- 1
> }
> 
> m <- as.data.frame(matrix(0, ncol=1000, nrow=1000))
> system.time( example2(m) )
> 
>   user  system elapsed
> 36.007   0.148  36.233
> 
> 
> r.
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 
> 


From ivo.welch at gmail.com  Mon Jul  4 07:19:53 2011
From: ivo.welch at gmail.com (ivo welch)
Date: Sun, 3 Jul 2011 22:19:53 -0700
Subject: [Rd] speeding up perception
In-Reply-To: <CAPr7RtU44GO-7_m02-ek+ps3YcR_qr3m7vV8BSvhAYJ=F_bnbA@mail.gmail.com>
References: <CAPr7RtU44GO-7_m02-ek+ps3YcR_qr3m7vV8BSvhAYJ=F_bnbA@mail.gmail.com>
Message-ID: <CAPr7RtVx1ao9bubQiCrf9LjgpdZdB24vSnjZvGxW4a8Sy-pKag@mail.gmail.com>

thank you, simon. ?this was very interesting indeed.  I also now
understand how far out of my depth I am here.

fortunately, as an end user, obviously, *I* now know how to avoid the
problem.  I particularly like the as.list() transformation and back to
as.data.frame() to speed things up without loss of (much)
functionality.


more broadly, I view the avoidance of individual access through the
use of apply and vector operations as a mixed "IQ test" and "knowledge
test" (which I often fail).  However, even for the most clever, there
are also situations where the KISS programming principle makes
explicit loops still preferable.  Personally, I would have preferred
it if R had, in its standard "statistical data set" data structure,
foregone the row names feature in exchange for retaining fast direct
access.  R could have reserved its current implementation "with row
names but slow access" for a less common (possibly pseudo-inheriting)
data structure.


If end users commonly do iterations over a data frame, which I would
guess to be the case, then the impression of R by (novice) end users
could be greatly enhanced if the extreme penalties could be eliminated
or at least flagged.  For example, I wonder if modest special internal
code could store data frames internally and transparently as lists of
vectors UNTIL a row name is assigned to.  Easier and uglier, a simple
but specific warning message could be issued with a suggestion if
there is an individual read/write into a data frame ("Warning: data
frames are much slower than lists of vectors for individual element
access").


I would also suggest changing the "Introduction to R" 6.3  from "A
data frame may for many purposes be regarded as a matrix with columns
possibly of differing modes and attributes. It may be displayed in
matrix form, and its rows and columns extracted using matrix indexing
conventions." to "A data frame may for many purposes be regarded as a
matrix with columns possibly of differing modes and attributes. It may
be displayed in matrix form, and its rows and columns extracted using
matrix indexing conventions.  However, data frames can be much slower
than matrices or even lists of vectors (which, like data frames, can
contain different types of columns) when individual elements need to
be accessed."  Reading about it immediately upon introduction could
flag the problem in a more visible manner.


regards,

/iaw


From ripley at stats.ox.ac.uk  Mon Jul  4 14:08:38 2011
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 4 Jul 2011 13:08:38 +0100 (BST)
Subject: [Rd] Recent and upcoming changes to R-devel
Message-ID: <alpine.OSX.1.00.1107041124460.59432@tystie.local>

There was an R-core meeting the week before last, and various planned 
changes will appear in R-devel over the next few weeks.

These are changes planned for R 2.14.0 scheduled for Oct 31.  As we 
are sick of people referring to R-devel as '2.14' or '2.14.0', that 
version number will not be used until we reach 2.14.0 alpha.  You will 
be able to have a package depend on an svn version number when 
referring to R-devel rather than using R (>= 2.14.0).

All packages are installed with lazy-loading (there were 72 CRAN 
packages and 8 BioC packages which opted out).  This means that the 
code is always parsed at install time which inter alia simplifies the 
descriptions.  R 2.13.1 RC warns on installation about packages which 
ask not to be lazy-loaded, and R-devel ignores such requests (with a 
warning).

In the near future all packages will have a name space.  If the 
sources do not contain one, a default NAMESPACE file will be added. 
This again will simplify the descriptions and also a lot of internal 
code.  Maintainers of packages without name spaces (currently 42% of 
CRAN) are encouraged to add one themselves.

R-devel is installed with the base and recommended packages 
byte-compiled (the equivalent of 'make bytecode' in R 2.13.x, but done 
less inefficiently).  There is a new option
R CMD INSTALL --byte-compile
to byte-compile contributed packages, but that remains optional. 
Byte-compilation is quite expensive (so you definitely want to do it 
at install time, which requires lazy-loading), and relatively few 
packages benefit appreciably from byte-compilation.  A larger number 
of packages benefit from byte-compilation of R itself: for example AER 
runs its checks 10% faster.  The byte-compiler technology is thanks to 
Luke Tierney.

There is support for figures in Rd files: currently with a first-pass
implementation (thanks to Duncan Murdoch).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From mtmorgan at fhcrc.org  Mon Jul  4 14:29:12 2011
From: mtmorgan at fhcrc.org (Martin Morgan)
Date: Mon, 04 Jul 2011 05:29:12 -0700
Subject: [Rd] Recent and upcoming changes to R-devel
In-Reply-To: <alpine.OSX.1.00.1107041124460.59432@tystie.local>
References: <alpine.OSX.1.00.1107041124460.59432@tystie.local>
Message-ID: <4E11B218.1020600@fhcrc.org>

On 07/04/2011 05:08 AM, Prof Brian Ripley wrote:
> There was an R-core meeting the week before last, and various planned
> changes will appear in R-devel over the next few weeks.
>
> These are changes planned for R 2.14.0 scheduled for Oct 31. As we are
> sick of people referring to R-devel as '2.14' or '2.14.0', that version
> number will not be used until we reach 2.14.0 alpha. You will be able to
> have a package depend on an svn version number when referring to R-devel
> rather than using R (>= 2.14.0).
>
> All packages are installed with lazy-loading (there were 72 CRAN
> packages and 8 BioC packages which opted out). This means that the code
> is always parsed at install time which inter alia simplifies the
> descriptions. R 2.13.1 RC warns on installation about packages which ask
> not to be lazy-loaded, and R-devel ignores such requests (with a warning).
>
> In the near future all packages will have a name space. If the sources
> do not contain one, a default NAMESPACE file will be added. This again
> will simplify the descriptions and also a lot of internal code.
> Maintainers of packages without name spaces (currently 42% of CRAN) are
> encouraged to add one themselves.
>
> R-devel is installed with the base and recommended packages
> byte-compiled (the equivalent of 'make bytecode' in R 2.13.x, but done
> less inefficiently). There is a new option
> R CMD INSTALL --byte-compile
> to byte-compile contributed packages, but that remains optional.

Anticipating the future, contributed package byte-compilation will have 
large effects on CRAN and especially Bioconductor build systems. For 
instance, a moderate-sized package like Biobase built without vignettes 
installs in about 19s with byte compilation, 9s with, while a more 
complicated package IRanges is 1m25s, vs. 29s.

For Bioconductor this will certainly require new hardware across all 
supported platforms, and almost certainly significant effort to improve 
build system efficiencies.

Martin

> Byte-compilation is quite expensive (so you definitely want to do it at
> install time, which requires lazy-loading), and relatively few packages
> benefit appreciably from byte-compilation. A larger number of packages
> benefit from byte-compilation of R itself: for example AER runs its
> checks 10% faster. The byte-compiler technology is thanks to Luke Tierney.
>
> There is support for figures in Rd files: currently with a first-pass
> implementation (thanks to Duncan Murdoch).
>


-- 
Computational Biology
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N. PO Box 19024 Seattle, WA 98109

Location: M1-B861
Telephone: 206 667-2793


From ripley at stats.ox.ac.uk  Mon Jul  4 14:48:35 2011
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 4 Jul 2011 13:48:35 +0100 (BST)
Subject: [Rd] Recent and upcoming changes to R-devel
In-Reply-To: <4E11B218.1020600@fhcrc.org>
References: <alpine.OSX.1.00.1107041124460.59432@tystie.local>
	<4E11B218.1020600@fhcrc.org>
Message-ID: <alpine.LFD.2.02.1107041344290.25381@gannet.stats.ox.ac.uk>

On Mon, 4 Jul 2011, Martin Morgan wrote:

> On 07/04/2011 05:08 AM, Prof Brian Ripley wrote:
>> There was an R-core meeting the week before last, and various planned
>> changes will appear in R-devel over the next few weeks.
>> 
>> These are changes planned for R 2.14.0 scheduled for Oct 31. As we are
>> sick of people referring to R-devel as '2.14' or '2.14.0', that version
>> number will not be used until we reach 2.14.0 alpha. You will be able to
>> have a package depend on an svn version number when referring to R-devel
>> rather than using R (>= 2.14.0).
>> 
>> All packages are installed with lazy-loading (there were 72 CRAN
>> packages and 8 BioC packages which opted out). This means that the code
>> is always parsed at install time which inter alia simplifies the
>> descriptions. R 2.13.1 RC warns on installation about packages which ask
>> not to be lazy-loaded, and R-devel ignores such requests (with a warning).
>> 
>> In the near future all packages will have a name space. If the sources
>> do not contain one, a default NAMESPACE file will be added. This again
>> will simplify the descriptions and also a lot of internal code.
>> Maintainers of packages without name spaces (currently 42% of CRAN) are
>> encouraged to add one themselves.
>> 
>> R-devel is installed with the base and recommended packages
>> byte-compiled (the equivalent of 'make bytecode' in R 2.13.x, but done
>> less inefficiently). There is a new option
>> R CMD INSTALL --byte-compile
>> to byte-compile contributed packages, but that remains optional.
>
> Anticipating the future, contributed package byte-compilation will have large 
> effects on CRAN and especially Bioconductor build systems. For instance, a 
> moderate-sized package like Biobase built without vignettes installs in about 
> 19s with byte compilation, 9s with, while a more complicated package IRanges 
> is 1m25s, vs. 29s.

I presume the first is 'with' the second 'without'.  Yes, as I did say
'byte compilation is quite expensive', and it is not clear if it will 
ever become the default for contributed packages.

> For Bioconductor this will certainly require new hardware across all 
> supported platforms, and almost certainly significant effort to improve build 
> system efficiencies.
>
> Martin
>
>> Byte-compilation is quite expensive (so you definitely want to do it at
>> install time, which requires lazy-loading), and relatively few packages
>> benefit appreciably from byte-compilation. A larger number of packages
>> benefit from byte-compilation of R itself: for example AER runs its
>> checks 10% faster. The byte-compiler technology is thanks to Luke Tierney.
>> 
>> There is support for figures in Rd files: currently with a first-pass
>> implementation (thanks to Duncan Murdoch).
>> 
>
>
> -- 
> Computational Biology
> Fred Hutchinson Cancer Research Center
> 1100 Fairview Ave. N. PO Box 19024 Seattle, WA 98109
>
> Location: M1-B861
> Telephone: 206 667-2793
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From timothee.carayol at gmail.com  Mon Jul  4 08:47:59 2011
From: timothee.carayol at gmail.com (=?ISO-8859-1?Q?Timoth=E9e_Carayol?=)
Date: Mon, 4 Jul 2011 07:47:59 +0100
Subject: [Rd] speeding up perception
In-Reply-To: <CAPr7RtVx1ao9bubQiCrf9LjgpdZdB24vSnjZvGxW4a8Sy-pKag@mail.gmail.com>
References: <CAPr7RtU44GO-7_m02-ek+ps3YcR_qr3m7vV8BSvhAYJ=F_bnbA@mail.gmail.com>
	<CAPr7RtVx1ao9bubQiCrf9LjgpdZdB24vSnjZvGxW4a8Sy-pKag@mail.gmail.com>
Message-ID: <CAGam+C6yEFw-E+oT7tktDVY8L9KtJZ0aHrQ_y0VLyEMnZDmfSw@mail.gmail.com>

Hi --

It's my first post on this list; as a relatively new user with little
knowledge of R internals, I am a bit intimidated by the depth of some
of the discussions here, so please spare me if I say something
incredibly silly.

I feel that someone at this point should mention Matthew Dowle's
excellent data.table package
(http://cran.r-project.org/web/packages/data.table/index.html) which
seems to me to address many of the inefficiencies of data.frame.
data.tables have no row names; and operations that only need data from
one or two columns are (I believe) just as quick whether the total
number of columns is 5 or 1000. This results in very quick operations
(and, often, elegant code as well).

Regards
Timothee

On Mon, Jul 4, 2011 at 6:19 AM, ivo welch <ivo.welch at gmail.com> wrote:
> thank you, simon. ?this was very interesting indeed. ?I also now
> understand how far out of my depth I am here.
>
> fortunately, as an end user, obviously, *I* now know how to avoid the
> problem. ?I particularly like the as.list() transformation and back to
> as.data.frame() to speed things up without loss of (much)
> functionality.
>
>
> more broadly, I view the avoidance of individual access through the
> use of apply and vector operations as a mixed "IQ test" and "knowledge
> test" (which I often fail). ?However, even for the most clever, there
> are also situations where the KISS programming principle makes
> explicit loops still preferable. ?Personally, I would have preferred
> it if R had, in its standard "statistical data set" data structure,
> foregone the row names feature in exchange for retaining fast direct
> access. ?R could have reserved its current implementation "with row
> names but slow access" for a less common (possibly pseudo-inheriting)
> data structure.
>
>
> If end users commonly do iterations over a data frame, which I would
> guess to be the case, then the impression of R by (novice) end users
> could be greatly enhanced if the extreme penalties could be eliminated
> or at least flagged. ?For example, I wonder if modest special internal
> code could store data frames internally and transparently as lists of
> vectors UNTIL a row name is assigned to. ?Easier and uglier, a simple
> but specific warning message could be issued with a suggestion if
> there is an individual read/write into a data frame ("Warning: data
> frames are much slower than lists of vectors for individual element
> access").
>
>
> I would also suggest changing the "Introduction to R" 6.3 ?from "A
> data frame may for many purposes be regarded as a matrix with columns
> possibly of differing modes and attributes. It may be displayed in
> matrix form, and its rows and columns extracted using matrix indexing
> conventions." to "A data frame may for many purposes be regarded as a
> matrix with columns possibly of differing modes and attributes. It may
> be displayed in matrix form, and its rows and columns extracted using
> matrix indexing conventions. ?However, data frames can be much slower
> than matrices or even lists of vectors (which, like data frames, can
> contain different types of columns) when individual elements need to
> be accessed." ?Reading about it immediately upon introduction could
> flag the problem in a more visible manner.
>
>
> regards,
>
> /iaw
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From simon.urbanek at r-project.org  Mon Jul  4 18:41:44 2011
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Mon, 4 Jul 2011 12:41:44 -0400
Subject: [Rd] speeding up perception
In-Reply-To: <CAGam+C6yEFw-E+oT7tktDVY8L9KtJZ0aHrQ_y0VLyEMnZDmfSw@mail.gmail.com>
References: <CAPr7RtU44GO-7_m02-ek+ps3YcR_qr3m7vV8BSvhAYJ=F_bnbA@mail.gmail.com>
	<CAPr7RtVx1ao9bubQiCrf9LjgpdZdB24vSnjZvGxW4a8Sy-pKag@mail.gmail.com>
	<CAGam+C6yEFw-E+oT7tktDVY8L9KtJZ0aHrQ_y0VLyEMnZDmfSw@mail.gmail.com>
Message-ID: <555C7B55-67D9-4DC6-90CB-BE8B0F620BFB@r-project.org>

Timoth?e,

On Jul 4, 2011, at 2:47 AM, Timoth?e Carayol wrote:

> Hi --
> 
> It's my first post on this list; as a relatively new user with little
> knowledge of R internals, I am a bit intimidated by the depth of some
> of the discussions here, so please spare me if I say something
> incredibly silly.
> 
> I feel that someone at this point should mention Matthew Dowle's
> excellent data.table package
> (http://cran.r-project.org/web/packages/data.table/index.html) which
> seems to me to address many of the inefficiencies of data.frame.
> data.tables have no row names; and operations that only need data from
> one or two columns are (I believe) just as quick whether the total
> number of columns is 5 or 1000. This results in very quick operations
> (and, often, elegant code as well).
> 

I agree that data.table is a very good alternative (for other reasons) that should be promoted more. The only slight snag is that it doesn't help with the issue at hand since it simply does a pass-though for subassignments to data frame's methods and thus suffers from the same problems (in fact there is a rather stark asymmetry in how it handles subsetting vs subassignment - which is a bit surprising [if I read the code correctly you can't use the same indexing in both]). In fact I would propose that it should not do that but handle the simple cases itself more efficiently without unneeded copies. That would make it indeed a very interesting alternative.

Cheers,
Simon


> 
> On Mon, Jul 4, 2011 at 6:19 AM, ivo welch <ivo.welch at gmail.com> wrote:
>> thank you, simon.  this was very interesting indeed.  I also now
>> understand how far out of my depth I am here.
>> 
>> fortunately, as an end user, obviously, *I* now know how to avoid the
>> problem.  I particularly like the as.list() transformation and back to
>> as.data.frame() to speed things up without loss of (much)
>> functionality.
>> 
>> 
>> more broadly, I view the avoidance of individual access through the
>> use of apply and vector operations as a mixed "IQ test" and "knowledge
>> test" (which I often fail).  However, even for the most clever, there
>> are also situations where the KISS programming principle makes
>> explicit loops still preferable.  Personally, I would have preferred
>> it if R had, in its standard "statistical data set" data structure,
>> foregone the row names feature in exchange for retaining fast direct
>> access.  R could have reserved its current implementation "with row
>> names but slow access" for a less common (possibly pseudo-inheriting)
>> data structure.
>> 
>> 
>> If end users commonly do iterations over a data frame, which I would
>> guess to be the case, then the impression of R by (novice) end users
>> could be greatly enhanced if the extreme penalties could be eliminated
>> or at least flagged.  For example, I wonder if modest special internal
>> code could store data frames internally and transparently as lists of
>> vectors UNTIL a row name is assigned to.  Easier and uglier, a simple
>> but specific warning message could be issued with a suggestion if
>> there is an individual read/write into a data frame ("Warning: data
>> frames are much slower than lists of vectors for individual element
>> access").
>> 
>> 
>> I would also suggest changing the "Introduction to R" 6.3  from "A
>> data frame may for many purposes be regarded as a matrix with columns
>> possibly of differing modes and attributes. It may be displayed in
>> matrix form, and its rows and columns extracted using matrix indexing
>> conventions." to "A data frame may for many purposes be regarded as a
>> matrix with columns possibly of differing modes and attributes. It may
>> be displayed in matrix form, and its rows and columns extracted using
>> matrix indexing conventions.  However, data frames can be much slower
>> than matrices or even lists of vectors (which, like data frames, can
>> contain different types of columns) when individual elements need to
>> be accessed."  Reading about it immediately upon introduction could
>> flag the problem in a more visible manner.
>> 
>> 
>> regards,
>> 
>> /iaw
>> 
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>> 
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 
> 


From timhesterberg at gmail.com  Mon Jul  4 19:38:31 2011
From: timhesterberg at gmail.com (Tim Hesterberg)
Date: Mon, 04 Jul 2011 10:38:31 -0700
Subject: [Rd] speeding up perception
In-Reply-To: <CAPr7RtVx1ao9bubQiCrf9LjgpdZdB24vSnjZvGxW4a8Sy-pKag@mail.gmail.com>
	(message from ivo welch on Sun, 3 Jul 2011 22:19:53 -0700)
References: <CAPr7RtVx1ao9bubQiCrf9LjgpdZdB24vSnjZvGxW4a8Sy-pKag@mail.gmail.com>
Message-ID: <yajf62ni3q08.fsf@gmail.com>

I've written a "dataframe" package that replaces existing methods for
data frame creation and subscripting with versions that use less
memory.  For example, as.data.frame(a vector) makes 4 copies of the
data in R 2.9.2, and 1 copy with the package.  There is a small speed
gain.

I and others have been using it at Google for some years, and it is time
to either put it on CRAN, or move it into R.

R core folks - would you prefer that this be released to CRAN, or
would you like to consider merging it directly into R?

I took existing functions, and did some hacks to reduce the number of
times R copies objects.  Some of it is ugly.  This could be done more
efficiently, and with cleaner code, with some changes or hooks in R
internal code, but I'm not prepared to do that.

I often use lists instead of data frames.  In another package I have a
'subscriptRows' function that subscripts a list as if it were
a data frame.  I could merge that into the dataframe package.

Memory use - number of copies made
#                               R 2.9.2                 library(dataframe)
#       as.data.frame(y)        4                       1
#       data.frame(y)           8                       3
#       data.frame(y, z)        8                       3
#       as.data.frame(l)        10                      3
#       data.frame(l)           15                      5
#       d$z <- z                3,2                     1,1
#       d[["z"]] <- z           4,3                     2,1
#       d[, "z"] <- z           6,4,2                   2,2,1
#       d["z"] <- z             6,5,2                   2,2,1
#       d["z"] <- list(z=z)     6,3,2                   2,2,1
#       d["z"] <- Z #list(z=z)  6,2,2                   2,1,1
#       a <- d["y"]             2                       1
#       a <- d[, "y", drop=F]   2                       1
# y and z are vectors, Z and l are lists, and d a data frame.
# Where two numbers are given, they refer to:
#   (copies of the old data frame),
#   (copies of the new column)
# A third number refers to numbers of
#   (copies made of an integer vector of row names)

#                      -------  seconds (multiple repetitions) -------
#                      creation/column subscripting     row subscripting
# R 2.9.2            : 34.2 43.9 43.3                   10.6 13.0
# library(dataframe) : 22.5 21.8 21.8                    9.7  9.5  9.5

I reported one of the simpler hacks to this list earlier, and it
was included in some version of R after 2.9.2, so the current version
of R isn't as bad as 2.9.2.


From Mark.Bravington at csiro.au  Tue Jul  5 02:16:58 2011
From: Mark.Bravington at csiro.au (Mark.Bravington at csiro.au)
Date: Tue, 5 Jul 2011 10:16:58 +1000
Subject: [Rd] Recent and upcoming changes to R-devel
In-Reply-To: <alpine.OSX.1.00.1107041124460.59432@tystie.local>
References: <alpine.OSX.1.00.1107041124460.59432@tystie.local>
Message-ID: <3D59DF35E7CF3E42BA70C23BD5CBE6A85F4C199E22@exvic-mbx04.nexus.csiro.au>

I may have misunderstood, but: 

Please could we have an optional installation that does not *not* byte-compile base and recommended?

Reason: it's not possible to debug byte-compiled code-- at least not with the 'debug' package, which is quite widely used. I quite often end up using 'mtrace' on functions in base/recommended packages to figure out what they are doing. And sometimes I (and others) experiment with changing functions in base/recommended to improve functionality. That seems to be harder with BC versions, and might even be impossible, as best I can tell from hints in the documentation of 'compile').

Personally, if I had to choose only one, I'd rather live with the speed penalty from not byte-compiling. But of course, if both are available, I could install both.

Thanks

Mark

-- 
Mark Bravington
CSIRO Mathematical & Information Sciences
Marine Laboratory
Castray Esplanade
Hobart 7001
TAS

ph (+61) 3 6232 5118
fax (+61) 3 6232 5012
mob (+61) 438 315 623

Prof Brian Ripley wrote:
> There was an R-core meeting the week before last, and various planned
> changes will appear in R-devel over the next few weeks. 
> 
> These are changes planned for R 2.14.0 scheduled for Oct 31.  As we
> are sick of people referring to R-devel as '2.14' or '2.14.0', that
> version number will not be used until we reach 2.14.0 alpha.  You
> will be able to have a package depend on an svn version number when
> referring to R-devel rather than using R (>= 2.14.0).    
> 
> All packages are installed with lazy-loading (there were 72 CRAN
> packages and 8 BioC packages which opted out).  This means that the
> code is always parsed at install time which inter alia simplifies the
> descriptions.  R 2.13.1 RC warns on installation about packages which
> ask not to be lazy-loaded, and R-devel ignores such requests (with a
> warning).     
> 
> In the near future all packages will have a name space.  If the
> sources do not contain one, a default NAMESPACE file will be added. 
> This again will simplify the descriptions and also a lot of internal
> code.  Maintainers of packages without name spaces (currently 42% of 
> CRAN) are encouraged to add one themselves.
> 
> R-devel is installed with the base and recommended packages
> byte-compiled (the equivalent of 'make bytecode' in R 2.13.x, but
> done less inefficiently).  There is a new option R CMD INSTALL
> --byte-compile to byte-compile contributed packages, but that remains
> optional.    
> Byte-compilation is quite expensive (so you definitely want to do it
> at install time, which requires lazy-loading), and relatively few
> packages benefit appreciably from byte-compilation.  A larger number
> of packages benefit from byte-compilation of R itself: for example
> AER runs its checks 10% faster.  The byte-compiler technology is
> thanks to Luke Tierney.     
> 
> There is support for figures in Rd files: currently with a first-pass
> implementation (thanks to Duncan Murdoch). 

From johannes_graumann at web.de  Tue Jul  5 08:00:08 2011
From: johannes_graumann at web.de (Johannes Graumann)
Date: Tue, 5 Jul 2011 09:00:08 +0300
Subject: [Rd] Circumventing code/documentation mismatches ('R CMD check')
Message-ID: <iuu953$82q$1@dough.gmane.org>

Hello,

As prompted by B. Ripley (see below), I am transfering this over from R-User 
...

For a package I am writing a function that looks like

test <- function(Argument1=NA){
	# Prerequisite testing
	if(!(is.na(Argument1))){
		if(!(is.character(Argument1))){
			stop("Wrong class.")
		}
	}
	# Function Body
	cat("Hello World\n")
}

Documentation of this is straight forward:

...
\usage{test(Argument1=NA)}
...

However writing the function could be made more concise like so:

test2 <- function(Argument1=NA_character_){
	# Prerequisite testing
	if(!(is.character(Argument1))){
		stop("Wrong class.")
	}
	# Function Body
	cat("Hello World\n")
}

To prevent confusion I do not want to use 'NA_character_' in the user-
exposed documentation and using 

...
\usage{test2(Argument1=NA)}
...

leads to a warning reagrding a code/documentation mismatch.

Is there any way to prevent that?

Sincerely, Joh

Prof Brian Ripley wrote:

> On Mon, 4 Jul 2011, Johannes Graumann wrote:
> 
>> Hello,
>>
>> I'm writing a package am running 'R CMD check' on it.
>>
>> Is there any way to make 'R CMD check' not warn about a missmatch between
>> 'NA_character_' (in the function definition) and 'NA' (in the
>> documentation)?
> 
> Be consistent ....  Why do you want incorrect documentation of your
> package?  (It is not clear of the circumstances here: normally 1 vs 1L
> and similar are not reported if they are the only errors.)
> 
> And please do note the posting guide
> 
> - this is not really the correct list
> - you were asked to give an actual example with output.


From pdalgd at gmail.com  Tue Jul  5 08:59:26 2011
From: pdalgd at gmail.com (peter dalgaard)
Date: Tue, 5 Jul 2011 08:59:26 +0200
Subject: [Rd] Circumventing code/documentation mismatches ('R CMD check')
In-Reply-To: <iuu953$82q$1@dough.gmane.org>
References: <iuu953$82q$1@dough.gmane.org>
Message-ID: <4FECF80B-BBA9-4707-9B4C-8ABA64CD60A7@gmail.com>


On Jul 5, 2011, at 08:00 , Johannes Graumann wrote:

> Hello,
> 
> As prompted by B. Ripley (see below), I am transfering this over from R-User 
> ...
> 
> For a package I am writing a function that looks like
> 
> test <- function(Argument1=NA){
> 	# Prerequisite testing
> 	if(!(is.na(Argument1))){
> 		if(!(is.character(Argument1))){
> 			stop("Wrong class.")
> 		}
> 	}
> 	# Function Body
> 	cat("Hello World\n")
> }
> 
> Documentation of this is straight forward:
> 
> ...
> \usage{test(Argument1=NA)}
> ...
> 
> However writing the function could be made more concise like so:
> 
> test2 <- function(Argument1=NA_character_){
> 	# Prerequisite testing
> 	if(!(is.character(Argument1))){
> 		stop("Wrong class.")
> 	}
> 	# Function Body
> 	cat("Hello World\n")
> }
> 
> To prevent confusion I do not want to use 'NA_character_' in the user-
> exposed documentation and using 
> 
> ...
> \usage{test2(Argument1=NA)}
> ...
> 
> leads to a warning reagrding a code/documentation mismatch.
> 
> Is there any way to prevent that?

You don't want to do that... 

That strategy breaks if someone passes the documented "default" explicitly, which certainly _causes_ confusion rather than prevent it. I.e.

test2(NA) # fails

test3 <- function(a=NA) test2(a) # 3rd party code might build on your function
test3() # fails

If your function only accept character values, even if NA, then that is what should be documented. In the end, you'll find that an explicit is.na() is the right thing to do. 



-- 
Peter Dalgaard
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From mdowle at mdowle.plus.com  Tue Jul  5 09:32:45 2011
From: mdowle at mdowle.plus.com (Matthew Dowle)
Date: Tue, 05 Jul 2011 08:32:45 +0100
Subject: [Rd] [datatable-help]  speeding up perception
In-Reply-To: <555C7B55-67D9-4DC6-90CB-BE8B0F620BFB@r-project.org>
References: <CAPr7RtU44GO-7_m02-ek+ps3YcR_qr3m7vV8BSvhAYJ=F_bnbA@mail.gmail.com>
	<CAPr7RtVx1ao9bubQiCrf9LjgpdZdB24vSnjZvGxW4a8Sy-pKag@mail.gmail.com>
	<CAGam+C6yEFw-E+oT7tktDVY8L9KtJZ0aHrQ_y0VLyEMnZDmfSw@mail.gmail.com>
	<555C7B55-67D9-4DC6-90CB-BE8B0F620BFB@r-project.org>
Message-ID: <1309851165.4231.40.camel@netbook>


Simon,

Thanks for the great suggestion. I've written a skeleton assignment
function for data.table which incurs no copies, which works for this
case. For completeness, if I understand correctly, this is for : 
  i) convenience of new users who don't know how to vectorize yet
  ii) more complex examples which can't be vectorized.

Before:

> system.time(for (r in 1:R) DT[r,20] <- 1.0)
   user  system elapsed 
 12.792   0.488  13.340 

After :

> system.time(for (r in 1:R) DT[r,20] <- 1.0)
   user  system elapsed 
  2.908   0.020   2.935

Where this can be reduced further as follows :

> system.time(for (r in 1:R) `[<-.data.table`(DT,r,2,1.0))
   user  system elapsed 
  0.132   0.000   0.131 
> 

Still working on it. When it doesn't break other data.table tests, I'll
commit to R-Forge ...

Matthew


On Mon, 2011-07-04 at 12:41 -0400, Simon Urbanek wrote:
> Timoth?e,
> 
> On Jul 4, 2011, at 2:47 AM, Timoth?e Carayol wrote:
> 
> > Hi --
> > 
> > It's my first post on this list; as a relatively new user with little
> > knowledge of R internals, I am a bit intimidated by the depth of some
> > of the discussions here, so please spare me if I say something
> > incredibly silly.
> > 
> > I feel that someone at this point should mention Matthew Dowle's
> > excellent data.table package
> > (http://cran.r-project.org/web/packages/data.table/index.html) which
> > seems to me to address many of the inefficiencies of data.frame.
> > data.tables have no row names; and operations that only need data from
> > one or two columns are (I believe) just as quick whether the total
> > number of columns is 5 or 1000. This results in very quick operations
> > (and, often, elegant code as well).
> > 
> 
> I agree that data.table is a very good alternative (for other reasons) that should be promoted more. The only slight snag is that it doesn't help with the issue at hand since it simply does a pass-though for subassignments to data frame's methods and thus suffers from the same problems (in fact there is a rather stark asymmetry in how it handles subsetting vs subassignment - which is a bit surprising [if I read the code correctly you can't use the same indexing in both]). In fact I would propose that it should not do that but handle the simple cases itself more efficiently without unneeded copies. That would make it indeed a very interesting alternative.
> 
> Cheers,
> Simon
> 
> 
> > 
> > On Mon, Jul 4, 2011 at 6:19 AM, ivo welch <ivo.welch at gmail.com> wrote:
> >> thank you, simon.  this was very interesting indeed.  I also now
> >> understand how far out of my depth I am here.
> >> 
> >> fortunately, as an end user, obviously, *I* now know how to avoid the
> >> problem.  I particularly like the as.list() transformation and back to
> >> as.data.frame() to speed things up without loss of (much)
> >> functionality.
> >> 
> >> 
> >> more broadly, I view the avoidance of individual access through the
> >> use of apply and vector operations as a mixed "IQ test" and "knowledge
> >> test" (which I often fail).  However, even for the most clever, there
> >> are also situations where the KISS programming principle makes
> >> explicit loops still preferable.  Personally, I would have preferred
> >> it if R had, in its standard "statistical data set" data structure,
> >> foregone the row names feature in exchange for retaining fast direct
> >> access.  R could have reserved its current implementation "with row
> >> names but slow access" for a less common (possibly pseudo-inheriting)
> >> data structure.
> >> 
> >> 
> >> If end users commonly do iterations over a data frame, which I would
> >> guess to be the case, then the impression of R by (novice) end users
> >> could be greatly enhanced if the extreme penalties could be eliminated
> >> or at least flagged.  For example, I wonder if modest special internal
> >> code could store data frames internally and transparently as lists of
> >> vectors UNTIL a row name is assigned to.  Easier and uglier, a simple
> >> but specific warning message could be issued with a suggestion if
> >> there is an individual read/write into a data frame ("Warning: data
> >> frames are much slower than lists of vectors for individual element
> >> access").
> >> 
> >> 
> >> I would also suggest changing the "Introduction to R" 6.3  from "A
> >> data frame may for many purposes be regarded as a matrix with columns
> >> possibly of differing modes and attributes. It may be displayed in
> >> matrix form, and its rows and columns extracted using matrix indexing
> >> conventions." to "A data frame may for many purposes be regarded as a
> >> matrix with columns possibly of differing modes and attributes. It may
> >> be displayed in matrix form, and its rows and columns extracted using
> >> matrix indexing conventions.  However, data frames can be much slower
> >> than matrices or even lists of vectors (which, like data frames, can
> >> contain different types of columns) when individual elements need to
> >> be accessed."  Reading about it immediately upon introduction could
> >> flag the problem in a more visible manner.
> >> 
> >> 
> >> regards,
> >> 
> >> /iaw
> >> 
> >> ______________________________________________
> >> R-devel at r-project.org mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-devel
> >> 
> > 
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
> > 
> > 
> 
> _______________________________________________
> datatable-help mailing list
> datatable-help at lists.r-forge.r-project.org
> https://lists.r-forge.r-project.org/cgi-bin/mailman/listinfo/datatable-help


From tobias.verbeke at openanalytics.eu  Tue Jul  5 12:52:54 2011
From: tobias.verbeke at openanalytics.eu (Tobias Verbeke)
Date: Tue, 05 Jul 2011 12:52:54 +0200
Subject: [Rd] Recent and upcoming changes to R-devel
In-Reply-To: <3D59DF35E7CF3E42BA70C23BD5CBE6A85F4C199E22@exvic-mbx04.nexus.csiro.au>
References: <alpine.OSX.1.00.1107041124460.59432@tystie.local>
	<3D59DF35E7CF3E42BA70C23BD5CBE6A85F4C199E22@exvic-mbx04.nexus.csiro.au>
Message-ID: <4E12ED06.5040603@openanalytics.eu>

L.S.

On 07/05/2011 02:16 AM, Mark.Bravington at csiro.au wrote:
> I may have misunderstood, but:
>
> Please could we have an optional installation that does not*not*  byte-compile base and recommended?
>
> Reason: it's not possible to debug byte-compiled code-- at least not with the 'debug' package, which is quite widely used. I quite often end up using 'mtrace' on functions in base/recommended packages to figure out what they are doing. And sometimes I (and others) experiment with changing functions in base/recommended to improve functionality. That seems to be harder with BC versions, and might even be impossible, as best I can tell from hints in the documentation of 'compile').
>
> Personally, if I had to choose only one, I'd rather live with the speed penalty from not byte-compiling. But of course, if both are available, I could install both.

I completely second this request. All speed improvements and the byte
compiler in particular are leaps forward and I am very grateful and
admiring towards the people that make this happen.

That being said, 'moving away' from the sources (with the lazy loading
files and byte-compilation) may be a step back for R package developers
that (during development and maybe on separate development installations 
[as opposed to production installations of R]) require
the sources of all packages to be efficient in their work.

As many of you know there is an open source Eclipse/StatET visual
debugger ready and for that application as well (similar to Mark's
request) presence of non-compiled code is highly desirable.

For the particular purpose of debugging R packages, I would even plead
to go beyond the current options and support the addition of an
R package install option that allows to include the sources (e.g. in
a standard folder Rsrc/) in installed packages.

I am fully aware that one can always fetch the source tarballs from
CRAN for that purpose, but it would be much more easy if a simple
installation option could put the R sources of a package in a separate
folder [or archive inside an existing folder] such that R development
tools (such as the Eclipse/StatET IDE) can offer inspection of sources
or display them (e.g. during debugging) out of the box.

If one has the srcref, one can always load the absolutely correct source 
code this way, even if one doesn't know the parent function with
the source attribute.

Any comments?

Best,
Tobias

P.S. One could even consider a post-install option e.g. to add 'real'
R sources (and source references) to Windows packages (which are by
definition already 'installed' and for which such information is not
by default included in the CRAN binaries of these packages).

>> > Prof Brian Ripley wrote:
>> >  There was an R-core meeting the week before last, and various planned
>> >  changes will appear in R-devel over the next few weeks.
>> >
>> >  These are changes planned for R 2.14.0 scheduled for Oct 31.  As we
>> >  are sick of people referring to R-devel as '2.14' or '2.14.0', that
>> >  version number will not be used until we reach 2.14.0 alpha.  You
>> >  will be able to have a package depend on an svn version number when
>> >  referring to R-devel rather than using R (>= 2.14.0).
>> >
>> >  All packages are installed with lazy-loading (there were 72 CRAN
>> >  packages and 8 BioC packages which opted out).  This means that the
>> >  code is always parsed at install time which inter alia simplifies the
>> >  descriptions.  R 2.13.1 RC warns on installation about packages which
>> >  ask not to be lazy-loaded, and R-devel ignores such requests (with a
>> >  warning).
>> >
>> >  In the near future all packages will have a name space.  If the
>> >  sources do not contain one, a default NAMESPACE file will be added.
>> >  This again will simplify the descriptions and also a lot of internal
>> >  code.  Maintainers of packages without name spaces (currently 42% of
>> >  CRAN) are encouraged to add one themselves.
>> >
>> >  R-devel is installed with the base and recommended packages
>> >  byte-compiled (the equivalent of 'make bytecode' in R 2.13.x, but
>> >  done less inefficiently).  There is a new option R CMD INSTALL
>> >  --byte-compile to byte-compile contributed packages, but that remains
>> >  optional.
>> >  Byte-compilation is quite expensive (so you definitely want to do it
>> >  at install time, which requires lazy-loading), and relatively few
>> >  packages benefit appreciably from byte-compilation.  A larger number
>> >  of packages benefit from byte-compilation of R itself: for example
>> >  AER runs its checks 10% faster.  The byte-compiler technology is
>> >  thanks to Luke Tierney.
>> >
>> >  There is support for figures in Rd files: currently with a first-pass
>> >  implementation (thanks to Duncan Murdoch).
> ______________________________________________
> R-devel at r-project.org  mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From murdoch.duncan at gmail.com  Tue Jul  5 15:25:45 2011
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Tue, 05 Jul 2011 09:25:45 -0400
Subject: [Rd] Recent and upcoming changes to R-devel
In-Reply-To: <4E12ED06.5040603@openanalytics.eu>
References: <alpine.OSX.1.00.1107041124460.59432@tystie.local>	<3D59DF35E7CF3E42BA70C23BD5CBE6A85F4C199E22@exvic-mbx04.nexus.csiro.au>
	<4E12ED06.5040603@openanalytics.eu>
Message-ID: <4E1310D9.6050901@gmail.com>

On 05/07/2011 6:52 AM, Tobias Verbeke wrote:
> L.S.
>
> On 07/05/2011 02:16 AM, Mark.Bravington at csiro.au wrote:
> >  I may have misunderstood, but:
> >
> >  Please could we have an optional installation that does not*not*  byte-compile base and recommended?
> >
> >  Reason: it's not possible to debug byte-compiled code-- at least not with the 'debug' package, which is quite widely used. I quite often end up using 'mtrace' on functions in base/recommended packages to figure out what they are doing. And sometimes I (and others) experiment with changing functions in base/recommended to improve functionality. That seems to be harder with BC versions, and might even be impossible, as best I can tell from hints in the documentation of 'compile').
> >
> >  Personally, if I had to choose only one, I'd rather live with the speed penalty from not byte-compiling. But of course, if both are available, I could install both.
>
> I completely second this request. All speed improvements and the byte
> compiler in particular are leaps forward and I am very grateful and
> admiring towards the people that make this happen.
>
> That being said, 'moving away' from the sources (with the lazy loading
> files and byte-compilation) may be a step back for R package developers
> that (during development and maybe on separate development installations
> [as opposed to production installations of R]) require
> the sources of all packages to be efficient in their work.
>
> As many of you know there is an open source Eclipse/StatET visual
> debugger ready and for that application as well (similar to Mark's
> request) presence of non-compiled code is highly desirable.
>
> For the particular purpose of debugging R packages, I would even plead
> to go beyond the current options and support the addition of an
> R package install option that allows to include the sources (e.g. in
> a standard folder Rsrc/) in installed packages.
>
> I am fully aware that one can always fetch the source tarballs from
> CRAN for that purpose, but it would be much more easy if a simple
> installation option could put the R sources of a package in a separate
> folder [or archive inside an existing folder] such that R development
> tools (such as the Eclipse/StatET IDE) can offer inspection of sources
> or display them (e.g. during debugging) out of the box.
>
> If one has the srcref, one can always load the absolutely correct source
> code this way, even if one doesn't know the parent function with
> the source attribute.
>
> Any comments?

I think these requests have already been met.  If you modify the body of 
a closure (as trace() does), then the byte compiled version is 
discarded, and you go back to the regular interpreted code.  If you 
install packages with the R_KEEP_PKG_SOURCE=yes environment variable 
set, the you keep all source for all functions.  (It's attached to the 
function itself, not as a file that may be out of date.)  It's possible 
that byte compiling turns off R_KEEP_PKG_SOURCE, but that is something 
that is either easily fixed, or avoided by re-installing without byte 
compiling.

Duncan Murdoch

> Best,
> Tobias
>
> P.S. One could even consider a post-install option e.g. to add 'real'
> R sources (and source references) to Windows packages (which are by
> definition already 'installed' and for which such information is not
> by default included in the CRAN binaries of these packages).
>
> >>  >  Prof Brian Ripley wrote:
> >>  >   There was an R-core meeting the week before last, and various planned
> >>  >   changes will appear in R-devel over the next few weeks.
> >>  >
> >>  >   These are changes planned for R 2.14.0 scheduled for Oct 31.  As we
> >>  >   are sick of people referring to R-devel as '2.14' or '2.14.0', that
> >>  >   version number will not be used until we reach 2.14.0 alpha.  You
> >>  >   will be able to have a package depend on an svn version number when
> >>  >   referring to R-devel rather than using R (>= 2.14.0).
> >>  >
> >>  >   All packages are installed with lazy-loading (there were 72 CRAN
> >>  >   packages and 8 BioC packages which opted out).  This means that the
> >>  >   code is always parsed at install time which inter alia simplifies the
> >>  >   descriptions.  R 2.13.1 RC warns on installation about packages which
> >>  >   ask not to be lazy-loaded, and R-devel ignores such requests (with a
> >>  >   warning).
> >>  >
> >>  >   In the near future all packages will have a name space.  If the
> >>  >   sources do not contain one, a default NAMESPACE file will be added.
> >>  >   This again will simplify the descriptions and also a lot of internal
> >>  >   code.  Maintainers of packages without name spaces (currently 42% of
> >>  >   CRAN) are encouraged to add one themselves.
> >>  >
> >>  >   R-devel is installed with the base and recommended packages
> >>  >   byte-compiled (the equivalent of 'make bytecode' in R 2.13.x, but
> >>  >   done less inefficiently).  There is a new option R CMD INSTALL
> >>  >   --byte-compile to byte-compile contributed packages, but that remains
> >>  >   optional.
> >>  >   Byte-compilation is quite expensive (so you definitely want to do it
> >>  >   at install time, which requires lazy-loading), and relatively few
> >>  >   packages benefit appreciably from byte-compilation.  A larger number
> >>  >   of packages benefit from byte-compilation of R itself: for example
> >>  >   AER runs its checks 10% faster.  The byte-compiler technology is
> >>  >   thanks to Luke Tierney.
> >>  >
> >>  >   There is support for figures in Rd files: currently with a first-pass
> >>  >   implementation (thanks to Duncan Murdoch).
> >  ______________________________________________
> >  R-devel at r-project.org  mailing list
> >  https://stat.ethz.ch/mailman/listinfo/r-devel
> >
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From tobias.verbeke at openanalytics.eu  Tue Jul  5 16:17:25 2011
From: tobias.verbeke at openanalytics.eu (Tobias Verbeke)
Date: Tue, 05 Jul 2011 16:17:25 +0200
Subject: [Rd] Recent and upcoming changes to R-devel
In-Reply-To: <4E1310D9.6050901@gmail.com>
References: <alpine.OSX.1.00.1107041124460.59432@tystie.local>	<3D59DF35E7CF3E42BA70C23BD5CBE6A85F4C199E22@exvic-mbx04.nexus.csiro.au>
	<4E12ED06.5040603@openanalytics.eu> <4E1310D9.6050901@gmail.com>
Message-ID: <4E131CF5.5000206@openanalytics.eu>

Dear Duncan,

On 07/05/2011 03:25 PM, Duncan Murdoch wrote:
> On 05/07/2011 6:52 AM, Tobias Verbeke wrote:
>> L.S.
>>
>> On 07/05/2011 02:16 AM, Mark.Bravington at csiro.au wrote:
>> > I may have misunderstood, but:
>> >
>> > Please could we have an optional installation that does not*not*
>> byte-compile base and recommended?
>> >
>> > Reason: it's not possible to debug byte-compiled code-- at least not
>> with the 'debug' package, which is quite widely used. I quite often
>> end up using 'mtrace' on functions in base/recommended packages to
>> figure out what they are doing. And sometimes I (and others)
>> experiment with changing functions in base/recommended to improve
>> functionality. That seems to be harder with BC versions, and might
>> even be impossible, as best I can tell from hints in the documentation
>> of 'compile').
>> >
>> > Personally, if I had to choose only one, I'd rather live with the
>> speed penalty from not byte-compiling. But of course, if both are
>> available, I could install both.
>>
>> I completely second this request. All speed improvements and the byte
>> compiler in particular are leaps forward and I am very grateful and
>> admiring towards the people that make this happen.
>>
>> That being said, 'moving away' from the sources (with the lazy loading
>> files and byte-compilation) may be a step back for R package developers
>> that (during development and maybe on separate development installations
>> [as opposed to production installations of R]) require
>> the sources of all packages to be efficient in their work.
>>
>> As many of you know there is an open source Eclipse/StatET visual
>> debugger ready and for that application as well (similar to Mark's
>> request) presence of non-compiled code is highly desirable.
>>
>> For the particular purpose of debugging R packages, I would even plead
>> to go beyond the current options and support the addition of an
>> R package install option that allows to include the sources (e.g. in
>> a standard folder Rsrc/) in installed packages.
>>
>> I am fully aware that one can always fetch the source tarballs from
>> CRAN for that purpose, but it would be much more easy if a simple
>> installation option could put the R sources of a package in a separate
>> folder [or archive inside an existing folder] such that R development
>> tools (such as the Eclipse/StatET IDE) can offer inspection of sources
>> or display them (e.g. during debugging) out of the box.
>>
>> If one has the srcref, one can always load the absolutely correct source
>> code this way, even if one doesn't know the parent function with
>> the source attribute.
>>
>> Any comments?
>
> I think these requests have already been met. If you modify the body of
> a closure (as trace() does), then the byte compiled version is
> discarded, and you go back to the regular interpreted code. If you
> install packages with the R_KEEP_PKG_SOURCE=yes environment variable
> set, the you keep all source for all functions. (It's attached to the
> function itself, not as a file that may be out of date.) It's possible
> that byte compiling turns off R_KEEP_PKG_SOURCE, but that is something
> that is either easily fixed, or avoided by re-installing without byte
> compiling.

Many thanks for your reaction. Is the R_KEEP_PKG_SOURCE=yes environment
variable also supported during R installation ?

I hope I'm not overlooking anything, but when compiling

ftp://ftp.stat.math.ethz.ch/Software/R/R-devel.tar.gz

a few minutes ago I encountered the following issue:

[...]

building package 'tools'
mkdir -p -- ../../../library/tools
make[4]: Entering directory `/home/tobias/rAdmin/R-devel/src/library/tools'
mkdir -p -- ../../../library/tools/R
mkdir -p -- ../../../library/tools/po
make[4]: Leaving directory `/home/tobias/rAdmin/R-devel/src/library/tools'
make[4]: Entering directory `/home/tobias/rAdmin/R-devel/src/library/tools'
make[5]: Entering directory 
`/home/tobias/rAdmin/R-devel/src/library/tools/src'
making text.d from text.c
making init.d from init.c
making Rmd5.d from Rmd5.c
making md5.d from md5.c
gcc -std=gnu99 -I../../../../include  -I/usr/local/include 
-fvisibility=hidden -fpic  -g -O2 -c text.c -o text.o
gcc -std=gnu99 -I../../../../include  -I/usr/local/include 
-fvisibility=hidden -fpic  -g -O2 -c init.c -o init.o
gcc -std=gnu99 -I../../../../include  -I/usr/local/include 
-fvisibility=hidden -fpic  -g -O2 -c Rmd5.c -o Rmd5.o
gcc -std=gnu99 -I../../../../include  -I/usr/local/include 
-fvisibility=hidden -fpic  -g -O2 -c md5.c -o md5.o
gcc -std=gnu99 -shared -L/usr/local/lib64 -o tools.so text.o init.o 
Rmd5.o md5.o -L../../../../lib -lR
make[6]: Entering directory 
`/home/tobias/rAdmin/R-devel/src/library/tools/src'
make[6]: `Makedeps' is up to date.
make[6]: Leaving directory 
`/home/tobias/rAdmin/R-devel/src/library/tools/src'
make[6]: Entering directory 
`/home/tobias/rAdmin/R-devel/src/library/tools/src'
mkdir -p -- ../../../../library/tools/libs
make[6]: Leaving directory 
`/home/tobias/rAdmin/R-devel/src/library/tools/src'
make[5]: Leaving directory 
`/home/tobias/rAdmin/R-devel/src/library/tools/src'
make[4]: Leaving directory `/home/tobias/rAdmin/R-devel/src/library/tools'
Error in parse(n = -1, file = file) :
   function is too long to keep source (at line 2967)
Error: unable to load R code in package ?tools?
Execution halted

[...]

tobias at openanalytics:~/rAdmin$ echo $R_KEEP_PKG_SOURCE
yes

I do not have this issue when R_KEEP_PKG_SOURCE is set
to 'false' during compilation.

Best,
Tobias

>> P.S. One could even consider a post-install option e.g. to add 'real'
>> R sources (and source references) to Windows packages (which are by
>> definition already 'installed' and for which such information is not
>> by default included in the CRAN binaries of these packages).
>>
>> >> > Prof Brian Ripley wrote:
>> >> > There was an R-core meeting the week before last, and various
>> planned
>> >> > changes will appear in R-devel over the next few weeks.
>> >> >
>> >> > These are changes planned for R 2.14.0 scheduled for Oct 31. As we
>> >> > are sick of people referring to R-devel as '2.14' or '2.14.0', that
>> >> > version number will not be used until we reach 2.14.0 alpha. You
>> >> > will be able to have a package depend on an svn version number when
>> >> > referring to R-devel rather than using R (>= 2.14.0).
>> >> >
>> >> > All packages are installed with lazy-loading (there were 72 CRAN
>> >> > packages and 8 BioC packages which opted out). This means that the
>> >> > code is always parsed at install time which inter alia simplifies
>> the
>> >> > descriptions. R 2.13.1 RC warns on installation about packages which
>> >> > ask not to be lazy-loaded, and R-devel ignores such requests (with a
>> >> > warning).
>> >> >
>> >> > In the near future all packages will have a name space. If the
>> >> > sources do not contain one, a default NAMESPACE file will be added.
>> >> > This again will simplify the descriptions and also a lot of internal
>> >> > code. Maintainers of packages without name spaces (currently 42% of
>> >> > CRAN) are encouraged to add one themselves.
>> >> >
>> >> > R-devel is installed with the base and recommended packages
>> >> > byte-compiled (the equivalent of 'make bytecode' in R 2.13.x, but
>> >> > done less inefficiently). There is a new option R CMD INSTALL
>> >> > --byte-compile to byte-compile contributed packages, but that
>> remains
>> >> > optional.
>> >> > Byte-compilation is quite expensive (so you definitely want to do it
>> >> > at install time, which requires lazy-loading), and relatively few
>> >> > packages benefit appreciably from byte-compilation. A larger number
>> >> > of packages benefit from byte-compilation of R itself: for example
>> >> > AER runs its checks 10% faster. The byte-compiler technology is
>> >> > thanks to Luke Tierney.
>> >> >
>> >> > There is support for figures in Rd files: currently with a
>> first-pass
>> >> > implementation (thanks to Duncan Murdoch).
>> > ______________________________________________
>> > R-devel at r-project.org mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-devel
>> >
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From murdoch.duncan at gmail.com  Tue Jul  5 16:21:51 2011
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Tue, 05 Jul 2011 10:21:51 -0400
Subject: [Rd] Recent and upcoming changes to R-devel
In-Reply-To: <4E131CF5.5000206@openanalytics.eu>
References: <alpine.OSX.1.00.1107041124460.59432@tystie.local>	<3D59DF35E7CF3E42BA70C23BD5CBE6A85F4C199E22@exvic-mbx04.nexus.csiro.au>
	<4E12ED06.5040603@openanalytics.eu> <4E1310D9.6050901@gmail.com>
	<4E131CF5.5000206@openanalytics.eu>
Message-ID: <4E131DFF.7060808@gmail.com>

On 05/07/2011 10:17 AM, Tobias Verbeke wrote:
> Dear Duncan,
>
> On 07/05/2011 03:25 PM, Duncan Murdoch wrote:
> >  On 05/07/2011 6:52 AM, Tobias Verbeke wrote:
> >>  L.S.
> >>
> >>  On 07/05/2011 02:16 AM, Mark.Bravington at csiro.au wrote:
> >>  >  I may have misunderstood, but:
> >>  >
> >>  >  Please could we have an optional installation that does not*not*
> >>  byte-compile base and recommended?
> >>  >
> >>  >  Reason: it's not possible to debug byte-compiled code-- at least not
> >>  with the 'debug' package, which is quite widely used. I quite often
> >>  end up using 'mtrace' on functions in base/recommended packages to
> >>  figure out what they are doing. And sometimes I (and others)
> >>  experiment with changing functions in base/recommended to improve
> >>  functionality. That seems to be harder with BC versions, and might
> >>  even be impossible, as best I can tell from hints in the documentation
> >>  of 'compile').
> >>  >
> >>  >  Personally, if I had to choose only one, I'd rather live with the
> >>  speed penalty from not byte-compiling. But of course, if both are
> >>  available, I could install both.
> >>
> >>  I completely second this request. All speed improvements and the byte
> >>  compiler in particular are leaps forward and I am very grateful and
> >>  admiring towards the people that make this happen.
> >>
> >>  That being said, 'moving away' from the sources (with the lazy loading
> >>  files and byte-compilation) may be a step back for R package developers
> >>  that (during development and maybe on separate development installations
> >>  [as opposed to production installations of R]) require
> >>  the sources of all packages to be efficient in their work.
> >>
> >>  As many of you know there is an open source Eclipse/StatET visual
> >>  debugger ready and for that application as well (similar to Mark's
> >>  request) presence of non-compiled code is highly desirable.
> >>
> >>  For the particular purpose of debugging R packages, I would even plead
> >>  to go beyond the current options and support the addition of an
> >>  R package install option that allows to include the sources (e.g. in
> >>  a standard folder Rsrc/) in installed packages.
> >>
> >>  I am fully aware that one can always fetch the source tarballs from
> >>  CRAN for that purpose, but it would be much more easy if a simple
> >>  installation option could put the R sources of a package in a separate
> >>  folder [or archive inside an existing folder] such that R development
> >>  tools (such as the Eclipse/StatET IDE) can offer inspection of sources
> >>  or display them (e.g. during debugging) out of the box.
> >>
> >>  If one has the srcref, one can always load the absolutely correct source
> >>  code this way, even if one doesn't know the parent function with
> >>  the source attribute.
> >>
> >>  Any comments?
> >
> >  I think these requests have already been met. If you modify the body of
> >  a closure (as trace() does), then the byte compiled version is
> >  discarded, and you go back to the regular interpreted code. If you
> >  install packages with the R_KEEP_PKG_SOURCE=yes environment variable
> >  set, the you keep all source for all functions. (It's attached to the
> >  function itself, not as a file that may be out of date.) It's possible
> >  that byte compiling turns off R_KEEP_PKG_SOURCE, but that is something
> >  that is either easily fixed, or avoided by re-installing without byte
> >  compiling.
>
> Many thanks for your reaction. Is the R_KEEP_PKG_SOURCE=yes environment
> variable also supported during R installation ?

Yes, other than the error you saw below, which is a temporary problem.  
Not sure which function exceeded the length limit, but the length limit 
is going away before 2.14.0 is released.

Duncan Murdoch

> I hope I'm not overlooking anything, but when compiling
>
> ftp://ftp.stat.math.ethz.ch/Software/R/R-devel.tar.gz
>
> a few minutes ago I encountered the following issue:
>
> [...]
>
> building package 'tools'
> mkdir -p -- ../../../library/tools
> make[4]: Entering directory `/home/tobias/rAdmin/R-devel/src/library/tools'
> mkdir -p -- ../../../library/tools/R
> mkdir -p -- ../../../library/tools/po
> make[4]: Leaving directory `/home/tobias/rAdmin/R-devel/src/library/tools'
> make[4]: Entering directory `/home/tobias/rAdmin/R-devel/src/library/tools'
> make[5]: Entering directory
> `/home/tobias/rAdmin/R-devel/src/library/tools/src'
> making text.d from text.c
> making init.d from init.c
> making Rmd5.d from Rmd5.c
> making md5.d from md5.c
> gcc -std=gnu99 -I../../../../include  -I/usr/local/include
> -fvisibility=hidden -fpic  -g -O2 -c text.c -o text.o
> gcc -std=gnu99 -I../../../../include  -I/usr/local/include
> -fvisibility=hidden -fpic  -g -O2 -c init.c -o init.o
> gcc -std=gnu99 -I../../../../include  -I/usr/local/include
> -fvisibility=hidden -fpic  -g -O2 -c Rmd5.c -o Rmd5.o
> gcc -std=gnu99 -I../../../../include  -I/usr/local/include
> -fvisibility=hidden -fpic  -g -O2 -c md5.c -o md5.o
> gcc -std=gnu99 -shared -L/usr/local/lib64 -o tools.so text.o init.o
> Rmd5.o md5.o -L../../../../lib -lR
> make[6]: Entering directory
> `/home/tobias/rAdmin/R-devel/src/library/tools/src'
> make[6]: `Makedeps' is up to date.
> make[6]: Leaving directory
> `/home/tobias/rAdmin/R-devel/src/library/tools/src'
> make[6]: Entering directory
> `/home/tobias/rAdmin/R-devel/src/library/tools/src'
> mkdir -p -- ../../../../library/tools/libs
> make[6]: Leaving directory
> `/home/tobias/rAdmin/R-devel/src/library/tools/src'
> make[5]: Leaving directory
> `/home/tobias/rAdmin/R-devel/src/library/tools/src'
> make[4]: Leaving directory `/home/tobias/rAdmin/R-devel/src/library/tools'
> Error in parse(n = -1, file = file) :
>     function is too long to keep source (at line 2967)
> Error: unable to load R code in package ?tools?
> Execution halted
>
> [...]
>
> tobias at openanalytics:~/rAdmin$ echo $R_KEEP_PKG_SOURCE
> yes
>
> I do not have this issue when R_KEEP_PKG_SOURCE is set
> to 'false' during compilation.
>
> Best,
> Tobias
>
> >>  P.S. One could even consider a post-install option e.g. to add 'real'
> >>  R sources (and source references) to Windows packages (which are by
> >>  definition already 'installed' and for which such information is not
> >>  by default included in the CRAN binaries of these packages).
> >>
> >>  >>  >  Prof Brian Ripley wrote:
> >>  >>  >  There was an R-core meeting the week before last, and various
> >>  planned
> >>  >>  >  changes will appear in R-devel over the next few weeks.
> >>  >>  >
> >>  >>  >  These are changes planned for R 2.14.0 scheduled for Oct 31. As we
> >>  >>  >  are sick of people referring to R-devel as '2.14' or '2.14.0', that
> >>  >>  >  version number will not be used until we reach 2.14.0 alpha. You
> >>  >>  >  will be able to have a package depend on an svn version number when
> >>  >>  >  referring to R-devel rather than using R (>= 2.14.0).
> >>  >>  >
> >>  >>  >  All packages are installed with lazy-loading (there were 72 CRAN
> >>  >>  >  packages and 8 BioC packages which opted out). This means that the
> >>  >>  >  code is always parsed at install time which inter alia simplifies
> >>  the
> >>  >>  >  descriptions. R 2.13.1 RC warns on installation about packages which
> >>  >>  >  ask not to be lazy-loaded, and R-devel ignores such requests (with a
> >>  >>  >  warning).
> >>  >>  >
> >>  >>  >  In the near future all packages will have a name space. If the
> >>  >>  >  sources do not contain one, a default NAMESPACE file will be added.
> >>  >>  >  This again will simplify the descriptions and also a lot of internal
> >>  >>  >  code. Maintainers of packages without name spaces (currently 42% of
> >>  >>  >  CRAN) are encouraged to add one themselves.
> >>  >>  >
> >>  >>  >  R-devel is installed with the base and recommended packages
> >>  >>  >  byte-compiled (the equivalent of 'make bytecode' in R 2.13.x, but
> >>  >>  >  done less inefficiently). There is a new option R CMD INSTALL
> >>  >>  >  --byte-compile to byte-compile contributed packages, but that
> >>  remains
> >>  >>  >  optional.
> >>  >>  >  Byte-compilation is quite expensive (so you definitely want to do it
> >>  >>  >  at install time, which requires lazy-loading), and relatively few
> >>  >>  >  packages benefit appreciably from byte-compilation. A larger number
> >>  >>  >  of packages benefit from byte-compilation of R itself: for example
> >>  >>  >  AER runs its checks 10% faster. The byte-compiler technology is
> >>  >>  >  thanks to Luke Tierney.
> >>  >>  >
> >>  >>  >  There is support for figures in Rd files: currently with a
> >>  first-pass
> >>  >>  >  implementation (thanks to Duncan Murdoch).
> >>  >  ______________________________________________
> >>  >  R-devel at r-project.org mailing list
> >>  >  https://stat.ethz.ch/mailman/listinfo/r-devel
> >>  >
> >>
> >>  ______________________________________________
> >>  R-devel at r-project.org mailing list
> >>  https://stat.ethz.ch/mailman/listinfo/r-devel
> >
>


From murdoch.duncan at gmail.com  Tue Jul  5 17:21:33 2011
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Tue, 05 Jul 2011 11:21:33 -0400
Subject: [Rd] Recent and upcoming changes to R-devel
In-Reply-To: <4E132BA0.5000003@walware.de>
References: <alpine.OSX.1.00.1107041124460.59432@tystie.local>	<3D59DF35E7CF3E42BA70C23BD5CBE6A85F4C199E22@exvic-mbx04.nexus.csiro.au>
	<4E12ED06.5040603@openanalytics.eu>
	<4E1310D9.6050901@gmail.com> <4E132BA0.5000003@walware.de>
Message-ID: <4E132BFD.5090604@gmail.com>

On 05/07/2011 11:20 AM, Stephan Wahlbrink wrote:
> Dear developers,
>
> Duncan Murdoch wrote [2011-07-05 15:25]:
> >  On 05/07/2011 6:52 AM, Tobias Verbeke wrote:
> >>  L.S.
> >>
> >>  On 07/05/2011 02:16 AM, Mark.Bravington at csiro.au wrote:
> >>  >  I may have misunderstood, but:
> >>  >
> >>  >  Please could we have an optional installation that does not*not*
> >>  byte-compile base and recommended?
> >>  >
> >>  >  Reason: it's not possible to debug byte-compiled code-- at least not
> >>  with the 'debug' package, which is quite widely used. I quite often
> >>  end up using 'mtrace' on functions in base/recommended packages to
> >>  figure out what they are doing. And sometimes I (and others)
> >>  experiment with changing functions in base/recommended to improve
> >>  functionality. That seems to be harder with BC versions, and might
> >>  even be impossible, as best I can tell from hints in the documentation
> >>  of 'compile').
> >>  >
> >>  >  Personally, if I had to choose only one, I'd rather live with the
> >>  speed penalty from not byte-compiling. But of course, if both are
> >>  available, I could install both.
> >>
> >>  I completely second this request. All speed improvements and the byte
> >>  compiler in particular are leaps forward and I am very grateful and
> >>  admiring towards the people that make this happen.
> >>
> >>  That being said, 'moving away' from the sources (with the lazy loading
> >>  files and byte-compilation) may be a step back for R package developers
> >>  that (during development and maybe on separate development installations
> >>  [as opposed to production installations of R]) require
> >>  the sources of all packages to be efficient in their work.
> >>
> >>  As many of you know there is an open source Eclipse/StatET visual
> >>  debugger ready and for that application as well (similar to Mark's
> >>  request) presence of non-compiled code is highly desirable.
> >>
> >>  For the particular purpose of debugging R packages, I would even plead
> >>  to go beyond the current options and support the addition of an
> >>  R package install option that allows to include the sources (e.g. in
> >>  a standard folder Rsrc/) in installed packages.
> >>
> >>  I am fully aware that one can always fetch the source tarballs from
> >>  CRAN for that purpose, but it would be much more easy if a simple
> >>  installation option could put the R sources of a package in a separate
> >>  folder [or archive inside an existing folder] such that R development
> >>  tools (such as the Eclipse/StatET IDE) can offer inspection of sources
> >>  or display them (e.g. during debugging) out of the box.
> >>
> >>  If one has the srcref, one can always load the absolutely correct source
> >>  code this way, even if one doesn't know the parent function with
> >>  the source attribute.
> >>
> >>  Any comments?
> >
> >  I think these requests have already been met. If you modify the body of
> >  a closure (as trace() does), then the byte compiled version is
> >  discarded, and you go back to the regular interpreted code. If you
> >  install packages with the R_KEEP_PKG_SOURCE=yes environment variable
> >  set, the you keep all source for all functions. (It's attached to the
> >  function itself, not as a file that may be out of date.) It's possible
> >  that byte compiling turns off R_KEEP_PKG_SOURCE, but that is something
> >  that is either easily fixed, or avoided by re-installing without byte
> >  compiling.
>
> I don?t know how the new installation works exactly, but would it be
> possible, to simply install both types, the old expression bodies and
> the new byte-compiled, as single package at the same time?

Yes, that's what is done.
>   This would
> allow the R user and developer to simply use the variant which is the
> best at the moment. If he wants to debug code, he can switch of the use
> of byte-compiled code  and use the old R expressions (with attached
> srcrefs). If debugging is not required, he can profit from the
> byte-compiled version. The best would be a toggle, to switch it at
> runtime, but a startup option would be sufficient too.
>
> I think direct access to the code is one big advantage of open source
> software. For developer it makes it easier to find and fix bugs if
> something is wrong. But it can also help users a lot to understand how a
> function or algorithm works and learn from code written by other persons
> ? if the access to the sources is easy.
>
> As long byte-code doesn?t support the debugging features of R, it is
> required for best debugging support to run the functions completely
> without byte-complied code. If I understood it correctly, byte-code
> frames would disable srcrefs as well as features like ?step return? to
> that frames. Therefore I ask for a way that it is easy to switch between
> both execution types.

What gave you that impression?

Duncan Murdoch

> Best,
> Stephan
>
>
> >
> >  Duncan Murdoch
> >
> >>  Best,
> >>  Tobias
> >>
> >>  P.S. One could even consider a post-install option e.g. to add 'real'
> >>  R sources (and source references) to Windows packages (which are by
> >>  definition already 'installed' and for which such information is not
> >>  by default included in the CRAN binaries of these packages).
> >>
> >>  >>  >  Prof Brian Ripley wrote:
> >>  >>  >  There was an R-core meeting the week before last, and various
> >>  planned
> >>  >>  >  changes will appear in R-devel over the next few weeks.
> >>  >>  >
> >>  >>  >  These are changes planned for R 2.14.0 scheduled for Oct 31. As we
> >>  >>  >  are sick of people referring to R-devel as '2.14' or '2.14.0', that
> >>  >>  >  version number will not be used until we reach 2.14.0 alpha. You
> >>  >>  >  will be able to have a package depend on an svn version number when
> >>  >>  >  referring to R-devel rather than using R (>= 2.14.0).
> >>  >>  >
> >>  >>  >  All packages are installed with lazy-loading (there were 72 CRAN
> >>  >>  >  packages and 8 BioC packages which opted out). This means that the
> >>  >>  >  code is always parsed at install time which inter alia simplifies
> >>  the
> >>  >>  >  descriptions. R 2.13.1 RC warns on installation about packages which
> >>  >>  >  ask not to be lazy-loaded, and R-devel ignores such requests (with a
> >>  >>  >  warning).
> >>  >>  >
> >>  >>  >  In the near future all packages will have a name space. If the
> >>  >>  >  sources do not contain one, a default NAMESPACE file will be added.
> >>  >>  >  This again will simplify the descriptions and also a lot of internal
> >>  >>  >  code. Maintainers of packages without name spaces (currently 42% of
> >>  >>  >  CRAN) are encouraged to add one themselves.
> >>  >>  >
> >>  >>  >  R-devel is installed with the base and recommended packages
> >>  >>  >  byte-compiled (the equivalent of 'make bytecode' in R 2.13.x, but
> >>  >>  >  done less inefficiently). There is a new option R CMD INSTALL
> >>  >>  >  --byte-compile to byte-compile contributed packages, but that
> >>  remains
> >>  >>  >  optional.
> >>  >>  >  Byte-compilation is quite expensive (so you definitely want to do it
> >>  >>  >  at install time, which requires lazy-loading), and relatively few
> >>  >>  >  packages benefit appreciably from byte-compilation. A larger number
> >>  >>  >  of packages benefit from byte-compilation of R itself: for example
> >>  >>  >  AER runs its checks 10% faster. The byte-compiler technology is
> >>  >>  >  thanks to Luke Tierney.
> >>  >>  >
> >>  >>  >  There is support for figures in Rd files: currently with a
> >>  first-pass
> >>  >>  >  implementation (thanks to Duncan Murdoch).
>
> --
> Stephan Wahlbrink
> Humboldtstr. 19
> 44137 Dortmund
> Germany
> http://www.walware.de/goto/opensource
>


From stephan.wahlbrink at walware.de  Tue Jul  5 17:20:00 2011
From: stephan.wahlbrink at walware.de (Stephan Wahlbrink)
Date: Tue, 05 Jul 2011 17:20:00 +0200
Subject: [Rd] Recent and upcoming changes to R-devel
In-Reply-To: <4E1310D9.6050901@gmail.com>
References: <alpine.OSX.1.00.1107041124460.59432@tystie.local>	<3D59DF35E7CF3E42BA70C23BD5CBE6A85F4C199E22@exvic-mbx04.nexus.csiro.au>
	<4E12ED06.5040603@openanalytics.eu> <4E1310D9.6050901@gmail.com>
Message-ID: <4E132BA0.5000003@walware.de>

Dear developers,

Duncan Murdoch wrote [2011-07-05 15:25]:
> On 05/07/2011 6:52 AM, Tobias Verbeke wrote:
>> L.S.
>>
>> On 07/05/2011 02:16 AM, Mark.Bravington at csiro.au wrote:
>> > I may have misunderstood, but:
>> >
>> > Please could we have an optional installation that does not*not*
>> byte-compile base and recommended?
>> >
>> > Reason: it's not possible to debug byte-compiled code-- at least not
>> with the 'debug' package, which is quite widely used. I quite often
>> end up using 'mtrace' on functions in base/recommended packages to
>> figure out what they are doing. And sometimes I (and others)
>> experiment with changing functions in base/recommended to improve
>> functionality. That seems to be harder with BC versions, and might
>> even be impossible, as best I can tell from hints in the documentation
>> of 'compile').
>> >
>> > Personally, if I had to choose only one, I'd rather live with the
>> speed penalty from not byte-compiling. But of course, if both are
>> available, I could install both.
>>
>> I completely second this request. All speed improvements and the byte
>> compiler in particular are leaps forward and I am very grateful and
>> admiring towards the people that make this happen.
>>
>> That being said, 'moving away' from the sources (with the lazy loading
>> files and byte-compilation) may be a step back for R package developers
>> that (during development and maybe on separate development installations
>> [as opposed to production installations of R]) require
>> the sources of all packages to be efficient in their work.
>>
>> As many of you know there is an open source Eclipse/StatET visual
>> debugger ready and for that application as well (similar to Mark's
>> request) presence of non-compiled code is highly desirable.
>>
>> For the particular purpose of debugging R packages, I would even plead
>> to go beyond the current options and support the addition of an
>> R package install option that allows to include the sources (e.g. in
>> a standard folder Rsrc/) in installed packages.
>>
>> I am fully aware that one can always fetch the source tarballs from
>> CRAN for that purpose, but it would be much more easy if a simple
>> installation option could put the R sources of a package in a separate
>> folder [or archive inside an existing folder] such that R development
>> tools (such as the Eclipse/StatET IDE) can offer inspection of sources
>> or display them (e.g. during debugging) out of the box.
>>
>> If one has the srcref, one can always load the absolutely correct source
>> code this way, even if one doesn't know the parent function with
>> the source attribute.
>>
>> Any comments?
>
> I think these requests have already been met. If you modify the body of
> a closure (as trace() does), then the byte compiled version is
> discarded, and you go back to the regular interpreted code. If you
> install packages with the R_KEEP_PKG_SOURCE=yes environment variable
> set, the you keep all source for all functions. (It's attached to the
> function itself, not as a file that may be out of date.) It's possible
> that byte compiling turns off R_KEEP_PKG_SOURCE, but that is something
> that is either easily fixed, or avoided by re-installing without byte
> compiling.

I don?t know how the new installation works exactly, but would it be 
possible, to simply install both types, the old expression bodies and 
the new byte-compiled, as single package at the same time? This would 
allow the R user and developer to simply use the variant which is the 
best at the moment. If he wants to debug code, he can switch of the use 
of byte-compiled code  and use the old R expressions (with attached 
srcrefs). If debugging is not required, he can profit from the 
byte-compiled version. The best would be a toggle, to switch it at 
runtime, but a startup option would be sufficient too.

I think direct access to the code is one big advantage of open source 
software. For developer it makes it easier to find and fix bugs if 
something is wrong. But it can also help users a lot to understand how a 
function or algorithm works and learn from code written by other persons 
? if the access to the sources is easy.

As long byte-code doesn?t support the debugging features of R, it is 
required for best debugging support to run the functions completely 
without byte-complied code. If I understood it correctly, byte-code 
frames would disable srcrefs as well as features like ?step return? to 
that frames. Therefore I ask for a way that it is easy to switch between 
both execution types.

Best,
Stephan


>
> Duncan Murdoch
>
>> Best,
>> Tobias
>>
>> P.S. One could even consider a post-install option e.g. to add 'real'
>> R sources (and source references) to Windows packages (which are by
>> definition already 'installed' and for which such information is not
>> by default included in the CRAN binaries of these packages).
>>
>> >> > Prof Brian Ripley wrote:
>> >> > There was an R-core meeting the week before last, and various
>> planned
>> >> > changes will appear in R-devel over the next few weeks.
>> >> >
>> >> > These are changes planned for R 2.14.0 scheduled for Oct 31. As we
>> >> > are sick of people referring to R-devel as '2.14' or '2.14.0', that
>> >> > version number will not be used until we reach 2.14.0 alpha. You
>> >> > will be able to have a package depend on an svn version number when
>> >> > referring to R-devel rather than using R (>= 2.14.0).
>> >> >
>> >> > All packages are installed with lazy-loading (there were 72 CRAN
>> >> > packages and 8 BioC packages which opted out). This means that the
>> >> > code is always parsed at install time which inter alia simplifies
>> the
>> >> > descriptions. R 2.13.1 RC warns on installation about packages which
>> >> > ask not to be lazy-loaded, and R-devel ignores such requests (with a
>> >> > warning).
>> >> >
>> >> > In the near future all packages will have a name space. If the
>> >> > sources do not contain one, a default NAMESPACE file will be added.
>> >> > This again will simplify the descriptions and also a lot of internal
>> >> > code. Maintainers of packages without name spaces (currently 42% of
>> >> > CRAN) are encouraged to add one themselves.
>> >> >
>> >> > R-devel is installed with the base and recommended packages
>> >> > byte-compiled (the equivalent of 'make bytecode' in R 2.13.x, but
>> >> > done less inefficiently). There is a new option R CMD INSTALL
>> >> > --byte-compile to byte-compile contributed packages, but that
>> remains
>> >> > optional.
>> >> > Byte-compilation is quite expensive (so you definitely want to do it
>> >> > at install time, which requires lazy-loading), and relatively few
>> >> > packages benefit appreciably from byte-compilation. A larger number
>> >> > of packages benefit from byte-compilation of R itself: for example
>> >> > AER runs its checks 10% faster. The byte-compiler technology is
>> >> > thanks to Luke Tierney.
>> >> >
>> >> > There is support for figures in Rd files: currently with a
>> first-pass
>> >> > implementation (thanks to Duncan Murdoch).

--
Stephan Wahlbrink
Humboldtstr. 19
44137 Dortmund
Germany
http://www.walware.de/goto/opensource


From tobias.verbeke at openanalytics.eu  Tue Jul  5 19:45:07 2011
From: tobias.verbeke at openanalytics.eu (Tobias Verbeke)
Date: Tue, 05 Jul 2011 19:45:07 +0200
Subject: [Rd] Recent and upcoming changes to R-devel
In-Reply-To: <4E131DFF.7060808@gmail.com>
References: <alpine.OSX.1.00.1107041124460.59432@tystie.local>	<3D59DF35E7CF3E42BA70C23BD5CBE6A85F4C199E22@exvic-mbx04.nexus.csiro.au>
	<4E12ED06.5040603@openanalytics.eu> <4E1310D9.6050901@gmail.com>
	<4E131CF5.5000206@openanalytics.eu> <4E131DFF.7060808@gmail.com>
Message-ID: <4E134DA3.2040600@openanalytics.eu>

On 07/05/2011 04:21 PM, Duncan Murdoch wrote:
> On 05/07/2011 10:17 AM, Tobias Verbeke wrote:
>> Dear Duncan,
>>
>> On 07/05/2011 03:25 PM, Duncan Murdoch wrote:
>> > On 05/07/2011 6:52 AM, Tobias Verbeke wrote:
>> >> L.S.
>> >>
>> >> On 07/05/2011 02:16 AM, Mark.Bravington at csiro.au wrote:
>> >> > I may have misunderstood, but:
>> >> >
>> >> > Please could we have an optional installation that does not*not*
>> >> byte-compile base and recommended?
>> >> >
>> >> > Reason: it's not possible to debug byte-compiled code-- at least not
>> >> with the 'debug' package, which is quite widely used. I quite often
>> >> end up using 'mtrace' on functions in base/recommended packages to
>> >> figure out what they are doing. And sometimes I (and others)
>> >> experiment with changing functions in base/recommended to improve
>> >> functionality. That seems to be harder with BC versions, and might
>> >> even be impossible, as best I can tell from hints in the documentation
>> >> of 'compile').
>> >> >
>> >> > Personally, if I had to choose only one, I'd rather live with the
>> >> speed penalty from not byte-compiling. But of course, if both are
>> >> available, I could install both.
>> >>
>> >> I completely second this request. All speed improvements and the byte
>> >> compiler in particular are leaps forward and I am very grateful and
>> >> admiring towards the people that make this happen.
>> >>
>> >> That being said, 'moving away' from the sources (with the lazy loading
>> >> files and byte-compilation) may be a step back for R package
>> developers
>> >> that (during development and maybe on separate development
>> installations
>> >> [as opposed to production installations of R]) require
>> >> the sources of all packages to be efficient in their work.
>> >>
>> >> As many of you know there is an open source Eclipse/StatET visual
>> >> debugger ready and for that application as well (similar to Mark's
>> >> request) presence of non-compiled code is highly desirable.
>> >>
>> >> For the particular purpose of debugging R packages, I would even plead
>> >> to go beyond the current options and support the addition of an
>> >> R package install option that allows to include the sources (e.g. in
>> >> a standard folder Rsrc/) in installed packages.
>> >>
>> >> I am fully aware that one can always fetch the source tarballs from
>> >> CRAN for that purpose, but it would be much more easy if a simple
>> >> installation option could put the R sources of a package in a separate
>> >> folder [or archive inside an existing folder] such that R development
>> >> tools (such as the Eclipse/StatET IDE) can offer inspection of sources
>> >> or display them (e.g. during debugging) out of the box.
>> >>
>> >> If one has the srcref, one can always load the absolutely correct
>> source
>> >> code this way, even if one doesn't know the parent function with
>> >> the source attribute.
>> >>
>> >> Any comments?
>> >
>> > I think these requests have already been met. If you modify the body of
>> > a closure (as trace() does), then the byte compiled version is
>> > discarded, and you go back to the regular interpreted code. If you
>> > install packages with the R_KEEP_PKG_SOURCE=yes environment variable
>> > set, the you keep all source for all functions. (It's attached to the
>> > function itself, not as a file that may be out of date.) It's possible

Can you expand on when files put inside a package at install
time will be out of date compared to the source information
attached to a function ?

I (naively) thought the source information was created and attached
at install time as well and that it did not change afterwards either.

I guess the arguments for files is that they have precise
locations and allow for easy indexing by development tools
external to R (but may be corrected here as well).

>> > that byte compiling turns off R_KEEP_PKG_SOURCE, but that is something
>> > that is either easily fixed, or avoided by re-installing without byte
>> > compiling.
>>
>> Many thanks for your reaction. Is the R_KEEP_PKG_SOURCE=yes environment
>> variable also supported during R installation ?
>
> Yes, other than the error you saw below, which is a temporary problem.
> Not sure which function exceeded the length limit, but the length limit
> is going away before 2.14.0 is released.

Thanks again, Duncan, for the clarification.

Is it useful (or just whimsical) to have an R
function that would allow for a given stock CRAN
Windows R installation with stock Windows CRAN binary
add-on packages to add the source information that
would be useful e.g. for a debugger post factum?

I can imagine something like

update.packages(., checkSourcesKept = TRUE)

as I don't think this can currently be solved
with a combination of INSTALL_opts="--with-keep.source"
and type="source" given that there will not be a check
for the presence of source information to determine
which packages require being updated (or in this
case 'completed' with source information).

The alternative scenario would be to expect users
that want this functionality to compile R and all
add-on packages from source (also on Windows or
Mac).

Best,
Tobias

>> I hope I'm not overlooking anything, but when compiling
>>
>> ftp://ftp.stat.math.ethz.ch/Software/R/R-devel.tar.gz
>>
>> a few minutes ago I encountered the following issue:
>>
>> [...]
>>
>> building package 'tools'
>> mkdir -p -- ../../../library/tools
>> make[4]: Entering directory
>> `/home/tobias/rAdmin/R-devel/src/library/tools'
>> mkdir -p -- ../../../library/tools/R
>> mkdir -p -- ../../../library/tools/po
>> make[4]: Leaving directory
>> `/home/tobias/rAdmin/R-devel/src/library/tools'
>> make[4]: Entering directory
>> `/home/tobias/rAdmin/R-devel/src/library/tools'
>> make[5]: Entering directory
>> `/home/tobias/rAdmin/R-devel/src/library/tools/src'
>> making text.d from text.c
>> making init.d from init.c
>> making Rmd5.d from Rmd5.c
>> making md5.d from md5.c
>> gcc -std=gnu99 -I../../../../include -I/usr/local/include
>> -fvisibility=hidden -fpic -g -O2 -c text.c -o text.o
>> gcc -std=gnu99 -I../../../../include -I/usr/local/include
>> -fvisibility=hidden -fpic -g -O2 -c init.c -o init.o
>> gcc -std=gnu99 -I../../../../include -I/usr/local/include
>> -fvisibility=hidden -fpic -g -O2 -c Rmd5.c -o Rmd5.o
>> gcc -std=gnu99 -I../../../../include -I/usr/local/include
>> -fvisibility=hidden -fpic -g -O2 -c md5.c -o md5.o
>> gcc -std=gnu99 -shared -L/usr/local/lib64 -o tools.so text.o init.o
>> Rmd5.o md5.o -L../../../../lib -lR
>> make[6]: Entering directory
>> `/home/tobias/rAdmin/R-devel/src/library/tools/src'
>> make[6]: `Makedeps' is up to date.
>> make[6]: Leaving directory
>> `/home/tobias/rAdmin/R-devel/src/library/tools/src'
>> make[6]: Entering directory
>> `/home/tobias/rAdmin/R-devel/src/library/tools/src'
>> mkdir -p -- ../../../../library/tools/libs
>> make[6]: Leaving directory
>> `/home/tobias/rAdmin/R-devel/src/library/tools/src'
>> make[5]: Leaving directory
>> `/home/tobias/rAdmin/R-devel/src/library/tools/src'
>> make[4]: Leaving directory
>> `/home/tobias/rAdmin/R-devel/src/library/tools'
>> Error in parse(n = -1, file = file) :
>> function is too long to keep source (at line 2967)
>> Error: unable to load R code in package ?tools?
>> Execution halted
>>
>> [...]
>>
>> tobias at openanalytics:~/rAdmin$ echo $R_KEEP_PKG_SOURCE
>> yes
>>
>> I do not have this issue when R_KEEP_PKG_SOURCE is set
>> to 'false' during compilation.
>>
>> Best,
>> Tobias
>>
>> >> P.S. One could even consider a post-install option e.g. to add 'real'
>> >> R sources (and source references) to Windows packages (which are by
>> >> definition already 'installed' and for which such information is not
>> >> by default included in the CRAN binaries of these packages).
>> >>
>> >> >> > Prof Brian Ripley wrote:
>> >> >> > There was an R-core meeting the week before last, and various
>> >> planned
>> >> >> > changes will appear in R-devel over the next few weeks.
>> >> >> >
>> >> >> > These are changes planned for R 2.14.0 scheduled for Oct 31.
>> As we
>> >> >> > are sick of people referring to R-devel as '2.14' or '2.14.0',
>> that
>> >> >> > version number will not be used until we reach 2.14.0 alpha. You
>> >> >> > will be able to have a package depend on an svn version number
>> when
>> >> >> > referring to R-devel rather than using R (>= 2.14.0).
>> >> >> >
>> >> >> > All packages are installed with lazy-loading (there were 72 CRAN
>> >> >> > packages and 8 BioC packages which opted out). This means that
>> the
>> >> >> > code is always parsed at install time which inter alia simplifies
>> >> the
>> >> >> > descriptions. R 2.13.1 RC warns on installation about packages
>> which
>> >> >> > ask not to be lazy-loaded, and R-devel ignores such requests
>> (with a
>> >> >> > warning).
>> >> >> >
>> >> >> > In the near future all packages will have a name space. If the
>> >> >> > sources do not contain one, a default NAMESPACE file will be
>> added.
>> >> >> > This again will simplify the descriptions and also a lot of
>> internal
>> >> >> > code. Maintainers of packages without name spaces (currently
>> 42% of
>> >> >> > CRAN) are encouraged to add one themselves.
>> >> >> >
>> >> >> > R-devel is installed with the base and recommended packages
>> >> >> > byte-compiled (the equivalent of 'make bytecode' in R 2.13.x, but
>> >> >> > done less inefficiently). There is a new option R CMD INSTALL
>> >> >> > --byte-compile to byte-compile contributed packages, but that
>> >> remains
>> >> >> > optional.
>> >> >> > Byte-compilation is quite expensive (so you definitely want to
>> do it
>> >> >> > at install time, which requires lazy-loading), and relatively few
>> >> >> > packages benefit appreciably from byte-compilation. A larger
>> number
>> >> >> > of packages benefit from byte-compilation of R itself: for
>> example
>> >> >> > AER runs its checks 10% faster. The byte-compiler technology is
>> >> >> > thanks to Luke Tierney.
>> >> >> >
>> >> >> > There is support for figures in Rd files: currently with a
>> >> first-pass
>> >> >> > implementation (thanks to Duncan Murdoch).
>> >> > ______________________________________________
>> >> > R-devel at r-project.org mailing list
>> >> > https://stat.ethz.ch/mailman/listinfo/r-devel
>> >> >
>> >>
>> >> ______________________________________________
>> >> R-devel at r-project.org mailing list
>> >> https://stat.ethz.ch/mailman/listinfo/r-devel
>> >
>>
>


From mdowle at mdowle.plus.com  Tue Jul  5 20:08:23 2011
From: mdowle at mdowle.plus.com (Matthew Dowle)
Date: Tue, 05 Jul 2011 19:08:23 +0100
Subject: [Rd] [datatable-help]  speeding up perception
In-Reply-To: <1309851165.4231.40.camel@netbook>
References: <CAPr7RtU44GO-7_m02-ek+ps3YcR_qr3m7vV8BSvhAYJ=F_bnbA@mail.gmail.com>
	<CAPr7RtVx1ao9bubQiCrf9LjgpdZdB24vSnjZvGxW4a8Sy-pKag@mail.gmail.com>
	<CAGam+C6yEFw-E+oT7tktDVY8L9KtJZ0aHrQ_y0VLyEMnZDmfSw@mail.gmail.com>
	<555C7B55-67D9-4DC6-90CB-BE8B0F620BFB@r-project.org>
	<1309851165.4231.40.camel@netbook>
Message-ID: <1309889303.4231.60.camel@netbook>

Simon (and all),

I've tried to make assignment as fast as calling `[<-.data.table`
directly, for user convenience. Profiling shows (IIUC) that it isn't
dispatch, but x being copied. Is there a way to prevent '[<-' from
copying x?  Small reproducible example in vanilla R 2.13.0 :

> x = list(a=1:10000,b=1:10000)
> class(x) = "newclass"
> "[<-.newclass" = function(x,i,j,value) x      # i.e. do nothing
> tracemem(x)
[1] "<0xa1ec758>"
> x[1,2] = 42L
tracemem[0xa1ec758 -> 0xa1ec558]:    # but, x is still copied, why?
> 

I've tried returning NULL from [<-.newclass but then x gets assigned
NULL :

> "[<-.newclass" = function(x,i,j,value) NULL
> x[1,2] = 42L
tracemem[0xa1ec558 -> 0x9c5f318]: 
> x
NULL
> 

Any pointers much appreciated. If that copy is preventable it should
save the user needing to use `[<-.data.table`(...) syntax to get the
best speed (20 times faster on the small example used so far).

Matthew


On Tue, 2011-07-05 at 08:32 +0100, Matthew Dowle wrote:
> Simon,
> 
> Thanks for the great suggestion. I've written a skeleton assignment
> function for data.table which incurs no copies, which works for this
> case. For completeness, if I understand correctly, this is for : 
>   i) convenience of new users who don't know how to vectorize yet
>   ii) more complex examples which can't be vectorized.
> 
> Before:
> 
> > system.time(for (r in 1:R) DT[r,20] <- 1.0)
>    user  system elapsed 
>  12.792   0.488  13.340 
> 
> After :
> 
> > system.time(for (r in 1:R) DT[r,20] <- 1.0)
>    user  system elapsed 
>   2.908   0.020   2.935
> 
> Where this can be reduced further as follows :
> 
> > system.time(for (r in 1:R) `[<-.data.table`(DT,r,2,1.0))
>    user  system elapsed 
>   0.132   0.000   0.131 
> > 
> 
> Still working on it. When it doesn't break other data.table tests, I'll
> commit to R-Forge ...
> 
> Matthew
> 
> 
> On Mon, 2011-07-04 at 12:41 -0400, Simon Urbanek wrote:
> > Timoth?e,
> > 
> > On Jul 4, 2011, at 2:47 AM, Timoth?e Carayol wrote:
> > 
> > > Hi --
> > > 
> > > It's my first post on this list; as a relatively new user with little
> > > knowledge of R internals, I am a bit intimidated by the depth of some
> > > of the discussions here, so please spare me if I say something
> > > incredibly silly.
> > > 
> > > I feel that someone at this point should mention Matthew Dowle's
> > > excellent data.table package
> > > (http://cran.r-project.org/web/packages/data.table/index.html) which
> > > seems to me to address many of the inefficiencies of data.frame.
> > > data.tables have no row names; and operations that only need data from
> > > one or two columns are (I believe) just as quick whether the total
> > > number of columns is 5 or 1000. This results in very quick operations
> > > (and, often, elegant code as well).
> > > 
> > 
> > I agree that data.table is a very good alternative (for other reasons) that should be promoted more. The only slight snag is that it doesn't help with the issue at hand since it simply does a pass-though for subassignments to data frame's methods and thus suffers from the same problems (in fact there is a rather stark asymmetry in how it handles subsetting vs subassignment - which is a bit surprising [if I read the code correctly you can't use the same indexing in both]). In fact I would propose that it should not do that but handle the simple cases itself more efficiently without unneeded copies. That would make it indeed a very interesting alternative.
> > 
> > Cheers,
> > Simon
> > 
> > 
> > > 
> > > On Mon, Jul 4, 2011 at 6:19 AM, ivo welch <ivo.welch at gmail.com> wrote:
> > >> thank you, simon.  this was very interesting indeed.  I also now
> > >> understand how far out of my depth I am here.
> > >> 
> > >> fortunately, as an end user, obviously, *I* now know how to avoid the
> > >> problem.  I particularly like the as.list() transformation and back to
> > >> as.data.frame() to speed things up without loss of (much)
> > >> functionality.
> > >> 
> > >> 
> > >> more broadly, I view the avoidance of individual access through the
> > >> use of apply and vector operations as a mixed "IQ test" and "knowledge
> > >> test" (which I often fail).  However, even for the most clever, there
> > >> are also situations where the KISS programming principle makes
> > >> explicit loops still preferable.  Personally, I would have preferred
> > >> it if R had, in its standard "statistical data set" data structure,
> > >> foregone the row names feature in exchange for retaining fast direct
> > >> access.  R could have reserved its current implementation "with row
> > >> names but slow access" for a less common (possibly pseudo-inheriting)
> > >> data structure.
> > >> 
> > >> 
> > >> If end users commonly do iterations over a data frame, which I would
> > >> guess to be the case, then the impression of R by (novice) end users
> > >> could be greatly enhanced if the extreme penalties could be eliminated
> > >> or at least flagged.  For example, I wonder if modest special internal
> > >> code could store data frames internally and transparently as lists of
> > >> vectors UNTIL a row name is assigned to.  Easier and uglier, a simple
> > >> but specific warning message could be issued with a suggestion if
> > >> there is an individual read/write into a data frame ("Warning: data
> > >> frames are much slower than lists of vectors for individual element
> > >> access").
> > >> 
> > >> 
> > >> I would also suggest changing the "Introduction to R" 6.3  from "A
> > >> data frame may for many purposes be regarded as a matrix with columns
> > >> possibly of differing modes and attributes. It may be displayed in
> > >> matrix form, and its rows and columns extracted using matrix indexing
> > >> conventions." to "A data frame may for many purposes be regarded as a
> > >> matrix with columns possibly of differing modes and attributes. It may
> > >> be displayed in matrix form, and its rows and columns extracted using
> > >> matrix indexing conventions.  However, data frames can be much slower
> > >> than matrices or even lists of vectors (which, like data frames, can
> > >> contain different types of columns) when individual elements need to
> > >> be accessed."  Reading about it immediately upon introduction could
> > >> flag the problem in a more visible manner.
> > >> 
> > >> 
> > >> regards,
> > >> 
> > >> /iaw
> > >> 
> > >> ______________________________________________
> > >> R-devel at r-project.org mailing list
> > >> https://stat.ethz.ch/mailman/listinfo/r-devel
> > >> 
> > > 
> > > ______________________________________________
> > > R-devel at r-project.org mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-devel
> > > 
> > > 
> > 
> > _______________________________________________
> > datatable-help mailing list
> > datatable-help at lists.r-forge.r-project.org
> > https://lists.r-forge.r-project.org/cgi-bin/mailman/listinfo/datatable-help
> 
> 
> _______________________________________________
> datatable-help mailing list
> datatable-help at lists.r-forge.r-project.org
> https://lists.r-forge.r-project.org/cgi-bin/mailman/listinfo/datatable-help


From simon.urbanek at r-project.org  Tue Jul  5 20:12:55 2011
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Tue, 5 Jul 2011 14:12:55 -0400
Subject: [Rd] Recent and upcoming changes to R-devel
In-Reply-To: <4E134DA3.2040600@openanalytics.eu>
References: <alpine.OSX.1.00.1107041124460.59432@tystie.local>	<3D59DF35E7CF3E42BA70C23BD5CBE6A85F4C199E22@exvic-mbx04.nexus.csiro.au>
	<4E12ED06.5040603@openanalytics.eu> <4E1310D9.6050901@gmail.com>
	<4E131CF5.5000206@openanalytics.eu> <4E131DFF.7060808@gmail.com>
	<4E134DA3.2040600@openanalytics.eu>
Message-ID: <3D74A26C-7D71-4F41-BD77-CFBC49C933C6@r-project.org>


On Jul 5, 2011, at 1:45 PM, Tobias Verbeke wrote:

> On 07/05/2011 04:21 PM, Duncan Murdoch wrote:
>> On 05/07/2011 10:17 AM, Tobias Verbeke wrote:
>>> Dear Duncan,
>>> 
>>> On 07/05/2011 03:25 PM, Duncan Murdoch wrote:
>>> > On 05/07/2011 6:52 AM, Tobias Verbeke wrote:
>>> >> L.S.
>>> >>
>>> >> On 07/05/2011 02:16 AM, Mark.Bravington at csiro.au wrote:
>>> >> > I may have misunderstood, but:
>>> >> >
>>> >> > Please could we have an optional installation that does not*not*
>>> >> byte-compile base and recommended?
>>> >> >
>>> >> > Reason: it's not possible to debug byte-compiled code-- at least not
>>> >> with the 'debug' package, which is quite widely used. I quite often
>>> >> end up using 'mtrace' on functions in base/recommended packages to
>>> >> figure out what they are doing. And sometimes I (and others)
>>> >> experiment with changing functions in base/recommended to improve
>>> >> functionality. That seems to be harder with BC versions, and might
>>> >> even be impossible, as best I can tell from hints in the documentation
>>> >> of 'compile').
>>> >> >
>>> >> > Personally, if I had to choose only one, I'd rather live with the
>>> >> speed penalty from not byte-compiling. But of course, if both are
>>> >> available, I could install both.
>>> >>
>>> >> I completely second this request. All speed improvements and the byte
>>> >> compiler in particular are leaps forward and I am very grateful and
>>> >> admiring towards the people that make this happen.
>>> >>
>>> >> That being said, 'moving away' from the sources (with the lazy loading
>>> >> files and byte-compilation) may be a step back for R package
>>> developers
>>> >> that (during development and maybe on separate development
>>> installations
>>> >> [as opposed to production installations of R]) require
>>> >> the sources of all packages to be efficient in their work.
>>> >>
>>> >> As many of you know there is an open source Eclipse/StatET visual
>>> >> debugger ready and for that application as well (similar to Mark's
>>> >> request) presence of non-compiled code is highly desirable.
>>> >>
>>> >> For the particular purpose of debugging R packages, I would even plead
>>> >> to go beyond the current options and support the addition of an
>>> >> R package install option that allows to include the sources (e.g. in
>>> >> a standard folder Rsrc/) in installed packages.
>>> >>
>>> >> I am fully aware that one can always fetch the source tarballs from
>>> >> CRAN for that purpose, but it would be much more easy if a simple
>>> >> installation option could put the R sources of a package in a separate
>>> >> folder [or archive inside an existing folder] such that R development
>>> >> tools (such as the Eclipse/StatET IDE) can offer inspection of sources
>>> >> or display them (e.g. during debugging) out of the box.
>>> >>
>>> >> If one has the srcref, one can always load the absolutely correct
>>> source
>>> >> code this way, even if one doesn't know the parent function with
>>> >> the source attribute.
>>> >>
>>> >> Any comments?
>>> >
>>> > I think these requests have already been met. If you modify the body of
>>> > a closure (as trace() does), then the byte compiled version is
>>> > discarded, and you go back to the regular interpreted code. If you
>>> > install packages with the R_KEEP_PKG_SOURCE=yes environment variable
>>> > set, the you keep all source for all functions. (It's attached to the
>>> > function itself, not as a file that may be out of date.) It's possible
> 
> Can you expand on when files put inside a package at install time will be out of date compared to the source information attached to a function ?
> 

When you edit such files.


> I (naively) thought the source information was created and attached at install time as well and that it did not change afterwards either.
> 

... unless you edit it.


> I guess the arguments for files is that they have precise locations and allow for easy indexing by development tools external to R (but may be corrected here as well).
> 

Yes, but the moment you change a file it is no longer reflected in R unless you re-source it.

This is usually not an issue if you have a separate installed copy, but if you edit directly on the installed sources (something less frequent with lazy-loaded packages but more so in the old days), the files won't reflect what's actually parsed. This is a common problem, not specific to R, really. By keeping the sources with the objects, you guarantee that they match even if the sources files have been edited - useful for debugging. It not as esoteric as it sounds - just store a function in a workspace and then continue working on a project ...


>>> > that byte compiling turns off R_KEEP_PKG_SOURCE, but that is something
>>> > that is either easily fixed, or avoided by re-installing without byte
>>> > compiling.
>>> 
>>> Many thanks for your reaction. Is the R_KEEP_PKG_SOURCE=yes environment
>>> variable also supported during R installation ?
>> 
>> Yes, other than the error you saw below, which is a temporary problem.
>> Not sure which function exceeded the length limit, but the length limit
>> is going away before 2.14.0 is released.
> 
> Thanks again, Duncan, for the clarification.
> 
> Is it useful (or just whimsical) to have an R
> function that would allow for a given stock CRAN
> Windows R installation with stock Windows CRAN binary
> add-on packages to add the source information that
> would be useful e.g. for a debugger post factum?
> 
> I can imagine something like
> 
> update.packages(., checkSourcesKept = TRUE)
> 
> as I don't think this can currently be solved
> with a combination of INSTALL_opts="--with-keep.source"
> and type="source" given that there will not be a check
> for the presence of source information to determine
> which packages require being updated (or in this
> case 'completed' with source information).
> 
> The alternative scenario would be to expect users
> that want this functionality to compile R and all
> add-on packages from source (also on Windows or
> Mac).
> 

We are providing even debugging symbols in packages [on Mac OS X], so I suppose keeping sources is a probable option unless it has some significant speed penalty. However, AFAICS most of the discussion so far is quite pointless since it's based solely on speculations and misinterpretations of the original message.

Cheers,
Simon


> 
>>> I hope I'm not overlooking anything, but when compiling
>>> 
>>> ftp://ftp.stat.math.ethz.ch/Software/R/R-devel.tar.gz
>>> 
>>> a few minutes ago I encountered the following issue:
>>> 
>>> [...]
>>> 
>>> building package 'tools'
>>> mkdir -p -- ../../../library/tools
>>> make[4]: Entering directory
>>> `/home/tobias/rAdmin/R-devel/src/library/tools'
>>> mkdir -p -- ../../../library/tools/R
>>> mkdir -p -- ../../../library/tools/po
>>> make[4]: Leaving directory
>>> `/home/tobias/rAdmin/R-devel/src/library/tools'
>>> make[4]: Entering directory
>>> `/home/tobias/rAdmin/R-devel/src/library/tools'
>>> make[5]: Entering directory
>>> `/home/tobias/rAdmin/R-devel/src/library/tools/src'
>>> making text.d from text.c
>>> making init.d from init.c
>>> making Rmd5.d from Rmd5.c
>>> making md5.d from md5.c
>>> gcc -std=gnu99 -I../../../../include -I/usr/local/include
>>> -fvisibility=hidden -fpic -g -O2 -c text.c -o text.o
>>> gcc -std=gnu99 -I../../../../include -I/usr/local/include
>>> -fvisibility=hidden -fpic -g -O2 -c init.c -o init.o
>>> gcc -std=gnu99 -I../../../../include -I/usr/local/include
>>> -fvisibility=hidden -fpic -g -O2 -c Rmd5.c -o Rmd5.o
>>> gcc -std=gnu99 -I../../../../include -I/usr/local/include
>>> -fvisibility=hidden -fpic -g -O2 -c md5.c -o md5.o
>>> gcc -std=gnu99 -shared -L/usr/local/lib64 -o tools.so text.o init.o
>>> Rmd5.o md5.o -L../../../../lib -lR
>>> make[6]: Entering directory
>>> `/home/tobias/rAdmin/R-devel/src/library/tools/src'
>>> make[6]: `Makedeps' is up to date.
>>> make[6]: Leaving directory
>>> `/home/tobias/rAdmin/R-devel/src/library/tools/src'
>>> make[6]: Entering directory
>>> `/home/tobias/rAdmin/R-devel/src/library/tools/src'
>>> mkdir -p -- ../../../../library/tools/libs
>>> make[6]: Leaving directory
>>> `/home/tobias/rAdmin/R-devel/src/library/tools/src'
>>> make[5]: Leaving directory
>>> `/home/tobias/rAdmin/R-devel/src/library/tools/src'
>>> make[4]: Leaving directory
>>> `/home/tobias/rAdmin/R-devel/src/library/tools'
>>> Error in parse(n = -1, file = file) :
>>> function is too long to keep source (at line 2967)
>>> Error: unable to load R code in package ?tools?
>>> Execution halted
>>> 
>>> [...]
>>> 
>>> tobias at openanalytics:~/rAdmin$ echo $R_KEEP_PKG_SOURCE
>>> yes
>>> 
>>> I do not have this issue when R_KEEP_PKG_SOURCE is set
>>> to 'false' during compilation.
>>> 
>>> Best,
>>> Tobias
>>> 
>>> >> P.S. One could even consider a post-install option e.g. to add 'real'
>>> >> R sources (and source references) to Windows packages (which are by
>>> >> definition already 'installed' and for which such information is not
>>> >> by default included in the CRAN binaries of these packages).
>>> >>
>>> >> >> > Prof Brian Ripley wrote:
>>> >> >> > There was an R-core meeting the week before last, and various
>>> >> planned
>>> >> >> > changes will appear in R-devel over the next few weeks.
>>> >> >> >
>>> >> >> > These are changes planned for R 2.14.0 scheduled for Oct 31.
>>> As we
>>> >> >> > are sick of people referring to R-devel as '2.14' or '2.14.0',
>>> that
>>> >> >> > version number will not be used until we reach 2.14.0 alpha. You
>>> >> >> > will be able to have a package depend on an svn version number
>>> when
>>> >> >> > referring to R-devel rather than using R (>= 2.14.0).
>>> >> >> >
>>> >> >> > All packages are installed with lazy-loading (there were 72 CRAN
>>> >> >> > packages and 8 BioC packages which opted out). This means that
>>> the
>>> >> >> > code is always parsed at install time which inter alia simplifies
>>> >> the
>>> >> >> > descriptions. R 2.13.1 RC warns on installation about packages
>>> which
>>> >> >> > ask not to be lazy-loaded, and R-devel ignores such requests
>>> (with a
>>> >> >> > warning).
>>> >> >> >
>>> >> >> > In the near future all packages will have a name space. If the
>>> >> >> > sources do not contain one, a default NAMESPACE file will be
>>> added.
>>> >> >> > This again will simplify the descriptions and also a lot of
>>> internal
>>> >> >> > code. Maintainers of packages without name spaces (currently
>>> 42% of
>>> >> >> > CRAN) are encouraged to add one themselves.
>>> >> >> >
>>> >> >> > R-devel is installed with the base and recommended packages
>>> >> >> > byte-compiled (the equivalent of 'make bytecode' in R 2.13.x, but
>>> >> >> > done less inefficiently). There is a new option R CMD INSTALL
>>> >> >> > --byte-compile to byte-compile contributed packages, but that
>>> >> remains
>>> >> >> > optional.
>>> >> >> > Byte-compilation is quite expensive (so you definitely want to
>>> do it
>>> >> >> > at install time, which requires lazy-loading), and relatively few
>>> >> >> > packages benefit appreciably from byte-compilation. A larger
>>> number
>>> >> >> > of packages benefit from byte-compilation of R itself: for
>>> example
>>> >> >> > AER runs its checks 10% faster. The byte-compiler technology is
>>> >> >> > thanks to Luke Tierney.
>>> >> >> >
>>> >> >> > There is support for figures in Rd files: currently with a
>>> >> first-pass
>>> >> >> > implementation (thanks to Duncan Murdoch).
>>> >> > ______________________________________________
>>> >> > R-devel at r-project.org mailing list
>>> >> > https://stat.ethz.ch/mailman/listinfo/r-devel
>>> >> >
>>> >>
>>> >> ______________________________________________
>>> >> R-devel at r-project.org mailing list
>>> >> https://stat.ethz.ch/mailman/listinfo/r-devel
>>> >
>>> 
>> 
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 
> 


From murdoch.duncan at gmail.com  Tue Jul  5 20:16:29 2011
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Tue, 05 Jul 2011 14:16:29 -0400
Subject: [Rd] Recent and upcoming changes to R-devel
In-Reply-To: <4E134DA3.2040600@openanalytics.eu>
References: <alpine.OSX.1.00.1107041124460.59432@tystie.local>	<3D59DF35E7CF3E42BA70C23BD5CBE6A85F4C199E22@exvic-mbx04.nexus.csiro.au>
	<4E12ED06.5040603@openanalytics.eu> <4E1310D9.6050901@gmail.com>
	<4E131CF5.5000206@openanalytics.eu> <4E131DFF.7060808@gmail.com>
	<4E134DA3.2040600@openanalytics.eu>
Message-ID: <4E1354FD.9010903@gmail.com>

On 05/07/2011 1:45 PM, Tobias Verbeke wrote:
> On 07/05/2011 04:21 PM, Duncan Murdoch wrote:
> >  On 05/07/2011 10:17 AM, Tobias Verbeke wrote:
> >>  Dear Duncan,
> >>
> >>  On 07/05/2011 03:25 PM, Duncan Murdoch wrote:
> >>  >  On 05/07/2011 6:52 AM, Tobias Verbeke wrote:
> >>  >>  L.S.
> >>  >>
> >>  >>  On 07/05/2011 02:16 AM, Mark.Bravington at csiro.au wrote:
> >>  >>  >  I may have misunderstood, but:
> >>  >>  >
> >>  >>  >  Please could we have an optional installation that does not*not*
> >>  >>  byte-compile base and recommended?
> >>  >>  >
> >>  >>  >  Reason: it's not possible to debug byte-compiled code-- at least not
> >>  >>  with the 'debug' package, which is quite widely used. I quite often
> >>  >>  end up using 'mtrace' on functions in base/recommended packages to
> >>  >>  figure out what they are doing. And sometimes I (and others)
> >>  >>  experiment with changing functions in base/recommended to improve
> >>  >>  functionality. That seems to be harder with BC versions, and might
> >>  >>  even be impossible, as best I can tell from hints in the documentation
> >>  >>  of 'compile').
> >>  >>  >
> >>  >>  >  Personally, if I had to choose only one, I'd rather live with the
> >>  >>  speed penalty from not byte-compiling. But of course, if both are
> >>  >>  available, I could install both.
> >>  >>
> >>  >>  I completely second this request. All speed improvements and the byte
> >>  >>  compiler in particular are leaps forward and I am very grateful and
> >>  >>  admiring towards the people that make this happen.
> >>  >>
> >>  >>  That being said, 'moving away' from the sources (with the lazy loading
> >>  >>  files and byte-compilation) may be a step back for R package
> >>  developers
> >>  >>  that (during development and maybe on separate development
> >>  installations
> >>  >>  [as opposed to production installations of R]) require
> >>  >>  the sources of all packages to be efficient in their work.
> >>  >>
> >>  >>  As many of you know there is an open source Eclipse/StatET visual
> >>  >>  debugger ready and for that application as well (similar to Mark's
> >>  >>  request) presence of non-compiled code is highly desirable.
> >>  >>
> >>  >>  For the particular purpose of debugging R packages, I would even plead
> >>  >>  to go beyond the current options and support the addition of an
> >>  >>  R package install option that allows to include the sources (e.g. in
> >>  >>  a standard folder Rsrc/) in installed packages.
> >>  >>
> >>  >>  I am fully aware that one can always fetch the source tarballs from
> >>  >>  CRAN for that purpose, but it would be much more easy if a simple
> >>  >>  installation option could put the R sources of a package in a separate
> >>  >>  folder [or archive inside an existing folder] such that R development
> >>  >>  tools (such as the Eclipse/StatET IDE) can offer inspection of sources
> >>  >>  or display them (e.g. during debugging) out of the box.
> >>  >>
> >>  >>  If one has the srcref, one can always load the absolutely correct
> >>  source
> >>  >>  code this way, even if one doesn't know the parent function with
> >>  >>  the source attribute.
> >>  >>
> >>  >>  Any comments?
> >>  >
> >>  >  I think these requests have already been met. If you modify the body of
> >>  >  a closure (as trace() does), then the byte compiled version is
> >>  >  discarded, and you go back to the regular interpreted code. If you
> >>  >  install packages with the R_KEEP_PKG_SOURCE=yes environment variable
> >>  >  set, the you keep all source for all functions. (It's attached to the
> >>  >  function itself, not as a file that may be out of date.) It's possible
>
> Can you expand on when files put inside a package at install
> time will be out of date compared to the source information
> attached to a function ?

Suppose you're debugging.  You change a function, source it:  now it's 
not the same as the one in the package source, it's the one in your editor.

> I (naively) thought the source information was created and attached
> at install time as well and that it did not change afterwards either.

It won't change if the function doesn't change, but during debugging (or 
in some strange examples, during normal execution) the function might 
change.

> I guess the arguments for files is that they have precise
> locations and allow for easy indexing by development tools
> external to R (but may be corrected here as well).

As in pre-2.13.0, it will keep the locations and time stamps of the 
files, but we were finding it was too unreliable not to have an actual 
copy of the contents, so 2.13.0 also keeps a copy of the file, and 
that's the main source of content to display.

> >>  >  that byte compiling turns off R_KEEP_PKG_SOURCE, but that is something
> >>  >  that is either easily fixed, or avoided by re-installing without byte
> >>  >  compiling.
> >>
> >>  Many thanks for your reaction. Is the R_KEEP_PKG_SOURCE=yes environment
> >>  variable also supported during R installation ?
> >
> >  Yes, other than the error you saw below, which is a temporary problem.
> >  Not sure which function exceeded the length limit, but the length limit
> >  is going away before 2.14.0 is released.
>
> Thanks again, Duncan, for the clarification.
>
> Is it useful (or just whimsical) to have an R
> function that would allow for a given stock CRAN
> Windows R installation with stock Windows CRAN binary
> add-on packages to add the source information that
> would be useful e.g. for a debugger post factum?
>
> I can imagine something like
>
> update.packages(., checkSourcesKept = TRUE)

I suspect it would be hard to do that for base, tools and compiler, 
because those packages are handled specially during installation.  The 
other base packages (and all contributed packages) are handled more 
similarly, so install.packages() (with the right arguments) should do it 
(though I admit I haven't tried doing this with the other base 
packages.  If you're debugging those, you'll often end up looking at R 
internals, and then you need to be able to build R...).

If you want to have binary copies of packages on CRAN that include the 
debug info, I suspect you'll get some resistance from CRAN, because 
they'll add a lot to the file size, processing time, etc., for 
relatively rare use.

> as I don't think this can currently be solved
> with a combination of INSTALL_opts="--with-keep.source"
> and type="source" given that there will not be a check
> for the presence of source information to determine
> which packages require being updated (or in this
> case 'completed' with source information).
>
> The alternative scenario would be to expect users
> that want this functionality to compile R and all
> add-on packages from source (also on Windows or
> Mac).

I don't think you need to compile R except for those 3 packages (and 
maybe the other base packages), but I  I think it's reasonable to expect 
people who want source level debugging to be ready to do source installs 
of packages.  And back to the point above:  if a user can do a source 
install, they can do one with debugging info, so there's no real point 
in CRAN doing it for them.

Duncan Murdoch

> Best,
> Tobias
>
> >>  I hope I'm not overlooking anything, but when compiling
> >>
> >>  ftp://ftp.stat.math.ethz.ch/Software/R/R-devel.tar.gz
> >>
> >>  a few minutes ago I encountered the following issue:
> >>
> >>  [...]
> >>
> >>  building package 'tools'
> >>  mkdir -p -- ../../../library/tools
> >>  make[4]: Entering directory
> >>  `/home/tobias/rAdmin/R-devel/src/library/tools'
> >>  mkdir -p -- ../../../library/tools/R
> >>  mkdir -p -- ../../../library/tools/po
> >>  make[4]: Leaving directory
> >>  `/home/tobias/rAdmin/R-devel/src/library/tools'
> >>  make[4]: Entering directory
> >>  `/home/tobias/rAdmin/R-devel/src/library/tools'
> >>  make[5]: Entering directory
> >>  `/home/tobias/rAdmin/R-devel/src/library/tools/src'
> >>  making text.d from text.c
> >>  making init.d from init.c
> >>  making Rmd5.d from Rmd5.c
> >>  making md5.d from md5.c
> >>  gcc -std=gnu99 -I../../../../include -I/usr/local/include
> >>  -fvisibility=hidden -fpic -g -O2 -c text.c -o text.o
> >>  gcc -std=gnu99 -I../../../../include -I/usr/local/include
> >>  -fvisibility=hidden -fpic -g -O2 -c init.c -o init.o
> >>  gcc -std=gnu99 -I../../../../include -I/usr/local/include
> >>  -fvisibility=hidden -fpic -g -O2 -c Rmd5.c -o Rmd5.o
> >>  gcc -std=gnu99 -I../../../../include -I/usr/local/include
> >>  -fvisibility=hidden -fpic -g -O2 -c md5.c -o md5.o
> >>  gcc -std=gnu99 -shared -L/usr/local/lib64 -o tools.so text.o init.o
> >>  Rmd5.o md5.o -L../../../../lib -lR
> >>  make[6]: Entering directory
> >>  `/home/tobias/rAdmin/R-devel/src/library/tools/src'
> >>  make[6]: `Makedeps' is up to date.
> >>  make[6]: Leaving directory
> >>  `/home/tobias/rAdmin/R-devel/src/library/tools/src'
> >>  make[6]: Entering directory
> >>  `/home/tobias/rAdmin/R-devel/src/library/tools/src'
> >>  mkdir -p -- ../../../../library/tools/libs
> >>  make[6]: Leaving directory
> >>  `/home/tobias/rAdmin/R-devel/src/library/tools/src'
> >>  make[5]: Leaving directory
> >>  `/home/tobias/rAdmin/R-devel/src/library/tools/src'
> >>  make[4]: Leaving directory
> >>  `/home/tobias/rAdmin/R-devel/src/library/tools'
> >>  Error in parse(n = -1, file = file) :
> >>  function is too long to keep source (at line 2967)
> >>  Error: unable to load R code in package ?tools?
> >>  Execution halted
> >>
> >>  [...]
> >>
> >>  tobias at openanalytics:~/rAdmin$ echo $R_KEEP_PKG_SOURCE
> >>  yes
> >>
> >>  I do not have this issue when R_KEEP_PKG_SOURCE is set
> >>  to 'false' during compilation.
> >>
> >>  Best,
> >>  Tobias
> >>
> >>  >>  P.S. One could even consider a post-install option e.g. to add 'real'
> >>  >>  R sources (and source references) to Windows packages (which are by
> >>  >>  definition already 'installed' and for which such information is not
> >>  >>  by default included in the CRAN binaries of these packages).
> >>  >>
> >>  >>  >>  >  Prof Brian Ripley wrote:
> >>  >>  >>  >  There was an R-core meeting the week before last, and various
> >>  >>  planned
> >>  >>  >>  >  changes will appear in R-devel over the next few weeks.
> >>  >>  >>  >
> >>  >>  >>  >  These are changes planned for R 2.14.0 scheduled for Oct 31.
> >>  As we
> >>  >>  >>  >  are sick of people referring to R-devel as '2.14' or '2.14.0',
> >>  that
> >>  >>  >>  >  version number will not be used until we reach 2.14.0 alpha. You
> >>  >>  >>  >  will be able to have a package depend on an svn version number
> >>  when
> >>  >>  >>  >  referring to R-devel rather than using R (>= 2.14.0).
> >>  >>  >>  >
> >>  >>  >>  >  All packages are installed with lazy-loading (there were 72 CRAN
> >>  >>  >>  >  packages and 8 BioC packages which opted out). This means that
> >>  the
> >>  >>  >>  >  code is always parsed at install time which inter alia simplifies
> >>  >>  the
> >>  >>  >>  >  descriptions. R 2.13.1 RC warns on installation about packages
> >>  which
> >>  >>  >>  >  ask not to be lazy-loaded, and R-devel ignores such requests
> >>  (with a
> >>  >>  >>  >  warning).
> >>  >>  >>  >
> >>  >>  >>  >  In the near future all packages will have a name space. If the
> >>  >>  >>  >  sources do not contain one, a default NAMESPACE file will be
> >>  added.
> >>  >>  >>  >  This again will simplify the descriptions and also a lot of
> >>  internal
> >>  >>  >>  >  code. Maintainers of packages without name spaces (currently
> >>  42% of
> >>  >>  >>  >  CRAN) are encouraged to add one themselves.
> >>  >>  >>  >
> >>  >>  >>  >  R-devel is installed with the base and recommended packages
> >>  >>  >>  >  byte-compiled (the equivalent of 'make bytecode' in R 2.13.x, but
> >>  >>  >>  >  done less inefficiently). There is a new option R CMD INSTALL
> >>  >>  >>  >  --byte-compile to byte-compile contributed packages, but that
> >>  >>  remains
> >>  >>  >>  >  optional.
> >>  >>  >>  >  Byte-compilation is quite expensive (so you definitely want to
> >>  do it
> >>  >>  >>  >  at install time, which requires lazy-loading), and relatively few
> >>  >>  >>  >  packages benefit appreciably from byte-compilation. A larger
> >>  number
> >>  >>  >>  >  of packages benefit from byte-compilation of R itself: for
> >>  example
> >>  >>  >>  >  AER runs its checks 10% faster. The byte-compiler technology is
> >>  >>  >>  >  thanks to Luke Tierney.
> >>  >>  >>  >
> >>  >>  >>  >  There is support for figures in Rd files: currently with a
> >>  >>  first-pass
> >>  >>  >>  >  implementation (thanks to Duncan Murdoch).
> >>  >>  >  ______________________________________________
> >>  >>  >  R-devel at r-project.org mailing list
> >>  >>  >  https://stat.ethz.ch/mailman/listinfo/r-devel
> >>  >>  >
> >>  >>
> >>  >>  ______________________________________________
> >>  >>  R-devel at r-project.org mailing list
> >>  >>  https://stat.ethz.ch/mailman/listinfo/r-devel
> >>  >
> >>
> >
>


From simon.urbanek at r-project.org  Tue Jul  5 21:18:26 2011
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Tue, 5 Jul 2011 15:18:26 -0400
Subject: [Rd] [datatable-help]  speeding up perception
In-Reply-To: <1309889303.4231.60.camel@netbook>
References: <CAPr7RtU44GO-7_m02-ek+ps3YcR_qr3m7vV8BSvhAYJ=F_bnbA@mail.gmail.com>
	<CAPr7RtVx1ao9bubQiCrf9LjgpdZdB24vSnjZvGxW4a8Sy-pKag@mail.gmail.com>
	<CAGam+C6yEFw-E+oT7tktDVY8L9KtJZ0aHrQ_y0VLyEMnZDmfSw@mail.gmail.com>
	<555C7B55-67D9-4DC6-90CB-BE8B0F620BFB@r-project.org>
	<1309851165.4231.40.camel@netbook>
	<1309889303.4231.60.camel@netbook>
Message-ID: <E0D40E65-E676-4083-A058-090E3D2CC20C@r-project.org>


On Jul 5, 2011, at 2:08 PM, Matthew Dowle wrote:

> Simon (and all),
> 
> I've tried to make assignment as fast as calling `[<-.data.table`
> directly, for user convenience. Profiling shows (IIUC) that it isn't
> dispatch, but x being copied. Is there a way to prevent '[<-' from
> copying x?

Good point, and conceptually, no. It's a subassignment after all - see R-lang 3.4.4 - it is equivalent to 

`*tmp*` <- x
x <- `[<-`(`*tmp*`, i, j, value)
rm(`*tmp*`)

so there is always a copy involved.

Now, a conceptual copy doesn't mean real copy in R since R tries to keep the pass-by-value illusion while passing references in cases where it knows that modifications cannot occur and/or they are safe. The default subassign method uses that feature which means it can afford to not duplicate if there is only one reference -- then it's safe to not duplicate as we are replacing that only existing reference. And in the case of a matrix, that will be true at the latest from the second subassignment on.

Unfortunately the method dispatch (AFAICS) introduces one more reference in the dispatch chain so there will always be two references so duplication is necessary. Since we have only 0 / 1 / 2+ information on the references, we can't distinguish whether the second reference is due to the dispatch or due to the passed object having more than one reference, so we have to duplicate in any case. That is unfortunate, and I don't see a way around (unless we handle subassignment methods is some special way).

Cheers,
Simon



>  Small reproducible example in vanilla R 2.13.0 :
> 
>> x = list(a=1:10000,b=1:10000)
>> class(x) = "newclass"
>> "[<-.newclass" = function(x,i,j,value) x      # i.e. do nothing
>> tracemem(x)
> [1] "<0xa1ec758>"
>> x[1,2] = 42L
> tracemem[0xa1ec758 -> 0xa1ec558]:    # but, x is still copied, why?
>> 
> 
> I've tried returning NULL from [<-.newclass but then x gets assigned
> NULL :
> 
>> "[<-.newclass" = function(x,i,j,value) NULL
>> x[1,2] = 42L
> tracemem[0xa1ec558 -> 0x9c5f318]: 
>> x
> NULL
>> 
> 
> Any pointers much appreciated. If that copy is preventable it should
> save the user needing to use `[<-.data.table`(...) syntax to get the
> best speed (20 times faster on the small example used so far).
> 
> Matthew
> 
> 
> On Tue, 2011-07-05 at 08:32 +0100, Matthew Dowle wrote:
>> Simon,
>> 
>> Thanks for the great suggestion. I've written a skeleton assignment
>> function for data.table which incurs no copies, which works for this
>> case. For completeness, if I understand correctly, this is for : 
>>  i) convenience of new users who don't know how to vectorize yet
>>  ii) more complex examples which can't be vectorized.
>> 
>> Before:
>> 
>>> system.time(for (r in 1:R) DT[r,20] <- 1.0)
>>   user  system elapsed 
>> 12.792   0.488  13.340 
>> 
>> After :
>> 
>>> system.time(for (r in 1:R) DT[r,20] <- 1.0)
>>   user  system elapsed 
>>  2.908   0.020   2.935
>> 
>> Where this can be reduced further as follows :
>> 
>>> system.time(for (r in 1:R) `[<-.data.table`(DT,r,2,1.0))
>>   user  system elapsed 
>>  0.132   0.000   0.131 
>>> 
>> 
>> Still working on it. When it doesn't break other data.table tests, I'll
>> commit to R-Forge ...
>> 
>> Matthew
>> 
>> 
>> On Mon, 2011-07-04 at 12:41 -0400, Simon Urbanek wrote:
>>> Timoth?e,
>>> 
>>> On Jul 4, 2011, at 2:47 AM, Timoth?e Carayol wrote:
>>> 
>>>> Hi --
>>>> 
>>>> It's my first post on this list; as a relatively new user with little
>>>> knowledge of R internals, I am a bit intimidated by the depth of some
>>>> of the discussions here, so please spare me if I say something
>>>> incredibly silly.
>>>> 
>>>> I feel that someone at this point should mention Matthew Dowle's
>>>> excellent data.table package
>>>> (http://cran.r-project.org/web/packages/data.table/index.html) which
>>>> seems to me to address many of the inefficiencies of data.frame.
>>>> data.tables have no row names; and operations that only need data from
>>>> one or two columns are (I believe) just as quick whether the total
>>>> number of columns is 5 or 1000. This results in very quick operations
>>>> (and, often, elegant code as well).
>>>> 
>>> 
>>> I agree that data.table is a very good alternative (for other reasons) that should be promoted more. The only slight snag is that it doesn't help with the issue at hand since it simply does a pass-though for subassignments to data frame's methods and thus suffers from the same problems (in fact there is a rather stark asymmetry in how it handles subsetting vs subassignment - which is a bit surprising [if I read the code correctly you can't use the same indexing in both]). In fact I would propose that it should not do that but handle the simple cases itself more efficiently without unneeded copies. That would make it indeed a very interesting alternative.
>>> 
>>> Cheers,
>>> Simon
>>> 
>>> 
>>>> 
>>>> On Mon, Jul 4, 2011 at 6:19 AM, ivo welch <ivo.welch at gmail.com> wrote:
>>>>> thank you, simon.  this was very interesting indeed.  I also now
>>>>> understand how far out of my depth I am here.
>>>>> 
>>>>> fortunately, as an end user, obviously, *I* now know how to avoid the
>>>>> problem.  I particularly like the as.list() transformation and back to
>>>>> as.data.frame() to speed things up without loss of (much)
>>>>> functionality.
>>>>> 
>>>>> 
>>>>> more broadly, I view the avoidance of individual access through the
>>>>> use of apply and vector operations as a mixed "IQ test" and "knowledge
>>>>> test" (which I often fail).  However, even for the most clever, there
>>>>> are also situations where the KISS programming principle makes
>>>>> explicit loops still preferable.  Personally, I would have preferred
>>>>> it if R had, in its standard "statistical data set" data structure,
>>>>> foregone the row names feature in exchange for retaining fast direct
>>>>> access.  R could have reserved its current implementation "with row
>>>>> names but slow access" for a less common (possibly pseudo-inheriting)
>>>>> data structure.
>>>>> 
>>>>> 
>>>>> If end users commonly do iterations over a data frame, which I would
>>>>> guess to be the case, then the impression of R by (novice) end users
>>>>> could be greatly enhanced if the extreme penalties could be eliminated
>>>>> or at least flagged.  For example, I wonder if modest special internal
>>>>> code could store data frames internally and transparently as lists of
>>>>> vectors UNTIL a row name is assigned to.  Easier and uglier, a simple
>>>>> but specific warning message could be issued with a suggestion if
>>>>> there is an individual read/write into a data frame ("Warning: data
>>>>> frames are much slower than lists of vectors for individual element
>>>>> access").
>>>>> 
>>>>> 
>>>>> I would also suggest changing the "Introduction to R" 6.3  from "A
>>>>> data frame may for many purposes be regarded as a matrix with columns
>>>>> possibly of differing modes and attributes. It may be displayed in
>>>>> matrix form, and its rows and columns extracted using matrix indexing
>>>>> conventions." to "A data frame may for many purposes be regarded as a
>>>>> matrix with columns possibly of differing modes and attributes. It may
>>>>> be displayed in matrix form, and its rows and columns extracted using
>>>>> matrix indexing conventions.  However, data frames can be much slower
>>>>> than matrices or even lists of vectors (which, like data frames, can
>>>>> contain different types of columns) when individual elements need to
>>>>> be accessed."  Reading about it immediately upon introduction could
>>>>> flag the problem in a more visible manner.
>>>>> 
>>>>> 
>>>>> regards,
>>>>> 
>>>>> /iaw
>>>>> 
>>>>> ______________________________________________
>>>>> R-devel at r-project.org mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>> 
>>>> 
>>>> ______________________________________________
>>>> R-devel at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>> 
>>>> 
>>> 
>>> _______________________________________________
>>> datatable-help mailing list
>>> datatable-help at lists.r-forge.r-project.org
>>> https://lists.r-forge.r-project.org/cgi-bin/mailman/listinfo/datatable-help
>> 
>> 
>> _______________________________________________
>> datatable-help mailing list
>> datatable-help at lists.r-forge.r-project.org
>> https://lists.r-forge.r-project.org/cgi-bin/mailman/listinfo/datatable-help
> 
> 
> 


From hadley at rice.edu  Wed Jul  6 00:29:39 2011
From: hadley at rice.edu (Hadley Wickham)
Date: Tue, 5 Jul 2011 18:29:39 -0400
Subject: [Rd] Syntactically valid names
In-Reply-To: <2698F67A-DDF9-4EAF-B7A8-A4FAAE9CCC6B@gmail.com>
References: <BANLkTimg=TM0hYA1=Obknb+486Hv8T7Fdw@mail.gmail.com>
	<2698F67A-DDF9-4EAF-B7A8-A4FAAE9CCC6B@gmail.com>
Message-ID: <CABdHhvHmRP6XOkTq+4Z1NjvW1SRwqt1AUwga_ovbMOfi3zi5oA@mail.gmail.com>

> I wouldn't expect so. The basic structure might be handled using a regexp of sorts, but even that is tricky because of the "dot not followed by number" rule, and then there's the stop list of reserved words, which would make your code clumsy whatever you do.
>
> How on Earth would you expect anything to be significantly more elegant than your
>
> function(x) x == make.names(x)
>
> anyway??! (OK, if there was a wrapper for the C level isValidName() function...)

Good point.  Thanks!

Hadley

-- 
Assistant Professor / Dobelman Family Junior Chair
Department of Statistics / Rice University
http://had.co.nz/


From luke-tierney at uiowa.edu  Wed Jul  6 01:07:30 2011
From: luke-tierney at uiowa.edu (luke-tierney at uiowa.edu)
Date: Tue, 5 Jul 2011 18:07:30 -0500
Subject: [Rd] [datatable-help]  speeding up perception
In-Reply-To: <1309889303.4231.60.camel@netbook>
References: <CAPr7RtU44GO-7_m02-ek+ps3YcR_qr3m7vV8BSvhAYJ=F_bnbA@mail.gmail.com>
	<CAPr7RtVx1ao9bubQiCrf9LjgpdZdB24vSnjZvGxW4a8Sy-pKag@mail.gmail.com>
	<CAGam+C6yEFw-E+oT7tktDVY8L9KtJZ0aHrQ_y0VLyEMnZDmfSw@mail.gmail.com>
	<555C7B55-67D9-4DC6-90CB-BE8B0F620BFB@r-project.org>
	<1309851165.4231.40.camel@netbook> <1309889303.4231.60.camel@netbook>
Message-ID: <alpine.DEB.2.00.1107051804280.1928@luke-inspiron>

On Tue, 5 Jul 2011, Matthew Dowle wrote:

> Simon (and all),
>
> I've tried to make assignment as fast as calling `[<-.data.table`
> directly, for user convenience. Profiling shows (IIUC) that it isn't
> dispatch, but x being copied. Is there a way to prevent '[<-' from
> copying x?  Small reproducible example in vanilla R 2.13.0 :
>
>> x = list(a=1:10000,b=1:10000)
>> class(x) = "newclass"
>> "[<-.newclass" = function(x,i,j,value) x      # i.e. do nothing
>> tracemem(x)
> [1] "<0xa1ec758>"
>> x[1,2] = 42L
> tracemem[0xa1ec758 -> 0xa1ec558]:    # but, x is still copied, why?
>>

This one is a red herring -- the class(x) <- "newclass" assignment is
bumping up the NAMED value and as a result the following assignment
needs to duplicate. (the primitive class<- could be modified to avoid
the NAMED bump but it's fairly intricate code so I'm not going to look
into it now).

[A bit more later in reply to Simon's message]

luke

>
> I've tried returning NULL from [<-.newclass but then x gets assigned
> NULL :
>
>> "[<-.newclass" = function(x,i,j,value) NULL
>> x[1,2] = 42L
> tracemem[0xa1ec558 -> 0x9c5f318]: 
>> x
> NULL
>> 
>
> Any pointers much appreciated. If that copy is preventable it should
> save the user needing to use `[<-.data.table`(...) syntax to get the
> best speed (20 times faster on the small example used so far).
>
> Matthew
>
>
> On Tue, 2011-07-05 at 08:32 +0100, Matthew Dowle wrote:
>> Simon,
>> 
>> Thanks for the great suggestion. I've written a skeleton assignment
>> function for data.table which incurs no copies, which works for this
>> case. For completeness, if I understand correctly, this is for :
>>   i) convenience of new users who don't know how to vectorize yet
>>   ii) more complex examples which can't be vectorized.
>> 
>> Before:
>> 
>> > system.time(for (r in 1:R) DT[r,20] <- 1.0)
>>    user  system elapsed
>>  12.792   0.488  13.340 
>> 
>> After :
>> 
>> > system.time(for (r in 1:R) DT[r,20] <- 1.0)
>>    user  system elapsed
>>   2.908   0.020   2.935
>> 
>> Where this can be reduced further as follows :
>> 
>> > system.time(for (r in 1:R) `[<-.data.table`(DT,r,2,1.0))
>>    user  system elapsed
>>   0.132   0.000   0.131 
>> > 
>> 
>> Still working on it. When it doesn't break other data.table tests, I'll
>> commit to R-Forge ...
>> 
>> Matthew
>> 
>> 
>> On Mon, 2011-07-04 at 12:41 -0400, Simon Urbanek wrote:
>> > Timoth?e,
>> > 
>> > On Jul 4, 2011, at 2:47 AM, Timoth?e Carayol wrote:
>> > 
>> > > Hi --
>> > > 
>> > > It's my first post on this list; as a relatively new user with little
>> > > knowledge of R internals, I am a bit intimidated by the depth of some
>> > > of the discussions here, so please spare me if I say something
>> > > incredibly silly.
>> > > 
>> > > I feel that someone at this point should mention Matthew Dowle's
>> > > excellent data.table package
>> > > (http://cran.r-project.org/web/packages/data.table/index.html) which
>> > > seems to me to address many of the inefficiencies of data.frame.
>> > > data.tables have no row names; and operations that only need data from
>> > > one or two columns are (I believe) just as quick whether the total
>> > > number of columns is 5 or 1000. This results in very quick operations
>> > > (and, often, elegant code as well).
>> > > 
>> > 
>> > I agree that data.table is a very good alternative (for other reasons) that should be promoted more. The only slight snag is that it doesn't help with the issue at hand since it simply does a pass-though for subassignments to data frame's methods and thus suffers from the same problems (in fact there is a rather stark asymmetry in how it handles subsetting vs subassignment - which is a bit surprising [if I read the code correctly you can't use the same indexing in both]). In fact I would propose that it should not do that but handle the simple cases itself more efficiently without unneeded copies. That would make it indeed a very interesting alternative.
>> > 
>> > Cheers,
>> > Simon
>> > 
>> > 
>> > > 
>> > > On Mon, Jul 4, 2011 at 6:19 AM, ivo welch <ivo.welch at gmail.com> wrote:
>> > >> thank you, simon.  this was very interesting indeed.  I also now
>> > >> understand how far out of my depth I am here.
>> > >> 
>> > >> fortunately, as an end user, obviously, *I* now know how to avoid the
>> > >> problem.  I particularly like the as.list() transformation and back to
>> > >> as.data.frame() to speed things up without loss of (much)
>> > >> functionality.
>> > >> 
>> > >> 
>> > >> more broadly, I view the avoidance of individual access through the
>> > >> use of apply and vector operations as a mixed "IQ test" and "knowledge
>> > >> test" (which I often fail).  However, even for the most clever, there
>> > >> are also situations where the KISS programming principle makes
>> > >> explicit loops still preferable.  Personally, I would have preferred
>> > >> it if R had, in its standard "statistical data set" data structure,
>> > >> foregone the row names feature in exchange for retaining fast direct
>> > >> access.  R could have reserved its current implementation "with row
>> > >> names but slow access" for a less common (possibly pseudo-inheriting)
>> > >> data structure.
>> > >> 
>> > >> 
>> > >> If end users commonly do iterations over a data frame, which I would
>> > >> guess to be the case, then the impression of R by (novice) end users
>> > >> could be greatly enhanced if the extreme penalties could be eliminated
>> > >> or at least flagged.  For example, I wonder if modest special internal
>> > >> code could store data frames internally and transparently as lists of
>> > >> vectors UNTIL a row name is assigned to.  Easier and uglier, a simple
>> > >> but specific warning message could be issued with a suggestion if
>> > >> there is an individual read/write into a data frame ("Warning: data
>> > >> frames are much slower than lists of vectors for individual element
>> > >> access").
>> > >> 
>> > >> 
>> > >> I would also suggest changing the "Introduction to R" 6.3  from "A
>> > >> data frame may for many purposes be regarded as a matrix with columns
>> > >> possibly of differing modes and attributes. It may be displayed in
>> > >> matrix form, and its rows and columns extracted using matrix indexing
>> > >> conventions." to "A data frame may for many purposes be regarded as a
>> > >> matrix with columns possibly of differing modes and attributes. It may
>> > >> be displayed in matrix form, and its rows and columns extracted using
>> > >> matrix indexing conventions.  However, data frames can be much slower
>> > >> than matrices or even lists of vectors (which, like data frames, can
>> > >> contain different types of columns) when individual elements need to
>> > >> be accessed."  Reading about it immediately upon introduction could
>> > >> flag the problem in a more visible manner.
>> > >> 
>> > >> 
>> > >> regards,
>> > >> 
>> > >> /iaw
>> > >> 
>> > >> ______________________________________________
>> > >> R-devel at r-project.org mailing list
>> > >> https://stat.ethz.ch/mailman/listinfo/r-devel
>> > >> 
>> > > 
>> > > ______________________________________________
>> > > R-devel at r-project.org mailing list
>> > > https://stat.ethz.ch/mailman/listinfo/r-devel
>> > > 
>> > > 
>> > 
>> > _______________________________________________
>> > datatable-help mailing list
>> > datatable-help at lists.r-forge.r-project.org
>> > https://lists.r-forge.r-project.org/cgi-bin/mailman/listinfo/datatable-help
>> 
>> 
>> _______________________________________________
>> datatable-help mailing list
>> datatable-help at lists.r-forge.r-project.org
>> https://lists.r-forge.r-project.org/cgi-bin/mailman/listinfo/datatable-help
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
Luke Tierney
Statistics and Actuarial Science
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu

From luke-tierney at uiowa.edu  Wed Jul  6 01:18:58 2011
From: luke-tierney at uiowa.edu (luke-tierney at uiowa.edu)
Date: Tue, 5 Jul 2011 18:18:58 -0500
Subject: [Rd] [datatable-help]  speeding up perception
In-Reply-To: <E0D40E65-E676-4083-A058-090E3D2CC20C@r-project.org>
References: <CAPr7RtU44GO-7_m02-ek+ps3YcR_qr3m7vV8BSvhAYJ=F_bnbA@mail.gmail.com>
	<CAPr7RtVx1ao9bubQiCrf9LjgpdZdB24vSnjZvGxW4a8Sy-pKag@mail.gmail.com>
	<CAGam+C6yEFw-E+oT7tktDVY8L9KtJZ0aHrQ_y0VLyEMnZDmfSw@mail.gmail.com>
	<555C7B55-67D9-4DC6-90CB-BE8B0F620BFB@r-project.org>
	<1309851165.4231.40.camel@netbook> <1309889303.4231.60.camel@netbook>
	<E0D40E65-E676-4083-A058-090E3D2CC20C@r-project.org>
Message-ID: <alpine.DEB.2.00.1107051808040.1928@luke-inspiron>

On Tue, 5 Jul 2011, Simon Urbanek wrote:

>
> On Jul 5, 2011, at 2:08 PM, Matthew Dowle wrote:
>
>> Simon (and all),
>>
>> I've tried to make assignment as fast as calling `[<-.data.table`
>> directly, for user convenience. Profiling shows (IIUC) that it isn't
>> dispatch, but x being copied. Is there a way to prevent '[<-' from
>> copying x?
>
> Good point, and conceptually, no. It's a subassignment after all - see R-lang 3.4.4 - it is equivalent to
>
> `*tmp*` <- x
> x <- `[<-`(`*tmp*`, i, j, value)
> rm(`*tmp*`)
>
> so there is always a copy involved.
>
> Now, a conceptual copy doesn't mean real copy in R since R tries to keep the pass-by-value illusion while passing references in cases where it knows that modifications cannot occur and/or they are safe. The default subassign method uses that feature which means it can afford to not duplicate if there is only one reference -- then it's safe to not duplicate as we are replacing that only existing reference. And in the case of a matrix, that will be true at the latest from the second subassignment on.
>
> Unfortunately the method dispatch (AFAICS) introduces one more reference in the dispatch chain so there will always be two references so duplication is necessary. Since we have only 0 / 1 / 2+ information on the references, we can't distinguish whether the second reference is due to the dispatch or due to the passed object having more than one reference, so we have to duplicate in any case. That is unfortunate, and I don't see a way around (unless we handle subassignment methods is some special way).

I don't believe dispatch is bumping NAMED (and a quick experiment
seems to confirm this though I don't guarantee I did that right). The
issue is that a replacement function implemented as a closure, which
is the only option for a package, will always see NAMED on the object
to be modified as 2 (because the value is obtained by forcing the
argument promise) and so any R level assignments will duplicate.  This
also isn't really an issue of imprecise reference counting -- there
really are (at least) two legitimate references -- one though the
argument and one through the caller's environment.

It would be good it we could come up with a way for packages to be
able to define replacement functions that do not duplicate in cases
where we really don't want them to, but this would require coming up
with some sort of protocol, minimally involving an efficient way to
detect whether a replacement funciton is bing called in a replacement
context or directly.

There are some replacement functions that use C code to cheat, but
these may create problems if called directly, so I won't advertise
them.

Best,

luke

>
> Cheers,
> Simon
>
>
>
>>  Small reproducible example in vanilla R 2.13.0 :
>>
>>> x = list(a=1:10000,b=1:10000)
>>> class(x) = "newclass"
>>> "[<-.newclass" = function(x,i,j,value) x      # i.e. do nothing
>>> tracemem(x)
>> [1] "<0xa1ec758>"
>>> x[1,2] = 42L
>> tracemem[0xa1ec758 -> 0xa1ec558]:    # but, x is still copied, why?
>>>
>>
>> I've tried returning NULL from [<-.newclass but then x gets assigned
>> NULL :
>>
>>> "[<-.newclass" = function(x,i,j,value) NULL
>>> x[1,2] = 42L
>> tracemem[0xa1ec558 -> 0x9c5f318]:
>>> x
>> NULL
>>>
>>
>> Any pointers much appreciated. If that copy is preventable it should
>> save the user needing to use `[<-.data.table`(...) syntax to get the
>> best speed (20 times faster on the small example used so far).
>>
>> Matthew
>>
>>
>> On Tue, 2011-07-05 at 08:32 +0100, Matthew Dowle wrote:
>>> Simon,
>>>
>>> Thanks for the great suggestion. I've written a skeleton assignment
>>> function for data.table which incurs no copies, which works for this
>>> case. For completeness, if I understand correctly, this is for :
>>>  i) convenience of new users who don't know how to vectorize yet
>>>  ii) more complex examples which can't be vectorized.
>>>
>>> Before:
>>>
>>>> system.time(for (r in 1:R) DT[r,20] <- 1.0)
>>>   user  system elapsed
>>> 12.792   0.488  13.340
>>>
>>> After :
>>>
>>>> system.time(for (r in 1:R) DT[r,20] <- 1.0)
>>>   user  system elapsed
>>>  2.908   0.020   2.935
>>>
>>> Where this can be reduced further as follows :
>>>
>>>> system.time(for (r in 1:R) `[<-.data.table`(DT,r,2,1.0))
>>>   user  system elapsed
>>>  0.132   0.000   0.131
>>>>
>>>
>>> Still working on it. When it doesn't break other data.table tests, I'll
>>> commit to R-Forge ...
>>>
>>> Matthew
>>>
>>>
>>> On Mon, 2011-07-04 at 12:41 -0400, Simon Urbanek wrote:
>>>> Timoth?e,
>>>>
>>>> On Jul 4, 2011, at 2:47 AM, Timoth?e Carayol wrote:
>>>>
>>>>> Hi --
>>>>>
>>>>> It's my first post on this list; as a relatively new user with little
>>>>> knowledge of R internals, I am a bit intimidated by the depth of some
>>>>> of the discussions here, so please spare me if I say something
>>>>> incredibly silly.
>>>>>
>>>>> I feel that someone at this point should mention Matthew Dowle's
>>>>> excellent data.table package
>>>>> (http://cran.r-project.org/web/packages/data.table/index.html) which
>>>>> seems to me to address many of the inefficiencies of data.frame.
>>>>> data.tables have no row names; and operations that only need data from
>>>>> one or two columns are (I believe) just as quick whether the total
>>>>> number of columns is 5 or 1000. This results in very quick operations
>>>>> (and, often, elegant code as well).
>>>>>
>>>>
>>>> I agree that data.table is a very good alternative (for other reasons) that should be promoted more. The only slight snag is that it doesn't help with the issue at hand since it simply does a pass-though for subassignments to data frame's methods and thus suffers from the same problems (in fact there is a rather stark asymmetry in how it handles subsetting vs subassignment - which is a bit surprising [if I read the code correctly you can't use the same indexing in both]). In fact I would propose that it should not do that but handle the simple cases itself more efficiently without unneeded copies. That would make it indeed a very interesting alternative.
>>>>
>>>> Cheers,
>>>> Simon
>>>>
>>>>
>>>>>
>>>>> On Mon, Jul 4, 2011 at 6:19 AM, ivo welch <ivo.welch at gmail.com> wrote:
>>>>>> thank you, simon.  this was very interesting indeed.  I also now
>>>>>> understand how far out of my depth I am here.
>>>>>>
>>>>>> fortunately, as an end user, obviously, *I* now know how to avoid the
>>>>>> problem.  I particularly like the as.list() transformation and back to
>>>>>> as.data.frame() to speed things up without loss of (much)
>>>>>> functionality.
>>>>>>
>>>>>>
>>>>>> more broadly, I view the avoidance of individual access through the
>>>>>> use of apply and vector operations as a mixed "IQ test" and "knowledge
>>>>>> test" (which I often fail).  However, even for the most clever, there
>>>>>> are also situations where the KISS programming principle makes
>>>>>> explicit loops still preferable.  Personally, I would have preferred
>>>>>> it if R had, in its standard "statistical data set" data structure,
>>>>>> foregone the row names feature in exchange for retaining fast direct
>>>>>> access.  R could have reserved its current implementation "with row
>>>>>> names but slow access" for a less common (possibly pseudo-inheriting)
>>>>>> data structure.
>>>>>>
>>>>>>
>>>>>> If end users commonly do iterations over a data frame, which I would
>>>>>> guess to be the case, then the impression of R by (novice) end users
>>>>>> could be greatly enhanced if the extreme penalties could be eliminated
>>>>>> or at least flagged.  For example, I wonder if modest special internal
>>>>>> code could store data frames internally and transparently as lists of
>>>>>> vectors UNTIL a row name is assigned to.  Easier and uglier, a simple
>>>>>> but specific warning message could be issued with a suggestion if
>>>>>> there is an individual read/write into a data frame ("Warning: data
>>>>>> frames are much slower than lists of vectors for individual element
>>>>>> access").
>>>>>>
>>>>>>
>>>>>> I would also suggest changing the "Introduction to R" 6.3  from "A
>>>>>> data frame may for many purposes be regarded as a matrix with columns
>>>>>> possibly of differing modes and attributes. It may be displayed in
>>>>>> matrix form, and its rows and columns extracted using matrix indexing
>>>>>> conventions." to "A data frame may for many purposes be regarded as a
>>>>>> matrix with columns possibly of differing modes and attributes. It may
>>>>>> be displayed in matrix form, and its rows and columns extracted using
>>>>>> matrix indexing conventions.  However, data frames can be much slower
>>>>>> than matrices or even lists of vectors (which, like data frames, can
>>>>>> contain different types of columns) when individual elements need to
>>>>>> be accessed."  Reading about it immediately upon introduction could
>>>>>> flag the problem in a more visible manner.
>>>>>>
>>>>>>
>>>>>> regards,
>>>>>>
>>>>>> /iaw
>>>>>>
>>>>>> ______________________________________________
>>>>>> R-devel at r-project.org mailing list
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>>>
>>>>>
>>>>> ______________________________________________
>>>>> R-devel at r-project.org mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>>
>>>>>
>>>>
>>>> _______________________________________________
>>>> datatable-help mailing list
>>>> datatable-help at lists.r-forge.r-project.org
>>>> https://lists.r-forge.r-project.org/cgi-bin/mailman/listinfo/datatable-help
>>>
>>>
>>> _______________________________________________
>>> datatable-help mailing list
>>> datatable-help at lists.r-forge.r-project.org
>>> https://lists.r-forge.r-project.org/cgi-bin/mailman/listinfo/datatable-help
>>
>>
>>
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

-- 
Luke Tierney
Statistics and Actuarial Science
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu

From hadley at rice.edu  Wed Jul  6 01:40:17 2011
From: hadley at rice.edu (Hadley Wickham)
Date: Tue, 5 Jul 2011 19:40:17 -0400
Subject: [Rd] Syntactically valid names
In-Reply-To: <CAFFLneRp4YY7U+Nn26C+hFhX-4PWaHZMFizBmgEtNg2jjK93qg@mail.gmail.com>
References: <BANLkTimg=TM0hYA1=Obknb+486Hv8T7Fdw@mail.gmail.com>
	<2698F67A-DDF9-4EAF-B7A8-A4FAAE9CCC6B@gmail.com>
	<CABdHhvHmRP6XOkTq+4Z1NjvW1SRwqt1AUwga_ovbMOfi3zi5oA@mail.gmail.com>
	<CAFFLneRp4YY7U+Nn26C+hFhX-4PWaHZMFizBmgEtNg2jjK93qg@mail.gmail.com>
Message-ID: <CABdHhvEhKGZjSX_fRAAi6_PjHgnhOra_f7w8Y+R8Pw8gGm92OQ@mail.gmail.com>

On Tue, Jul 5, 2011 at 7:31 PM, steven mosher <moshersteven at gmail.com> wrote:
> ?regexp approach is kinda ugly
> http://www.r-bloggers.com/testing-for-valid-variable-names/

Hmm, I think that suggests a couple of small bug in make.names:

> make.names("...")
[1] "..."
> make.names("..1")
[1] "..1"

and

> x <- paste(rep("x", 1e6), collapse = "")
> x == make.names(x)
[1] TRUE


Hadley

-- 
Assistant Professor / Dobelman Family Junior Chair
Department of Statistics / Rice University
http://had.co.nz/


From moshersteven at gmail.com  Wed Jul  6 01:31:09 2011
From: moshersteven at gmail.com (steven mosher)
Date: Tue, 5 Jul 2011 16:31:09 -0700
Subject: [Rd] Syntactically valid names
In-Reply-To: <CABdHhvHmRP6XOkTq+4Z1NjvW1SRwqt1AUwga_ovbMOfi3zi5oA@mail.gmail.com>
References: <BANLkTimg=TM0hYA1=Obknb+486Hv8T7Fdw@mail.gmail.com>
	<2698F67A-DDF9-4EAF-B7A8-A4FAAE9CCC6B@gmail.com>
	<CABdHhvHmRP6XOkTq+4Z1NjvW1SRwqt1AUwga_ovbMOfi3zi5oA@mail.gmail.com>
Message-ID: <CAFFLneRp4YY7U+Nn26C+hFhX-4PWaHZMFizBmgEtNg2jjK93qg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20110705/e4951f6a/attachment.pl>

From cubranic at stat.ubc.ca  Wed Jul  6 01:55:28 2011
From: cubranic at stat.ubc.ca (Davor Cubranic)
Date: Tue, 5 Jul 2011 16:55:28 -0700
Subject: [Rd] Syntactically valid names
In-Reply-To: <BANLkTimg=TM0hYA1=Obknb+486Hv8T7Fdw@mail.gmail.com>
References: <BANLkTimg=TM0hYA1=Obknb+486Hv8T7Fdw@mail.gmail.com>
Message-ID: <201107051655.28642.cubranic@stat.ubc.ca>

On June 30, 2011 01:37:57 PM Hadley Wickham wrote:

> Is there any easy way to tell if a string is a syntactically valid name?
[...]
> 
> One implementation would be:
> 
> is.syntactic <- function(x) x == make.names(x)
> 
> but I wonder if there's a more elegant way.

This is without quoting, right? Because "make.names" replaces spaces with 
periods, and using quoting I can create syntactically valid names that do 
include spaces:

    `x prime` <- 3
    ls()

Davor


From hadley at rice.edu  Wed Jul  6 01:59:16 2011
From: hadley at rice.edu (Hadley Wickham)
Date: Tue, 5 Jul 2011 19:59:16 -0400
Subject: [Rd] Syntactically valid names
In-Reply-To: <201107051655.28642.cubranic@stat.ubc.ca>
References: <BANLkTimg=TM0hYA1=Obknb+486Hv8T7Fdw@mail.gmail.com>
	<201107051655.28642.cubranic@stat.ubc.ca>
Message-ID: <CABdHhvGqxsfep6UXxp+7JRXXjWMMA8x4WEgk3GSxDqA0AY0qKw@mail.gmail.com>

> This is without quoting, right? Because "make.names" replaces spaces with
> periods, and using quoting I can create syntactically valid names that do
> include spaces:
>
> ? ?`x prime` <- 3
> ? ?ls()

That's not a syntactically valid name - you use backticks to refer to
names that are not syntactically valid.

Hadley

-- 
Assistant Professor / Dobelman Family Junior Chair
Department of Statistics / Rice University
http://had.co.nz/


From pdalgd at gmail.com  Wed Jul  6 02:22:04 2011
From: pdalgd at gmail.com (peter dalgaard)
Date: Wed, 6 Jul 2011 02:22:04 +0200
Subject: [Rd] Syntactically valid names
In-Reply-To: <CABdHhvEhKGZjSX_fRAAi6_PjHgnhOra_f7w8Y+R8Pw8gGm92OQ@mail.gmail.com>
References: <BANLkTimg=TM0hYA1=Obknb+486Hv8T7Fdw@mail.gmail.com>
	<2698F67A-DDF9-4EAF-B7A8-A4FAAE9CCC6B@gmail.com>
	<CABdHhvHmRP6XOkTq+4Z1NjvW1SRwqt1AUwga_ovbMOfi3zi5oA@mail.gmail.com>
	<CAFFLneRp4YY7U+Nn26C+hFhX-4PWaHZMFizBmgEtNg2jjK93qg@mail.gmail.com>
	<CABdHhvEhKGZjSX_fRAAi6_PjHgnhOra_f7w8Y+R8Pw8gGm92OQ@mail.gmail.com>
Message-ID: <A16B0AB7-C30E-482D-B91A-EC3CC2C70CB9@gmail.com>


On Jul 6, 2011, at 01:40 , Hadley Wickham wrote:

> On Tue, Jul 5, 2011 at 7:31 PM, steven mosher <moshersteven at gmail.com> wrote:
>>  regexp approach is kinda ugly
>> http://www.r-bloggers.com/testing-for-valid-variable-names/
> 
> Hmm, I think that suggests a couple of small bug in make.names:
> 
>> make.names("...")
> [1] "..."
>> make.names("..1")
> [1] "..1"
> 

What's wrong with that? They are names alright, just with special meanings.

> x <- quote(...)
> mode(x)
[1] "name"


> and
> 
>> x <- paste(rep("x", 1e6), collapse = "")
>> x == make.names(x)
> [1] TRUE
> 
> 

Mildly insane, but technically OK, no?


> Hadley
> 
> -- 
> Assistant Professor / Dobelman Family Junior Chair
> Department of Statistics / Rice University
> http://had.co.nz/

-- 
Peter Dalgaard
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From cubranic at stat.ubc.ca  Wed Jul  6 03:00:45 2011
From: cubranic at stat.ubc.ca (Davor Cubranic)
Date: Tue, 5 Jul 2011 18:00:45 -0700
Subject: [Rd] Syntactically valid names
In-Reply-To: <CABdHhvGqxsfep6UXxp+7JRXXjWMMA8x4WEgk3GSxDqA0AY0qKw@mail.gmail.com>
References: <BANLkTimg=TM0hYA1=Obknb+486Hv8T7Fdw@mail.gmail.com>
	<201107051655.28642.cubranic@stat.ubc.ca>
	<CABdHhvGqxsfep6UXxp+7JRXXjWMMA8x4WEgk3GSxDqA0AY0qKw@mail.gmail.com>
Message-ID: <201107051800.46172.cubranic@stat.ubc.ca>

On July 5, 2011 04:59:16 PM Hadley Wickham wrote:
> That's not a syntactically valid name - you use backticks to refer to
> names that are not syntactically valid.

I was too loose in my terminology: I meant that `x prime` is a valid name, but 
as you said, it is not syntactically valid.

Davor


From dwinsemius at comcast.net  Wed Jul  6 03:01:52 2011
From: dwinsemius at comcast.net (David Winsemius)
Date: Tue, 5 Jul 2011 21:01:52 -0400
Subject: [Rd] [datatable-help]  speeding up perception
In-Reply-To: <alpine.DEB.2.00.1107051808040.1928@luke-inspiron>
References: <CAPr7RtU44GO-7_m02-ek+ps3YcR_qr3m7vV8BSvhAYJ=F_bnbA@mail.gmail.com>
	<CAPr7RtVx1ao9bubQiCrf9LjgpdZdB24vSnjZvGxW4a8Sy-pKag@mail.gmail.com>
	<CAGam+C6yEFw-E+oT7tktDVY8L9KtJZ0aHrQ_y0VLyEMnZDmfSw@mail.gmail.com>
	<555C7B55-67D9-4DC6-90CB-BE8B0F620BFB@r-project.org>
	<1309851165.4231.40.camel@netbook>
	<1309889303.4231.60.camel@netbook>
	<E0D40E65-E676-4083-A058-090E3D2CC20C@r-project.org>
	<alpine.DEB.2.00.1107051808040.1928@luke-inspiron>
Message-ID: <121657AE-3C91-4880-BC29-781A78B05278@comcast.net>


On Jul 5, 2011, at 7:18 PM, <luke-tierney at uiowa.edu> <luke-tierney at uiowa.edu 
 > wrote:

> On Tue, 5 Jul 2011, Simon Urbanek wrote:
>
>>
>> On Jul 5, 2011, at 2:08 PM, Matthew Dowle wrote:
>>
>>> Simon (and all),
>>>
>>> I've tried to make assignment as fast as calling `[<-.data.table`
>>> directly, for user convenience. Profiling shows (IIUC) that it isn't
>>> dispatch, but x being copied. Is there a way to prevent '[<-' from
>>> copying x?
>>
>> Good point, and conceptually, no. It's a subassignment after all -  
>> see R-lang 3.4.4 - it is equivalent to
>>
>> `*tmp*` <- x
>> x <- `[<-`(`*tmp*`, i, j, value)
>> rm(`*tmp*`)
>>
>> so there is always a copy involved.
>>
>> Now, a conceptual copy doesn't mean real copy in R since R tries to  
>> keep the pass-by-value illusion while passing references in cases  
>> where it knows that modifications cannot occur and/or they are  
>> safe. The default subassign method uses that feature which means it  
>> can afford to not duplicate if there is only one reference -- then  
>> it's safe to not duplicate as we are replacing that only existing  
>> reference. And in the case of a matrix, that will be true at the  
>> latest from the second subassignment on.
>>
>> Unfortunately the method dispatch (AFAICS) introduces one more  
>> reference in the dispatch chain so there will always be two  
>> references so duplication is necessary. Since we have only 0 / 1 /  
>> 2+ information on the references, we can't distinguish whether the  
>> second reference is due to the dispatch or due to the passed object  
>> having more than one reference, so we have to duplicate in any  
>> case. That is unfortunate, and I don't see a way around (unless we  
>> handle subassignment methods is some special way).
>
> I don't believe dispatch is bumping NAMED (and a quick experiment
> seems to confirm this though I don't guarantee I did that right). The
> issue is that a replacement function implemented as a closure, which
> is the only option for a package, will always see NAMED on the object
> to be modified as 2 (because the value is obtained by forcing the
> argument promise) and so any R level assignments will duplicate.  This
> also isn't really an issue of imprecise reference counting -- there
> really are (at least) two legitimate references -- one though the
> argument and one through the caller's environment.
>
> It would be good it we could come up with a way for packages to be
> able to define replacement functions that do not duplicate in cases
> where we really don't want them to, but this would require coming up
> with some sort of protocol, minimally involving an efficient way to
> detect whether a replacement funciton is being called in a replacement
> context or directly.

Would "$<-" always satisfy that condition. It would be big help to me  
if it could be designed to avoid duplication the rest of the data.frame.

-- 

>
> There are some replacement functions that use C code to cheat, but
> these may create problems if called directly, so I won't advertise
> them.
>
> Best,
>
> luke
>
>>
>> Cheers,
>> Simon
>>
>>
>>
>
> -- 
> Luke Tierney
> Statistics and Actuarial Science
> Ralph E. Wareham Professor of Mathematical Sciences
> University of Iowa                  Phone:             319-335-3386
> Department of Statistics and        Fax:               319-335-3017
>   Actuarial Science
> 241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
> Iowa City, IA 52242                 WWW:  http:// 
> www.stat.uiowa.edu______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

David Winsemius, MD
West Hartford, CT


From simon.urbanek at r-project.org  Wed Jul  6 03:11:10 2011
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Tue, 5 Jul 2011 21:11:10 -0400
Subject: [Rd] [datatable-help]  speeding up perception
In-Reply-To: <121657AE-3C91-4880-BC29-781A78B05278@comcast.net>
References: <CAPr7RtU44GO-7_m02-ek+ps3YcR_qr3m7vV8BSvhAYJ=F_bnbA@mail.gmail.com>
	<CAPr7RtVx1ao9bubQiCrf9LjgpdZdB24vSnjZvGxW4a8Sy-pKag@mail.gmail.com>
	<CAGam+C6yEFw-E+oT7tktDVY8L9KtJZ0aHrQ_y0VLyEMnZDmfSw@mail.gmail.com>
	<555C7B55-67D9-4DC6-90CB-BE8B0F620BFB@r-project.org>
	<1309851165.4231.40.camel@netbook>
	<1309889303.4231.60.camel@netbook>
	<E0D40E65-E676-4083-A058-090E3D2CC20C@r-project.org>
	<alpine.DEB.2.00.1107051808040.1928@luke-inspiron>
	<121657AE-3C91-4880-BC29-781A78B05278@comcast.net>
Message-ID: <11C6B5AE-0B1F-4154-B21C-7A5B3ED08619@r-project.org>

No subassignment function satisfies that condition, because you can always call them directly. However, that doesn't stop the default method from making that assumption, so I'm not sure it's an issue.

David, Just to clarify - the data frame content is not copied, we are talking about the vector holding columns.

Cheers,
Simon

Sent from my iPhone

On Jul 5, 2011, at 9:01 PM, David Winsemius <dwinsemius at comcast.net> wrote:

> 
> On Jul 5, 2011, at 7:18 PM, <luke-tierney at uiowa.edu> <luke-tierney at uiowa.edu> wrote:
> 
>> On Tue, 5 Jul 2011, Simon Urbanek wrote:
>> 
>>> 
>>> On Jul 5, 2011, at 2:08 PM, Matthew Dowle wrote:
>>> 
>>>> Simon (and all),
>>>> 
>>>> I've tried to make assignment as fast as calling `[<-.data.table`
>>>> directly, for user convenience. Profiling shows (IIUC) that it isn't
>>>> dispatch, but x being copied. Is there a way to prevent '[<-' from
>>>> copying x?
>>> 
>>> Good point, and conceptually, no. It's a subassignment after all - see R-lang 3.4.4 - it is equivalent to
>>> 
>>> `*tmp*` <- x
>>> x <- `[<-`(`*tmp*`, i, j, value)
>>> rm(`*tmp*`)
>>> 
>>> so there is always a copy involved.
>>> 
>>> Now, a conceptual copy doesn't mean real copy in R since R tries to keep the pass-by-value illusion while passing references in cases where it knows that modifications cannot occur and/or they are safe. The default subassign method uses that feature which means it can afford to not duplicate if there is only one reference -- then it's safe to not duplicate as we are replacing that only existing reference. And in the case of a matrix, that will be true at the latest from the second subassignment on.
>>> 
>>> Unfortunately the method dispatch (AFAICS) introduces one more reference in the dispatch chain so there will always be two references so duplication is necessary. Since we have only 0 / 1 / 2+ information on the references, we can't distinguish whether the second reference is due to the dispatch or due to the passed object having more than one reference, so we have to duplicate in any case. That is unfortunate, and I don't see a way around (unless we handle subassignment methods is some special way).
>> 
>> I don't believe dispatch is bumping NAMED (and a quick experiment
>> seems to confirm this though I don't guarantee I did that right). The
>> issue is that a replacement function implemented as a closure, which
>> is the only option for a package, will always see NAMED on the object
>> to be modified as 2 (because the value is obtained by forcing the
>> argument promise) and so any R level assignments will duplicate.  This
>> also isn't really an issue of imprecise reference counting -- there
>> really are (at least) two legitimate references -- one though the
>> argument and one through the caller's environment.
>> 
>> It would be good it we could come up with a way for packages to be
>> able to define replacement functions that do not duplicate in cases
>> where we really don't want them to, but this would require coming up
>> with some sort of protocol, minimally involving an efficient way to
>> detect whether a replacement funciton is being called in a replacement
>> context or directly.
> 
> Would "$<-" always satisfy that condition. It would be big help to me if it could be designed to avoid duplication the rest of the data.frame.
> 
> -- 
> 
>> 
>> There are some replacement functions that use C code to cheat, but
>> these may create problems if called directly, so I won't advertise
>> them.
>> 
>> Best,
>> 
>> luke
>> 
>>> 
>>> Cheers,
>>> Simon
>>> 
>>> 
>>> 
>> 
>> -- 
>> Luke Tierney
>> Statistics and Actuarial Science
>> Ralph E. Wareham Professor of Mathematical Sciences
>> University of Iowa                  Phone:             319-335-3386
>> Department of Statistics and        Fax:               319-335-3017
>>  Actuarial Science
>> 241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
>> Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
> 
> David Winsemius, MD
> West Hartford, CT
> 
> 


From hadley at rice.edu  Wed Jul  6 03:25:35 2011
From: hadley at rice.edu (Hadley Wickham)
Date: Tue, 5 Jul 2011 21:25:35 -0400
Subject: [Rd] Syntactically valid names
In-Reply-To: <A16B0AB7-C30E-482D-B91A-EC3CC2C70CB9@gmail.com>
References: <BANLkTimg=TM0hYA1=Obknb+486Hv8T7Fdw@mail.gmail.com>
	<2698F67A-DDF9-4EAF-B7A8-A4FAAE9CCC6B@gmail.com>
	<CABdHhvHmRP6XOkTq+4Z1NjvW1SRwqt1AUwga_ovbMOfi3zi5oA@mail.gmail.com>
	<CAFFLneRp4YY7U+Nn26C+hFhX-4PWaHZMFizBmgEtNg2jjK93qg@mail.gmail.com>
	<CABdHhvEhKGZjSX_fRAAi6_PjHgnhOra_f7w8Y+R8Pw8gGm92OQ@mail.gmail.com>
	<A16B0AB7-C30E-482D-B91A-EC3CC2C70CB9@gmail.com>
Message-ID: <CABdHhvGAP7+0FeDtyBiKFtdnP6+oAnYjkEUpBQVChY5toVqHCA@mail.gmail.com>

> What's wrong with that? They are names alright, just with special meanings.

But you can't really use them for variables:

> ... <- 4
> ...
Error: '...' used in an incorrect context
> ..1 <- 4
> ..1
Error: 'nthcdr' needs a list to CDR down

And make.names generally protects you against that:

> make.names("function")
[1] "function."
> make.names("break")
[1] "break."
> make.names("TRUE")
[1] "TRUE."

>>> x <- paste(rep("x", 1e6), collapse = "")
>>> x == make.names(x)
>> [1] TRUE
>
> Mildly insane, but technically OK, no?

I don't think so:

> x <- paste(rep("x", 1e6), collapse = "")
> assign(x, 1)
Error in assign(x, 1) : variable names are limited to 10000 bytes

Hadley

-- 
Assistant Professor / Dobelman Family Junior Chair
Department of Statistics / Rice University
http://had.co.nz/


From stephan.wahlbrink at walware.de  Wed Jul  6 09:46:25 2011
From: stephan.wahlbrink at walware.de (Stephan Wahlbrink)
Date: Wed, 06 Jul 2011 09:46:25 +0200
Subject: [Rd] Recent and upcoming changes to R-devel
In-Reply-To: <4E132BFD.5090604@gmail.com>
References: <alpine.OSX.1.00.1107041124460.59432@tystie.local>	<3D59DF35E7CF3E42BA70C23BD5CBE6A85F4C199E22@exvic-mbx04.nexus.csiro.au>
	<4E12ED06.5040603@openanalytics.eu>
	<4E1310D9.6050901@gmail.com> <4E132BA0.5000003@walware.de>
	<4E132BFD.5090604@gmail.com>
Message-ID: <4E1412D1.2040700@walware.de>

Duncan Murdoch wrote [2011-07-05 17:21]:
> On 05/07/2011 11:20 AM, Stephan Wahlbrink wrote:
>> Dear developers,
>>
>> Duncan Murdoch wrote [2011-07-05 15:25]:
>> > On 05/07/2011 6:52 AM, Tobias Verbeke wrote:
>> >> L.S.
>> >>
>> >> On 07/05/2011 02:16 AM, Mark.Bravington at csiro.au wrote:
>> >> > I may have misunderstood, but:
>> >> >
>> >> > Please could we have an optional installation that does not*not*
>> >> byte-compile base and recommended?
>> >> >
>> >> > Reason: it's not possible to debug byte-compiled code-- at least not
>> >> with the 'debug' package, which is quite widely used. I quite often
>> >> end up using 'mtrace' on functions in base/recommended packages to
>> >> figure out what they are doing. And sometimes I (and others)
>> >> experiment with changing functions in base/recommended to improve
>> >> functionality. That seems to be harder with BC versions, and might
>> >> even be impossible, as best I can tell from hints in the documentation
>> >> of 'compile').
>> >> >
>> >> > Personally, if I had to choose only one, I'd rather live with the
>> >> speed penalty from not byte-compiling. But of course, if both are
>> >> available, I could install both.
>> >>
>> >> I completely second this request. All speed improvements and the byte
>> >> compiler in particular are leaps forward and I am very grateful and
>> >> admiring towards the people that make this happen.
>> >>
>> >> That being said, 'moving away' from the sources (with the lazy loading
>> >> files and byte-compilation) may be a step back for R package
>> developers
>> >> that (during development and maybe on separate development
>> installations
>> >> [as opposed to production installations of R]) require
>> >> the sources of all packages to be efficient in their work.
>> >>
>> >> As many of you know there is an open source Eclipse/StatET visual
>> >> debugger ready and for that application as well (similar to Mark's
>> >> request) presence of non-compiled code is highly desirable.
>> >>
>> >> For the particular purpose of debugging R packages, I would even plead
>> >> to go beyond the current options and support the addition of an
>> >> R package install option that allows to include the sources (e.g. in
>> >> a standard folder Rsrc/) in installed packages.
>> >>
>> >> I am fully aware that one can always fetch the source tarballs from
>> >> CRAN for that purpose, but it would be much more easy if a simple
>> >> installation option could put the R sources of a package in a separate
>> >> folder [or archive inside an existing folder] such that R development
>> >> tools (such as the Eclipse/StatET IDE) can offer inspection of sources
>> >> or display them (e.g. during debugging) out of the box.
>> >>
>> >> If one has the srcref, one can always load the absolutely correct
>> source
>> >> code this way, even if one doesn't know the parent function with
>> >> the source attribute.
>> >>
>> >> Any comments?
>> >
>> > I think these requests have already been met. If you modify the body of
>> > a closure (as trace() does), then the byte compiled version is
>> > discarded, and you go back to the regular interpreted code. If you
>> > install packages with the R_KEEP_PKG_SOURCE=yes environment variable
>> > set, the you keep all source for all functions. (It's attached to the
>> > function itself, not as a file that may be out of date.) It's possible
>> > that byte compiling turns off R_KEEP_PKG_SOURCE, but that is something
>> > that is either easily fixed, or avoided by re-installing without byte
>> > compiling.
>>
>> I don?t know how the new installation works exactly, but would it be
>> possible, to simply install both types, the old expression bodies and
>> the new byte-compiled, as single package at the same time?
>
> Yes, that's what is done.

Perfect! And which version (byte-compiled or expressions) is used at 
runtime under which condition?

Thanks,
Stephan


>> This would
>> allow the R user and developer to simply use the variant which is the
>> best at the moment. If he wants to debug code, he can switch of the use
>> of byte-compiled code and use the old R expressions (with attached
>> srcrefs). If debugging is not required, he can profit from the
>> byte-compiled version. The best would be a toggle, to switch it at
>> runtime, but a startup option would be sufficient too.
>>
>> I think direct access to the code is one big advantage of open source
>> software. For developer it makes it easier to find and fix bugs if
>> something is wrong. But it can also help users a lot to understand how a
>> function or algorithm works and learn from code written by other persons
>> ? if the access to the sources is easy.
>>
>> As long byte-code doesn?t support the debugging features of R, it is
>> required for best debugging support to run the functions completely
>> without byte-complied code. If I understood it correctly, byte-code
>> frames would disable srcrefs as well as features like ?step return? to
>> that frames. Therefore I ask for a way that it is easy to switch between
>> both execution types.
>
> What gave you that impression?
>
> Duncan Murdoch
>
>> Best,
>> Stephan
>>
>>
>> >
>> > Duncan Murdoch
>> >
>> >> Best,
>> >> Tobias
>> >>
>> >> P.S. One could even consider a post-install option e.g. to add 'real'
>> >> R sources (and source references) to Windows packages (which are by
>> >> definition already 'installed' and for which such information is not
>> >> by default included in the CRAN binaries of these packages).
>> >>
>> >> >> > Prof Brian Ripley wrote:
>> >> >> > There was an R-core meeting the week before last, and various
>> >> planned
>> >> >> > changes will appear in R-devel over the next few weeks.
>> >> >> >
>> >> >> > These are changes planned for R 2.14.0 scheduled for Oct 31.
>> As we
>> >> >> > are sick of people referring to R-devel as '2.14' or '2.14.0',
>> that
>> >> >> > version number will not be used until we reach 2.14.0 alpha. You
>> >> >> > will be able to have a package depend on an svn version number
>> when
>> >> >> > referring to R-devel rather than using R (>= 2.14.0).
>> >> >> >
>> >> >> > All packages are installed with lazy-loading (there were 72 CRAN
>> >> >> > packages and 8 BioC packages which opted out). This means that
>> the
>> >> >> > code is always parsed at install time which inter alia simplifies
>> >> the
>> >> >> > descriptions. R 2.13.1 RC warns on installation about packages
>> which
>> >> >> > ask not to be lazy-loaded, and R-devel ignores such requests
>> (with a
>> >> >> > warning).
>> >> >> >
>> >> >> > In the near future all packages will have a name space. If the
>> >> >> > sources do not contain one, a default NAMESPACE file will be
>> added.
>> >> >> > This again will simplify the descriptions and also a lot of
>> internal
>> >> >> > code. Maintainers of packages without name spaces (currently
>> 42% of
>> >> >> > CRAN) are encouraged to add one themselves.
>> >> >> >
>> >> >> > R-devel is installed with the base and recommended packages
>> >> >> > byte-compiled (the equivalent of 'make bytecode' in R 2.13.x, but
>> >> >> > done less inefficiently). There is a new option R CMD INSTALL
>> >> >> > --byte-compile to byte-compile contributed packages, but that
>> >> remains
>> >> >> > optional.
>> >> >> > Byte-compilation is quite expensive (so you definitely want to
>> do it
>> >> >> > at install time, which requires lazy-loading), and relatively few
>> >> >> > packages benefit appreciably from byte-compilation. A larger
>> number
>> >> >> > of packages benefit from byte-compilation of R itself: for
>> example
>> >> >> > AER runs its checks 10% faster. The byte-compiler technology is
>> >> >> > thanks to Luke Tierney.
>> >> >> >
>> >> >> > There is support for figures in Rd files: currently with a
>> >> first-pass
>> >> >> > implementation (thanks to Duncan Murdoch).
>>
>> --
>> Stephan Wahlbrink
>> Humboldtstr. 19
>> 44137 Dortmund
>> Germany
>> http://www.walware.de/goto/opensource
>>
>
>


From pdalgd at gmail.com  Wed Jul  6 10:10:04 2011
From: pdalgd at gmail.com (peter dalgaard)
Date: Wed, 6 Jul 2011 10:10:04 +0200
Subject: [Rd] Syntactically valid names
In-Reply-To: <CABdHhvGAP7+0FeDtyBiKFtdnP6+oAnYjkEUpBQVChY5toVqHCA@mail.gmail.com>
References: <BANLkTimg=TM0hYA1=Obknb+486Hv8T7Fdw@mail.gmail.com>
	<2698F67A-DDF9-4EAF-B7A8-A4FAAE9CCC6B@gmail.com>
	<CABdHhvHmRP6XOkTq+4Z1NjvW1SRwqt1AUwga_ovbMOfi3zi5oA@mail.gmail.com>
	<CAFFLneRp4YY7U+Nn26C+hFhX-4PWaHZMFizBmgEtNg2jjK93qg@mail.gmail.com>
	<CABdHhvEhKGZjSX_fRAAi6_PjHgnhOra_f7w8Y+R8Pw8gGm92OQ@mail.gmail.com>
	<A16B0AB7-C30E-482D-B91A-EC3CC2C70CB9@gmail.com>
	<CABdHhvGAP7+0FeDtyBiKFtdnP6+oAnYjkEUpBQVChY5toVqHCA@mail.gmail.com>
Message-ID: <99993C2E-69F9-4AD9-8484-38C1CF765944@gmail.com>


On Jul 6, 2011, at 03:25 , Hadley Wickham wrote:

>> What's wrong with that? They are names alright, just with special meanings.
> 
> But you can't really use them for variables:
> 
>> ... <- 4
>> ...
> Error: '...' used in an incorrect context
>> ..1 <- 4
>> ..1
> Error: 'nthcdr' needs a list to CDR down
> 
> And make.names generally protects you against that:
> 
>> make.names("function")
> [1] "function."
>> make.names("break")
> [1] "break."
>> make.names("TRUE")
> [1] "TRUE."


That's two different issues:

> y <- list()
> y$... <- 2
> y$..2 <- 3
> y$break <- 4
Error: unexpected 'break' in "y$break"

Notice that there is nothing _syntactically_ wrong with ... & friends as names:

> quote(...<-4)
... <- 4

It's the _evaluator_ that throws the error because the ...-name has a special interpretation.


> 
>>>> x <- paste(rep("x", 1e6), collapse = "")
>>>> x == make.names(x)
>>> [1] TRUE
>> 
>> Mildly insane, but technically OK, no?
> 
> I don't think so:
> 
>> x <- paste(rep("x", 1e6), collapse = "")
>> assign(x, 1)
> Error in assign(x, 1) : variable names are limited to 10000 bytes

But that's a sanity check, with an arbitrary cutoff.  A string of a million "x"s is syntactically a valid name, it's just the evaluator that refuses to play with it. (Not to say that it would be wrong to put in a similar sanity check in make.names.) 

-- 
Peter Dalgaard
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From mdowle at mdowle.plus.com  Wed Jul  6 10:36:05 2011
From: mdowle at mdowle.plus.com (Matthew Dowle)
Date: Wed, 06 Jul 2011 09:36:05 +0100
Subject: [Rd] [datatable-help]  speeding up perception
In-Reply-To: <11C6B5AE-0B1F-4154-B21C-7A5B3ED08619@r-project.org>
References: <CAPr7RtU44GO-7_m02-ek+ps3YcR_qr3m7vV8BSvhAYJ=F_bnbA@mail.gmail.com>
	<CAPr7RtVx1ao9bubQiCrf9LjgpdZdB24vSnjZvGxW4a8Sy-pKag@mail.gmail.com>
	<CAGam+C6yEFw-E+oT7tktDVY8L9KtJZ0aHrQ_y0VLyEMnZDmfSw@mail.gmail.com>
	<555C7B55-67D9-4DC6-90CB-BE8B0F620BFB@r-project.org>
	<1309851165.4231.40.camel@netbook> <1309889303.4231.60.camel@netbook>
	<E0D40E65-E676-4083-A058-090E3D2CC20C@r-project.org>
	<alpine.DEB.2.00.1107051808040.1928@luke-inspiron>
	<121657AE-3C91-4880-BC29-781A78B05278@comcast.net>
	<11C6B5AE-0B1F-4154-B21C-7A5B3ED08619@r-project.org>
Message-ID: <1309941365.4231.122.camel@netbook>


On Tue, 2011-07-05 at 21:11 -0400, Simon Urbanek wrote:
> No subassignment function satisfies that condition, because you can always call them directly. However, that doesn't stop the default method from making that assumption, so I'm not sure it's an issue.
> 
> David, Just to clarify - the data frame content is not copied, we are talking about the vector holding columns.

If it is just the vector holding the columns that is copied (and not the
columns themselves), why does n make a difference in this test (on R
2.13.0)?

> n = 1000
> x = data.frame(a=1:n,b=1:n)
> system.time(for (i in 1:1000) x[1,1] <- 42L)
   user  system elapsed 
  0.628   0.000   0.628 
> n = 100000
> x = data.frame(a=1:n,b=1:n)      # still 2 columns, but longer columns
> system.time(for (i in 1:1000) x[1,1] <- 42L)
   user  system elapsed 
 20.145   1.232  21.455 
> 

With $<- :

> n = 1000
> x = data.frame(a=1:n,b=1:n)
> system.time(for (i in 1:1000) x$a[1] <- 42L)
   user  system elapsed 
  0.304   0.000   0.307 
> n = 100000
> x = data.frame(a=1:n,b=1:n)
> system.time(for (i in 1:1000) x$a[1] <- 42L)
   user  system elapsed 
 37.586   0.388  38.161 
> 

If it's because the 1st column needs to be copied (only) because that's
the one being assigned to (in this test), that magnitude of slow down
doesn't seem consistent with the time of a vector copy of the 1st
column : 

> n=100000
> v = 1:n
> system.time(for (i in 1:1000) v[1] <- 42L)
   user  system elapsed 
  0.016   0.000   0.017 
> system.time(for (i in 1:1000) {v2=v;v2[1] <- 42L})
   user  system elapsed 
  1.816   1.076   2.900

Finally, increasing the number of columns, again only the 1st is
assigned to :

> n=100000
> x = data.frame(rep(list(1:n),100))
> dim(x)
[1] 100000    100
> system.time(for (i in 1:1000) x[1,1] <- 42L)
   user  system elapsed 
167.974  50.903 219.711 
> 



> 
> Cheers,
> Simon
> 
> Sent from my iPhone
> 
> On Jul 5, 2011, at 9:01 PM, David Winsemius <dwinsemius at comcast.net> wrote:
> 
> > 
> > On Jul 5, 2011, at 7:18 PM, <luke-tierney at uiowa.edu> <luke-tierney at uiowa.edu> wrote:
> > 
> >> On Tue, 5 Jul 2011, Simon Urbanek wrote:
> >> 
> >>> 
> >>> On Jul 5, 2011, at 2:08 PM, Matthew Dowle wrote:
> >>> 
> >>>> Simon (and all),
> >>>> 
> >>>> I've tried to make assignment as fast as calling `[<-.data.table`
> >>>> directly, for user convenience. Profiling shows (IIUC) that it isn't
> >>>> dispatch, but x being copied. Is there a way to prevent '[<-' from
> >>>> copying x?
> >>> 
> >>> Good point, and conceptually, no. It's a subassignment after all - see R-lang 3.4.4 - it is equivalent to
> >>> 
> >>> `*tmp*` <- x
> >>> x <- `[<-`(`*tmp*`, i, j, value)
> >>> rm(`*tmp*`)
> >>> 
> >>> so there is always a copy involved.
> >>> 
> >>> Now, a conceptual copy doesn't mean real copy in R since R tries to keep the pass-by-value illusion while passing references in cases where it knows that modifications cannot occur and/or they are safe. The default subassign method uses that feature which means it can afford to not duplicate if there is only one reference -- then it's safe to not duplicate as we are replacing that only existing reference. And in the case of a matrix, that will be true at the latest from the second subassignment on.
> >>> 
> >>> Unfortunately the method dispatch (AFAICS) introduces one more reference in the dispatch chain so there will always be two references so duplication is necessary. Since we have only 0 / 1 / 2+ information on the references, we can't distinguish whether the second reference is due to the dispatch or due to the passed object having more than one reference, so we have to duplicate in any case. That is unfortunate, and I don't see a way around (unless we handle subassignment methods is some special way).
> >> 
> >> I don't believe dispatch is bumping NAMED (and a quick experiment
> >> seems to confirm this though I don't guarantee I did that right). The
> >> issue is that a replacement function implemented as a closure, which
> >> is the only option for a package, will always see NAMED on the object
> >> to be modified as 2 (because the value is obtained by forcing the
> >> argument promise) and so any R level assignments will duplicate.  This
> >> also isn't really an issue of imprecise reference counting -- there
> >> really are (at least) two legitimate references -- one though the
> >> argument and one through the caller's environment.
> >> 
> >> It would be good it we could come up with a way for packages to be
> >> able to define replacement functions that do not duplicate in cases
> >> where we really don't want them to, but this would require coming up
> >> with some sort of protocol, minimally involving an efficient way to
> >> detect whether a replacement funciton is being called in a replacement
> >> context or directly.
> > 
> > Would "$<-" always satisfy that condition. It would be big help to me if it could be designed to avoid duplication the rest of the data.frame.
> > 
> > -- 
> > 
> >> 
> >> There are some replacement functions that use C code to cheat, but
> >> these may create problems if called directly, so I won't advertise
> >> them.
> >> 
> >> Best,
> >> 
> >> luke
> >> 
> >>> 
> >>> Cheers,
> >>> Simon
> >>> 
> >>> 
> >>> 
> >> 
> >> -- 
> >> Luke Tierney
> >> Statistics and Actuarial Science
> >> Ralph E. Wareham Professor of Mathematical Sciences
> >> University of Iowa                  Phone:             319-335-3386
> >> Department of Statistics and        Fax:               319-335-3017
> >>  Actuarial Science
> >> 241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
> >> Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu______________________________________________
> >> R-devel at r-project.org mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-devel
> > 
> > David Winsemius, MD
> > West Hartford, CT
> > 
> >


From murdoch.duncan at gmail.com  Wed Jul  6 13:43:51 2011
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Wed, 06 Jul 2011 07:43:51 -0400
Subject: [Rd] Recent and upcoming changes to R-devel
In-Reply-To: <4E1412D1.2040700@walware.de>
References: <alpine.OSX.1.00.1107041124460.59432@tystie.local>	<3D59DF35E7CF3E42BA70C23BD5CBE6A85F4C199E22@exvic-mbx04.nexus.csiro.au>
	<4E12ED06.5040603@openanalytics.eu>
	<4E1310D9.6050901@gmail.com> <4E132BA0.5000003@walware.de>
	<4E132BFD.5090604@gmail.com> <4E1412D1.2040700@walware.de>
Message-ID: <4E144A77.8090601@gmail.com>

On 11-07-06 3:46 AM, Stephan Wahlbrink wrote:
> Duncan Murdoch wrote [2011-07-05 17:21]:
>> On 05/07/2011 11:20 AM, Stephan Wahlbrink wrote:
>>> Dear developers,
>>>
>>> Duncan Murdoch wrote [2011-07-05 15:25]:
>>>> On 05/07/2011 6:52 AM, Tobias Verbeke wrote:
>>>>> L.S.
>>>>>
>>>>> On 07/05/2011 02:16 AM, Mark.Bravington at csiro.au wrote:
>>>>>> I may have misunderstood, but:
>>>>>>
>>>>>> Please could we have an optional installation that does not*not*
>>>>> byte-compile base and recommended?
>>>>>>
>>>>>> Reason: it's not possible to debug byte-compiled code-- at least not
>>>>> with the 'debug' package, which is quite widely used. I quite often
>>>>> end up using 'mtrace' on functions in base/recommended packages to
>>>>> figure out what they are doing. And sometimes I (and others)
>>>>> experiment with changing functions in base/recommended to improve
>>>>> functionality. That seems to be harder with BC versions, and might
>>>>> even be impossible, as best I can tell from hints in the documentation
>>>>> of 'compile').
>>>>>>
>>>>>> Personally, if I had to choose only one, I'd rather live with the
>>>>> speed penalty from not byte-compiling. But of course, if both are
>>>>> available, I could install both.
>>>>>
>>>>> I completely second this request. All speed improvements and the byte
>>>>> compiler in particular are leaps forward and I am very grateful and
>>>>> admiring towards the people that make this happen.
>>>>>
>>>>> That being said, 'moving away' from the sources (with the lazy loading
>>>>> files and byte-compilation) may be a step back for R package
>>> developers
>>>>> that (during development and maybe on separate development
>>> installations
>>>>> [as opposed to production installations of R]) require
>>>>> the sources of all packages to be efficient in their work.
>>>>>
>>>>> As many of you know there is an open source Eclipse/StatET visual
>>>>> debugger ready and for that application as well (similar to Mark's
>>>>> request) presence of non-compiled code is highly desirable.
>>>>>
>>>>> For the particular purpose of debugging R packages, I would even plead
>>>>> to go beyond the current options and support the addition of an
>>>>> R package install option that allows to include the sources (e.g. in
>>>>> a standard folder Rsrc/) in installed packages.
>>>>>
>>>>> I am fully aware that one can always fetch the source tarballs from
>>>>> CRAN for that purpose, but it would be much more easy if a simple
>>>>> installation option could put the R sources of a package in a separate
>>>>> folder [or archive inside an existing folder] such that R development
>>>>> tools (such as the Eclipse/StatET IDE) can offer inspection of sources
>>>>> or display them (e.g. during debugging) out of the box.
>>>>>
>>>>> If one has the srcref, one can always load the absolutely correct
>>> source
>>>>> code this way, even if one doesn't know the parent function with
>>>>> the source attribute.
>>>>>
>>>>> Any comments?
>>>>
>>>> I think these requests have already been met. If you modify the body of
>>>> a closure (as trace() does), then the byte compiled version is
>>>> discarded, and you go back to the regular interpreted code. If you
>>>> install packages with the R_KEEP_PKG_SOURCE=yes environment variable
>>>> set, the you keep all source for all functions. (It's attached to the
>>>> function itself, not as a file that may be out of date.) It's possible
>>>> that byte compiling turns off R_KEEP_PKG_SOURCE, but that is something
>>>> that is either easily fixed, or avoided by re-installing without byte
>>>> compiling.
>>>
>>> I don?t know how the new installation works exactly, but would it be
>>> possible, to simply install both types, the old expression bodies and
>>> the new byte-compiled, as single package at the same time?
>>
>> Yes, that's what is done.
>
> Perfect! And which version (byte-compiled or expressions) is used at
> runtime under which condition?

The byte code is executed, the interpreted version is displayed.  There 
are conditions under which the byte code is dropped.  I don't know a 
comprehensive list, but the idea is that if the body changes, then the 
compiled version is no longer valid.

Duncan Murdoch

>
> Thanks,
> Stephan
>
>
>>> This would
>>> allow the R user and developer to simply use the variant which is the
>>> best at the moment. If he wants to debug code, he can switch of the use
>>> of byte-compiled code and use the old R expressions (with attached
>>> srcrefs). If debugging is not required, he can profit from the
>>> byte-compiled version. The best would be a toggle, to switch it at
>>> runtime, but a startup option would be sufficient too.
>>>
>>> I think direct access to the code is one big advantage of open source
>>> software. For developer it makes it easier to find and fix bugs if
>>> something is wrong. But it can also help users a lot to understand how a
>>> function or algorithm works and learn from code written by other persons
>>> ? if the access to the sources is easy.
>>>
>>> As long byte-code doesn?t support the debugging features of R, it is
>>> required for best debugging support to run the functions completely
>>> without byte-complied code. If I understood it correctly, byte-code
>>> frames would disable srcrefs as well as features like ?step return? to
>>> that frames. Therefore I ask for a way that it is easy to switch between
>>> both execution types.
>>
>> What gave you that impression?
>>
>> Duncan Murdoch
>>
>>> Best,
>>> Stephan
>>>
>>>
>>>>
>>>> Duncan Murdoch
>>>>
>>>>> Best,
>>>>> Tobias
>>>>>
>>>>> P.S. One could even consider a post-install option e.g. to add 'real'
>>>>> R sources (and source references) to Windows packages (which are by
>>>>> definition already 'installed' and for which such information is not
>>>>> by default included in the CRAN binaries of these packages).
>>>>>
>>>>>>>> Prof Brian Ripley wrote:
>>>>>>>> There was an R-core meeting the week before last, and various
>>>>> planned
>>>>>>>> changes will appear in R-devel over the next few weeks.
>>>>>>>>
>>>>>>>> These are changes planned for R 2.14.0 scheduled for Oct 31.
>>> As we
>>>>>>>> are sick of people referring to R-devel as '2.14' or '2.14.0',
>>> that
>>>>>>>> version number will not be used until we reach 2.14.0 alpha. You
>>>>>>>> will be able to have a package depend on an svn version number
>>> when
>>>>>>>> referring to R-devel rather than using R (>= 2.14.0).
>>>>>>>>
>>>>>>>> All packages are installed with lazy-loading (there were 72 CRAN
>>>>>>>> packages and 8 BioC packages which opted out). This means that
>>> the
>>>>>>>> code is always parsed at install time which inter alia simplifies
>>>>> the
>>>>>>>> descriptions. R 2.13.1 RC warns on installation about packages
>>> which
>>>>>>>> ask not to be lazy-loaded, and R-devel ignores such requests
>>> (with a
>>>>>>>> warning).
>>>>>>>>
>>>>>>>> In the near future all packages will have a name space. If the
>>>>>>>> sources do not contain one, a default NAMESPACE file will be
>>> added.
>>>>>>>> This again will simplify the descriptions and also a lot of
>>> internal
>>>>>>>> code. Maintainers of packages without name spaces (currently
>>> 42% of
>>>>>>>> CRAN) are encouraged to add one themselves.
>>>>>>>>
>>>>>>>> R-devel is installed with the base and recommended packages
>>>>>>>> byte-compiled (the equivalent of 'make bytecode' in R 2.13.x, but
>>>>>>>> done less inefficiently). There is a new option R CMD INSTALL
>>>>>>>> --byte-compile to byte-compile contributed packages, but that
>>>>> remains
>>>>>>>> optional.
>>>>>>>> Byte-compilation is quite expensive (so you definitely want to
>>> do it
>>>>>>>> at install time, which requires lazy-loading), and relatively few
>>>>>>>> packages benefit appreciably from byte-compilation. A larger
>>> number
>>>>>>>> of packages benefit from byte-compilation of R itself: for
>>> example
>>>>>>>> AER runs its checks 10% faster. The byte-compiler technology is
>>>>>>>> thanks to Luke Tierney.
>>>>>>>>
>>>>>>>> There is support for figures in Rd files: currently with a
>>>>> first-pass
>>>>>>>> implementation (thanks to Duncan Murdoch).
>>>
>>> --
>>> Stephan Wahlbrink
>>> Humboldtstr. 19
>>> 44137 Dortmund
>>> Germany
>>> http://www.walware.de/goto/opensource
>>>
>>
>>


From hadley at rice.edu  Wed Jul  6 14:58:02 2011
From: hadley at rice.edu (Hadley Wickham)
Date: Wed, 6 Jul 2011 08:58:02 -0400
Subject: [Rd] Syntactically valid names
In-Reply-To: <99993C2E-69F9-4AD9-8484-38C1CF765944@gmail.com>
References: <BANLkTimg=TM0hYA1=Obknb+486Hv8T7Fdw@mail.gmail.com>
	<2698F67A-DDF9-4EAF-B7A8-A4FAAE9CCC6B@gmail.com>
	<CABdHhvHmRP6XOkTq+4Z1NjvW1SRwqt1AUwga_ovbMOfi3zi5oA@mail.gmail.com>
	<CAFFLneRp4YY7U+Nn26C+hFhX-4PWaHZMFizBmgEtNg2jjK93qg@mail.gmail.com>
	<CABdHhvEhKGZjSX_fRAAi6_PjHgnhOra_f7w8Y+R8Pw8gGm92OQ@mail.gmail.com>
	<A16B0AB7-C30E-482D-B91A-EC3CC2C70CB9@gmail.com>
	<CABdHhvGAP7+0FeDtyBiKFtdnP6+oAnYjkEUpBQVChY5toVqHCA@mail.gmail.com>
	<99993C2E-69F9-4AD9-8484-38C1CF765944@gmail.com>
Message-ID: <CABdHhvFVrGi4HORYSc=naX3cGjOWr7b3npYEJE75H-OcgX033A@mail.gmail.com>

> That's two different issues:
>
>> y <- list()
>> y$... <- 2
>> y$..2 <- 3
>> y$break <- 4
> Error: unexpected 'break' in "y$break"
>
> Notice that there is nothing _syntactically_ wrong with ... & friends as names:
>
>> quote(...<-4)
> ... <- 4
>
> It's the _evaluator_ that throws the error because the ...-name has a special interpretation.

Oh good point.  Thanks for the clarification.

Hadley


-- 
Assistant Professor / Dobelman Family Junior Chair
Department of Statistics / Rice University
http://had.co.nz/


From kasperdanielhansen at gmail.com  Wed Jul  6 15:25:15 2011
From: kasperdanielhansen at gmail.com (Kasper Daniel Hansen)
Date: Wed, 6 Jul 2011 09:25:15 -0400
Subject: [Rd] Recent and upcoming changes to R-devel
In-Reply-To: <alpine.OSX.1.00.1107041124460.59432@tystie.local>
References: <alpine.OSX.1.00.1107041124460.59432@tystie.local>
Message-ID: <CAC2h7uujLLQQCzQBgfCmbaFg-UeQqMzntQfXD8YhyjSRQpAWDw@mail.gmail.com>

On Mon, Jul 4, 2011 at 8:08 AM, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> In the near future all packages will have a name space. ?If the sources do
> not contain one, a default NAMESPACE file will be added. This again will
> simplify the descriptions and also a lot of internal code. ?Maintainers of
> packages without name spaces (currently 42% of CRAN) are encouraged to add
> one themselves.

This is great.  However, it would also be great if a user could
disable this for a given package at install time, for example with a
command line argument to R CMD INSTALL.  Use case: in the early stages
of package development I find it incredible useful to not have a
NAMESPACE.  This is of course before I release it to anyone else;
purely for development.

My guess is that this has already been anticipated, but in case not, I
wanted to raise the issue.

Kasper


From simon.urbanek at r-project.org  Wed Jul  6 15:32:03 2011
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Wed, 6 Jul 2011 09:32:03 -0400
Subject: [Rd] [datatable-help]  speeding up perception
In-Reply-To: <1309941365.4231.122.camel@netbook>
References: <CAPr7RtU44GO-7_m02-ek+ps3YcR_qr3m7vV8BSvhAYJ=F_bnbA@mail.gmail.com>
	<CAPr7RtVx1ao9bubQiCrf9LjgpdZdB24vSnjZvGxW4a8Sy-pKag@mail.gmail.com>
	<CAGam+C6yEFw-E+oT7tktDVY8L9KtJZ0aHrQ_y0VLyEMnZDmfSw@mail.gmail.com>
	<555C7B55-67D9-4DC6-90CB-BE8B0F620BFB@r-project.org>
	<1309851165.4231.40.camel@netbook>
	<1309889303.4231.60.camel@netbook>
	<E0D40E65-E676-4083-A058-090E3D2CC20C@r-project.org>
	<alpine.DEB.2.00.1107051808040.1928@luke-inspiron>
	<121657AE-3C91-4880-BC29-781A78B05278@comcast.net>
	<11C6B5AE-0B1F-4154-B21C-7A5B3ED08619@r-project.org>
	<1309941365.4231.122.camel@netbook>
Message-ID: <40822836-425E-4266-B203-35B3357FAB6F@r-project.org>

Interesting, and I stand corrected:

> x = data.frame(a=1:n,b=1:n)
> .Internal(inspect(x))
@103511c00 19 VECSXP g0c2 [OBJ,NAM(2),ATT] (len=2, tl=0)
  @102c7b000 13 INTSXP g0c7 [] (len=100000, tl=0) 1,2,3,4,5,...
  @102af3000 13 INTSXP g0c7 [] (len=100000, tl=0) 1,2,3,4,5,...

> x[1,1]=42L
> .Internal(inspect(x))
@10349c720 19 VECSXP g0c2 [OBJ,NAM(2),ATT] (len=2, tl=0)
  @102c19000 13 INTSXP g0c7 [] (len=100000, tl=0) 42,2,3,4,5,...
  @102b55000 13 INTSXP g0c7 [] (len=100000, tl=0) 1,2,3,4,5,...

> x[[1]][1]=42L
> .Internal(inspect(x))
@103511a78 19 VECSXP g1c2 [OBJ,MARK,NAM(2),ATT] (len=2, tl=0)
  @102e65000 13 INTSXP g0c7 [] (len=100000, tl=0) 42,2,3,4,5,...
  @101f14000 13 INTSXP g1c7 [MARK] (len=100000, tl=0) 1,2,3,4,5,...

> x[[1]][1]=42L
> .Internal(inspect(x))
@10349c800 19 VECSXP g0c2 [OBJ,NAM(2),ATT] (len=2, tl=0)
  @102a2f000 13 INTSXP g0c7 [] (len=100000, tl=0) 42,2,3,4,5,...
  @102ec7000 13 INTSXP g0c7 [] (len=100000, tl=0) 1,2,3,4,5,...


I have R to release ;) so I won't be looking into this right now, but it's something worth investigating ... Since all the inner contents have NAMED=0 I would not expect any duplication to be needed, but apparently becomes so is at some point ...

Cheers,
Simon


On Jul 6, 2011, at 4:36 AM, Matthew Dowle wrote:

> 
> On Tue, 2011-07-05 at 21:11 -0400, Simon Urbanek wrote:
>> No subassignment function satisfies that condition, because you can always call them directly. However, that doesn't stop the default method from making that assumption, so I'm not sure it's an issue.
>> 
>> David, Just to clarify - the data frame content is not copied, we are talking about the vector holding columns.
> 
> If it is just the vector holding the columns that is copied (and not the
> columns themselves), why does n make a difference in this test (on R
> 2.13.0)?
> 
>> n = 1000
>> x = data.frame(a=1:n,b=1:n)
>> system.time(for (i in 1:1000) x[1,1] <- 42L)
>   user  system elapsed 
>  0.628   0.000   0.628 
>> n = 100000
>> x = data.frame(a=1:n,b=1:n)      # still 2 columns, but longer columns
>> system.time(for (i in 1:1000) x[1,1] <- 42L)
>   user  system elapsed 
> 20.145   1.232  21.455 
>> 
> 
> With $<- :
> 
>> n = 1000
>> x = data.frame(a=1:n,b=1:n)
>> system.time(for (i in 1:1000) x$a[1] <- 42L)
>   user  system elapsed 
>  0.304   0.000   0.307 
>> n = 100000
>> x = data.frame(a=1:n,b=1:n)
>> system.time(for (i in 1:1000) x$a[1] <- 42L)
>   user  system elapsed 
> 37.586   0.388  38.161 
>> 
> 
> If it's because the 1st column needs to be copied (only) because that's
> the one being assigned to (in this test), that magnitude of slow down
> doesn't seem consistent with the time of a vector copy of the 1st
> column : 
> 
>> n=100000
>> v = 1:n
>> system.time(for (i in 1:1000) v[1] <- 42L)
>   user  system elapsed 
>  0.016   0.000   0.017 
>> system.time(for (i in 1:1000) {v2=v;v2[1] <- 42L})
>   user  system elapsed 
>  1.816   1.076   2.900
> 
> Finally, increasing the number of columns, again only the 1st is
> assigned to :
> 
>> n=100000
>> x = data.frame(rep(list(1:n),100))
>> dim(x)
> [1] 100000    100
>> system.time(for (i in 1:1000) x[1,1] <- 42L)
>   user  system elapsed 
> 167.974  50.903 219.711 
>> 
> 
> 
> 
>> 
>> Cheers,
>> Simon
>> 
>> Sent from my iPhone
>> 
>> On Jul 5, 2011, at 9:01 PM, David Winsemius <dwinsemius at comcast.net> wrote:
>> 
>>> 
>>> On Jul 5, 2011, at 7:18 PM, <luke-tierney at uiowa.edu> <luke-tierney at uiowa.edu> wrote:
>>> 
>>>> On Tue, 5 Jul 2011, Simon Urbanek wrote:
>>>> 
>>>>> 
>>>>> On Jul 5, 2011, at 2:08 PM, Matthew Dowle wrote:
>>>>> 
>>>>>> Simon (and all),
>>>>>> 
>>>>>> I've tried to make assignment as fast as calling `[<-.data.table`
>>>>>> directly, for user convenience. Profiling shows (IIUC) that it isn't
>>>>>> dispatch, but x being copied. Is there a way to prevent '[<-' from
>>>>>> copying x?
>>>>> 
>>>>> Good point, and conceptually, no. It's a subassignment after all - see R-lang 3.4.4 - it is equivalent to
>>>>> 
>>>>> `*tmp*` <- x
>>>>> x <- `[<-`(`*tmp*`, i, j, value)
>>>>> rm(`*tmp*`)
>>>>> 
>>>>> so there is always a copy involved.
>>>>> 
>>>>> Now, a conceptual copy doesn't mean real copy in R since R tries to keep the pass-by-value illusion while passing references in cases where it knows that modifications cannot occur and/or they are safe. The default subassign method uses that feature which means it can afford to not duplicate if there is only one reference -- then it's safe to not duplicate as we are replacing that only existing reference. And in the case of a matrix, that will be true at the latest from the second subassignment on.
>>>>> 
>>>>> Unfortunately the method dispatch (AFAICS) introduces one more reference in the dispatch chain so there will always be two references so duplication is necessary. Since we have only 0 / 1 / 2+ information on the references, we can't distinguish whether the second reference is due to the dispatch or due to the passed object having more than one reference, so we have to duplicate in any case. That is unfortunate, and I don't see a way around (unless we handle subassignment methods is some special way).
>>>> 
>>>> I don't believe dispatch is bumping NAMED (and a quick experiment
>>>> seems to confirm this though I don't guarantee I did that right). The
>>>> issue is that a replacement function implemented as a closure, which
>>>> is the only option for a package, will always see NAMED on the object
>>>> to be modified as 2 (because the value is obtained by forcing the
>>>> argument promise) and so any R level assignments will duplicate.  This
>>>> also isn't really an issue of imprecise reference counting -- there
>>>> really are (at least) two legitimate references -- one though the
>>>> argument and one through the caller's environment.
>>>> 
>>>> It would be good it we could come up with a way for packages to be
>>>> able to define replacement functions that do not duplicate in cases
>>>> where we really don't want them to, but this would require coming up
>>>> with some sort of protocol, minimally involving an efficient way to
>>>> detect whether a replacement funciton is being called in a replacement
>>>> context or directly.
>>> 
>>> Would "$<-" always satisfy that condition. It would be big help to me if it could be designed to avoid duplication the rest of the data.frame.
>>> 
>>> -- 
>>> 
>>>> 
>>>> There are some replacement functions that use C code to cheat, but
>>>> these may create problems if called directly, so I won't advertise
>>>> them.
>>>> 
>>>> Best,
>>>> 
>>>> luke
>>>> 
>>>>> 
>>>>> Cheers,
>>>>> Simon
>>>>> 
>>>>> 
>>>>> 
>>>> 
>>>> -- 
>>>> Luke Tierney
>>>> Statistics and Actuarial Science
>>>> Ralph E. Wareham Professor of Mathematical Sciences
>>>> University of Iowa                  Phone:             319-335-3386
>>>> Department of Statistics and        Fax:               319-335-3017
>>>> Actuarial Science
>>>> 241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
>>>> Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu______________________________________________
>>>> R-devel at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>> 
>>> David Winsemius, MD
>>> West Hartford, CT
>>> 
>>> 
> 
> 
> 


From murdoch.duncan at gmail.com  Wed Jul  6 15:42:33 2011
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Wed, 06 Jul 2011 09:42:33 -0400
Subject: [Rd] Recent and upcoming changes to R-devel
In-Reply-To: <CAC2h7uujLLQQCzQBgfCmbaFg-UeQqMzntQfXD8YhyjSRQpAWDw@mail.gmail.com>
References: <alpine.OSX.1.00.1107041124460.59432@tystie.local>
	<CAC2h7uujLLQQCzQBgfCmbaFg-UeQqMzntQfXD8YhyjSRQpAWDw@mail.gmail.com>
Message-ID: <4E146649.7080909@gmail.com>

On 11-07-06 9:25 AM, Kasper Daniel Hansen wrote:
> On Mon, Jul 4, 2011 at 8:08 AM, Prof Brian Ripley<ripley at stats.ox.ac.uk>  wrote:
>> In the near future all packages will have a name space.  If the sources do
>> not contain one, a default NAMESPACE file will be added. This again will
>> simplify the descriptions and also a lot of internal code.  Maintainers of
>> packages without name spaces (currently 42% of CRAN) are encouraged to add
>> one themselves.
>
> This is great.  However, it would also be great if a user could
> disable this for a given package at install time, for example with a
> command line argument to R CMD INSTALL.  Use case: in the early stages
> of package development I find it incredible useful to not have a
> NAMESPACE.  This is of course before I release it to anyone else;
> purely for development.
>
> My guess is that this has already been anticipated, but in case not, I
> wanted to raise the issue.

I think the idea is to completely remove support for the bad search 
order you get when you don't have a namespace.  That search order is a 
mixed blessing in debugging:  it makes it easy to replace functions with 
new versions, but it also makes it very easy to execute the wrong code 
if you happen to have something sitting in the global environment that 
has a conflicting name.

It seems like something a front end could do to make assignInNamespace 
easier to use to make working with namespaces easier.

Duncan Murdoch


From kasperdanielhansen at gmail.com  Wed Jul  6 16:06:37 2011
From: kasperdanielhansen at gmail.com (Kasper Daniel Hansen)
Date: Wed, 6 Jul 2011 10:06:37 -0400
Subject: [Rd] Recent and upcoming changes to R-devel
In-Reply-To: <4E146649.7080909@gmail.com>
References: <alpine.OSX.1.00.1107041124460.59432@tystie.local>
	<CAC2h7uujLLQQCzQBgfCmbaFg-UeQqMzntQfXD8YhyjSRQpAWDw@mail.gmail.com>
	<4E146649.7080909@gmail.com>
Message-ID: <CAC2h7utabPFeseTpikDwWzEXV_im2G_-M7X4nr0Souk81pqhPg@mail.gmail.com>

On Wed, Jul 6, 2011 at 9:42 AM, Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
> On 11-07-06 9:25 AM, Kasper Daniel Hansen wrote:
>>
>> On Mon, Jul 4, 2011 at 8:08 AM, Prof Brian Ripley<ripley at stats.ox.ac.uk>
>> ?wrote:
>>>
>>> In the near future all packages will have a name space. ?If the sources
>>> do
>>> not contain one, a default NAMESPACE file will be added. This again will
>>> simplify the descriptions and also a lot of internal code. ?Maintainers
>>> of
>>> packages without name spaces (currently 42% of CRAN) are encouraged to
>>> add
>>> one themselves.
>>
>> This is great. ?However, it would also be great if a user could
>> disable this for a given package at install time, for example with a
>> command line argument to R CMD INSTALL. ?Use case: in the early stages
>> of package development I find it incredible useful to not have a
>> NAMESPACE. ?This is of course before I release it to anyone else;
>> purely for development.
>>
>> My guess is that this has already been anticipated, but in case not, I
>> wanted to raise the issue.
>
> I think the idea is to completely remove support for the bad search order
> you get when you don't have a namespace. ?That search order is a mixed
> blessing in debugging: ?it makes it easy to replace functions with new
> versions, but it also makes it very easy to execute the wrong code if you
> happen to have something sitting in the global environment that has a
> conflicting name.
>
> It seems like something a front end could do to make assignInNamespace
> easier to use to make working with namespaces easier.

Indeed I find this change to be a very welcome addition to R that will
make the life easier for many of us; especially when you are using
someone else's package.

My use case is also not debugging my own code when it is a at a
semi-mature level.  I am addressing the very early stages of
developing a new package.  In my workflow (which may differ from other
people's; but I do think a substantial number would agree with me),
the early stages of development usually entails bringing order to a
number of R scripts; figuring out the structure of the basic objects
as well as the arguments for the important functions/methods.  In this
early stage, almost any evaluation I do at the R prompt is followed by
refactoring some part of the code (and then sourcing it into R).

Once the code is semi-matured (after the first couple of weeks), I
don't think I would mind a NAMESPACE at all, but I believe I would
find it very frustrating during the very early development stage
(which I am going through right now for one package and which I went
through a month ago for another package).

Kasper


From luke-tierney at uiowa.edu  Wed Jul  6 16:08:11 2011
From: luke-tierney at uiowa.edu (luke-tierney at uiowa.edu)
Date: Wed, 6 Jul 2011 09:08:11 -0500
Subject: [Rd] [datatable-help]  speeding up perception
In-Reply-To: <40822836-425E-4266-B203-35B3357FAB6F@r-project.org>
References: <CAPr7RtU44GO-7_m02-ek+ps3YcR_qr3m7vV8BSvhAYJ=F_bnbA@mail.gmail.com>
	<CAPr7RtVx1ao9bubQiCrf9LjgpdZdB24vSnjZvGxW4a8Sy-pKag@mail.gmail.com>
	<CAGam+C6yEFw-E+oT7tktDVY8L9KtJZ0aHrQ_y0VLyEMnZDmfSw@mail.gmail.com>
	<555C7B55-67D9-4DC6-90CB-BE8B0F620BFB@r-project.org>
	<1309851165.4231.40.camel@netbook> <1309889303.4231.60.camel@netbook>
	<E0D40E65-E676-4083-A058-090E3D2CC20C@r-project.org>
	<alpine.DEB.2.00.1107051808040.1928@luke-inspiron>
	<121657AE-3C91-4880-BC29-781A78B05278@comcast.net>
	<11C6B5AE-0B1F-4154-B21C-7A5B3ED08619@r-project.org>
	<1309941365.4231.122.camel@netbook>
	<40822836-425E-4266-B203-35B3357FAB6F@r-project.org>
Message-ID: <alpine.DEB.2.00.1107060904060.1928@luke-inspiron>

On Wed, 6 Jul 2011, Simon Urbanek wrote:

> Interesting, and I stand corrected:
>
>> x = data.frame(a=1:n,b=1:n)
>> .Internal(inspect(x))
> @103511c00 19 VECSXP g0c2 [OBJ,NAM(2),ATT] (len=2, tl=0)
>  @102c7b000 13 INTSXP g0c7 [] (len=100000, tl=0) 1,2,3,4,5,...
>  @102af3000 13 INTSXP g0c7 [] (len=100000, tl=0) 1,2,3,4,5,...
>
>> x[1,1]=42L
>> .Internal(inspect(x))
> @10349c720 19 VECSXP g0c2 [OBJ,NAM(2),ATT] (len=2, tl=0)
>  @102c19000 13 INTSXP g0c7 [] (len=100000, tl=0) 42,2,3,4,5,...
>  @102b55000 13 INTSXP g0c7 [] (len=100000, tl=0) 1,2,3,4,5,...
>
>> x[[1]][1]=42L
>> .Internal(inspect(x))
> @103511a78 19 VECSXP g1c2 [OBJ,MARK,NAM(2),ATT] (len=2, tl=0)
>  @102e65000 13 INTSXP g0c7 [] (len=100000, tl=0) 42,2,3,4,5,...
>  @101f14000 13 INTSXP g1c7 [MARK] (len=100000, tl=0) 1,2,3,4,5,...
>
>> x[[1]][1]=42L
>> .Internal(inspect(x))
> @10349c800 19 VECSXP g0c2 [OBJ,NAM(2),ATT] (len=2, tl=0)
>  @102a2f000 13 INTSXP g0c7 [] (len=100000, tl=0) 42,2,3,4,5,...
>  @102ec7000 13 INTSXP g0c7 [] (len=100000, tl=0) 1,2,3,4,5,...
>
>
> I have R to release ;) so I won't be looking into this right now, but it's something worth investigating ... Since all the inner contents have NAMED=0 I would not expect any duplication to be needed, but apparently becomes so is at some point ...


The internals assume in various places that deep copies are made (one
of the reasons NAMED setings are not propagated to sub-sturcture).
The main issues are avoiding cycles and that there is no easy way to
check for sharing.  There may be some circumstances in which a shallow
copy would be OK but making sure it would be in all cases is probably
more trouble than it is worth at this point. (I've tried this in the
past in a few cases and always had to back off.)


Best,

luke

>
> Cheers,
> Simon
>
>
> On Jul 6, 2011, at 4:36 AM, Matthew Dowle wrote:
>
>>
>> On Tue, 2011-07-05 at 21:11 -0400, Simon Urbanek wrote:
>>> No subassignment function satisfies that condition, because you can always call them directly. However, that doesn't stop the default method from making that assumption, so I'm not sure it's an issue.
>>>
>>> David, Just to clarify - the data frame content is not copied, we are talking about the vector holding columns.
>>
>> If it is just the vector holding the columns that is copied (and not the
>> columns themselves), why does n make a difference in this test (on R
>> 2.13.0)?
>>
>>> n = 1000
>>> x = data.frame(a=1:n,b=1:n)
>>> system.time(for (i in 1:1000) x[1,1] <- 42L)
>>   user  system elapsed
>>  0.628   0.000   0.628
>>> n = 100000
>>> x = data.frame(a=1:n,b=1:n)      # still 2 columns, but longer columns
>>> system.time(for (i in 1:1000) x[1,1] <- 42L)
>>   user  system elapsed
>> 20.145   1.232  21.455
>>>
>>
>> With $<- :
>>
>>> n = 1000
>>> x = data.frame(a=1:n,b=1:n)
>>> system.time(for (i in 1:1000) x$a[1] <- 42L)
>>   user  system elapsed
>>  0.304   0.000   0.307
>>> n = 100000
>>> x = data.frame(a=1:n,b=1:n)
>>> system.time(for (i in 1:1000) x$a[1] <- 42L)
>>   user  system elapsed
>> 37.586   0.388  38.161
>>>
>>
>> If it's because the 1st column needs to be copied (only) because that's
>> the one being assigned to (in this test), that magnitude of slow down
>> doesn't seem consistent with the time of a vector copy of the 1st
>> column :
>>
>>> n=100000
>>> v = 1:n
>>> system.time(for (i in 1:1000) v[1] <- 42L)
>>   user  system elapsed
>>  0.016   0.000   0.017
>>> system.time(for (i in 1:1000) {v2=v;v2[1] <- 42L})
>>   user  system elapsed
>>  1.816   1.076   2.900
>>
>> Finally, increasing the number of columns, again only the 1st is
>> assigned to :
>>
>>> n=100000
>>> x = data.frame(rep(list(1:n),100))
>>> dim(x)
>> [1] 100000    100
>>> system.time(for (i in 1:1000) x[1,1] <- 42L)
>>   user  system elapsed
>> 167.974  50.903 219.711
>>>
>>
>>
>>
>>>
>>> Cheers,
>>> Simon
>>>
>>> Sent from my iPhone
>>>
>>> On Jul 5, 2011, at 9:01 PM, David Winsemius <dwinsemius at comcast.net> wrote:
>>>
>>>>
>>>> On Jul 5, 2011, at 7:18 PM, <luke-tierney at uiowa.edu> <luke-tierney at uiowa.edu> wrote:
>>>>
>>>>> On Tue, 5 Jul 2011, Simon Urbanek wrote:
>>>>>
>>>>>>
>>>>>> On Jul 5, 2011, at 2:08 PM, Matthew Dowle wrote:
>>>>>>
>>>>>>> Simon (and all),
>>>>>>>
>>>>>>> I've tried to make assignment as fast as calling `[<-.data.table`
>>>>>>> directly, for user convenience. Profiling shows (IIUC) that it isn't
>>>>>>> dispatch, but x being copied. Is there a way to prevent '[<-' from
>>>>>>> copying x?
>>>>>>
>>>>>> Good point, and conceptually, no. It's a subassignment after all - see R-lang 3.4.4 - it is equivalent to
>>>>>>
>>>>>> `*tmp*` <- x
>>>>>> x <- `[<-`(`*tmp*`, i, j, value)
>>>>>> rm(`*tmp*`)
>>>>>>
>>>>>> so there is always a copy involved.
>>>>>>
>>>>>> Now, a conceptual copy doesn't mean real copy in R since R tries to keep the pass-by-value illusion while passing references in cases where it knows that modifications cannot occur and/or they are safe. The default subassign method uses that feature which means it can afford to not duplicate if there is only one reference -- then it's safe to not duplicate as we are replacing that only existing reference. And in the case of a matrix, that will be true at the latest from the second subassignment on.
>>>>>>
>>>>>> Unfortunately the method dispatch (AFAICS) introduces one more reference in the dispatch chain so there will always be two references so duplication is necessary. Since we have only 0 / 1 / 2+ information on the references, we can't distinguish whether the second reference is due to the dispatch or due to the passed object having more than one reference, so we have to duplicate in any case. That is unfortunate, and I don't see a way around (unless we handle subassignment methods is some special way).
>>>>>
>>>>> I don't believe dispatch is bumping NAMED (and a quick experiment
>>>>> seems to confirm this though I don't guarantee I did that right). The
>>>>> issue is that a replacement function implemented as a closure, which
>>>>> is the only option for a package, will always see NAMED on the object
>>>>> to be modified as 2 (because the value is obtained by forcing the
>>>>> argument promise) and so any R level assignments will duplicate.  This
>>>>> also isn't really an issue of imprecise reference counting -- there
>>>>> really are (at least) two legitimate references -- one though the
>>>>> argument and one through the caller's environment.
>>>>>
>>>>> It would be good it we could come up with a way for packages to be
>>>>> able to define replacement functions that do not duplicate in cases
>>>>> where we really don't want them to, but this would require coming up
>>>>> with some sort of protocol, minimally involving an efficient way to
>>>>> detect whether a replacement funciton is being called in a replacement
>>>>> context or directly.
>>>>
>>>> Would "$<-" always satisfy that condition. It would be big help to me if it could be designed to avoid duplication the rest of the data.frame.
>>>>
>>>> --
>>>>
>>>>>
>>>>> There are some replacement functions that use C code to cheat, but
>>>>> these may create problems if called directly, so I won't advertise
>>>>> them.
>>>>>
>>>>> Best,
>>>>>
>>>>> luke
>>>>>
>>>>>>
>>>>>> Cheers,
>>>>>> Simon
>>>>>>
>>>>>>
>>>>>>
>>>>>
>>>>> --
>>>>> Luke Tierney
>>>>> Statistics and Actuarial Science
>>>>> Ralph E. Wareham Professor of Mathematical Sciences
>>>>> University of Iowa                  Phone:             319-335-3386
>>>>> Department of Statistics and        Fax:               319-335-3017
>>>>> Actuarial Science
>>>>> 241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
>>>>> Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu______________________________________________
>>>>> R-devel at r-project.org mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>
>>>> David Winsemius, MD
>>>> West Hartford, CT
>>>>
>>>>
>>
>>
>>
>
>

-- 
Luke Tierney
Statistics and Actuarial Science
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu


From simon.urbanek at r-project.org  Wed Jul  6 16:17:01 2011
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Wed, 6 Jul 2011 10:17:01 -0400
Subject: [Rd] Recent and upcoming changes to R-devel
In-Reply-To: <CAC2h7utabPFeseTpikDwWzEXV_im2G_-M7X4nr0Souk81pqhPg@mail.gmail.com>
References: <alpine.OSX.1.00.1107041124460.59432@tystie.local>
	<CAC2h7uujLLQQCzQBgfCmbaFg-UeQqMzntQfXD8YhyjSRQpAWDw@mail.gmail.com>
	<4E146649.7080909@gmail.com>
	<CAC2h7utabPFeseTpikDwWzEXV_im2G_-M7X4nr0Souk81pqhPg@mail.gmail.com>
Message-ID: <59614E65-86AC-4CB5-81E5-96A57A292056@r-project.org>


On Jul 6, 2011, at 10:06 AM, Kasper Daniel Hansen wrote:

> On Wed, Jul 6, 2011 at 9:42 AM, Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
>> On 11-07-06 9:25 AM, Kasper Daniel Hansen wrote:
>>> 
>>> On Mon, Jul 4, 2011 at 8:08 AM, Prof Brian Ripley<ripley at stats.ox.ac.uk>
>>>  wrote:
>>>> 
>>>> In the near future all packages will have a name space.  If the sources
>>>> do
>>>> not contain one, a default NAMESPACE file will be added. This again will
>>>> simplify the descriptions and also a lot of internal code.  Maintainers
>>>> of
>>>> packages without name spaces (currently 42% of CRAN) are encouraged to
>>>> add
>>>> one themselves.
>>> 
>>> This is great.  However, it would also be great if a user could
>>> disable this for a given package at install time, for example with a
>>> command line argument to R CMD INSTALL.  Use case: in the early stages
>>> of package development I find it incredible useful to not have a
>>> NAMESPACE.  This is of course before I release it to anyone else;
>>> purely for development.
>>> 
>>> My guess is that this has already been anticipated, but in case not, I
>>> wanted to raise the issue.
>> 
>> I think the idea is to completely remove support for the bad search order
>> you get when you don't have a namespace.  That search order is a mixed
>> blessing in debugging:  it makes it easy to replace functions with new
>> versions, but it also makes it very easy to execute the wrong code if you
>> happen to have something sitting in the global environment that has a
>> conflicting name.
>> 
>> It seems like something a front end could do to make assignInNamespace
>> easier to use to make working with namespaces easier.
> 
> Indeed I find this change to be a very welcome addition to R that will
> make the life easier for many of us; especially when you are using
> someone else's package.
> 
> My use case is also not debugging my own code when it is a at a
> semi-mature level.  I am addressing the very early stages of
> developing a new package.  In my workflow (which may differ from other
> people's; but I do think a substantial number would agree with me),
> the early stages of development usually entails bringing order to a
> number of R scripts; figuring out the structure of the basic objects
> as well as the arguments for the important functions/methods.  In this
> early stage, almost any evaluation I do at the R prompt is followed by
> refactoring some part of the code (and then sourcing it into R).
> 
> Once the code is semi-matured (after the first couple of weeks), I
> don't think I would mind a NAMESPACE at all, but I believe I would
> find it very frustrating during the very early development stage
> (which I am going through right now for one package and which I went
> through a month ago for another package).
> 

.. but in that case it's easier to just source the package code. That allows much easier and consistent editing as well since you know everything is in the workspace, so you are not dealing with two sets of code.

Cheers,
Simon


From bbolker at gmail.com  Wed Jul  6 16:18:08 2011
From: bbolker at gmail.com (Ben Bolker)
Date: Wed, 06 Jul 2011 10:18:08 -0400
Subject: [Rd] apparent typo in heatmap docs: bump?
Message-ID: <4E146EA0.7090607@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1


  An apparent bug in the documentation for heatmap() was pointed out a
few days ago
<http://tolstoy.newcastle.edu.au/R/e14/devel/11/07/0656.html>.

  I haven't seen any discussion of this.  Would it be better to submit
it as a bug so it doesn't get lost?

  cheers
    Ben Bolker


-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.10 (GNU/Linux)
Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org/

iEYEARECAAYFAk4UbqAACgkQc5UpGjwzenOv7QCgkd8e9C0ANgO7/8kM9nCdWYAG
w/MAn2tF12ACCj+6cI6M3qJ7mu7r59g3
=NM3G
-----END PGP SIGNATURE-----
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: heatmap.txt
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20110706/7c3009eb/attachment.txt>

From mailinglist.honeypot at gmail.com  Wed Jul  6 21:04:15 2011
From: mailinglist.honeypot at gmail.com (Steve Lianoglou)
Date: Wed, 6 Jul 2011 15:04:15 -0400
Subject: [Rd] Recent and upcoming changes to R-devel
In-Reply-To: <59614E65-86AC-4CB5-81E5-96A57A292056@r-project.org>
References: <alpine.OSX.1.00.1107041124460.59432@tystie.local>
	<CAC2h7uujLLQQCzQBgfCmbaFg-UeQqMzntQfXD8YhyjSRQpAWDw@mail.gmail.com>
	<4E146649.7080909@gmail.com>
	<CAC2h7utabPFeseTpikDwWzEXV_im2G_-M7X4nr0Souk81pqhPg@mail.gmail.com>
	<59614E65-86AC-4CB5-81E5-96A57A292056@r-project.org>
Message-ID: <CAHA9McMG0CCxhk6eOKrtFx2PNF2=bu61pZs8m5ZwW9EDoLoALQ@mail.gmail.com>

On Wed, Jul 6, 2011 at 10:17 AM, Simon Urbanek
<simon.urbanek at r-project.org> wrote:
>
> On Jul 6, 2011, at 10:06 AM, Kasper Daniel Hansen wrote:
[aggressive trimming]
>>> It seems like something a front end could do to make assignInNamespace
>>> easier to use to make working with namespaces easier.
>>
>> Indeed I find this change to be a very welcome addition to R that will
>> make the life easier for many of us; especially when you are using
>> someone else's package.
>>
>> My use case is also not debugging my own code when it is a at a
>> semi-mature level. ?I am addressing the very early stages of
>> developing a new package. ?In my workflow (which may differ from other
>> people's; but I do think a substantial number would agree with me),
>> the early stages of development usually entails bringing order to a
>> number of R scripts; figuring out the structure of the basic objects
>> as well as the arguments for the important functions/methods. ?In this
>> early stage, almost any evaluation I do at the R prompt is followed by
>> refactoring some part of the code (and then sourcing it into R).
>>
>> Once the code is semi-matured (after the first couple of weeks), I
>> don't think I would mind a NAMESPACE at all, but I believe I would
>> find it very frustrating during the very early development stage
>> (which I am going through right now for one package and which I went
>> through a month ago for another package).
>>
>
> .. but in that case it's easier to just source the package code. That allows much easier and consistent editing as well since you know everything is in the workspace, so you are not dealing with two sets of code.

And you could also try using Hadley's devtools package:
https://github.com/hadley/devtools/

Last I tried to use it, there was some problems with some S4isms (that
maybe were my fault(?)) so I switched back to sourcing my
development-package-dirs (which is fine until you've got compiled
code) -- but maybe the problems I was having with devtools have also
been resolved. It could be worth a try.

-steve

-- 
Steve Lianoglou
Graduate Student: Computational Systems Biology
?| Memorial Sloan-Kettering Cancer Center
?| Weill Medical College of Cornell University
Contact Info: http://cbio.mskcc.org/~lianos/contact


From Mark.Bravington at csiro.au  Thu Jul  7 09:38:15 2011
From: Mark.Bravington at csiro.au (Mark.Bravington at csiro.au)
Date: Thu, 7 Jul 2011 17:38:15 +1000
Subject: [Rd] Recent and upcoming changes to R-devel
In-Reply-To: <4E144A77.8090601@gmail.com>
References: <alpine.OSX.1.00.1107041124460.59432@tystie.local>
	<3D59DF35E7CF3E42BA70C23BD5CBE6A85F4C199E22@exvic-mbx04.nexus.csiro.au>
	<4E12ED06.5040603@openanalytics.eu> <4E1310D9.6050901@gmail.com>
	<4E132BA0.5000003@walware.de> <4E132BFD.5090604@gmail.com>
	<4E1412D1.2040700@walware.de>,<4E144A77.8090601@gmail.com>
Message-ID: <3D59DF35E7CF3E42BA70C23BD5CBE6A85F4E504957@exvic-mbx04.nexus.csiro.au>

Thank you Duncan for those reassurances. I still have a couple of questions, as below, but there was definitely some good news for me in your replies:

 - IIUC , the "source" attribute will still be available for everything, provided R_KEEP_PKG_SOURCE=yes (I am not so worried about srcrefs)
 
 - modifying the body of a function automatically disposes of the previous byte-compiled version.
 
*If* that's all there is, then I can easily enough modify 'debug' to work with byte-compiled code. But I am still a bit concerned, because I can remember in the past reading somewhat scary things like this:

"... sealing the namespace means the byte compiler can make certain assumptions for efficiency..."
 
Q1: So my concern is something like this: even if I can successfully 'mtrace' one function X in eg baseenv() for when X is called directly, maybe a byte-compiled version of another function Y (in baseenv() or elsewhere) that calls X will somehow still invoke the original byte-compiled version of X. EG if the BC knows that X has been reduced to piece-of-BC-code #924, then maybe the BC of Y contains "call BC#924 now" rather than "call X now". So only direct calls to X would pick up the modified version. Can someone confirm that "this sort of thing" won't happen?

Obviously this is a bit non-specific and is based on ignorance-- but that's inevitable because there isn't much information about the BC. The manuals for R-2.14-dev have almost none, for example.
 
The Windows binary of R-devel on 6-7-2011 didn't yet seem to be the pre-byte-compiled version mentioned in Brian Ripley's announcement, so I haven't been able to test anything.
 
Q2: From DM's first reply:
 
"... but that is something
 that is either ..., or avoided by re-installing without byte compiling."

How would the latter be done? And would it be possible for the Windows binary distro, or just for the source distro? My reading of the first email about changes in R-2.14-devel was that the distro would be byte-compiled already.

Thanks

Mark Bravington
CSIRO CMIS
Marine Lab
Hobart
Australia
________________________________________
From: Duncan Murdoch [murdoch.duncan at gmail.com]
Sent: 06 July 2011 21:43
To: Stephan Wahlbrink
Cc: tobias.verbeke at openanalytics.eu; Bravington, Mark (CMIS, Hobart); ripley at stats.ox.ac.uk; R-devel at r-project.org
Subject: Re: [Rd] Recent and upcoming changes to R-devel

On 11-07-06 3:46 AM, Stephan Wahlbrink wrote:
> Duncan Murdoch wrote [2011-07-05 17:21]:
>> On 05/07/2011 11:20 AM, Stephan Wahlbrink wrote:
>>> Dear developers,
>>>
>>> Duncan Murdoch wrote [2011-07-05 15:25]:
>>>> On 05/07/2011 6:52 AM, Tobias Verbeke wrote:
>>>>> L.S.
>>>>>
>>>>> On 07/05/2011 02:16 AM, Mark.Bravington at csiro.au wrote:
>>>>>> I may have misunderstood, but:
>>>>>>
>>>>>> Please could we have an optional installation that does not*not*
>>>>> byte-compile base and recommended?
>>>>>>
>>>>>> Reason: it's not possible to debug byte-compiled code-- at least not
>>>>> with the 'debug' package, which is quite widely used. I quite often
>>>>> end up using 'mtrace' on functions in base/recommended packages to
>>>>> figure out what they are doing. And sometimes I (and others)
>>>>> experiment with changing functions in base/recommended to improve
>>>>> functionality. That seems to be harder with BC versions, and might
>>>>> even be impossible, as best I can tell from hints in the documentation
>>>>> of 'compile').
>>>>>>
>>>>>> Personally, if I had to choose only one, I'd rather live with the
>>>>> speed penalty from not byte-compiling. But of course, if both are
>>>>> available, I could install both.
>>>>>
>>>>> I completely second this request. All speed improvements and the byte
>>>>> compiler in particular are leaps forward and I am very grateful and
>>>>> admiring towards the people that make this happen.
>>>>>
>>>>> That being said, 'moving away' from the sources (with the lazy loading
>>>>> files and byte-compilation) may be a step back for R package
>>> developers
>>>>> that (during development and maybe on separate development
>>> installations
>>>>> [as opposed to production installations of R]) require
>>>>> the sources of all packages to be efficient in their work.
>>>>>
>>>>> As many of you know there is an open source Eclipse/StatET visual
>>>>> debugger ready and for that application as well (similar to Mark's
>>>>> request) presence of non-compiled code is highly desirable.
>>>>>
>>>>> For the particular purpose of debugging R packages, I would even plead
>>>>> to go beyond the current options and support the addition of an
>>>>> R package install option that allows to include the sources (e.g. in
>>>>> a standard folder Rsrc/) in installed packages.
>>>>>
>>>>> I am fully aware that one can always fetch the source tarballs from
>>>>> CRAN for that purpose, but it would be much more easy if a simple
>>>>> installation option could put the R sources of a package in a separate
>>>>> folder [or archive inside an existing folder] such that R development
>>>>> tools (such as the Eclipse/StatET IDE) can offer inspection of sources
>>>>> or display them (e.g. during debugging) out of the box.
>>>>>
>>>>> If one has the srcref, one can always load the absolutely correct
>>> source
>>>>> code this way, even if one doesn't know the parent function with
>>>>> the source attribute.
>>>>>
>>>>> Any comments?
>>>>
>>>> I think these requests have already been met. If you modify the body of
>>>> a closure (as trace() does), then the byte compiled version is
>>>> discarded, and you go back to the regular interpreted code. If you
>>>> install packages with the R_KEEP_PKG_SOURCE=yes environment variable
>>>> set, the you keep all source for all functions. (It's attached to the
>>>> function itself, not as a file that may be out of date.) It's possible
>>>> that byte compiling turns off R_KEEP_PKG_SOURCE, but that is something
>>>> that is either easily fixed, or avoided by re-installing without byte
>>>> compiling.
>>>
>>> I don?t know how the new installation works exactly, but would it be
>>> possible, to simply install both types, the old expression bodies and
>>> the new byte-compiled, as single package at the same time?
>>
>> Yes, that's what is done.
>
> Perfect! And which version (byte-compiled or expressions) is used at
> runtime under which condition?

The byte code is executed, the interpreted version is displayed.  There
are conditions under which the byte code is dropped.  I don't know a
comprehensive list, but the idea is that if the body changes, then the
compiled version is no longer valid.

Duncan Murdoch

>
> Thanks,
> Stephan
>
>
>>> This would
>>> allow the R user and developer to simply use the variant which is the
>>> best at the moment. If he wants to debug code, he can switch of the use
>>> of byte-compiled code and use the old R expressions (with attached
>>> srcrefs). If debugging is not required, he can profit from the
>>> byte-compiled version. The best would be a toggle, to switch it at
>>> runtime, but a startup option would be sufficient too.
>>>
>>> I think direct access to the code is one big advantage of open source
>>> software. For developer it makes it easier to find and fix bugs if
>>> something is wrong. But it can also help users a lot to understand how a
>>> function or algorithm works and learn from code written by other persons
>>> ? if the access to the sources is easy.
>>>
>>> As long byte-code doesn?t support the debugging features of R, it is
>>> required for best debugging support to run the functions completely
>>> without byte-complied code. If I understood it correctly, byte-code
>>> frames would disable srcrefs as well as features like ?step return? to
>>> that frames. Therefore I ask for a way that it is easy to switch between
>>> both execution types.
>>
>> What gave you that impression?
>>
>> Duncan Murdoch
>>
>>> Best,
>>> Stephan
>>>
>>>
>>>>
>>>> Duncan Murdoch
>>>>
>>>>> Best,
>>>>> Tobias
>>>>>
>>>>> P.S. One could even consider a post-install option e.g. to add 'real'
>>>>> R sources (and source references) to Windows packages (which are by
>>>>> definition already 'installed' and for which such information is not
>>>>> by default included in the CRAN binaries of these packages).
>>>>>
>>>>>>>> Prof Brian Ripley wrote:
>>>>>>>> There was an R-core meeting the week before last, and various
>>>>> planned
>>>>>>>> changes will appear in R-devel over the next few weeks.
>>>>>>>>
>>>>>>>> These are changes planned for R 2.14.0 scheduled for Oct 31.
>>> As we
>>>>>>>> are sick of people referring to R-devel as '2.14' or '2.14.0',
>>> that
>>>>>>>> version number will not be used until we reach 2.14.0 alpha. You
>>>>>>>> will be able to have a package depend on an svn version number
>>> when
>>>>>>>> referring to R-devel rather than using R (>= 2.14.0).
>>>>>>>>
>>>>>>>> All packages are installed with lazy-loading (there were 72 CRAN
>>>>>>>> packages and 8 BioC packages which opted out). This means that
>>> the
>>>>>>>> code is always parsed at install time which inter alia simplifies
>>>>> the
>>>>>>>> descriptions. R 2.13.1 RC warns on installation about packages
>>> which
>>>>>>>> ask not to be lazy-loaded, and R-devel ignores such requests
>>> (with a
>>>>>>>> warning).
>>>>>>>>
>>>>>>>> In the near future all packages will have a name space. If the
>>>>>>>> sources do not contain one, a default NAMESPACE file will be
>>> added.
>>>>>>>> This again will simplify the descriptions and also a lot of
>>> internal
>>>>>>>> code. Maintainers of packages without name spaces (currently
>>> 42% of
>>>>>>>> CRAN) are encouraged to add one themselves.
>>>>>>>>
>>>>>>>> R-devel is installed with the base and recommended packages
>>>>>>>> byte-compiled (the equivalent of 'make bytecode' in R 2.13.x, but
>>>>>>>> done less inefficiently). There is a new option R CMD INSTALL
>>>>>>>> --byte-compile to byte-compile contributed packages, but that
>>>>> remains
>>>>>>>> optional.
>>>>>>>> Byte-compilation is quite expensive (so you definitely want to
>>> do it
>>>>>>>> at install time, which requires lazy-loading), and relatively few
>>>>>>>> packages benefit appreciably from byte-compilation. A larger
>>> number
>>>>>>>> of packages benefit from byte-compilation of R itself: for
>>> example
>>>>>>>> AER runs its checks 10% faster. The byte-compiler technology is
>>>>>>>> thanks to Luke Tierney.
>>>>>>>>
>>>>>>>> There is support for figures in Rd files: currently with a
>>>>> first-pass
>>>>>>>> implementation (thanks to Duncan Murdoch).
>>>
>>> --
>>> Stephan Wahlbrink
>>> Humboldtstr. 19
>>> 44137 Dortmund
>>> Germany
>>> http://www.walware.de/goto/opensource
>>>
>>
>>


From Mark.Bravington at csiro.au  Thu Jul  7 09:53:13 2011
From: Mark.Bravington at csiro.au (Mark.Bravington at csiro.au)
Date: Thu, 7 Jul 2011 17:53:13 +1000
Subject: [Rd] Developing with Namespaces (was: Recent and upcoming changes
 to R-devel)
Message-ID: <3D59DF35E7CF3E42BA70C23BD5CBE6A85F4E504958@exvic-mbx04.nexus.csiro.au>

Hi Kasper

FWIW, the package development and maintenance support in 'mvbutils' makes it trivial to work with namespaced packages, even at the earliest stages of development. It's completely easy to add and remove functions to/from the namespace (and the exported visible bit) while the package is loaded. Debugging is seamless (via the 'debug' package, anyway). There is no need to re-load the sources or rebuild the package when you make changes-- the source version and installed version all get updated automatically.

I've done a namespaced-package-from-standing-start in under 10mins, and could probably shorten that if a beer was at stake. I've also forced a few colleagues to build packages this way, and have had positive feedback.

Of course, to get all these wonderful things, you do have to somewhat buy in to 'mvbutils' way of working. But that's not necessarily a bad thing ;)

For me, it's definitely been preferable to build in the namespace from the start, because otherwise additional errors can appear when functions become "unexported". Namespaces really do clarify things and avoid the search-order problems, so the key thing is just to make sure the development cycle is easy when namespaces are involved.

bye
Mark



Mark Bravington
CSIRO CMIS
Marine Lab
Hobart
Australia
________________________________________
From: r-devel-bounces at r-project.org [r-devel-bounces at r-project.org] On Behalf Of Kasper Daniel Hansen [kasperdanielhansen at gmail.com]
Sent: 07 July 2011 00:06
To: Duncan Murdoch
Cc: Prof Brian Ripley; R-devel at r-project.org
Subject: Re: [Rd] Recent and upcoming changes to R-devel

On Wed, Jul 6, 2011 at 9:42 AM, Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
> On 11-07-06 9:25 AM, Kasper Daniel Hansen wrote:
>>
>> On Mon, Jul 4, 2011 at 8:08 AM, Prof Brian Ripley<ripley at stats.ox.ac.uk>
>>  wrote:
>>>
>>> In the near future all packages will have a name space.  If the sources
>>> do
>>> not contain one, a default NAMESPACE file will be added. This again will
>>> simplify the descriptions and also a lot of internal code.  Maintainers
>>> of
>>> packages without name spaces (currently 42% of CRAN) are encouraged to
>>> add
>>> one themselves.
>>
>> This is great.  However, it would also be great if a user could
>> disable this for a given package at install time, for example with a
>> command line argument to R CMD INSTALL.  Use case: in the early stages
>> of package development I find it incredible useful to not have a
>> NAMESPACE.  This is of course before I release it to anyone else;
>> purely for development.
>>
>> My guess is that this has already been anticipated, but in case not, I
>> wanted to raise the issue.
>
> I think the idea is to completely remove support for the bad search order
> you get when you don't have a namespace.  That search order is a mixed
> blessing in debugging:  it makes it easy to replace functions with new
> versions, but it also makes it very easy to execute the wrong code if you
> happen to have something sitting in the global environment that has a
> conflicting name.
>
> It seems like something a front end could do to make assignInNamespace
> easier to use to make working with namespaces easier.

Indeed I find this change to be a very welcome addition to R that will
make the life easier for many of us; especially when you are using
someone else's package.

My use case is also not debugging my own code when it is a at a
semi-mature level.  I am addressing the very early stages of
developing a new package.  In my workflow (which may differ from other
people's; but I do think a substantial number would agree with me),
the early stages of development usually entails bringing order to a
number of R scripts; figuring out the structure of the basic objects
as well as the arguments for the important functions/methods.  In this
early stage, almost any evaluation I do at the R prompt is followed by
refactoring some part of the code (and then sourcing it into R).

Once the code is semi-matured (after the first couple of weeks), I
don't think I would mind a NAMESPACE at all, but I believe I would
find it very frustrating during the very early development stage
(which I am going through right now for one package and which I went
through a month ago for another package).

Kasper

______________________________________________
R-devel at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-devel


From murdoch.duncan at gmail.com  Thu Jul  7 16:17:53 2011
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Thu, 07 Jul 2011 10:17:53 -0400
Subject: [Rd] Recent and upcoming changes to R-devel
In-Reply-To: <3D59DF35E7CF3E42BA70C23BD5CBE6A85F4E504957@exvic-mbx04.nexus.csiro.au>
References: <alpine.OSX.1.00.1107041124460.59432@tystie.local>	<3D59DF35E7CF3E42BA70C23BD5CBE6A85F4C199E22@exvic-mbx04.nexus.csiro.au>
	<4E12ED06.5040603@openanalytics.eu>
	<4E1310D9.6050901@gmail.com> <4E132BA0.5000003@walware.de>
	<4E132BFD.5090604@gmail.com> <4E1412D1.2040700@walware.de>,
	<4E144A77.8090601@gmail.com>
	<3D59DF35E7CF3E42BA70C23BD5CBE6A85F4E504957@exvic-mbx04.nexus.csiro.au>
Message-ID: <4E15C011.50106@gmail.com>

On 07/07/2011 3:38 AM, Mark.Bravington at csiro.au wrote:
> Thank you Duncan for those reassurances. I still have a couple of questions, as below, but there was definitely some good news for me in your replies:
>
>   - IIUC , the "source" attribute will still be available for everything, provided R_KEEP_PKG_SOURCE=yes (I am not so worried about srcrefs)

No, the "source" attribute will go away.  Setting R_KEEP_PKG_SOURCE=yes 
will give you a "srcref" attribute on the function (not just on its 
body.  You can then get a copy of the source using something like

as.character(attr(fn, "srcref"))

It would be possible to put in special case code so that attr(fn, 
"source") did this, but the main point of this exercise is to simplify 
things, so special-casing is out.  This will be inconvenient for you 
during the transition, but it shouldn't be too bad.
>   - modifying the body of a function automatically disposes of the previous byte-compiled version.

I believe that's true, but I haven't been involved in writing that 
part.  I would guess there are ways to subvert this (i.e. modify the 
body without discarding the byte-compiled version), but hopefully it is 
easier to do it right.
>
> *If* that's all there is, then I can easily enough modify 'debug' to work with byte-compiled code. But I am still a bit concerned, because I can remember in the past reading somewhat scary things like this:
>
> "... sealing the namespace means the byte compiler can make certain assumptions for efficiency..."
>
> Q1: So my concern is something like this: even if I can successfully 'mtrace' one function X in eg baseenv() for when X is called directly, maybe a byte-compiled version of another function Y (in baseenv() or elsewhere) that calls X will somehow still invoke the original byte-compiled version of X. EG if the BC knows that X has been reduced to piece-of-BC-code #924, then maybe the BC of Y contains "call BC#924 now" rather than "call X now". So only direct calls to X would pick up the modified version. Can someone confirm that "this sort of thing" won't happen?
>
> Obviously this is a bit non-specific and is based on ignorance-- but that's inevitable because there isn't much information about the BC. The manuals for R-2.14-dev have almost none, for example.
>
> The Windows binary of R-devel on 6-7-2011 didn't yet seem to be the pre-byte-compiled version mentioned in Brian Ripley's announcement, so I haven't been able to test anything.
>
> Q2: From DM's first reply:
>
> "... but that is something
>   that is either ..., or avoided by re-installing without byte compiling."
>
> How would the latter be done? And would it be possible for the Windows binary distro, or just for the source distro? My reading of the first email about changes in R-2.14-devel was that the distro would be byte-compiled already.

What I had in mind was re-installing from source, since the binary 
distribution will definitely be byte compiled.  But there may be other 
ways to do it.

Duncan Murdoch


From e2holmes at gmail.com  Thu Jul  7 09:39:09 2011
From: e2holmes at gmail.com (Eli Holmes)
Date: Thu, 7 Jul 2011 00:39:09 -0700
Subject: [Rd] Vignette pdfs missing from zip file after Rcmd build --binary
	call
Message-ID: <CALtun9uhUyiMrqGHLxptsG9jvEwUAnb0JM3uzLVJbKH0JJfd2w@mail.gmail.com>

Hi,

I have been building R packages for awhile on Windows, and I recently
upgraded R and all required package creation tools to 2.13.0.  I
understand that there have been changes in that the R CMD build
command no longer alters the source directory, cf comments in
http://tolstoy.newcastle.edu.au/R/e13/help/11/04/10379.html
For example, if I issue the command Rcmd build FOO, then directory FOO
is not altered in contrast to earlier R versions where vignette pdfs
would be added to the source directory FOO.

I have no problem creating the FOO.tar.gz file, and it has the
vignette pdfs in the inst/doc as usual.   The problem is the .zip file
that I create for Windows users.  In earlier versions of R, this .zip
file had the newly created vignette pdfs but now it does not.  Here is
what I see:

source FOO:
inst/doc
foo.Rnw

FOO.tar.gz   (after Rcmd build FOO)
inst/doc
foo.pdf
foo.Rnw

FOO.zip (after Rcmd build --binary FOO or R CMD install --build FOO)
inst/doc
foo.Rnw

Yes, I can manually move the pdf files from the tar.gz directory to
the zip directory, but there are many and I am liable to make
mistakes.

I suppose I could unzip the tar.gz file and then make a temporary
source file FOOtmp and run Rcmd build --binary FOOtmp, but I am hoping
there is an easier way.

Things I have tried unsuccessful
 I tried using the tar.gz file as the source
Rcmd build --binary FOO.tar.gz   (got error)
R CMD install --build FOO.tar.gz (same error)

I tried using some of the options for R CMD install:
--force-multiarch  and --merge-multiarch.  No change.

Any ideas?

Thanks!


From simon.urbanek at r-project.org  Thu Jul  7 18:14:40 2011
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Thu, 7 Jul 2011 12:14:40 -0400
Subject: [Rd] Vignette pdfs missing from zip file after Rcmd build
	--binary call
In-Reply-To: <CALtun9uhUyiMrqGHLxptsG9jvEwUAnb0JM3uzLVJbKH0JJfd2w@mail.gmail.com>
References: <CALtun9uhUyiMrqGHLxptsG9jvEwUAnb0JM3uzLVJbKH0JJfd2w@mail.gmail.com>
Message-ID: <45A85158-A3B6-4B03-A15D-B34037D12037@r-project.org>


On Jul 7, 2011, at 3:39 AM, Eli Holmes wrote:

> Hi,
> 
> I have been building R packages for awhile on Windows, and I recently
> upgraded R and all required package creation tools to 2.13.0.  I
> understand that there have been changes in that the R CMD build
> command no longer alters the source directory, cf comments in
> http://tolstoy.newcastle.edu.au/R/e13/help/11/04/10379.html
> For example, if I issue the command Rcmd build FOO, then directory FOO
> is not altered in contrast to earlier R versions where vignette pdfs
> would be added to the source directory FOO.
> 
> I have no problem creating the FOO.tar.gz file, and it has the
> vignette pdfs in the inst/doc as usual.   The problem is the .zip file
> that I create for Windows users.  In earlier versions of R, this .zip
> file had the newly created vignette pdfs but now it does not.  Here is
> what I see:
> 
> source FOO:
> inst/doc
> foo.Rnw
> 
> FOO.tar.gz   (after Rcmd build FOO)
> inst/doc
> foo.pdf
> foo.Rnw
> 
> FOO.zip (after Rcmd build --binary FOO or R CMD install --build FOO)
> inst/doc
> foo.Rnw
> 

It sort of defeats the purpose to create a tar ball and then not use it ;)


> Yes, I can manually move the pdf files from the tar.gz directory to
> the zip directory, but there are many and I am liable to make
> mistakes.
> 
> I suppose I could unzip the tar.gz file and then make a temporary
> source file FOOtmp and run Rcmd build --binary FOOtmp, but I am hoping
> there is an easier way.
> 
> Things I have tried unsuccessful
> I tried using the tar.gz file as the source

That is indeed  the usual way.


> Rcmd build --binary FOO.tar.gz   (got error)
> R CMD install --build FOO.tar.gz (same error)

Can you be more specific as of what errors you get?  It may point to issues in the your package. Also the command is R CMD INSTALL not install.

Cheers,
Simon



> 
> I tried using some of the options for R CMD install:
> --force-multiarch  and --merge-multiarch.  No change.
> 
> Any ideas?
> 
> Thanks!
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 
> 


From saravanan.thirumuruganathan at gmail.com  Thu Jul  7 18:44:20 2011
From: saravanan.thirumuruganathan at gmail.com (Saravanan)
Date: Thu, 07 Jul 2011 11:44:20 -0500
Subject: [Rd] Suggestions for R-devel / R-help digest format
Message-ID: <4E15E264.2090004@gmail.com>

Hello,

I am passive reader of both R-devel and R-help mailing lists. I am 
sending the following comments to r-devel as it seemed more suitable. I 
am aware that this list uses GNU mailman for the list management. I have 
my options set that it sends a email digest. One thing I find is that 
the digest consists of emails that ordered temporarlly. For eg lets say 
there are two threads t1 and t2 and the emails arrive as e1 of t1, e2 of 
t2, e3 of t3  . The digest lists them as e1,e2 and then e3. Is it 
possible to somehow configure it as T1 : e1,e3 and then T2 : e2 ?

This is the digest format that google groups uses which is incredibly 
helpful as you can read all the messages in a thread. Additionally, it 
also helpfully includes a header that lists all the threads in digest so 
that you can jump to the one you are interested in. I checked the 
mailman options but could not find any.

Does anyone else have the same issue? It is not a big issue in R-devel 
but R-help is a much more high traffic mailing list. I am interested in 
hearing how you read/filter your digest mails in either R-help or other 
high volume mailing lists.

Regards,
Saravanan


From mailinglist.honeypot at gmail.com  Thu Jul  7 18:59:23 2011
From: mailinglist.honeypot at gmail.com (Steve Lianoglou)
Date: Thu, 7 Jul 2011 12:59:23 -0400
Subject: [Rd] Suggestions for R-devel / R-help digest format
In-Reply-To: <4E15E264.2090004@gmail.com>
References: <4E15E264.2090004@gmail.com>
Message-ID: <CAHA9McMHvYsrfwuRZxps9JkjEC1o0TAWVF2-G_v-mTT5_UF0zg@mail.gmail.com>

Hi,

On Thu, Jul 7, 2011 at 12:44 PM, Saravanan
<saravanan.thirumuruganathan at gmail.com> wrote:
> Hello,
>
> I am passive reader of both R-devel and R-help mailing lists. I am sending
> the following comments to r-devel as it seemed more suitable. I am aware
> that this list uses GNU mailman for the list management. I have my options
> set that it sends a email digest. One thing I find is that the digest
> consists of emails that ordered temporarlly. For eg lets say there are two
> threads t1 and t2 and the emails arrive as e1 of t1, e2 of t2, e3 of t3 ?.
> The digest lists them as e1,e2 and then e3. Is it possible to somehow
> configure it as T1 : e1,e3 and then T2 : e2 ?
>
> This is the digest format that google groups uses which is incredibly
> helpful as you can read all the messages in a thread. Additionally, it also
> helpfully includes a header that lists all the threads in digest so that you
> can jump to the one you are interested in. I checked the mailman options but
> could not find any.
>
> Does anyone else have the same issue? It is not a big issue in R-devel but
> R-help is a much more high traffic mailing list. I am interested in hearing
> how you read/filter your digest mails in either R-help or other high volume
> mailing lists.

I solve this "issue" by having set up 1 mailing-list only gmail
account I use for, well, all of mailing lists. I  configure each
mailing list to send me all emails to this address as they come (not
as digest) and setup filters in gmail to tag each email for each group
with an appropriate group label, eg. all R emails get an "R" label,
bioconductor gets a "bioconductor" label, etc.

This way, I can check each mailing list with the frequency I desire
and my "normal" email boxes don't get flooded. gmail then takes care
of the threading as you'd expect for each mailing list ...

Maybe you'd find that setup helpful.

-steve

-- 
Steve Lianoglou
Graduate Student: Computational Systems Biology
?| Memorial Sloan-Kettering Cancer Center
?| Weill Medical College of Cornell University
Contact Info: http://cbio.mskcc.org/~lianos/contact


From brian at braverock.com  Thu Jul  7 19:02:52 2011
From: brian at braverock.com (Brian G. Peterson)
Date: Thu, 07 Jul 2011 12:02:52 -0500
Subject: [Rd] Suggestions for R-devel / R-help digest format
In-Reply-To: <4E15E264.2090004@gmail.com>
References: <4E15E264.2090004@gmail.com>
Message-ID: <1310058172.17786.13.camel@brian-rcg>

On Thu, 2011-07-07 at 11:44 -0500, Saravanan wrote:
> Hello,
> 
> I am passive reader of both R-devel and R-help mailing lists. I am 
> sending the following comments to r-devel as it seemed more suitable. I 
> am aware that this list uses GNU mailman for the list management. I have 
> my options set that it sends a email digest. One thing I find is that 
> the digest consists of emails that ordered temporarlly. For eg lets say 
> there are two threads t1 and t2 and the emails arrive as e1 of t1, e2 of 
> t2, e3 of t3  . The digest lists them as e1,e2 and then e3. Is it 
> possible to somehow configure it as T1 : e1,e3 and then T2 : e2 ?
> 
> This is the digest format that google groups uses which is incredibly 
> helpful as you can read all the messages in a thread. Additionally, it 
> also helpfully includes a header that lists all the threads in digest so 
> that you can jump to the one you are interested in. I checked the 
> mailman options but could not find any.
> 
> Does anyone else have the same issue? It is not a big issue in R-devel 
> but R-help is a much more high traffic mailing list. I am interested in 
> hearing how you read/filter your digest mails in either R-help or other 
> high volume mailing lists.

This really has nothing to do with R, but rather mailman.

I use folders, filtered on the server using SIEVE and/or procmail.  No
digest required. I get the mails immediately, not later in the day or
the next day,  and can use all my various email clients easily to
read/respond.

mailman supports a MIME digest format that includes a table of contents
with links to each MIME part.  mailman does not support a threaded
digest, to the best of my knowledge.

Regards,

   - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From saravanan.thirumuruganathan at gmail.com  Thu Jul  7 21:26:08 2011
From: saravanan.thirumuruganathan at gmail.com (Saravanan)
Date: Thu, 07 Jul 2011 14:26:08 -0500
Subject: [Rd] Suggestions for R-devel / R-help digest format
In-Reply-To: <1310058172.17786.13.camel@brian-rcg>
References: <4E15E264.2090004@gmail.com> <1310058172.17786.13.camel@brian-rcg>
Message-ID: <4E160850.1040404@gmail.com>

Thanks Steve and Brian !

Probably, I will create a gmail account for mailing lists and let it 
take care of the threading.

Regards,
Saravanan

On 07/07/2011 12:02 PM, Brian G. Peterson wrote:
> On Thu, 2011-07-07 at 11:44 -0500, Saravanan wrote:
>> Hello,
>>
>> I am passive reader of both R-devel and R-help mailing lists. I am
>> sending the following comments to r-devel as it seemed more suitable. I
>> am aware that this list uses GNU mailman for the list management. I have
>> my options set that it sends a email digest. One thing I find is that
>> the digest consists of emails that ordered temporarlly. For eg lets say
>> there are two threads t1 and t2 and the emails arrive as e1 of t1, e2 of
>> t2, e3 of t3  . The digest lists them as e1,e2 and then e3. Is it
>> possible to somehow configure it as T1 : e1,e3 and then T2 : e2 ?
>>
>> This is the digest format that google groups uses which is incredibly
>> helpful as you can read all the messages in a thread. Additionally, it
>> also helpfully includes a header that lists all the threads in digest so
>> that you can jump to the one you are interested in. I checked the
>> mailman options but could not find any.
>>
>> Does anyone else have the same issue? It is not a big issue in R-devel
>> but R-help is a much more high traffic mailing list. I am interested in
>> hearing how you read/filter your digest mails in either R-help or other
>> high volume mailing lists.
> This really has nothing to do with R, but rather mailman.
>
> I use folders, filtered on the server using SIEVE and/or procmail.  No
> digest required. I get the mails immediately, not later in the day or
> the next day,  and can use all my various email clients easily to
> read/respond.
>
> mailman supports a MIME digest format that includes a table of contents
> with links to each MIME part.  mailman does not support a threaded
> digest, to the best of my knowledge.
>
> Regards,
>
>     - Brian
>


From hpages at fhcrc.org  Thu Jul  7 23:53:27 2011
From: hpages at fhcrc.org (=?ISO-8859-1?Q?Herv=E9_Pag=E8s?=)
Date: Thu, 07 Jul 2011 14:53:27 -0700
Subject: [Rd] Recent and upcoming changes to R-devel
In-Reply-To: <alpine.OSX.1.00.1107041124460.59432@tystie.local>
References: <alpine.OSX.1.00.1107041124460.59432@tystie.local>
Message-ID: <4E162AD7.10609@fhcrc.org>

Hi,

On 11-07-04 05:08 AM, Prof Brian Ripley wrote:
> There was an R-core meeting the week before last, and various planned
> changes will appear in R-devel over the next few weeks.
>
> These are changes planned for R 2.14.0 scheduled for Oct 31. As we are
> sick of people referring to R-devel as '2.14' or '2.14.0', that version
> number will not be used until we reach 2.14.0 alpha.

Now with R-devel r56301:

   > R.version.string
   [1] "R Under development (unstable) (2011-07-06 r56301)"

But:

   > R.Version()[c("major", "minor")]
   $major
   [1] "2"

   $minor
   [1] "14.0"

   > R.version[c("major", "minor")]
       _
   major 2
   minor 14.0

Not sure what's the benefit...

> You will be able to
> have a package depend on an svn version number when referring to R-devel
> rather than using R (>= 2.14.0).

Isn't it that R 2.13 patched and R devel share the same svn version
numbers? So using something like R (>= r56301) doesn't actually mean
anything.

Cheers,
H.

>
> All packages are installed with lazy-loading (there were 72 CRAN
> packages and 8 BioC packages which opted out). This means that the code
> is always parsed at install time which inter alia simplifies the
> descriptions. R 2.13.1 RC warns on installation about packages which ask
> not to be lazy-loaded, and R-devel ignores such requests (with a warning).
>
> In the near future all packages will have a name space. If the sources
> do not contain one, a default NAMESPACE file will be added. This again
> will simplify the descriptions and also a lot of internal code.
> Maintainers of packages without name spaces (currently 42% of CRAN) are
> encouraged to add one themselves.
>
> R-devel is installed with the base and recommended packages
> byte-compiled (the equivalent of 'make bytecode' in R 2.13.x, but done
> less inefficiently). There is a new option
> R CMD INSTALL --byte-compile
> to byte-compile contributed packages, but that remains optional.
> Byte-compilation is quite expensive (so you definitely want to do it at
> install time, which requires lazy-loading), and relatively few packages
> benefit appreciably from byte-compilation. A larger number of packages
> benefit from byte-compilation of R itself: for example AER runs its
> checks 10% faster. The byte-compiler technology is thanks to Luke Tierney.
>
> There is support for figures in Rd files: currently with a first-pass
> implementation (thanks to Duncan Murdoch).
>


-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fhcrc.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From mdowle at mdowle.plus.com  Fri Jul  8 13:17:32 2011
From: mdowle at mdowle.plus.com (Matthew Dowle)
Date: Fri, 8 Jul 2011 12:17:32 +0100
Subject: [Rd] Suggestions for R-devel / R-help digest format
References: <4E15E264.2090004@gmail.com> <1310058172.17786.13.camel@brian-rcg>
	<4E160850.1040404@gmail.com>
Message-ID: <iv6p0g$a62$1@dough.gmane.org>


Don't most people use a newsreader? For example, pointed to here :
    gmane.comp.lang.r.general
    gmane.comp.lang.r.devel

IIUC, NNTP downloads headers only, when you open any post it downloads the 
body at that
point. So it's more efficient than email (assuming you don't open every 
single post). I guess
RSS is similar/better. Newsreaders handle threading and you can watch/ignore 
threads easily.

Actually subscribing via email?  The only reason I am subscribed is to post 
unmoderated (and to
encourage Martin with +1 on his subscriber count); I have email delivery 
turned off in the mailman
settings.  Thought everyone did that!   If I counted correctly, there are 36 
gmane mirrors for
various packages and sigs. You can watch all these (including r-devel and 
r-help) via gmane without
needing to subscribe on mailman at all.

Matthew

"Saravanan" <saravanan.thirumuruganathan at gmail.com> wrote in message 
news:4E160850.1040404 at gmail.com...
> Thanks Steve and Brian !
>
> Probably, I will create a gmail account for mailing lists and let it take 
> care of the threading.
>
> Regards,
> Saravanan
>
> On 07/07/2011 12:02 PM, Brian G. Peterson wrote:
>> On Thu, 2011-07-07 at 11:44 -0500, Saravanan wrote:
>>> Hello,
>>>
>>> I am passive reader of both R-devel and R-help mailing lists. I am
>>> sending the following comments to r-devel as it seemed more suitable. I
>>> am aware that this list uses GNU mailman for the list management. I have
>>> my options set that it sends a email digest. One thing I find is that
>>> the digest consists of emails that ordered temporarlly. For eg lets say
>>> there are two threads t1 and t2 and the emails arrive as e1 of t1, e2 of
>>> t2, e3 of t3  . The digest lists them as e1,e2 and then e3. Is it
>>> possible to somehow configure it as T1 : e1,e3 and then T2 : e2 ?
>>>
>>> This is the digest format that google groups uses which is incredibly
>>> helpful as you can read all the messages in a thread. Additionally, it
>>> also helpfully includes a header that lists all the threads in digest so
>>> that you can jump to the one you are interested in. I checked the
>>> mailman options but could not find any.
>>>
>>> Does anyone else have the same issue? It is not a big issue in R-devel
>>> but R-help is a much more high traffic mailing list. I am interested in
>>> hearing how you read/filter your digest mails in either R-help or other
>>> high volume mailing lists.
>> This really has nothing to do with R, but rather mailman.
>>
>> I use folders, filtered on the server using SIEVE and/or procmail.  No
>> digest required. I get the mails immediately, not later in the day or
>> the next day,  and can use all my various email clients easily to
>> read/respond.
>>
>> mailman supports a MIME digest format that includes a table of contents
>> with links to each MIME part.  mailman does not support a threaded
>> digest, to the best of my knowledge.
>>
>> Regards,
>>
>>     - Brian
>>
>


From e2holmes at gmail.com  Fri Jul  8 01:29:24 2011
From: e2holmes at gmail.com (Eli Holmes)
Date: Thu, 7 Jul 2011 16:29:24 -0700
Subject: [Rd] Vignette pdfs missing from zip file after Rcmd build
 --binary call
In-Reply-To: <45A85158-A3B6-4B03-A15D-B34037D12037@r-project.org>
References: <CALtun9uhUyiMrqGHLxptsG9jvEwUAnb0JM3uzLVJbKH0JJfd2w@mail.gmail.com>
	<45A85158-A3B6-4B03-A15D-B34037D12037@r-project.org>
Message-ID: <CALtun9torkEvcQGLx-rS=dtgpkB1Vs+gOH5TYXxNTgQHmF3CSA@mail.gmail.com>

Thanks for the reply.  In a round about way, it helped me figure out
how to solve the problem.   I had always called Rcmd build --binary
first to make a zip file for windows users and in <2.11.0 this command
added all the vignette pdfs to the source directory, and then I called
Rcmd build --no-vignettes to prepare the tar.gz file.  This made me
realize that someone on a unix machine would not build the tar.gz file
this way, and I should follow a more unix-like build sequence.

For the sake of documenting my solution, here's my responses and how
they led to my solution.  Note, I'm buildiing packages on a windows
machine.

> It sort of defeats the purpose to create a tar ball and then not use it ;)

??  I do use it.  The unix and mac users download the tar.gz file to
install the package.  I create windows binaries for windows users.
In the past, I created the zip file from the source directory not the
tar.gz built from the source directory using the command Rcmd build
--binary FOO, where FOO is the source directory.  BTW, I know the
tar.gz file build on a windows machine is not perfect for mac/unix
users; this is just for development in a small group with mac and
windows users.

I've been building packages for years with 2.11.0 and below, but
something has changed in 2.13.0 in how vignette pdfs are being
transferred to the package zip files.


>> Things I have tried unsuccessful
>> I tried using the tar.gz file as the source
>
> That is indeed ?the usual way.
>
> Can you be more specific as of what errors you get?  It may point to issues in the your package. Also the command is R CMD INSTALL not install.

Sorry I meant to capitalize 'install'.  I do call it that way.
The error I got was
Warning: invalid package 'FOO.tar.gz'
Error: ERROR: no package specified

I assumed the error had to do with a tar.gz file not being a valid
'target' for Rcmd build.  I have never seen a tar.gz file used in a
Rcmd build call before.  But your comment suggested that FOO.tar.gz
should work, and on closer inspection, I realized I had misspelled the
filename.   Once I fixed that, R CMD install --build FOO.tar.gz worked
and built the zip file.

>> Rcmd build --binary FOO.tar.gz ?(got error)
cannot change to directory 'FOO.tar.gz'  (ok, I just won't use that)
>> R CMD INSTALL --build FOO.tar.gz (built the package binaries in the zip file, so I'll switch to this)


Once I got here, I realized I could just build the zip file using 'R
CMD INSTALL --build FOO.tar.gz' as appeared to be intended.  But on
closer inspection, my tar.gz file also appeared to be missing some of
the vignette pdfs in a package that included a makefile in inst/doc.
It turned out that this file did not have the line 'texi2dvi --pdf
*.tex' to create pdfs from all the tex files after Rcmd's sweave call
before it called the makefile.  In R 2.11.0, these pdfs were being
added to the source directory by the call 'Rcmd build --binary' .   I
never ran 'Rcmd build' first so never noticed that the tar.gz file
from 'Rcmd build' would be missing vignette pdfs.

I added the line texi2dvi --pdf *.tex' to the makefile and everything
seems ok now.  So my new steps for building unix and windows packages
is the following:

Rcmd build FOO (build the tar.gz file)
R CMD INSTALL --build FOO.tar.gz  (build the windows binaries; I'm
doing this on a windows machine btw).

Again, thanks!

--Eli


From lawrence.michael at gene.com  Sat Jul  9 00:17:23 2011
From: lawrence.michael at gene.com (Michael Lawrence)
Date: Fri, 8 Jul 2011 15:17:23 -0700
Subject: [Rd] invalid body argument for function
In-Reply-To: <4E0CA5E2.9040307@gmail.com>
References: <BANLkTi=BhB-Qg_S_=MRKpdKg3q+-F=EGxA@mail.gmail.com>
	<4E0CA5E2.9040307@gmail.com>
Message-ID: <CAOQ5NyfG7FuK9wEvUCUcuJ3kCibN0t3cv0KC7kX9c+OAjHrufQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20110708/fc4b86a5/attachment.pl>

From pdalgd at gmail.com  Sat Jul  9 01:34:06 2011
From: pdalgd at gmail.com (peter dalgaard)
Date: Sat, 9 Jul 2011 01:34:06 +0200
Subject: [Rd] invalid body argument for function
In-Reply-To: <CAOQ5NyfG7FuK9wEvUCUcuJ3kCibN0t3cv0KC7kX9c+OAjHrufQ@mail.gmail.com>
References: <BANLkTi=BhB-Qg_S_=MRKpdKg3q+-F=EGxA@mail.gmail.com>
	<4E0CA5E2.9040307@gmail.com>
	<CAOQ5NyfG7FuK9wEvUCUcuJ3kCibN0t3cv0KC7kX9c+OAjHrufQ@mail.gmail.com>
Message-ID: <8AFF03C6-E8F9-4BE2-B1C8-1FC4B8665C80@gmail.com>


On Jul 9, 2011, at 00:17 , Michael Lawrence wrote:

> On Thu, Jun 30, 2011 at 9:35 AM, Duncan Murdoch <murdoch.duncan at gmail.com>wrote:
> 
>> On 29/06/2011 9:09 PM, Michael Lawrence wrote:
>> 
>>> Hi guys,
>>> 
>>> Looks like mkCLOSXP cannot handle external pointers as the function body.
>>> Work around is obvious, but I guess it's a bug nonetheless.
>>> 
>> 
>> I don't know if it's a bug.  The mkCLOSXP code has a list of types that it
>> accepts; external pointers and environments aren't in that list, so you get
>> the same error you saw with
>> 
>> fun<- eval(substitute(function() x, list(x = environment())))
>> 
>> There's a comment
>> 
>> /* This is called by function() {}, where an invalid
>>  body should be impossible. When called from
>>  other places (eg do_asfunction) they
>>  should do this checking in advance */
>> 
>> 
>> I don't know whether there's any reason for the restriction, but I'd want
>> to look closely at what gets done with the body to make sure that putting an
>> environment or external pointer or other weird type there doesn't cause
>> other problems.
>> 
>> 
> Ok, well if this is user error, then the error message should not ask the
> user to report it as a bug. The comment cited above is incorrect, since it
> is indeed possible to give function() an incorrect body.

Yes. Of course, the message predates certain object types, so things may have gotten out of sync. 

I don't actually see why is would be a problem to allow any (user-visible) object as the body of a function. All that happens is that the return value of the function would be that object, no?

There was at some point a push towards regularizing the language, and these checks may be a relic of that.

(The current code is "svn blame"d to me, but it was a branch update in Dec.1999, and I can't really be bothered to go find out on which branch it happened on and who did it. Besides, the wench is dead...)

-- 
Peter Dalgaard
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From ivo.welch at gmail.com  Sat Jul  9 05:47:08 2011
From: ivo.welch at gmail.com (ivo welch)
Date: Fri, 8 Jul 2011 20:47:08 -0700
Subject: [Rd] short documentation suggestion for "by" --- reference
 simplify2array and ave
Message-ID: <CAPr7RtWWUDnQRPdx-6-Mc5vX7Shx1vAcTEL=VeDRXF-Om=bcJg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20110708/1b9697a2/attachment.pl>

From spencer.graves at prodsyse.com  Sun Jul 10 05:44:21 2011
From: spencer.graves at prodsyse.com (Spencer Graves)
Date: Sat, 09 Jul 2011 20:44:21 -0700
Subject: [Rd] sort infelicity
Message-ID: <4E192015.5030305@prodsyse.com>

Hello:


       sort(c('A', 'b', 'C')) seems to produce different answers in R 
interactive than in "R CMD check", at least under both Fedora 13 and 
Windows 7 with Windows 7 sessionInfo() copied below:


       In interactive, the result is c('A', 'b', 'C');  with R CMD 
check, it is c('A', 'C', 'b').  This produced the infelicity of a bug in 
"R CMD check" that I could not replicate with interactive R because a 
*.Rd file contained the equivalent example of 
stopifnot(all.equal(sort(c('A', 'b', 'C')), c('A', 'b', 'C'))):  It 
worked just fine interactively but failed R CMD check.


       Once I understood this problem, it was easy to fix.  However, it 
was not easy to find, especially since I got the same problem under 
Fedora 13 Linux and Windows 7.


       This seems to be a sufficiently obscure anomaly that I thought 
someone might like to see it reported here.


       Best Wishes,
       Spencer Graves


 > sessionInfo()
R version 2.12.2 (2011-02-25)
Platform: x86_64-pc-mingw32/x64 (64-bit)

locale:
[1] LC_COLLATE=English_United States.1252
[2] LC_CTYPE=English_United States.1252
[3] LC_MONETARY=English_United States.1252
[4] LC_NUMERIC=C
[5] LC_TIME=English_United States.1252

attached base packages:
[1] splines   stats     graphics  grDevices utils     datasets  methods
[8] base

other attached packages:
[1] SIM_1.4-6      fda_2.2.6      zoo_1.6-5      RCurl_1.5-0.1  
bitops_1.0-4.1
[6] R2HTML_2.2     oce_0.3-1

loaded via a namespace (and not attached):
[1] grid_2.12.2     lattice_0.19-30 tools_2.12.2


From ripley at stats.ox.ac.uk  Sun Jul 10 08:56:22 2011
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 10 Jul 2011 07:56:22 +0100 (BST)
Subject: [Rd] sort infelicity
In-Reply-To: <4E192015.5030305@prodsyse.com>
References: <4E192015.5030305@prodsyse.com>
Message-ID: <alpine.LFD.2.02.1107100747230.12029@gannet.stats.ox.ac.uk>

Collation is locale-specific.  To stop problems such as that you 
encontered, where it matters 'R CMD check' uses LC_COLLATE=C (and 
documents it).  Otherwise package checks would be system-dependent, 
and maybe even user-dependent.

It really isn't a good idea to misuse examples in help files for 
regression tests, not least when the result is not system-independent.

I'm afraid the only obscurity here is why you didn't undertand basic 
facts about collation (facts which are linked to from the help page of 
sort()).  E.g.

      The collating sequence of locales such as ?en_US? is
      normally different from ?C? (which should use ASCII) and can be
      surprising.  Beware of making _any_ assumptions about the
      collation order

Please do also take note of what the posting guide asked you to do 
about obsolete versions of R.

On Sat, 9 Jul 2011, Spencer Graves wrote:

> Hello:
>
>
>      sort(c('A', 'b', 'C')) seems to produce different answers in R 
> interactive than in "R CMD check", at least under both Fedora 13 and Windows 
> 7 with Windows 7 sessionInfo() copied below:
>
>
>      In interactive, the result is c('A', 'b', 'C');  with R CMD check, it 
> is c('A', 'C', 'b').  This produced the infelicity of a bug in "R CMD check" 
> that I could not replicate with interactive R because a *.Rd file contained 
> the equivalent example of stopifnot(all.equal(sort(c('A', 'b', 'C')), c('A', 
> 'b', 'C'))):  It worked just fine interactively but failed R CMD check.
>
>
>      Once I understood this problem, it was easy to fix.  However, it was 
> not easy to find, especially since I got the same problem under Fedora 13 
> Linux and Windows 7.
>
>
>      This seems to be a sufficiently obscure anomaly that I thought someone 
> might like to see it reported here.
>
>
>      Best Wishes,
>      Spencer Graves
>
>
>> sessionInfo()
> R version 2.12.2 (2011-02-25)
> Platform: x86_64-pc-mingw32/x64 (64-bit)
>
> locale:
> [1] LC_COLLATE=English_United States.1252
> [2] LC_CTYPE=English_United States.1252
> [3] LC_MONETARY=English_United States.1252
> [4] LC_NUMERIC=C
> [5] LC_TIME=English_United States.1252
>
> attached base packages:
> [1] splines   stats     graphics  grDevices utils     datasets  methods
> [8] base
>
> other attached packages:
> [1] SIM_1.4-6      fda_2.2.6      zoo_1.6-5      RCurl_1.5-0.1 
> bitops_1.0-4.1
> [6] R2HTML_2.2     oce_0.3-1
>
> loaded via a namespace (and not attached):
> [1] grid_2.12.2     lattice_0.19-30 tools_2.12.2
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From pdalgd at gmail.com  Sun Jul 10 09:03:50 2011
From: pdalgd at gmail.com (peter dalgaard)
Date: Sun, 10 Jul 2011 09:03:50 +0200
Subject: [Rd] sort infelicity
In-Reply-To: <4E192015.5030305@prodsyse.com>
References: <4E192015.5030305@prodsyse.com>
Message-ID: <BEAC23EE-E846-4ABE-A820-0368062A0112@gmail.com>


On Jul 10, 2011, at 05:44 , Spencer Graves wrote:

> Hello:
> 
> 
>      sort(c('A', 'b', 'C')) seems to produce different answers in R interactive than in "R CMD check", at least under both Fedora 13 and Windows 7 with Windows 7 sessionInfo() copied below:
> 
> 
>      In interactive, the result is c('A', 'b', 'C');  with R CMD check, it is c('A', 'C', 'b').  This produced the infelicity of a bug in "R CMD check" that I could not replicate with interactive R because a *.Rd file contained the equivalent example of stopifnot(all.equal(sort(c('A', 'b', 'C')), c('A', 'b', 'C'))):  It worked just fine interactively but failed R CMD check.
> 
> 
>      Once I understood this problem, it was easy to fix.  However, it was not easy to find, especially since I got the same problem under Fedora 13 Linux and Windows 7.
> 
> 
>      This seems to be a sufficiently obscure anomaly that I thought someone might like to see it reported here.
> 

Well, the problem is here:
[snip]
> 
> locale:
> [1] LC_COLLATE=English_United States.1252
==========================================
> [2] LC_CTYPE=English_United States.1252
> [3] LC_MONETARY=English_United States.1252
> [4] LC_NUMERIC=C
> [5] LC_TIME=English_United States.1252

All checks in R (unless we overlooked some) run with LC_COLLATE=C, because otherwise they give different results in different locales. One notorious example is that people expect that a file or an object called "zzz" comes out last in a sort, but Estonian sorts "z" between "s" and "t"...

Notice that your .Rd example would, for the same reason, break for people with different locale settings.



-- 
Peter Dalgaard
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From spencer.graves at prodsyse.com  Sun Jul 10 17:26:35 2011
From: spencer.graves at prodsyse.com (Spencer Graves)
Date: Sun, 10 Jul 2011 08:26:35 -0700
Subject: [Rd] sort infelicity
In-Reply-To: <BEAC23EE-E846-4ABE-A820-0368062A0112@gmail.com>
References: <4E192015.5030305@prodsyse.com>
	<BEAC23EE-E846-4ABE-A820-0368062A0112@gmail.com>
Message-ID: <4E19C4AB.9050805@prodsyse.com>

       Thanks to Professors Ripley and Dalgaard for explaining the problem.


       In this case, I actually looked at the help page for "sort" but 
didn't see an option in the argument string that suggested a source for 
the problem:  I saw no argument like locale=getOption('locale').  I 
fixed the problem, as suggested, by sorting the theoretical answer 
before stopifnot(all.equal(...)).


       Thanks again,
       Spencer Graves


On 7/10/2011 12:03 AM, peter dalgaard wrote:
> On Jul 10, 2011, at 05:44 , Spencer Graves wrote:
>
>> Hello:
>>
>>
>>       sort(c('A', 'b', 'C')) seems to produce different answers in R interactive than in "R CMD check", at least under both Fedora 13 and Windows 7 with Windows 7 sessionInfo() copied below:
>>
>>
>>       In interactive, the result is c('A', 'b', 'C');  with R CMD check, it is c('A', 'C', 'b').  This produced the infelicity of a bug in "R CMD check" that I could not replicate with interactive R because a *.Rd file contained the equivalent example of stopifnot(all.equal(sort(c('A', 'b', 'C')), c('A', 'b', 'C'))):  It worked just fine interactively but failed R CMD check.
>>
>>
>>       Once I understood this problem, it was easy to fix.  However, it was not easy to find, especially since I got the same problem under Fedora 13 Linux and Windows 7.
>>
>>
>>       This seems to be a sufficiently obscure anomaly that I thought someone might like to see it reported here.
>>
> Well, the problem is here:
> [snip]
>> locale:
>> [1] LC_COLLATE=English_United States.1252
> ==========================================
>> [2] LC_CTYPE=English_United States.1252
>> [3] LC_MONETARY=English_United States.1252
>> [4] LC_NUMERIC=C
>> [5] LC_TIME=English_United States.1252
> All checks in R (unless we overlooked some) run with LC_COLLATE=C, because otherwise they give different results in different locales. One notorious example is that people expect that a file or an object called "zzz" comes out last in a sort, but Estonian sorts "z" between "s" and "t"...
>
> Notice that your .Rd example would, for the same reason, break for people with different locale settings.


From simon.urbanek at r-project.org  Tue Jul 12 03:23:45 2011
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Mon, 11 Jul 2011 21:23:45 -0400
Subject: [Rd] [datatable-help]  speeding up perception
In-Reply-To: <1310430115.12980.88.camel@netbook>
References: <CAPr7RtU44GO-7_m02-ek+ps3YcR_qr3m7vV8BSvhAYJ=F_bnbA@mail.gmail.com>
	<CAPr7RtVx1ao9bubQiCrf9LjgpdZdB24vSnjZvGxW4a8Sy-pKag@mail.gmail.com>
	<CAGam+C6yEFw-E+oT7tktDVY8L9KtJZ0aHrQ_y0VLyEMnZDmfSw@mail.gmail.com>
	<555C7B55-67D9-4DC6-90CB-BE8B0F620BFB@r-project.org>
	<1309851165.4231.40.camel@netbook>
	<1309889303.4231.60.camel@netbook>
	<E0D40E65-E676-4083-A058-090E3D2CC20C@r-project.org>
	<alpine.DEB.2.00.1107051808040.1928@luke-inspiron>
	<121657AE-3C91-4880-BC29-781A78B05278@comcast.net>
	<11C6B5AE-0B1F-4154-B21C-7A5B3ED08619@r-project.org>
	<1309941365.4231.122.camel@netbook>
	<40822836-425E-4266-B203-35B3357FAB6F@r-project.org>
	<alpine.DEB.2.00.1107060904060.1928@luke-inspiron>
	<1310430115.12980.88.camel@netbook>
Message-ID: <58C95DDB-ADA7-4FD9-832A-C038F9111E5C@r-project.org>

Matthew,

I was hoping I misunderstood you first proposal, but I suspect I did not ;).

Personally, I find  DT[1,V1 <- 3] highly disturbing - I would expect it to evaluate to
{ V1 <- 3; DT[1, V1] }
thus returning the first element of the third column.

I do understand that within(foo, expr, ...) was the motivation for passing expressions, but unlike within() the subsetting operator [ is not expected to take expression as its second argument. Such abuse is quite unexpected and I would say dangerous.

That said, I don't think it works, either. Taking you example and data.table form r-forge:

> m = matrix(1,nrow=100000,ncol=100)
> DF = as.data.frame(m)
> DT = as.data.table(m)
> for (i in 1:1000) DT[1,V1 <- 3]
> DT[1,]
     V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V12 V13 V14 V15 V16 V17 V18 V19 V20 V21
[1,]  1  1  1  1  1  1  1  1  1   1   1   1   1   1   1   1   1   1   1   1   1

as you can see, DT is not modified.

Also I suspect there is something quite amiss because even trivial things don't work:

> DF[1:4,1:4]
  V1 V2 V3 V4
1  3  1  1  1
2  1  1  1  1
3  1  1  1  1
4  1  1  1  1
> DT[1:4,1:4]
[1] 1 2 3 4


When I first saw your proposal, I thought you have rather something like
within(DT, V1[1] <- 3)
in mind which looks innocent enough but performs terribly (note that I had to scale down the loop by a factor of 100!!!):

> system.time(for (i in 1:10) within(DT, V1[1] <- 3))
   user  system elapsed 
  2.701   4.437   7.138 

With the for loop something like within(DF, for (i in 1:1000) V1[i] <- 3)) performs reasonably:

> system.time(within(DT, for (i in 1:1000) V1[i] <- 3))
   user  system elapsed 
  0.392   0.613   1.003 

(Note: system.time() can be misleading when within() is involved, because the expression is evaluated in a different environment so within() won't actually change the object in the  global environment - it also interacts with the possible duplication)

Cheers,
Simon

On Jul 11, 2011, at 8:21 PM, Matthew Dowle wrote:

> Thanks for the replies and info. An attempt at fast
> assign is now committed to data.table v1.6.3 on
> R-Forge. From NEWS :
> 
> o   Fast update is now implemented, FR#200.
>    DT[i,j]<-value is now handled by data.table in C rather
>    than falling through to data.frame methods.
> 
>    Thanks to Ivo Welch for raising speed issues on r-devel,
>    to Simon Urbanek for the suggestion, and Luke Tierney and
>    Simon for information on R internals.
> 
>    [<- syntax still incurs one working copy of the whole
>    table (as of R 2.13.0) due to R's [<- dispatch mechanism
>    copying to `*tmp*`, so, for ultimate speed and brevity,
>    'within' syntax is now available as follows.
> 
> o   A new 'within' argument has been added to [.data.table,
>    by default TRUE. It is very similar to the within()
>    function in base R. If an assignment appears in j, it
>    assigns to the column of DT, by reference; e.g.,
> 
>    DT[i,colname<-value]
> 
>    This syntax makes no copies of any part of memory at all.
> 
>> m = matrix(1,nrow=100000,ncol=100)
>> DF = as.data.frame(m)
>> DT = as.data.table(m)
>> system.time(for (i in 1:1000) DF[1,1] <- 3)
>       user  system elapsed 
>    287.730 323.196 613.453 
>> system.time(for (i in 1:1000) DT[1,V1 <- 3])
>       user  system elapsed 
>      1.152   0.004   1.161         # 528 times faster
> 
> Please note :
> 
>    *******************************************************
>    **  Within syntax is presently highly experimental.  **
>    *******************************************************
> 
> http://datatable.r-forge.r-project.org/
> 
> 
> On Wed, 2011-07-06 at 09:08 -0500, luke-tierney at uiowa.edu wrote:
>> On Wed, 6 Jul 2011, Simon Urbanek wrote:
>> 
>>> Interesting, and I stand corrected:
>>> 
>>>> x = data.frame(a=1:n,b=1:n)
>>>> .Internal(inspect(x))
>>> @103511c00 19 VECSXP g0c2 [OBJ,NAM(2),ATT] (len=2, tl=0)
>>> @102c7b000 13 INTSXP g0c7 [] (len=100000, tl=0) 1,2,3,4,5,...
>>> @102af3000 13 INTSXP g0c7 [] (len=100000, tl=0) 1,2,3,4,5,...
>>> 
>>>> x[1,1]=42L
>>>> .Internal(inspect(x))
>>> @10349c720 19 VECSXP g0c2 [OBJ,NAM(2),ATT] (len=2, tl=0)
>>> @102c19000 13 INTSXP g0c7 [] (len=100000, tl=0) 42,2,3,4,5,...
>>> @102b55000 13 INTSXP g0c7 [] (len=100000, tl=0) 1,2,3,4,5,...
>>> 
>>>> x[[1]][1]=42L
>>>> .Internal(inspect(x))
>>> @103511a78 19 VECSXP g1c2 [OBJ,MARK,NAM(2),ATT] (len=2, tl=0)
>>> @102e65000 13 INTSXP g0c7 [] (len=100000, tl=0) 42,2,3,4,5,...
>>> @101f14000 13 INTSXP g1c7 [MARK] (len=100000, tl=0) 1,2,3,4,5,...
>>> 
>>>> x[[1]][1]=42L
>>>> .Internal(inspect(x))
>>> @10349c800 19 VECSXP g0c2 [OBJ,NAM(2),ATT] (len=2, tl=0)
>>> @102a2f000 13 INTSXP g0c7 [] (len=100000, tl=0) 42,2,3,4,5,...
>>> @102ec7000 13 INTSXP g0c7 [] (len=100000, tl=0) 1,2,3,4,5,...
>>> 
>>> 
>>> I have R to release ;) so I won't be looking into this right now, but it's something worth investigating ... Since all the inner contents have NAMED=0 I would not expect any duplication to be needed, but apparently becomes so is at some point ...
>> 
>> 
>> The internals assume in various places that deep copies are made (one
>> of the reasons NAMED setings are not propagated to sub-sturcture).
>> The main issues are avoiding cycles and that there is no easy way to
>> check for sharing.  There may be some circumstances in which a shallow
>> copy would be OK but making sure it would be in all cases is probably
>> more trouble than it is worth at this point. (I've tried this in the
>> past in a few cases and always had to back off.)
>> 
>> 
>> Best,
>> 
>> luke
>> 
>>> 
>>> Cheers,
>>> Simon
>>> 
>>> 
>>> On Jul 6, 2011, at 4:36 AM, Matthew Dowle wrote:
>>> 
>>>> 
>>>> On Tue, 2011-07-05 at 21:11 -0400, Simon Urbanek wrote:
>>>>> No subassignment function satisfies that condition, because you can always call them directly. However, that doesn't stop the default method from making that assumption, so I'm not sure it's an issue.
>>>>> 
>>>>> David, Just to clarify - the data frame content is not copied, we are talking about the vector holding columns.
>>>> 
>>>> If it is just the vector holding the columns that is copied (and not the
>>>> columns themselves), why does n make a difference in this test (on R
>>>> 2.13.0)?
>>>> 
>>>>> n = 1000
>>>>> x = data.frame(a=1:n,b=1:n)
>>>>> system.time(for (i in 1:1000) x[1,1] <- 42L)
>>>>  user  system elapsed
>>>> 0.628   0.000   0.628
>>>>> n = 100000
>>>>> x = data.frame(a=1:n,b=1:n)      # still 2 columns, but longer columns
>>>>> system.time(for (i in 1:1000) x[1,1] <- 42L)
>>>>  user  system elapsed
>>>> 20.145   1.232  21.455
>>>>> 
>>>> 
>>>> With $<- :
>>>> 
>>>>> n = 1000
>>>>> x = data.frame(a=1:n,b=1:n)
>>>>> system.time(for (i in 1:1000) x$a[1] <- 42L)
>>>>  user  system elapsed
>>>> 0.304   0.000   0.307
>>>>> n = 100000
>>>>> x = data.frame(a=1:n,b=1:n)
>>>>> system.time(for (i in 1:1000) x$a[1] <- 42L)
>>>>  user  system elapsed
>>>> 37.586   0.388  38.161
>>>>> 
>>>> 
>>>> If it's because the 1st column needs to be copied (only) because that's
>>>> the one being assigned to (in this test), that magnitude of slow down
>>>> doesn't seem consistent with the time of a vector copy of the 1st
>>>> column :
>>>> 
>>>>> n=100000
>>>>> v = 1:n
>>>>> system.time(for (i in 1:1000) v[1] <- 42L)
>>>>  user  system elapsed
>>>> 0.016   0.000   0.017
>>>>> system.time(for (i in 1:1000) {v2=v;v2[1] <- 42L})
>>>>  user  system elapsed
>>>> 1.816   1.076   2.900
>>>> 
>>>> Finally, increasing the number of columns, again only the 1st is
>>>> assigned to :
>>>> 
>>>>> n=100000
>>>>> x = data.frame(rep(list(1:n),100))
>>>>> dim(x)
>>>> [1] 100000    100
>>>>> system.time(for (i in 1:1000) x[1,1] <- 42L)
>>>>  user  system elapsed
>>>> 167.974  50.903 219.711
>>>>> 
>>>> 
>>>> 
>>>> 
>>>>> 
>>>>> Cheers,
>>>>> Simon
>>>>> 
>>>>> Sent from my iPhone
>>>>> 
>>>>> On Jul 5, 2011, at 9:01 PM, David Winsemius <dwinsemius at comcast.net> wrote:
>>>>> 
>>>>>> 
>>>>>> On Jul 5, 2011, at 7:18 PM, <luke-tierney at uiowa.edu> <luke-tierney at uiowa.edu> wrote:
>>>>>> 
>>>>>>> On Tue, 5 Jul 2011, Simon Urbanek wrote:
>>>>>>> 
>>>>>>>> 
>>>>>>>> On Jul 5, 2011, at 2:08 PM, Matthew Dowle wrote:
>>>>>>>> 
>>>>>>>>> Simon (and all),
>>>>>>>>> 
>>>>>>>>> I've tried to make assignment as fast as calling `[<-.data.table`
>>>>>>>>> directly, for user convenience. Profiling shows (IIUC) that it isn't
>>>>>>>>> dispatch, but x being copied. Is there a way to prevent '[<-' from
>>>>>>>>> copying x?
>>>>>>>> 
>>>>>>>> Good point, and conceptually, no. It's a subassignment after all - see R-lang 3.4.4 - it is equivalent to
>>>>>>>> 
>>>>>>>> `*tmp*` <- x
>>>>>>>> x <- `[<-`(`*tmp*`, i, j, value)
>>>>>>>> rm(`*tmp*`)
>>>>>>>> 
>>>>>>>> so there is always a copy involved.
>>>>>>>> 
>>>>>>>> Now, a conceptual copy doesn't mean real copy in R since R tries to keep the pass-by-value illusion while passing references in cases where it knows that modifications cannot occur and/or they are safe. The default subassign method uses that feature which means it can afford to not duplicate if there is only one reference -- then it's safe to not duplicate as we are replacing that only existing reference. And in the case of a matrix, that will be true at the latest from the second subassignment on.
>>>>>>>> 
>>>>>>>> Unfortunately the method dispatch (AFAICS) introduces one more reference in the dispatch chain so there will always be two references so duplication is necessary. Since we have only 0 / 1 / 2+ information on the references, we can't distinguish whether the second reference is due to the dispatch or due to the passed object having more than one reference, so we have to duplicate in any case. That is unfortunate, and I don't see a way around (unless we handle subassignment methods is some special way).
>>>>>>> 
>>>>>>> I don't believe dispatch is bumping NAMED (and a quick experiment
>>>>>>> seems to confirm this though I don't guarantee I did that right). The
>>>>>>> issue is that a replacement function implemented as a closure, which
>>>>>>> is the only option for a package, will always see NAMED on the object
>>>>>>> to be modified as 2 (because the value is obtained by forcing the
>>>>>>> argument promise) and so any R level assignments will duplicate.  This
>>>>>>> also isn't really an issue of imprecise reference counting -- there
>>>>>>> really are (at least) two legitimate references -- one though the
>>>>>>> argument and one through the caller's environment.
>>>>>>> 
>>>>>>> It would be good it we could come up with a way for packages to be
>>>>>>> able to define replacement functions that do not duplicate in cases
>>>>>>> where we really don't want them to, but this would require coming up
>>>>>>> with some sort of protocol, minimally involving an efficient way to
>>>>>>> detect whether a replacement funciton is being called in a replacement
>>>>>>> context or directly.
>>>>>> 
>>>>>> Would "$<-" always satisfy that condition. It would be big help to me if it could be designed to avoid duplication the rest of the data.frame.
>>>>>> 
>>>>>> --
>>>>>> 
>>>>>>> 
>>>>>>> There are some replacement functions that use C code to cheat, but
>>>>>>> these may create problems if called directly, so I won't advertise
>>>>>>> them.
>>>>>>> 
>>>>>>> Best,
>>>>>>> 
>>>>>>> luke
>>>>>>> 
>>>>>>>> 
>>>>>>>> Cheers,
>>>>>>>> Simon
>>>>>>>> 
>>>>>>>> 
>>>>>>>> 
>>>>>>> 
>>>>>>> --
>>>>>>> Luke Tierney
>>>>>>> Statistics and Actuarial Science
>>>>>>> Ralph E. Wareham Professor of Mathematical Sciences
>>>>>>> University of Iowa                  Phone:             319-335-3386
>>>>>>> Department of Statistics and        Fax:               319-335-3017
>>>>>>> Actuarial Science
>>>>>>> 241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
>>>>>>> Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu______________________________________________
>>>>>>> R-devel at r-project.org mailing list
>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>>> 
>>>>>> David Winsemius, MD
>>>>>> West Hartford, CT
>>>>>> 
>>>>>> 
>>>> 
>>>> 
>>>> 
>>> 
>>> 
>> 
>> -- 
>> Luke Tierney
>> Statistics and Actuarial Science
>> Ralph E. Wareham Professor of Mathematical Sciences
>> University of Iowa                  Phone:             319-335-3386
>> Department of Statistics and        Fax:               319-335-3017
>>    Actuarial Science
>> 241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
>> Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu
> 
> 
> 


From mdowle at mdowle.plus.com  Tue Jul 12 12:24:03 2011
From: mdowle at mdowle.plus.com (Matthew Dowle)
Date: Tue, 12 Jul 2011 11:24:03 +0100
Subject: [Rd] [datatable-help]  speeding up perception
In-Reply-To: <58C95DDB-ADA7-4FD9-832A-C038F9111E5C@r-project.org>
References: <CAPr7RtU44GO-7_m02-ek+ps3YcR_qr3m7vV8BSvhAYJ=F_bnbA@mail.gmail.com>
	<CAPr7RtVx1ao9bubQiCrf9LjgpdZdB24vSnjZvGxW4a8Sy-pKag@mail.gmail.com>
	<CAGam+C6yEFw-E+oT7tktDVY8L9KtJZ0aHrQ_y0VLyEMnZDmfSw@mail.gmail.com>
	<555C7B55-67D9-4DC6-90CB-BE8B0F620BFB@r-project.org>
	<1309851165.4231.40.camel@netbook> <1309889303.4231.60.camel@netbook>
	<E0D40E65-E676-4083-A058-090E3D2CC20C@r-project.org>
	<alpine.DEB.2.00.1107051808040.1928@luke-inspiron>
	<121657AE-3C91-4880-BC29-781A78B05278@comcast.net>
	<11C6B5AE-0B1F-4154-B21C-7A5B3ED08619@r-project.org>
	<1309941365.4231.122.camel@netbook>
	<40822836-425E-4266-B203-35B3357FAB6F@r-project.org>
	<alpine.DEB.2.00.1107060904060.1928@luke-inspiron>
	<1310430115.12980.88.camel@netbook>
	<58C95DDB-ADA7-4FD9-832A-C038F9111E5C@r-project.org>
Message-ID: <63f186626080e470268cba3b90249583.squirrel@webmail.plus.net>

> Matthew,
>
> I was hoping I misunderstood you first proposal, but I suspect I did not
> ;).
>
> Personally, I find  DT[1,V1 <- 3] highly disturbing - I would expect it to
> evaluate to
> { V1 <- 3; DT[1, V1] }
> thus returning the first element of the third column.

Please see FAQ 1.1, since further below it seems to be an expectation
issue about 'with' syntax, too.

>
> That said, I don't think it works, either. Taking you example and
> data.table form r-forge:
[ snip ]
> as you can see, DT is not modified.

Works for me on R 2.13.0. I'll try latest R later. If I can't reproduce
the non-working state I'll need some more environment information please.

> Also I suspect there is something quite amiss because even trivial things
> don't work:
>
>> DF[1:4,1:4]
>   V1 V2 V3 V4
> 1  3  1  1  1
> 2  1  1  1  1
> 3  1  1  1  1
> 4  1  1  1  1
>> DT[1:4,1:4]
> [1] 1 2 3 4

That's correct and fundamental to data.table. See FAQs 1.1, 1.7, 1.8, 1.9
and 1.10.

>
> When I first saw your proposal, I thought you have rather something like
> within(DT, V1[1] <- 3)
> in mind which looks innocent enough but performs terribly (note that I had
> to scale down the loop by a factor of 100!!!):
>
>> system.time(for (i in 1:10) within(DT, V1[1] <- 3))
>    user  system elapsed
>   2.701   4.437   7.138

No, since 'with' is already built into data.table, I was thinking of
building 'within' in, too. I'll take a look at within(). Might as well
provide as many options as possible to the user to use as they wish.

> With the for loop something like within(DF, for (i in 1:1000) V1[i] <- 3))
> performs reasonably:
>
>> system.time(within(DT, for (i in 1:1000) V1[i] <- 3))
>    user  system elapsed
>   0.392   0.613   1.003
>
> (Note: system.time() can be misleading when within() is involved, because
> the expression is evaluated in a different environment so within() won't
> actually change the object in the  global environment - it also interacts
> with the possible duplication)

Noted, thanks. That's pretty fast. Does within() on data.frame fix the
original issue Ivo raised, then?  If so, job done.

>
> Cheers,
> Simon
>
> On Jul 11, 2011, at 8:21 PM, Matthew Dowle wrote:
>
>> Thanks for the replies and info. An attempt at fast
>> assign is now committed to data.table v1.6.3 on
>> R-Forge. From NEWS :
>>
>> o   Fast update is now implemented, FR#200.
>>    DT[i,j]<-value is now handled by data.table in C rather
>>    than falling through to data.frame methods.
>>
>>    Thanks to Ivo Welch for raising speed issues on r-devel,
>>    to Simon Urbanek for the suggestion, and Luke Tierney and
>>    Simon for information on R internals.
>>
>>    [<- syntax still incurs one working copy of the whole
>>    table (as of R 2.13.0) due to R's [<- dispatch mechanism
>>    copying to `*tmp*`, so, for ultimate speed and brevity,
>>    'within' syntax is now available as follows.
>>
>> o   A new 'within' argument has been added to [.data.table,
>>    by default TRUE. It is very similar to the within()
>>    function in base R. If an assignment appears in j, it
>>    assigns to the column of DT, by reference; e.g.,
>>
>>    DT[i,colname<-value]
>>
>>    This syntax makes no copies of any part of memory at all.
>>
>>> m = matrix(1,nrow=100000,ncol=100)
>>> DF = as.data.frame(m)
>>> DT = as.data.table(m)
>>> system.time(for (i in 1:1000) DF[1,1] <- 3)
>>       user  system elapsed
>>    287.730 323.196 613.453
>>> system.time(for (i in 1:1000) DT[1,V1 <- 3])
>>       user  system elapsed
>>      1.152   0.004   1.161         # 528 times faster
>>
>> Please note :
>>
>>    *******************************************************
>>    **  Within syntax is presently highly experimental.  **
>>    *******************************************************
>>
>> http://datatable.r-forge.r-project.org/
>>
>>
>> On Wed, 2011-07-06 at 09:08 -0500, luke-tierney at uiowa.edu wrote:
>>> On Wed, 6 Jul 2011, Simon Urbanek wrote:
>>>
>>>> Interesting, and I stand corrected:
>>>>
>>>>> x = data.frame(a=1:n,b=1:n)
>>>>> .Internal(inspect(x))
>>>> @103511c00 19 VECSXP g0c2 [OBJ,NAM(2),ATT] (len=2, tl=0)
>>>> @102c7b000 13 INTSXP g0c7 [] (len=100000, tl=0) 1,2,3,4,5,...
>>>> @102af3000 13 INTSXP g0c7 [] (len=100000, tl=0) 1,2,3,4,5,...
>>>>
>>>>> x[1,1]=42L
>>>>> .Internal(inspect(x))
>>>> @10349c720 19 VECSXP g0c2 [OBJ,NAM(2),ATT] (len=2, tl=0)
>>>> @102c19000 13 INTSXP g0c7 [] (len=100000, tl=0) 42,2,3,4,5,...
>>>> @102b55000 13 INTSXP g0c7 [] (len=100000, tl=0) 1,2,3,4,5,...
>>>>
>>>>> x[[1]][1]=42L
>>>>> .Internal(inspect(x))
>>>> @103511a78 19 VECSXP g1c2 [OBJ,MARK,NAM(2),ATT] (len=2, tl=0)
>>>> @102e65000 13 INTSXP g0c7 [] (len=100000, tl=0) 42,2,3,4,5,...
>>>> @101f14000 13 INTSXP g1c7 [MARK] (len=100000, tl=0) 1,2,3,4,5,...
>>>>
>>>>> x[[1]][1]=42L
>>>>> .Internal(inspect(x))
>>>> @10349c800 19 VECSXP g0c2 [OBJ,NAM(2),ATT] (len=2, tl=0)
>>>> @102a2f000 13 INTSXP g0c7 [] (len=100000, tl=0) 42,2,3,4,5,...
>>>> @102ec7000 13 INTSXP g0c7 [] (len=100000, tl=0) 1,2,3,4,5,...
>>>>
>>>>
>>>> I have R to release ;) so I won't be looking into this right now, but
>>>> it's something worth investigating ... Since all the inner contents
>>>> have NAMED=0 I would not expect any duplication to be needed, but
>>>> apparently becomes so is at some point ...
>>>
>>>
>>> The internals assume in various places that deep copies are made (one
>>> of the reasons NAMED setings are not propagated to sub-sturcture).
>>> The main issues are avoiding cycles and that there is no easy way to
>>> check for sharing.  There may be some circumstances in which a shallow
>>> copy would be OK but making sure it would be in all cases is probably
>>> more trouble than it is worth at this point. (I've tried this in the
>>> past in a few cases and always had to back off.)
>>>
>>>
>>> Best,
>>>
>>> luke
>>>
>>>>
>>>> Cheers,
>>>> Simon
>>>>
>>>>
>>>> On Jul 6, 2011, at 4:36 AM, Matthew Dowle wrote:
>>>>
>>>>>
>>>>> On Tue, 2011-07-05 at 21:11 -0400, Simon Urbanek wrote:
>>>>>> No subassignment function satisfies that condition, because you can
>>>>>> always call them directly. However, that doesn't stop the default
>>>>>> method from making that assumption, so I'm not sure it's an issue.
>>>>>>
>>>>>> David, Just to clarify - the data frame content is not copied, we
>>>>>> are talking about the vector holding columns.
>>>>>
>>>>> If it is just the vector holding the columns that is copied (and not
>>>>> the
>>>>> columns themselves), why does n make a difference in this test (on R
>>>>> 2.13.0)?
>>>>>
>>>>>> n = 1000
>>>>>> x = data.frame(a=1:n,b=1:n)
>>>>>> system.time(for (i in 1:1000) x[1,1] <- 42L)
>>>>>  user  system elapsed
>>>>> 0.628   0.000   0.628
>>>>>> n = 100000
>>>>>> x = data.frame(a=1:n,b=1:n)      # still 2 columns, but longer
>>>>>> columns
>>>>>> system.time(for (i in 1:1000) x[1,1] <- 42L)
>>>>>  user  system elapsed
>>>>> 20.145   1.232  21.455
>>>>>>
>>>>>
>>>>> With $<- :
>>>>>
>>>>>> n = 1000
>>>>>> x = data.frame(a=1:n,b=1:n)
>>>>>> system.time(for (i in 1:1000) x$a[1] <- 42L)
>>>>>  user  system elapsed
>>>>> 0.304   0.000   0.307
>>>>>> n = 100000
>>>>>> x = data.frame(a=1:n,b=1:n)
>>>>>> system.time(for (i in 1:1000) x$a[1] <- 42L)
>>>>>  user  system elapsed
>>>>> 37.586   0.388  38.161
>>>>>>
>>>>>
>>>>> If it's because the 1st column needs to be copied (only) because
>>>>> that's
>>>>> the one being assigned to (in this test), that magnitude of slow down
>>>>> doesn't seem consistent with the time of a vector copy of the 1st
>>>>> column :
>>>>>
>>>>>> n=100000
>>>>>> v = 1:n
>>>>>> system.time(for (i in 1:1000) v[1] <- 42L)
>>>>>  user  system elapsed
>>>>> 0.016   0.000   0.017
>>>>>> system.time(for (i in 1:1000) {v2=v;v2[1] <- 42L})
>>>>>  user  system elapsed
>>>>> 1.816   1.076   2.900
>>>>>
>>>>> Finally, increasing the number of columns, again only the 1st is
>>>>> assigned to :
>>>>>
>>>>>> n=100000
>>>>>> x = data.frame(rep(list(1:n),100))
>>>>>> dim(x)
>>>>> [1] 100000    100
>>>>>> system.time(for (i in 1:1000) x[1,1] <- 42L)
>>>>>  user  system elapsed
>>>>> 167.974  50.903 219.711
>>>>>>
>>>>>
>>>>>
>>>>>
>>>>>>
>>>>>> Cheers,
>>>>>> Simon
>>>>>>
>>>>>> Sent from my iPhone
>>>>>>
>>>>>> On Jul 5, 2011, at 9:01 PM, David Winsemius <dwinsemius at comcast.net>
>>>>>> wrote:
>>>>>>
>>>>>>>
>>>>>>> On Jul 5, 2011, at 7:18 PM, <luke-tierney at uiowa.edu>
>>>>>>> <luke-tierney at uiowa.edu> wrote:
>>>>>>>
>>>>>>>> On Tue, 5 Jul 2011, Simon Urbanek wrote:
>>>>>>>>
>>>>>>>>>
>>>>>>>>> On Jul 5, 2011, at 2:08 PM, Matthew Dowle wrote:
>>>>>>>>>
>>>>>>>>>> Simon (and all),
>>>>>>>>>>
>>>>>>>>>> I've tried to make assignment as fast as calling
>>>>>>>>>> `[<-.data.table`
>>>>>>>>>> directly, for user convenience. Profiling shows (IIUC) that it
>>>>>>>>>> isn't
>>>>>>>>>> dispatch, but x being copied. Is there a way to prevent '[<-'
>>>>>>>>>> from
>>>>>>>>>> copying x?
>>>>>>>>>
>>>>>>>>> Good point, and conceptually, no. It's a subassignment after all
>>>>>>>>> - see R-lang 3.4.4 - it is equivalent to
>>>>>>>>>
>>>>>>>>> `*tmp*` <- x
>>>>>>>>> x <- `[<-`(`*tmp*`, i, j, value)
>>>>>>>>> rm(`*tmp*`)
>>>>>>>>>
>>>>>>>>> so there is always a copy involved.
>>>>>>>>>
>>>>>>>>> Now, a conceptual copy doesn't mean real copy in R since R tries
>>>>>>>>> to keep the pass-by-value illusion while passing references in
>>>>>>>>> cases where it knows that modifications cannot occur and/or they
>>>>>>>>> are safe. The default subassign method uses that feature which
>>>>>>>>> means it can afford to not duplicate if there is only one
>>>>>>>>> reference -- then it's safe to not duplicate as we are replacing
>>>>>>>>> that only existing reference. And in the case of a matrix, that
>>>>>>>>> will be true at the latest from the second subassignment on.
>>>>>>>>>
>>>>>>>>> Unfortunately the method dispatch (AFAICS) introduces one more
>>>>>>>>> reference in the dispatch chain so there will always be two
>>>>>>>>> references so duplication is necessary. Since we have only 0 / 1
>>>>>>>>> / 2+ information on the references, we can't distinguish whether
>>>>>>>>> the second reference is due to the dispatch or due to the passed
>>>>>>>>> object having more than one reference, so we have to duplicate in
>>>>>>>>> any case. That is unfortunate, and I don't see a way around
>>>>>>>>> (unless we handle subassignment methods is some special way).
>>>>>>>>
>>>>>>>> I don't believe dispatch is bumping NAMED (and a quick experiment
>>>>>>>> seems to confirm this though I don't guarantee I did that right).
>>>>>>>> The
>>>>>>>> issue is that a replacement function implemented as a closure,
>>>>>>>> which
>>>>>>>> is the only option for a package, will always see NAMED on the
>>>>>>>> object
>>>>>>>> to be modified as 2 (because the value is obtained by forcing the
>>>>>>>> argument promise) and so any R level assignments will duplicate.
>>>>>>>> This
>>>>>>>> also isn't really an issue of imprecise reference counting --
>>>>>>>> there
>>>>>>>> really are (at least) two legitimate references -- one though the
>>>>>>>> argument and one through the caller's environment.
>>>>>>>>
>>>>>>>> It would be good it we could come up with a way for packages to be
>>>>>>>> able to define replacement functions that do not duplicate in
>>>>>>>> cases
>>>>>>>> where we really don't want them to, but this would require coming
>>>>>>>> up
>>>>>>>> with some sort of protocol, minimally involving an efficient way
>>>>>>>> to
>>>>>>>> detect whether a replacement funciton is being called in a
>>>>>>>> replacement
>>>>>>>> context or directly.
>>>>>>>
>>>>>>> Would "$<-" always satisfy that condition. It would be big help to
>>>>>>> me if it could be designed to avoid duplication the rest of the
>>>>>>> data.frame.
>>>>>>>
>>>>>>> --
>>>>>>>
>>>>>>>>
>>>>>>>> There are some replacement functions that use C code to cheat, but
>>>>>>>> these may create problems if called directly, so I won't advertise
>>>>>>>> them.
>>>>>>>>
>>>>>>>> Best,
>>>>>>>>
>>>>>>>> luke
>>>>>>>>
>>>>>>>>>
>>>>>>>>> Cheers,
>>>>>>>>> Simon
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>
>>>>>>>> --
>>>>>>>> Luke Tierney
>>>>>>>> Statistics and Actuarial Science
>>>>>>>> Ralph E. Wareham Professor of Mathematical Sciences
>>>>>>>> University of Iowa                  Phone:
>>>>>>>> 319-335-3386
>>>>>>>> Department of Statistics and        Fax:
>>>>>>>> 319-335-3017
>>>>>>>> Actuarial Science
>>>>>>>> 241 Schaeffer Hall                  email:
>>>>>>>> luke at stat.uiowa.edu
>>>>>>>> Iowa City, IA 52242                 WWW:
>>>>>>>> http://www.stat.uiowa.edu______________________________________________
>>>>>>>> R-devel at r-project.org mailing list
>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>>>>
>>>>>>> David Winsemius, MD
>>>>>>> West Hartford, CT
>>>>>>>
>>>>>>>
>>>>>
>>>>>
>>>>>
>>>>
>>>>
>>>
>>> --
>>> Luke Tierney
>>> Statistics and Actuarial Science
>>> Ralph E. Wareham Professor of Mathematical Sciences
>>> University of Iowa                  Phone:             319-335-3386
>>> Department of Statistics and        Fax:               319-335-3017
>>>    Actuarial Science
>>> 241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
>>> Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu
>>
>>
>>
>
>


From jeroen.ooms at stat.ucla.edu  Tue Jul 12 14:24:30 2011
From: jeroen.ooms at stat.ucla.edu (jeroen00ms)
Date: Tue, 12 Jul 2011 05:24:30 -0700 (PDT)
Subject: [Rd] [linux] connection never times out
Message-ID: <1310473470487-3662088.post@n4.nabble.com>

According to the download.file manual the timeout of a connection can be set
using options(timeout=10). This seems to work as expected on windows, but on
linux the connection does not timeout. I reproduced the problem both 0on
R-2.13 on Ubuntu and on R-2.12.1 on CentOS, but not in Windows.

> options(timeout=5)
> download.file("http://123.123.123.123/bla", dest=tempfile())

I am running Ubuntu 11.04 with the R binaries from CRAN:

> sessionInfo()
R version 2.13.0 (2011-04-13)
Platform: i686-pc-linux-gnu (32-bit)

locale:
 [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              
 [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    
 [5] LC_MONETARY=C              LC_MESSAGES=en_US.UTF-8   
 [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 
 [9] LC_ADDRESS=C               LC_TELEPHONE=C            
[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     
> 

--
View this message in context: http://r.789695.n4.nabble.com/linux-connection-never-times-out-tp3662088p3662088.html
Sent from the R devel mailing list archive at Nabble.com.


From andreas at eckner.com  Mon Jul 11 23:24:45 2011
From: andreas at eckner.com (andreas at eckner.com)
Date: Mon, 11 Jul 2011 14:24:45 -0700
Subject: [Rd] =?utf-8?q?save=2Eimage_compression=5Flevel_argument?=
Message-ID: <20110711142445.02e456bf039156f791943764322f7efe.fc28463a38.wbe@email17.secureserver.net>

Hi,

in "save.image", it would be nice if there was a "compression_level"
argument that is passed along to "save".

Or is there a reason for disabling the "compression_level" option for
saving workspaces, but enabling it for manually saving individual
objects?

Thanks,
Andreas


From mdowle at mdowle.plus.com  Tue Jul 12 02:21:55 2011
From: mdowle at mdowle.plus.com (Matthew Dowle)
Date: Tue, 12 Jul 2011 01:21:55 +0100
Subject: [Rd] [datatable-help]  speeding up perception
In-Reply-To: <alpine.DEB.2.00.1107060904060.1928@luke-inspiron>
References: <CAPr7RtU44GO-7_m02-ek+ps3YcR_qr3m7vV8BSvhAYJ=F_bnbA@mail.gmail.com>
	<CAPr7RtVx1ao9bubQiCrf9LjgpdZdB24vSnjZvGxW4a8Sy-pKag@mail.gmail.com>
	<CAGam+C6yEFw-E+oT7tktDVY8L9KtJZ0aHrQ_y0VLyEMnZDmfSw@mail.gmail.com>
	<555C7B55-67D9-4DC6-90CB-BE8B0F620BFB@r-project.org>
	<1309851165.4231.40.camel@netbook> <1309889303.4231.60.camel@netbook>
	<E0D40E65-E676-4083-A058-090E3D2CC20C@r-project.org>
	<alpine.DEB.2.00.1107051808040.1928@luke-inspiron>
	<121657AE-3C91-4880-BC29-781A78B05278@comcast.net>
	<11C6B5AE-0B1F-4154-B21C-7A5B3ED08619@r-project.org>
	<1309941365.4231.122.camel@netbook>
	<40822836-425E-4266-B203-35B3357FAB6F@r-project.org>
	<alpine.DEB.2.00.1107060904060.1928@luke-inspiron>
Message-ID: <1310430115.12980.88.camel@netbook>

Thanks for the replies and info. An attempt at fast
assign is now committed to data.table v1.6.3 on
R-Forge. From NEWS :

o   Fast update is now implemented, FR#200.
    DT[i,j]<-value is now handled by data.table in C rather
    than falling through to data.frame methods.
    
    Thanks to Ivo Welch for raising speed issues on r-devel,
    to Simon Urbanek for the suggestion, and Luke Tierney and
    Simon for information on R internals.

    [<- syntax still incurs one working copy of the whole
    table (as of R 2.13.0) due to R's [<- dispatch mechanism
    copying to `*tmp*`, so, for ultimate speed and brevity,
    'within' syntax is now available as follows.
        
o   A new 'within' argument has been added to [.data.table,
    by default TRUE. It is very similar to the within()
    function in base R. If an assignment appears in j, it
    assigns to the column of DT, by reference; e.g.,
         
    DT[i,colname<-value]
        
    This syntax makes no copies of any part of memory at all.
        
    > m = matrix(1,nrow=100000,ncol=100)
    > DF = as.data.frame(m)
    > DT = as.data.table(m)
    > system.time(for (i in 1:1000) DF[1,1] <- 3)
       user  system elapsed 
    287.730 323.196 613.453 
    > system.time(for (i in 1:1000) DT[1,V1 <- 3])
       user  system elapsed 
      1.152   0.004   1.161         # 528 times faster

Please note :
        
    *******************************************************
    **  Within syntax is presently highly experimental.  **
    *******************************************************

http://datatable.r-forge.r-project.org/


On Wed, 2011-07-06 at 09:08 -0500, luke-tierney at uiowa.edu wrote:
> On Wed, 6 Jul 2011, Simon Urbanek wrote:
> 
> > Interesting, and I stand corrected:
> >
> >> x = data.frame(a=1:n,b=1:n)
> >> .Internal(inspect(x))
> > @103511c00 19 VECSXP g0c2 [OBJ,NAM(2),ATT] (len=2, tl=0)
> >  @102c7b000 13 INTSXP g0c7 [] (len=100000, tl=0) 1,2,3,4,5,...
> >  @102af3000 13 INTSXP g0c7 [] (len=100000, tl=0) 1,2,3,4,5,...
> >
> >> x[1,1]=42L
> >> .Internal(inspect(x))
> > @10349c720 19 VECSXP g0c2 [OBJ,NAM(2),ATT] (len=2, tl=0)
> >  @102c19000 13 INTSXP g0c7 [] (len=100000, tl=0) 42,2,3,4,5,...
> >  @102b55000 13 INTSXP g0c7 [] (len=100000, tl=0) 1,2,3,4,5,...
> >
> >> x[[1]][1]=42L
> >> .Internal(inspect(x))
> > @103511a78 19 VECSXP g1c2 [OBJ,MARK,NAM(2),ATT] (len=2, tl=0)
> >  @102e65000 13 INTSXP g0c7 [] (len=100000, tl=0) 42,2,3,4,5,...
> >  @101f14000 13 INTSXP g1c7 [MARK] (len=100000, tl=0) 1,2,3,4,5,...
> >
> >> x[[1]][1]=42L
> >> .Internal(inspect(x))
> > @10349c800 19 VECSXP g0c2 [OBJ,NAM(2),ATT] (len=2, tl=0)
> >  @102a2f000 13 INTSXP g0c7 [] (len=100000, tl=0) 42,2,3,4,5,...
> >  @102ec7000 13 INTSXP g0c7 [] (len=100000, tl=0) 1,2,3,4,5,...
> >
> >
> > I have R to release ;) so I won't be looking into this right now, but it's something worth investigating ... Since all the inner contents have NAMED=0 I would not expect any duplication to be needed, but apparently becomes so is at some point ...
> 
> 
> The internals assume in various places that deep copies are made (one
> of the reasons NAMED setings are not propagated to sub-sturcture).
> The main issues are avoiding cycles and that there is no easy way to
> check for sharing.  There may be some circumstances in which a shallow
> copy would be OK but making sure it would be in all cases is probably
> more trouble than it is worth at this point. (I've tried this in the
> past in a few cases and always had to back off.)
> 
> 
> Best,
> 
> luke
> 
> >
> > Cheers,
> > Simon
> >
> >
> > On Jul 6, 2011, at 4:36 AM, Matthew Dowle wrote:
> >
> >>
> >> On Tue, 2011-07-05 at 21:11 -0400, Simon Urbanek wrote:
> >>> No subassignment function satisfies that condition, because you can always call them directly. However, that doesn't stop the default method from making that assumption, so I'm not sure it's an issue.
> >>>
> >>> David, Just to clarify - the data frame content is not copied, we are talking about the vector holding columns.
> >>
> >> If it is just the vector holding the columns that is copied (and not the
> >> columns themselves), why does n make a difference in this test (on R
> >> 2.13.0)?
> >>
> >>> n = 1000
> >>> x = data.frame(a=1:n,b=1:n)
> >>> system.time(for (i in 1:1000) x[1,1] <- 42L)
> >>   user  system elapsed
> >>  0.628   0.000   0.628
> >>> n = 100000
> >>> x = data.frame(a=1:n,b=1:n)      # still 2 columns, but longer columns
> >>> system.time(for (i in 1:1000) x[1,1] <- 42L)
> >>   user  system elapsed
> >> 20.145   1.232  21.455
> >>>
> >>
> >> With $<- :
> >>
> >>> n = 1000
> >>> x = data.frame(a=1:n,b=1:n)
> >>> system.time(for (i in 1:1000) x$a[1] <- 42L)
> >>   user  system elapsed
> >>  0.304   0.000   0.307
> >>> n = 100000
> >>> x = data.frame(a=1:n,b=1:n)
> >>> system.time(for (i in 1:1000) x$a[1] <- 42L)
> >>   user  system elapsed
> >> 37.586   0.388  38.161
> >>>
> >>
> >> If it's because the 1st column needs to be copied (only) because that's
> >> the one being assigned to (in this test), that magnitude of slow down
> >> doesn't seem consistent with the time of a vector copy of the 1st
> >> column :
> >>
> >>> n=100000
> >>> v = 1:n
> >>> system.time(for (i in 1:1000) v[1] <- 42L)
> >>   user  system elapsed
> >>  0.016   0.000   0.017
> >>> system.time(for (i in 1:1000) {v2=v;v2[1] <- 42L})
> >>   user  system elapsed
> >>  1.816   1.076   2.900
> >>
> >> Finally, increasing the number of columns, again only the 1st is
> >> assigned to :
> >>
> >>> n=100000
> >>> x = data.frame(rep(list(1:n),100))
> >>> dim(x)
> >> [1] 100000    100
> >>> system.time(for (i in 1:1000) x[1,1] <- 42L)
> >>   user  system elapsed
> >> 167.974  50.903 219.711
> >>>
> >>
> >>
> >>
> >>>
> >>> Cheers,
> >>> Simon
> >>>
> >>> Sent from my iPhone
> >>>
> >>> On Jul 5, 2011, at 9:01 PM, David Winsemius <dwinsemius at comcast.net> wrote:
> >>>
> >>>>
> >>>> On Jul 5, 2011, at 7:18 PM, <luke-tierney at uiowa.edu> <luke-tierney at uiowa.edu> wrote:
> >>>>
> >>>>> On Tue, 5 Jul 2011, Simon Urbanek wrote:
> >>>>>
> >>>>>>
> >>>>>> On Jul 5, 2011, at 2:08 PM, Matthew Dowle wrote:
> >>>>>>
> >>>>>>> Simon (and all),
> >>>>>>>
> >>>>>>> I've tried to make assignment as fast as calling `[<-.data.table`
> >>>>>>> directly, for user convenience. Profiling shows (IIUC) that it isn't
> >>>>>>> dispatch, but x being copied. Is there a way to prevent '[<-' from
> >>>>>>> copying x?
> >>>>>>
> >>>>>> Good point, and conceptually, no. It's a subassignment after all - see R-lang 3.4.4 - it is equivalent to
> >>>>>>
> >>>>>> `*tmp*` <- x
> >>>>>> x <- `[<-`(`*tmp*`, i, j, value)
> >>>>>> rm(`*tmp*`)
> >>>>>>
> >>>>>> so there is always a copy involved.
> >>>>>>
> >>>>>> Now, a conceptual copy doesn't mean real copy in R since R tries to keep the pass-by-value illusion while passing references in cases where it knows that modifications cannot occur and/or they are safe. The default subassign method uses that feature which means it can afford to not duplicate if there is only one reference -- then it's safe to not duplicate as we are replacing that only existing reference. And in the case of a matrix, that will be true at the latest from the second subassignment on.
> >>>>>>
> >>>>>> Unfortunately the method dispatch (AFAICS) introduces one more reference in the dispatch chain so there will always be two references so duplication is necessary. Since we have only 0 / 1 / 2+ information on the references, we can't distinguish whether the second reference is due to the dispatch or due to the passed object having more than one reference, so we have to duplicate in any case. That is unfortunate, and I don't see a way around (unless we handle subassignment methods is some special way).
> >>>>>
> >>>>> I don't believe dispatch is bumping NAMED (and a quick experiment
> >>>>> seems to confirm this though I don't guarantee I did that right). The
> >>>>> issue is that a replacement function implemented as a closure, which
> >>>>> is the only option for a package, will always see NAMED on the object
> >>>>> to be modified as 2 (because the value is obtained by forcing the
> >>>>> argument promise) and so any R level assignments will duplicate.  This
> >>>>> also isn't really an issue of imprecise reference counting -- there
> >>>>> really are (at least) two legitimate references -- one though the
> >>>>> argument and one through the caller's environment.
> >>>>>
> >>>>> It would be good it we could come up with a way for packages to be
> >>>>> able to define replacement functions that do not duplicate in cases
> >>>>> where we really don't want them to, but this would require coming up
> >>>>> with some sort of protocol, minimally involving an efficient way to
> >>>>> detect whether a replacement funciton is being called in a replacement
> >>>>> context or directly.
> >>>>
> >>>> Would "$<-" always satisfy that condition. It would be big help to me if it could be designed to avoid duplication the rest of the data.frame.
> >>>>
> >>>> --
> >>>>
> >>>>>
> >>>>> There are some replacement functions that use C code to cheat, but
> >>>>> these may create problems if called directly, so I won't advertise
> >>>>> them.
> >>>>>
> >>>>> Best,
> >>>>>
> >>>>> luke
> >>>>>
> >>>>>>
> >>>>>> Cheers,
> >>>>>> Simon
> >>>>>>
> >>>>>>
> >>>>>>
> >>>>>
> >>>>> --
> >>>>> Luke Tierney
> >>>>> Statistics and Actuarial Science
> >>>>> Ralph E. Wareham Professor of Mathematical Sciences
> >>>>> University of Iowa                  Phone:             319-335-3386
> >>>>> Department of Statistics and        Fax:               319-335-3017
> >>>>> Actuarial Science
> >>>>> 241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
> >>>>> Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu______________________________________________
> >>>>> R-devel at r-project.org mailing list
> >>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
> >>>>
> >>>> David Winsemius, MD
> >>>> West Hartford, CT
> >>>>
> >>>>
> >>
> >>
> >>
> >
> >
> 
> -- 
> Luke Tierney
> Statistics and Actuarial Science
> Ralph E. Wareham Professor of Mathematical Sciences
> University of Iowa                  Phone:             319-335-3386
> Department of Statistics and        Fax:               319-335-3017
>     Actuarial Science
> 241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
> Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu


From mdowle at mdowle.plus.com  Tue Jul 12 15:12:06 2011
From: mdowle at mdowle.plus.com (Matthew Dowle)
Date: Tue, 12 Jul 2011 14:12:06 +0100
Subject: [Rd] [datatable-help]  speeding up perception
In-Reply-To: <63f186626080e470268cba3b90249583.squirrel@webmail.plus.net>
References: <CAPr7RtU44GO-7_m02-ek+ps3YcR_qr3m7vV8BSvhAYJ=F_bnbA@mail.gmail.com>
	<CAPr7RtVx1ao9bubQiCrf9LjgpdZdB24vSnjZvGxW4a8Sy-pKag@mail.gmail.com>
	<CAGam+C6yEFw-E+oT7tktDVY8L9KtJZ0aHrQ_y0VLyEMnZDmfSw@mail.gmail.com>
	<555C7B55-67D9-4DC6-90CB-BE8B0F620BFB@r-project.org>
	<1309851165.4231.40.camel@netbook> <1309889303.4231.60.camel@netbook>
	<E0D40E65-E676-4083-A058-090E3D2CC20C@r-project.org>
	<alpine.DEB.2.00.1107051808040.1928@luke-inspiron>
	<121657AE-3C91-4880-BC29-781A78B05278@comcast.net>
	<11C6B5AE-0B1F-4154-B21C-7A5B3ED08619@r-project.org>
	<1309941365.4231.122.camel@netbook>
	<40822836-425E-4266-B203-35B3357FAB6F@r-project.org>
	<alpine.DEB.2.00.1107060904060.1928@luke-inspiron>
	<1310430115.12980.88.camel@netbook>
	<58C95DDB-ADA7-4FD9-832A-C038F9111E5C@r-project.org>
	<63f186626080e470268cba3b90249583.squirrel@webmail.plus.net>
Message-ID: <56e5a38fb8cb984287d66e7eb6b014fe.squirrel@webmail.plus.net>


Simon,
If you didn't install.packages() with method="source" from R-Forge, that
would explain (some of) it. R-Forge builds binaries once each night. This
commit was long after the cutoff.
Matthew

>> Matthew,
>>
>> I was hoping I misunderstood you first proposal, but I suspect I did not
>> ;).
>>
>> Personally, I find  DT[1,V1 <- 3] highly disturbing - I would expect it
>> to
>> evaluate to
>> { V1 <- 3; DT[1, V1] }
>> thus returning the first element of the third column.
>
> Please see FAQ 1.1, since further below it seems to be an expectation
> issue about 'with' syntax, too.
>
>>
>> That said, I don't think it works, either. Taking you example and
>> data.table form r-forge:
> [ snip ]
>> as you can see, DT is not modified.
>
> Works for me on R 2.13.0. I'll try latest R later. If I can't reproduce
> the non-working state I'll need some more environment information please.
>
>> Also I suspect there is something quite amiss because even trivial
>> things
>> don't work:
>>
>>> DF[1:4,1:4]
>>   V1 V2 V3 V4
>> 1  3  1  1  1
>> 2  1  1  1  1
>> 3  1  1  1  1
>> 4  1  1  1  1
>>> DT[1:4,1:4]
>> [1] 1 2 3 4
>
> That's correct and fundamental to data.table. See FAQs 1.1, 1.7, 1.8, 1.9
> and 1.10.
>
>>
>> When I first saw your proposal, I thought you have rather something like
>> within(DT, V1[1] <- 3)
>> in mind which looks innocent enough but performs terribly (note that I
>> had
>> to scale down the loop by a factor of 100!!!):
>>
>>> system.time(for (i in 1:10) within(DT, V1[1] <- 3))
>>    user  system elapsed
>>   2.701   4.437   7.138
>
> No, since 'with' is already built into data.table, I was thinking of
> building 'within' in, too. I'll take a look at within(). Might as well
> provide as many options as possible to the user to use as they wish.
>
>> With the for loop something like within(DF, for (i in 1:1000) V1[i] <-
>> 3))
>> performs reasonably:
>>
>>> system.time(within(DT, for (i in 1:1000) V1[i] <- 3))
>>    user  system elapsed
>>   0.392   0.613   1.003
>>
>> (Note: system.time() can be misleading when within() is involved,
>> because
>> the expression is evaluated in a different environment so within() won't
>> actually change the object in the  global environment - it also
>> interacts
>> with the possible duplication)
>
> Noted, thanks. That's pretty fast. Does within() on data.frame fix the
> original issue Ivo raised, then?  If so, job done.
>
>>
>> Cheers,
>> Simon
>>
>> On Jul 11, 2011, at 8:21 PM, Matthew Dowle wrote:
>>
>>> Thanks for the replies and info. An attempt at fast
>>> assign is now committed to data.table v1.6.3 on
>>> R-Forge. From NEWS :
>>>
>>> o   Fast update is now implemented, FR#200.
>>>    DT[i,j]<-value is now handled by data.table in C rather
>>>    than falling through to data.frame methods.
>>>
>>>    Thanks to Ivo Welch for raising speed issues on r-devel,
>>>    to Simon Urbanek for the suggestion, and Luke Tierney and
>>>    Simon for information on R internals.
>>>
>>>    [<- syntax still incurs one working copy of the whole
>>>    table (as of R 2.13.0) due to R's [<- dispatch mechanism
>>>    copying to `*tmp*`, so, for ultimate speed and brevity,
>>>    'within' syntax is now available as follows.
>>>
>>> o   A new 'within' argument has been added to [.data.table,
>>>    by default TRUE. It is very similar to the within()
>>>    function in base R. If an assignment appears in j, it
>>>    assigns to the column of DT, by reference; e.g.,
>>>
>>>    DT[i,colname<-value]
>>>
>>>    This syntax makes no copies of any part of memory at all.
>>>
>>>> m = matrix(1,nrow=100000,ncol=100)
>>>> DF = as.data.frame(m)
>>>> DT = as.data.table(m)
>>>> system.time(for (i in 1:1000) DF[1,1] <- 3)
>>>       user  system elapsed
>>>    287.730 323.196 613.453
>>>> system.time(for (i in 1:1000) DT[1,V1 <- 3])
>>>       user  system elapsed
>>>      1.152   0.004   1.161         # 528 times faster
>>>
>>> Please note :
>>>
>>>    *******************************************************
>>>    **  Within syntax is presently highly experimental.  **
>>>    *******************************************************
>>>
>>> http://datatable.r-forge.r-project.org/
>>>
>>>
>>> On Wed, 2011-07-06 at 09:08 -0500, luke-tierney at uiowa.edu wrote:
>>>> On Wed, 6 Jul 2011, Simon Urbanek wrote:
>>>>
>>>>> Interesting, and I stand corrected:
>>>>>
>>>>>> x = data.frame(a=1:n,b=1:n)
>>>>>> .Internal(inspect(x))
>>>>> @103511c00 19 VECSXP g0c2 [OBJ,NAM(2),ATT] (len=2, tl=0)
>>>>> @102c7b000 13 INTSXP g0c7 [] (len=100000, tl=0) 1,2,3,4,5,...
>>>>> @102af3000 13 INTSXP g0c7 [] (len=100000, tl=0) 1,2,3,4,5,...
>>>>>
>>>>>> x[1,1]=42L
>>>>>> .Internal(inspect(x))
>>>>> @10349c720 19 VECSXP g0c2 [OBJ,NAM(2),ATT] (len=2, tl=0)
>>>>> @102c19000 13 INTSXP g0c7 [] (len=100000, tl=0) 42,2,3,4,5,...
>>>>> @102b55000 13 INTSXP g0c7 [] (len=100000, tl=0) 1,2,3,4,5,...
>>>>>
>>>>>> x[[1]][1]=42L
>>>>>> .Internal(inspect(x))
>>>>> @103511a78 19 VECSXP g1c2 [OBJ,MARK,NAM(2),ATT] (len=2, tl=0)
>>>>> @102e65000 13 INTSXP g0c7 [] (len=100000, tl=0) 42,2,3,4,5,...
>>>>> @101f14000 13 INTSXP g1c7 [MARK] (len=100000, tl=0) 1,2,3,4,5,...
>>>>>
>>>>>> x[[1]][1]=42L
>>>>>> .Internal(inspect(x))
>>>>> @10349c800 19 VECSXP g0c2 [OBJ,NAM(2),ATT] (len=2, tl=0)
>>>>> @102a2f000 13 INTSXP g0c7 [] (len=100000, tl=0) 42,2,3,4,5,...
>>>>> @102ec7000 13 INTSXP g0c7 [] (len=100000, tl=0) 1,2,3,4,5,...
>>>>>
>>>>>
>>>>> I have R to release ;) so I won't be looking into this right now, but
>>>>> it's something worth investigating ... Since all the inner contents
>>>>> have NAMED=0 I would not expect any duplication to be needed, but
>>>>> apparently becomes so is at some point ...
>>>>
>>>>
>>>> The internals assume in various places that deep copies are made (one
>>>> of the reasons NAMED setings are not propagated to sub-sturcture).
>>>> The main issues are avoiding cycles and that there is no easy way to
>>>> check for sharing.  There may be some circumstances in which a shallow
>>>> copy would be OK but making sure it would be in all cases is probably
>>>> more trouble than it is worth at this point. (I've tried this in the
>>>> past in a few cases and always had to back off.)
>>>>
>>>>
>>>> Best,
>>>>
>>>> luke
>>>>
>>>>>
>>>>> Cheers,
>>>>> Simon
>>>>>
>>>>>
>>>>> On Jul 6, 2011, at 4:36 AM, Matthew Dowle wrote:
>>>>>
>>>>>>
>>>>>> On Tue, 2011-07-05 at 21:11 -0400, Simon Urbanek wrote:
>>>>>>> No subassignment function satisfies that condition, because you can
>>>>>>> always call them directly. However, that doesn't stop the default
>>>>>>> method from making that assumption, so I'm not sure it's an issue.
>>>>>>>
>>>>>>> David, Just to clarify - the data frame content is not copied, we
>>>>>>> are talking about the vector holding columns.
>>>>>>
>>>>>> If it is just the vector holding the columns that is copied (and not
>>>>>> the
>>>>>> columns themselves), why does n make a difference in this test (on R
>>>>>> 2.13.0)?
>>>>>>
>>>>>>> n = 1000
>>>>>>> x = data.frame(a=1:n,b=1:n)
>>>>>>> system.time(for (i in 1:1000) x[1,1] <- 42L)
>>>>>>  user  system elapsed
>>>>>> 0.628   0.000   0.628
>>>>>>> n = 100000
>>>>>>> x = data.frame(a=1:n,b=1:n)      # still 2 columns, but longer
>>>>>>> columns
>>>>>>> system.time(for (i in 1:1000) x[1,1] <- 42L)
>>>>>>  user  system elapsed
>>>>>> 20.145   1.232  21.455
>>>>>>>
>>>>>>
>>>>>> With $<- :
>>>>>>
>>>>>>> n = 1000
>>>>>>> x = data.frame(a=1:n,b=1:n)
>>>>>>> system.time(for (i in 1:1000) x$a[1] <- 42L)
>>>>>>  user  system elapsed
>>>>>> 0.304   0.000   0.307
>>>>>>> n = 100000
>>>>>>> x = data.frame(a=1:n,b=1:n)
>>>>>>> system.time(for (i in 1:1000) x$a[1] <- 42L)
>>>>>>  user  system elapsed
>>>>>> 37.586   0.388  38.161
>>>>>>>
>>>>>>
>>>>>> If it's because the 1st column needs to be copied (only) because
>>>>>> that's
>>>>>> the one being assigned to (in this test), that magnitude of slow
>>>>>> down
>>>>>> doesn't seem consistent with the time of a vector copy of the 1st
>>>>>> column :
>>>>>>
>>>>>>> n=100000
>>>>>>> v = 1:n
>>>>>>> system.time(for (i in 1:1000) v[1] <- 42L)
>>>>>>  user  system elapsed
>>>>>> 0.016   0.000   0.017
>>>>>>> system.time(for (i in 1:1000) {v2=v;v2[1] <- 42L})
>>>>>>  user  system elapsed
>>>>>> 1.816   1.076   2.900
>>>>>>
>>>>>> Finally, increasing the number of columns, again only the 1st is
>>>>>> assigned to :
>>>>>>
>>>>>>> n=100000
>>>>>>> x = data.frame(rep(list(1:n),100))
>>>>>>> dim(x)
>>>>>> [1] 100000    100
>>>>>>> system.time(for (i in 1:1000) x[1,1] <- 42L)
>>>>>>  user  system elapsed
>>>>>> 167.974  50.903 219.711
>>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>>
>>>>>>> Cheers,
>>>>>>> Simon
>>>>>>>
>>>>>>> Sent from my iPhone
>>>>>>>
>>>>>>> On Jul 5, 2011, at 9:01 PM, David Winsemius
>>>>>>> <dwinsemius at comcast.net>
>>>>>>> wrote:
>>>>>>>
>>>>>>>>
>>>>>>>> On Jul 5, 2011, at 7:18 PM, <luke-tierney at uiowa.edu>
>>>>>>>> <luke-tierney at uiowa.edu> wrote:
>>>>>>>>
>>>>>>>>> On Tue, 5 Jul 2011, Simon Urbanek wrote:
>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> On Jul 5, 2011, at 2:08 PM, Matthew Dowle wrote:
>>>>>>>>>>
>>>>>>>>>>> Simon (and all),
>>>>>>>>>>>
>>>>>>>>>>> I've tried to make assignment as fast as calling
>>>>>>>>>>> `[<-.data.table`
>>>>>>>>>>> directly, for user convenience. Profiling shows (IIUC) that it
>>>>>>>>>>> isn't
>>>>>>>>>>> dispatch, but x being copied. Is there a way to prevent '[<-'
>>>>>>>>>>> from
>>>>>>>>>>> copying x?
>>>>>>>>>>
>>>>>>>>>> Good point, and conceptually, no. It's a subassignment after all
>>>>>>>>>> - see R-lang 3.4.4 - it is equivalent to
>>>>>>>>>>
>>>>>>>>>> `*tmp*` <- x
>>>>>>>>>> x <- `[<-`(`*tmp*`, i, j, value)
>>>>>>>>>> rm(`*tmp*`)
>>>>>>>>>>
>>>>>>>>>> so there is always a copy involved.
>>>>>>>>>>
>>>>>>>>>> Now, a conceptual copy doesn't mean real copy in R since R tries
>>>>>>>>>> to keep the pass-by-value illusion while passing references in
>>>>>>>>>> cases where it knows that modifications cannot occur and/or they
>>>>>>>>>> are safe. The default subassign method uses that feature which
>>>>>>>>>> means it can afford to not duplicate if there is only one
>>>>>>>>>> reference -- then it's safe to not duplicate as we are replacing
>>>>>>>>>> that only existing reference. And in the case of a matrix, that
>>>>>>>>>> will be true at the latest from the second subassignment on.
>>>>>>>>>>
>>>>>>>>>> Unfortunately the method dispatch (AFAICS) introduces one more
>>>>>>>>>> reference in the dispatch chain so there will always be two
>>>>>>>>>> references so duplication is necessary. Since we have only 0 / 1
>>>>>>>>>> / 2+ information on the references, we can't distinguish whether
>>>>>>>>>> the second reference is due to the dispatch or due to the passed
>>>>>>>>>> object having more than one reference, so we have to duplicate
>>>>>>>>>> in
>>>>>>>>>> any case. That is unfortunate, and I don't see a way around
>>>>>>>>>> (unless we handle subassignment methods is some special way).
>>>>>>>>>
>>>>>>>>> I don't believe dispatch is bumping NAMED (and a quick experiment
>>>>>>>>> seems to confirm this though I don't guarantee I did that right).
>>>>>>>>> The
>>>>>>>>> issue is that a replacement function implemented as a closure,
>>>>>>>>> which
>>>>>>>>> is the only option for a package, will always see NAMED on the
>>>>>>>>> object
>>>>>>>>> to be modified as 2 (because the value is obtained by forcing the
>>>>>>>>> argument promise) and so any R level assignments will duplicate.
>>>>>>>>> This
>>>>>>>>> also isn't really an issue of imprecise reference counting --
>>>>>>>>> there
>>>>>>>>> really are (at least) two legitimate references -- one though the
>>>>>>>>> argument and one through the caller's environment.
>>>>>>>>>
>>>>>>>>> It would be good it we could come up with a way for packages to
>>>>>>>>> be
>>>>>>>>> able to define replacement functions that do not duplicate in
>>>>>>>>> cases
>>>>>>>>> where we really don't want them to, but this would require coming
>>>>>>>>> up
>>>>>>>>> with some sort of protocol, minimally involving an efficient way
>>>>>>>>> to
>>>>>>>>> detect whether a replacement funciton is being called in a
>>>>>>>>> replacement
>>>>>>>>> context or directly.
>>>>>>>>
>>>>>>>> Would "$<-" always satisfy that condition. It would be big help to
>>>>>>>> me if it could be designed to avoid duplication the rest of the
>>>>>>>> data.frame.
>>>>>>>>
>>>>>>>> --
>>>>>>>>
>>>>>>>>>
>>>>>>>>> There are some replacement functions that use C code to cheat,
>>>>>>>>> but
>>>>>>>>> these may create problems if called directly, so I won't
>>>>>>>>> advertise
>>>>>>>>> them.
>>>>>>>>>
>>>>>>>>> Best,
>>>>>>>>>
>>>>>>>>> luke
>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> Cheers,
>>>>>>>>>> Simon
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>
>>>>>>>>> --
>>>>>>>>> Luke Tierney
>>>>>>>>> Statistics and Actuarial Science
>>>>>>>>> Ralph E. Wareham Professor of Mathematical Sciences
>>>>>>>>> University of Iowa                  Phone:
>>>>>>>>> 319-335-3386
>>>>>>>>> Department of Statistics and        Fax:
>>>>>>>>> 319-335-3017
>>>>>>>>> Actuarial Science
>>>>>>>>> 241 Schaeffer Hall                  email:
>>>>>>>>> luke at stat.uiowa.edu
>>>>>>>>> Iowa City, IA 52242                 WWW:
>>>>>>>>> http://www.stat.uiowa.edu______________________________________________
>>>>>>>>> R-devel at r-project.org mailing list
>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>>>>>
>>>>>>>> David Winsemius, MD
>>>>>>>> West Hartford, CT
>>>>>>>>
>>>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>
>>>>>
>>>>
>>>> --
>>>> Luke Tierney
>>>> Statistics and Actuarial Science
>>>> Ralph E. Wareham Professor of Mathematical Sciences
>>>> University of Iowa                  Phone:             319-335-3386
>>>> Department of Statistics and        Fax:               319-335-3017
>>>>    Actuarial Science
>>>> 241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
>>>> Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu
>>>
>>>
>>>
>>
>>
>
>


From simon.urbanek at r-project.org  Tue Jul 12 15:59:41 2011
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Tue, 12 Jul 2011 09:59:41 -0400
Subject: [Rd] [datatable-help]  speeding up perception
In-Reply-To: <63f186626080e470268cba3b90249583.squirrel@webmail.plus.net>
References: <CAPr7RtU44GO-7_m02-ek+ps3YcR_qr3m7vV8BSvhAYJ=F_bnbA@mail.gmail.com>
	<CAPr7RtVx1ao9bubQiCrf9LjgpdZdB24vSnjZvGxW4a8Sy-pKag@mail.gmail.com>
	<CAGam+C6yEFw-E+oT7tktDVY8L9KtJZ0aHrQ_y0VLyEMnZDmfSw@mail.gmail.com>
	<555C7B55-67D9-4DC6-90CB-BE8B0F620BFB@r-project.org>
	<1309851165.4231.40.camel@netbook>
	<1309889303.4231.60.camel@netbook>
	<E0D40E65-E676-4083-A058-090E3D2CC20C@r-project.org>
	<alpine.DEB.2.00.1107051808040.1928@luke-inspiron>
	<121657AE-3C91-4880-BC29-781A78B05278@comcast.net>
	<11C6B5AE-0B1F-4154-B21C-7A5B3ED08619@r-project.org>
	<1309941365.4231.122.camel@netbook>
	<40822836-425E-4266-B203-35B3357FAB6F@r-project.org>
	<alpine.DEB.2.00.1107060904060.1928@luke-inspiron>
	<1310430115.12980.88.camel@netbook>
	<58C95DDB-ADA7-4FD9-832A-C038F9111E5C@r-project.org>
	<63f186626080e470268cba3b90249583.squirrel@webmail.plus.net>
Message-ID: <142B6D47-A2B6-4084-8B5B-991C594FAFB4@r-project.org>

On Jul 12, 2011, at 6:24 AM, Matthew Dowle wrote:

>> Matthew,
>> 
>> I was hoping I misunderstood you first proposal, but I suspect I did not
>> ;).
>> 
>> Personally, I find  DT[1,V1 <- 3] highly disturbing - I would expect it to
>> evaluate to
>> { V1 <- 3; DT[1, V1] }
>> thus returning the first element of the third column.
> 
> Please see FAQ 1.1, since further below it seems to be an expectation
> issue about 'with' syntax, too.
> 

Just to clarify - the NEWS has led me to believe that the destructive DT[i, x <- y] syntax is new. That is what my objection is about. I'm fine with subsetting operators working on expressions but I'm not happy with subsetting operators modifying the the object they are subsetting - since it's subsetting not subassignemnt - that's what I was referring to.



>> That said, I don't think it works, either. Taking you example and
>> data.table form r-forge:
> [ snip ]
>> as you can see, DT is not modified.
> 
> Works for me on R 2.13.0. I'll try latest R later. If I can't reproduce
> the non-working state I'll need some more environment information please.
> 

The issue persist on several machines I tested - including R 2.13.0:

> sessionInfo()
R version 2.13.0 Patched (2011-05-15 r55914)
Platform: x86_64-apple-darwin9.8.0/x86_64 (64-bit)

locale:
[1] en_US.UTF-8/en_US.UTF-8/C/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] data.table_1.6.3


> sessionInfo()
R version 2.13.0 (2011-04-13)
Platform: x86_64-unknown-linux-gnu/amd64 (64-bit)

locale:
 [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              
 [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    
 [5] LC_MONETARY=C              LC_MESSAGES=en_US.UTF-8   
 [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 
 [9] LC_ADDRESS=C               LC_TELEPHONE=C            
[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] data.table_1.6.3
> DT = as.data.table(m)
> for (i in 1:1000) DT[1,V1 <- 3]
> DT[1,]
     V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V12 V13 V14 V15 V16 V17 V18 V19 V20 V21
[1,]  1  1  1  1  1  1  1  1  1   1   1   1   1   1   1   1   1   1   1   1   1



>> Also I suspect there is something quite amiss because even trivial things
>> don't work:
>> 
>>> DF[1:4,1:4]
>>  V1 V2 V3 V4
>> 1  3  1  1  1
>> 2  1  1  1  1
>> 3  1  1  1  1
>> 4  1  1  1  1
>>> DT[1:4,1:4]
>> [1] 1 2 3 4
> 
> That's correct and fundamental to data.table. See FAQs 1.1, 1.7, 1.8, 1.9
> and 1.10.
> 

Fair enough, I expected data.table to be a drop-in replacement of data.frames - I just wanted to check the values. Apparently it's not, by design, hence assumption was wrong.


>> 
>> When I first saw your proposal, I thought you have rather something like
>> within(DT, V1[1] <- 3)
>> in mind which looks innocent enough but performs terribly (note that I had
>> to scale down the loop by a factor of 100!!!):
>> 
>>> system.time(for (i in 1:10) within(DT, V1[1] <- 3))
>>   user  system elapsed
>>  2.701   4.437   7.138
> 
> No, since 'with' is already built into data.table, I was thinking of
> building 'within' in, too. I'll take a look at within(). Might as well
> provide as many options as possible to the user to use as they wish.
> 
>> With the for loop something like within(DF, for (i in 1:1000) V1[i] <- 3))
>> performs reasonably:
>> 
>>> system.time(within(DT, for (i in 1:1000) V1[i] <- 3))
>>   user  system elapsed
>>  0.392   0.613   1.003
>> 
>> (Note: system.time() can be misleading when within() is involved, because
>> the expression is evaluated in a different environment so within() won't
>> actually change the object in the  global environment - it also interacts
>> with the possible duplication)
> 
> Noted, thanks. That's pretty fast. Does within() on data.frame fix the
> original issue Ivo raised, then?  If so, job done.
> 

I don't think so - at least not in the strict sense of no copies (more digging may be needed, though, since it does so in system.time, possibly due to the NAMED value of the forced promise but I did not check). However, it allows to express the modification inside the expression which will save the global copy and thus be faster that the outside loop.

Cheers,
Simon



>> 
>> Cheers,
>> Simon
>> 
>> On Jul 11, 2011, at 8:21 PM, Matthew Dowle wrote:
>> 
>>> Thanks for the replies and info. An attempt at fast
>>> assign is now committed to data.table v1.6.3 on
>>> R-Forge. From NEWS :
>>> 
>>> o   Fast update is now implemented, FR#200.
>>>   DT[i,j]<-value is now handled by data.table in C rather
>>>   than falling through to data.frame methods.
>>> 
>>>   Thanks to Ivo Welch for raising speed issues on r-devel,
>>>   to Simon Urbanek for the suggestion, and Luke Tierney and
>>>   Simon for information on R internals.
>>> 
>>>   [<- syntax still incurs one working copy of the whole
>>>   table (as of R 2.13.0) due to R's [<- dispatch mechanism
>>>   copying to `*tmp*`, so, for ultimate speed and brevity,
>>>   'within' syntax is now available as follows.
>>> 
>>> o   A new 'within' argument has been added to [.data.table,
>>>   by default TRUE. It is very similar to the within()
>>>   function in base R. If an assignment appears in j, it
>>>   assigns to the column of DT, by reference; e.g.,
>>> 
>>>   DT[i,colname<-value]
>>> 
>>>   This syntax makes no copies of any part of memory at all.
>>> 
>>>> m = matrix(1,nrow=100000,ncol=100)
>>>> DF = as.data.frame(m)
>>>> DT = as.data.table(m)
>>>> system.time(for (i in 1:1000) DF[1,1] <- 3)
>>>      user  system elapsed
>>>   287.730 323.196 613.453
>>>> system.time(for (i in 1:1000) DT[1,V1 <- 3])
>>>      user  system elapsed
>>>     1.152   0.004   1.161         # 528 times faster
>>> 
>>> Please note :
>>> 
>>>   *******************************************************
>>>   **  Within syntax is presently highly experimental.  **
>>>   *******************************************************
>>> 
>>> http://datatable.r-forge.r-project.org/
>>> 
>>> 
>>> On Wed, 2011-07-06 at 09:08 -0500, luke-tierney at uiowa.edu wrote:
>>>> On Wed, 6 Jul 2011, Simon Urbanek wrote:
>>>> 
>>>>> Interesting, and I stand corrected:
>>>>> 
>>>>>> x = data.frame(a=1:n,b=1:n)
>>>>>> .Internal(inspect(x))
>>>>> @103511c00 19 VECSXP g0c2 [OBJ,NAM(2),ATT] (len=2, tl=0)
>>>>> @102c7b000 13 INTSXP g0c7 [] (len=100000, tl=0) 1,2,3,4,5,...
>>>>> @102af3000 13 INTSXP g0c7 [] (len=100000, tl=0) 1,2,3,4,5,...
>>>>> 
>>>>>> x[1,1]=42L
>>>>>> .Internal(inspect(x))
>>>>> @10349c720 19 VECSXP g0c2 [OBJ,NAM(2),ATT] (len=2, tl=0)
>>>>> @102c19000 13 INTSXP g0c7 [] (len=100000, tl=0) 42,2,3,4,5,...
>>>>> @102b55000 13 INTSXP g0c7 [] (len=100000, tl=0) 1,2,3,4,5,...
>>>>> 
>>>>>> x[[1]][1]=42L
>>>>>> .Internal(inspect(x))
>>>>> @103511a78 19 VECSXP g1c2 [OBJ,MARK,NAM(2),ATT] (len=2, tl=0)
>>>>> @102e65000 13 INTSXP g0c7 [] (len=100000, tl=0) 42,2,3,4,5,...
>>>>> @101f14000 13 INTSXP g1c7 [MARK] (len=100000, tl=0) 1,2,3,4,5,...
>>>>> 
>>>>>> x[[1]][1]=42L
>>>>>> .Internal(inspect(x))
>>>>> @10349c800 19 VECSXP g0c2 [OBJ,NAM(2),ATT] (len=2, tl=0)
>>>>> @102a2f000 13 INTSXP g0c7 [] (len=100000, tl=0) 42,2,3,4,5,...
>>>>> @102ec7000 13 INTSXP g0c7 [] (len=100000, tl=0) 1,2,3,4,5,...
>>>>> 
>>>>> 
>>>>> I have R to release ;) so I won't be looking into this right now, but
>>>>> it's something worth investigating ... Since all the inner contents
>>>>> have NAMED=0 I would not expect any duplication to be needed, but
>>>>> apparently becomes so is at some point ...
>>>> 
>>>> 
>>>> The internals assume in various places that deep copies are made (one
>>>> of the reasons NAMED setings are not propagated to sub-sturcture).
>>>> The main issues are avoiding cycles and that there is no easy way to
>>>> check for sharing.  There may be some circumstances in which a shallow
>>>> copy would be OK but making sure it would be in all cases is probably
>>>> more trouble than it is worth at this point. (I've tried this in the
>>>> past in a few cases and always had to back off.)
>>>> 
>>>> 
>>>> Best,
>>>> 
>>>> luke
>>>> 
>>>>> 
>>>>> Cheers,
>>>>> Simon
>>>>> 
>>>>> 
>>>>> On Jul 6, 2011, at 4:36 AM, Matthew Dowle wrote:
>>>>> 
>>>>>> 
>>>>>> On Tue, 2011-07-05 at 21:11 -0400, Simon Urbanek wrote:
>>>>>>> No subassignment function satisfies that condition, because you can
>>>>>>> always call them directly. However, that doesn't stop the default
>>>>>>> method from making that assumption, so I'm not sure it's an issue.
>>>>>>> 
>>>>>>> David, Just to clarify - the data frame content is not copied, we
>>>>>>> are talking about the vector holding columns.
>>>>>> 
>>>>>> If it is just the vector holding the columns that is copied (and not
>>>>>> the
>>>>>> columns themselves), why does n make a difference in this test (on R
>>>>>> 2.13.0)?
>>>>>> 
>>>>>>> n = 1000
>>>>>>> x = data.frame(a=1:n,b=1:n)
>>>>>>> system.time(for (i in 1:1000) x[1,1] <- 42L)
>>>>>> user  system elapsed
>>>>>> 0.628   0.000   0.628
>>>>>>> n = 100000
>>>>>>> x = data.frame(a=1:n,b=1:n)      # still 2 columns, but longer
>>>>>>> columns
>>>>>>> system.time(for (i in 1:1000) x[1,1] <- 42L)
>>>>>> user  system elapsed
>>>>>> 20.145   1.232  21.455
>>>>>>> 
>>>>>> 
>>>>>> With $<- :
>>>>>> 
>>>>>>> n = 1000
>>>>>>> x = data.frame(a=1:n,b=1:n)
>>>>>>> system.time(for (i in 1:1000) x$a[1] <- 42L)
>>>>>> user  system elapsed
>>>>>> 0.304   0.000   0.307
>>>>>>> n = 100000
>>>>>>> x = data.frame(a=1:n,b=1:n)
>>>>>>> system.time(for (i in 1:1000) x$a[1] <- 42L)
>>>>>> user  system elapsed
>>>>>> 37.586   0.388  38.161
>>>>>>> 
>>>>>> 
>>>>>> If it's because the 1st column needs to be copied (only) because
>>>>>> that's
>>>>>> the one being assigned to (in this test), that magnitude of slow down
>>>>>> doesn't seem consistent with the time of a vector copy of the 1st
>>>>>> column :
>>>>>> 
>>>>>>> n=100000
>>>>>>> v = 1:n
>>>>>>> system.time(for (i in 1:1000) v[1] <- 42L)
>>>>>> user  system elapsed
>>>>>> 0.016   0.000   0.017
>>>>>>> system.time(for (i in 1:1000) {v2=v;v2[1] <- 42L})
>>>>>> user  system elapsed
>>>>>> 1.816   1.076   2.900
>>>>>> 
>>>>>> Finally, increasing the number of columns, again only the 1st is
>>>>>> assigned to :
>>>>>> 
>>>>>>> n=100000
>>>>>>> x = data.frame(rep(list(1:n),100))
>>>>>>> dim(x)
>>>>>> [1] 100000    100
>>>>>>> system.time(for (i in 1:1000) x[1,1] <- 42L)
>>>>>> user  system elapsed
>>>>>> 167.974  50.903 219.711
>>>>>>> 
>>>>>> 
>>>>>> 
>>>>>> 
>>>>>>> 
>>>>>>> Cheers,
>>>>>>> Simon
>>>>>>> 
>>>>>>> Sent from my iPhone
>>>>>>> 
>>>>>>> On Jul 5, 2011, at 9:01 PM, David Winsemius <dwinsemius at comcast.net>
>>>>>>> wrote:
>>>>>>> 
>>>>>>>> 
>>>>>>>> On Jul 5, 2011, at 7:18 PM, <luke-tierney at uiowa.edu>
>>>>>>>> <luke-tierney at uiowa.edu> wrote:
>>>>>>>> 
>>>>>>>>> On Tue, 5 Jul 2011, Simon Urbanek wrote:
>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>> On Jul 5, 2011, at 2:08 PM, Matthew Dowle wrote:
>>>>>>>>>> 
>>>>>>>>>>> Simon (and all),
>>>>>>>>>>> 
>>>>>>>>>>> I've tried to make assignment as fast as calling
>>>>>>>>>>> `[<-.data.table`
>>>>>>>>>>> directly, for user convenience. Profiling shows (IIUC) that it
>>>>>>>>>>> isn't
>>>>>>>>>>> dispatch, but x being copied. Is there a way to prevent '[<-'
>>>>>>>>>>> from
>>>>>>>>>>> copying x?
>>>>>>>>>> 
>>>>>>>>>> Good point, and conceptually, no. It's a subassignment after all
>>>>>>>>>> - see R-lang 3.4.4 - it is equivalent to
>>>>>>>>>> 
>>>>>>>>>> `*tmp*` <- x
>>>>>>>>>> x <- `[<-`(`*tmp*`, i, j, value)
>>>>>>>>>> rm(`*tmp*`)
>>>>>>>>>> 
>>>>>>>>>> so there is always a copy involved.
>>>>>>>>>> 
>>>>>>>>>> Now, a conceptual copy doesn't mean real copy in R since R tries
>>>>>>>>>> to keep the pass-by-value illusion while passing references in
>>>>>>>>>> cases where it knows that modifications cannot occur and/or they
>>>>>>>>>> are safe. The default subassign method uses that feature which
>>>>>>>>>> means it can afford to not duplicate if there is only one
>>>>>>>>>> reference -- then it's safe to not duplicate as we are replacing
>>>>>>>>>> that only existing reference. And in the case of a matrix, that
>>>>>>>>>> will be true at the latest from the second subassignment on.
>>>>>>>>>> 
>>>>>>>>>> Unfortunately the method dispatch (AFAICS) introduces one more
>>>>>>>>>> reference in the dispatch chain so there will always be two
>>>>>>>>>> references so duplication is necessary. Since we have only 0 / 1
>>>>>>>>>> / 2+ information on the references, we can't distinguish whether
>>>>>>>>>> the second reference is due to the dispatch or due to the passed
>>>>>>>>>> object having more than one reference, so we have to duplicate in
>>>>>>>>>> any case. That is unfortunate, and I don't see a way around
>>>>>>>>>> (unless we handle subassignment methods is some special way).
>>>>>>>>> 
>>>>>>>>> I don't believe dispatch is bumping NAMED (and a quick experiment
>>>>>>>>> seems to confirm this though I don't guarantee I did that right).
>>>>>>>>> The
>>>>>>>>> issue is that a replacement function implemented as a closure,
>>>>>>>>> which
>>>>>>>>> is the only option for a package, will always see NAMED on the
>>>>>>>>> object
>>>>>>>>> to be modified as 2 (because the value is obtained by forcing the
>>>>>>>>> argument promise) and so any R level assignments will duplicate.
>>>>>>>>> This
>>>>>>>>> also isn't really an issue of imprecise reference counting --
>>>>>>>>> there
>>>>>>>>> really are (at least) two legitimate references -- one though the
>>>>>>>>> argument and one through the caller's environment.
>>>>>>>>> 
>>>>>>>>> It would be good it we could come up with a way for packages to be
>>>>>>>>> able to define replacement functions that do not duplicate in
>>>>>>>>> cases
>>>>>>>>> where we really don't want them to, but this would require coming
>>>>>>>>> up
>>>>>>>>> with some sort of protocol, minimally involving an efficient way
>>>>>>>>> to
>>>>>>>>> detect whether a replacement funciton is being called in a
>>>>>>>>> replacement
>>>>>>>>> context or directly.
>>>>>>>> 
>>>>>>>> Would "$<-" always satisfy that condition. It would be big help to
>>>>>>>> me if it could be designed to avoid duplication the rest of the
>>>>>>>> data.frame.
>>>>>>>> 
>>>>>>>> --
>>>>>>>> 
>>>>>>>>> 
>>>>>>>>> There are some replacement functions that use C code to cheat, but
>>>>>>>>> these may create problems if called directly, so I won't advertise
>>>>>>>>> them.
>>>>>>>>> 
>>>>>>>>> Best,
>>>>>>>>> 
>>>>>>>>> luke
>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>> Cheers,
>>>>>>>>>> Simon
>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>> 
>>>>>>>>> --
>>>>>>>>> Luke Tierney
>>>>>>>>> Statistics and Actuarial Science
>>>>>>>>> Ralph E. Wareham Professor of Mathematical Sciences
>>>>>>>>> University of Iowa                  Phone:
>>>>>>>>> 319-335-3386
>>>>>>>>> Department of Statistics and        Fax:
>>>>>>>>> 319-335-3017
>>>>>>>>> Actuarial Science
>>>>>>>>> 241 Schaeffer Hall                  email:
>>>>>>>>> luke at stat.uiowa.edu
>>>>>>>>> Iowa City, IA 52242                 WWW:
>>>>>>>>> http://www.stat.uiowa.edu______________________________________________
>>>>>>>>> R-devel at r-project.org mailing list
>>>>>>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>>>>> 
>>>>>>>> David Winsemius, MD
>>>>>>>> West Hartford, CT
>>>>>>>> 
>>>>>>>> 
>>>>>> 
>>>>>> 
>>>>>> 
>>>>> 
>>>>> 
>>>> 
>>>> --
>>>> Luke Tierney
>>>> Statistics and Actuarial Science
>>>> Ralph E. Wareham Professor of Mathematical Sciences
>>>> University of Iowa                  Phone:             319-335-3386
>>>> Department of Statistics and        Fax:               319-335-3017
>>>>   Actuarial Science
>>>> 241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
>>>> Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu
>>> 
>>> 
>>> 
>> 
>> 
> 
> 
> 


From ligges at statistik.tu-dortmund.de  Tue Jul 12 18:45:25 2011
From: ligges at statistik.tu-dortmund.de (Uwe Ligges)
Date: Tue, 12 Jul 2011 18:45:25 +0200
Subject: [Rd] [linux] connection never times out
In-Reply-To: <1310473470487-3662088.post@n4.nabble.com>
References: <1310473470487-3662088.post@n4.nabble.com>
Message-ID: <4E1C7A25.9060605@statistik.tu-dortmund.de>

?connections tells us:

"Note that this is a timeout for no response, not for the whole operation."

And indeed, it will take roughly 20 seconds rather than 60 - at least on 
the Linux machine I tried it on with R-2.13.1.

Best,
Uwe Ligges




On 12.07.2011 14:24, jeroen00ms wrote:
> According to the download.file manual the timeout of a connection can be set
> using options(timeout=10). This seems to work as expected on windows, but on
> linux the connection does not timeout. I reproduced the problem both 0on
> R-2.13 on Ubuntu and on R-2.12.1 on CentOS, but not in Windows.
>
>> options(timeout=5)
>> download.file("http://123.123.123.123/bla", dest=tempfile())
>
> I am running Ubuntu 11.04 with the R binaries from CRAN:
>
>> sessionInfo()
> R version 2.13.0 (2011-04-13)
> Platform: i686-pc-linux-gnu (32-bit)
>
> locale:
>   [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
>   [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
>   [5] LC_MONETARY=C              LC_MESSAGES=en_US.UTF-8
>   [7] LC_PAPER=en_US.UTF-8       LC_NAME=C
>   [9] LC_ADDRESS=C               LC_TELEPHONE=C
> [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/linux-connection-never-times-out-tp3662088p3662088.html
> Sent from the R devel mailing list archive at Nabble.com.
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From jeroen.ooms at stat.ucla.edu  Tue Jul 12 22:22:51 2011
From: jeroen.ooms at stat.ucla.edu (Jeroen Ooms)
Date: Tue, 12 Jul 2011 22:22:51 +0200
Subject: [Rd] [linux] connection never times out
In-Reply-To: <4E1CA795.4030201@statistik.tu-dortmund.de>
References: <1310473470487-3662088.post@n4.nabble.com>
	<4E1C7A25.9060605@statistik.tu-dortmund.de>
	<CABFfbXuoXctnrXB_Mw7MmOeuZT2Tjnm-ZfgexScjx_NNqF1q1g@mail.gmail.com>
	<4E1CA795.4030201@statistik.tu-dortmund.de>
Message-ID: <CABFfbXtp-BGi8Kxd9+1pgysVpxQieFmdYEMh-NcKXxWOC=8ncw@mail.gmail.com>

> Can you please verify the behaviour is still the same in a recent R-devel or
> at least R-2.13.1? And that there was no other already answered request on
> R-help or R-devel re. timeouts?

The code below is R 2.13.1. It shows that the timeout time is more
than 3 minutes, although it was set to 5 seconds.

> options(timeout=5)
> system.time(download.file("http://123.123.123.123", dest=tempfile()))
trying URL 'http://123.123.123.123'
Error in download.file("http://123.123.123.123", dest = tempfile()) :
  cannot open URL 'http://123.123.123.123'
In addition: Warning message:
In download.file("http://123.123.123.123", dest = tempfile()) :
  unable to connect to '123.123.123.123' on port 80.
Timing stopped at: 0 0 189.375
> sessionInfo()
R version 2.13.1 (2011-07-08)
Platform: i686-pc-linux-gnu (32-bit)

locale:
 [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
 [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
 [5] LC_MONETARY=C              LC_MESSAGES=en_US.UTF-8
 [7] LC_PAPER=en_US.UTF-8       LC_NAME=C
 [9] LC_ADDRESS=C               LC_TELEPHONE=C
[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base
>


From simon.urbanek at r-project.org  Tue Jul 12 23:23:24 2011
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Tue, 12 Jul 2011 17:23:24 -0400
Subject: [Rd] [linux] connection never times out
In-Reply-To: <CABFfbXtp-BGi8Kxd9+1pgysVpxQieFmdYEMh-NcKXxWOC=8ncw@mail.gmail.com>
References: <1310473470487-3662088.post@n4.nabble.com>
	<4E1C7A25.9060605@statistik.tu-dortmund.de>
	<CABFfbXuoXctnrXB_Mw7MmOeuZT2Tjnm-ZfgexScjx_NNqF1q1g@mail.gmail.com>
	<4E1CA795.4030201@statistik.tu-dortmund.de>
	<CABFfbXtp-BGi8Kxd9+1pgysVpxQieFmdYEMh-NcKXxWOC=8ncw@mail.gmail.com>
Message-ID: <87B38946-32A5-4B84-A649-52F02F248D22@r-project.org>


On Jul 12, 2011, at 4:22 PM, Jeroen Ooms wrote:

>> Can you please verify the behaviour is still the same in a recent R-devel or
>> at least R-2.13.1? And that there was no other already answered request on
>> R-help or R-devel re. timeouts?
> 
> The code below is R 2.13.1. It shows that the timeout time is more
> than 3 minutes, although it was set to 5 seconds.
> 

Please set
options(internet.info=0)
and re-run your test.
Are you running this from a command-line R or do you have any graphics or GUIs running? (I'm asking because any fast handler activity will cancel timeouts)

Thanks,
Simon


>> options(timeout=5)
>> system.time(download.file("http://123.123.123.123", dest=tempfile()))
> trying URL 'http://123.123.123.123'
> Error in download.file("http://123.123.123.123", dest = tempfile()) :
>  cannot open URL 'http://123.123.123.123'
> In addition: Warning message:
> In download.file("http://123.123.123.123", dest = tempfile()) :
>  unable to connect to '123.123.123.123' on port 80.
> Timing stopped at: 0 0 189.375
>> sessionInfo()
> R version 2.13.1 (2011-07-08)
> Platform: i686-pc-linux-gnu (32-bit)
> 
> locale:
> [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
> [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
> [5] LC_MONETARY=C              LC_MESSAGES=en_US.UTF-8
> [7] LC_PAPER=en_US.UTF-8       LC_NAME=C
> [9] LC_ADDRESS=C               LC_TELEPHONE=C
> [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C
> 
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>> 
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 
> 


From simon.urbanek at r-project.org  Tue Jul 12 23:44:27 2011
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Tue, 12 Jul 2011 17:44:27 -0400
Subject: [Rd] [linux] connection never times out
In-Reply-To: <87B38946-32A5-4B84-A649-52F02F248D22@r-project.org>
References: <1310473470487-3662088.post@n4.nabble.com>
	<4E1C7A25.9060605@statistik.tu-dortmund.de>
	<CABFfbXuoXctnrXB_Mw7MmOeuZT2Tjnm-ZfgexScjx_NNqF1q1g@mail.gmail.com>
	<4E1CA795.4030201@statistik.tu-dortmund.de>
	<CABFfbXtp-BGi8Kxd9+1pgysVpxQieFmdYEMh-NcKXxWOC=8ncw@mail.gmail.com>
	<87B38946-32A5-4B84-A649-52F02F248D22@r-project.org>
Message-ID: <04CE1008-3A51-4413-9B14-0BCE8C3FD00B@r-project.org>

Never mind, I found the issue - contrary to the documentation Linux does modify tv in the call to select() so our measure of elapsed time doesn't increase. Work-around now present in R-devel.

Cheers,
Simon


On Jul 12, 2011, at 5:23 PM, Simon Urbanek wrote:

> 
> On Jul 12, 2011, at 4:22 PM, Jeroen Ooms wrote:
> 
>>> Can you please verify the behaviour is still the same in a recent R-devel or
>>> at least R-2.13.1? And that there was no other already answered request on
>>> R-help or R-devel re. timeouts?
>> 
>> The code below is R 2.13.1. It shows that the timeout time is more
>> than 3 minutes, although it was set to 5 seconds.
>> 
> 
> Please set
> options(internet.info=0)
> and re-run your test.
> Are you running this from a command-line R or do you have any graphics or GUIs running? (I'm asking because any fast handler activity will cancel timeouts)
> 
> Thanks,
> Simon
> 
> 
>>> options(timeout=5)
>>> system.time(download.file("http://123.123.123.123", dest=tempfile()))
>> trying URL 'http://123.123.123.123'
>> Error in download.file("http://123.123.123.123", dest = tempfile()) :
>> cannot open URL 'http://123.123.123.123'
>> In addition: Warning message:
>> In download.file("http://123.123.123.123", dest = tempfile()) :
>> unable to connect to '123.123.123.123' on port 80.
>> Timing stopped at: 0 0 189.375
>>> sessionInfo()
>> R version 2.13.1 (2011-07-08)
>> Platform: i686-pc-linux-gnu (32-bit)
>> 
>> locale:
>> [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
>> [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
>> [5] LC_MONETARY=C              LC_MESSAGES=en_US.UTF-8
>> [7] LC_PAPER=en_US.UTF-8       LC_NAME=C
>> [9] LC_ADDRESS=C               LC_TELEPHONE=C
>> [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C
>> 
>> attached base packages:
>> [1] stats     graphics  grDevices utils     datasets  methods   base
>>> 
>> 
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>> 
>> 
> 


From ripley at stats.ox.ac.uk  Wed Jul 13 15:07:51 2011
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 13 Jul 2011 14:07:51 +0100 (BST)
Subject: [Rd] save.image compression_level argument
In-Reply-To: <20110711142445.02e456bf039156f791943764322f7efe.fc28463a38.wbe@email17.secureserver.net>
References: <20110711142445.02e456bf039156f791943764322f7efe.fc28463a38.wbe@email17.secureserver.net>
Message-ID: <alpine.LFD.2.02.1107121720560.14411@gannet.stats.ox.ac.uk>

On Mon, 11 Jul 2011, andreas at eckner.com wrote:

> Hi,
>
> in "save.image", it would be nice if there was a "compression_level"
> argument that is passed along to "save".
>
> Or is there a reason for disabling the "compression_level" option for
> saving workspaces, but enabling it for manually saving individual
> objects?

Why not just call save() yourself?  save.image() is a rarely used 
(directly) convenience wrapper.

And why do you want to change the compression level?  There is very 
little advantage in using it for 'gzip' compression, and really the 
only significant use is reduce it to save time for xz compression.

If you are volunteering others to add a feature, it behooves you to 
explain why it would be useful to you and might be to others.  Nothing 
is 'disabled': this is a new feature request ....
>
> Thanks,
> Andreas
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ehlers at ucalgary.ca  Wed Jul 13 22:47:03 2011
From: ehlers at ucalgary.ca (Peter Ehlers)
Date: Wed, 13 Jul 2011 13:47:03 -0700
Subject: [Rd] Tiny typo in help.Rd
Message-ID: <4E1E0447.9060407@ucalgary.ca>

A recent quote by Bert Gunter from the Details section of help('help')
over on R-help has this (line 82 in help.Rd):

   character string.  There include those which cannot syntactically

where the word 'There' should be 'These'.

(still there in r56374)

Peter Ehlers


From murdoch.duncan at gmail.com  Wed Jul 13 23:00:00 2011
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Wed, 13 Jul 2011 17:00:00 -0400
Subject: [Rd] Tiny typo in help.Rd
In-Reply-To: <4E1E0447.9060407@ucalgary.ca>
References: <4E1E0447.9060407@ucalgary.ca>
Message-ID: <4E1E0750.8070801@gmail.com>

On 11-07-13 4:47 PM, Peter Ehlers wrote:
> A recent quote by Bert Gunter from the Details section of help('help')
> over on R-help has this (line 82 in help.Rd):
>
>     character string.  There include those which cannot syntactically
>
> where the word 'There' should be 'These'.
>
> (still there in r56374)

Fixed now.  Thanks!

Duncan


From alireza.s.mahani at gmail.com  Wed Jul 13 15:28:55 2011
From: alireza.s.mahani at gmail.com (asmahani)
Date: Wed, 13 Jul 2011 06:28:55 -0700 (PDT)
Subject: [Rd] Performance of .C and .Call functions vs. native R code
Message-ID: <1310563735586-3665017.post@n4.nabble.com>

Hello,

I am in the process of writing an R extension for parallelized MCMC, with
heavy use of compiled code (C++). I have been getting my feet wet by
implementing a simple matrix-vector multiplication function in C++ (which
calls a BLAS level 2 function dgemv), and comparing it to the '%*%' operator
in R (which apparently calls a BLAS level 3 function dgemm).

Interestingly, I cannot replicate the performance of the R native operator,
using either '.C' or '.Call'. The relative times are 17 (R), 30 (.C), and 26
(.Call). In other words, R native operator is 1.5x faster than my compiled
code. Can you explain to me why this is? Through testing I strongly suspect
that the BLAS function itself isn't what takes the bulk part of the time,
but perhaps data transfer and other overhead associated with the calls (.C
and .Call) are the main issues. Are there any ways to reach the performance
level of native R code in this case?

Thank you,
Alireza Mahani

--
View this message in context: http://r.789695.n4.nabble.com/Performance-of-C-and-Call-functions-vs-native-R-code-tp3665017p3665017.html
Sent from the R devel mailing list archive at Nabble.com.


From andreas at eckner.com  Wed Jul 13 18:35:31 2011
From: andreas at eckner.com (Andreas Eckner)
Date: Wed, 13 Jul 2011 12:35:31 -0400
Subject: [Rd] save.image compression_level argument
In-Reply-To: <alpine.LFD.2.02.1107121720560.14411@gannet.stats.ox.ac.uk>
References: <20110711142445.02e456bf039156f791943764322f7efe.fc28463a38.wbe@email17.secureserver.net>
	<alpine.LFD.2.02.1107121720560.14411@gannet.stats.ox.ac.uk>
Message-ID: <A1D64CE4AE2C4DB7A31159064B4BD78C@mainuser>

> On Mon, 11 Jul 2011, andreas at eckner.com wrote:
> 
>> Hi,
>>
>> in "save.image", it would be nice if there was a "compression_level"
>> argument that is passed along to "save".
>>
>> Or is there a reason for disabling the "compression_level" option for 
>> saving workspaces, but enabling it for manually saving individual 
>> objects?
> 
> Why not just call save() yourself?  save.image() is a rarely used
> (directly) convenience wrapper.

I believe a decent number of R users indeed use save.image() due to its
convenience. 

> And why do you want to change the compression level?  There is very little
advantage in using it for 'gzip' compression, and really the only
significant use is reduce it to save time for xz compression.
> 
> If you are volunteering others to add a feature, it behooves you to
explain why it would be useful to you and might be to others.  Nothing is
'disabled': this is a new feature request ....

For large workspaces on the order of several hundred MB (I'm working on
astronomical datasets), 'gzip' with the default compression_level=6 can take
several minutes. Using compression_level=1 is about three times faster,
while the file size only increases by ~10% (results may of course vary
across application and system, but the numbers are probably representative).
When saving work in progress (as opposed to sharing it externally), the time
it takes to save workspace is probably the prime concern for most users.

I have attached a proposed modified definition of save.image() at the end of
the message.

Cheers,
Andreas

>>
>> Thanks,
>> Andreas
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595


save.image <- function (file = ".RData", version = NULL, ascii = FALSE,
                        compress = !ascii, safe = TRUE, compression_level)
{
    if (! is.character(file) || file == "")
        stop("'file' must be non-empty string")

    opts <- getOption("save.image.defaults")
    if(is.null(opts)) opts <- getOption("save.defaults")

    if (missing(safe) && ! is.null(opts$safe))
        safe <- opts$safe
    if (missing(ascii) && ! is.null(opts$ascii))
        ascii <- opts$ascii
    if (missing(compress) && ! is.null(opts$compress))
        compress <- opts$compress
    if (missing(version)) version <- opts$version

    if (safe) {
        ## find a temporary file name in the same directory so we can
        ## rename it to the final output file on success
        outfile <- paste(file, "Tmp", sep = "")
        i <- 0
        while (file.exists(outfile)) {
            i <- i + 1
            outfile <- paste(file, "Tmp", i, sep = "")
        }
    }
    else outfile <- file

    on.exit(file.remove(outfile))
    if (! missing(compression_level))
        save(list = ls(envir = .GlobalEnv, all.names = TRUE), file =
outfile,
            version = version, ascii = ascii, compress = compress,
            envir = .GlobalEnv, precheck = FALSE, compression_level =
compression_level)
    else
        save(list = ls(envir = .GlobalEnv, all.names = TRUE), file =
outfile,
            version = version, ascii = ascii, compress = compress,
            envir = .GlobalEnv, precheck = FALSE)
    if (safe)
        if (! file.rename(outfile, file)) {
            on.exit()
            stop("image could not be renamed and is left in ", outfile)
        }
    on.exit()
}


From r.hijmans at gmail.com  Wed Jul 13 20:06:17 2011
From: r.hijmans at gmail.com (Robert J. Hijmans)
Date: Wed, 13 Jul 2011 11:06:17 -0700
Subject: [Rd] image adds lines
Message-ID: <CANtt_hx3JXKpFr-FaSapZm7ht3F6KLGqayVKVeD9gMpU2MiqtQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20110713/e6578df7/attachment.pl>

From jeff.a.ryan at gmail.com  Thu Jul 14 14:12:32 2011
From: jeff.a.ryan at gmail.com (Jeff Ryan)
Date: Thu, 14 Jul 2011 07:12:32 -0500
Subject: [Rd] Performance of .C and .Call functions vs. native R code
In-Reply-To: <1310563735586-3665017.post@n4.nabble.com>
References: <1310563735586-3665017.post@n4.nabble.com>
Message-ID: <05EF8F1E-DC69-4A25-9021-47D9BF841316@gmail.com>

The .Call overhead isn't the issue. If you'd like some insight into what you are doing wrong (and right), you need to provide code for the list to reproduce your timings with.

This is outlined in the posting guide as well.

Best,
Jeff



On Jul 13, 2011, at 8:28 AM, asmahani <alireza.s.mahani at gmail.com> wrote:

> Hello,
> 
> I am in the process of writing an R extension for parallelized MCMC, with
> heavy use of compiled code (C++). I have been getting my feet wet by
> implementing a simple matrix-vector multiplication function in C++ (which
> calls a BLAS level 2 function dgemv), and comparing it to the '%*%' operator
> in R (which apparently calls a BLAS level 3 function dgemm).
> 
> Interestingly, I cannot replicate the performance of the R native operator,
> using either '.C' or '.Call'. The relative times are 17 (R), 30 (.C), and 26
> (.Call). In other words, R native operator is 1.5x faster than my compiled
> code. Can you explain to me why this is? Through testing I strongly suspect
> that the BLAS function itself isn't what takes the bulk part of the time,
> but perhaps data transfer and other overhead associated with the calls (.C
> and .Call) are the main issues. Are there any ways to reach the performance
> level of native R code in this case?
> 
> Thank you,
> Alireza Mahani
> 
> --
> View this message in context: http://r.789695.n4.nabble.com/Performance-of-C-and-Call-functions-vs-native-R-code-tp3665017p3665017.html
> Sent from the R devel mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From alireza.s.mahani at gmail.com  Thu Jul 14 17:21:07 2011
From: alireza.s.mahani at gmail.com (Alireza Mahani)
Date: Thu, 14 Jul 2011 08:21:07 -0700 (PDT)
Subject: [Rd] Performance of .C and .Call functions vs. native R code
In-Reply-To: <05EF8F1E-DC69-4A25-9021-47D9BF841316@gmail.com>
References: <1310563735586-3665017.post@n4.nabble.com>
	<05EF8F1E-DC69-4A25-9021-47D9BF841316@gmail.com>
Message-ID: <1310656867725-3667896.post@n4.nabble.com>

(I am using a LINUX machine)

Jeff,

In creating reproducible results, I 'partially' answered my question. I have
attached two scripts, 'mvMultiply.r' and 'mvMultiply.cc'. Please copy both
files into your chosen directory, then run 'Rscript mvMultiply.r' in that
directory while changing the two boolean parameters 'INCLUDE_DATAPREP' and
'ROWMAJOR' to all four permutations. (The variable 'diffVec' is there to
verify that the two methods produce the same output vector.)

Below are the results that I get, along with discussion (tR and tCall are in
sec):

INCLUDE_DATAPREP,ROWMAJOR,tR,tCall
F,F,13.536,13.875
F,T,13.824,14.299
T,F,13.688,18.167
T,T,13.982,30.730

Interpretation: The execution time for the .Call line is nearly identical to
the call to R operator '%*%'. Two data preparation lines for the .Call
method create the overhead: 

A <- t(A) (~12sec, or 12usec per call)
dim(A) <- dim(A)[1] * dim(A)[2] (~4sec, or 4usec per call)

While the first line can be avoided by providing options in c++ function (as
is done in the BLAS API), I wonder if the second line can be avoided, aside
from the obvious option of rewriting the R scripts to use vectors instead of
matrices. But this defies one of the primary advantages of using R, which is
succinct, high-level coding. In particular, if one has several matrices as
input into a .Call function, then the overhead from matrix-to-vector
transformations can add up. To summarize, my questions are:

1- Do the above results seem reasonable to you? Is there a similar penalty
in R's '%*%' operator for transforming matrices to vectors before calling
BLAS functions?
2- Are there techniques for reducing the above overhead for developers
looking to augment their R code with compiled code?

Regards,
Alireza

---------------------------------------
# mvMultiply.r
# comparing performance of matrix multiplication in R (using '%*%' operator)
vs. calling compiled code (using .Call function)
# y [m x 1] = A [m x n] %*% x [n x 1]

rm(list = ls())
system("R CMD SHLIB mvMultiply.cc")
dyn.load("mvMultiply.so")

INCLUDE_DATAPREP <- F
ROWMAJOR <- F #indicates whether the c++ function treats A in a row-major or
column-major fashion

m <- 100
n <- 10
N <- 1000000

diffVec <- array(0, dim = N)
tR <- 0.0
tCall <- 0.0
for (i in 1:N) {
	
	A <- runif(m*n); dim(A) <- c(m,n)
	x <- runif(n)

	t1 <- proc.time()[3]
	y1 <- A %*% x
	tR <- tR + proc.time()[3] - t1
	
	if (INCLUDE_DATAPREP) {
		t1 <- proc.time()[3]
	}
	if (ROWMAJOR) { #since R will convert matrix to vector in a column-major
fashion, if the c++ function expects a row-major format, we need to
transpose A before converting it to a vector
		A <- t(A)
	}
	dim(A) <- dim(A)[1] * dim(A)[2]
	if (!INCLUDE_DATAPREP) {
		t1 <- proc.time()[3]
	}
	y2 <- .Call("matvecMultiply", as.double(A), as.double(x),
as.logical(c(ROWMAJOR)))
	tCall <- tCall + proc.time()[3] - t1
	
	diffVec[i] <- max(abs(y2 - y1))
}
cat("Data prep time for '.Call' included: ", INCLUDE_DATAPREP, "\n")
cat("C++ function expects row-major matrix: ", ROWMAJOR, "\n")
cat("Time - Using '%*%' operator in R: ", tR, "sec\n")
cat("Time - Using '.Call' function: ", tCall, "sec\n")
cat("Maximum difference between methods: ", max(diffVec), "\n")

dyn.unload("mvMultiply.so")
---------------------------------------
# mvMultiply.cc
#include <Rinternals.h>
#include <R.h>

extern "C" {

SEXP matvecMultiply(SEXP A, SEXP x, SEXP rowmajor) {
	double *rA = REAL(A), *rx = REAL(x), *ry;
	int *rrm = LOGICAL(rowmajor);
	int n = length(x);
	int m = length(A) / n;
	SEXP y;
	PROTECT(y = allocVector(REALSXP, m));
	ry = REAL(y);
	for (int i = 0; i < m; i++) {
		ry[i] = 0.0;
		for (int j = 0; j < n; j++) {
			if (rrm[0] == 1) {
				ry[i] += rA[i * n + j] * rx[j];
			} else {
				ry[i] += rA[j * m + i] * rx[j];
			}
		}
	}
	UNPROTECT(1);
	return(y);
}

}


--
View this message in context: http://r.789695.n4.nabble.com/Performance-of-C-and-Call-functions-vs-native-R-code-tp3665017p3667896.html
Sent from the R devel mailing list archive at Nabble.com.


From bajaj141003 at gmail.com  Thu Jul 14 17:56:55 2011
From: bajaj141003 at gmail.com (Nipesh Bajaj)
Date: Thu, 14 Jul 2011 21:26:55 +0530
Subject: [Rd] Creating package Vignette
Message-ID: <CA+A3Hr6-SNPmegaZMJzb=yA+1pSLvAGUAd0us=h01nq7xzoVxQ@mail.gmail.com>

Hi all, I was trying to create some vignette files for my newly
developed package, however wondering whether there could be any
simpler way to do so. In writing R extension it is advised to go
through Sweave route, however I have already got a big pdf file and
want to use this as package vignette.

So far I have manually created the inst/doc folder in the main package
skeleton, and put that file into this, which is not working by calling
"vignette(file_name)" after I build  and load the package. I am
getting following error without opening that pdf file: "vignette
'file_name' *not* found"

So I like to know, is there any way to use any arbitrary pdf file as vignette?

Any suggestion is highly appreciated.

Thanks,


From gmbecker at ucdavis.edu  Thu Jul 14 17:59:49 2011
From: gmbecker at ucdavis.edu (Gabriel Becker)
Date: Thu, 14 Jul 2011 08:59:49 -0700
Subject: [Rd] Performance of .C and .Call functions vs. native R code
In-Reply-To: <1310656867725-3667896.post@n4.nabble.com>
References: <1310563735586-3665017.post@n4.nabble.com>
	<05EF8F1E-DC69-4A25-9021-47D9BF841316@gmail.com>
	<1310656867725-3667896.post@n4.nabble.com>
Message-ID: <CADwqtCP=WhYN5fwWiV=_n2ytxb1RL755QbU+wSs3oRgokOfmVA@mail.gmail.com>

On Thu, Jul 14, 2011 at 8:21 AM, Alireza Mahani
<alireza.s.mahani at gmail.com>wrote:

> (I am using a LINUX machine)
>
> Jeff,
>
> In creating reproducible results, I 'partially' answered my question. I
> have
> attached two scripts, 'mvMultiply.r' and 'mvMultiply.cc'. Please copy both
> files into your chosen directory, then run 'Rscript mvMultiply.r' in that
> directory while changing the two boolean parameters 'INCLUDE_DATAPREP' and
> 'ROWMAJOR' to all four permutations. (The variable 'diffVec' is there to
> verify that the two methods produce the same output vector.)
>
> Below are the results that I get, along with discussion (tR and tCall are
> in
> sec):
>
> INCLUDE_DATAPREP,ROWMAJOR,tR,tCall
> F,F,13.536,13.875
> F,T,13.824,14.299
> T,F,13.688,18.167
> T,T,13.982,30.730
>
> Interpretation: The execution time for the .Call line is nearly identical
> to
> the call to R operator '%*%'. Two data preparation lines for the .Call
> method create the overhead:
>
> A <- t(A) (~12sec, or 12usec per call)
> dim(A) <- dim(A)[1] * dim(A)[2] (~4sec, or 4usec per call)
>
>

AFAIK R stores matrices as vectors internally anyway and the dims just tell
it the position of the various elements, so I'm not sure that second line is
needed at all.

I have attached a tiny piece of c code which verifies this.

The output I get from that is:

> dyn.load("/home/gmbecker/gabe/matvectest.so")
> vec = 1.1:8.1
> mat = matrix(vec, ncol = 4)
> .Call("R_MatVecTest", vec, mat, 8L)
[1] TRUE


Note if you create the matrix with byrow=TRUE they may not be the same.

Hope that helps,
Gabe


> While the first line can be avoided by providing options in c++ function
> (as
> is done in the BLAS API), I wonder if the second line can be avoided, aside
> from the obvious option of rewriting the R scripts to use evectors instead
> of
> matrices. But this defies one of the primary advantages of using R, which
> is
> succinct, high-level coding. In particular, if one has several matrices as
> input into a .Call function, then the overhead from matrix-to-vector
> transformations can add up. To summarize, my questions are:
>
> 1- Do the above results seem reasonable to you? Is there a similar penalty
> in R's '%*%' operator for transforming matrices to vectors before calling
> BLAS functions?
> 2- Are there techniques for reducing the above overhead for developers
> looking to augment their R code with compiled code?
>
> Regards,
> Alireza
>
> ---------------------------------------
> # mvMultiply.r
> # comparing performance of matrix multiplication in R (using '%*%'
> operator)
> vs. calling compiled code (using .Call function)
> # y [m x 1] = A [m x n] %*% x [n x 1]
>
> rm(list = ls())
> system("R CMD SHLIB mvMultiply.cc")
> dyn.load("mvMultiply.so")
>
> INCLUDE_DATAPREP <- F
> ROWMAJOR <- F #indicates whether the c++ function treats A in a row-major
> or
> column-major fashion
>
> m <- 100
> n <- 10
> N <- 1000000
>
> diffVec <- array(0, dim = N)
> tR <- 0.0
> tCall <- 0.0
> for (i in 1:N) {
>
>        A <- runif(m*n); dim(A) <- c(m,n)
>        x <- runif(n)
>
>        t1 <- proc.time()[3]
>        y1 <- A %*% x
>        tR <- tR + proc.time()[3] - t1
>
>        if (INCLUDE_DATAPREP) {
>                t1 <- proc.time()[3]
>        }
>        if (ROWMAJOR) { #since R will convert matrix to vector in a
> column-major
> fashion, if the c++ function expects a row-major format, we need to
> transpose A before converting it to a vector
>                A <- t(A)
>        }
>        dim(A) <- dim(A)[1] * dim(A)[2]
>        if (!INCLUDE_DATAPREP) {
>                t1 <- proc.time()[3]
>        }
>        y2 <- .Call("matvecMultiply", as.double(A), as.double(x),
> as.logical(c(ROWMAJOR)))
>        tCall <- tCall + proc.time()[3] - t1
>
>        diffVec[i] <- max(abs(y2 - y1))
> }
> cat("Data prep time for '.Call' included: ", INCLUDE_DATAPREP, "\n")
> cat("C++ function expects row-major matrix: ", ROWMAJOR, "\n")
> cat("Time - Using '%*%' operator in R: ", tR, "sec\n")
> cat("Time - Using '.Call' function: ", tCall, "sec\n")
> cat("Maximum difference between methods: ", max(diffVec), "\n")
>
> dyn.unload("mvMultiply.so")
> ---------------------------------------
> # mvMultiply.cc
> #include <Rinternals.h>
> #include <R.h>
>
> extern "C" {
>
> SEXP matvecMultiply(SEXP A, SEXP x, SEXP rowmajor) {
>        double *rA = REAL(A), *rx = REAL(x), *ry;
>        int *rrm = LOGICAL(rowmajor);
>        int n = length(x);
>        int m = length(A) / n;
>        SEXP y;
>        PROTECT(y = allocVector(REALSXP, m));
>        ry = REAL(y);
>        for (int i = 0; i < m; i++) {
>                ry[i] = 0.0;
>                for (int j = 0; j < n; j++) {
>                        if (rrm[0] == 1) {
>                                ry[i] += rA[i * n + j] * rx[j];
>                        } else {
>                                ry[i] += rA[j * m + i] * rx[j];
>                        }
>                }
>        }
>        UNPROTECT(1);
>        return(y);
> }
>
> }
>
>
> --
> View this message in context:
> http://r.789695.n4.nabble.com/Performance-of-C-and-Call-functions-vs-native-R-code-tp3665017p3667896.html
> Sent from the R devel mailing list archive at Nabble.com.
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>



-- 
Gabriel Becker
Graduate Student
Statistics Department
University of California, Davis

From alireza.s.mahani at gmail.com  Thu Jul 14 18:18:22 2011
From: alireza.s.mahani at gmail.com (Alireza Mahani)
Date: Thu, 14 Jul 2011 09:18:22 -0700 (PDT)
Subject: [Rd] Performance of .C and .Call functions vs. native R code
In-Reply-To: <CADwqtCP=WhYN5fwWiV=_n2ytxb1RL755QbU+wSs3oRgokOfmVA@mail.gmail.com>
References: <1310563735586-3665017.post@n4.nabble.com>
	<05EF8F1E-DC69-4A25-9021-47D9BF841316@gmail.com>
	<1310656867725-3667896.post@n4.nabble.com>
	<CADwqtCP=WhYN5fwWiV=_n2ytxb1RL755QbU+wSs3oRgokOfmVA@mail.gmail.com>
Message-ID: <1310660302985-3668047.post@n4.nabble.com>

You are absolutely right Gabe! I removed the line 'dim(A) <- dim(A)[1] *
dim(A)[2]' and my code still executes properly. As you said, matrices are
internally stored as one-dimensional arrays (column-major by default), it's
just that R exposes them differently by assigning different attributes to
them, but as far as C/C++ code is concerned there is no distinction.

Many thanks,
Alireza


--
View this message in context: http://r.789695.n4.nabble.com/Performance-of-C-and-Call-functions-vs-native-R-code-tp3665017p3668047.html
Sent from the R devel mailing list archive at Nabble.com.


From ripley at stats.ox.ac.uk  Thu Jul 14 18:32:41 2011
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 14 Jul 2011 17:32:41 +0100 (BST)
Subject: [Rd] Creating package Vignette
In-Reply-To: <CA+A3Hr6-SNPmegaZMJzb=yA+1pSLvAGUAd0us=h01nq7xzoVxQ@mail.gmail.com>
References: <CA+A3Hr6-SNPmegaZMJzb=yA+1pSLvAGUAd0us=h01nq7xzoVxQ@mail.gmail.com>
Message-ID: <alpine.LFD.2.02.1107141726570.3525@gannet.stats.ox.ac.uk>

It depends what you mean by 'vignette': the R docs have been unclear 
(but R >= 2.13.0 are more consistent).  In most cases a 'vignette' is 
an Sweave document, the vignette source being the .Rnw file, and the 
vignette PDF the processed .pdf file.

At present vignette() means Sweave documents, as only they have 
metadata like titles.  This is planned to be changed soon.

On Thu, 14 Jul 2011, Nipesh Bajaj wrote:

> Hi all, I was trying to create some vignette files for my newly
> developed package, however wondering whether there could be any
> simpler way to do so. In writing R extension it is advised to go
> through Sweave route, however I have already got a big pdf file and
> want to use this as package vignette.
>
> So far I have manually created the inst/doc folder in the main package
> skeleton, and put that file into this, which is not working by calling
> "vignette(file_name)" after I build  and load the package. I am

file_name is not an argument to vignette(): it is 'topic'.  And topics 
are normally file basenames (without any extension), not file names.

> getting following error without opening that pdf file: "vignette
> 'file_name' *not* found"
>
> So I like to know, is there any way to use any arbitrary pdf file as 
> vignette?

By definition, no.

>
> Any suggestion is highly appreciated.
>
> Thanks,
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From bbolker at gmail.com  Thu Jul 14 19:00:08 2011
From: bbolker at gmail.com (Ben Bolker)
Date: Thu, 14 Jul 2011 17:00:08 +0000
Subject: [Rd] Creating package Vignette
References: <CA+A3Hr6-SNPmegaZMJzb=yA+1pSLvAGUAd0us=h01nq7xzoVxQ@mail.gmail.com>
	<alpine.LFD.2.02.1107141726570.3525@gannet.stats.ox.ac.uk>
Message-ID: <loom.20110714T183517-879@post.gmane.org>

Prof Brian Ripley <ripley <at> stats.ox.ac.uk> writes:

> 
> It depends what you mean by 'vignette': the R docs have been unclear 
> (but R >= 2.13.0 are more consistent).  In most cases a 'vignette' is 
> an Sweave document, the vignette source being the .Rnw file, and the 
> vignette PDF the processed .pdf file.
> 
> At present vignette() means Sweave documents, as only they have 
> metadata like titles.  This is planned to be changed soon.
> 
> On Thu, 14 Jul 2011, Nipesh Bajaj wrote:
> 
> > Hi all, I was trying to create some vignette files for my newly
> > developed package, however wondering whether there could be any
> > simpler way to do so. In writing R extension it is advised to go
> > through Sweave route, however I have already got a big pdf file and
> > want to use this as package vignette.
> >
> > So far I have manually created the inst/doc folder in the main package
> > skeleton, and put that file into this, which is not working by calling
> > "vignette(file_name)" after I build  and load the package. I am
> 
> file_name is not an argument to vignette(): it is 'topic'.  And topics 
> are normally file basenames (without any extension), not file names.
> 
> > getting following error without opening that pdf file: "vignette
> > 'file_name' *not* found"
> >
> > So I like to know, is there any way to use any arbitrary pdf file as 
> > vignette?
> 
> By definition, no.
> 
> >
> > Any suggestion is highly appreciated.

  One possibility: as a workaround, you could include your
own "xvignette" function in your package: see below.
It won't show you indices, but it will pick up any appropriately
named file that you include in the inst/doc directory of your
package ...

xvignette <- function(vname,pkg,ext="pdf") {
   vname <- paste(vname,ext,sep=".")
   fn <- system.file("doc",vname,package=pkg)
   if (nchar(fn)==0) stop("file not found")
   utils:::print.vignette(list(pdf=fn))
   invisible(fn)
 }

  You'll have to somehow alert your package users to the
fact that this alternative documentation exists -- perhaps in the help
package for the package itself.

  You might fill in the default value of "pkg" above with your
package name to make it easier on the user: I thought about
using some version of getPackageName(environment(xvignette))
to do it automatically, but that seems too complicated ...


From mjhubisz at gmail.com  Thu Jul 14 19:12:57 2011
From: mjhubisz at gmail.com (Melissa Jane Hubisz)
Date: Thu, 14 Jul 2011 13:12:57 -0400
Subject: [Rd] Creating package Vignette
In-Reply-To: <loom.20110714T183517-879@post.gmane.org>
References: <CA+A3Hr6-SNPmegaZMJzb=yA+1pSLvAGUAd0us=h01nq7xzoVxQ@mail.gmail.com>
	<alpine.LFD.2.02.1107141726570.3525@gannet.stats.ox.ac.uk>
	<loom.20110714T183517-879@post.gmane.org>
Message-ID: <CAC8fApJxDg=mVVSA1a+oJn-Ka9JVw6+Woe864310g5qSzn6vLg@mail.gmail.com>

Another workaround is to create a "dummy" vignette which does nothing
but include the pdf file.  Something like this:
vignette.Rnw:

% \VignetteIndexEntry{vignette}
% \VignetteKeywords{keywords here}
% \VignettePackage{package name}

\documentclass[a4paper]{article}
\usepackage{hyperref}
\usepackage{pdfpages}
\begin{document}
\includepdf[fitpaper=true,pages=-]{vignette-source.pdf}
\end{document}

Not sure if this is totally kosher, but I did this for my package when
the vignette was too computationally intensive to be submitted to
CRAN.
-Melissa

On Thu, Jul 14, 2011 at 1:00 PM, Ben Bolker <bbolker at gmail.com> wrote:
> Prof Brian Ripley <ripley <at> stats.ox.ac.uk> writes:
>
>>
>> It depends what you mean by 'vignette': the R docs have been unclear
>> (but R >= 2.13.0 are more consistent). ?In most cases a 'vignette' is
>> an Sweave document, the vignette source being the .Rnw file, and the
>> vignette PDF the processed .pdf file.
>>
>> At present vignette() means Sweave documents, as only they have
>> metadata like titles. ?This is planned to be changed soon.
>>
>> On Thu, 14 Jul 2011, Nipesh Bajaj wrote:
>>
>> > Hi all, I was trying to create some vignette files for my newly
>> > developed package, however wondering whether there could be any
>> > simpler way to do so. In writing R extension it is advised to go
>> > through Sweave route, however I have already got a big pdf file and
>> > want to use this as package vignette.
>> >
>> > So far I have manually created the inst/doc folder in the main package
>> > skeleton, and put that file into this, which is not working by calling
>> > "vignette(file_name)" after I build ?and load the package. I am
>>
>> file_name is not an argument to vignette(): it is 'topic'. ?And topics
>> are normally file basenames (without any extension), not file names.
>>
>> > getting following error without opening that pdf file: "vignette
>> > 'file_name' *not* found"
>> >
>> > So I like to know, is there any way to use any arbitrary pdf file as
>> > vignette?
>>
>> By definition, no.
>>
>> >
>> > Any suggestion is highly appreciated.
>
> ?One possibility: as a workaround, you could include your
> own "xvignette" function in your package: see below.
> It won't show you indices, but it will pick up any appropriately
> named file that you include in the inst/doc directory of your
> package ...
>
> xvignette <- function(vname,pkg,ext="pdf") {
> ? vname <- paste(vname,ext,sep=".")
> ? fn <- system.file("doc",vname,package=pkg)
> ? if (nchar(fn)==0) stop("file not found")
> ? utils:::print.vignette(list(pdf=fn))
> ? invisible(fn)
> ?}
>
> ?You'll have to somehow alert your package users to the
> fact that this alternative documentation exists -- perhaps in the help
> package for the package itself.
>
> ?You might fill in the default value of "pkg" above with your
> package name to make it easier on the user: I thought about
> using some version of getPackageName(environment(xvignette))
> to do it automatically, but that seems too complicated ...
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From ripley at stats.ox.ac.uk  Thu Jul 14 19:39:56 2011
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 14 Jul 2011 18:39:56 +0100
Subject: [Rd] Creating package Vignette
In-Reply-To: <CAC8fApJxDg=mVVSA1a+oJn-Ka9JVw6+Woe864310g5qSzn6vLg@mail.gmail.com>
References: <CA+A3Hr6-SNPmegaZMJzb=yA+1pSLvAGUAd0us=h01nq7xzoVxQ@mail.gmail.com>
	<alpine.LFD.2.02.1107141726570.3525@gannet.stats.ox.ac.uk>
	<loom.20110714T183517-879@post.gmane.org>
	<CAC8fApJxDg=mVVSA1a+oJn-Ka9JVw6+Woe864310g5qSzn6vLg@mail.gmail.com>
Message-ID: <alpine.LFD.2.02.1107141834530.5730@gannet.stats.ox.ac.uk>

On Thu, 14 Jul 2011, Melissa Jane Hubisz wrote:

> Another workaround is to create a "dummy" vignette which does nothing
> but include the pdf file.  Something like this:
> vignette.Rnw:
>
> % \VignetteIndexEntry{vignette}
> % \VignetteKeywords{keywords here}
> % \VignettePackage{package name}
>
> \documentclass[a4paper]{article}
> \usepackage{hyperref}
> \usepackage{pdfpages}
> \begin{document}
> \includepdf[fitpaper=true,pages=-]{vignette-source.pdf}
> \end{document}
>
> Not sure if this is totally kosher, but I did this for my package when
> the vignette was too computationally intensive to be submitted to
> CRAN.

That's fine (because a lot of not-very-necessary work has had to have 
been done to accept vignettes with no R code), but we are working on 
ways to avoid such subterfuges.

In particular, CRAN does accept packages with Sweave vignettes that 
take too long to check -- one takes ca 8 hours (or would if it worked, 
which it currently does not).  We just ask that we are told so on 
submission.

> -Melissa
>
> On Thu, Jul 14, 2011 at 1:00 PM, Ben Bolker <bbolker at gmail.com> wrote:
>> Prof Brian Ripley <ripley <at> stats.ox.ac.uk> writes:
>>
>>>
>>> It depends what you mean by 'vignette': the R docs have been unclear
>>> (but R >= 2.13.0 are more consistent). ?In most cases a 'vignette' is
>>> an Sweave document, the vignette source being the .Rnw file, and the
>>> vignette PDF the processed .pdf file.
>>>
>>> At present vignette() means Sweave documents, as only they have
>>> metadata like titles. ?This is planned to be changed soon.
>>>
>>> On Thu, 14 Jul 2011, Nipesh Bajaj wrote:
>>>
>>>> Hi all, I was trying to create some vignette files for my newly
>>>> developed package, however wondering whether there could be any
>>>> simpler way to do so. In writing R extension it is advised to go
>>>> through Sweave route, however I have already got a big pdf file and
>>>> want to use this as package vignette.
>>>>
>>>> So far I have manually created the inst/doc folder in the main package
>>>> skeleton, and put that file into this, which is not working by calling
>>>> "vignette(file_name)" after I build ?and load the package. I am
>>>
>>> file_name is not an argument to vignette(): it is 'topic'. ?And topics
>>> are normally file basenames (without any extension), not file names.
>>>
>>>> getting following error without opening that pdf file: "vignette
>>>> 'file_name' *not* found"
>>>>
>>>> So I like to know, is there any way to use any arbitrary pdf file as
>>>> vignette?
>>>
>>> By definition, no.
>>>
>>>>
>>>> Any suggestion is highly appreciated.
>>
>> ?One possibility: as a workaround, you could include your
>> own "xvignette" function in your package: see below.
>> It won't show you indices, but it will pick up any appropriately
>> named file that you include in the inst/doc directory of your
>> package ...
>>
>> xvignette <- function(vname,pkg,ext="pdf") {
>> ? vname <- paste(vname,ext,sep=".")
>> ? fn <- system.file("doc",vname,package=pkg)
>> ? if (nchar(fn)==0) stop("file not found")
>> ? utils:::print.vignette(list(pdf=fn))
>> ? invisible(fn)
>> ?}
>>
>> ?You'll have to somehow alert your package users to the
>> fact that this alternative documentation exists -- perhaps in the help
>> package for the package itself.
>>
>> ?You might fill in the default value of "pkg" above with your
>> package name to make it easier on the user: I thought about
>> using some version of getPackageName(environment(xvignette))
>> to do it automatically, but that seems too complicated ...
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From moshersteven at gmail.com  Thu Jul 14 21:35:17 2011
From: moshersteven at gmail.com (steven mosher)
Date: Thu, 14 Jul 2011 12:35:17 -0700
Subject: [Rd] Creating package Vignette
In-Reply-To: <CAC8fApJxDg=mVVSA1a+oJn-Ka9JVw6+Woe864310g5qSzn6vLg@mail.gmail.com>
References: <CA+A3Hr6-SNPmegaZMJzb=yA+1pSLvAGUAd0us=h01nq7xzoVxQ@mail.gmail.com>
	<alpine.LFD.2.02.1107141726570.3525@gannet.stats.ox.ac.uk>
	<loom.20110714T183517-879@post.gmane.org>
	<CAC8fApJxDg=mVVSA1a+oJn-Ka9JVw6+Woe864310g5qSzn6vLg@mail.gmail.com>
Message-ID: <CAFFLneTwHtoX3=F-TVk0spcnuJJZkU7HG7qSRrd8u2tfpUwJgg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20110714/41c04eaa/attachment.pl>

From tjsm at pml.ac.uk  Fri Jul 15 10:11:22 2011
From: tjsm at pml.ac.uk (Tim Smyth)
Date: Fri, 15 Jul 2011 01:11:22 -0700 (PDT)
Subject: [Rd] fuzzy c-means algorithm in e1071
Message-ID: <1310717482041-3669374.post@n4.nabble.com>

I was wondering if it was possible to add an extra option to the cmeans
function please?  In a recent paper by Moore et al.,(2009) they state:

"The fuzzy membership function is: 1 - F(Z2), where Z2 is a squared
Mahalanobis distance and F is a cumulative chi-square distribution.  The
Mahalanobis distance is the multivariate equivalent of the standardised
random variable Z = (X - M)/S, which is the distance of the univariate
random variable X from its mean M normalised by the standard deviation S. 
In other words, the Mahalanobis distance is a weighted form of the
Euclidean, and is preferable because it incorporates the shape of the the
distribution of points around the cluster centre (i.e., the geometric shape
of the point cloud expressed in terms of variance). The fuzzy membership
ranges from 0 to 1 ..."

so as well as having a dist = "euclidean" option, would it be possible to
have a dist = "mahalanobis" option please?  I realise that this is quite an
ask - you could even point me in the right direction of where to start
editing code and getting it compiled on my own machine.

Tim Smyth

ref: T.S. Moore et al. Remote Sensing of Environment 113 (2009) 2424-2430 

--
View this message in context: http://r.789695.n4.nabble.com/fuzzy-c-means-algorithm-in-e1071-tp3669374p3669374.html
Sent from the R devel mailing list archive at Nabble.com.


From rflight79 at gmail.com  Fri Jul 15 14:52:35 2011
From: rflight79 at gmail.com (Robert M. Flight)
Date: Fri, 15 Jul 2011 08:52:35 -0400
Subject: [Rd] Suggestions for R-devel / R-help digest format
In-Reply-To: <4E160850.1040404@gmail.com>
References: <4E15E264.2090004@gmail.com> <1310058172.17786.13.camel@brian-rcg>
	<4E160850.1040404@gmail.com>
Message-ID: <CAJLyBTWv5qOR3UwJQfk+v=HW8wvWiL65ksE2J5EeeK2AoGa3QA@mail.gmail.com>

Hi Saravanan,

You don't need a separate GMail account, as others suggested. I use
GMail as my client, and I have FILTERS set up to catch any R-Help, etc
(really anything that comes from a specific address), archive the
message so I don't see it in my inbox (unless someone is replying
directly to me in addition to sending to R-help), and then I can view
the emails at my leisure.

HTH,

-Robert

Robert M. Flight, Ph.D.
University of Louisville Bioinformatics Laboratory
University of Louisville
Louisville, KY

PH 502-852-1809 (HSC)
PH 502-852-0467 (Belknap)
EM robert.flight at louisville.edu
EM rflight79 at gmail.com

Williams and Holland's Law:
? ? ?? If enough data is collected, anything may be proven by
statistical methods.



On Thu, Jul 7, 2011 at 15:26, Saravanan
<saravanan.thirumuruganathan at gmail.com> wrote:
> Thanks Steve and Brian !
>
> Probably, I will create a gmail account for mailing lists and let it take
> care of the threading.
>
> Regards,
> Saravanan
>
> On 07/07/2011 12:02 PM, Brian G. Peterson wrote:
>>
>> On Thu, 2011-07-07 at 11:44 -0500, Saravanan wrote:
>>>
>>> Hello,
>>>
>>> I am passive reader of both R-devel and R-help mailing lists. I am
>>> sending the following comments to r-devel as it seemed more suitable. I
>>> am aware that this list uses GNU mailman for the list management. I have
>>> my options set that it sends a email digest. One thing I find is that
>>> the digest consists of emails that ordered temporarlly. For eg lets say
>>> there are two threads t1 and t2 and the emails arrive as e1 of t1, e2 of
>>> t2, e3 of t3 ?. The digest lists them as e1,e2 and then e3. Is it
>>> possible to somehow configure it as T1 : e1,e3 and then T2 : e2 ?
>>>
>>> This is the digest format that google groups uses which is incredibly
>>> helpful as you can read all the messages in a thread. Additionally, it
>>> also helpfully includes a header that lists all the threads in digest so
>>> that you can jump to the one you are interested in. I checked the
>>> mailman options but could not find any.
>>>
>>> Does anyone else have the same issue? It is not a big issue in R-devel
>>> but R-help is a much more high traffic mailing list. I am interested in
>>> hearing how you read/filter your digest mails in either R-help or other
>>> high volume mailing lists.
>>
>> This really has nothing to do with R, but rather mailman.
>>
>> I use folders, filtered on the server using SIEVE and/or procmail. ?No
>> digest required. I get the mails immediately, not later in the day or
>> the next day, ?and can use all my various email clients easily to
>> read/respond.
>>
>> mailman supports a MIME digest format that includes a table of contents
>> with links to each MIME part. ?mailman does not support a threaded
>> digest, to the best of my knowledge.
>>
>> Regards,
>>
>> ? ?- Brian
>>
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From ligges at statistik.tu-dortmund.de  Fri Jul 15 15:10:21 2011
From: ligges at statistik.tu-dortmund.de (Uwe Ligges)
Date: Fri, 15 Jul 2011 15:10:21 +0200
Subject: [Rd] fuzzy c-means algorithm in e1071
In-Reply-To: <1310717482041-3669374.post@n4.nabble.com>
References: <1310717482041-3669374.post@n4.nabble.com>
Message-ID: <4E203C3D.10406@statistik.tu-dortmund.de>

Since you are talking about a contributed package: Please send feature 
requests to the package maintainer:

 > maintainer("e1071")
[1] "Friedrich Leisch <Friedrich.Leisch at R-project.org>"


It is even a better idea to send him your own code suggestions so that 
it will be quicker to add the requested features to the package.

Best,
Uwe Ligges




On 15.07.2011 10:11, Tim Smyth wrote:
> I was wondering if it was possible to add an extra option to the cmeans
> function please?  In a recent paper by Moore et al.,(2009) they state:
>
> "The fuzzy membership function is: 1 - F(Z2), where Z2 is a squared
> Mahalanobis distance and F is a cumulative chi-square distribution.  The
> Mahalanobis distance is the multivariate equivalent of the standardised
> random variable Z = (X - M)/S, which is the distance of the univariate
> random variable X from its mean M normalised by the standard deviation S.
> In other words, the Mahalanobis distance is a weighted form of the
> Euclidean, and is preferable because it incorporates the shape of the the
> distribution of points around the cluster centre (i.e., the geometric shape
> of the point cloud expressed in terms of variance). The fuzzy membership
> ranges from 0 to 1 ..."
>
> so as well as having a dist = "euclidean" option, would it be possible to
> have a dist = "mahalanobis" option please?  I realise that this is quite an
> ask - you could even point me in the right direction of where to start
> editing code and getting it compiled on my own machine.
>
> Tim Smyth
>
> ref: T.S. Moore et al. Remote Sensing of Environment 113 (2009) 2424-2430
>
> --
> View this message in context: http://r.789695.n4.nabble.com/fuzzy-c-means-algorithm-in-e1071-tp3669374p3669374.html
> Sent from the R devel mailing list archive at Nabble.com.
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From rpeng at jhsph.edu  Fri Jul 15 20:43:15 2011
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Fri, 15 Jul 2011 14:43:15 -0400
Subject: [Rd] NULL names for POSIXlt objects
Message-ID: <CAE_NaA2-uOzkm8neqACHgMueeWm-Vt4FKJhRq88uf-4NqydfLg@mail.gmail.com>

I noticed in current R-patched (and R-devel) the following behavior:

> names(as.POSIXlt("2003-01-01"))
NULL

which I believe previously listed the names of the different elements
(e.g. 'sec', 'mday', 'year', etc.).

It seems to be related to r54188. I see the code here is is wrapped
with <FIXME> but I'm not sure it was meant to change the previous
behavior.

-roger

-- 
Roger D. Peng? |? http://www.biostat.jhsph.edu/~rpeng/


From therneau at mayo.edu  Fri Jul 15 23:23:47 2011
From: therneau at mayo.edu (Terry Therneau)
Date: Fri, 15 Jul 2011 16:23:47 -0500
Subject: [Rd] Confusing inheritance problem
Message-ID: <1310765027.11629.48.camel@nemo>

 I have library in development with a function that works when called
from the top level, but fails under R CMD check.  The paricular line of
failure is
	rsum <- rowSums(kmat>0)
where kmat is a dsCMatrix object.

  I'm currently stumped and looking for some ideas.

  I've created a stripped down library "ktest" that has only 3
functions: pedigree.R to create a pedigree or pedigreeList object, 
	   kinship.R with "kinship" methods for the two objects 
	   one small compute function called by the others
along with the minimal amount of other information such that a call to
   R --vanilla CMD check ktest
gives no errors until the fatal one.

 There are two test cases.  A 3 line one that creates a dsCMatrix and
call rowSums at the top level works fine, but the same call inside the
kmat.pedigreeList function gives an error
        'x' must be an array of at least two dimensions
Adding a print statement above the rowSums call shows that the argument
is a 14 by 14 dsCMatrix.

 I'm happy to send the library to anyone else to try and duplicate.
    Terry Therneau

tmt% R --vanilla

> sessionInfo()
R version 2.13.0 (2011-04-13)
Platform: x86_64-unknown-linux-gnu (64-bit)

locale:
 [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              
 [3] LC_TIME=en_US.UTF-8        LC_COLLATE=C              
 [5] LC_MONETARY=C              LC_MESSAGES=en_US.UTF-8   
 [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 
 [9] LC_ADDRESS=C               LC_TELEPHONE=C            
[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods
base


From david at revolutionanalytics.com  Fri Jul 15 23:53:49 2011
From: david at revolutionanalytics.com (David Smith)
Date: Fri, 15 Jul 2011 14:53:49 -0700
Subject: [Rd] Suggestions for R-devel / R-help digest format
In-Reply-To: <CAJLyBTWv5qOR3UwJQfk+v=HW8wvWiL65ksE2J5EeeK2AoGa3QA@mail.gmail.com>
References: <4E15E264.2090004@gmail.com> <1310058172.17786.13.camel@brian-rcg>
	<4E160850.1040404@gmail.com>
	<CAJLyBTWv5qOR3UwJQfk+v=HW8wvWiL65ksE2J5EeeK2AoGa3QA@mail.gmail.com>
Message-ID: <CABgvEC_=yNjhGDiJPE4mKAoAi7fox7gdcvGMhvXiGGxc71+Jkw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20110715/1a53035b/attachment.pl>

From saravanan.thirumuruganathan at gmail.com  Sat Jul 16 00:04:52 2011
From: saravanan.thirumuruganathan at gmail.com (Saravanan)
Date: Fri, 15 Jul 2011 17:04:52 -0500
Subject: [Rd] Suggestions for R-devel / R-help digest format
In-Reply-To: <CABgvEC_=yNjhGDiJPE4mKAoAi7fox7gdcvGMhvXiGGxc71+Jkw@mail.gmail.com>
References: <4E15E264.2090004@gmail.com> <1310058172.17786.13.camel@brian-rcg>
	<4E160850.1040404@gmail.com>
	<CAJLyBTWv5qOR3UwJQfk+v=HW8wvWiL65ksE2J5EeeK2AoGa3QA@mail.gmail.com>
	<CABgvEC_=yNjhGDiJPE4mKAoAi7fox7gdcvGMhvXiGGxc71+Jkw@mail.gmail.com>
Message-ID: <4E20B984.8090306@gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20110715/8d0ee89d/attachment.pl>

From hpages at fhcrc.org  Sat Jul 16 01:53:09 2011
From: hpages at fhcrc.org (=?windows-1252?Q?Herv=E9_Pag=E8s?=)
Date: Fri, 15 Jul 2011 16:53:09 -0700
Subject: [Rd] R CMD check --force-multiarch does not install all the
 archs for testing
In-Reply-To: <4E0B65F7.7080701@statistik.tu-dortmund.de>
References: <4E0912EE.7020105@fhcrc.org>
	<4E099478.4030609@statistik.tu-dortmund.de>
	<4E0A2507.1010904@fhcrc.org>
	<CA592CAB-B8DB-49C6-9AFD-9DB2C2087806@r-project.org>
	<4E0A2F44.3010109@fhcrc.org>
	<EBE00C6D-476A-4DA5-9CB8-A0F320D2B042@r-project.org>
	<4E0A4381.6020506@fhcrc.org>
	<4E0B65F7.7080701@statistik.tu-dortmund.de>
Message-ID: <4E20D2E5.7050109@fhcrc.org>

Hi Uwe,

On 11-06-29 10:50 AM, Uwe Ligges wrote:
>
>
> On 28.06.2011 23:11, Herv? Pag?s wrote:
>> Simon,
>>
>> On 11-06-28 01:44 PM, Simon Urbanek wrote:
>>>
>>> On Jun 28, 2011, at 3:45 PM, Herv? Pag?s wrote:
>>>
>>>> Hi Simon,
>>>>
>>>> On 11-06-28 12:19 PM, Simon Urbanek wrote:
>>>>>
>>>>> On Jun 28, 2011, at 3:01 PM, Herv? Pag?s wrote:
>>>>>
>>>>>> Hi Uwe,
>>>>>>
>>>>>> On 11-06-28 01:44 AM, Uwe Ligges wrote:
>>>>>>>
>>>>>>>
>>>>>>> On 28.06.2011 01:31, Herv? Pag?s wrote:
>>>>>>>> Hi,
>>>>>>>>
>>>>>>>> Why isn't 'R CMD check --force-multiarch' installing the package
>>>>>>>> for all the architectures that are going to be checked?
>>>>>>>
>>>>>>> Herv?,
>>>>>>>
>>>>>>> no, since it cannot know that that you need
>>>>>>>
>>>>>>> --merge-multiarch
>>>>>>>
>>>>>>> as an additional install flag for this particular package.
>>>>>>
>>>>>> Why not just use this flag anyway? Does it hurt to use it on
>>>>>> packages that don't strictly need it?
>>>>>>
>>>>>
>>>>> It does for two reasons: a) everything is built twice
>>>>
>>>> That's exactly what I want when I do 'R CMD check --force-multiarch'
>>>>
>>>
>>> No, that's not what it does (and I assume you mean --force-biarch). It
>>> builds the package just once, you're simply overriding the default
>>> behavior of checking for configure.win, that's all. The two flags are
>>> orthogonal, --force-biarch makes no sense with --merge-multiarch, they
>>> are for all practical purposes mutually exclusive by definition of
>>> what they do.
>>
>> I really mean --force-multiarch, not --force-biarch. AFAIK 'R CMD check'
>> has no --force-biarch option.
>>
>>>
>>>
>>>>> and b) package authors don't expect the necessity to support
>>>>> --libs-only if the package doesn't require separate build runs.
>>>>
>>>> When specifying --force-multiarch, the user really expects the
>>>> package to be installed for all sub-archs.
>>>>
>>>
>>> ... with the assumption that the package supports it even though it
>>> has a configure script which may to may not work unlike
>>> --merge-multiarch which will always work.
>>
>> Just to clarify:
>>
>> --force-multiarch is an 'R CMD check' option, not an 'R CMD INSTALL'
>> option
>>
>> --merge-multiarch is an 'R CMD INSTALL' option, not an 'R CMD check'
>> option
>>
>> Now you are saying that --merge-multiarch will always work (on Windows
>> of course, all this discussion is about how to achieve multiarch check
>> on Windows). Great, this is what I've been observing too so far!
>
>
> It does not "always work", unfortunately. We need force-biarch on CRAN for:
>
> RGtk2
> rgdal
> rphast
> SQLiteMap
>
>
>
>> On packages with or without native codes, with or without configure.win
>> scripts, etc... It always seems to work. So, again, why isn't
>> 'R CMD check --force-multiarch' installing with --merge-multiarch?
>> Note that I'm not attached to that solution in particular, just trying
>> to suggest an easy fix for 'R CMD check --force-multiarch' (which right
>> now is broken on some packages).
>
> Well, merge-multiarch will result in small overhead. It will try to
> compile the code in ./src for the other architecture and merge that into
> the same package. For packages without any compiled code, it does not
> make sense to ask R to do that since the packages are identical under
> 32-bit and 64-bit, hence you don't want the overhead.

If the package doesn't contain any native code, then trying to compile
the native code for the other arch shouldn't result in a big overhead.
What's the overhead of an if statement like:

   ## add DLL for x64
   if (file.exists("installation_folder/pkgname/libs") &&
       !file.exists("installation_folder/pkgname/libs/x64")) {
       ...
       ## compile and install DLL for x64
       ...
   }
   ## test if installed package can be loaded for x64
   ...

Note that the "add DLL for x64" step is currently doing something
*very* useful, which is to make sure that the package can be loaded for
this arch:

D:\biocbld\bbs-2.9-bioc\meat>..\R\bin\R CMD INSTALL --merge-multiarch 
BSgenome_1.21.3.tar.gz

install for i386

* installing to library 'D:/biocbld/bbs-2.9-bioc/R/library'
* installing *source* package 'BSgenome' ...
** R
** inst
** preparing package for lazy loading

Attaching package: 'IRanges'

The following object(s) are masked from 'package:base':

     Map, cbind, eval, intersect, mapply, order, paste, pmax, pmax.int,
     pmin, pmin.int, rbind, rep.int, setdiff, table, union

** help
*** installing help indices
** building package indices ...
*** tangling vignette sources ...
    'BSgenomeForge.Rnw'
    'GenomeSearching.Rnw'
** testing if installed package can be loaded

add DLL for x64

* installing to library 'D:/biocbld/bbs-2.9-bioc/R/library'
* installing *source* package 'BSgenome' ...
** testing if installed package can be loaded

* DONE (BSgenome)

It's useful even for packages without compiled code since they can
depend on packages that do have compiled code but have been installed
only for 1 arch.

Unfortunately this is something that --force-biarch doesn't do
(it returns success even if compilation for x64 failed).

> For packages that
> include compiled code, you still do not want it, since R does it
> automatically - unless there is a configure.win file.

I want to be able to use the same command for any package.
'R CMD check' looses *a lot* of value if people need to know what's
in a package in order to choose the right command for doing a
multiarch check. I'm not talking about special configuration options
that some packages with compiled code need. I'm talking about the
situation where everything is setup properly so the 2 separate
checks work:

   R --arch i386 CMD check pkg_x.y.z.tar.gz
   R --arch x64 CMD check pkg_x.y.z.tar.gz

but 'R CMD check --force-multi-arch pkg_x.y.z.tar.gz' doesn't because
it fails to install for x64.

So if installing with --merge-multiarch is "almost" the right thing
to do (even though it works for only 100% of the BioC packages and
99.8% of the CRAN packages -- note that this percentage would drop
without using --merge-multiarch), then I do want to use it because
this is the closest I can get to *the* command that works for any
package.

Another reason I want to use it for any package that includes
compiled code, even for packages with a configure.win file, is
because "R automatic bi-arch installation" feature will report
success even if compilation for x64 failed.

>
> Therefore, the option is just an add on that is necessary for only 18
> out of roughly 3000 CRAN packages.
>
> So we have 4 packages that require --force-biarch and 18 requiring
> --merge-multiarch on CRAN. All the others build automatically for both
> architectures if they need to. So no reason to do that all the time.

For our BioC builds, we are lucky that --merge-multiarch works for all
our software packages. Unfortunately we cannot just use

   R CMD check --install-args="--merge-multiarch"

because 'R CMD check' calls 'R CMD INSTALL --merge-multiarch' on the
*extracted* tarball but --merge-multiarch only works on a *source*
tarball. So we use this long and ugly command (that is basically the
2-step command that you provided earlier in this thread, thanks!):

 
http://bioconductor.org/checkResults/2.9/bioc-LATEST/Biobase/moscato1-checksrc.html

Anyway I still think anybody should be able to run a multiarch check.
Not only the CRAN, or R-forge, or BioC build systems. It should be
easy (1 step command). It should not need any "add-on".

Thanks,
H.

>
> Best wishes,
> Uwe
>
>
>
>>> --force-biarch is just a way to flag packages that have configure.win
>>> that has no effect on the binary settings (flags etc.). It forces R to
>>> try multi-arch build in one flight, but it may or may not work
>>> depending on the package. It is a way to save time by not running
>>> --merge-multiarch (and thus building the package twice).
>>
>> --force-biarch is an 'R CMD INSTALL' option that I don't use. Why would
>> I use something that might fail when I can use --merge-multiarch which
>> always works.
>  >
>> Thanks,
>> H.
>>
>>>
>>> Cheers,
>>> Simon
>>>
>>>
>>>>>
>>>>> The cross-platform way is to not use --merge-multiarch but use
>>>>> --libs-only instead as needed (easy to check after the first arch
>>>>> run which will tell you whether it's needed or not). I suspect that
>>>>> --merge-multiarch is just a convenience shortcut (and it's unclear
>>>>> to me why it's Windows-only...).
>>>>
>>>> A great convenience indeed as it allows to do the multiarch install in
>>>> a single step. And it's unclear to me too why it's Windows-only but I
>>>> would have hoped you would know...
>>>>
>>>> Thanks,
>>>> H.
>>>>
>>>>
>>>>>
>>>>> Cheers,
>>>>> Simon
>>>>>
>>>>>
>>>>>>> You will have to check it in repository maintainer's mode (as the
>>>>>>> CRAN
>>>>>>> maintainers do everywhere). Essentially this is for me (when also
>>>>>>> producing WIndows binaries):
>>>>>>>
>>>>>>>
>>>>>>> Step 1: Installation
>>>>>>>
>>>>>>> R CMD INSTALL --pkglock --compact-docs --build --merge-multiarch
>>>>>>> --library="D:/path/to/library" fabia_1.5.0.tar.gz>
>>>>>>> fabia-install.out 2>&1
>>>>>>>
>>>>>>> (where the merge-multiarch part applies only to this package, of
>>>>>>> course)
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> Step 2: Check (without installation, since that happened before
>>>>>>> already,
>>>>>>> using the install log from step 1)
>>>>>>>
>>>>>>> R CMD check --library="D:/path/to/library" --force-multiarch
>>>>>>> --install="check:fabia-install.out" fabia
>>>>>>
>>>>>> Whaoooo! Would be nice if there was a plan to make 'R CMD check' also
>>>>>> usable by normal people (including the package developer), not just
>>>>>> by a few privileged people that know about those undocumented tricks.
>>>>>>
>>>>>> Thanks,
>>>>>> H.
>>>>>>
>>>>>>>
>>>>>>> Best wishes,
>>>>>>> Uwe
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>> For some packages, it only installs for the default arch ('i386').
>>>>>>>> Then testing the package for 'x64' fails.
>>>>>>>>
>>>>>>>> For example,
>>>>>>>>
>>>>>>>> Output of R CMD check --force-multiarch fabia_1.5.0.tar.gz:
>>>>>>>> -----------------------------------------------------------
>>>>>>>> * using log directory 'D:/biocbld/bbs-2.9-bioc/meat/fabia.Rcheck'
>>>>>>>> * using R version 2.14.0 Under development (unstable) (2011-05-30
>>>>>>>> r56020)
>>>>>>>> * using platform: i386-pc-mingw32 (32-bit)
>>>>>>>> * using session charset: ISO8859-1
>>>>>>>> * using option '--no-vignettes'
>>>>>>>> * checking for file 'fabia/DESCRIPTION' ... OK
>>>>>>>> * this is package 'fabia' version '1.5.0'
>>>>>>>> * checking package name space information ... OK
>>>>>>>> * checking package dependencies ... OK
>>>>>>>> * checking if this is a source package ... OK
>>>>>>>> * checking whether package 'fabia' can be installed ... OK
>>>>>>>> * checking installed package size ... OK
>>>>>>>> * checking package directory ... OK
>>>>>>>> * checking for portable file names ... OK
>>>>>>>> * checking DESCRIPTION meta-information ... OK
>>>>>>>> * checking top-level files ... OK
>>>>>>>> * checking index information ... OK
>>>>>>>> * checking package subdirectories ... OK
>>>>>>>> * checking R files for non-ASCII characters ... OK
>>>>>>>> * checking R files for syntax errors ... OK
>>>>>>>> * loading checks for arch 'i386'
>>>>>>>> ** checking whether the package can be loaded ... OK
>>>>>>>> ** checking whether the package can be loaded with stated
>>>>>>>> dependencies
>>>>>>>> ... OK
>>>>>>>> ** checking whether the package can be unloaded cleanly ... OK
>>>>>>>> ** checking whether the name space can be loaded with stated
>>>>>>>> dependencies ... OK
>>>>>>>> ** checking whether the name space can be unloaded cleanly ... OK
>>>>>>>> * loading checks for arch 'x64'
>>>>>>>> ** checking whether the package can be loaded ...Warning: running
>>>>>>>> command '"D:/biocbld/bbs-2.9-bioc/R/bin/x64/Rterm.exe"
>>>>>>>> R_ENVIRON_USER='no_such_file' --no-site-file --no-init-file
>>>>>>>> --no-save
>>>>>>>> --no-restore --slave -f
>>>>>>>> D:\biocbld\bbs-2.9-bioc\tmpdir\RtmpO65p5H\Rin57456988' had status 1
>>>>>>>> ERROR
>>>>>>>> Error: package 'fabia' is not installed for 'arch=x64'
>>>>>>>> Execution halted
>>>>>>>>
>>>>>>>> It looks like this package has a loading problem: see the
>>>>>>>> messages for
>>>>>>>> details.
>>>>>>>>
>>>>>>>> Content of fabia.Rcheck\00install.out:
>>>>>>>> --------------------------------------
>>>>>>>>
>>>>>>>> * installing *source* package 'fabia' ...
>>>>>>>> Building libRcpp.a in RcppSrc...
>>>>>>>> rm -f Rcpp.o libRcpp.a
>>>>>>>> g++ -c Rcpp.cpp -o Rcpp.o -I"D:/biocbld/BBS-2?1.9-B/R/include"
>>>>>>>> -I"D:/biocbld/BBS-2?1.9-B/R/src/include" -Wall -O2
>>>>>>>> ar r libRcpp.a Rcpp.o
>>>>>>>> C:\Rtools213\MinGW\bin\ar.exe: creating libRcpp.a
>>>>>>>> ranlib libRcpp.a
>>>>>>>> rm -f Rcpp.o
>>>>>>>> rm -f Rcpp.o
>>>>>>>> ** libs
>>>>>>>> running src/Makefile.win ...
>>>>>>>> rm -f fabia.o fabia.dll *.a *.o *.so *.dll
>>>>>>>> g++ -c fabiac.cpp -o fabia.o -I../RcppSrc
>>>>>>>> -I"D:/biocbld/BBS-2?1.9-B/R/include" -Wall -O2
>>>>>>>> g++ -shared -s -static-libgcc fabia.o -L../RcppSrc -lRcpp
>>>>>>>> -L"D:/biocbld/BBS-2?1.9-B/R/bin/i386" -lR -o fabia.dll
>>>>>>>> rm -f fabia.o *.a *.o *.so
>>>>>>>> installing to
>>>>>>>> D:/biocbld/bbs-2.9-bioc/meat/fabia.Rcheck/fabia/libs/i386
>>>>>>>> ** R
>>>>>>>> ** demo
>>>>>>>> ** inst
>>>>>>>> ** preparing package for lazy loading
>>>>>>>> Creating a generic function for "plot" from package "graphics" in
>>>>>>>> package "fabia"
>>>>>>>> ** help
>>>>>>>> *** installing help indices
>>>>>>>> ** building package indices ...
>>>>>>>> *** tangling vignette sources ...
>>>>>>>> 'fabia.Rnw'
>>>>>>>> ** testing if installed package can be loaded
>>>>>>>>
>>>>>>>> * DONE (fabia)
>>>>>>>>
>>>>>>>> The source tarball for this package is available here:
>>>>>>>> http://bioconductor.org/packages/2.9/bioc/html/fabia.html
>>>>>>>>
>>>>>>>> What command should be used to perform a multiarch check of this
>>>>>>>> package?
>>>>>>>>
>>>>>>>> This is on a 64-bit Windows Server 2008 R2 Enterprise machine
>>>>>>>> using a
>>>>>>>> recent combined Windows 32/64 bit binary of R-devel from CRAN.
>>>>>>>>
>>>>>>>> Thanks!
>>>>>>>> H.
>>>>>>>>
>>>>>>
>>>>>>
>>>>>> --
>>>>>> Herv? Pag?s
>>>>>>
>>>>>> Program in Computational Biology
>>>>>> Division of Public Health Sciences
>>>>>> Fred Hutchinson Cancer Research Center
>>>>>> 1100 Fairview Ave. N, M1-B514
>>>>>> P.O. Box 19024
>>>>>> Seattle, WA 98109-1024
>>>>>>
>>>>>> E-mail: hpages at fhcrc.org
>>>>>> Phone: (206) 667-5791
>>>>>> Fax: (206) 667-1319
>>>>>>
>>>>>> ______________________________________________
>>>>>> R-devel at r-project.org mailing list
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>>>
>>>>>>
>>>>>
>>>>
>>>>
>>>> --
>>>> Herv? Pag?s
>>>>
>>>> Program in Computational Biology
>>>> Division of Public Health Sciences
>>>> Fred Hutchinson Cancer Research Center
>>>> 1100 Fairview Ave. N, M1-B514
>>>> P.O. Box 19024
>>>> Seattle, WA 98109-1024
>>>>
>>>> E-mail: hpages at fhcrc.org
>>>> Phone: (206) 667-5791
>>>> Fax: (206) 667-1319
>>>>
>>>>
>>>
>>
>>


-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fhcrc.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From ligges at statistik.tu-dortmund.de  Sat Jul 16 19:27:38 2011
From: ligges at statistik.tu-dortmund.de (Uwe Ligges)
Date: Sat, 16 Jul 2011 19:27:38 +0200
Subject: [Rd] Confusing inheritance problem
In-Reply-To: <1310765027.11629.48.camel@nemo>
References: <1310765027.11629.48.camel@nemo>
Message-ID: <4E21CA0A.1050107@statistik.tu-dortmund.de>



On 15.07.2011 23:23, Terry Therneau wrote:
>   I have library in development with a function that works when called
> from the top level, but fails under R CMD check.  The paricular line of
> failure is
> 	rsum<- rowSums(kmat>0)
> where kmat is a dsCMatrix object.
>
>    I'm currently stumped and looking for some ideas.
>
>    I've created a stripped down library "ktest" that has only 3
> functions: pedigree.R to create a pedigree or pedigreeList object,
> 	   kinship.R with "kinship" methods for the two objects
> 	   one small compute function called by the others
> along with the minimal amount of other information such that a call to
>     R --vanilla CMD check ktest
> gives no errors until the fatal one.
>
>   There are two test cases.  A 3 line one that creates a dsCMatrix and
> call rowSums at the top level works fine, but the same call inside the
> kmat.pedigreeList function gives an error
>          'x' must be an array of at least two dimensions
> Adding a print statement above the rowSums call shows that the argument
> is a 14 by 14 dsCMatrix.
>
>   I'm happy to send the library to anyone else to try and duplicate.
>      Terry Therneau
>
> tmt% R --vanilla
>
>> sessionInfo()
> R version 2.13.0 (2011-04-13)
> Platform: x86_64-unknown-linux-gnu (64-bit)
>
> locale:
>   [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
>   [3] LC_TIME=en_US.UTF-8        LC_COLLATE=C
>   [5] LC_MONETARY=C              LC_MESSAGES=en_US.UTF-8
>   [7] LC_PAPER=en_US.UTF-8       LC_NAME=C
>   [9] LC_ADDRESS=C               LC_TELEPHONE=C
> [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods
> base



Terry,

1. Your R is not recent.
2. You do this without having Matrix loaded (according to 
sessionInfo())? This may already be the cause of your problems.
3. You may want to make your package available on some website. I am 
sure there are people who will take a look (including me, but not today).

Best wishes,
Uwe









> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From ggrothendieck at gmail.com  Sun Jul 17 04:13:46 2011
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sat, 16 Jul 2011 22:13:46 -0400
Subject: [Rd] NAMESPACE
Message-ID: <CAP01uRnFyMXiEWHCLN=Cp9gdHHRw53hTAjucnb=r0c+kU62GhA@mail.gmail.com>

> Packages without explicit ?NAMESPACE? files will have a default one created at build or INSTALL time,
> so all packages will have namespaces. A consequence of this is that ?.First.lib? functions need to be
> renamed, usually as ?.onLoad? but sometimes as ?.onAttach?.

Couldn't R simply regard .First.lib as the appropriate function to
save many packages from
being needlessly changed?

-- 
Statistics & Software Consulting
GKX Group, GKX Associates Inc.
tel: 1-877-GKX-GROUP
email: ggrothendieck at gmail.com


From murdoch.duncan at gmail.com  Sun Jul 17 13:58:21 2011
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Sun, 17 Jul 2011 07:58:21 -0400
Subject: [Rd] NAMESPACE
In-Reply-To: <CAP01uRnFyMXiEWHCLN=Cp9gdHHRw53hTAjucnb=r0c+kU62GhA@mail.gmail.com>
References: <CAP01uRnFyMXiEWHCLN=Cp9gdHHRw53hTAjucnb=r0c+kU62GhA@mail.gmail.com>
Message-ID: <4E22CE5D.4070404@gmail.com>

On 11-07-16 10:13 PM, Gabor Grothendieck wrote:
>> Packages without explicit ?NAMESPACE? files will have a default one created at build or INSTALL time,
>> so all packages will have namespaces. A consequence of this is that ?.First.lib? functions need to be
>> renamed, usually as ?.onLoad? but sometimes as ?.onAttach?.
>
> Couldn't R simply regard .First.lib as the appropriate function to
> save many packages from
> being needlessly changed?
>

It is doing that (see today's version of the NEWS), but the problem is 
that it may choose the wrong one.  If your package requires 
initialization then yourpkg::somefun only works if the initialization 
happens in .onLoad.  But some packages only work if the initialization 
happens when the package is attached.  It's better for the author to 
make the choice than for R to do it automatically.

Duncan Murdoch


From ggrothendieck at gmail.com  Sun Jul 17 14:06:12 2011
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Sun, 17 Jul 2011 08:06:12 -0400
Subject: [Rd] NAMESPACE
In-Reply-To: <4E22CE5D.4070404@gmail.com>
References: <CAP01uRnFyMXiEWHCLN=Cp9gdHHRw53hTAjucnb=r0c+kU62GhA@mail.gmail.com>
	<4E22CE5D.4070404@gmail.com>
Message-ID: <CAP01uRktXAJZ4hL1TPYrekmTmCuO4Zu=RKs+pXU_bcnvJvZ8=Q@mail.gmail.com>

On Sun, Jul 17, 2011 at 7:58 AM, Duncan Murdoch
<murdoch.duncan at gmail.com> wrote:
> On 11-07-16 10:13 PM, Gabor Grothendieck wrote:
>>>
>>> Packages without explicit ?NAMESPACE? files will have a default one
>>> created at build or INSTALL time,
>>> so all packages will have namespaces. A consequence of this is that
>>> ?.First.lib? functions need to be
>>> renamed, usually as ?.onLoad? but sometimes as ?.onAttach?.
>>
>> Couldn't R simply regard .First.lib as the appropriate function to
>> save many packages from
>> being needlessly changed?
>>
>
> It is doing that (see today's version of the NEWS), but the problem is that
> it may choose the wrong one. ?If your package requires initialization then
> yourpkg::somefun only works if the initialization happens in .onLoad. ?But
> some packages only work if the initialization happens when the package is
> attached. ?It's better for the author to make the choice than for R to do it
> automatically.

OK. Thanks.

-- 
Statistics & Software Consulting
GKX Group, GKX Associates Inc.
tel: 1-877-GKX-GROUP
email: ggrothendieck at gmail.com


From ripley at stats.ox.ac.uk  Sun Jul 17 15:33:33 2011
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 17 Jul 2011 14:33:33 +0100 (BST)
Subject: [Rd] NAMESPACE
In-Reply-To: <4E22CE5D.4070404@gmail.com>
References: <CAP01uRnFyMXiEWHCLN=Cp9gdHHRw53hTAjucnb=r0c+kU62GhA@mail.gmail.com>
	<4E22CE5D.4070404@gmail.com>
Message-ID: <alpine.LFD.2.02.1107171429560.31569@gannet.stats.ox.ac.uk>

On Sun, 17 Jul 2011, Duncan Murdoch wrote:

> On 11-07-16 10:13 PM, Gabor Grothendieck wrote:
>>> Packages without explicit ?NAMESPACE? files will have a default one 
>>> created at build or INSTALL time,
>>> so all packages will have namespaces. A consequence of this is that 
>>> ?.First.lib? functions need to be
>>> renamed, usually as ?.onLoad? but sometimes as ?.onAttach?.
>> 
>> Couldn't R simply regard .First.lib as the appropriate function to
>> save many packages from
>> being needlessly changed?
>> 
>
> It is doing that (see today's version of the NEWS), but the problem is that 
> it may choose the wrong one.  If your package requires initialization then 
> yourpkg::somefun only works if the initialization happens in .onLoad.  But 
> some packages only work if the initialization happens when the package is 
> attached.  It's better for the author to make the choice than for R to do it 
> automatically.

True in principle, but so far using .First.lib as a surrogate for
.onAttach seems to work in all cases (whereas using it as surrogate 
for .onLoad did not in ca 20 packages).   Once people start importing 
from packages with automatically produced namespaces, this will likely 
change.

So there is nothing 'needless' about needing to choose the right thing 
for your package.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ligges at statistik.tu-dortmund.de  Mon Jul 18 13:46:58 2011
From: ligges at statistik.tu-dortmund.de (Uwe Ligges)
Date: Mon, 18 Jul 2011 13:46:58 +0200
Subject: [Rd] image adds lines
In-Reply-To: <CANtt_hx3JXKpFr-FaSapZm7ht3F6KLGqayVKVeD9gMpU2MiqtQ@mail.gmail.com>
References: <CANtt_hx3JXKpFr-FaSapZm7ht3F6KLGqayVKVeD9gMpU2MiqtQ@mail.gmail.com>
Message-ID: <4E241D32.2010906@statistik.tu-dortmund.de>

Yes, thanks. I think I know where this is coming from and will discuss 
with Duncan Murdoch who was working on this topic recently.

Best,
Uwe







On 13.07.2011 20:06, Robert J. Hijmans wrote:
> There seems to be a bug in "image" in R 13.1 (on windows 32&  64 bits) and
> on R-devel that is not present in R 13.0 and before.
>
> The below creates a plot with many white lines, horizontal and vertical,
> more or less regularly spaced. The effect is particularly dramatic when the
> plotting window is made very small.
>
> image(t(volcano)[ncol(volcano):1,])
>
> Robert Hijmans
> University of California, Davis
>
>
>> sessionInfo()
> R version 2.13.1 (2011-07-08)
> Platform: x86_64-pc-mingw32/x64 (64-bit)
>
> locale:
> [1] LC_COLLATE=English_United States.1252
> [2] LC_CTYPE=English_United States.1252
> [3] LC_MONETARY=English_United States.1252
> [4] LC_NUMERIC=C
> [5] LC_TIME=English_United States.1252
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From alireza.s.mahani at gmail.com  Mon Jul 18 17:52:10 2011
From: alireza.s.mahani at gmail.com (Alireza Mahani)
Date: Mon, 18 Jul 2011 08:52:10 -0700 (PDT)
Subject: [Rd] Manipulating single-precision (float) arrays in .Call functions
Message-ID: <1311004330702-3675684.post@n4.nabble.com>

I am writing a wrapper function in C++ that calls a GPU kernel. My array type
for the GPU kernel is float, so I would like my wrapper function to receive
float arrays from R. I understand that I can use 'as.single' in R to copy a
double-precision vector from R in single-precision format while using the
'.C' interface, but is there any way to do something similar for '.Call'?
Given that the latter passes pointers from R to C/C++, I'm guessing this may
be impossible, but I wanted to double-check. If you can suggest a solution,
a small code sample would be much appreciated.

The reason I prefer '.Call' to '.C' is because the former passes pointers
and therefore creates less data transfer overhead as opposed to the latter
which copies data. Is this argument controversial?

-Alireza

--
View this message in context: http://r.789695.n4.nabble.com/Manipulating-single-precision-float-arrays-in-Call-functions-tp3675684p3675684.html
Sent from the R devel mailing list archive at Nabble.com.


From therneau at mayo.edu  Mon Jul 18 18:16:10 2011
From: therneau at mayo.edu (Terry Therneau)
Date: Mon, 18 Jul 2011 11:16:10 -0500
Subject: [Rd] Confusing inheritance problem
In-Reply-To: <4E21CA0A.1050107@statistik.tu-dortmund.de>
References: <1310765027.11629.48.camel@nemo>
	<4E21CA0A.1050107@statistik.tu-dortmund.de>
Message-ID: <1311005770.12897.21.camel@nemo>

 I've packaged the test library up as a tar file at
	ftp.mayo.edu
	directory therneau, file ktest.tar
	login username: mayoftp
             password:  KPlLiFoz
This will disappear in 3 days (Mayo is very fussy about outside access).

In response to Uwe's comments
  1. "2.13.0" is not recent
    It's not the latest, but it is recent.  This is for machines at work where 
where upgrades happen more infrequently
  2. "Matrix not loaded"  
The sessionInfo was only to show what version we have.  Forgetting to load Matrix 
isn't the problem -- when I do that the error is quick and obvious.

 Thanks in advance for any pointers.

Terry T.



On Sat, 2011-07-16 at 19:27 +0200, Uwe Ligges wrote:
> 
> On 15.07.2011 23:23, Terry Therneau wrote:
> >   I have library in development with a function that works when called
> > from the top level, but fails under R CMD check.  The paricular line of
> > failure is
> > 	rsum<- rowSums(kmat>0)
> > where kmat is a dsCMatrix object.
> >
> >    I'm currently stumped and looking for some ideas.
> >
> >    I've created a stripped down library "ktest" that has only 3
> > functions: pedigree.R to create a pedigree or pedigreeList object,
> > 	   kinship.R with "kinship" methods for the two objects
> > 	   one small compute function called by the others
> > along with the minimal amount of other information such that a call to
> >     R --vanilla CMD check ktest
> > gives no errors until the fatal one.
> >
> >   There are two test cases.  A 3 line one that creates a dsCMatrix and
> > call rowSums at the top level works fine, but the same call inside the
> > kmat.pedigreeList function gives an error
> >          'x' must be an array of at least two dimensions
> > Adding a print statement above the rowSums call shows that the argument
> > is a 14 by 14 dsCMatrix.
> >
> >   I'm happy to send the library to anyone else to try and duplicate.
> >      Terry Therneau
> >
> > tmt% R --vanilla
> >
> >> sessionInfo()
> > R version 2.13.0 (2011-04-13)
> > Platform: x86_64-unknown-linux-gnu (64-bit)
> >
> > locale:
> >   [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
> >   [3] LC_TIME=en_US.UTF-8        LC_COLLATE=C
> >   [5] LC_MONETARY=C              LC_MESSAGES=en_US.UTF-8
> >   [7] LC_PAPER=en_US.UTF-8       LC_NAME=C
> >   [9] LC_ADDRESS=C               LC_TELEPHONE=C
> > [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C
> >
> > attached base packages:
> > [1] stats     graphics  grDevices utils     datasets  methods
> > base
> 
> 
> 
> Terry,
> 
> 1. Your R is not recent.
> 2. You do this without having Matrix loaded (according to 
> sessionInfo())? This may already be the cause of your problems.
> 3. You may want to make your package available on some website. I am 
> sure there are people who will take a look (including me, but not today).
> 
> Best wishes,
> Uwe
> 
> 
> 
> 
> 
> 
> 
> 
> 
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel


From murdoch.duncan at gmail.com  Mon Jul 18 18:19:13 2011
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Mon, 18 Jul 2011 12:19:13 -0400
Subject: [Rd] Manipulating single-precision (float) arrays in .Call
	functions
In-Reply-To: <1311004330702-3675684.post@n4.nabble.com>
References: <1311004330702-3675684.post@n4.nabble.com>
Message-ID: <4E245D01.1010407@gmail.com>

On 18/07/2011 11:52 AM, Alireza Mahani wrote:
> I am writing a wrapper function in C++ that calls a GPU kernel. My array type
> for the GPU kernel is float, so I would like my wrapper function to receive
> float arrays from R. I understand that I can use 'as.single' in R to copy a
> double-precision vector from R in single-precision format while using the
> '.C' interface, but is there any way to do something similar for '.Call'?
> Given that the latter passes pointers from R to C/C++, I'm guessing this may
> be impossible, but I wanted to double-check. If you can suggest a solution,
> a small code sample would be much appreciated.
>
> The reason I prefer '.Call' to '.C' is because the former passes pointers
> and therefore creates less data transfer overhead as opposed to the latter
> which copies data. Is this argument controversial?

R has no native type holding singles.  It exports a double to a single 
vector in .C if you ask it to, but that's not available in .Call.  
You'll need to do the copying yourself.

Duncan Murdoch


From bajaj141003 at gmail.com  Mon Jul 18 20:52:01 2011
From: bajaj141003 at gmail.com (Nipesh Bajaj)
Date: Tue, 19 Jul 2011 00:22:01 +0530
Subject: [Rd] Fwd: Understanding R's "Environment" concept
In-Reply-To: <CA+A3Hr6MrAOxNk3L9DZp_CQ-323J6g1efs0q+Apgk6U2OEDmJw@mail.gmail.com>
References: <CA+A3Hr6MrAOxNk3L9DZp_CQ-323J6g1efs0q+Apgk6U2OEDmJw@mail.gmail.com>
Message-ID: <CA+A3Hr5F7BG-DpwfcptauzejJ2_m3i9SgWCCn=HMTO7eXPyQqA@mail.gmail.com>

*********Initially, I posted this topic in R-help however, folks there
suggested me to post this in R-devel forum. Here is my
problem*********


Hi all, I am trying to understand the R's "environment" concept
however the underlying help files look quite technical to me. Can
experts here provide me some more intuitive ideas behind this concept
like, why it is there, what exactly it is doing in R's architecture
etc.?

I mainly need some non-technical intuitive explanation.

Thanks,


From cubranic at stat.ubc.ca  Mon Jul 18 22:58:53 2011
From: cubranic at stat.ubc.ca (Davor Cubranic)
Date: Mon, 18 Jul 2011 13:58:53 -0700
Subject: [Rd] Fwd: Understanding R's "Environment" concept
In-Reply-To: <CA+A3Hr5F7BG-DpwfcptauzejJ2_m3i9SgWCCn=HMTO7eXPyQqA@mail.gmail.com>
References: <CA+A3Hr6MrAOxNk3L9DZp_CQ-323J6g1efs0q+Apgk6U2OEDmJw@mail.gmail.com>
	<CA+A3Hr5F7BG-DpwfcptauzejJ2_m3i9SgWCCn=HMTO7eXPyQqA@mail.gmail.com>
Message-ID: <88F55755-CBDD-4804-B2EA-CCB0130F5B39@stat.ubc.ca>

On 2011-07-18, at 11:52 AM, Nipesh Bajaj wrote:

> Hi all, I am trying to understand the R's "environment" concept
> however the underlying help files look quite technical to me. Can
> experts here provide me some more intuitive ideas behind this concept
> like, why it is there, what exactly it is doing in R's architecture
> etc.?

The "official" definition in the language manual (http://cran.r-project.org/doc/manuals/R-lang.html#Environment-objects) is quite technical, but this sentence makes intuitive sense as well:

    When R looks up the value for a symbol the frame is examined and if a matching symbol is found its value will be returned.

In other words, the environment holds all the defined values (variables, functions, etc.) and lets you refer to them by name. When you assign a value to a variable, say "x <- 10", this is stored in the environment. Then when the program is executed, when you refer to the name "x", it is looked up in the environment and replaced by its value there. (The reality is a little more complicated than that, but this is a close enough simplification.)

As for "why and what it's doing there", you will need to understand some programming language concepts first -- I don't think it's possible to explain this "non-technically".

Davor

From Greg.Snow at imail.org  Mon Jul 18 23:14:17 2011
From: Greg.Snow at imail.org (Greg Snow)
Date: Mon, 18 Jul 2011 15:14:17 -0600
Subject: [Rd] Fwd: Understanding R's "Environment" concept
In-Reply-To: <CA+A3Hr5F7BG-DpwfcptauzejJ2_m3i9SgWCCn=HMTO7eXPyQqA@mail.gmail.com>
References: <CA+A3Hr6MrAOxNk3L9DZp_CQ-323J6g1efs0q+Apgk6U2OEDmJw@mail.gmail.com>
	<CA+A3Hr5F7BG-DpwfcptauzejJ2_m3i9SgWCCn=HMTO7eXPyQqA@mail.gmail.com>
Message-ID: <B37C0A15B8FB3C468B5BC7EBC7DA14CC6349CC6747@LP-EXMBVS10.CO.IHC.COM>

Here is an attempt at the general concept without getting technical.

How many people in the world answer to the name/title "Dad"?

Yet based on context we can usually tell who someone is talking about when they use "Dad".  

It is the same in programming, I may write a function which includes a variable named "length", but my function may call another function that also uses a variable named "length" which could be very different from my "length", the user that calls my function may have their own variable called "length".  How is the computer to know which "length" to use when (and not replace the user's "length" with the one in the function)?  By context of the functions, which context is called environments.  Each function has its own environment and will create and use variables within that environment without affecting variables of the same name in other environments.

Environments can be used for other things as well, but that starts getting more technical.

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at imail.org
801.408.8111


> -----Original Message-----
> From: r-devel-bounces at r-project.org [mailto:r-devel-bounces at r-
> project.org] On Behalf Of Nipesh Bajaj
> Sent: Monday, July 18, 2011 12:52 PM
> To: r-devel at r-project.org
> Subject: [Rd] Fwd: Understanding R's "Environment" concept
> 
> *********Initially, I posted this topic in R-help however, folks there
> suggested me to post this in R-devel forum. Here is my
> problem*********
> 
> 
> Hi all, I am trying to understand the R's "environment" concept
> however the underlying help files look quite technical to me. Can
> experts here provide me some more intuitive ideas behind this concept
> like, why it is there, what exactly it is doing in R's architecture
> etc.?
> 
> I mainly need some non-technical intuitive explanation.
> 
> Thanks,
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From wdunlap at tibco.com  Mon Jul 18 23:53:30 2011
From: wdunlap at tibco.com (William Dunlap)
Date: Mon, 18 Jul 2011 21:53:30 +0000
Subject: [Rd] filter(x,
	method="recursive") when there are NA's in x messes up
Message-ID: <E66794E69CFDE04D9A70842786030B93FBD7@PA-MBX04.na.tibco.com>

If x contains an NA then filter(x, f, method="recursive")
puts NA's and 0.0's in the output in an odd pattern.
It puts NA's in the length(f) positions ending at the position
of the input NA and it puts a run of length(f) 0.0's starting
at the position immediately after the input NA.  The values
after the 0.0's look like the recursion was restarted there.

I'd naively expect that all output values at and after the position
of the input NA to be NA, but it looks like it is trying to
restart the recursion.  I would not expect the output values
before the position of the input NA to depend on future values.
Are the NA's misplaced?

> x <- c(1:4, NA, 6:9)
> cbind(x, "1"=filter(x, 0.5, method="recursive"),
+          "2"=filter(x, c(0.5, 0.0), method="recursive"),
+          "3"=filter(x, c(0.5, 0.0, 0.0), method="recursive")) 
Time Series:
Start = 1 
End = 9 
Frequency = 1 
   x      1     2   3
1  1  1.000  1.00 1.0
2  2  2.500  2.50 2.5
3  3  4.250  4.25  NA
4  4  6.125    NA  NA
5 NA     NA    NA  NA
6  6  0.000  0.00 0.0
7  7  7.000  0.00 0.0
8  8 11.500  8.00 0.0
9  9 14.750 13.00 9.0

Bill Dunlap
Spotfire, TIBCO Software
wdunlap tibco.com 


From ripley at stats.ox.ac.uk  Tue Jul 19 00:08:18 2011
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 18 Jul 2011 23:08:18 +0100 (BST)
Subject: [Rd] [R] Problem compiling in extra/xdr
In-Reply-To: <alpine.LNX.2.00.1107181539100.13110@waverley.Belkin>
References: <alpine.LNX.2.00.1107181539100.13110@waverley.Belkin>
Message-ID: <alpine.LFD.2.02.1107182251020.4163@gannet.stats.ox.ac.uk>

1) R-help is the wrong list: see the posting guide.  I've moved this 
to R-devel.

2) A glibc system should not be compiling in that directory.  glibc 
2.14 is rather recent and NEWS does say

* The RPC implementation in libc is obsoleted.  Old programs keep working
   but new programs cannot be linked with the routines in libc anymore.
   Programs in need of RPC functionality must be linked against TI-RPC.
   The TI-RPC implementation is IPv6 enabled and there are other benefits.

   Visible changes of this change include (obviously) the inability to link
   programs using RPC functions without referencing the TI-RPC library and the
   removal of the RPC headers from the glibc headers.
   Implemented by Ulrich Drepper.

So the answer seems to be that your libc is too new.

On Mon, 18 Jul 2011, Allin Cottrell wrote:

> I'm building R 2.13.1 on i686-pc-linux-gnu, using gcc 4.6.1
> and with glibc 2.14.
>
> I get this error:
>
> In file included from xdr.c:61:0:
> ./rpc/types.h:63:14: error: conflicting types for 'malloc'
> make[4]: *** [xdr.o] Error 1
>
> I can make the build proceed some by commenting out the declaration "extern 
> char *malloc();" in xdr/rpc/types.h,
> but then I get a slew of other errors:
>
> xdr_float.c: In function 'xdr_float':
> xdr_float.c:119:21: error: storage size of 'is' isn't known
> xdr_float.c:120:20: error: storage size of 'vs' isn't known
>
> and so on.
>
> config.log is rather big to post here; I'm putting it at
> http://www.wfu.edu/~cottrell/tmp/R.config.log .
>
> --
> Allin Cottrell
> Department of Economics
> Wake Forest University, NC
>
> ______________________________________________
> R-help at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From alireza.s.mahani at gmail.com  Tue Jul 19 00:15:22 2011
From: alireza.s.mahani at gmail.com (Alireza Mahani)
Date: Mon, 18 Jul 2011 15:15:22 -0700 (PDT)
Subject: [Rd] Manipulating single-precision (float) arrays in .Call
	functions
In-Reply-To: <4E245D01.1010407@gmail.com>
References: <1311004330702-3675684.post@n4.nabble.com>
	<4E245D01.1010407@gmail.com>
Message-ID: <1311027322959-3676667.post@n4.nabble.com>

Duncan,

Thank you for your reply. This is a rather unfortunate limitation, because
for large data sizes there is a significant difference between the
performance of '.C' and '.Call'. I will have to do some tests to see what
sort of penalty I incur for copying from double to float inside my C++ code
(so I can use '.Call'). I won't be able to use memset(); rather, I have to
have an explicit loop and use casting. Is there a more efficient option?

-Alireza


--
View this message in context: http://r.789695.n4.nabble.com/Manipulating-single-precision-float-arrays-in-Call-functions-tp3675684p3676667.html
Sent from the R devel mailing list archive at Nabble.com.


From simon.urbanek at r-project.org  Tue Jul 19 01:53:15 2011
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Mon, 18 Jul 2011 19:53:15 -0400
Subject: [Rd] Manipulating single-precision (float) arrays in .Call
	functions
In-Reply-To: <1311027322959-3676667.post@n4.nabble.com>
References: <1311004330702-3675684.post@n4.nabble.com>
	<4E245D01.1010407@gmail.com>
	<1311027322959-3676667.post@n4.nabble.com>
Message-ID: <CC044BB2-F9EA-440F-8A64-5B4639B2A529@r-project.org>


On Jul 18, 2011, at 6:15 PM, Alireza Mahani wrote:

> Duncan,
> 
> Thank you for your reply. This is a rather unfortunate limitation, because for large data sizes there is a significant difference between the performance of '.C' and '.Call'.

I think you may have missed the main point - R does NOT have any objects that are in "float" (single precision) representations because that is insufficient precision, so one way or another you'll have to do something like

SEXP foo(SEXP bar) {
	double *d = REAL(bar);
	int i, n = LENGTH(bar);
	float *f = (float*) R_alloc(sizeof(float), n);
	for (i = 0; i < n; i++) f[i] = d[i];
	// continue with floats ..


There is simply no other way as, again, there are no floats in R anywhere. This has nothing to do with .C/.Call, either way floats will need to be created from the input vectors.

If you make up your own stuff, you could use raw vectors to store floats if only your functions work on it (or external pointers if you want to keep track of the memory yourself).


> I will have to do some tests to see what
> sort of penalty I incur for copying from double to float inside my C++ code
> (so I can use '.Call'). I won't be able to use memset(); rather, I have to
> have an explicit loop and use casting. Is there a more efficient option?
> 

I'm not aware of any, if you use floats, you incur a penalty regardless (one for additional storage, another for converting).

One of the reasons that GPU processing has not caught much traction in stat computing is because it's (practically) limited to single precision computations (and we even need quad precision occasionally). Although some GPUs support double precision, they are still not fast enough to be a real threat to CPUs. (I'm talking about generic usability - for very specialized tasks GPU can deliver big speedups, but those require low-level exploitation of the architecture).

Cheers,
Simon


From alireza.s.mahani at gmail.com  Tue Jul 19 06:07:04 2011
From: alireza.s.mahani at gmail.com (Alireza Mahani)
Date: Mon, 18 Jul 2011 21:07:04 -0700 (PDT)
Subject: [Rd] Manipulating single-precision (float) arrays in .Call
	functions
In-Reply-To: <CC044BB2-F9EA-440F-8A64-5B4639B2A529@r-project.org>
References: <1311004330702-3675684.post@n4.nabble.com>
	<4E245D01.1010407@gmail.com>
	<1311027322959-3676667.post@n4.nabble.com>
	<CC044BB2-F9EA-440F-8A64-5B4639B2A529@r-project.org>
Message-ID: <1311048424023-3677232.post@n4.nabble.com>

Simon,

Thank you for elaborating on the limitations of R in handling float types. I
think I'm pretty much there with you.

As for the insufficiency of single-precision math (and hence limitations of
GPU), my personal take so far has been that double-precision becomes crucial
when some sort of error accumulation occurs. For example, in differential
equations where boundary values are integrated to arrive at interior values,
etc. On the other hand, in my personal line of work (Hierarchical Bayesian
models for quantitative marketing), we have so much inherent uncertainty and
noise at so many levels in the problem (and no significant error
accumulation sources) that single vs double precision issue is often
inconsequential for us. So I think it really depends on the field as well as
the nature of the problem.

Regards,
Alireza


--
View this message in context: http://r.789695.n4.nabble.com/Manipulating-single-precision-float-arrays-in-Call-functions-tp3675684p3677232.html
Sent from the R devel mailing list archive at Nabble.com.


From ripley at stats.ox.ac.uk  Tue Jul 19 07:51:11 2011
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 19 Jul 2011 06:51:11 +0100 (BST)
Subject: [Rd] [R] Problem compiling in extra/xdr
In-Reply-To: <alpine.DEB.2.00.1107181954150.5589@myrtle>
References: <alpine.LNX.2.00.1107181539100.13110@waverley.Belkin>
	<alpine.LFD.2.02.1107182251020.4163@gannet.stats.ox.ac.uk>
	<alpine.DEB.2.00.1107181954150.5589@myrtle>
Message-ID: <alpine.LFD.2.02.1107190631570.28269@gannet.stats.ox.ac.uk>

On Mon, 18 Jul 2011, Allin Cottrell wrote:

> On Mon, 18 Jul 2011, Prof Brian Ripley wrote:
>
>> 1) R-help is the wrong list: see the posting guide.  I've moved this to 
>> R-devel.
>> 
>> 2) A glibc system should not be compiling in that directory.  glibc 2.14 is 
>> rather recent and NEWS does say
>> 
>> * The RPC implementation in libc is obsoleted.  Old programs keep working
>>  but new programs cannot be linked with the routines in libc anymore.
>>  Programs in need of RPC functionality must be linked against TI-RPC.
>>  The TI-RPC implementation is IPv6 enabled and there are other benefits.
>>
>>  Visible changes of this change include (obviously) the inability to link
>>  programs using RPC functions without referencing the TI-RPC library and 
>> the
>>  removal of the RPC headers from the glibc headers.
>>  Implemented by Ulrich Drepper.
>> 
>> So the answer seems to be that your libc is too new.
>
> OK, thanks. I should have remembered the info about RPC in the glibc-2.14 
> news. Then there will presumably be a problem building current R on current 
> Fedora?

What is 'current Fedora'?  glibc 2.14 postdates the current release, 
Fedora 15, which uses 2.13.   I do not know what Fedora 16 will use in 
several months ....

The main problem will be that the xdr included in R is only for 
platforms with 32-bit longs -- but that may be true for your i686 
Linux.  It needs _X86_ defined to compile for i686: I would have 
expected that to be true on your platform, but am testing a cleaned-up 
version.  If that works it will appear in R-patched within 24 hours.

>
>> On Mon, 18 Jul 2011, Allin Cottrell wrote:
>> 
>>> I'm building R 2.13.1 on i686-pc-linux-gnu, using gcc 4.6.1
>>> and with glibc 2.14.
>>> 
>>> I get this error:
>>> 
>>> In file included from xdr.c:61:0:
>>> ./rpc/types.h:63:14: error: conflicting types for 'malloc'
>>> make[4]: *** [xdr.o] Error 1
>>> 
>>> I can make the build proceed some by commenting out the declaration 
>>> "extern char *malloc();" in xdr/rpc/types.h,
>>> but then I get a slew of other errors:
>>> 
>>> xdr_float.c: In function 'xdr_float':
>>> xdr_float.c:119:21: error: storage size of 'is' isn't known
>>> xdr_float.c:120:20: error: storage size of 'vs' isn't known
>>> 
>>> and so on.
>>> 
>>> config.log is rather big to post here; I'm putting it at
>>> http://www.wfu.edu/~cottrell/tmp/R.config.log .
>>> 
>>> --
>>> Allin Cottrell
>>> Department of Economics
>>> Wake Forest University, NC
>>> 
>>> ______________________________________________
>>> R-help at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide 
>>> http://www.R-project.org/posting-guide.html
>>> and provide commented, minimal, self-contained, reproducible code.
>>> 
>> 
>> 
>
> -- 
> Allin Cottrell
> Department of Economics
> Wake Forest University
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Tue Jul 19 08:26:08 2011
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 19 Jul 2011 07:26:08 +0100 (BST)
Subject: [Rd] Manipulating single-precision (float) arrays in .Call
 functions
In-Reply-To: <1311048424023-3677232.post@n4.nabble.com>
References: <1311004330702-3675684.post@n4.nabble.com>
	<4E245D01.1010407@gmail.com>
	<1311027322959-3676667.post@n4.nabble.com>
	<CC044BB2-F9EA-440F-8A64-5B4639B2A529@r-project.org>
	<1311048424023-3677232.post@n4.nabble.com>
Message-ID: <alpine.LFD.2.02.1107190640280.28269@gannet.stats.ox.ac.uk>

On Mon, 18 Jul 2011, Alireza Mahani wrote:

> Simon,
>
> Thank you for elaborating on the limitations of R in handling float types. I
> think I'm pretty much there with you.
>
> As for the insufficiency of single-precision math (and hence limitations of
> GPU), my personal take so far has been that double-precision becomes crucial
> when some sort of error accumulation occurs. For example, in differential
> equations where boundary values are integrated to arrive at interior values,
> etc. On the other hand, in my personal line of work (Hierarchical Bayesian
> models for quantitative marketing), we have so much inherent uncertainty and
> noise at so many levels in the problem (and no significant error
> accumulation sources) that single vs double precision issue is often
> inconsequential for us. So I think it really depends on the field as well as
> the nature of the problem.

The main reason to use only double precision in R was that on modern 
CPUs double precision calculations are as fast as single-precision 
ones, and with 64-bit CPUs they are a single access.  So the extra 
precision comes more-or-less for free.  You also under-estimate the 
extent to which stability of commonly used algorithms relies on double 
precision.  (There are stable single-precision versions, but they are 
no longer commonly used.  And as Simon said, in some cases stability 
is ensured by using extra precision where available.)

I disagree slightly with Simon on GPUs: I am told by local experts 
that the double-precision on the latest GPUs (those from the last year 
or so) is perfectly usable.  See the performance claims on 
http://en.wikipedia.org/wiki/Nvidia_Tesla of about 50% of the SP 
performance in DP.

>
> Regards,
> Alireza
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/Manipulating-single-precision-float-arrays-in-Call-functions-tp3675684p3677232.html
> Sent from the R devel mailing list archive at Nabble.com.
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From mtmorgan at fhcrc.org  Tue Jul 19 10:23:39 2011
From: mtmorgan at fhcrc.org (Martin Morgan)
Date: Tue, 19 Jul 2011 01:23:39 -0700
Subject: [Rd] requiring NAMESPACE re-installation marked as old.packages?
Message-ID: <4E253F0B.70506@fhcrc.org>

It would be convenient if, under R-devel r56422, packages that require 
re-installation because they do not have a NAMESPACE were marked as 
old.packages, so their lack of functionality can be discovered more easily.

 > "snow" %in% row.names(old.packages())
[1] FALSE
 > library(snow)
Error in library(snow) :
   package 'snow' does not have a NAMESPACE and should be re-installed
 > install.packages("snow", repos="http://cran.r-project.org")
Installing package(s) into 
'/home/mtmorgan/R/x86_64-unknown-linux-gnu-library/2.14'
(as 'lib' is unspecified)
trying URL 'http://cran.r-project.org/src/contrib/snow_0.3-5.tar.gz'
Content type 'application/x-gzip' length 21059 bytes (20 Kb)
opened URL
==================================================
downloaded 20 Kb

* installing *source* package 'snow' ...
** R
** inst
** Creating default NAMESPACE file
** preparing package for lazy loading
** help
*** installing help indices
** building package indices ...
** testing if installed package can be loaded
Warning: running .First.lib() for package 'snow' as .onLoad/.onAssign 
were not found
Error in initDefaultClusterOptions() :
   cannot change value of locked binding for 'defaultClusterOptions'
Error: loading failed
Execution halted
ERROR: loading failed
* removing '/home/mtmorgan/R/x86_64-unknown-linux-gnu-library/2.14/snow'
* restoring previous 
'/home/mtmorgan/R/x86_64-unknown-linux-gnu-library/2.14/snow'

The downloaded packages are in
         '/tmp/RtmpoGypnz/downloaded_packages'
Warning message:
In install.packages("snow", repos = "http://cran.r-project.org") :
   installation of package 'snow' had non-zero exit status

Martin
-- 
Computational Biology
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N. PO Box 19024 Seattle, WA 98109

Location: M1-B861
Telephone: 206 667-2793


From maechler at stat.math.ethz.ch  Tue Jul 19 11:35:52 2011
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 19 Jul 2011 11:35:52 +0200
Subject: [Rd] (no subject)
In-Reply-To: <16AC753F-32F8-4060-BD37-F94883AD8CA4@r-project.org>
References: <23FB7F0C9B136E42B87064C97EAD4991809381@staffexchange3.ul.campus>
	<16AC753F-32F8-4060-BD37-F94883AD8CA4@r-project.org>
Message-ID: <20005.20472.901447.584817@stat.math.ethz.ch>

>>>>> Simon Urbanek <simon.urbanek at r-project.org>
>>>>>     on Tue, 28 Jun 2011 11:31:58 -0400 writes:

    > On Jun 28, 2011, at 9:58 AM, Michelle.Carey wrote:

    >> Hi,
    >> 
    >> 
    >> 
    >> I am trying to write code in C for an R package. I need
    >> high precision in the form of the mpfr and gmp packages.

    > gmp is available as R package
    > http://cran.r-project.org/web/packages/gmp/index.html do
    > you really need it at a lower level than that?

and so is MPFR:  Package 'Rmpfr'

(both on CRAN and R-forge)

.. and I am asking the same question as Simon:
Are you sure you need to use your own C code, 
instead of simply using the R packages 'gmp' and 'Rmpfr' and
work with R code?

--
Martin Maechler, ETH Zurich


    >> I have installed mpfr and gmp under the instructions of
    >> the following website
    >> http://pauillac.inria.fr/cdrom_a_graver/prog/pc/mpfr/eng.htm
    >> and I get no errors. I have put the header files (mpfr.h
    >> and gmp.h) in the folder C:\Program
    >> Files\R\R-2.13.0\include;

    > That doesn't sound right - the instructions you quote
    > install in the default location which is presumably
    > /usr/local so you should simply set your flags to use that
    > location.


    >> allowing my c code to identify the header files by
    >> incorporating include<gmp.h> and include<mpfr.h>.
    >> Unfortunately when I try to include any of the functions
    >> listed in the header file I get compile error stating
    >> that these functions are undeclared,

    > That doesn't sound plausible, either - undeclared function
    > don't cause errors (not in C). Undefined functions do and
    > you have to make sure you link the libraries installed
    > above.


    >> even though the source code is in the same directory as
    >> my c code that I am trying to compile.

    > That bears no meaning - as you said you installed the
    > libraries, so you have to use those. Source code of
    > gmp/mpfr has no effect on your code and should not be
    > anywhere near it.

    > Cheers, Simon


    >> Any help on this matter would be greatly appreciated.
    >> 
    >> 
    >> Kind Regards,
    >> 
    >> Michelle Carey


From mdowle at mdowle.plus.com  Tue Jul 19 13:48:06 2011
From: mdowle at mdowle.plus.com (Matthew Dowle)
Date: Tue, 19 Jul 2011 12:48:06 +0100
Subject: [Rd] Manipulating single-precision (float) arrays in .Call
	functions
References: <1311004330702-3675684.post@n4.nabble.com><4E245D01.1010407@gmail.com><1311027322959-3676667.post@n4.nabble.com><CC044BB2-F9EA-440F-8A64-5B4639B2A529@r-project.org><1311048424023-3677232.post@n4.nabble.com>
	<alpine.LFD.2.02.1107190640280.28269@gannet.stats.ox.ac.uk>
Message-ID: <j03qto$hvm$1@dough.gmane.org>


"Prof Brian Ripley" <ripley at stats.ox.ac.uk> wrote in message 
news:alpine.LFD.2.02.1107190640280.28269 at gannet.stats.ox.ac.uk...
> On Mon, 18 Jul 2011, Alireza Mahani wrote:
>
>> Simon,
>>
>> Thank you for elaborating on the limitations of R in handling float 
>> types. I
>> think I'm pretty much there with you.
>>
>> As for the insufficiency of single-precision math (and hence limitations 
>> of
>> GPU), my personal take so far has been that double-precision becomes 
>> crucial
>> when some sort of error accumulation occurs. For example, in differential
>> equations where boundary values are integrated to arrive at interior 
>> values,
>> etc. On the other hand, in my personal line of work (Hierarchical 
>> Bayesian
>> models for quantitative marketing), we have so much inherent uncertainty 
>> and
>> noise at so many levels in the problem (and no significant error
>> accumulation sources) that single vs double precision issue is often
>> inconsequential for us. So I think it really depends on the field as well 
>> as
>> the nature of the problem.
>
> The main reason to use only double precision in R was that on modern CPUs 
> double precision calculations are as fast as single-precision ones, and 
> with 64-bit CPUs they are a single access.
> So the extra precision comes more-or-less for free.

But, isn't it much more of the 'less free' when large data sets are
considered? If a double matrix takes 3GB, it's 1.5GB in single.
That might alleviate the dreaded out-of-memory error for some
users in some circumstances. On 64bit, 50GB reduces to 25GB
and that might make the difference between getting
something done, or not. If single were appropriate, of course.
For GPU too, i/o often dominates iiuc.

For space reasons, is there any possibility of R supporting single
precision (and single bit logical to reduce memory for logicals by
32 times)? I guess there might be complaints from users using
single inappropriately (or worse, not realising we have an instable
result due to single).

Matthew

> You also under-estimate the extent to which stability of commonly used 
> algorithms relies on double precision.  (There are stable single-precision 
> versions, but they are no longer commonly used.  And as Simon said, in 
> some cases stability is ensured by using extra precision where available.)
>
> I disagree slightly with Simon on GPUs: I am told by local experts that 
> the double-precision on the latest GPUs (those from the last year or so) 
> is perfectly usable.  See the performance claims on 
> http://en.wikipedia.org/wiki/Nvidia_Tesla of about 50% of the SP 
> performance in DP.
>
>>
>> Regards,
>> Alireza
>>
>>
>> --
>> View this message in context: 
>> http://r.789695.n4.nabble.com/Manipulating-single-precision-float-arrays-in-Call-functions-tp3675684p3677232.html
>> Sent from the R devel mailing list archive at Nabble.com.
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>


From ripley at stats.ox.ac.uk  Tue Jul 19 14:29:33 2011
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 19 Jul 2011 13:29:33 +0100 (BST)
Subject: [Rd] Welcome Uwe Ligges to R-core
Message-ID: <alpine.LFD.2.02.1107191136140.25622@gannet.stats.ox.ac.uk>

Uwe is now a member of R-core.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From marc_schwartz at me.com  Tue Jul 19 14:34:18 2011
From: marc_schwartz at me.com (Marc Schwartz)
Date: Tue, 19 Jul 2011 07:34:18 -0500
Subject: [Rd] Welcome Uwe Ligges to R-core
In-Reply-To: <alpine.LFD.2.02.1107191136140.25622@gannet.stats.ox.ac.uk>
References: <alpine.LFD.2.02.1107191136140.25622@gannet.stats.ox.ac.uk>
Message-ID: <ECC3B6F7-7937-4409-A7B7-64F484DCCFA4@me.com>


On Jul 19, 2011, at 7:29 AM, Prof Brian Ripley wrote:

> Uwe is now a member of R-core.

Congratulations Uwe!

Regards,

Marc Schwartz


From simon.urbanek at r-project.org  Tue Jul 19 15:32:54 2011
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Tue, 19 Jul 2011 09:32:54 -0400
Subject: [Rd] Manipulating single-precision (float) arrays in .Call
	functions
In-Reply-To: <j03qto$hvm$1@dough.gmane.org>
References: <1311004330702-3675684.post@n4.nabble.com><4E245D01.1010407@gmail.com><1311027322959-3676667.post@n4.nabble.com><CC044BB2-F9EA-440F-8A64-5B4639B2A529@r-project.org><1311048424023-3677232.post@n4.nabble.com>
	<alpine.LFD.2.02.1107190640280.28269@gannet.stats.ox.ac.uk>
	<j03qto$hvm$1@dough.gmane.org>
Message-ID: <24B0CDFD-6CC6-4E65-809A-23736A0B1D27@r-project.org>


On Jul 19, 2011, at 7:48 AM, Matthew Dowle wrote:

> 
> "Prof Brian Ripley" <ripley at stats.ox.ac.uk> wrote in message 
> news:alpine.LFD.2.02.1107190640280.28269 at gannet.stats.ox.ac.uk...
>> On Mon, 18 Jul 2011, Alireza Mahani wrote:
>> 
>>> Simon,
>>> 
>>> Thank you for elaborating on the limitations of R in handling float 
>>> types. I
>>> think I'm pretty much there with you.
>>> 
>>> As for the insufficiency of single-precision math (and hence limitations 
>>> of
>>> GPU), my personal take so far has been that double-precision becomes 
>>> crucial
>>> when some sort of error accumulation occurs. For example, in differential
>>> equations where boundary values are integrated to arrive at interior 
>>> values,
>>> etc. On the other hand, in my personal line of work (Hierarchical 
>>> Bayesian
>>> models for quantitative marketing), we have so much inherent uncertainty 
>>> and
>>> noise at so many levels in the problem (and no significant error
>>> accumulation sources) that single vs double precision issue is often
>>> inconsequential for us. So I think it really depends on the field as well 
>>> as
>>> the nature of the problem.
>> 
>> The main reason to use only double precision in R was that on modern CPUs 
>> double precision calculations are as fast as single-precision ones, and 
>> with 64-bit CPUs they are a single access.
>> So the extra precision comes more-or-less for free.
> 
> But, isn't it much more of the 'less free' when large data sets are considered? If a double matrix takes 3GB, it's 1.5GB in single.
> That might alleviate the dreaded out-of-memory error for some users in some circumstances. On 64bit, 50GB reduces to 25GB

I'd like to see your 50Gb matrix in R ;) - you can't have a float matrix bigger than 8Gb, for doubles it is 16Gb so you don't gain anything in scalability. IMHO memory is not a strong case these days when hundreds GB of RAM are affordable...

Also you would not complain about pointers going from 4 to 8 bytes in 64-bit thus doubling your memory use for string vectors...

Cheers,
Simon


> and that might make the difference between getting
> something done, or not. If single were appropriate, of course.
> For GPU too, i/o often dominates iiuc.
> 
> For space reasons, is there any possibility of R supporting single
> precision (and single bit logical to reduce memory for logicals by
> 32 times)? I guess there might be complaints from users using
> single inappropriately (or worse, not realising we have an instable
> result due to single).
> 
> Matthew
> 
>> You also under-estimate the extent to which stability of commonly used 
>> algorithms relies on double precision.  (There are stable single-precision 
>> versions, but they are no longer commonly used.  And as Simon said, in 
>> some cases stability is ensured by using extra precision where available.)
>> 
>> I disagree slightly with Simon on GPUs: I am told by local experts that 
>> the double-precision on the latest GPUs (those from the last year or so) 
>> is perfectly usable.  See the performance claims on 
>> http://en.wikipedia.org/wiki/Nvidia_Tesla of about 50% of the SP 
>> performance in DP.
>> 
>>> 
>>> Regards,
>>> Alireza
>>> 
>>> 
>>> --
>>> View this message in context: 
>>> http://r.789695.n4.nabble.com/Manipulating-single-precision-float-arrays-in-Call-functions-tp3675684p3677232.html
>>> Sent from the R devel mailing list archive at Nabble.com.
>>> 
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>> 
>> 
>> -- 
>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>> University of Oxford,             Tel:  +44 1865 272861 (self)
>> 1 South Parks Road,                     +44 1865 272866 (PA)
>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>> 
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 
> 


From murdoch.duncan at gmail.com  Tue Jul 19 16:34:40 2011
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Tue, 19 Jul 2011 10:34:40 -0400
Subject: [Rd] Manipulating single-precision (float) arrays in .Call
	functions
In-Reply-To: <j03qto$hvm$1@dough.gmane.org>
References: <1311004330702-3675684.post@n4.nabble.com><4E245D01.1010407@gmail.com><1311027322959-3676667.post@n4.nabble.com><CC044BB2-F9EA-440F-8A64-5B4639B2A529@r-project.org><1311048424023-3677232.post@n4.nabble.com>	<alpine.LFD.2.02.1107190640280.28269@gannet.stats.ox.ac.uk>
	<j03qto$hvm$1@dough.gmane.org>
Message-ID: <4E259600.5070103@gmail.com>

On 11-07-19 7:48 AM, Matthew Dowle wrote:
>
> "Prof Brian Ripley"<ripley at stats.ox.ac.uk>  wrote in message
> news:alpine.LFD.2.02.1107190640280.28269 at gannet.stats.ox.ac.uk...
>> On Mon, 18 Jul 2011, Alireza Mahani wrote:
>>
>>> Simon,
>>>
>>> Thank you for elaborating on the limitations of R in handling float
>>> types. I
>>> think I'm pretty much there with you.
>>>
>>> As for the insufficiency of single-precision math (and hence limitations
>>> of
>>> GPU), my personal take so far has been that double-precision becomes
>>> crucial
>>> when some sort of error accumulation occurs. For example, in differential
>>> equations where boundary values are integrated to arrive at interior
>>> values,
>>> etc. On the other hand, in my personal line of work (Hierarchical
>>> Bayesian
>>> models for quantitative marketing), we have so much inherent uncertainty
>>> and
>>> noise at so many levels in the problem (and no significant error
>>> accumulation sources) that single vs double precision issue is often
>>> inconsequential for us. So I think it really depends on the field as well
>>> as
>>> the nature of the problem.
>>
>> The main reason to use only double precision in R was that on modern CPUs
>> double precision calculations are as fast as single-precision ones, and
>> with 64-bit CPUs they are a single access.
>> So the extra precision comes more-or-less for free.
>
> But, isn't it much more of the 'less free' when large data sets are
> considered? If a double matrix takes 3GB, it's 1.5GB in single.
> That might alleviate the dreaded out-of-memory error for some
> users in some circumstances. On 64bit, 50GB reduces to 25GB
> and that might make the difference between getting
> something done, or not. If single were appropriate, of course.
> For GPU too, i/o often dominates iiuc.
>
> For space reasons, is there any possibility of R supporting single
> precision (and single bit logical to reduce memory for logicals by
> 32 times)? I guess there might be complaints from users using
> single inappropriately (or worse, not realising we have an instable
> result due to single).

You can do any of this using external pointers now.  That will remind 
you that every single function to operate on such objects needs to be 
rewritten.

It's a huge amount of work, benefiting very few people.  I don't think 
anyone in R Core will do it.

Duncan Murdoch

> Matthew
>
>> You also under-estimate the extent to which stability of commonly used
>> algorithms relies on double precision.  (There are stable single-precision
>> versions, but they are no longer commonly used.  And as Simon said, in
>> some cases stability is ensured by using extra precision where available.)
>>
>> I disagree slightly with Simon on GPUs: I am told by local experts that
>> the double-precision on the latest GPUs (those from the last year or so)
>> is perfectly usable.  See the performance claims on
>> http://en.wikipedia.org/wiki/Nvidia_Tesla of about 50% of the SP
>> performance in DP.
>>
>>>
>>> Regards,
>>> Alireza
>>>
>>>
>>> --
>>> View this message in context:
>>> http://r.789695.n4.nabble.com/Manipulating-single-precision-float-arrays-in-Call-functions-tp3675684p3677232.html
>>> Sent from the R devel mailing list archive at Nabble.com.
>>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>
>>
>> --
>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>> University of Oxford,             Tel:  +44 1865 272861 (self)
>> 1 South Parks Road,                     +44 1865 272866 (PA)
>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From mdowle at mdowle.plus.com  Tue Jul 19 16:53:46 2011
From: mdowle at mdowle.plus.com (Matthew Dowle)
Date: Tue, 19 Jul 2011 15:53:46 +0100
Subject: [Rd] Manipulating single-precision (float) arrays in
	.Callfunctions
References: <1311004330702-3675684.post@n4.nabble.com><4E245D01.1010407@gmail.com><1311027322959-3676667.post@n4.nabble.com><CC044BB2-F9EA-440F-8A64-5B4639B2A529@r-project.org><1311048424023-3677232.post@n4.nabble.com>	<alpine.LFD.2.02.1107190640280.28269@gannet.stats.ox.ac.uk><j03qto$hvm$1@dough.gmane.org>
	<4E259600.5070103@gmail.com>
Message-ID: <j045ps$rda$1@dough.gmane.org>


"Duncan Murdoch" <murdoch.duncan at gmail.com> wrote in message 
news:4E259600.5070103 at gmail.com...
> On 11-07-19 7:48 AM, Matthew Dowle wrote:
>>
>> "Prof Brian Ripley"<ripley at stats.ox.ac.uk>  wrote in message
>> news:alpine.LFD.2.02.1107190640280.28269 at gannet.stats.ox.ac.uk...
>>> On Mon, 18 Jul 2011, Alireza Mahani wrote:
>>>
>>>> Simon,
>>>>
>>>> Thank you for elaborating on the limitations of R in handling float
>>>> types. I
>>>> think I'm pretty much there with you.
>>>>
>>>> As for the insufficiency of single-precision math (and hence 
>>>> limitations
>>>> of
>>>> GPU), my personal take so far has been that double-precision becomes
>>>> crucial
>>>> when some sort of error accumulation occurs. For example, in 
>>>> differential
>>>> equations where boundary values are integrated to arrive at interior
>>>> values,
>>>> etc. On the other hand, in my personal line of work (Hierarchical
>>>> Bayesian
>>>> models for quantitative marketing), we have so much inherent 
>>>> uncertainty
>>>> and
>>>> noise at so many levels in the problem (and no significant error
>>>> accumulation sources) that single vs double precision issue is often
>>>> inconsequential for us. So I think it really depends on the field as 
>>>> well
>>>> as
>>>> the nature of the problem.
>>>
>>> The main reason to use only double precision in R was that on modern 
>>> CPUs
>>> double precision calculations are as fast as single-precision ones, and
>>> with 64-bit CPUs they are a single access.
>>> So the extra precision comes more-or-less for free.
>>
>> But, isn't it much more of the 'less free' when large data sets are
>> considered? If a double matrix takes 3GB, it's 1.5GB in single.
>> That might alleviate the dreaded out-of-memory error for some
>> users in some circumstances. On 64bit, 50GB reduces to 25GB
>> and that might make the difference between getting
>> something done, or not. If single were appropriate, of course.
>> For GPU too, i/o often dominates iiuc.
>>
>> For space reasons, is there any possibility of R supporting single
>> precision (and single bit logical to reduce memory for logicals by
>> 32 times)? I guess there might be complaints from users using
>> single inappropriately (or worse, not realising we have an instable
>> result due to single).
>
> You can do any of this using external pointers now.  That will remind you 
> that every single function to operate on such objects needs to be 
> rewritten.
>
> It's a huge amount of work, benefiting very few people.  I don't think 
> anyone in R Core will do it.

Ok, thanks for the responses.
Matthew

>
> Duncan Murdoch
>
>> Matthew
>>
>>> You also under-estimate the extent to which stability of commonly used
>>> algorithms relies on double precision.  (There are stable 
>>> single-precision
>>> versions, but they are no longer commonly used.  And as Simon said, in
>>> some cases stability is ensured by using extra precision where 
>>> available.)
>>>
>>> I disagree slightly with Simon on GPUs: I am told by local experts that
>>> the double-precision on the latest GPUs (those from the last year or so)
>>> is perfectly usable.  See the performance claims on
>>> http://en.wikipedia.org/wiki/Nvidia_Tesla of about 50% of the SP
>>> performance in DP.
>>>
>>>>
>>>> Regards,
>>>> Alireza
>>>>
>>>>
>>>> --
>>>> View this message in context:
>>>> http://r.789695.n4.nabble.com/Manipulating-single-precision-float-arrays-in-Call-functions-tp3675684p3677232.html
>>>> Sent from the R devel mailing list archive at Nabble.com.
>>>>
>>>> ______________________________________________
>>>> R-devel at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>
>>>
>>> --
>>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>>> University of Oxford,             Tel:  +44 1865 272861 (self)
>>> 1 South Parks Road,                     +44 1865 272866 (PA)
>>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>>
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From bates at stat.wisc.edu  Tue Jul 19 17:00:47 2011
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 19 Jul 2011 10:00:47 -0500
Subject: [Rd] Performance of .C and .Call functions vs. native R code
In-Reply-To: <1310656867725-3667896.post@n4.nabble.com>
References: <1310563735586-3665017.post@n4.nabble.com>
	<05EF8F1E-DC69-4A25-9021-47D9BF841316@gmail.com>
	<1310656867725-3667896.post@n4.nabble.com>
Message-ID: <CAO7JsnQYzcPncw+Z-vbGEMDhX+aL1TeQisT6VJLqTWWhEoYhDg@mail.gmail.com>

On Thu, Jul 14, 2011 at 10:21 AM, Alireza Mahani
<alireza.s.mahani at gmail.com> wrote:
> (I am using a LINUX machine)
>
> Jeff,
>
> In creating reproducible results, I 'partially' answered my question. I have
> attached two scripts, 'mvMultiply.r' and 'mvMultiply.cc'. Please copy both
> files into your chosen directory, then run 'Rscript mvMultiply.r' in that
> directory while changing the two boolean parameters 'INCLUDE_DATAPREP' and
> 'ROWMAJOR' to all four permutations. (The variable 'diffVec' is there to
> verify that the two methods produce the same output vector.)
>
> Below are the results that I get, along with discussion (tR and tCall are in
> sec):
>
> INCLUDE_DATAPREP,ROWMAJOR,tR,tCall
> F,F,13.536,13.875
> F,T,13.824,14.299
> T,F,13.688,18.167
> T,T,13.982,30.730
>
> Interpretation: The execution time for the .Call line is nearly identical to
> the call to R operator '%*%'. Two data preparation lines for the .Call
> method create the overhead:
>
> A <- t(A) (~12sec, or 12usec per call)
> dim(A) <- dim(A)[1] * dim(A)[2] (~4sec, or 4usec per call)
>
> While the first line can be avoided by providing options in c++ function (as
> is done in the BLAS API), I wonder if the second line can be avoided, aside
> from the obvious option of rewriting the R scripts to use vectors instead of
> matrices. But this defies one of the primary advantages of using R, which is
> succinct, high-level coding. In particular, if one has several matrices as
> input into a .Call function, then the overhead from matrix-to-vector
> transformations can add up. To summarize, my questions are:
>
> 1- Do the above results seem reasonable to you? Is there a similar penalty
> in R's '%*%' operator for transforming matrices to vectors before calling
> BLAS functions?
> 2- Are there techniques for reducing the above overhead for developers
> looking to augment their R code with compiled code?
>
> Regards,
> Alireza
>
> ---------------------------------------
> # mvMultiply.r
> # comparing performance of matrix multiplication in R (using '%*%' operator)
> vs. calling compiled code (using .Call function)
> # y [m x 1] = A [m x n] %*% x [n x 1]
>
> rm(list = ls())
> system("R CMD SHLIB mvMultiply.cc")
> dyn.load("mvMultiply.so")
>
> INCLUDE_DATAPREP <- F
> ROWMAJOR <- F #indicates whether the c++ function treats A in a row-major or
> column-major fashion
>
> m <- 100
> n <- 10
> N <- 1000000
>
> diffVec <- array(0, dim = N)
> tR <- 0.0
> tCall <- 0.0
> for (i in 1:N) {
>
> ? ? ? ?A <- runif(m*n); dim(A) <- c(m,n)
> ? ? ? ?x <- runif(n)
>
> ? ? ? ?t1 <- proc.time()[3]
> ? ? ? ?y1 <- A %*% x
> ? ? ? ?tR <- tR + proc.time()[3] - t1
>
> ? ? ? ?if (INCLUDE_DATAPREP) {
> ? ? ? ? ? ? ? ?t1 <- proc.time()[3]
> ? ? ? ?}
> ? ? ? ?if (ROWMAJOR) { #since R will convert matrix to vector in a column-major
> fashion, if the c++ function expects a row-major format, we need to
> transpose A before converting it to a vector
> ? ? ? ? ? ? ? ?A <- t(A)
> ? ? ? ?}
> ? ? ? ?dim(A) <- dim(A)[1] * dim(A)[2]
> ? ? ? ?if (!INCLUDE_DATAPREP) {
> ? ? ? ? ? ? ? ?t1 <- proc.time()[3]
> ? ? ? ?}
> ? ? ? ?y2 <- .Call("matvecMultiply", as.double(A), as.double(x),
> as.logical(c(ROWMAJOR)))
> ? ? ? ?tCall <- tCall + proc.time()[3] - t1
>
> ? ? ? ?diffVec[i] <- max(abs(y2 - y1))
> }
> cat("Data prep time for '.Call' included: ", INCLUDE_DATAPREP, "\n")
> cat("C++ function expects row-major matrix: ", ROWMAJOR, "\n")
> cat("Time - Using '%*%' operator in R: ", tR, "sec\n")
> cat("Time - Using '.Call' function: ", tCall, "sec\n")
> cat("Maximum difference between methods: ", max(diffVec), "\n")
>
> dyn.unload("mvMultiply.so")
> ---------------------------------------
> # mvMultiply.cc
> #include <Rinternals.h>
> #include <R.h>
>
> extern "C" {
>
> SEXP matvecMultiply(SEXP A, SEXP x, SEXP rowmajor) {
> ? ? ? ?double *rA = REAL(A), *rx = REAL(x), *ry;
> ? ? ? ?int *rrm = LOGICAL(rowmajor);
> ? ? ? ?int n = length(x);
> ? ? ? ?int m = length(A) / n;
> ? ? ? ?SEXP y;
> ? ? ? ?PROTECT(y = allocVector(REALSXP, m));
> ? ? ? ?ry = REAL(y);
> ? ? ? ?for (int i = 0; i < m; i++) {
> ? ? ? ? ? ? ? ?ry[i] = 0.0;
> ? ? ? ? ? ? ? ?for (int j = 0; j < n; j++) {
> ? ? ? ? ? ? ? ? ? ? ? ?if (rrm[0] == 1) {
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?ry[i] += rA[i * n + j] * rx[j];
> ? ? ? ? ? ? ? ? ? ? ? ?} else {
> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?ry[i] += rA[j * m + i] * rx[j];
> ? ? ? ? ? ? ? ? ? ? ? ?}
> ? ? ? ? ? ? ? ?}
> ? ? ? ?}
> ? ? ? ?UNPROTECT(1);
> ? ? ? ?return(y);
> }
>
> }
>

I realize that you are just beginning to use the .C and .Call
interfaces and your example is therefore a simple one.  However, if
you plan to continue with such development it is worthwhile learning
of some of the tools available.  I think one of the most important is
the "inline" package that can take a C or C++ code segment as a text
string and go through all the steps of creating and loading a
.Call'able compiled function.

Second, if you are going to use C++ (the code you show could be C code
as it doesn't use any C++ extensions) then you should look at the Rcpp
package written by Dirk Eddelbuettel and Romain Francois which allows
for comparatively painless interfacing of R objects and C++ objects.
The Rcpp-devel list, which I have copied on this reply, is for
questions related to that system.  The inline package allows for
various "plugin" constructions to wrap your code in the appropriate
headers and point the compiler to the locations of header files and
libraries.  There are two extensions to Rcpp for numerical linear
algebra in C++, RcppArmadillo and RcppEigen.  I show the use of
RcppEigen here.

Third there are several packages in R that do the busy work of
benchmarking expressions and neatly formulating the results.  I use
the rbenchmark package.

Putting all these together yields the enclosed script and results.

In Eigen, a MatrixXd object is the equivalent of R's numeric matrix
(similarly MatrixXi for integer and MatrixXcd for complex) and a
VectorXd object is the equivalent of a numeric vector.  A "mapped"
matrix or vector is one that uses the storage allocated by R, thereby
avoiding a copy operation (similar to your accessing elements of the
arrays through the pointer returned by REAL()).  To adhere to R's
functional programming semantics it is a good idea to declare such
objects as const.  The 'as' and 'wrap' functions are provided by Rcpp
with extensions in RcppEigen to the Eigen classes in the development
version.  In the released versions of Rcpp and RcppEigen we use
intermediate Rcpp objects. These functions have the advantage of
checking the types of R objects being passed.  The Eigen code for
matrix multiplication will check the consistency of dimensions in the
operation.

Given the size of the matrix you are working with it is not surprising
that interpretation overhead and checking will be a large part of the
elapsed time, hence the relative differences between different methods
of doing the numerical calculation will be small.  The operation of
multiplying a 100 x 10 matrix by a 10-vector involves "only" 1000
floating point operations.  Furthermore, each element of the matrix is
used only once so sophisticated methods of manipulating cache contents
won't buy you much.  These benchmark results are from a system that
uses Atlas BLAS (basic linear algebra subroutines); other systems will
provide different results.  Interestingly, I found on some systems
using R's BLAS, which are not accelerated, the R code is closer in
speed to the code using Eigen.  An example is given in the second
version of the output.
-------------- next part --------------

R Under development (unstable) (2011-07-19 r56429)
Copyright (C) 2011 The R Foundation for Statistical Computing
ISBN 3-900051-07-0
Platform: x86_64-unknown-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> library(rbenchmark)
> library(inline)
> library(RcppEigen)
Loading required package: Rcpp
> ## create an R function from Eigen-based C++ code
> ff <- if (packageVersion("RcppEigen") > "0.1.1") { #  development version
+     cxxfunction(signature(Xs = "matrix", ys = "numeric"), '
+     typedef  Eigen::Map<Eigen::MatrixXd>  MMatrixXd;
+     typedef  Eigen::Map<Eigen::VectorXd>  MVectorXd;
+ 
+     const MMatrixXd      Xe(as<MMatrixXd>(Xs));
+     const MVectorXd      ye(as<MVectorXd>(ys));
+     return   wrap(Xe * ye);
+ ', plugin = "RcppEigen")
+ } else {
+     cxxfunction(signature(Xs = "matrix", ys = "numeric"), '
+     typedef  Eigen::Map<Eigen::MatrixXd>  MMatrixXd;
+     typedef  Eigen::Map<Eigen::VectorXd>  MVectorXd;
+ 
+     const NumericMatrix   X(Xs);
+     const NumericVector   y(ys);
+     const MMatrixXd      Xe(X.rows(), X.cols(), X.begin());
+     const MVectorXd      ye(y.size(), y.begin());
+     Eigen::VectorXd     res = Xe * ye;
+     return    NumericVector(res.data(), res.data() + res.size());
+ ', plugin = "RcppEigen")
+ }
> 
> set.seed(1)                             # for reproducible results
> m <- 100
> n <- 10
> A <- matrix(runif(m * n), ncol = n)
> y <- runif(n)
> all.equal(as.vector(A %*% y), ff(A, y))
[1] TRUE
> benchmark(Rcode = expression(A %*% y), Eigen = ff(A, y), replications=100000)
   test replications elapsed relative user.self sys.self user.child sys.child
2 Eigen       100000   1.198 1.000000      1.19     0.00          0         0
1 Rcode       100000   1.890 1.577629      1.88     0.01          0         0
> m <- 1000
> n <- 1000
> A <- matrix(runif(m * n), ncol = n)
> y <- runif(n)
> all.equal(as.vector(A %*% y), ff(A, y))
[1] TRUE
> benchmark(Rcode = expression(A %*% y), Eigen = ff(A, y), replications=1000)
   test replications elapsed relative user.self sys.self user.child sys.child
2 Eigen         1000   3.259 1.000000      3.25     0.00          0         0
1 Rcode         1000  15.648 4.801473     17.27     0.39          0         0
> 
> proc.time()
   user  system elapsed 
 31.070   1.160  30.495 
-------------- next part --------------

R version 2.13.1 (2011-07-08)
Copyright (C) 2011 The R Foundation for Statistical Computing
ISBN 3-900051-07-0
Platform: x86_64-unknown-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> library(rbenchmark)
> library(inline)
> library(RcppEigen)
Loading required package: Rcpp
> ## create an R function from Eigen-based C++ code
> ff <- if (packageVersion("RcppEigen") > "0.1.1") { #  development version
+     cxxfunction(signature(Xs = "matrix", ys = "numeric"), '
+     typedef  Eigen::Map<Eigen::MatrixXd>  MMatrixXd;
+     typedef  Eigen::Map<Eigen::VectorXd>  MVectorXd;
+ 
+     const MMatrixXd      Xe(as<MMatrixXd>(Xs));
+     const MVectorXd      ye(as<MVectorXd>(ys));
+     return   wrap(Xe * ye);
+ ', plugin = "RcppEigen")
+ } else {
+     cxxfunction(signature(Xs = "matrix", ys = "numeric"), '
+     typedef  Eigen::Map<Eigen::MatrixXd>  MMatrixXd;
+     typedef  Eigen::Map<Eigen::VectorXd>  MVectorXd;
+ 
+     const NumericMatrix   X(Xs);
+     const NumericVector   y(ys);
+     const MMatrixXd      Xe(X.begin(), X.rows(), X.cols());
+     const MVectorXd      ye(y.begin(), y.size());
+     Eigen::VectorXd     res = Xe * ye;
+     return    NumericVector(res.data(), res.data() + res.size());
+ ', plugin = "RcppEigen")
+ }
> 
> set.seed(1)                             # for reproducible results
> m <- 100
> n <- 10
> A <- matrix(runif(m * n), ncol = n)
> y <- runif(n)
> all.equal(as.vector(A %*% y), ff(A, y))
[1] TRUE
> benchmark(Rcode = expression(A %*% y), Eigen = ff(A, y), replications=100000)
   test replications elapsed relative user.self sys.self user.child sys.child
2 Eigen       100000   0.808 1.000000     0.808    0.001          0         0
1 Rcode       100000   1.037 1.283416     1.031    0.006          0         0
> m <- 1000
> n <- 1000
> A <- matrix(runif(m * n), ncol = n)
> y <- runif(n)
> all.equal(as.vector(A %*% y), ff(A, y))
[1] TRUE
> benchmark(Rcode = expression(A %*% y), Eigen = ff(A, y), replications=1000)
   test replications elapsed relative user.self sys.self user.child sys.child
2 Eigen         1000   1.957 1.000000     1.956        0          0         0
1 Rcode         1000   6.837 3.493613     6.835        0          0         0
> 
> proc.time()
   user  system elapsed 
 14.178   0.388  14.527 

From alireza.s.mahani at gmail.com  Tue Jul 19 17:07:35 2011
From: alireza.s.mahani at gmail.com (Alireza Mahani)
Date: Tue, 19 Jul 2011 08:07:35 -0700 (PDT)
Subject: [Rd] Measuring and comparing .C and .Call overhead
Message-ID: <1311088055629-3678361.post@n4.nabble.com>

Further pursuing my curiosity to measure the efficiency of R/C++ interface, I
conducted a simple matrix-vector multiplication test using .C and .Call
functions in R. In each case, I measured the execution time in R, as well as
inside the C++ function. Subtracting the two, I came up with a measure of
overhead associated with each call. I assume that this overhead would be
non-existent of the entire code was implemented in C++. [Please let me know
if my approach is somehow faulty.] The results of this simple test have
significant implications for my software development approach. I hope others
find this information useful as well. I have attached the code files in the
bottom. Below I provide a summary output, and interpret the results. [All
times in table below are in microseconds]

m/n/time_.Call_R/time_.Call_C++/.Call_overhead/time_.C_R/time_.C_C++/.C_overhead
100/10/3.23/1.33/*1.90*/16.89/0.96/*15.93*
200/15/5.38/3.44/*1.94*/30.77/2.81/*27.96*
500/20/12.48/10.4/*2.08*/78.12/9.37/*68.75*

Interpretation:

1- .Call overhead holds nearly constant, i.e. independent of data size. This
is expected since .Call works by passing pointers. (How can we explain the
slight increase in overhead?)
2- C++ times for .C are somewhat better than .Call. This is likely to be due
to the overhead associated with unpacking the SEXP pointers in a .Call
function.
3- The overhead for .C dominates the execution time. For a 500x20 matrix,
the overhead is ~90% of total time. This means that whenever we need to make
repeated calls to a C/C++ function from R, and when performance is important
to us, .Call is much preferred to .C, even at modest data sizes.
4- Overhead for .C scales sub-linearly with data size. I imagine that this
overhead can be reduced through registering the functions and using the
style field in R_CMethodDef to optimize data transfer (per Section 5.4 of
"Writing R Extensions"), but perhaps not by more than a half.
5- Even using .Call, the overhead is still a significant percentage of total
time, though the contribution decreases with data size (and compute load of
function). However, in the context of parallelization benefits, this
overhead puts an unpleasant cap on achievable gains. For example, even if
the compute time goes down by 100x, if overhead was even 10% of the time,
our effective gain will saturate at 10x. In other words, effective
parallelization can only be achieved by moving big chunks of the code to
C/C++ so as to avoid repeated calls from R to C.

I look forward to your comments.

-Alireza
--------------------------------
#code file: mvMultiply.r
#measuring execution time for a matrix-vector multiplication (A%*%x) using
.C and .Call functions
#execution time is measured both in R and inside the C++ functions;
subtracting the two is 
#used as an indicator of overhead associated with each call

rm(list = ls())
system("R CMD SHLIB mvMultiply.cc")
dyn.load("mvMultiply.so")

m <- 100 #number of rows in matrix A
n <- 10 #number of columns in matrix A (= number of rows in vector x)
N <- 1000000

A <- runif(m*n)
x <- runif(n)

# measuring .Call
tCall_c <- 0.0
t1 <- proc.time()[3]
for (i in 1:N) {
	tCall_c <- tCall_c + .Call("matvecMultiply", as.double(A), as.double(x))
}
tCall_R <- proc.time()[3] - t1
cat(".Call - Time measured in R: ", round(tCall_R,2), "sec\n")
cat(".Call - Time measured in C++: ", round(tCall_c,2), "sec\n")
cat(".Call - Implied overhead: ", round(tCall_R,2) - round(tCall_c,2), "sec 
->  per call: ", 1000000*(round(tCall_R,2) - round(tCall_c,2))/N, "usec\n")

# measuring .C
tC_c <- 0.0
t1 <- proc.time()[3]
for (i in 1:N) {
	tC_c <- tC_c + .C("matvecMultiply2", as.double(A), as.double(x),
double(c(m)), as.integer(c(m)), as.integer(c(n)), t = double(1))$t
}
tC_R <- proc.time()[3] - t1
cat(".C - Time measured in R: ", round(tC_R,2), "sec\n")
cat(".C - Time measured in C++: ", round(tC_c,2), "sec\n")
cat(".C - Implied overhead: ", round(tC_R,2) - round(tC_c,2), "sec  ->  per
call: ", 1000000*(round(tC_R,2) - round(tC_c,2))/N, "usec\n")

dyn.unload("mvMultiply.so")
--------------------------------
#code file: myMultiply.cc
#include <Rinternals.h>
#include <R.h>
#include &lt;sys/time.h&gt;

extern "C" {

SEXP matvecMultiply(SEXP A, SEXP x) {
	timeval time1, time2;
	gettimeofday(&time1, NULL);
	SEXP execTime_sxp;
	PROTECT(execTime_sxp = allocVector(REALSXP, 1));
	double *execTime; execTime = REAL(execTime_sxp);
	double *rA = REAL(A), *rx = REAL(x), *ry;
	int n = length(x);
	int m = length(A) / n;
	SEXP y;
	PROTECT(y = allocVector(REALSXP, m));
	ry = REAL(y);
	for (int i = 0; i < m; i++) {
		ry[i] = 0.0;
		for (int j = 0; j < n; j++) {
			ry[i] += rA[j * m + i] * rx[j];
		}
	}
	UNPROTECT(1);
	gettimeofday(&time2, NULL);
	*execTime = (time2.tv_sec - time1.tv_sec) + (time2.tv_usec -
time1.tv_usec)/1000000.0;
	UNPROTECT(1);
	return execTime_sxp;
}

void matvecMultiply2(double *A, double *x, double *y, int *m_ptr, int
*n_ptr, double *execTime) {
	timeval time1, time2;
	gettimeofday(&time1, NULL);
	int m = *m_ptr;
	int n = *n_ptr;
	for (int i = 0; i < m; i++) {
		y[i] = 0.0;
		for (int j = 0; j < n; j++) {
			y[i] += A[j * m + i] * x[j];
		}
	}
	gettimeofday(&time2, NULL);
	*execTime = (time2.tv_sec - time1.tv_sec) + (time2.tv_usec -
time1.tv_usec)/1000000.0;
}

}
--------------------------------


--
View this message in context: http://r.789695.n4.nabble.com/Measuring-and-comparing-C-and-Call-overhead-tp3678361p3678361.html
Sent from the R devel mailing list archive at Nabble.com.


From bates at stat.wisc.edu  Tue Jul 19 17:09:11 2011
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 19 Jul 2011 10:09:11 -0500
Subject: [Rd] Performance of .C and .Call functions vs. native R code
In-Reply-To: <CAO7JsnQYzcPncw+Z-vbGEMDhX+aL1TeQisT6VJLqTWWhEoYhDg@mail.gmail.com>
References: <1310563735586-3665017.post@n4.nabble.com>
	<05EF8F1E-DC69-4A25-9021-47D9BF841316@gmail.com>
	<1310656867725-3667896.post@n4.nabble.com>
	<CAO7JsnQYzcPncw+Z-vbGEMDhX+aL1TeQisT6VJLqTWWhEoYhDg@mail.gmail.com>
Message-ID: <CAO7JsnQ9GN5N5EaDPZ+WJsz_FyQN4GuN0sPUXFgQ6=nGyrHf=g@mail.gmail.com>

I just saw that I left a syntax error in the .R and the first
_Rout.txt files.  Notice that in the second _Rout.txt file the order
of the arguments in the constructors for the MMatrixXd and the
MVectorXd are in a different order than in the .R and the first
_Rout.txt files.  The correct order has the pointer first, then the
dimensions.  For the first _Rout.txt file this part of the code is not
used.

On Tue, Jul 19, 2011 at 10:00 AM, Douglas Bates <bates at stat.wisc.edu> wrote:
> On Thu, Jul 14, 2011 at 10:21 AM, Alireza Mahani
> <alireza.s.mahani at gmail.com> wrote:
>> (I am using a LINUX machine)
>>
>> Jeff,
>>
>> In creating reproducible results, I 'partially' answered my question. I have
>> attached two scripts, 'mvMultiply.r' and 'mvMultiply.cc'. Please copy both
>> files into your chosen directory, then run 'Rscript mvMultiply.r' in that
>> directory while changing the two boolean parameters 'INCLUDE_DATAPREP' and
>> 'ROWMAJOR' to all four permutations. (The variable 'diffVec' is there to
>> verify that the two methods produce the same output vector.)
>>
>> Below are the results that I get, along with discussion (tR and tCall are in
>> sec):
>>
>> INCLUDE_DATAPREP,ROWMAJOR,tR,tCall
>> F,F,13.536,13.875
>> F,T,13.824,14.299
>> T,F,13.688,18.167
>> T,T,13.982,30.730
>>
>> Interpretation: The execution time for the .Call line is nearly identical to
>> the call to R operator '%*%'. Two data preparation lines for the .Call
>> method create the overhead:
>>
>> A <- t(A) (~12sec, or 12usec per call)
>> dim(A) <- dim(A)[1] * dim(A)[2] (~4sec, or 4usec per call)
>>
>> While the first line can be avoided by providing options in c++ function (as
>> is done in the BLAS API), I wonder if the second line can be avoided, aside
>> from the obvious option of rewriting the R scripts to use vectors instead of
>> matrices. But this defies one of the primary advantages of using R, which is
>> succinct, high-level coding. In particular, if one has several matrices as
>> input into a .Call function, then the overhead from matrix-to-vector
>> transformations can add up. To summarize, my questions are:
>>
>> 1- Do the above results seem reasonable to you? Is there a similar penalty
>> in R's '%*%' operator for transforming matrices to vectors before calling
>> BLAS functions?
>> 2- Are there techniques for reducing the above overhead for developers
>> looking to augment their R code with compiled code?
>>
>> Regards,
>> Alireza
>>
>> ---------------------------------------
>> # mvMultiply.r
>> # comparing performance of matrix multiplication in R (using '%*%' operator)
>> vs. calling compiled code (using .Call function)
>> # y [m x 1] = A [m x n] %*% x [n x 1]
>>
>> rm(list = ls())
>> system("R CMD SHLIB mvMultiply.cc")
>> dyn.load("mvMultiply.so")
>>
>> INCLUDE_DATAPREP <- F
>> ROWMAJOR <- F #indicates whether the c++ function treats A in a row-major or
>> column-major fashion
>>
>> m <- 100
>> n <- 10
>> N <- 1000000
>>
>> diffVec <- array(0, dim = N)
>> tR <- 0.0
>> tCall <- 0.0
>> for (i in 1:N) {
>>
>> ? ? ? ?A <- runif(m*n); dim(A) <- c(m,n)
>> ? ? ? ?x <- runif(n)
>>
>> ? ? ? ?t1 <- proc.time()[3]
>> ? ? ? ?y1 <- A %*% x
>> ? ? ? ?tR <- tR + proc.time()[3] - t1
>>
>> ? ? ? ?if (INCLUDE_DATAPREP) {
>> ? ? ? ? ? ? ? ?t1 <- proc.time()[3]
>> ? ? ? ?}
>> ? ? ? ?if (ROWMAJOR) { #since R will convert matrix to vector in a column-major
>> fashion, if the c++ function expects a row-major format, we need to
>> transpose A before converting it to a vector
>> ? ? ? ? ? ? ? ?A <- t(A)
>> ? ? ? ?}
>> ? ? ? ?dim(A) <- dim(A)[1] * dim(A)[2]
>> ? ? ? ?if (!INCLUDE_DATAPREP) {
>> ? ? ? ? ? ? ? ?t1 <- proc.time()[3]
>> ? ? ? ?}
>> ? ? ? ?y2 <- .Call("matvecMultiply", as.double(A), as.double(x),
>> as.logical(c(ROWMAJOR)))
>> ? ? ? ?tCall <- tCall + proc.time()[3] - t1
>>
>> ? ? ? ?diffVec[i] <- max(abs(y2 - y1))
>> }
>> cat("Data prep time for '.Call' included: ", INCLUDE_DATAPREP, "\n")
>> cat("C++ function expects row-major matrix: ", ROWMAJOR, "\n")
>> cat("Time - Using '%*%' operator in R: ", tR, "sec\n")
>> cat("Time - Using '.Call' function: ", tCall, "sec\n")
>> cat("Maximum difference between methods: ", max(diffVec), "\n")
>>
>> dyn.unload("mvMultiply.so")
>> ---------------------------------------
>> # mvMultiply.cc
>> #include <Rinternals.h>
>> #include <R.h>
>>
>> extern "C" {
>>
>> SEXP matvecMultiply(SEXP A, SEXP x, SEXP rowmajor) {
>> ? ? ? ?double *rA = REAL(A), *rx = REAL(x), *ry;
>> ? ? ? ?int *rrm = LOGICAL(rowmajor);
>> ? ? ? ?int n = length(x);
>> ? ? ? ?int m = length(A) / n;
>> ? ? ? ?SEXP y;
>> ? ? ? ?PROTECT(y = allocVector(REALSXP, m));
>> ? ? ? ?ry = REAL(y);
>> ? ? ? ?for (int i = 0; i < m; i++) {
>> ? ? ? ? ? ? ? ?ry[i] = 0.0;
>> ? ? ? ? ? ? ? ?for (int j = 0; j < n; j++) {
>> ? ? ? ? ? ? ? ? ? ? ? ?if (rrm[0] == 1) {
>> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?ry[i] += rA[i * n + j] * rx[j];
>> ? ? ? ? ? ? ? ? ? ? ? ?} else {
>> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?ry[i] += rA[j * m + i] * rx[j];
>> ? ? ? ? ? ? ? ? ? ? ? ?}
>> ? ? ? ? ? ? ? ?}
>> ? ? ? ?}
>> ? ? ? ?UNPROTECT(1);
>> ? ? ? ?return(y);
>> }
>>
>> }
>>
>
> I realize that you are just beginning to use the .C and .Call
> interfaces and your example is therefore a simple one. ?However, if
> you plan to continue with such development it is worthwhile learning
> of some of the tools available. ?I think one of the most important is
> the "inline" package that can take a C or C++ code segment as a text
> string and go through all the steps of creating and loading a
> .Call'able compiled function.
>
> Second, if you are going to use C++ (the code you show could be C code
> as it doesn't use any C++ extensions) then you should look at the Rcpp
> package written by Dirk Eddelbuettel and Romain Francois which allows
> for comparatively painless interfacing of R objects and C++ objects.
> The Rcpp-devel list, which I have copied on this reply, is for
> questions related to that system. ?The inline package allows for
> various "plugin" constructions to wrap your code in the appropriate
> headers and point the compiler to the locations of header files and
> libraries. ?There are two extensions to Rcpp for numerical linear
> algebra in C++, RcppArmadillo and RcppEigen. ?I show the use of
> RcppEigen here.
>
> Third there are several packages in R that do the busy work of
> benchmarking expressions and neatly formulating the results. ?I use
> the rbenchmark package.
>
> Putting all these together yields the enclosed script and results.
>
> In Eigen, a MatrixXd object is the equivalent of R's numeric matrix
> (similarly MatrixXi for integer and MatrixXcd for complex) and a
> VectorXd object is the equivalent of a numeric vector. ?A "mapped"
> matrix or vector is one that uses the storage allocated by R, thereby
> avoiding a copy operation (similar to your accessing elements of the
> arrays through the pointer returned by REAL()). ?To adhere to R's
> functional programming semantics it is a good idea to declare such
> objects as const. ?The 'as' and 'wrap' functions are provided by Rcpp
> with extensions in RcppEigen to the Eigen classes in the development
> version. ?In the released versions of Rcpp and RcppEigen we use
> intermediate Rcpp objects. These functions have the advantage of
> checking the types of R objects being passed. ?The Eigen code for
> matrix multiplication will check the consistency of dimensions in the
> operation.
>
> Given the size of the matrix you are working with it is not surprising
> that interpretation overhead and checking will be a large part of the
> elapsed time, hence the relative differences between different methods
> of doing the numerical calculation will be small. ?The operation of
> multiplying a 100 x 10 matrix by a 10-vector involves "only" 1000
> floating point operations. ?Furthermore, each element of the matrix is
> used only once so sophisticated methods of manipulating cache contents
> won't buy you much. ?These benchmark results are from a system that
> uses Atlas BLAS (basic linear algebra subroutines); other systems will
> provide different results. ?Interestingly, I found on some systems
> using R's BLAS, which are not accelerated, the R code is closer in
> speed to the code using Eigen. ?An example is given in the second
> version of the output.
>


From simon.urbanek at r-project.org  Tue Jul 19 18:56:15 2011
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Tue, 19 Jul 2011 12:56:15 -0400
Subject: [Rd] Tesla GPUs [Was: Manipulating single-precision (float) arrays
	in .Call functions]
In-Reply-To: <alpine.LFD.2.02.1107190640280.28269@gannet.stats.ox.ac.uk>
References: <1311004330702-3675684.post@n4.nabble.com>
	<4E245D01.1010407@gmail.com>
	<1311027322959-3676667.post@n4.nabble.com>
	<CC044BB2-F9EA-440F-8A64-5B4639B2A529@r-project.org>
	<1311048424023-3677232.post@n4.nabble.com>
	<alpine.LFD.2.02.1107190640280.28269@gannet.stats.ox.ac.uk>
Message-ID: <C1E35915-690A-4FA6-93A5-DEA2FE3CE749@r-project.org>


On Jul 19, 2011, at 2:26 AM, Prof Brian Ripley wrote:

> On Mon, 18 Jul 2011, Alireza Mahani wrote:
> 
>> Simon,
>> 
>> Thank you for elaborating on the limitations of R in handling float types. I
>> think I'm pretty much there with you.
>> 
>> As for the insufficiency of single-precision math (and hence limitations of
>> GPU), my personal take so far has been that double-precision becomes crucial
>> when some sort of error accumulation occurs. For example, in differential
>> equations where boundary values are integrated to arrive at interior values,
>> etc. On the other hand, in my personal line of work (Hierarchical Bayesian
>> models for quantitative marketing), we have so much inherent uncertainty and
>> noise at so many levels in the problem (and no significant error
>> accumulation sources) that single vs double precision issue is often
>> inconsequential for us. So I think it really depends on the field as well as
>> the nature of the problem.
> 
> The main reason to use only double precision in R was that on modern CPUs double precision calculations are as fast as single-precision ones, and with 64-bit CPUs they are a single access.  So the extra precision comes more-or-less for free.  You also under-estimate the extent to which stability of commonly used algorithms relies on double precision.  (There are stable single-precision versions, but they are no longer commonly used.  And as Simon said, in some cases stability is ensured by using extra precision where available.)
> 
> I disagree slightly with Simon on GPUs: I am told by local experts that the double-precision on the latest GPUs (those from the last year or so) is perfectly usable.  See the performance claims on http://en.wikipedia.org/wiki/Nvidia_Tesla of about 50% of the SP performance in DP.
> 

That would be good news. Unfortunately those seem to be still targeted at a specialized market and are not really graphics cards in traditional sense. Although this is sort of required for the purpose it removes the benefit of ubiquity. So, yes, I agree with you that it may be an interesting way forward, but I fear it's too much of a niche to be widely supported. I may want to ask our GPU specialists here to see if they have any around so I could re-visit our OpenCL R benchmarks. Last time we abandoned our OpenCL R plans exactly due to the lack of speed in double precision.

Thanks,
Simon


From simon.urbanek at r-project.org  Tue Jul 19 20:47:46 2011
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Tue, 19 Jul 2011 14:47:46 -0400
Subject: [Rd] Measuring and comparing .C and .Call overhead
In-Reply-To: <1311088055629-3678361.post@n4.nabble.com>
References: <1311088055629-3678361.post@n4.nabble.com>
Message-ID: <0A34662C-DC53-4AA8-904D-5A8FAA1CA70E@r-project.org>


On Jul 19, 2011, at 11:07 AM, Alireza Mahani wrote:

> Further pursuing my curiosity to measure the efficiency of R/C++ interface, I
> conducted a simple matrix-vector multiplication test using .C and .Call
> functions in R. In each case, I measured the execution time in R, as well as
> inside the C++ function. Subtracting the two, I came up with a measure of
> overhead associated with each call. I assume that this overhead would be
> non-existent of the entire code was implemented in C++. [Please let me know
> if my approach is somehow faulty.] The results of this simple test have
> significant implications for my software development approach. I hope others
> find this information useful as well. I have attached the code files in the
> bottom. Below I provide a summary output, and interpret the results. [All
> times in table below are in microseconds]
> 
> m/n/time_.Call_R/time_.Call_C++/.Call_overhead/time_.C_R/time_.C_C++/.C_overhead
> 100/10/3.23/1.33/*1.90*/16.89/0.96/*15.93*
> 200/15/5.38/3.44/*1.94*/30.77/2.81/*27.96*
> 500/20/12.48/10.4/*2.08*/78.12/9.37/*68.75*
> 

Your "overhead" calculations are anything but since they include a loop, value replacement, arithmetics, unnecessary function calls (you don't need as.double() for .Call - it's more efficient to use coerceVector() or throw an error depending on what you desire) - that have nothing to do with .Call.

In addition, if you really cared about overhead, you would avoid symbol lookup millions of times:

foo.c:
#include <Rinternals.h>
SEXP foo() { return R_NilValue; }

> system.time(for(i in 1:1e6) .Call("foo"))
   user  system elapsed 
  4.943   0.016   4.960 
> foo = getNativeSymbolInfo("foo")$address
> system.time(for(i in 1:1e6) .Call(foo))
   user  system elapsed 
  0.363   0.001   0.365 

This is very close to just calling a constant closure with the same result:

> f=function() NULL
> system.time(for(i in 1:1e6) f())
   user  system elapsed 
  0.238   0.001   0.239 

So .Call overhead is essentially negligible - and not what you were measuring ;).

There is only a tiny overhead in forcing the argument promise, but it's constant:

> x=rnorm(1e6)
> system.time(for(i in 1:1e6) .Call(foo,x))
   user  system elapsed 
  0.455   0.001   0.456 
> x=rnorm(1e9)
> system.time(for(i in 1:1e6) .Call(foo,x))
   user  system elapsed 
  0.454   0.000   0.455 

and it's again no different that for a closure:
> f=function(x) NULL
> system.time(for(i in 1:1e6) f())
   user  system elapsed 
  0.259   0.001   0.259 
> system.time(for(i in 1:1e6) f(x))
   user  system elapsed 
  0.329   0.001   0.329 

so this is inherent to any call. .Call is the fastest you can get, there is nothing faster (other than hacking your code into R internals...).

Cheers,
Simon


> Interpretation:
> 
> 1- .Call overhead holds nearly constant, i.e. independent of data size. This
> is expected since .Call works by passing pointers. (How can we explain the
> slight increase in overhead?)
> 2- C++ times for .C are somewhat better than .Call. This is likely to be due
> to the overhead associated with unpacking the SEXP pointers in a .Call
> function.
> 3- The overhead for .C dominates the execution time. For a 500x20 matrix,
> the overhead is ~90% of total time. This means that whenever we need to make
> repeated calls to a C/C++ function from R, and when performance is important
> to us, .Call is much preferred to .C, even at modest data sizes.
> 4- Overhead for .C scales sub-linearly with data size. I imagine that this
> overhead can be reduced through registering the functions and using the
> style field in R_CMethodDef to optimize data transfer (per Section 5.4 of
> "Writing R Extensions"), but perhaps not by more than a half.
> 5- Even using .Call, the overhead is still a significant percentage of total
> time, though the contribution decreases with data size (and compute load of
> function). However, in the context of parallelization benefits, this
> overhead puts an unpleasant cap on achievable gains. For example, even if
> the compute time goes down by 100x, if overhead was even 10% of the time,
> our effective gain will saturate at 10x. In other words, effective
> parallelization can only be achieved by moving big chunks of the code to
> C/C++ so as to avoid repeated calls from R to C.
> 
> I look forward to your comments.
> 
> -Alireza
> --------------------------------
> #code file: mvMultiply.r
> #measuring execution time for a matrix-vector multiplication (A%*%x) using
> .C and .Call functions
> #execution time is measured both in R and inside the C++ functions;
> subtracting the two is 
> #used as an indicator of overhead associated with each call
> 
> rm(list = ls())
> system("R CMD SHLIB mvMultiply.cc")
> dyn.load("mvMultiply.so")
> 
> m <- 100 #number of rows in matrix A
> n <- 10 #number of columns in matrix A (= number of rows in vector x)
> N <- 1000000
> 
> A <- runif(m*n)
> x <- runif(n)
> 
> # measuring .Call
> tCall_c <- 0.0
> t1 <- proc.time()[3]
> for (i in 1:N) {
> 	tCall_c <- tCall_c + .Call("matvecMultiply", as.double(A), as.double(x))
> }
> tCall_R <- proc.time()[3] - t1
> cat(".Call - Time measured in R: ", round(tCall_R,2), "sec\n")
> cat(".Call - Time measured in C++: ", round(tCall_c,2), "sec\n")
> cat(".Call - Implied overhead: ", round(tCall_R,2) - round(tCall_c,2), "sec 
> ->  per call: ", 1000000*(round(tCall_R,2) - round(tCall_c,2))/N, "usec\n")
> 
> # measuring .C
> tC_c <- 0.0
> t1 <- proc.time()[3]
> for (i in 1:N) {
> 	tC_c <- tC_c + .C("matvecMultiply2", as.double(A), as.double(x),
> double(c(m)), as.integer(c(m)), as.integer(c(n)), t = double(1))$t
> }
> tC_R <- proc.time()[3] - t1
> cat(".C - Time measured in R: ", round(tC_R,2), "sec\n")
> cat(".C - Time measured in C++: ", round(tC_c,2), "sec\n")
> cat(".C - Implied overhead: ", round(tC_R,2) - round(tC_c,2), "sec  ->  per
> call: ", 1000000*(round(tC_R,2) - round(tC_c,2))/N, "usec\n")
> 
> dyn.unload("mvMultiply.so")
> --------------------------------
> #code file: myMultiply.cc
> #include <Rinternals.h>
> #include <R.h>
> #include &lt;sys/time.h&gt;
> 
> extern "C" {
> 
> SEXP matvecMultiply(SEXP A, SEXP x) {
> 	timeval time1, time2;
> 	gettimeofday(&time1, NULL);
> 	SEXP execTime_sxp;
> 	PROTECT(execTime_sxp = allocVector(REALSXP, 1));
> 	double *execTime; execTime = REAL(execTime_sxp);
> 	double *rA = REAL(A), *rx = REAL(x), *ry;
> 	int n = length(x);
> 	int m = length(A) / n;
> 	SEXP y;
> 	PROTECT(y = allocVector(REALSXP, m));
> 	ry = REAL(y);
> 	for (int i = 0; i < m; i++) {
> 		ry[i] = 0.0;
> 		for (int j = 0; j < n; j++) {
> 			ry[i] += rA[j * m + i] * rx[j];
> 		}
> 	}
> 	UNPROTECT(1);
> 	gettimeofday(&time2, NULL);
> 	*execTime = (time2.tv_sec - time1.tv_sec) + (time2.tv_usec -
> time1.tv_usec)/1000000.0;
> 	UNPROTECT(1);
> 	return execTime_sxp;
> }
> 
> void matvecMultiply2(double *A, double *x, double *y, int *m_ptr, int
> *n_ptr, double *execTime) {
> 	timeval time1, time2;
> 	gettimeofday(&time1, NULL);
> 	int m = *m_ptr;
> 	int n = *n_ptr;
> 	for (int i = 0; i < m; i++) {
> 		y[i] = 0.0;
> 		for (int j = 0; j < n; j++) {
> 			y[i] += A[j * m + i] * x[j];
> 		}
> 	}
> 	gettimeofday(&time2, NULL);
> 	*execTime = (time2.tv_sec - time1.tv_sec) + (time2.tv_usec -
> time1.tv_usec)/1000000.0;
> }
> 
> }
> --------------------------------
> 
> 
> --
> View this message in context: http://r.789695.n4.nabble.com/Measuring-and-comparing-C-and-Call-overhead-tp3678361p3678361.html
> Sent from the R devel mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 
> 


From alireza.s.mahani at gmail.com  Tue Jul 19 21:13:43 2011
From: alireza.s.mahani at gmail.com (Alireza Mahani)
Date: Tue, 19 Jul 2011 12:13:43 -0700 (PDT)
Subject: [Rd] Performance of .C and .Call functions vs. native R code
In-Reply-To: <CAO7JsnQ9GN5N5EaDPZ+WJsz_FyQN4GuN0sPUXFgQ6=nGyrHf=g@mail.gmail.com>
References: <1310563735586-3665017.post@n4.nabble.com>
	<05EF8F1E-DC69-4A25-9021-47D9BF841316@gmail.com>
	<1310656867725-3667896.post@n4.nabble.com>
	<CAO7JsnQYzcPncw+Z-vbGEMDhX+aL1TeQisT6VJLqTWWhEoYhDg@mail.gmail.com>
	<CAO7JsnQ9GN5N5EaDPZ+WJsz_FyQN4GuN0sPUXFgQ6=nGyrHf=g@mail.gmail.com>
Message-ID: <1311102823361-3679056.post@n4.nabble.com>

Prof. Bates,

It looks like you read my mind! I am working on writing an R package for
high-performance MCMC estimation of a class of Hierarchical Bayesian models
most often used in the field of quantitative marketing. This would
essentially be a parallelized version of Peter Rossi's bayesm package. While
I've made great progress in parallelizing the most mathematically difficult
part of the algorithm, namely slice sampling of low-level coefficients, yet
I've realized that putting the entire code together while minimizing bugs is
a big challenge in C/C++/CUDA environments. I have therefore decided to
follow a more logical path of first developing the code logic in R, and then
exporting it function by function to compiled code. 

The tools that you mentioned seem to be exactly the kind of stuff I need in
order to be able to do go through this incremental, test-oriented
development process with relatively little pain.

I'm not sure if this is what you had in mind while suggesting the tools to
me, so please let me know if I'm misinterpreting your comments, or if I need
to be aware of other tools beyond what you mentioned.

Many thanks,
Alireza


--
View this message in context: http://r.789695.n4.nabble.com/Performance-of-C-and-Call-functions-vs-native-R-code-tp3665017p3679056.html
Sent from the R devel mailing list archive at Nabble.com.


From hpages at fhcrc.org  Tue Jul 19 21:54:10 2011
From: hpages at fhcrc.org (=?ISO-8859-1?Q?Herv=E9_Pag=E8s?=)
Date: Tue, 19 Jul 2011 12:54:10 -0700
Subject: [Rd] Confusing inheritance problem
In-Reply-To: <1311005770.12897.21.camel@nemo>
References: <1310765027.11629.48.camel@nemo>	<4E21CA0A.1050107@statistik.tu-dortmund.de>
	<1311005770.12897.21.camel@nemo>
Message-ID: <4E25E0E2.3000601@fhcrc.org>

Hi Terry,

You use a NAMESPACE but you don't import Matrix.
So it looks like the "rowSums" method for CsparseMatrix objects
cannot be found (not sure why because you do have Matrix in the
Depends field so the rowSums generic and methods should be in the
search path).

Anyway, just adding

import(Matrix)

to the NAMESPACE file solves the problem for me.

Cheers,
H.

On 11-07-18 09:16 AM, Terry Therneau wrote:
>   I've packaged the test library up as a tar file at
> 	ftp.mayo.edu
> 	directory therneau, file ktest.tar
> 	login username: mayoftp
>               password:  KPlLiFoz
> This will disappear in 3 days (Mayo is very fussy about outside access).
>
> In response to Uwe's comments
>    1. "2.13.0" is not recent
>      It's not the latest, but it is recent.  This is for machines at work where
> where upgrades happen more infrequently
>    2. "Matrix not loaded"
> The sessionInfo was only to show what version we have.  Forgetting to load Matrix
> isn't the problem -- when I do that the error is quick and obvious.
>
>   Thanks in advance for any pointers.
>
> Terry T.
>
>
>
> On Sat, 2011-07-16 at 19:27 +0200, Uwe Ligges wrote:
>>
>> On 15.07.2011 23:23, Terry Therneau wrote:
>>>    I have library in development with a function that works when called
>>> from the top level, but fails under R CMD check.  The paricular line of
>>> failure is
>>> 	rsum<- rowSums(kmat>0)
>>> where kmat is a dsCMatrix object.
>>>
>>>     I'm currently stumped and looking for some ideas.
>>>
>>>     I've created a stripped down library "ktest" that has only 3
>>> functions: pedigree.R to create a pedigree or pedigreeList object,
>>> 	   kinship.R with "kinship" methods for the two objects
>>> 	   one small compute function called by the others
>>> along with the minimal amount of other information such that a call to
>>>      R --vanilla CMD check ktest
>>> gives no errors until the fatal one.
>>>
>>>    There are two test cases.  A 3 line one that creates a dsCMatrix and
>>> call rowSums at the top level works fine, but the same call inside the
>>> kmat.pedigreeList function gives an error
>>>           'x' must be an array of at least two dimensions
>>> Adding a print statement above the rowSums call shows that the argument
>>> is a 14 by 14 dsCMatrix.
>>>
>>>    I'm happy to send the library to anyone else to try and duplicate.
>>>       Terry Therneau
>>>
>>> tmt% R --vanilla
>>>
>>>> sessionInfo()
>>> R version 2.13.0 (2011-04-13)
>>> Platform: x86_64-unknown-linux-gnu (64-bit)
>>>
>>> locale:
>>>    [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
>>>    [3] LC_TIME=en_US.UTF-8        LC_COLLATE=C
>>>    [5] LC_MONETARY=C              LC_MESSAGES=en_US.UTF-8
>>>    [7] LC_PAPER=en_US.UTF-8       LC_NAME=C
>>>    [9] LC_ADDRESS=C               LC_TELEPHONE=C
>>> [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C
>>>
>>> attached base packages:
>>> [1] stats     graphics  grDevices utils     datasets  methods
>>> base
>>
>>
>>
>> Terry,
>>
>> 1. Your R is not recent.
>> 2. You do this without having Matrix loaded (according to
>> sessionInfo())? This may already be the cause of your problems.
>> 3. You may want to make your package available on some website. I am
>> sure there are people who will take a look (including me, but not today).
>>
>> Best wishes,
>> Uwe
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fhcrc.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From therneau at mayo.edu  Tue Jul 19 22:34:29 2011
From: therneau at mayo.edu (Therneau, Terry M., Ph.D.)
Date: Tue, 19 Jul 2011 15:34:29 -0500
Subject: [Rd] Confusing inheritance problem
In-Reply-To: <4E25E0E2.3000601@fhcrc.org>
References: <1310765027.11629.48.camel@nemo>	<4E21CA0A.1050107@statistik.tu-dortmund.de>
	<1311005770.12897.21.camel@nemo> <4E25E0E2.3000601@fhcrc.org>
Message-ID: <5DBE21404FF5E54C826BE1040C2CD03101700914@msgebe52.mfad.mfroot.org>


 Yup, that fixes the problem.  This was also pointed out by Uwe in a personal
email.
  One confusing aspect is that there are two test files in the skeleton package, one of 
which calls rowSums( a dsCMatrix object) at top level, and the other of which does the
same thing within a dispached method.  Only the second one fails.  An interesting puzzle, but not one I'll pursue with any vigor now that we have a fix.

Terry T.

-----Original Message-----
From: Herv? Pag?s [mailto:hpages at fhcrc.org] 
Sent: Tuesday, July 19, 2011 2:54 PM
To: Therneau, Terry M., Ph.D.
Cc: r-devel at r-project.org
Subject: Re: [Rd] Confusing inheritance problem

Hi Terry,

You use a NAMESPACE but you don't import Matrix.
So it looks like the "rowSums" method for CsparseMatrix objects
cannot be found (not sure why because you do have Matrix in the
Depends field so the rowSums generic and methods should be in the
search path).

Anyway, just adding

import(Matrix)

to the NAMESPACE file solves the problem for me.

Cheers,
H.

On 11-07-18 09:16 AM, Terry Therneau wrote:
>   I've packaged the test library up as a tar file at
> 	ftp.mayo.edu
> 	directory therneau, file ktest.tar
> 	login username: mayoftp
>               password:  KPlLiFoz
> This will disappear in 3 days (Mayo is very fussy about outside access).
>
> In response to Uwe's comments
>    1. "2.13.0" is not recent
>      It's not the latest, but it is recent.  This is for machines at work where
> where upgrades happen more infrequently
>    2. "Matrix not loaded"
> The sessionInfo was only to show what version we have.  Forgetting to load Matrix
> isn't the problem -- when I do that the error is quick and obvious.
>
>   Thanks in advance for any pointers.
>
> Terry T.
>
>
>
> On Sat, 2011-07-16 at 19:27 +0200, Uwe Ligges wrote:
>>
>> On 15.07.2011 23:23, Terry Therneau wrote:
>>>    I have library in development with a function that works when called
>>> from the top level, but fails under R CMD check.  The paricular line of
>>> failure is
>>> 	rsum<- rowSums(kmat>0)
>>> where kmat is a dsCMatrix object.
>>>
>>>     I'm currently stumped and looking for some ideas.
>>>
>>>     I've created a stripped down library "ktest" that has only 3
>>> functions: pedigree.R to create a pedigree or pedigreeList object,
>>> 	   kinship.R with "kinship" methods for the two objects
>>> 	   one small compute function called by the others
>>> along with the minimal amount of other information such that a call to
>>>      R --vanilla CMD check ktest
>>> gives no errors until the fatal one.
>>>
>>>    There are two test cases.  A 3 line one that creates a dsCMatrix and
>>> call rowSums at the top level works fine, but the same call inside the
>>> kmat.pedigreeList function gives an error
>>>           'x' must be an array of at least two dimensions
>>> Adding a print statement above the rowSums call shows that the argument
>>> is a 14 by 14 dsCMatrix.
>>>
>>>    I'm happy to send the library to anyone else to try and duplicate.
>>>       Terry Therneau
>>>
>>> tmt% R --vanilla
>>>
>>>> sessionInfo()
>>> R version 2.13.0 (2011-04-13)
>>> Platform: x86_64-unknown-linux-gnu (64-bit)
>>>
>>> locale:
>>>    [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
>>>    [3] LC_TIME=en_US.UTF-8        LC_COLLATE=C
>>>    [5] LC_MONETARY=C              LC_MESSAGES=en_US.UTF-8
>>>    [7] LC_PAPER=en_US.UTF-8       LC_NAME=C
>>>    [9] LC_ADDRESS=C               LC_TELEPHONE=C
>>> [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C
>>>
>>> attached base packages:
>>> [1] stats     graphics  grDevices utils     datasets  methods
>>> base
>>
>>
>>
>> Terry,
>>
>> 1. Your R is not recent.
>> 2. You do this without having Matrix loaded (according to
>> sessionInfo())? This may already be the cause of your problems.
>> 3. You may want to make your package available on some website. I am
>> sure there are people who will take a look (including me, but not today).
>>
>> Best wishes,
>> Uwe
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fhcrc.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From tim.triche at gmail.com  Tue Jul 19 16:44:33 2011
From: tim.triche at gmail.com (Tim Triche, Jr.)
Date: Tue, 19 Jul 2011 07:44:33 -0700
Subject: [Rd] hanging spaces prior to linebreak from cat()
Message-ID: <CAC+N9BVVhujS93qFqLAUiY_=gVRVTCeuDrb4jH2zju6K0hgC+Q@mail.gmail.com>

(re-sending after confirming list subscription; apologies if this ends up
being sent to the list twice)

Is the expected behavior from cat(), as used below, a hanging space before
\n at the end of the emitted line?

 firstheader = gsub("\\s+$", "", paste(c("Hybridization REF", s, s),
collapse = "\t"))
 cat(firstheader, "\n", file = filename)

When I run the above code, which is followed by appending the contents of a
data.frame via write.table(), what I get is a file whose first line ends in
a " \n" (note the space preceding the newline).  I would think that the
gsub() would have trimmed off this hanging space, but it doesn't.  Is this a
strange bug or expected behavior?  I have attached the version of cat.R in
from the SVN checkout I am running as my R build; it doesn't seem awry...
?!?

> sessionInfo()
R version 2.14.0 Under development (unstable) (2011-04-21 r55577)
Platform: x86_64-unknown-linux-gnu (64-bit)

locale:
 [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
 [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
 [5] LC_MONETARY=C              LC_MESSAGES=C
 [7] LC_PAPER=en_US.UTF-8       LC_NAME=C
 [9] LC_ADDRESS=C               LC_TELEPHONE=C
[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C

attached base packages:
[1] stats     graphics  grDevices datasets  utils     methods   base

other attached packages:
 [1] EGC.tools_1.0
IlluminaHumanMethylation27k.db_1.4.6
 [3] IlluminaHumanMethylation450k.db_1.4.6 org.Hs.eg.db_2.5.0
 [5] RSQLite_0.9-4                         DBI_0.2-5
 [7] AnnotationDbi_1.15.7                  Biobase_2.13.2
 [9] gtools_2.6.2                          reshape_0.8.4
[11] plyr_1.4

loaded via a namespace (and not attached):
[1] annotate_1.31.0   grid_2.14.0       lattice_0.19-17   matrixStats_0.2.2
[5] methylumi_1.9.5   xtable_1.5-6


-- 
If people do not believe that mathematics is simple, it is only because they
do not realize how complicated life is.
John von Neumann<http://www-groups.dcs.st-and.ac.uk/~history/Biographies/Von_Neumann.html>

From jeroen.ooms at stat.ucla.edu  Tue Jul 19 15:13:01 2011
From: jeroen.ooms at stat.ucla.edu (jeroen00ms)
Date: Tue, 19 Jul 2011 06:13:01 -0700 (PDT)
Subject: [Rd] Randomness not due to seed
Message-ID: <1311081181602-3678082.post@n4.nabble.com>

I am working on a reproducible computing platform for which I would like to
be able to _exactly_ reproduce an R object. However, I am experiencing
unexpected randomness in some calculations. I have a hard time finding out
exactly how it occurs. The code below illustrates the issue. 

mylm1 <- lm(dist~speed, data=cars);
mylm2 <- lm(dist~speed, data=cars);
identical(mylm1, mylm2); #TRUE

makelm <- function(){
	return(lm(dist~speed, data=cars));
}

mylm1 <- makelm();
mylm2 <- makelm();
identical(mylm1, mylm2); #FALSE

When inspecting both objects there seem to be some rounding differences.
Setting a seed does not make a difference. Is there any way I can remove
this randomness and exactly reproduce the object every time?





--
View this message in context: http://r.789695.n4.nabble.com/Randomness-not-due-to-seed-tp3678082p3678082.html
Sent from the R devel mailing list archive at Nabble.com.


From ralf.kellner at wiso.uni-erlangen.de  Tue Jul 19 19:33:34 2011
From: ralf.kellner at wiso.uni-erlangen.de (Kellner)
Date: Tue, 19 Jul 2011 10:33:34 -0700 (PDT)
Subject: [Rd] Improved Nelder-Mead algorithm - a potential replacement
 for optim's Nelder-Mead
In-Reply-To: <f4e199bfbcbf.4b912290@johnshopkins.edu>
References: <f4e199bfbcbf.4b912290@johnshopkins.edu>
Message-ID: <1311096814106-3678732.post@n4.nabble.com>

Hi,

i have a question concerning the Nelder-Mead algorithm in R. As far as i can
see, the shrink operation is not included in the optim() function. Does
anyone know an implementation of the Nelder-Mead algorithm including this
operation in R? Could maybe someone send me one? I would try to write the
code on my own, but due to my doctor thesis i am a little under time
pressure and would appreciate any help. Thank you in advance!

Greetings
Ralf

--
View this message in context: http://r.789695.n4.nabble.com/Improved-Nelder-Mead-algorithm-a-potential-replacement-for-optim-s-Nelder-Mead-tp1580101p3678732.html
Sent from the R devel mailing list archive at Nabble.com.


From wdunlap at tibco.com  Wed Jul 20 00:06:11 2011
From: wdunlap at tibco.com (William Dunlap)
Date: Tue, 19 Jul 2011 22:06:11 +0000
Subject: [Rd] Randomness not due to seed
In-Reply-To: <1311081181602-3678082.post@n4.nabble.com>
References: <1311081181602-3678082.post@n4.nabble.com>
Message-ID: <E66794E69CFDE04D9A70842786030B9301002E@PA-MBX04.na.tibco.com>

Did you actually see some rounding differences?

The lm objects made in the calls to maklm will
differ in the environments attached to the formula
(because you made the formula in the function).  If
I change both copies of that .Environment attribute
to .GlobalEnv (or any other environment), then identical
reports the objects are the same:

  > attr(attr(mylm1$model, "terms"), ".Environment") <- .GlobalEnv
  > attr(mylm1$terms, ".Environment") <- .GlobalEnv
  > attr(attr(mylm2$model, "terms"), ".Environment") <- .GlobalEnv
  > attr(mylm2$terms, ".Environment") <- .GlobalEnv
  > identical(mylm1, mylm2)
  [1] TRUE 

Bill Dunlap
Spotfire, TIBCO Software
wdunlap tibco.com 

> -----Original Message-----
> From: r-devel-bounces at r-project.org [mailto:r-devel-bounces at r-project.org] On Behalf Of jeroen00ms
> Sent: Tuesday, July 19, 2011 6:13 AM
> To: r-devel at r-project.org
> Subject: [Rd] Randomness not due to seed
> 
> I am working on a reproducible computing platform for which I would like to
> be able to _exactly_ reproduce an R object. However, I am experiencing
> unexpected randomness in some calculations. I have a hard time finding out
> exactly how it occurs. The code below illustrates the issue.
> 
> mylm1 <- lm(dist~speed, data=cars);
> mylm2 <- lm(dist~speed, data=cars);
> identical(mylm1, mylm2); #TRUE
> 
> makelm <- function(){
> 	return(lm(dist~speed, data=cars));
> }
> 
> mylm1 <- makelm();
> mylm2 <- makelm();
> identical(mylm1, mylm2); #FALSE
> 
> When inspecting both objects there seem to be some rounding differences.
> Setting a seed does not make a difference. Is there any way I can remove
> this randomness and exactly reproduce the object every time?
> 
> 
> 
> 
> 
> --
> View this message in context: http://r.789695.n4.nabble.com/Randomness-not-due-to-seed-
> tp3678082p3678082.html
> Sent from the R devel mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From rvaradhan at jhmi.edu  Wed Jul 20 01:37:18 2011
From: rvaradhan at jhmi.edu (Ravi Varadhan)
Date: Tue, 19 Jul 2011 23:37:18 +0000
Subject: [Rd] Improved Nelder-Mead algorithm - a potential replacement
 for optim's Nelder-Mead
In-Reply-To: <1311096814106-3678732.post@n4.nabble.com>
References: <f4e199bfbcbf.4b912290@johnshopkins.edu>,
	<1311096814106-3678732.post@n4.nabble.com>
Message-ID: <2F9EA67EF9AE1C48A147CB41BE2E15C30431E8@DOM-EB-MAIL1.win.ad.jhu.edu>

Take a look at the `nmk' function in"dfoptim" package.

Ravi.
________________________________________
From: r-devel-bounces at r-project.org [r-devel-bounces at r-project.org] on behalf of Kellner [ralf.kellner at wiso.uni-erlangen.de]
Sent: Tuesday, July 19, 2011 1:33 PM
To: r-devel at r-project.org
Subject: Re: [Rd] Improved Nelder-Mead algorithm - a potential replacement for optim's Nelder-Mead

Hi,

i have a question concerning the Nelder-Mead algorithm in R. As far as i can
see, the shrink operation is not included in the optim() function. Does
anyone know an implementation of the Nelder-Mead algorithm including this
operation in R? Could maybe someone send me one? I would try to write the
code on my own, but due to my doctor thesis i am a little under time
pressure and would appreciate any help. Thank you in advance!

Greetings
Ralf

--
View this message in context: http://r.789695.n4.nabble.com/Improved-Nelder-Mead-algorithm-a-potential-replacement-for-optim-s-Nelder-Mead-tp1580101p3678732.html
Sent from the R devel mailing list archive at Nabble.com.

______________________________________________
R-devel at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-devel


From simon.urbanek at r-project.org  Wed Jul 20 03:28:15 2011
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Tue, 19 Jul 2011 21:28:15 -0400
Subject: [Rd] hanging spaces prior to linebreak from cat()
In-Reply-To: <CAC+N9BVVhujS93qFqLAUiY_=gVRVTCeuDrb4jH2zju6K0hgC+Q@mail.gmail.com>
References: <CAC+N9BVVhujS93qFqLAUiY_=gVRVTCeuDrb4jH2zju6K0hgC+Q@mail.gmail.com>
Message-ID: <EAD274FE-8068-477A-9C6F-F7DC4843DA27@r-project.org>


On Jul 19, 2011, at 10:44 AM, Tim Triche, Jr. wrote:

> (re-sending after confirming list subscription; apologies if this ends up
> being sent to the list twice)
> 
> Is the expected behavior from cat(), as used below, a hanging space before
> \n at the end of the emitted line?
> 
> firstheader = gsub("\\s+$", "", paste(c("Hybridization REF", s, s),
> collapse = "\t"))
> cat(firstheader, "\n", file = filename)
> 
> When I run the above code, which is followed by appending the contents of a
> data.frame via write.table(), what I get is a file whose first line ends in
> a " \n" (note the space preceding the newline).

Yes, you use the default sep=' ' (space) so, obviously there will be a space before the newline as requested. If you don't want that then you probably want to use sep=''.

Cheers,
Simon


>  I would think that the
> gsub() would have trimmed off this hanging space, but it doesn't.  Is this a
> strange bug or expected behavior?  I have attached the version of cat.R in
> from the SVN checkout I am running as my R build; it doesn't seem awry...
> ?!?
> 
>> sessionInfo()
> R version 2.14.0 Under development (unstable) (2011-04-21 r55577)
> Platform: x86_64-unknown-linux-gnu (64-bit)
> 
> locale:
> [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
> [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
> [5] LC_MONETARY=C              LC_MESSAGES=C
> [7] LC_PAPER=en_US.UTF-8       LC_NAME=C
> [9] LC_ADDRESS=C               LC_TELEPHONE=C
> [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C
> 
> attached base packages:
> [1] stats     graphics  grDevices datasets  utils     methods   base
> 
> other attached packages:
> [1] EGC.tools_1.0
> IlluminaHumanMethylation27k.db_1.4.6
> [3] IlluminaHumanMethylation450k.db_1.4.6 org.Hs.eg.db_2.5.0
> [5] RSQLite_0.9-4                         DBI_0.2-5
> [7] AnnotationDbi_1.15.7                  Biobase_2.13.2
> [9] gtools_2.6.2                          reshape_0.8.4
> [11] plyr_1.4
> 
> loaded via a namespace (and not attached):
> [1] annotate_1.31.0   grid_2.14.0       lattice_0.19-17   matrixStats_0.2.2
> [5] methylumi_1.9.5   xtable_1.5-6
> 
> 
> -- 
> If people do not believe that mathematics is simple, it is only because they
> do not realize how complicated life is.
> John von Neumann<http://www-groups.dcs.st-and.ac.uk/~history/Biographies/Von_Neumann.html>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From ripley at stats.ox.ac.uk  Wed Jul 20 09:21:07 2011
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 20 Jul 2011 08:21:07 +0100 (BST)
Subject: [Rd] [R] Problem compiling in extra/xdr
In-Reply-To: <alpine.LFD.2.02.1107190631570.28269@gannet.stats.ox.ac.uk>
References: <alpine.LNX.2.00.1107181539100.13110@waverley.Belkin>
	<alpine.LFD.2.02.1107182251020.4163@gannet.stats.ox.ac.uk>
	<alpine.DEB.2.00.1107181954150.5589@myrtle>
	<alpine.LFD.2.02.1107190631570.28269@gannet.stats.ox.ac.uk>
Message-ID: <alpine.LFD.2.02.1107200812040.1173@gannet.stats.ox.ac.uk>

1) The current R-patched should compile src/extra/xdr on 32-bit Linux 
systems.

2) Longer-term or on a 64-bit platform the solution is to make use of 
libtirpc: you would need both this installed (common now) and its 
headers (unlikely).

Then if you arrange for /usr/include/tirpc to be in the C include path 
(not easy to do portably, but using C_INCLUDE_PATH will work on 
Linux), the tirpc versions should be found.

On Tue, 19 Jul 2011, Prof Brian Ripley wrote:

> On Mon, 18 Jul 2011, Allin Cottrell wrote:
>
>> On Mon, 18 Jul 2011, Prof Brian Ripley wrote:
>> 
>>> 1) R-help is the wrong list: see the posting guide.  I've moved this to 
>>> R-devel.
>>> 
>>> 2) A glibc system should not be compiling in that directory.  glibc 2.14 
>>> is rather recent and NEWS does say
>>> 
>>> * The RPC implementation in libc is obsoleted.  Old programs keep working
>>>  but new programs cannot be linked with the routines in libc anymore.
>>>  Programs in need of RPC functionality must be linked against TI-RPC.
>>>  The TI-RPC implementation is IPv6 enabled and there are other benefits.
>>>
>>>  Visible changes of this change include (obviously) the inability to link
>>>  programs using RPC functions without referencing the TI-RPC library and 
>>> the
>>>  removal of the RPC headers from the glibc headers.
>>>  Implemented by Ulrich Drepper.
>>> 
>>> So the answer seems to be that your libc is too new.
>> 
>> OK, thanks. I should have remembered the info about RPC in the glibc-2.14 
>> news. Then there will presumably be a problem building current R on current 
>> Fedora?
>
> What is 'current Fedora'?  glibc 2.14 postdates the current release, Fedora 
> 15, which uses 2.13.   I do not know what Fedora 16 will use in several 
> months ....
>
> The main problem will be that the xdr included in R is only for platforms 
> with 32-bit longs -- but that may be true for your i686 Linux.  It needs 
> _X86_ defined to compile for i686: I would have expected that to be true on 
> your platform, but am testing a cleaned-up version.  If that works it will 
> appear in R-patched within 24 hours.
>
>> 
>>> On Mon, 18 Jul 2011, Allin Cottrell wrote:
>>> 
>>>> I'm building R 2.13.1 on i686-pc-linux-gnu, using gcc 4.6.1
>>>> and with glibc 2.14.
>>>> 
>>>> I get this error:
>>>> 
>>>> In file included from xdr.c:61:0:
>>>> ./rpc/types.h:63:14: error: conflicting types for 'malloc'
>>>> make[4]: *** [xdr.o] Error 1
>>>> 
>>>> I can make the build proceed some by commenting out the declaration 
>>>> "extern char *malloc();" in xdr/rpc/types.h,
>>>> but then I get a slew of other errors:
>>>> 
>>>> xdr_float.c: In function 'xdr_float':
>>>> xdr_float.c:119:21: error: storage size of 'is' isn't known
>>>> xdr_float.c:120:20: error: storage size of 'vs' isn't known
>>>> 
>>>> and so on.
>>>> 
>>>> config.log is rather big to post here; I'm putting it at
>>>> http://www.wfu.edu/~cottrell/tmp/R.config.log .
>>>> 
>>>> --
>>>> Allin Cottrell
>>>> Department of Economics
>>>> Wake Forest University, NC
>>>> 
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide 
>>>> http://www.R-project.org/posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>> 
>>> 
>>> 
>> 
>> -- 
>> Allin Cottrell
>> Department of Economics
>> Wake Forest University
>> 
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From mdowle at mdowle.plus.com  Wed Jul 20 11:10:16 2011
From: mdowle at mdowle.plus.com (Matthew Dowle)
Date: Wed, 20 Jul 2011 10:10:16 +0100
Subject: [Rd] Manipulating single-precision (float) arrays in
	.Callfunctions
References: <1311004330702-3675684.post@n4.nabble.com><4E245D01.1010407@gmail.com><1311027322959-3676667.post@n4.nabble.com><CC044BB2-F9EA-440F-8A64-5B4639B2A529@r-project.org><1311048424023-3677232.post@n4.nabble.com>	<alpine.LFD.2.02.1107190640280.28269@gannet.stats.ox.ac.uk><j03qto$hvm$1@dough.gmane.org>
	<4E259600.5070103@gmail.com>
Message-ID: <j0661r$8h4$1@dough.gmane.org>


"Duncan Murdoch" <murdoch.duncan at gmail.com> wrote in message 
news:4E259600.5070103 at gmail.com...
> On 11-07-19 7:48 AM, Matthew Dowle wrote:
>>
>> "Prof Brian Ripley"<ripley at stats.ox.ac.uk>  wrote in message
>> news:alpine.LFD.2.02.1107190640280.28269 at gannet.stats.ox.ac.uk...
>>> On Mon, 18 Jul 2011, Alireza Mahani wrote:
>>>
>>>> Simon,
>>>>
>>>> Thank you for elaborating on the limitations of R in handling float
>>>> types. I
>>>> think I'm pretty much there with you.
>>>>
>>>> As for the insufficiency of single-precision math (and hence 
>>>> limitations
>>>> of
>>>> GPU), my personal take so far has been that double-precision becomes
>>>> crucial
>>>> when some sort of error accumulation occurs. For example, in 
>>>> differential
>>>> equations where boundary values are integrated to arrive at interior
>>>> values,
>>>> etc. On the other hand, in my personal line of work (Hierarchical
>>>> Bayesian
>>>> models for quantitative marketing), we have so much inherent 
>>>> uncertainty
>>>> and
>>>> noise at so many levels in the problem (and no significant error
>>>> accumulation sources) that single vs double precision issue is often
>>>> inconsequential for us. So I think it really depends on the field as 
>>>> well
>>>> as
>>>> the nature of the problem.
>>>
>>> The main reason to use only double precision in R was that on modern 
>>> CPUs
>>> double precision calculations are as fast as single-precision ones, and
>>> with 64-bit CPUs they are a single access.
>>> So the extra precision comes more-or-less for free.
>>
>> But, isn't it much more of the 'less free' when large data sets are
>> considered? If a double matrix takes 3GB, it's 1.5GB in single.
>> That might alleviate the dreaded out-of-memory error for some
>> users in some circumstances. On 64bit, 50GB reduces to 25GB
>> and that might make the difference between getting
>> something done, or not. If single were appropriate, of course.
>> For GPU too, i/o often dominates iiuc.
>>
>> For space reasons, is there any possibility of R supporting single
>> precision (and single bit logical to reduce memory for logicals by
>> 32 times)? I guess there might be complaints from users using
>> single inappropriately (or worse, not realising we have an instable
>> result due to single).
>
> You can do any of this using external pointers now.  That will remind you 
> that every single function to operate on such objects needs to be 
> rewritten.
>
> It's a huge amount of work, benefiting very few people.  I don't think 
> anyone in R Core will do it.
>
> Duncan Murdoch

I've been informed off list about the 'bit' package, which seems
great and answers my parenthetic complaint (at least).

http://cran.r-project.org/web/packages/bit/index.html

Matthew


From dajohnston at ucdavis.edu  Wed Jul 20 01:34:53 2011
From: dajohnston at ucdavis.edu (David A. Johnston)
Date: Tue, 19 Jul 2011 16:34:53 -0700 (PDT)
Subject: [Rd] apply() returning a list?
Message-ID: <1311118493785-3679619.post@n4.nabble.com>

Hi devel list,

I searched for past posts about this issue and found none, so my apologies
if this has been addressed before.  

The sapply() function has an argument 'simplify', and the mapply() function
has an argument 'SIMPLIFY'.  I am surprised that the apply() argument does
not have a similar argument.  Is there a reason for this?  

Here's a simple example:
> x = matrix(1:12, 3, 4)
> apply(x, 1, sum)
[1] 22 26 30

This is what I would like to see:
> apply(x, 1, sum, simplify = FALSE) 
[[1]]
[1] 22

[[2]]
[1] 26

[[3]]
[1] 30


Looking at the function definition of apply(), I imagine it wouldn't be too
hard to add such an argument.  Add an argument 'simplify' with default value
TRUE, and include the following line in the function definition.
if(simplify == FALSE)  return (ans)

Is this a good idea? Would this be a good addition to R's apply() function?
Are there stability issues in making this change (for example, what if some
previous code threw a 'simplify' argument into the '...')?

Best,

-David Johnston

--
View this message in context: http://r.789695.n4.nabble.com/apply-returning-a-list-tp3679619p3679619.html
Sent from the R devel mailing list archive at Nabble.com.


From marchywka at hotmail.com  Wed Jul 20 02:01:14 2011
From: marchywka at hotmail.com (Mike Marchywka)
Date: Tue, 19 Jul 2011 20:01:14 -0400
Subject: [Rd] Randomness not due to seed
In-Reply-To: <1311081181602-3678082.post@n4.nabble.com>
References: <1311081181602-3678082.post@n4.nabble.com>
Message-ID: <BLU166-w543D75409AB00B2B1DB127BE4C0@phx.gbl>








----------------------------------------
> Date: Tue, 19 Jul 2011 06:13:01 -0700
> From: jeroen.ooms at stat.ucla.edu
> To: r-devel at r-project.org
> Subject: [Rd] Randomness not due to seed
>
> I am working on a reproducible computing platform for which I would like to
> be able to _exactly_ reproduce an R object. However, I am experiencing
> unexpected randomness in some calculations. I have a hard time finding out
> exactly how it occurs. The code below illustrates the issue.
>
> mylm1 <- lm(dist~speed, data=cars);
> mylm2 <- lm(dist~speed, data=cars);
> identical(mylm1, mylm2); #TRUE
>
> makelm <- function(){
> return(lm(dist~speed, data=cars));
> }
>
> mylm1 <- makelm();
> mylm2 <- makelm();
> identical(mylm1, mylm2); #FALSE
>
> When inspecting both objects there seem to be some rounding differences.
> Setting a seed does not make a difference. Is there any way I can remove
> this randomness and exactly reproduce the object every time?

I don't know if anyone had a specific answer for this but in general floating point is not
something for which you want to make bitwise equality tests. You can check the Intel
website for some references but IIRC the FPU can start your calculation with bits or
settings ( flushing denorms to zero for example) left over from the last user although I can't document that.? 

for example, you can probably find more like this suggesting that changes in alignmnet
and rounding in preamble code can be significant, 

http://software.intel.com/en-us/articles/consistency-of-floating-point-results-using-the-intel-compiler/

and of course if your algorithm is numerically sensitive results could change a lot. Now
its also possible you have unitiliazed or corrupt memory, but you would need to 
consider that you will not get bit wise reproduvibility. You can of course go to java
if you really want that LOL. 


>
>
>
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/Randomness-not-due-to-seed-tp3678082p3678082.html
> Sent from the R devel mailing list archive at Nabble.com.
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
 		 	   		  

From murdoch.duncan at gmail.com  Wed Jul 20 13:52:21 2011
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Wed, 20 Jul 2011 07:52:21 -0400
Subject: [Rd] Randomness not due to seed
In-Reply-To: <BLU166-w543D75409AB00B2B1DB127BE4C0@phx.gbl>
References: <1311081181602-3678082.post@n4.nabble.com>
	<BLU166-w543D75409AB00B2B1DB127BE4C0@phx.gbl>
Message-ID: <4E26C175.1070806@gmail.com>

On 11-07-19 8:01 PM, Mike Marchywka wrote:
>
>
>
>
>
>
>
> ----------------------------------------
>> Date: Tue, 19 Jul 2011 06:13:01 -0700
>> From: jeroen.ooms at stat.ucla.edu
>> To: r-devel at r-project.org
>> Subject: [Rd] Randomness not due to seed
>>
>> I am working on a reproducible computing platform for which I would like to
>> be able to _exactly_ reproduce an R object. However, I am experiencing
>> unexpected randomness in some calculations. I have a hard time finding out
>> exactly how it occurs. The code below illustrates the issue.
>>
>> mylm1<- lm(dist~speed, data=cars);
>> mylm2<- lm(dist~speed, data=cars);
>> identical(mylm1, mylm2); #TRUE
>>
>> makelm<- function(){
>> return(lm(dist~speed, data=cars));
>> }
>>
>> mylm1<- makelm();
>> mylm2<- makelm();
>> identical(mylm1, mylm2); #FALSE
>>
>> When inspecting both objects there seem to be some rounding differences.
>> Setting a seed does not make a difference. Is there any way I can remove
>> this randomness and exactly reproduce the object every time?
>
> I don't know if anyone had a specific answer for this

I think Bill Dunlap's answer addressed it:  the claim appears to be false.

Duncan Murdoch

but in general floating point is not
> something for which you want to make bitwise equality tests. You can check the Intel
> website for some references but IIRC the FPU can start your calculation with bits or
> settings ( flushing denorms to zero for example) left over from the last user although I can't document that.
>
> for example, you can probably find more like this suggesting that changes in alignmnet
> and rounding in preamble code can be significant,
>
> http://software.intel.com/en-us/articles/consistency-of-floating-point-results-using-the-intel-compiler/
>
> and of course if your algorithm is numerically sensitive results could change a lot. Now
> its also possible you have unitiliazed or corrupt memory, but you would need to
> consider that you will not get bit wise reproduvibility. You can of course go to java
> if you really want that LOL.
>
>
>>
>>
>>
>>
>>
>> --
>> View this message in context: http://r.789695.n4.nabble.com/Randomness-not-due-to-seed-tp3678082p3678082.html
>> Sent from the R devel mailing list archive at Nabble.com.
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>   		 	   		
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From jeroen.ooms at stat.ucla.edu  Wed Jul 20 14:03:30 2011
From: jeroen.ooms at stat.ucla.edu (Jeroen Ooms)
Date: Wed, 20 Jul 2011 14:03:30 +0200
Subject: [Rd] Randomness not due to seed
In-Reply-To: <4E26C175.1070806@gmail.com>
References: <1311081181602-3678082.post@n4.nabble.com>
	<BLU166-w543D75409AB00B2B1DB127BE4C0@phx.gbl>
	<4E26C175.1070806@gmail.com>
Message-ID: <CABFfbXutiefddok9WWSxq0vYiwAomLA3hcmma5nW0PwJyJ8UfQ@mail.gmail.com>

>> I think Bill Dunlap's answer addressed it: ?the claim appears to be false.

Here is another example where there is randomness that is not due to
the seed. On the same machine, the same R binary, but through another
interface. First directly in the shell:

> sessionInfo()
R version 2.13.1 (2011-07-08)
Platform: i686-pc-linux-gnu (32-bit)

locale:
 [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
 [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
 [5] LC_MONETARY=C              LC_MESSAGES=en_US.UTF-8
 [7] LC_PAPER=en_US.UTF-8       LC_NAME=C
 [9] LC_ADDRESS=C               LC_TELEPHONE=C
[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

> set.seed(123)
> print(coef(lm(dist~speed, data=cars)),digits=22)
              (Intercept)                     speed
-17.579094890510951643137   3.932408759124087715975



# And this is through eclipse (java)

> sessionInfo()
R version 2.13.1 (2011-07-08)
Platform: i686-pc-linux-gnu (32-bit)

locale:
 [1] LC_CTYPE=en_US.UTF-8          LC_NUMERIC=C
 [3] LC_TIME=en_US.UTF-8           LC_COLLATE=en_US.UTF-8
 [5] LC_MONETARY=en_US.UTF-8       LC_MESSAGES=en_US.UTF-8
 [7] LC_PAPER=en_US.UTF-8          LC_NAME=en_US.UTF-8
 [9] LC_ADDRESS=en_US.UTF-8        LC_TELEPHONE=en_US.UTF-8
[11] LC_MEASUREMENT=en_US.UTF-8    LC_IDENTIFICATION=en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

other attached packages:
[1] rj_0.5.2-1

loaded via a namespace (and not attached):
[1] rJava_0.9-1  tools_2.13.1

> set.seed(123)
> print(coef(lm(dist~speed, data=cars)),digits=22)
             (Intercept)                    speed
-17.57909489051087703615   3.93240875912408460735


From edd at debian.org  Wed Jul 20 15:38:20 2011
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 20 Jul 2011 08:38:20 -0500
Subject: [Rd] Randomness not due to seed
In-Reply-To: <CABFfbXutiefddok9WWSxq0vYiwAomLA3hcmma5nW0PwJyJ8UfQ@mail.gmail.com>
References: <1311081181602-3678082.post@n4.nabble.com>
	<BLU166-w543D75409AB00B2B1DB127BE4C0@phx.gbl>
	<4E26C175.1070806@gmail.com>
	<CABFfbXutiefddok9WWSxq0vYiwAomLA3hcmma5nW0PwJyJ8UfQ@mail.gmail.com>
Message-ID: <20006.55884.14520.313303@max.nulle.part>


On 20 July 2011 at 14:03, Jeroen Ooms wrote:
| >> I think Bill Dunlap's answer addressed it: ?the claim appears to be false.
| 
| Here is another example where there is randomness that is not due to
| the seed. On the same machine, the same R binary, but through another
| interface. First directly in the shell:
| 
| > sessionInfo()
| R version 2.13.1 (2011-07-08)
| Platform: i686-pc-linux-gnu (32-bit)
| 
| locale:
|  [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
|  [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
|  [5] LC_MONETARY=C              LC_MESSAGES=en_US.UTF-8
|  [7] LC_PAPER=en_US.UTF-8       LC_NAME=C
|  [9] LC_ADDRESS=C               LC_TELEPHONE=C
| [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C
| 
| attached base packages:
| [1] stats     graphics  grDevices utils     datasets  methods   base
| 
| > set.seed(123)
| > print(coef(lm(dist~speed, data=cars)),digits=22)
|               (Intercept)                     speed
| -17.579094890510951643137   3.932408759124087715975

That's PBKAC --- even double precision does NOT get you 22 digits precision.  

You may want to read up on 'what every computer scientist should know about
floating point arithmetic' by Goldberg (which is both a true internet classic)
and ponder why a common setting for the various 'epsilon' settings of general
convergence is set to of the constants supplied by the OS and/or its C
library. R has

  #define SINGLE_EPS     FLT_EPSILON
  [...]
  #define DOUBLE_EPS     DBL_EPSILON

in Constants.h. You can then chase the definition of FLT_EPSILON and
DBL_EPSILON through your system headers (which is a good exercise).

One place you may end up in the manual -- the following from the GNU libc
documentationon :Floating Point Parameters"

FLT_EPSILON
     This is the minimum positive floating point number of type float such that
     1.0 + FLT_EPSILON != 1.0 is true. It's supposed to be no greater than 1E-5. 

DBL_EPSILON
LDBL_EPSILON
     These are similar to FLT_EPSILON, but for the data types double and long
     double, respectively. The type of the macro's value is the same as the type
     it describes. The values are not supposed to be greater than 1E-9.

So there -- nine digits. 

Dirk 
 
 
| # And this is through eclipse (java)
| 
| > sessionInfo()
| R version 2.13.1 (2011-07-08)
| Platform: i686-pc-linux-gnu (32-bit)
| 
| locale:
|  [1] LC_CTYPE=en_US.UTF-8          LC_NUMERIC=C
|  [3] LC_TIME=en_US.UTF-8           LC_COLLATE=en_US.UTF-8
|  [5] LC_MONETARY=en_US.UTF-8       LC_MESSAGES=en_US.UTF-8
|  [7] LC_PAPER=en_US.UTF-8          LC_NAME=en_US.UTF-8
|  [9] LC_ADDRESS=en_US.UTF-8        LC_TELEPHONE=en_US.UTF-8
| [11] LC_MEASUREMENT=en_US.UTF-8    LC_IDENTIFICATION=en_US.UTF-8
| 
| attached base packages:
| [1] stats     graphics  grDevices utils     datasets  methods   base
| 
| other attached packages:
| [1] rj_0.5.2-1
| 
| loaded via a namespace (and not attached):
| [1] rJava_0.9-1  tools_2.13.1
| 
| > set.seed(123)
| > print(coef(lm(dist~speed, data=cars)),digits=22)
|              (Intercept)                    speed
| -17.57909489051087703615   3.93240875912408460735
| 
| ______________________________________________
| R-devel at r-project.org mailing list
| https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
Gauss once played himself in a zero-sum game and won $50.
                      -- #11 at http://www.gaussfacts.com


From murdoch.duncan at gmail.com  Wed Jul 20 15:46:42 2011
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Wed, 20 Jul 2011 09:46:42 -0400
Subject: [Rd] Randomness not due to seed
In-Reply-To: <CABFfbXutiefddok9WWSxq0vYiwAomLA3hcmma5nW0PwJyJ8UfQ@mail.gmail.com>
References: <1311081181602-3678082.post@n4.nabble.com>
	<BLU166-w543D75409AB00B2B1DB127BE4C0@phx.gbl>
	<4E26C175.1070806@gmail.com>
	<CABFfbXutiefddok9WWSxq0vYiwAomLA3hcmma5nW0PwJyJ8UfQ@mail.gmail.com>
Message-ID: <4E26DC42.6050604@gmail.com>

I would guess the error below is because of Java messing around in the 
hardware.  It's pretty common on Windows for DLLs to attempt to change 
the precision setting on the floating point processor; I hadn't seen 
that before on Linux, but that would be my guess as to the cause.

It's also possible that one of the attached packages has messed with R 
functions somehow, e.g. by replacing the default print() or show() method.

A third possibility is that different math libraries are being used.

So I would consider the differences in the results to be a bit of a bug, 
but not one that is likely under our control, and not one that is so 
large that I would worry about working around it.

Duncan Murdoch

On 20/07/2011 8:03 AM, Jeroen Ooms wrote:
> >>  I think Bill Dunlap's answer addressed it:  the claim appears to be false.
>
> Here is another example where there is randomness that is not due to
> the seed. On the same machine, the same R binary, but through another
> interface. First directly in the shell:
>
> >  sessionInfo()
> R version 2.13.1 (2011-07-08)
> Platform: i686-pc-linux-gnu (32-bit)
>
> locale:
>   [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
>   [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
>   [5] LC_MONETARY=C              LC_MESSAGES=en_US.UTF-8
>   [7] LC_PAPER=en_US.UTF-8       LC_NAME=C
>   [9] LC_ADDRESS=C               LC_TELEPHONE=C
> [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> >  set.seed(123)
> >  print(coef(lm(dist~speed, data=cars)),digits=22)
>                (Intercept)                     speed
> -17.579094890510951643137   3.932408759124087715975
>
>
>
> # And this is through eclipse (java)
>
> >  sessionInfo()
> R version 2.13.1 (2011-07-08)
> Platform: i686-pc-linux-gnu (32-bit)
>
> locale:
>   [1] LC_CTYPE=en_US.UTF-8          LC_NUMERIC=C
>   [3] LC_TIME=en_US.UTF-8           LC_COLLATE=en_US.UTF-8
>   [5] LC_MONETARY=en_US.UTF-8       LC_MESSAGES=en_US.UTF-8
>   [7] LC_PAPER=en_US.UTF-8          LC_NAME=en_US.UTF-8
>   [9] LC_ADDRESS=en_US.UTF-8        LC_TELEPHONE=en_US.UTF-8
> [11] LC_MEASUREMENT=en_US.UTF-8    LC_IDENTIFICATION=en_US.UTF-8
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> other attached packages:
> [1] rj_0.5.2-1
>
> loaded via a namespace (and not attached):
> [1] rJava_0.9-1  tools_2.13.1
>
> >  set.seed(123)
> >  print(coef(lm(dist~speed, data=cars)),digits=22)
>               (Intercept)                    speed
> -17.57909489051087703615   3.93240875912408460735


From Martyn.Byng at nag.co.uk  Wed Jul 20 15:59:34 2011
From: Martyn.Byng at nag.co.uk (Martyn Byng)
Date: Wed, 20 Jul 2011 14:59:34 +0100
Subject: [Rd] Randomness not due to seed
References: <1311081181602-3678082.post@n4.nabble.com><BLU166-w543D75409AB00B2B1DB127BE4C0@phx.gbl><4E26C175.1070806@gmail.com><CABFfbXutiefddok9WWSxq0vYiwAomLA3hcmma5nW0PwJyJ8UfQ@mail.gmail.com>
	<4E26DC42.6050604@gmail.com>
Message-ID: <49E76DF37649DC48A4CE882BC8CE51C901C2C1D9@nagmail2.nag.co.uk>

Hi,

Even using the same math libraries you can get different results,
depending on what sorts of instructions those libraries use, see the
following (none R related) blog article:
http://blog.nag.com/2011/02/wandering-precision.html.

Martyn

-----Original Message-----
From: r-devel-bounces at r-project.org
[mailto:r-devel-bounces at r-project.org] On Behalf Of Duncan Murdoch
Sent: 20 July 2011 14:47
To: Jeroen Ooms
Cc: r-devel at r-project.org
Subject: Re: [Rd] Randomness not due to seed

I would guess the error below is because of Java messing around in the 
hardware.  It's pretty common on Windows for DLLs to attempt to change 
the precision setting on the floating point processor; I hadn't seen 
that before on Linux, but that would be my guess as to the cause.

It's also possible that one of the attached packages has messed with R 
functions somehow, e.g. by replacing the default print() or show()
method.

A third possibility is that different math libraries are being used.

So I would consider the differences in the results to be a bit of a bug,

but not one that is likely under our control, and not one that is so 
large that I would worry about working around it.

Duncan Murdoch

On 20/07/2011 8:03 AM, Jeroen Ooms wrote:
> >>  I think Bill Dunlap's answer addressed it:  the claim appears to
be false.
>
> Here is another example where there is randomness that is not due to
> the seed. On the same machine, the same R binary, but through another
> interface. First directly in the shell:
>
> >  sessionInfo()
> R version 2.13.1 (2011-07-08)
> Platform: i686-pc-linux-gnu (32-bit)
>
> locale:
>   [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
>   [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
>   [5] LC_MONETARY=C              LC_MESSAGES=en_US.UTF-8
>   [7] LC_PAPER=en_US.UTF-8       LC_NAME=C
>   [9] LC_ADDRESS=C               LC_TELEPHONE=C
> [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> >  set.seed(123)
> >  print(coef(lm(dist~speed, data=cars)),digits=22)
>                (Intercept)                     speed
> -17.579094890510951643137   3.932408759124087715975
>
>
>
> # And this is through eclipse (java)
>
> >  sessionInfo()
> R version 2.13.1 (2011-07-08)
> Platform: i686-pc-linux-gnu (32-bit)
>
> locale:
>   [1] LC_CTYPE=en_US.UTF-8          LC_NUMERIC=C
>   [3] LC_TIME=en_US.UTF-8           LC_COLLATE=en_US.UTF-8
>   [5] LC_MONETARY=en_US.UTF-8       LC_MESSAGES=en_US.UTF-8
>   [7] LC_PAPER=en_US.UTF-8          LC_NAME=en_US.UTF-8
>   [9] LC_ADDRESS=en_US.UTF-8        LC_TELEPHONE=en_US.UTF-8
> [11] LC_MEASUREMENT=en_US.UTF-8    LC_IDENTIFICATION=en_US.UTF-8
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> other attached packages:
> [1] rj_0.5.2-1
>
> loaded via a namespace (and not attached):
> [1] rJava_0.9-1  tools_2.13.1
>
> >  set.seed(123)
> >  print(coef(lm(dist~speed, data=cars)),digits=22)
>               (Intercept)                    speed
> -17.57909489051087703615   3.93240875912408460735

______________________________________________
R-devel at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-devel

________________________________________________________________________
This e-mail has been scanned for all viruses by Star.\ _...{{dropped:12}}


From murdoch.duncan at gmail.com  Wed Jul 20 16:06:49 2011
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Wed, 20 Jul 2011 10:06:49 -0400
Subject: [Rd] Randomness not due to seed
In-Reply-To: <49E76DF37649DC48A4CE882BC8CE51C901C2C1D9@nagmail2.nag.co.uk>
References: <1311081181602-3678082.post@n4.nabble.com><BLU166-w543D75409AB00B2B1DB127BE4C0@phx.gbl><4E26C175.1070806@gmail.com><CABFfbXutiefddok9WWSxq0vYiwAomLA3hcmma5nW0PwJyJ8UfQ@mail.gmail.com>
	<4E26DC42.6050604@gmail.com>
	<49E76DF37649DC48A4CE882BC8CE51C901C2C1D9@nagmail2.nag.co.uk>
Message-ID: <4E26E0F9.5040402@gmail.com>

On 20/07/2011 9:59 AM, Martyn Byng wrote:
> Hi,
>
> Even using the same math libraries you can get different results,
> depending on what sorts of instructions those libraries use, see the
> following (none R related) blog article:
> http://blog.nag.com/2011/02/wandering-precision.html.

That's another cause that I hadn't considered, also mostly out of our 
control.  (Whoever compiles R does have some control over what 
optimizations the compiler does, but they might not be aware of them all.)

Duncan Murdoch

> Martyn
>
> -----Original Message-----
> From: r-devel-bounces at r-project.org
> [mailto:r-devel-bounces at r-project.org] On Behalf Of Duncan Murdoch
> Sent: 20 July 2011 14:47
> To: Jeroen Ooms
> Cc: r-devel at r-project.org
> Subject: Re: [Rd] Randomness not due to seed
>
> I would guess the error below is because of Java messing around in the
> hardware.  It's pretty common on Windows for DLLs to attempt to change
> the precision setting on the floating point processor; I hadn't seen
> that before on Linux, but that would be my guess as to the cause.
>
> It's also possible that one of the attached packages has messed with R
> functions somehow, e.g. by replacing the default print() or show()
> method.
>
> A third possibility is that different math libraries are being used.
>
> So I would consider the differences in the results to be a bit of a bug,
>
> but not one that is likely under our control, and not one that is so
> large that I would worry about working around it.
>
> Duncan Murdoch
>
> On 20/07/2011 8:03 AM, Jeroen Ooms wrote:
> >  >>   I think Bill Dunlap's answer addressed it:  the claim appears to
> be false.
> >
> >  Here is another example where there is randomness that is not due to
> >  the seed. On the same machine, the same R binary, but through another
> >  interface. First directly in the shell:
> >
> >  >   sessionInfo()
> >  R version 2.13.1 (2011-07-08)
> >  Platform: i686-pc-linux-gnu (32-bit)
> >
> >  locale:
> >    [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
> >    [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
> >    [5] LC_MONETARY=C              LC_MESSAGES=en_US.UTF-8
> >    [7] LC_PAPER=en_US.UTF-8       LC_NAME=C
> >    [9] LC_ADDRESS=C               LC_TELEPHONE=C
> >  [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C
> >
> >  attached base packages:
> >  [1] stats     graphics  grDevices utils     datasets  methods   base
> >
> >  >   set.seed(123)
> >  >   print(coef(lm(dist~speed, data=cars)),digits=22)
> >                 (Intercept)                     speed
> >  -17.579094890510951643137   3.932408759124087715975
> >
> >
> >
> >  # And this is through eclipse (java)
> >
> >  >   sessionInfo()
> >  R version 2.13.1 (2011-07-08)
> >  Platform: i686-pc-linux-gnu (32-bit)
> >
> >  locale:
> >    [1] LC_CTYPE=en_US.UTF-8          LC_NUMERIC=C
> >    [3] LC_TIME=en_US.UTF-8           LC_COLLATE=en_US.UTF-8
> >    [5] LC_MONETARY=en_US.UTF-8       LC_MESSAGES=en_US.UTF-8
> >    [7] LC_PAPER=en_US.UTF-8          LC_NAME=en_US.UTF-8
> >    [9] LC_ADDRESS=en_US.UTF-8        LC_TELEPHONE=en_US.UTF-8
> >  [11] LC_MEASUREMENT=en_US.UTF-8    LC_IDENTIFICATION=en_US.UTF-8
> >
> >  attached base packages:
> >  [1] stats     graphics  grDevices utils     datasets  methods   base
> >
> >  other attached packages:
> >  [1] rj_0.5.2-1
> >
> >  loaded via a namespace (and not attached):
> >  [1] rJava_0.9-1  tools_2.13.1
> >
> >  >   set.seed(123)
> >  >   print(coef(lm(dist~speed, data=cars)),digits=22)
> >                (Intercept)                    speed
> >  -17.57909489051087703615   3.93240875912408460735
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>
> ________________________________________________________________________
> This e-mail has been scanned for all viruses by Star.
> ________________________________________________________________________
>
> ________________________________________________________________________
> The Numerical Algorithms Group Ltd is a company registered in England
> and Wales with company number 1249803. The registered office is:
> Wilkinson House, Jordan Hill Road, Oxford OX2 8DR, United Kingdom.
>
> This e-mail has been scanned for all viruses by Star. The service is
> powered by MessageLabs.
> ________________________________________________________________________


From jfox at mcmaster.ca  Wed Jul 20 16:29:15 2011
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 20 Jul 2011 10:29:15 -0400
Subject: [Rd] testing a Windows custom installer
Message-ID: <008801cc46e9$67a2d270$36e87750$@mcmaster.ca>

Dear R-devel list members,

For several years, I've created a custom R installer for my students who use
Windows. When I test the installer on my own Windows machines, selections in
the installation dialogs reflect my previous choices, which I suppose are
saved in the Windows registry. 

I'd like to be able to see what a student who has never installed R before
will see, to verify that an installation that takes all defaults in the
custom installer produces the desired result. In the past, I've dealt with
this problem by finding a Windows machine on which R has never been
installed, but that's inconvenient (and, as R proliferates will, I hope,
become impossible!). Is there a better approach?

Thanks,
 John

--------------------------------
John Fox
Senator William McMaster
  Professor of Social Statistics
Department of Sociology
McMaster University
Hamilton, Ontario, Canada
http://socserv.mcmaster.ca/jfox


From pgilbert at bank-banque-canada.ca  Wed Jul 20 16:42:07 2011
From: pgilbert at bank-banque-canada.ca (Paul Gilbert)
Date: Wed, 20 Jul 2011 14:42:07 +0000
Subject: [Rd] Randomness not due to seed
In-Reply-To: <1311081181602-3678082.post@n4.nabble.com>
References: <1311081181602-3678082.post@n4.nabble.com>
Message-ID: <6441154A9FF1CD4386AF4ABF141A056D265A90F2@WMEXOSCD2-N1.bocad.bank-banque-canada.ca>

It does not look like your calculation is using the random number generator, so the other responses are probably more to the point.  

However, beware that setting the seed is not enough to guarantee the same random numbers. You need to also make sure you are using the same uniform RNG and any other generators you use, such as the normal generator. R has a large selection of possibilities. Your start up settings could change the default behaviour. Also, relying on the default will be a bit risky if you are interested in reproducible calculations, because the R default could change in the future (as it has in the past, and as has the Splus generator in the past).

If the RNG is important for your reproducible calculations then you might want to look at the examples and tests in the setRNG package.

Paul

> -----Original Message-----
> From: r-devel-bounces at r-project.org [mailto:r-devel-bounces at r-
> project.org] On Behalf Of jeroen00ms
> Sent: July 19, 2011 9:13 AM
> To: r-devel at r-project.org
> Subject: [Rd] Randomness not due to seed
> 
> I am working on a reproducible computing platform for which I would
> like to
> be able to _exactly_ reproduce an R object. However, I am experiencing
> unexpected randomness in some calculations. I have a hard time finding
> out
> exactly how it occurs. The code below illustrates the issue.
> 
> mylm1 <- lm(dist~speed, data=cars);
> mylm2 <- lm(dist~speed, data=cars);
> identical(mylm1, mylm2); #TRUE
> 
> makelm <- function(){
> 	return(lm(dist~speed, data=cars));
> }
> 
> mylm1 <- makelm();
> mylm2 <- makelm();
> identical(mylm1, mylm2); #FALSE
> 
> When inspecting both objects there seem to be some rounding
> differences.
> Setting a seed does not make a difference. Is there any way I can
> remove
> this randomness and exactly reproduce the object every time?
> 
> 
> 
> 
> 
> --
> View this message in context: http://r.789695.n4.nabble.com/Randomness-
> not-due-to-seed-tp3678082p3678082.html
> Sent from the R devel mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
====================================================================================

La version fran?aise suit le texte anglais.

------------------------------------------------------------------------------------

This email may contain privileged and/or confidential information, and the Bank of
Canada does not waive any related rights. Any distribution, use, or copying of this
email or the information it contains by other than the intended recipient is
unauthorized. If you received this email in error please delete it immediately from
your system and notify the sender promptly by email that you have done so. 

------------------------------------------------------------------------------------

Le pr?sent courriel peut contenir de l'information privil?gi?e ou confidentielle.
La Banque du Canada ne renonce pas aux droits qui s'y rapportent. Toute diffusion,
utilisation ou copie de ce courriel ou des renseignements qu'il contient par une
personne autre que le ou les destinataires d?sign?s est interdite. Si vous recevez
ce courriel par erreur, veuillez le supprimer imm?diatement et envoyer sans d?lai ?
l'exp?diteur un message ?lectronique pour l'aviser que vous avez ?limin? de votre
ordinateur toute copie du courriel re?u.

From brian at braverock.com  Wed Jul 20 16:54:42 2011
From: brian at braverock.com (Brian G. Peterson)
Date: Wed, 20 Jul 2011 09:54:42 -0500
Subject: [Rd] testing a Windows custom installer
In-Reply-To: <008801cc46e9$67a2d270$36e87750$@mcmaster.ca>
References: <008801cc46e9$67a2d270$36e87750$@mcmaster.ca>
Message-ID: <1311173682.2400.63.camel@brian-rcg>

On Wed, 2011-07-20 at 10:29 -0400, John Fox wrote:
> For several years, I've created a custom R installer for my students
> who use Windows. When I test the installer on my own Windows machines,
> selections in the installation dialogs reflect my previous choices,
> which I suppose are saved in the Windows registry. 
> 
> I'd like to be able to see what a student who has never installed R
> before will see, to verify that an installation that takes all
> defaults in the custom installer produces the desired result. In the
> past, I've dealt with this problem by finding a Windows machine on
> which R has never been installed, but that's inconvenient (and, as R
> proliferates will, I hope, become impossible!). Is there a better
> approach? 

I keep a Windows XP virtual machine image around in a 'clean' state for
things like this.  I spin up the 'clean' image, test what I need to
test, and shut down the Windows image without snapshotting it.

Regards,

  - Brian

-- 
Brian G. Peterson
http://braverock.com/brian/
Ph: 773-459-4973
IM: bgpbraverock


From murdoch.duncan at gmail.com  Wed Jul 20 16:56:43 2011
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Wed, 20 Jul 2011 10:56:43 -0400
Subject: [Rd] testing a Windows custom installer
In-Reply-To: <008801cc46e9$67a2d270$36e87750$@mcmaster.ca>
References: <008801cc46e9$67a2d270$36e87750$@mcmaster.ca>
Message-ID: <4E26ECAB.2070106@gmail.com>

On 20/07/2011 10:29 AM, John Fox wrote:
> Dear R-devel list members,
>
> For several years, I've created a custom R installer for my students who use
> Windows. When I test the installer on my own Windows machines, selections in
> the installation dialogs reflect my previous choices, which I suppose are
> saved in the Windows registry.
>
> I'd like to be able to see what a student who has never installed R before
> will see, to verify that an installation that takes all defaults in the
> custom installer produces the desired result. In the past, I've dealt with
> this problem by finding a Windows machine on which R has never been
> installed, but that's inconvenient (and, as R proliferates will, I hope,
> become impossible!). Is there a better approach?

The settings are saved to the registry by the installer, Inno Setup, and 
restored automatically (for most of them), or using its 
"GetPreviousData" function (for the R specific ones).  I don't think 
there is a way to tell the installer to ignore the previous data, but 
there might be.

I believe if you uninstall R then the settings will be forgotten, 
because the settings are saved in locations like

HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Uninstall\R 
for Windows 2.13.0_is1 in string values named
"Inno Setup: *" for the ones used internally, and "Inno Setup Codefile: 
*" for the ones specific to R.

It's probably okay to delete that whole key and then it will forget the 
old settings, but that seems risky.

There's a command line option to the installer called "/SAVEINF" which 
saves the settings to a file and "/LOADINF" which loads them from a 
file, but I don't see how to tell it to just ignore the registry settings.

Duncan Murdoch


From dwinsemius at comcast.net  Wed Jul 20 17:13:22 2011
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 20 Jul 2011 11:13:22 -0400
Subject: [Rd] apply() returning a list?
In-Reply-To: <1311118493785-3679619.post@n4.nabble.com>
References: <1311118493785-3679619.post@n4.nabble.com>
Message-ID: <D9FF0717-ED0D-4892-8ACF-7D241972370E@comcast.net>


On Jul 19, 2011, at 7:34 PM, David A. Johnston wrote:

> Hi devel list,
>
> I searched for past posts about this issue and found none, so my  
> apologies
> if this has been addressed before.
>
> The sapply() function has an argument 'simplify', and the mapply()  
> function
> has an argument 'SIMPLIFY'.  I am surprised that the apply()  
> argument does
> not have a similar argument.  Is there a reason for this?
>
> Here's a simple example:
>> x = matrix(1:12, 3, 4)
>> apply(x, 1, sum)
> [1] 22 26 30
>
> This is what I would like to see:
>> apply(x, 1, sum, simplify = FALSE)
> [[1]]
> [1] 22
>
> [[2]]
> [1] 26
>
> [[3]]
> [1] 30
>

Probably no one thought it was that difficult to type the more concise  
and equally expressive:

 > as.list(apply(x, 1, sum))
[[1]]
[1] 22

[[2]]
[1] 26

[[3]]
[1] 30

-- 
David Winsemius, MD
West Hartford, CT


From pdalgd at gmail.com  Wed Jul 20 18:02:39 2011
From: pdalgd at gmail.com (peter dalgaard)
Date: Wed, 20 Jul 2011 18:02:39 +0200
Subject: [Rd] Randomness not due to seed
In-Reply-To: <20006.55884.14520.313303@max.nulle.part>
References: <1311081181602-3678082.post@n4.nabble.com>
	<BLU166-w543D75409AB00B2B1DB127BE4C0@phx.gbl>
	<4E26C175.1070806@gmail.com>
	<CABFfbXutiefddok9WWSxq0vYiwAomLA3hcmma5nW0PwJyJ8UfQ@mail.gmail.com>
	<20006.55884.14520.313303@max.nulle.part>
Message-ID: <0348E92E-B044-4954-95F9-BE8C32C22E21@gmail.com>


On Jul 20, 2011, at 15:38 , Dirk Eddelbuettel wrote:

> 
> On 20 July 2011 at 14:03, Jeroen Ooms wrote:
> | >> I think Bill Dunlap's answer addressed it:  the claim appears to be false.
> | 
> | Here is another example where there is randomness that is not due to
> | the seed. On the same machine, the same R binary, but through another
> | interface. First directly in the shell:
> | 
> | > sessionInfo()
> | R version 2.13.1 (2011-07-08)
> | Platform: i686-pc-linux-gnu (32-bit)
> | 
> | locale:
> |  [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
> |  [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
> |  [5] LC_MONETARY=C              LC_MESSAGES=en_US.UTF-8
> |  [7] LC_PAPER=en_US.UTF-8       LC_NAME=C
> |  [9] LC_ADDRESS=C               LC_TELEPHONE=C
> | [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C
> | 
> | attached base packages:
> | [1] stats     graphics  grDevices utils     datasets  methods   base
> | 
> | > set.seed(123)
> | > print(coef(lm(dist~speed, data=cars)),digits=22)
> |               (Intercept)                     speed
> | -17.579094890510951643137   3.932408759124087715975
> 
> That's PBKAC --- even double precision does NOT get you 22 digits precision.

Hmm, yes, but you would expect the SAME function on the SAME data to yield the same floating point number, and give the SAME printout on the SAME R on the SAME hardware... 

FWIW all the Mac versions that I can access give the same results as the eclipse version.

Let's look at the numbers side-by-side

-17.579094890510951643137   3.932408759124087715975
-17.57909489051087703615    3.93240875912408460735
                !                           !
 12.345678901234567890123   1.234567890123456789012

so we're seeing differences around the 15th/16th significant digit. This is consistent with a difference of about one unit of least precision in the actual objects, but there could conceivably be other explanations, e.g. the print() function picking up random garbage. Jeroen: Could you save() the results from the two cases, load() them in a new session and compute the difference?

>  
> 
> You may want to read up on 'what every computer scientist should know about
> floating point arithmetic' by Goldberg (which is both a true internet classic)
> and ponder why a common setting for the various 'epsilon' settings of general
> convergence is set to of the constants supplied by the OS and/or its C
> library. R has
> 
>  #define SINGLE_EPS     FLT_EPSILON
>  [...]
>  #define DOUBLE_EPS     DBL_EPSILON
> 
> in Constants.h. You can then chase the definition of FLT_EPSILON and
> DBL_EPSILON through your system headers (which is a good exercise).
> 
> One place you may end up in the manual -- the following from the GNU libc
> documentationon :Floating Point Parameters"
> 
> FLT_EPSILON
>     This is the minimum positive floating point number of type float such that
>     1.0 + FLT_EPSILON != 1.0 is true. It's supposed to be no greater than 1E-5. 
> 
> DBL_EPSILON
> LDBL_EPSILON
>     These are similar to FLT_EPSILON, but for the data types double and long
>     double, respectively. The type of the macro's value is the same as the type
>     it describes. The values are not supposed to be greater than 1E-9.
> 
> So there -- nine digits. 
> 
> Dirk 
> 
> 
> | # And this is through eclipse (java)
> | 
> | > sessionInfo()
> | R version 2.13.1 (2011-07-08)
> | Platform: i686-pc-linux-gnu (32-bit)
> | 
> | locale:
> |  [1] LC_CTYPE=en_US.UTF-8          LC_NUMERIC=C
> |  [3] LC_TIME=en_US.UTF-8           LC_COLLATE=en_US.UTF-8
> |  [5] LC_MONETARY=en_US.UTF-8       LC_MESSAGES=en_US.UTF-8
> |  [7] LC_PAPER=en_US.UTF-8          LC_NAME=en_US.UTF-8
> |  [9] LC_ADDRESS=en_US.UTF-8        LC_TELEPHONE=en_US.UTF-8
> | [11] LC_MEASUREMENT=en_US.UTF-8    LC_IDENTIFICATION=en_US.UTF-8
> | 
> | attached base packages:
> | [1] stats     graphics  grDevices utils     datasets  methods   base
> | 
> | other attached packages:
> | [1] rj_0.5.2-1
> | 
> | loaded via a namespace (and not attached):
> | [1] rJava_0.9-1  tools_2.13.1
> | 
> | > set.seed(123)
> | > print(coef(lm(dist~speed, data=cars)),digits=22)
> |              (Intercept)                    speed
> | 

> | 
> | ______________________________________________
> | R-devel at r-project.org mailing list
> | https://stat.ethz.ch/mailman/listinfo/r-devel
> 
> -- 
> Gauss once played himself in a zero-sum game and won $50.
>                      -- #11 at http://www.gaussfacts.com
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
Peter Dalgaard
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From raphael.rossignol at math.u-psud.fr  Wed Jul 20 13:54:02 2011
From: raphael.rossignol at math.u-psud.fr (Raphael Rossignol)
Date: Wed, 20 Jul 2011 13:54:02 +0200
Subject: [Rd] The C function getQ0 returns a non-positive covariance matrix
	and	causes errors in arima()
Message-ID: <20110720135402.kleeu57k004oc04s@webmail.math.u-psud.fr>

Hi,

the function makeARIMA(), designed to construct some state space  
representation of an ARIMA model, uses a C function called getQ0,  
which can be found at the end of arima.c in R source files (library  
stats). getQ0 takes two arguments, phi and theta, and returns the  
covariance matrix of the state prediction error at time zero. The  
reference for getQ0 (cited by help(arima)) is:
Gardner, G, Harvey, A. C. and Phillips, G. D. A. (1980) Algorithm
      AS154. An algorithm for exact maximum likelihood estimation of
      autoregressive-moving average models by means of Kalman filtering.
      _Applied Statistics_ *29*, 311-322.
where it is called subroutine STARMA (and coded in fortran 77).

My problem is that getQ0 returns incorrect covariance matrices in  
certain cases. Indeed, below is an example of a  
SARIMA(1,0,1)x(1,0,0)_12 where getQ0 gives a covariance matrix which  
possess negative eigenvalues ! Below, I obtain getQ0 results through  
makeARIMA().
Example:
> s <- 12
> phis <- 0.95
> phi1 <- 0.0001
> phi <- c(phi1,rep(0,s-2),phis,-phi1*phis)
> theta <- 0.7
> out <- makeARIMA(phi,theta,NULL)
> min(eigen(out$Pn)$value)
[1] -19.15890

There are consequences of this "bug" in the functions KalmanLike() and  
arima(). Indeed, arima() in its default behaviour uses first CSS  
method to get the initial value for an MLE search through optim. To  
compute the likelihood, it uses getQ0 at the initialization of the  
Kalman Filter. It may happen that getQ0 returns a covariance matrix  
which possesses negative eigenvalues and that this gives a negative  
gain in the Kalman filter, which in turn gives a likelihood equal to  
NaN. Here is a reproducible example where I forced the use of 'ML'.
> set.seed(1)
> x <- arima.sim(100,model=list(ar=phi,ma=theta))
> KalmanLike(x,mod=out,fast=FALSE)
$Lik
ssq
NaN

$s2
      ssq
0.971436

> arima(x,order=c(1,0,1),seasonal=list(period=12,order=c(1,0,0)),include.mean=FALSE,init=c(phi1,theta,phis),method='ML')
Erreur dans optim(init[mask], armafn, method = optim.method, hessian =  
TRUE,  :
   valeur non-finie fournie par optim

If needed, I can send a more natural example in which the above  
behaviour is obtained. This error message ("Error in optim ...  
non-finite finite-difference value") was already noted in the  
following message, which remained without answer:
https://stat.ethz.ch/pipermail/r-devel/2009-February/052009.html

I could not figure out whether there is a real bug in getQ0 or if this  
is due to some numerical instability. However, I tried to replace  
getQ0 in two ways. The first one is to compute first the covariance  
matrix of (X_{t-1},...,X_{t-p},Z_t,...,Z_{t-q}) and this is achieved  
through the method of difference equations (page 93 of Brockwell and  
Davis). This way was apparently suggested by a referee to Gardner et  
al. paper (see page 314 of their paper).
Q0bis <- function(phi,theta){
   ## Computes the initial covariance matrix for the state space  
representation of Gardner et al.
   p <- length(phi)
   q <- length(theta)
   r <- max(p,q+1)
   ttheta <- c(1,theta,rep(0,max(p,q+1)-q-1))
   A1 <- matrix(0,r,p)
   B <- (col(A1)+row(A1)<=p+1)
   C <- (col(A1)+row(A1)-1)
   A1[B] <- phi[C[B]]
   A2 <- matrix(0,r,q+1)
   C <- (col(A2)+row(A2)-1)
   B <- (C<=q+1)
   A2[B] <- ttheta[C[B]]
   A <- cbind(A1,A2)
   if (p==0) {
     S <- diag(q+1)
   }
   else {
     ## Compute the autocovariance function of U, the AR part of X
     r2 <- max(p+q,p+1)
     tphi <- c(1,-phi)
     C1 <- matrix(0,r2,r2)
     F <- row(C1)-col(C1)+1
     E <- (1<=F)&(F<=p+1)
     C1[E] <- tphi[F[E]]
     C2 <- matrix(0,r2,r2)
     F <- col(C2)+row(C2)-1
     E <- (F<=p+1) & col(C2)>=2
     C2[E] <- tphi[F[E]]
     Gam <- (C1+C2)
     g <- matrix(0,r2,1)
     g[1] <- 1
     rU <- solve(Gam,g)
     SU <- toeplitz(rU[1:(p+q),1])
     ## End of the difference equations method
     ## Then, compute correlation matrix of X
     A2 <- matrix(0,p,p+q)
     C <- col(A2)-row(A2)+1
     B <- (1<=C)&(C<=q+1)
     A2[B] <- ttheta[C[B]]
     SX <- A2%*%SU%*%t(A2)
     ## Now, compute correlation matrix between X and Z
     C1 <- matrix(0,q,q)
     F <- row(C1)-col(C1)+1
     E <- (F>=1) & (F<=p+1)
     C1[E] <- tphi[F[E]]
     g <- matrix(0,q,1)
     if (q) g[1:q,1] <- ttheta[1:q]
     rXZ <- forwardsolve(C1,g)
     SXZ <- matrix(0, p, q+1)
     F <- col(SXZ)-row(SXZ)
     E <- F>=1
     SXZ[E] <- rXZ[F[E]]
     S <- rbind(cbind(SX,SXZ),cbind(t(SXZ),diag(q+1)))
   }
   Q0 <- A%*%S%*%t(A)
}

The second way is to resolve brutally the equation of Gardner et al.  
in the form (12), page 314 of their paper.

Q0ter <- function(phi,theta){
   p <- length(phi)
   q <- length(theta)
   r <- max(p,q+1)
   T <- matrix(0,r,r)
   if (p) T[1:p,1] <- phi
   if (r) T[1:(r-1),2:r] <- diag(r-1)
   V <- matrix(0,r,r)
   ttheta <- c(1,theta)
   V[1:(q+1),1:(q+1)] <- ttheta%x%t(ttheta)
   V <- matrix(V,ncol=1)
   S <- diag(r*r)-T%x%T
   Q0 <- solve(S,V)
   Q0 <- matrix(Q0,ncol=r)
}

My conclusion for now is that these two other ways give the same  
answers (as returned by all.equal) and sometimes different answers  
than getQ0. It may happen that they give a Q0 with negative  
eigenvalues, but they are very very small, and then, the likelihood  
computed by KalmanLike is a number (and not NaN). Here is a function  
allowing to compare the three methods
test <- function(phi,theta){
   out <- makeARIMA(phi,theta,NULL)
   Q0bis <- Q0bis(phi,theta)
   Q0ter <- Q0ter(phi,theta)
   mod <- out
   modbis <- out
   modter <- out
   modbis$Pn <- Q0bis
   modter$Pn <- Q0ter
   set.seed(1)
   x <- arima.sim(100,model=list(ar=phi,ma=theta))
   s <- KalmanLike(x,mod=mod,fast=FALSE)
   sbis <- KalmanLike(x,modbis)
   ster <- KalmanLike(x,modter)
   test12 <- all.equal(out$Pn,Q0bis)
   test13 <- all.equal(out$Pn,Q0ter)
   test23 <- all.equal(Q0bis,Q0ter)
    
list(eigen=min(eigen(out$Pn)$value),eigenbis=min(eigen(Q0bis)$value),eigenter=min(eigen(Q0ter)$value),test12=test12,test13=test13,test23=test23,s=s,sbis=sbis,ster=ster)
}

And the test on the values of phi and theta above:
> test(phi,theta)
$eigen
[1] -19.15890

$eigenbis
[1] -9.680598e-23

$eigenter
[1] 1.859864e-23

$test12
[1] "Mean relative difference: 0.4255719"

$test13
[1] "Mean relative difference: 0.4255724"

$test23
[1] TRUE

$s
$s$Lik
ssq
NaN

$s$s2
      ssq
0.971436


$sbis
$sbis$Lik
       ssq
0.1322859

$sbis$s2
       ssq
0.9789775


$ster
$ster$Lik
       ssq
0.1322859

$ster$s2
       ssq
0.9789775



Here are my questions:
1) Does someone understand this strange behaviour of Q0 ?
2) Should I report this as a bug ?

By the way, Q0bis is only twice slower than makeARIMA() (but it is not  
optimised), and Q0ter is 50 times slower than Q0bis.

For information:
> sessionInfo()

R version 2.10.1 (2009-12-14)
x86_64-pc-linux-gnu

locale:
  [1] LC_CTYPE=fr_FR.utf8       LC_NUMERIC=C
  [3] LC_TIME=fr_FR.utf8        LC_COLLATE=fr_FR.utf8
  [5] LC_MONETARY=C             LC_MESSAGES=fr_FR.utf8
  [7] LC_PAPER=fr_FR.utf8       LC_NAME=C
  [9] LC_ADDRESS=C              LC_TELEPHONE=C
[11] LC_MEASUREMENT=fr_FR.utf8 LC_IDENTIFICATION=C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base


Best,

Raphael Rossignol

--
Assistant Professor of Mathematics
Univ. Paris-Sud 11


From regista10 at gmail.com  Wed Jul 20 16:40:12 2011
From: regista10 at gmail.com (WADA Kazuya)
Date: Wed, 20 Jul 2011 23:40:12 +0900
Subject: [Rd] Compiling R-2.13.1 with MKL in windows 7 64bit
Message-ID: <CAG-dEyboq7Yn0mvb6mdYSheq02sO4GQt3DPZpuHctCFLo7FTFQ@mail.gmail.com>

Hi

 I don't know how to complie R-2.13.1 with MKL in windows 7 64bit.

 I can compile normal R in windows using Rtools under R-admin.html description.
 but, this way doesn't use ./configure file.(use MkRules.local file as
setting.).
 In 'R-admin.html' description, use ./configure --with-blas="$MKL" in
 which case with MKL compile.
 But, There is no description when I use MkRules.local setting file.
 So, I don't understand what I could describe in MkRules.local file in
 case of compile R-2.13.1 with MKL in windows.

 Does anyone know how to complie R with MKL in windows?

 Thanks!


From jfox at mcmaster.ca  Wed Jul 20 18:18:35 2011
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 20 Jul 2011 12:18:35 -0400
Subject: [Rd] testing a Windows custom installer
In-Reply-To: <4E26ECAB.2070106@gmail.com>
References: <008801cc46e9$67a2d270$36e87750$@mcmaster.ca>
	<4E26ECAB.2070106@gmail.com>
Message-ID: <009a01cc46f8$add54250$097fc6f0$@mcmaster.ca>

Dear Duncan,

First, thanks to Duncan and others for addressing my question.

In light of Duncan's remarks, I've arrived at the following procedure, which
seems to work:

(1) Install R using my custom installer.

(2) Uninstall the newly installed R. This clears the registry but leaves my
other installed versions of R undisturbed.

(3) Reinstall R using the custom installer.

BTW, following this procedure, I found that an option in the installer that
wasn't set the way I want and was able to fix it.

Best,
 John

> -----Original Message-----
> From: Duncan Murdoch [mailto:murdoch.duncan at gmail.com]
> Sent: July-20-11 10:57 AM
> To: John Fox
> Cc: R-devel at r-project.org
> Subject: Re: [Rd] testing a Windows custom installer
> 
> On 20/07/2011 10:29 AM, John Fox wrote:
> > Dear R-devel list members,
> >
> > For several years, I've created a custom R installer for my students
> > who use Windows. When I test the installer on my own Windows machines,
> > selections in the installation dialogs reflect my previous choices,
> > which I suppose are saved in the Windows registry.
> >
> > I'd like to be able to see what a student who has never installed R
> > before will see, to verify that an installation that takes all
> > defaults in the custom installer produces the desired result. In the
> > past, I've dealt with this problem by finding a Windows machine on
> > which R has never been installed, but that's inconvenient (and, as R
> > proliferates will, I hope, become impossible!). Is there a better
> approach?
> 
> The settings are saved to the registry by the installer, Inno Setup, and
> restored automatically (for most of them), or using its
> "GetPreviousData" function (for the R specific ones).  I don't think
> there is a way to tell the installer to ignore the previous data, but
> there might be.
> 
> I believe if you uninstall R then the settings will be forgotten,
> because the settings are saved in locations like
> 
> HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Uninstall\R
> for Windows 2.13.0_is1 in string values named "Inno Setup: *" for the
> ones used internally, and "Inno Setup Codefile:
> *" for the ones specific to R.
> 
> It's probably okay to delete that whole key and then it will forget the
> old settings, but that seems risky.
> 
> There's a command line option to the installer called "/SAVEINF" which
> saves the settings to a file and "/LOADINF" which loads them from a
> file, but I don't see how to tell it to just ignore the registry
> settings.
> 
> Duncan Murdoch


From edd at debian.org  Wed Jul 20 18:20:49 2011
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 20 Jul 2011 11:20:49 -0500
Subject: [Rd] Randomness not due to seed
In-Reply-To: <0348E92E-B044-4954-95F9-BE8C32C22E21@gmail.com>
References: <1311081181602-3678082.post@n4.nabble.com>
	<BLU166-w543D75409AB00B2B1DB127BE4C0@phx.gbl>
	<4E26C175.1070806@gmail.com>
	<CABFfbXutiefddok9WWSxq0vYiwAomLA3hcmma5nW0PwJyJ8UfQ@mail.gmail.com>
	<20006.55884.14520.313303@max.nulle.part>
	<0348E92E-B044-4954-95F9-BE8C32C22E21@gmail.com>
Message-ID: <20007.97.615030.239241@max.nulle.part>


On 20 July 2011 at 18:02, peter dalgaard wrote:
| 
| On Jul 20, 2011, at 15:38 , Dirk Eddelbuettel wrote:
| 
| > 
| > On 20 July 2011 at 14:03, Jeroen Ooms wrote:
| > | >> I think Bill Dunlap's answer addressed it:  the claim appears to be false.
| > | 
| > | Here is another example where there is randomness that is not due to
| > | the seed. On the same machine, the same R binary, but through another
| > | interface. First directly in the shell:
| > | 
| > | > sessionInfo()
| > | R version 2.13.1 (2011-07-08)
| > | Platform: i686-pc-linux-gnu (32-bit)
| > | 
| > | locale:
| > |  [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
| > |  [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
| > |  [5] LC_MONETARY=C              LC_MESSAGES=en_US.UTF-8
| > |  [7] LC_PAPER=en_US.UTF-8       LC_NAME=C
| > |  [9] LC_ADDRESS=C               LC_TELEPHONE=C
| > | [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C
| > | 
| > | attached base packages:
| > | [1] stats     graphics  grDevices utils     datasets  methods   base
| > | 
| > | > set.seed(123)
| > | > print(coef(lm(dist~speed, data=cars)),digits=22)
| > |               (Intercept)                     speed
| > | -17.579094890510951643137   3.932408759124087715975
| > 
| > That's PBKAC --- even double precision does NOT get you 22 digits precision.
| 
| Hmm, yes, but you would expect the SAME function on the SAME data to yield the same floating point number, and give the SAME printout on the SAME R on the SAME hardware... 
| 
| FWIW all the Mac versions that I can access give the same results as the eclipse version.
| 
| Let's look at the numbers side-by-side
| 
| -17.579094890510951643137   3.932408759124087715975
| -17.57909489051087703615    3.93240875912408460735
|                 !                           !
|  12.345678901234567890123   1.234567890123456789012
| 
| so we're seeing differences around the 15th/16th significant digit. This is consistent with a difference of about one unit of least precision in the actual objects, but there could conceivably be other explanations, e.g. the print() function picking up random garbage. Jeroen: Could you save() the results from the two cases, load() them in a new session and compute the difference?

Yes 15 to 16 is common.  I should have added that to my post when I said '22
is too much'. And I did not want to give the impression that nine is what one
gets, nine is the minimum as per the libc docs I quoted but as you
illustrate, 15 to 16 can often be had.

Thanks for the follow-up.

Dirk

 
| > You may want to read up on 'what every computer scientist should know about
| > floating point arithmetic' by Goldberg (which is both a true internet classic)
| > and ponder why a common setting for the various 'epsilon' settings of general
| > convergence is set to of the constants supplied by the OS and/or its C
| > library. R has
| > 
| >  #define SINGLE_EPS     FLT_EPSILON
| >  [...]
| >  #define DOUBLE_EPS     DBL_EPSILON
| > 
| > in Constants.h. You can then chase the definition of FLT_EPSILON and
| > DBL_EPSILON through your system headers (which is a good exercise).
| > 
| > One place you may end up in the manual -- the following from the GNU libc
| > documentationon :Floating Point Parameters"
| > 
| > FLT_EPSILON
| >     This is the minimum positive floating point number of type float such that
| >     1.0 + FLT_EPSILON != 1.0 is true. It's supposed to be no greater than 1E-5. 
| > 
| > DBL_EPSILON
| > LDBL_EPSILON
| >     These are similar to FLT_EPSILON, but for the data types double and long
| >     double, respectively. The type of the macro's value is the same as the type
| >     it describes. The values are not supposed to be greater than 1E-9.
| > 
| > So there -- nine digits. 
| > 
| > Dirk 
| > 
| > 
| > | # And this is through eclipse (java)
| > | 
| > | > sessionInfo()
| > | R version 2.13.1 (2011-07-08)
| > | Platform: i686-pc-linux-gnu (32-bit)
| > | 
| > | locale:
| > |  [1] LC_CTYPE=en_US.UTF-8          LC_NUMERIC=C
| > |  [3] LC_TIME=en_US.UTF-8           LC_COLLATE=en_US.UTF-8
| > |  [5] LC_MONETARY=en_US.UTF-8       LC_MESSAGES=en_US.UTF-8
| > |  [7] LC_PAPER=en_US.UTF-8          LC_NAME=en_US.UTF-8
| > |  [9] LC_ADDRESS=en_US.UTF-8        LC_TELEPHONE=en_US.UTF-8
| > | [11] LC_MEASUREMENT=en_US.UTF-8    LC_IDENTIFICATION=en_US.UTF-8
| > | 
| > | attached base packages:
| > | [1] stats     graphics  grDevices utils     datasets  methods   base
| > | 
| > | other attached packages:
| > | [1] rj_0.5.2-1
| > | 
| > | loaded via a namespace (and not attached):
| > | [1] rJava_0.9-1  tools_2.13.1
| > | 
| > | > set.seed(123)
| > | > print(coef(lm(dist~speed, data=cars)),digits=22)
| > |              (Intercept)                    speed
| > | 
| 
| > | 
| > | ______________________________________________
| > | R-devel at r-project.org mailing list
| > | https://stat.ethz.ch/mailman/listinfo/r-devel
| > 
| > -- 
| > Gauss once played himself in a zero-sum game and won $50.
| >                      -- #11 at http://www.gaussfacts.com
| > 
| > ______________________________________________
| > R-devel at r-project.org mailing list
| > https://stat.ethz.ch/mailman/listinfo/r-devel
| 
| -- 
| Peter Dalgaard
| Center for Statistics, Copenhagen Business School
| Solbjerg Plads 3, 2000 Frederiksberg, Denmark
| Phone: (+45)38153501
| Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
| 

-- 
Gauss once played himself in a zero-sum game and won $50.
                      -- #11 at http://www.gaussfacts.com


From murdoch.duncan at gmail.com  Wed Jul 20 21:01:37 2011
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Wed, 20 Jul 2011 15:01:37 -0400
Subject: [Rd] Compiling R-2.13.1 with MKL in windows 7 64bit
In-Reply-To: <CAG-dEyboq7Yn0mvb6mdYSheq02sO4GQt3DPZpuHctCFLo7FTFQ@mail.gmail.com>
References: <CAG-dEyboq7Yn0mvb6mdYSheq02sO4GQt3DPZpuHctCFLo7FTFQ@mail.gmail.com>
Message-ID: <4E272611.7030308@gmail.com>

On 20/07/2011 10:40 AM, WADA Kazuya wrote:
> Hi
>
>   I don't know how to complie R-2.13.1 with MKL in windows 7 64bit.
>
>   I can compile normal R in windows using Rtools under R-admin.html description.
>   but, this way doesn't use ./configure file.(use MkRules.local file as
> setting.).
>   In 'R-admin.html' description, use ./configure --with-blas="$MKL" in
>   which case with MKL compile.
>   But, There is no description when I use MkRules.local setting file.
>   So, I don't understand what I could describe in MkRules.local file in
>   case of compile R-2.13.1 with MKL in windows.
>
>   Does anyone know how to complie R with MKL in windows?


The Windows make files assume you're using MinGW.  If you're not using 
that compiler, you're on your own.

Duncan Murdoch


From simon.urbanek at r-project.org  Wed Jul 20 21:35:28 2011
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Wed, 20 Jul 2011 15:35:28 -0400
Subject: [Rd] Compiling R-2.13.1 with MKL in windows 7 64bit
In-Reply-To: <4E272611.7030308@gmail.com>
References: <CAG-dEyboq7Yn0mvb6mdYSheq02sO4GQt3DPZpuHctCFLo7FTFQ@mail.gmail.com>
	<4E272611.7030308@gmail.com>
Message-ID: <A922F564-61E7-4C4D-801A-DADFF1701DBF@r-project.org>


On Jul 20, 2011, at 3:01 PM, Duncan Murdoch wrote:

> On 20/07/2011 10:40 AM, WADA Kazuya wrote:
>> Hi
>> 
>>  I don't know how to complie R-2.13.1 with MKL in windows 7 64bit.
>> 
>>  I can compile normal R in windows using Rtools under R-admin.html description.
>>  but, this way doesn't use ./configure file.(use MkRules.local file as
>> setting.).
>>  In 'R-admin.html' description, use ./configure --with-blas="$MKL" in
>>  which case with MKL compile.
>>  But, There is no description when I use MkRules.local setting file.
>>  So, I don't understand what I could describe in MkRules.local file in
>>  case of compile R-2.13.1 with MKL in windows.
>> 
>>  Does anyone know how to complie R with MKL in windows?
> 
> 
> The Windows make files assume you're using MinGW.  If you're not using that compiler, you're on your own.
> 

I think the questions was about MKL BLAS with MinGW.

WADA, see Windows FAQ 8.2 - in general, you don't need to compile R against a particular BLAS - you just use shared BLAS and put your favorite BLAS implementation (like MKL) instead of Rblas.dll. The instructions you quote are not for you - they are for unix. I did not use MKL on Windows myself (my tests on OSX indicated that it's slower than ATALS and thus not worth the hassle), so you'll have to try it for yourself.

Cheers,
Simon


From ripley at stats.ox.ac.uk  Wed Jul 20 22:54:53 2011
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 20 Jul 2011 21:54:53 +0100 (BST)
Subject: [Rd] Compiling R-2.13.1 with MKL in windows 7 64bit
In-Reply-To: <A922F564-61E7-4C4D-801A-DADFF1701DBF@r-project.org>
References: <CAG-dEyboq7Yn0mvb6mdYSheq02sO4GQt3DPZpuHctCFLo7FTFQ@mail.gmail.com>
	<4E272611.7030308@gmail.com>
	<A922F564-61E7-4C4D-801A-DADFF1701DBF@r-project.org>
Message-ID: <alpine.LFD.2.02.1107202147240.26703@gannet.stats.ox.ac.uk>

On Wed, 20 Jul 2011, Simon Urbanek wrote:

>
> On Jul 20, 2011, at 3:01 PM, Duncan Murdoch wrote:
>
>> On 20/07/2011 10:40 AM, WADA Kazuya wrote:
>>> Hi
>>>
>>>  I don't know how to complie R-2.13.1 with MKL in windows 7 64bit.
>>>
>>>  I can compile normal R in windows using Rtools under R-admin.html description.
>>>  but, this way doesn't use ./configure file.(use MkRules.local file as
>>> setting.).
>>>  In 'R-admin.html' description, use ./configure --with-blas="$MKL" in
>>>  which case with MKL compile.
>>>  But, There is no description when I use MkRules.local setting file.
>>>  So, I don't understand what I could describe in MkRules.local file in
>>>  case of compile R-2.13.1 with MKL in windows.
>>>
>>>  Does anyone know how to complie R with MKL in windows?
>>
>>
>> The Windows make files assume you're using MinGW.  If you're not using that compiler, you're on your own.
>>
>
> I think the questions was about MKL BLAS with MinGW.
>
> WADA, see Windows FAQ 8.2 - in general, you don't need to compile R 
> against a particular BLAS - you just use shared BLAS and put your 
> favorite BLAS implementation (like MKL) instead of Rblas.dll. The 
> instructions you quote are not for you - they are for unix. I did 
> not use MKL on Windows myself (my tests on OSX indicated that it's 
> slower than ATALS and thus not worth the hassle), so you'll have to 
> try it for yourself.

Yes, I think that was the question, and I have used MKL.  But its use 
on Windows is a pain (it took my staff man-days to get the license 
manager to work), and I would suggest using the Goto BLAS instead: 
instructions are in the current R documentation (the rw-FAQ AFAIR).

>
> Cheers,
> Simon
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From dajohnston at ucdavis.edu  Wed Jul 20 20:14:10 2011
From: dajohnston at ucdavis.edu (David A. Johnston)
Date: Wed, 20 Jul 2011 11:14:10 -0700 (PDT)
Subject: [Rd] apply() returning a list?
In-Reply-To: <D9FF0717-ED0D-4892-8ACF-7D241972370E@comcast.net>
References: <1311118493785-3679619.post@n4.nabble.com>
	<D9FF0717-ED0D-4892-8ACF-7D241972370E@comcast.net>
Message-ID: <1311185650286-3681695.post@n4.nabble.com>

I understand that the coercion to a list is somewhat trivial.  The coercion
is a little less obvious when apply()'s output is a matrix.  For example, 

> x = matrix(1:12, 3, 4)
> apply(x, 1, range)
     [,1] [,2] [,3]
[1,]    1    2    3
[2,]   10   11   12

> structure(as.list(as.data.frame(apply(x, 1, range))), names = NULL)
[[1]]
[1]  1 10

[[2]]
[1]  2 11

[[3]]
[1]  3 12


Still relatively easy, but I do think it would be logical and intuitive if
apply() had the same types of arguments as the other similar *apply
functions.  

-David Johnston





--
View this message in context: http://r.789695.n4.nabble.com/apply-returning-a-list-tp3679619p3681695.html
Sent from the R devel mailing list archive at Nabble.com.


From radler12 at op.pl  Thu Jul 21 10:54:47 2011
From: radler12 at op.pl (Krystian Radlak)
Date: Thu, 21 Jul 2011 01:54:47 -0700 (PDT)
Subject: [Rd] Saving data into a text or CSV file using foreach/doSNOW
Message-ID: <1311238487719-3683155.post@n4.nabble.com>

Dear list, 

I am using the foreach/doSNOW packages. I want to compute some data parallel
and save results in every iteration  into a text or CSV file, but then I see
in file there are some errors (some data is not saved to file and there are
empty lines). I have tested every function to append data into file and I
also tested append data to one file or more (For two processor I have saved
data into two files.), but I could solve this problem. How can I solve this
problem?

foreach(i = 1:n,.packages = "xyz") %dopar%  my_function(i)

my_function(i)
{
    #some code

zz <-
file(paste(cmd_args[7],toString(i%%as.numeric(cmd_args[3])),".txt",sep=""),
"at")
write(rr,file=zz)
close(zz)
} 


Best,

Krystian

--
View this message in context: http://r.789695.n4.nabble.com/Saving-data-into-a-text-or-CSV-file-using-foreach-doSNOW-tp3683155p3683155.html
Sent from the R devel mailing list archive at Nabble.com.


From ripley at stats.ox.ac.uk  Thu Jul 21 11:36:33 2011
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 21 Jul 2011 10:36:33 +0100 (BST)
Subject: [Rd] apply() returning a list?
In-Reply-To: <1311185650286-3681695.post@n4.nabble.com>
References: <1311118493785-3679619.post@n4.nabble.com>
	<D9FF0717-ED0D-4892-8ACF-7D241972370E@comcast.net>
	<1311185650286-3681695.post@n4.nabble.com>
Message-ID: <alpine.LFD.2.02.1107211033200.22806@toucan.stats.ox.ac.uk>

On Wed, 20 Jul 2011, David A. Johnston wrote:

> I understand that the coercion to a list is somewhat trivial.  The coercion
> is a little less obvious when apply()'s output is a matrix.  For example,
>
>> x = matrix(1:12, 3, 4)
>> apply(x, 1, range)
>     [,1] [,2] [,3]
> [1,]    1    2    3
> [2,]   10   11   12
>
>> structure(as.list(as.data.frame(apply(x, 1, range))), names = NULL)
> [[1]]
> [1]  1 10
>
> [[2]]
> [1]  2 11
>
> [[3]]
> [1]  3 12
>
>
> Still relatively easy, but I do think it would be logical and intuitive if
> apply() had the same types of arguments as the other similar *apply
> functions.

But you are not the person who would have to maintain the added 
complexity.  There need to be really good arguments for adding code to 
the standard packages (and base in particular): nothing is stopping 
you writing daj_apply() for your own use, and providing you give due 
credit (which many R packages do not for code modified from R) 
distributing it.

>
> -David Johnston
>
>
>
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/apply-returning-a-list-tp3679619p3681695.html
> Sent from the R devel mailing list archive at Nabble.com.
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ripley at stats.ox.ac.uk  Thu Jul 21 11:38:58 2011
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 21 Jul 2011 10:38:58 +0100 (BST)
Subject: [Rd] [R] Problem compiling in extra/xdr
In-Reply-To: <alpine.LFD.2.02.1107200812040.1173@gannet.stats.ox.ac.uk>
References: <alpine.LNX.2.00.1107181539100.13110@waverley.Belkin>
	<alpine.LFD.2.02.1107182251020.4163@gannet.stats.ox.ac.uk>
	<alpine.DEB.2.00.1107181954150.5589@myrtle>
	<alpine.LFD.2.02.1107190631570.28269@gannet.stats.ox.ac.uk>
	<alpine.LFD.2.02.1107200812040.1173@gannet.stats.ox.ac.uk>
Message-ID: <alpine.LFD.2.02.1107211038100.22806@toucan.stats.ox.ac.uk>

On Wed, 20 Jul 2011, Prof Brian Ripley wrote:

> 1) The current R-patched should compile src/extra/xdr on 32-bit Linux 
> systems.
>
> 2) Longer-term or on a 64-bit platform the solution is to make use of 
> libtirpc: you would need both this installed (common now) and its headers 
> (unlikely).
>
> Then if you arrange for /usr/include/tirpc to be in the C include path (not 
> easy to do portably, but using C_INCLUDE_PATH will work on Linux), the tirpc 
> versions should be found.

For the record, a more portable way is described in the R-admin manual 
in 2.13.1 patched and R-devel.

> On Tue, 19 Jul 2011, Prof Brian Ripley wrote:
>
>> On Mon, 18 Jul 2011, Allin Cottrell wrote:
>> 
>>> On Mon, 18 Jul 2011, Prof Brian Ripley wrote:
>>> 
>>>> 1) R-help is the wrong list: see the posting guide.  I've moved this to 
>>>> R-devel.
>>>> 
>>>> 2) A glibc system should not be compiling in that directory.  glibc 2.14 
>>>> is rather recent and NEWS does say
>>>> 
>>>> * The RPC implementation in libc is obsoleted.  Old programs keep working
>>>>  but new programs cannot be linked with the routines in libc anymore.
>>>>  Programs in need of RPC functionality must be linked against TI-RPC.
>>>>  The TI-RPC implementation is IPv6 enabled and there are other benefits.
>>>>
>>>>  Visible changes of this change include (obviously) the inability to link
>>>>  programs using RPC functions without referencing the TI-RPC library and 
>>>> the
>>>>  removal of the RPC headers from the glibc headers.
>>>>  Implemented by Ulrich Drepper.
>>>> 
>>>> So the answer seems to be that your libc is too new.
>>> 
>>> OK, thanks. I should have remembered the info about RPC in the glibc-2.14 
>>> news. Then there will presumably be a problem building current R on 
>>> current Fedora?
>> 
>> What is 'current Fedora'?  glibc 2.14 postdates the current release, Fedora 
>> 15, which uses 2.13.   I do not know what Fedora 16 will use in several 
>> months ....
>> 
>> The main problem will be that the xdr included in R is only for platforms 
>> with 32-bit longs -- but that may be true for your i686 Linux.  It needs 
>> _X86_ defined to compile for i686: I would have expected that to be true on 
>> your platform, but am testing a cleaned-up version.  If that works it will 
>> appear in R-patched within 24 hours.
>> 
>>> 
>>>> On Mon, 18 Jul 2011, Allin Cottrell wrote:
>>>> 
>>>>> I'm building R 2.13.1 on i686-pc-linux-gnu, using gcc 4.6.1
>>>>> and with glibc 2.14.
>>>>> 
>>>>> I get this error:
>>>>> 
>>>>> In file included from xdr.c:61:0:
>>>>> ./rpc/types.h:63:14: error: conflicting types for 'malloc'
>>>>> make[4]: *** [xdr.o] Error 1
>>>>> 
>>>>> I can make the build proceed some by commenting out the declaration 
>>>>> "extern char *malloc();" in xdr/rpc/types.h,
>>>>> but then I get a slew of other errors:
>>>>> 
>>>>> xdr_float.c: In function 'xdr_float':
>>>>> xdr_float.c:119:21: error: storage size of 'is' isn't known
>>>>> xdr_float.c:120:20: error: storage size of 'vs' isn't known
>>>>> 
>>>>> and so on.
>>>>> 
>>>>> config.log is rather big to post here; I'm putting it at
>>>>> http://www.wfu.edu/~cottrell/tmp/R.config.log .
>>>>> 
>>>>> --
>>>>> Allin Cottrell
>>>>> Department of Economics
>>>>> Wake Forest University, NC
>>>>> 
>>>>> ______________________________________________
>>>>> R-help at r-project.org mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide 
>>>>> http://www.R-project.org/posting-guide.html
>>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>> 
>>>> 
>>>> 
>>> 
>>> -- 
>>> Allin Cottrell
>>> Department of Economics
>>> Wake Forest University
>>> 
>> 
>> -- 
>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>> University of Oxford,             Tel:  +44 1865 272861 (self)
>> 1 South Parks Road,                     +44 1865 272866 (PA)
>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>> 
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From maechler at stat.math.ethz.ch  Thu Jul 21 14:56:33 2011
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 21 Jul 2011 14:56:33 +0200
Subject: [Rd] short documentation suggestion for "by" --- reference
 simplify2array and ave
In-Reply-To: <CAPr7RtWWUDnQRPdx-6-Mc5vX7Shx1vAcTEL=VeDRXF-Om=bcJg@mail.gmail.com>
References: <CAPr7RtWWUDnQRPdx-6-Mc5vX7Shx1vAcTEL=VeDRXF-Om=bcJg@mail.gmail.com>
Message-ID: <20008.8705.441827.740368@stat.math.ethz.ch>

>>>>> "iw" == ivo welch <ivo.welch at gmail.com>
>>>>>     on Fri, 8 Jul 2011 20:47:08 -0700 writes:

    iw> in the documentation for "by", please change the "See
    iw> also" section to \seealso{\code{\link{tapply}},
    iw> \code{\link{simplify2array}}, \code{\link{ave}}}

    iw> (simplify2array, by, and ave should probably also be
    iw> mentioned in the "See also" section of "apply".)

    iw> hope this helps.

Yes, thank you.

I did (something like) that.

--
Martin Maechler, ETH Zurich

    iw> /iaw ---- Ivo Welch (ivo.welch at gmail.com)


From maechler at stat.math.ethz.ch  Thu Jul 21 15:40:17 2011
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 21 Jul 2011 15:40:17 +0200
Subject: [Rd] heatmap documentation typo
In-Reply-To: <BANLkTin_kDuhnpkQzJSFgrPVe8PZTdk3kA@mail.gmail.com>
References: <BANLkTin_kDuhnpkQzJSFgrPVe8PZTdk3kA@mail.gmail.com>
Message-ID: <20008.11329.164463.17700@stat.math.ethz.ch>


>>>>> Jonathan Dushoff <dushoff at mcmaster.ca>
>>>>>     on Thu, 30 Jun 2011 14:09:30 -0400 writes:

    > Under Details, the documentation says "[if Rowv or Colv] is 'NULL',
    > _no reordering_ will be done for the corresponding side."  In fact, as
    > explained elsewhere in the documentation, 'NA' is required, not
    > 'NULL'.

    > An anonymous expert suggested that I should attach an svn diff file to
    > this report, so I will.

Not necessary.  I've now committed the fix.

Thank you, Jonathan, for the report.

Martin Maechler, ETH Zurich

    > ----------------------------------------------------------------------
    > Index: heatmap.Rd
    > ===================================================================
    > --- heatmap.Rd	(revision 56237)
    > +++ heatmap.Rd	(working copy)
    > @@ -94,7 +94,7 @@
    > If either is missing, as by default, then the ordering of the
    > corresponding dendrogram is by the mean value of the rows/columns,
    > i.e., in the case of rows, \code{Rowv <- rowMeans(x, na.rm=na.rm)}.
    > -  If either is \code{\link{NULL}}, \emph{no reordering} will be done for
    > +  If either is \code{\link{NA}}, \emph{no reordering} will be done for
    > the corresponding side.
 
    > By default (\code{scale = "row"}) the rows are scaled to have mean

    > ----------------------------------------------------------------------


From ligges at statistik.tu-dortmund.de  Thu Jul 21 18:13:12 2011
From: ligges at statistik.tu-dortmund.de (Uwe Ligges)
Date: Thu, 21 Jul 2011 18:13:12 +0200
Subject: [Rd] requiring NAMESPACE re-installation marked as old.packages?
In-Reply-To: <4E253F0B.70506@fhcrc.org>
References: <4E253F0B.70506@fhcrc.org>
Message-ID: <4E285018.6040106@statistik.tu-dortmund.de>

Most if not all packages will need a re-installation, not only those you 
talked about (in particular there is a new snow on CRAN already). Hence 
I think it is not worth the effort to find out what "old" means. Please 
note this may change during the development cycle and you may even need 
one or more re-installations before the next R is released. S4 packages 
may even need re-installation if other S4 packages (the former depend 
on) change.

Best,
Uwe




On 19.07.2011 10:23, Martin Morgan wrote:
> It would be convenient if, under R-devel r56422, packages that require
> re-installation because they do not have a NAMESPACE were marked as
> old.packages, so their lack of functionality can be discovered more easily.
>
>  > "snow" %in% row.names(old.packages())
> [1] FALSE
>  > library(snow)
> Error in library(snow) :
> package 'snow' does not have a NAMESPACE and should be re-installed
>  > install.packages("snow", repos="http://cran.r-project.org")
> Installing package(s) into
> '/home/mtmorgan/R/x86_64-unknown-linux-gnu-library/2.14'
> (as 'lib' is unspecified)
> trying URL 'http://cran.r-project.org/src/contrib/snow_0.3-5.tar.gz'
> Content type 'application/x-gzip' length 21059 bytes (20 Kb)
> opened URL
> ==================================================
> downloaded 20 Kb
>
> * installing *source* package 'snow' ...
> ** R
> ** inst
> ** Creating default NAMESPACE file
> ** preparing package for lazy loading
> ** help
> *** installing help indices
> ** building package indices ...
> ** testing if installed package can be loaded
> Warning: running .First.lib() for package 'snow' as .onLoad/.onAssign
> were not found
> Error in initDefaultClusterOptions() :
> cannot change value of locked binding for 'defaultClusterOptions'
> Error: loading failed
> Execution halted
> ERROR: loading failed
> * removing '/home/mtmorgan/R/x86_64-unknown-linux-gnu-library/2.14/snow'
> * restoring previous
> '/home/mtmorgan/R/x86_64-unknown-linux-gnu-library/2.14/snow'
>
> The downloaded packages are in
> '/tmp/RtmpoGypnz/downloaded_packages'
> Warning message:
> In install.packages("snow", repos = "http://cran.r-project.org") :
> installation of package 'snow' had non-zero exit status
>
> Martin


From jmc at r-project.org  Thu Jul 21 19:29:39 2011
From: jmc at r-project.org (John Chambers)
Date: Thu, 21 Jul 2011 10:29:39 -0700
Subject: [Rd] Same class name, different package
Message-ID: <4E286203.8070104@r-project.org>

In principle, two separately developed packages could use the same class 
name, and a user could then attach both and attempt to use methods for 
both classes.

That has never worked, but some changes have been added to r-devel to 
handle this case.  The changes involve extending the "signature" class 
to include package information.  For compatibility, packages will need 
to be re-installed from a version of R labelled 56466 or later, although 
an attempt is made to fill in missing information.

John


From csr21 at cantab.net  Thu Jul 21 20:53:09 2011
From: csr21 at cantab.net (Christophe Rhodes)
Date: Thu, 21 Jul 2011 19:53:09 +0100
Subject: [Rd] --max-vsize
Message-ID: <87sjpzlb3e.fsf@cantab.net>

Hi,

In both R 2.13 and the SVN trunk, I observe odd behaviour with the
--max-vsize command-line argument:

1. passing a largeish value (about 260M or greater) makes mem.limits()
   report NA for the vsize limit; gc() continues to report a value...

2. ...but that value (and the actual limit) is wrong by a factor of 8.

I attach a patch for issue 2, lightly tested.  I believe that fixing
issue 1 involves changing the return convention of do_memlimits -- not
returning a specialized integer vector, but a more general numeric; I
wasn't confident to do that.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: 0001-src-main-memory.c-set-R_MaxVSize-when-assigning-to-v.patch
Type: text/x-diff
Size: 824 bytes
Desc: fix --max-vsize command-line argument
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20110721/3faef1b7/attachment.bin>
-------------- next part --------------

Please let me know if there's anything else I can do.

Best,

Christophe

From mtmorgan at fhcrc.org  Fri Jul 22 06:29:51 2011
From: mtmorgan at fhcrc.org (Martin Morgan)
Date: Thu, 21 Jul 2011 21:29:51 -0700
Subject: [Rd] reg.finalizer and connection gc -- which runs when (and why)?
Message-ID: <4E28FCBF.1070703@fhcrc.org>

With this set-up

options(warn = 1)
tf <- tempfile()
finalizer <- function(obj) {
     message("finalizer")
     close(obj$f)
}

this code works

ev <- new.env()
ev$f <- file(tf, "w")
reg.finalizer(ev, finalizer)
rm(ev)
gc()

whereas this (reversing the order of file() and reg.finalizer())

ev <- new.env()
reg.finalizer(ev, finalizer)
ev$f <- file(tf, "w")
rm(ev)

produces

 > gc()
Warning: closing unused connection 3 (/tmp/Rtmp9CWLtN/file6998ee7b)
finalizer
Error in close.connection(obj$f) : invalid connection

In some respects, it seems like the user should get a chance to clean up 
their mess before the system does it for them, so the above seems like a 
bug. But maybe there is another way to understand this?

Martin
-- 
Computational Biology
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N. PO Box 19024 Seattle, WA 98109

Location: M1-B861
Telephone: 206 667-2793


From rhurlin at gwdg.de  Fri Jul 22 11:11:10 2011
From: rhurlin at gwdg.de (Rainer Hurling)
Date: Fri, 22 Jul 2011 11:11:10 +0200
Subject: [Rd] Problem with creating default NAMESPACE on FreeBSD
Message-ID: <4E293EAE.2010102@gwdg.de>

After building and installing R-devel from 21/07/2011 on FreeBSD 
9.0-CURRENT (amd64), starting R gives the following warning:

[..snip..]
Beim Start - Warnmeldung:
package 'datasets' in options("defaultPackages") was not found

This happens with all versions after 14/07/2011. I think it is related to

NEWS Sat, 16 Jul 2011
CHANGES IN R-devel PACKAGE INSTALLATION
Packages without explicit ?NAMESPACE? files will have a default one 
created at build or INSTALL time, so all packages will have namespaces. 
A consequence of this is that ?.First.lib? functions need to be renamed, 
usually as ?.onLoad? but sometimes as ?.onAttach?.


Package 'datasets' seems to be installed correctly under 
/usr/local/lib/R/library/:

ls -l datasets/
total 18
-rw-r--r--  1 root  wheel  -  289 22 Jul 08:12:04 2011 DESCRIPTION
-rw-r--r--  1 root  wheel  - 5547 22 Jul 08:12:04 2011 INDEX
drwxr-xr-x  2 root  wheel  -  512 22 Jul 08:12:04 2011 Meta
drwxr-xr-x  2 root  wheel  -  512 22 Jul 08:12:04 2011 R
drwxr-xr-x  2 root  wheel  -  512 22 Jul 08:12:04 2011 data
drwxr-xr-x  2 root  wheel  -  512 22 Jul 08:12:04 2011 help
drwxr-xr-x  2 root  wheel  -  512 22 Jul 08:12:04 2011 html


Calling options("defaultPackages") shows all relevant packages:

$defaultPackages
[1] "datasets"  "utils"     "grDevices" "graphics"  "stats"     "methods"


But trying to load library 'datasets' fails with the following message:

library(datasets)
Fehler in library(datasets) :
   package 'datasets' does not have a NAMESPACE and should be re-installed


Googling around for some days now it looks like this does not happen to 
anyone else (at least there are no infos). Does anybody knows what is 
going on here?

Any help would be appreciated,
Rainer Hurling



sessionInfo()
R Under development (unstable) (2011-07-21 r56467)
Platform: amd64-portbld-freebsd9.0 (64-bit)

locale:
[1] 
de_DE.ISO8859-15/de_DE.ISO8859-15/de_DE.ISO8859-15/C/de_DE.ISO8859-15/de_DE.ISO8859-15

attached base packages:
[1] stats     graphics  grDevices utils     methods   base


From ripley at stats.ox.ac.uk  Fri Jul 22 14:09:51 2011
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 22 Jul 2011 13:09:51 +0100 (BST)
Subject: [Rd] Problem with creating default NAMESPACE on FreeBSD
In-Reply-To: <4E293EAE.2010102@gwdg.de>
References: <4E293EAE.2010102@gwdg.de>
Message-ID: <alpine.LFD.2.02.1107221258260.19748@gannet.stats.ox.ac.uk>

It does not seem to happen to anyone else, including me on FreeBSD. 
Is this a completely clean install?  src/library/datasets/Makefile 
definitely installs a NAMESPACE file as part of the mkR1 target, so I 
would check that you have a src/library/datasets/NAMESPACE file in 
your sources.

(There was a short window on the 16th when it happened as a commit for 
src/library/datasets/NAMESPACE was missed at first.)


On Fri, 22 Jul 2011, Rainer Hurling wrote:

> After building and installing R-devel from 21/07/2011 on FreeBSD 9.0-CURRENT 
> (amd64), starting R gives the following warning:
>
> [..snip..]
> Beim Start - Warnmeldung:
> package 'datasets' in options("defaultPackages") was not found
>
> This happens with all versions after 14/07/2011. I think it is related to
>
> NEWS Sat, 16 Jul 2011
> CHANGES IN R-devel PACKAGE INSTALLATION
> Packages without explicit ?NAMESPACE? files will have a default one created 
> at build or INSTALL time, so all packages will have namespaces. A consequence 
> of this is that ?.First.lib? functions need to be renamed, usually as 
> ?.onLoad? but sometimes as ?.onAttach?.
>
>
> Package 'datasets' seems to be installed correctly under 
> /usr/local/lib/R/library/:
>
> ls -l datasets/
> total 18
> -rw-r--r--  1 root  wheel  -  289 22 Jul 08:12:04 2011 DESCRIPTION
> -rw-r--r--  1 root  wheel  - 5547 22 Jul 08:12:04 2011 INDEX
> drwxr-xr-x  2 root  wheel  -  512 22 Jul 08:12:04 2011 Meta
> drwxr-xr-x  2 root  wheel  -  512 22 Jul 08:12:04 2011 R
> drwxr-xr-x  2 root  wheel  -  512 22 Jul 08:12:04 2011 data
> drwxr-xr-x  2 root  wheel  -  512 22 Jul 08:12:04 2011 help
> drwxr-xr-x  2 root  wheel  -  512 22 Jul 08:12:04 2011 html
>
>
> Calling options("defaultPackages") shows all relevant packages:
>
> $defaultPackages
> [1] "datasets"  "utils"     "grDevices" "graphics"  "stats"     "methods"
>
>
> But trying to load library 'datasets' fails with the following message:
>
> library(datasets)
> Fehler in library(datasets) :
>  package 'datasets' does not have a NAMESPACE and should be re-installed
>
>
> Googling around for some days now it looks like this does not happen to 
> anyone else (at least there are no infos). Does anybody knows what is going 
> on here?
>
> Any help would be appreciated,
> Rainer Hurling
>
>
>
> sessionInfo()
> R Under development (unstable) (2011-07-21 r56467)
> Platform: amd64-portbld-freebsd9.0 (64-bit)
>
> locale:
> [1] 
> de_DE.ISO8859-15/de_DE.ISO8859-15/de_DE.ISO8859-15/C/de_DE.ISO8859-15/de_DE.ISO8859-15
>
> attached base packages:
> [1] stats     graphics  grDevices utils     methods   base
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ripley at stats.ox.ac.uk  Fri Jul 22 14:50:28 2011
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 22 Jul 2011 13:50:28 +0100 (BST)
Subject: [Rd] Problem with creating default NAMESPACE on FreeBSD
In-Reply-To: <alpine.LFD.2.02.1107221258260.19748@gannet.stats.ox.ac.uk>
References: <4E293EAE.2010102@gwdg.de>
	<alpine.LFD.2.02.1107221258260.19748@gannet.stats.ox.ac.uk>
Message-ID: <alpine.LFD.2.02.1107221346160.6165@gannet.stats.ox.ac.uk>

One possibility is that you are building from a tarball snapshot 
without mentioning it, and the (unspecified) tarball is incomplete.

For R-devel using tarballs is not a wise idea: they are really only 
checked carefully in pre-release periods.  (One of the checks is that 
AFAIR CRAN builds from the tarballs, but CRAN is not manned at 
present.)

But as far as I can the 'make dist' procedure does work now and did 
at some point in the last week.

On Fri, 22 Jul 2011, Prof Brian Ripley wrote:

> It does not seem to happen to anyone else, including me on FreeBSD. Is this a 
> completely clean install?  src/library/datasets/Makefile definitely installs 
> a NAMESPACE file as part of the mkR1 target, so I would check that you have a 
> src/library/datasets/NAMESPACE file in your sources.
>
> (There was a short window on the 16th when it happened as a commit for 
> src/library/datasets/NAMESPACE was missed at first.)
>
>
> On Fri, 22 Jul 2011, Rainer Hurling wrote:
>
>> After building and installing R-devel from 21/07/2011 on FreeBSD 
>> 9.0-CURRENT (amd64), starting R gives the following warning:
>> 
>> [..snip..]
>> Beim Start - Warnmeldung:
>> package 'datasets' in options("defaultPackages") was not found
>> 
>> This happens with all versions after 14/07/2011. I think it is related to
>> 
>> NEWS Sat, 16 Jul 2011
>> CHANGES IN R-devel PACKAGE INSTALLATION
>> Packages without explicit ?NAMESPACE? files will have a default one created 
>> at build or INSTALL time, so all packages will have namespaces. A 
>> consequence of this is that ?.First.lib? functions need to be renamed, 
>> usually as ?.onLoad? but sometimes as ?.onAttach?.
>> 
>> 
>> Package 'datasets' seems to be installed correctly under 
>> /usr/local/lib/R/library/:
>> 
>> ls -l datasets/
>> total 18
>> -rw-r--r--  1 root  wheel  -  289 22 Jul 08:12:04 2011 DESCRIPTION
>> -rw-r--r--  1 root  wheel  - 5547 22 Jul 08:12:04 2011 INDEX
>> drwxr-xr-x  2 root  wheel  -  512 22 Jul 08:12:04 2011 Meta
>> drwxr-xr-x  2 root  wheel  -  512 22 Jul 08:12:04 2011 R
>> drwxr-xr-x  2 root  wheel  -  512 22 Jul 08:12:04 2011 data
>> drwxr-xr-x  2 root  wheel  -  512 22 Jul 08:12:04 2011 help
>> drwxr-xr-x  2 root  wheel  -  512 22 Jul 08:12:04 2011 html
>> 
>> 
>> Calling options("defaultPackages") shows all relevant packages:
>> 
>> $defaultPackages
>> [1] "datasets"  "utils"     "grDevices" "graphics"  "stats"     "methods"
>> 
>> 
>> But trying to load library 'datasets' fails with the following message:
>> 
>> library(datasets)
>> Fehler in library(datasets) :
>>  package 'datasets' does not have a NAMESPACE and should be re-installed
>> 
>> 
>> Googling around for some days now it looks like this does not happen to 
>> anyone else (at least there are no infos). Does anybody knows what is going 
>> on here?
>> 
>> Any help would be appreciated,
>> Rainer Hurling
>> 
>> 
>> 
>> sessionInfo()
>> R Under development (unstable) (2011-07-21 r56467)
>> Platform: amd64-portbld-freebsd9.0 (64-bit)
>> 
>> locale:
>> [1] 
>> de_DE.ISO8859-15/de_DE.ISO8859-15/de_DE.ISO8859-15/C/de_DE.ISO8859-15/de_DE.ISO8859-15
>> 
>> attached base packages:
>> [1] stats     graphics  grDevices utils     methods   base
>> 
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>> 
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From luke-tierney at uiowa.edu  Fri Jul 22 15:05:15 2011
From: luke-tierney at uiowa.edu (luke-tierney at uiowa.edu)
Date: Fri, 22 Jul 2011 08:05:15 -0500
Subject: [Rd] reg.finalizer and connection gc -- which runs when (and
 why)?
In-Reply-To: <4E28FCBF.1070703@fhcrc.org>
References: <4E28FCBF.1070703@fhcrc.org>
Message-ID: <alpine.DEB.2.00.1107220758170.14799@luke-inspiron>

Connections use finalizers for cleanup.  Once finalizers become
eligible to run, the order in which they are run is unspecified.  If
you want to be sure yours runs first then you need to make sure the
connection one doesn't become eligible to run until yours has.

Best,

luke

On Thu, 21 Jul 2011, Martin Morgan wrote:

> With this set-up
>
> options(warn = 1)
> tf <- tempfile()
> finalizer <- function(obj) {
>    message("finalizer")
>    close(obj$f)
> }
>
> this code works
>
> ev <- new.env()
> ev$f <- file(tf, "w")
> reg.finalizer(ev, finalizer)
> rm(ev)
> gc()
>
> whereas this (reversing the order of file() and reg.finalizer())
>
> ev <- new.env()
> reg.finalizer(ev, finalizer)
> ev$f <- file(tf, "w")
> rm(ev)
>
> produces
>
>> gc()
> Warning: closing unused connection 3 (/tmp/Rtmp9CWLtN/file6998ee7b)
> finalizer
> Error in close.connection(obj$f) : invalid connection
>
> In some respects, it seems like the user should get a chance to clean up 
> their mess before the system does it for them, so the above seems like a bug. 
> But maybe there is another way to understand this?
>
> Martin
>

-- 
Luke Tierney
Statistics and Actuarial Science
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu


From alireza.s.mahani at gmail.com  Fri Jul 22 15:26:31 2011
From: alireza.s.mahani at gmail.com (Alireza Mahani)
Date: Fri, 22 Jul 2011 06:26:31 -0700 (PDT)
Subject: [Rd] Best practices for writing R functions
Message-ID: <1311341191873-3686674.post@n4.nabble.com>

I am developing an R package for internal use, and eventually for public
release. My understanding is that there is no easy way to avoid copying
function arguments in R (i.e. we don't have the concept of pointers in R),
which makes me wary of freely creating chains of function calls since each
function call implies data copy overhead.

Is the above assessment fair? Are there any good write-ups on best practices
for writing efficient R libraries that take into consideration the
above-mentioned limitations, and any others that might exist?

Thank you,
Alireza


--
View this message in context: http://r.789695.n4.nabble.com/Best-practices-for-writing-R-functions-tp3686674p3686674.html
Sent from the R devel mailing list archive at Nabble.com.


From spencer.graves at prodsyse.com  Fri Jul 22 17:14:22 2011
From: spencer.graves at prodsyse.com (Spencer Graves)
Date: Fri, 22 Jul 2011 08:14:22 -0700
Subject: [Rd] Best practices for writing R functions
In-Reply-To: <1311341191873-3686674.post@n4.nabble.com>
References: <1311341191873-3686674.post@n4.nabble.com>
Message-ID: <4E2993CE.4020903@prodsyse.com>

       From my personal experience and following this list some for a 
few years, the best practice is initially to ignore the compute time 
question, because the cost of your time getting it to do what you want 
is far greater, at least initially.  Don't worry about compute time 
until it becomes an issue.  When it does, the standard advice I've seen 
on this list is to experiment with different ways of writing the same 
thing in R, guided by "profiling R code", as described in the "Writing R 
Extensions" manual.  (Googling for "profiling R code" identified examples.)


       Hope this helps.
       Spencer Graves


On 7/22/2011 6:26 AM, Alireza Mahani wrote:
> I am developing an R package for internal use, and eventually for public
> release. My understanding is that there is no easy way to avoid copying
> function arguments in R (i.e. we don't have the concept of pointers in R),
> which makes me wary of freely creating chains of function calls since each
> function call implies data copy overhead.
>
> Is the above assessment fair? Are there any good write-ups on best practices
> for writing efficient R libraries that take into consideration the
> above-mentioned limitations, and any others that might exist?
>
> Thank you,
> Alireza
>
>
> --
> View this message in context: http://r.789695.n4.nabble.com/Best-practices-for-writing-R-functions-tp3686674p3686674.html
> Sent from the R devel mailing list archive at Nabble.com.
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From simon.urbanek at r-project.org  Fri Jul 22 17:36:27 2011
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Fri, 22 Jul 2011 11:36:27 -0400
Subject: [Rd] Best practices for writing R functions
In-Reply-To: <1311341191873-3686674.post@n4.nabble.com>
References: <1311341191873-3686674.post@n4.nabble.com>
Message-ID: <5344EE50-26A5-40BD-8771-C4805ED50C86@r-project.org>


On Jul 22, 2011, at 9:26 AM, Alireza Mahani wrote:

> I am developing an R package for internal use, and eventually for public
> release. My understanding is that there is no easy way to avoid copying
> function arguments in R (i.e. we don't have the concept of pointers in R),
> which makes me wary of freely creating chains of function calls since each
> function call implies data copy overhead.
> 
> Is the above assessment fair?

No. Although R maintains the illusion of pass-by-value, it does to considerable length to prevent copying if not needed:

> r=rnorm(1e6)
> tracemem(r)
[1] "<0x103fa2000>"
> f = function(x) x
> invisible(f(f(f(f(f(f(r)))))))

As you see not a single copy is created even though there are 6 function calls.

As Spencer said, you should not worry unless you see a sign of a problem.

Cheers,
Simon


> Are there any good write-ups on best practices
> for writing efficient R libraries that take into consideration the
> above-mentioned limitations, and any others that might exist?
> 
> Thank you,
> Alireza
> 
> 
> --
> View this message in context: http://r.789695.n4.nabble.com/Best-practices-for-writing-R-functions-tp3686674p3686674.html
> Sent from the R devel mailing list archive at Nabble.com.
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 
> 


From gmbecker at ucdavis.edu  Fri Jul 22 17:38:26 2011
From: gmbecker at ucdavis.edu (Gabriel Becker)
Date: Fri, 22 Jul 2011 08:38:26 -0700
Subject: [Rd] Best practices for writing R functions
In-Reply-To: <4E2993CE.4020903@prodsyse.com>
References: <1311341191873-3686674.post@n4.nabble.com>
	<4E2993CE.4020903@prodsyse.com>
Message-ID: <CADwqtCONeEqXTTg0OyDj=OJWeQxNKBm79M=iBg1jdy6JtO_w0Q@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20110722/25603248/attachment.pl>

From pgilbert at bank-banque-canada.ca  Fri Jul 22 20:59:31 2011
From: pgilbert at bank-banque-canada.ca (Paul Gilbert)
Date: Fri, 22 Jul 2011 18:59:31 +0000
Subject: [Rd] Best practices for writing R functions
In-Reply-To: <CADwqtCONeEqXTTg0OyDj=OJWeQxNKBm79M=iBg1jdy6JtO_w0Q@mail.gmail.com>
References: <1311341191873-3686674.post@n4.nabble.com>
	<4E2993CE.4020903@prodsyse.com>
	<CADwqtCONeEqXTTg0OyDj=OJWeQxNKBm79M=iBg1jdy6JtO_w0Q@mail.gmail.com>
Message-ID: <6441154A9FF1CD4386AF4ABF141A056D265A9E9B@WMEXOSCD2-N1.bocad.bank-banque-canada.ca>

> -----Original Message-----
> From: r-devel-bounces at r-project.org [mailto:r-devel-bounces at r-
> project.org] On Behalf Of Gabriel Becker
> Sent: July 22, 2011 11:38 AM
> To: Spencer Graves
> Cc: Alireza Mahani; r-devel at r-project.org
> Subject: Re: [Rd] Best practices for writing R functions
> 
> On Fri, Jul 22, 2011 at 8:14 AM, Spencer Graves
> <spencer.graves at prodsyse.com
> > wrote:
> 
> >      From my personal experience and following this list some for a
> few
> > years, the best practice is initially to ignore the compute time
> question,
> > because the cost of your time getting it to do what you want is far
> greater,
> > at least initially.  Don't worry about compute time until it becomes
> an
> > issue.  When it does, the standard advice I've seen on this list is
> to
> > experiment with different ways of writing the same thing in R, guided
> by
> > "profiling R code", as described in the "Writing R Extensions"
> manual.
> >  (Googling for "profiling R code" identified examples.)
> >
> >
> >      Hope this helps.
> >      Spencer Graves
> >
> >
> >
> > On 7/22/2011 6:26 AM, Alireza Mahani wrote:
> >
> >> I am developing an R package for internal use, and eventually for
> public
> >> release. My understanding is that there is no easy way to avoid
> copying
> >> function arguments in R (i.e. we don't have the concept of pointers
> in R),
> >> which makes me wary of freely creating chains of function calls
> since each
> >> function call implies data copy overhead.
> >>
> >
> AFAIK R does not automatically copy function arguments. R actually
> tries
> very hard to avoid copying while maintaining "pass by value"
> functionality.

But beware that a function makes a copy of the argument as soon as you try to modify something in that argument. So, for example, if you have a big list object as an argument and are going to modify one element in the list, you will save memory by making a local copy of the single element and modifying that. However, as others have said, I would not worry until you find there is a problem. 

Paul

> Consider the following functions and their output:
> 
> 
> nomod = function(dat)
>   {
>     TRUE
>   }
> 
> mod = function(dat, i)
>   {
>     dat[5] = 5
>     TRUE
>   }
> 
> 
> > vec = rep(0, times = 10)
> > tracemem(vec)
> [1] "<0x8c85978>"
> > nomod(vec)
> [1] TRUE
> > mod(vec)
> tracemem[0x8c85978 -> 0x8c85c70]: mod
> [1] TRUE
> 
> So in the nomod function, the argument never actually gets copied (that
> is
> what tracemem tracks). R only copies data when you modify an object,
> not
> when you simply pass it to a function
> 
> HTH,
> ~G
> 
> 
> 
> >
> >> Is the above assessment fair? Are there any good write-ups on best
> >> practices
> >> for writing efficient R libraries that take into consideration the
> >> above-mentioned limitations, and any others that might exist?
> >>
> >> Thank you,
> >> Alireza
> >>
> >>
> >> --
> >> View this message in context: http://r.789695.n4.nabble.com/**
> >> Best-practices-for-writing-R-**functions-
> tp3686674p3686674.**html<http://r.789695.n4.nabble.com/Best-practices-
> for-writing-R-functions-tp3686674p3686674.html>
> >> Sent from the R devel mailing list archive at Nabble.com.
> >>
> >> ______________________________**________________
> >> R-devel at r-project.org mailing list
> >> https://stat.ethz.ch/mailman/**listinfo/r-
> devel<https://stat.ethz.ch/mailman/listinfo/r-devel>
> >>
> >>
> > ______________________________**________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/**listinfo/r-
> devel<https://stat.ethz.ch/mailman/listinfo/r-devel>
> >
> 
> 
> 
> --
> Gabriel Becker
> Graduate Student
> Statistics Department
> University of California, Davis
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
====================================================================================

La version fran?aise suit le texte anglais.

------------------------------------------------------------------------------------

This email may contain privileged and/or confidential information, and the Bank of
Canada does not waive any related rights. Any distribution, use, or copying of this
email or the information it contains by other than the intended recipient is
unauthorized. If you received this email in error please delete it immediately from
your system and notify the sender promptly by email that you have done so. 

------------------------------------------------------------------------------------

Le pr?sent courriel peut contenir de l'information privil?gi?e ou confidentielle.
La Banque du Canada ne renonce pas aux droits qui s'y rapportent. Toute diffusion,
utilisation ou copie de ce courriel ou des renseignements qu'il contient par une
personne autre que le ou les destinataires d?sign?s est interdite. Si vous recevez
ce courriel par erreur, veuillez le supprimer imm?diatement et envoyer sans d?lai ?
l'exp?diteur un message ?lectronique pour l'aviser que vous avez ?limin? de votre
ordinateur toute copie du courriel re?u.

From hadley at rice.edu  Fri Jul 22 21:45:32 2011
From: hadley at rice.edu (Hadley Wickham)
Date: Fri, 22 Jul 2011 14:45:32 -0500
Subject: [Rd] Best practices for writing R functions
In-Reply-To: <6441154A9FF1CD4386AF4ABF141A056D265A9E9B@WMEXOSCD2-N1.bocad.bank-banque-canada.ca>
References: <1311341191873-3686674.post@n4.nabble.com>
	<4E2993CE.4020903@prodsyse.com>
	<CADwqtCONeEqXTTg0OyDj=OJWeQxNKBm79M=iBg1jdy6JtO_w0Q@mail.gmail.com>
	<6441154A9FF1CD4386AF4ABF141A056D265A9E9B@WMEXOSCD2-N1.bocad.bank-banque-canada.ca>
Message-ID: <CABdHhvHvB4vV5BymZfSguxCUH745nd6mrAk8EDw8Kk3RPN-C2Q@mail.gmail.com>

> But beware that a function makes a copy of the argument as soon as you try to modify something in that argument. So, for example, if you have a big list object as an argument and are going to modify one element in the list, you will save memory by making a local copy of the single element and modifying that. However, as others have said, I would not worry until you find there is a problem.

That's generally good practice in R - always modify the simplest
object possible.

Hadley


-- 
Assistant Professor / Dobelman Family Junior Chair
Department of Statistics / Rice University
http://had.co.nz/


From rhurlin at gwdg.de  Sat Jul 23 09:28:30 2011
From: rhurlin at gwdg.de (Rainer Hurling)
Date: Sat, 23 Jul 2011 09:28:30 +0200
Subject: [Rd] Problem with creating default NAMESPACE on FreeBSD
In-Reply-To: <alpine.LFD.2.02.1107221346160.6165@gannet.stats.ox.ac.uk>
References: <4E293EAE.2010102@gwdg.de>
	<alpine.LFD.2.02.1107221258260.19748@gannet.stats.ox.ac.uk>
	<alpine.LFD.2.02.1107221346160.6165@gannet.stats.ox.ac.uk>
Message-ID: <4E2A781E.6050209@gwdg.de>

On 22.07.2011 14:50 (UTC+1), Prof Brian Ripley wrote:
> One possibility is that you are building from a tarball snapshot without
> mentioning it, and the (unspecified) tarball is incomplete.
>
> For R-devel using tarballs is not a wise idea: they are really only
> checked carefully in pre-release periods. (One of the checks is that
> AFAIR CRAN builds from the tarballs, but CRAN is not manned at present.)
>
> But as far as I can the 'make dist' procedure does work now and did at
> some point in the last week.
>
> On Fri, 22 Jul 2011, Prof Brian Ripley wrote:
>
>> It does not seem to happen to anyone else, including me on FreeBSD. Is
>> this a completely clean install? src/library/datasets/Makefile
>> definitely installs a NAMESPACE file as part of the mkR1 target, so I
>> would check that you have a src/library/datasets/NAMESPACE file in
>> your sources.
>>
>> (There was a short window on the 16th when it happened as a commit for
>> src/library/datasets/NAMESPACE was missed at first.)

Many thanks for answering and sorry for the late reaction. We had some 
trouble in business yesterday.

It was not a totally clean installation before. I only deinstalled the 
old R version before installing the new one in the same place (make 
deinstall / make reinstall with a selfmade port math/R-devel for testing 
purposes). Other packages etc. had not been deleted before.

I forgot to mention that there were warnings in the build for several 
packages, like that:

installing to /usr/ports/math/R-devel/work/R-devel/library/mgcv/libs
** R
** data
** inst
** byte-compile and prepare package for lazy loading
** help
*** installing help indices
** building package indices ...
** testing if installed package can be loaded
During startup - Warning message:
package 'datasets' in options("defaultPackages") was not found

BUT:
Today I tried again with newest version r56478 and surprisingly, all 
that problems disappeared. Even no more warnings at build time.

For sureness I checked again older versions (between 15.07. and 21.07) 
but the build warnings there exist, with version r56478 they went away.

Rainer Hurling


P.S.: Thanks for the hint with the problems of the tarball. I had not 
been aware of it. In this case here it (mostly) also works with the 
tarball version.



>> On Fri, 22 Jul 2011, Rainer Hurling wrote:
>>
>>> After building and installing R-devel from 21/07/2011 on FreeBSD
>>> 9.0-CURRENT (amd64), starting R gives the following warning:
>>>
>>> [..snip..]
>>> Beim Start - Warnmeldung:
>>> package 'datasets' in options("defaultPackages") was not found
>>>
>>> This happens with all versions after 14/07/2011. I think it is
>>> related to
>>>
>>> NEWS Sat, 16 Jul 2011
>>> CHANGES IN R-devel PACKAGE INSTALLATION
>>> Packages without explicit ?NAMESPACE? files will have a default one
>>> created at build or INSTALL time, so all packages will have
>>> namespaces. A consequence of this is that ?.First.lib? functions need
>>> to be renamed, usually as ?.onLoad? but sometimes as ?.onAttach?.
>>>
>>>
>>> Package 'datasets' seems to be installed correctly under
>>> /usr/local/lib/R/library/:
>>>
>>> ls -l datasets/
>>> total 18
>>> -rw-r--r-- 1 root wheel - 289 22 Jul 08:12:04 2011 DESCRIPTION
>>> -rw-r--r-- 1 root wheel - 5547 22 Jul 08:12:04 2011 INDEX
>>> drwxr-xr-x 2 root wheel - 512 22 Jul 08:12:04 2011 Meta
>>> drwxr-xr-x 2 root wheel - 512 22 Jul 08:12:04 2011 R
>>> drwxr-xr-x 2 root wheel - 512 22 Jul 08:12:04 2011 data
>>> drwxr-xr-x 2 root wheel - 512 22 Jul 08:12:04 2011 help
>>> drwxr-xr-x 2 root wheel - 512 22 Jul 08:12:04 2011 html
>>>
>>>
>>> Calling options("defaultPackages") shows all relevant packages:
>>>
>>> $defaultPackages
>>> [1] "datasets" "utils" "grDevices" "graphics" "stats" "methods"
>>>
>>>
>>> But trying to load library 'datasets' fails with the following message:
>>>
>>> library(datasets)
>>> Fehler in library(datasets) :
>>> package 'datasets' does not have a NAMESPACE and should be re-installed
>>>
>>>
>>> Googling around for some days now it looks like this does not happen
>>> to anyone else (at least there are no infos). Does anybody knows what
>>> is going on here?
>>>
>>> Any help would be appreciated,
>>> Rainer Hurling
>>>
>>>
>>>
>>> sessionInfo()
>>> R Under development (unstable) (2011-07-21 r56467)
>>> Platform: amd64-portbld-freebsd9.0 (64-bit)
>>>
>>> locale:
>>> [1]
>>> de_DE.ISO8859-15/de_DE.ISO8859-15/de_DE.ISO8859-15/C/de_DE.ISO8859-15/de_DE.ISO8859-15
>>>
>>>
>>> attached base packages:
>>> [1] stats graphics grDevices utils methods base


From berryboessenkool at hotmail.com  Fri Jul 22 15:47:29 2011
From: berryboessenkool at hotmail.com (Berry Boessenkool)
Date: Fri, 22 Jul 2011 15:47:29 +0200
Subject: [Rd] default par
Message-ID: <BAY149-W36D4791B652A1054D0570DD54E0@phx.gbl>



Hello dear R-developers,

two questions on an otherwise magnificent program:

1)
Is there a way to set defaults for par differently than R offers normally?
I for example would like to have las default to 1. (or in the same style, sometimes type in plot() could be "l" per default).

Tthe following post desribes pretty much exactly that:
https://stat.ethz.ch/pipermail/r-help/2007-March/126646.html
It was written four years ago, but it seems like there has been no real elegant solution.
Did I just miss something there? If so, could someone give me an update?
If not, is there a chance that such a feature? would be added to future R-versions?
I could live with the idea to assign the par$element default in Rprofile.site.

2)
Would it appear sensible to have R give a warning, when points() is used, and some/all values are out of plotting range in the active device?
It has happened some times that I needed quite a bit of time to figure out why nothing was plotted.
Such a warning (or maybe even a beep?) would give users the clue to look at the values right away...
(What I mean is this: ?? plot(1:10)? ; points(11,3)??? just in case it's unclear)


Thanks ahead for pondering, and again: R ist the most beautiful thing I discovered in the last three years.
Keep up the good work!

Berry

-------------------------------------
Berry Boessenkool
University of Potsdam, Germany
-------------------------------------
 		 	   		  

From mauricio.zambrano at jrc.ec.europa.eu  Fri Jul 22 09:59:39 2011
From: mauricio.zambrano at jrc.ec.europa.eu (Mauricio Zambrano-Bigiarini)
Date: Fri, 22 Jul 2011 09:59:39 +0200
Subject: [Rd] Start-up messages when Importing from a package
Message-ID: <4E292DEB.5070403@jrc.ec.europa.eu>

Dear List,

I'm building a package  that uses 2 functions of the 'sp' package, and I 
declared them in my NAMESPACE file as follow:


importFrom("sp", proj4string, coordinates)


The package is built without any problem, but when I load the package, I 
get the following message:

"
Loading required package: sp

         Note: polygon geometry computations in maptools
         depend on the package gpclib, which has a
         restricted licence. It is disabled by default;
         to enable gpclib, type gpclibPermit()

Checking rgeos availability as gpclib substitute:
TRUE
"


However, if I only load the 'sp' package (in a different R session), I 
don't get any warning message.


Is there any way I can prevent the warning message appears when I load 
my package ?


Thanks in advance for any help,


Mauricio Zambrano-Bigiairni
-- 
=======================================================
FLOODS Action
Land Management and Natural Hazards Unit
Institute for Environment and Sustainability (IES)
European Commission, Joint Research Centre (JRC)
webinfo    : http://floods.jrc.ec.europa.eu/
=======================================================
DISCLAIMER:\ "The views expressed are purely those of th...{{dropped:11}}


From alireza.s.mahani at gmail.com  Sat Jul 23 14:57:41 2011
From: alireza.s.mahani at gmail.com (Alireza Mahani)
Date: Sat, 23 Jul 2011 05:57:41 -0700 (PDT)
Subject: [Rd] Best practices for writing R functions
In-Reply-To: <CABdHhvHvB4vV5BymZfSguxCUH745nd6mrAk8EDw8Kk3RPN-C2Q@mail.gmail.com>
References: <1311341191873-3686674.post@n4.nabble.com>
	<4E2993CE.4020903@prodsyse.com>
	<CADwqtCONeEqXTTg0OyDj=OJWeQxNKBm79M=iBg1jdy6JtO_w0Q@mail.gmail.com>
	<6441154A9FF1CD4386AF4ABF141A056D265A9E9B@WMEXOSCD2-N1.bocad.bank-banque-canada.ca>
	<CABdHhvHvB4vV5BymZfSguxCUH745nd6mrAk8EDw8Kk3RPN-C2Q@mail.gmail.com>
Message-ID: <1311425861566-3688850.post@n4.nabble.com>

The fact that R doesn't automatically copy the function argument is very
useful when you mainly want to pass arguments to another function. Thanks to
all of you for mentioning this!

Another trick to reduce verbosity of code (and focus on algorithm logic
rather than boilerplate code) is to maintain a global copy of variables (in
the global environment) which makes them visible to all functions (where
appropriate, of course). Once the development and testing is finished, one
can tidy things up and modify the function prototypes, add lines for
unpacking lists inside functions, etc.

--
View this message in context: http://r.789695.n4.nabble.com/Best-practices-for-writing-R-libraries-tp3686674p3688850.html
Sent from the R devel mailing list archive at Nabble.com.


From ripley at stats.ox.ac.uk  Sat Jul 23 17:03:38 2011
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 23 Jul 2011 16:03:38 +0100 (BST)
Subject: [Rd] Start-up messages when Importing from a package
In-Reply-To: <4E292DEB.5070403@jrc.ec.europa.eu>
References: <4E292DEB.5070403@jrc.ec.europa.eu>
Message-ID: <alpine.LFD.2.02.1107231602100.31769@gannet.stats.ox.ac.uk>

The message is from maptools (as it says), not sp.

No, there is nothing you can do when dependent packages put out 
messages.  You could ask the maintainer not to put the message out 
when rgeos is available ....

On Fri, 22 Jul 2011, Mauricio Zambrano-Bigiarini wrote:

> Dear List,
>
> I'm building a package  that uses 2 functions of the 'sp' package, and I 
> declared them in my NAMESPACE file as follow:
>
>
> importFrom("sp", proj4string, coordinates)
>
>
> The package is built without any problem, but when I load the package, I get 
> the following message:
>
> "
> Loading required package: sp
>
>        Note: polygon geometry computations in maptools
>        depend on the package gpclib, which has a
>        restricted licence. It is disabled by default;
>        to enable gpclib, type gpclibPermit()
>
> Checking rgeos availability as gpclib substitute:
> TRUE
> "
>
>
> However, if I only load the 'sp' package (in a different R session), I don't 
> get any warning message.
>
>
> Is there any way I can prevent the warning message appears when I load my 
> package ?
>
>
> Thanks in advance for any help,
>
>
> Mauricio Zambrano-Bigiairni
> -- 
> =======================================================
> FLOODS Action
> Land Management and Natural Hazards Unit
> Institute for Environment and Sustainability (IES)
> European Commission, Joint Research Centre (JRC)
> webinfo    : http://floods.jrc.ec.europa.eu/
> =======================================================
> DISCLAIMER:\ "The views expressed are purely those of th...{{dropped:11}}
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From jeroen.ooms at stat.ucla.edu  Sat Jul 23 22:28:40 2011
From: jeroen.ooms at stat.ucla.edu (Jeroen Ooms)
Date: Sat, 23 Jul 2011 22:28:40 +0200
Subject: [Rd] RApache error on RHEL/CentOS
Message-ID: <CABFfbXunL6VZeNpwaNTuRQLb1c4n2mvvKnX3U7V89bqb1a_Zcw@mail.gmail.com>

I am running RApache (www.rapache.net) on CentOS 5.6 using the R
binary from EPEL. After upgrading to R from 2.12 to 2.13 I am getting
the following error when starting Apache:

Cannot load /usr/lib/httpd/modules/mod_R.so into server: libgomp.so.1:
shared object cannot be dlopen()ed

I tried rebuilding RApache, but the error remains. Installed are:

R-devel.i386 2.13.0
libgomp.i386 4.1.1-52.el5.2
httpd (apache)?2.2.3
rapache 1.1.4

Has anything changed in R 2.13 that might cause this issue?


From simon.urbanek at r-project.org  Sat Jul 23 23:30:03 2011
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Sat, 23 Jul 2011 17:30:03 -0400
Subject: [Rd] RApache error on RHEL/CentOS
In-Reply-To: <CABFfbXunL6VZeNpwaNTuRQLb1c4n2mvvKnX3U7V89bqb1a_Zcw@mail.gmail.com>
References: <CABFfbXunL6VZeNpwaNTuRQLb1c4n2mvvKnX3U7V89bqb1a_Zcw@mail.gmail.com>
Message-ID: <B77118AA-BAC4-4308-A528-80020548447F@r-project.org>


On Jul 23, 2011, at 4:28 PM, Jeroen Ooms wrote:

> I am running RApache (www.rapache.net) on CentOS 5.6 using the R
> binary from EPEL. After upgrading to R from 2.12 to 2.13 I am getting
> the following error when starting Apache:
> 
> Cannot load /usr/lib/httpd/modules/mod_R.so into server: libgomp.so.1:
> shared object cannot be dlopen()ed
> 
> I tried rebuilding RApache, but the error remains. Installed are:
> 
> R-devel.i386 2.13.0
> libgomp.i386 4.1.1-52.el5.2
> httpd (apache) 2.2.3
> rapache 1.1.4
> 
> Has anything changed in R 2.13 that might cause this issue?
> 

Yes, R 2.13.0 uses OpenMP. You can compile R without gomp using  --disable-openmp

Cheers,
Simon


From radler12 at op.pl  Sun Jul 24 13:17:11 2011
From: radler12 at op.pl (Krystian Radlak)
Date: Sun, 24 Jul 2011 04:17:11 -0700 (PDT)
Subject: [Rd] Saving data into a text or CSV file using foreach/doSNOW
In-Reply-To: <1311238487719-3683155.post@n4.nabble.com>
References: <1311238487719-3683155.post@n4.nabble.com>
Message-ID: <1311506231845-3690132.post@n4.nabble.com>

I solved this problem using Sys.getpid() function.

zz <- file(paste(cmd_args[7],Sys.getpid(),".txt",sep=""), "at")
write(rr,file=zz)
close(zz) 

--
View this message in context: http://r.789695.n4.nabble.com/Saving-data-into-a-text-or-CSV-file-using-foreach-doSNOW-tp3683155p3690132.html
Sent from the R devel mailing list archive at Nabble.com.


From jmc at r-project.org  Sun Jul 24 23:58:23 2011
From: jmc at r-project.org (John Chambers)
Date: Sun, 24 Jul 2011 14:58:23 -0700
Subject: [Rd] Same class name, different package
In-Reply-To: <4E286203.8070104@r-project.org>
References: <4E286203.8070104@r-project.org>
Message-ID: <4E2C957F.7070803@r-project.org>

A point that has come up a couple of times with the new test is that two 
classes from two packages may be "the same class".  Should that turn on 
duplicate classes?

One situation where the answer seems to be No is when the two classes 
are identical declarations of S3 classes, via setOldClass().

A recent update (rev. 56492) tries to check for equivalent classes, with 
some special leeway for that case, and does not turn on the  duplicate 
class flag.  It's not clear what is really needed or wanted in all 
circumstances, so further experience will be helpful.

If duplicate classes do exist, a utility findDuplicateClasses(details = 
FALSE) will give the names of the duplicated classes.  It's not yet 
exported so you need to call methods:::findDuplicateClasses()

John


On 7/21/11 10:29 AM, John Chambers wrote:
> In principle, two separately developed packages could use the same
> class name, and a user could then attach both and attempt to use
> methods for both classes.
>
> That has never worked, but some changes have been added to r-devel to
> handle this case.  The changes involve extending the "signature" class
> to include package information.  For compatibility, packages will need
> to be re-installed from a version of R labelled 56466 or later,
> although an attempt is made to fill in missing information.
>
> John
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From maechler at stat.math.ethz.ch  Mon Jul 25 09:07:30 2011
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 25 Jul 2011 09:07:30 +0200
Subject: [Rd] Same class name, different package
In-Reply-To: <4E2C957F.7070803@r-project.org>
References: <4E286203.8070104@r-project.org> <4E2C957F.7070803@r-project.org>
Message-ID: <20013.5682.23561.615532@stat.math.ethz.ch>

>>>>> John Chambers <jmc at r-project.org>
>>>>>     on Sun, 24 Jul 2011 14:58:23 -0700 writes:

    > A point that has come up a couple of times with the new
    > test is that two classes from two packages may be "the
    > same class".  Should that turn on duplicate classes?

    > One situation where the answer seems to be No is when the
    > two classes are identical declarations of S3 classes, via
    > setOldClass().

    > A recent update (rev. 56492) tries to check for equivalent
    > classes, with some special leeway for that case, and does
    > not turn on the duplicate class flag.  It's not clear what
    > is really needed or wanted in all circumstances, so
    > further experience will be helpful.

    > If duplicate classes do exist, a utility
    > findDuplicateClasses(details = FALSE) will give the names
    > of the duplicated classes.  It's not yet exported so you
    > need to call methods:::findDuplicateClasses()

    > John

I haven't yet looked into the many situations that are "out
there" for CRAN and Bioconductor packages and am just speaking
from my own S4-using perspective:

I think 

  ImportClassesFrom(...)

should be much more widely used, in order to diminish such class
"conflicts".  
Wherever the new code produces warnings (does it?) about
duplicate class names, it would be good to "advertize" the 
ImportClassesFrom()   clause for those cases where the two
class definitions look to be identical.

Martin


    > On 7/21/11 10:29 AM, John Chambers wrote:
    >> In principle, two separately developed packages could use
    >> the same class name, and a user could then attach both
    >> and attempt to use methods for both classes.
    >> 
    >> That has never worked, but some changes have been added
    >> to r-devel to handle this case.  The changes involve
    >> extending the "signature" class to include package
    >> information.  For compatibility, packages will need to be
    >> re-installed from a version of R labelled 56466 or later,
    >> although an attempt is made to fill in missing
    >> information.
    >> 
    >> John


From mauricio.zambrano at jrc.ec.europa.eu  Mon Jul 25 10:49:29 2011
From: mauricio.zambrano at jrc.ec.europa.eu (Mauricio Zambrano-Bigiarini)
Date: Mon, 25 Jul 2011 10:49:29 +0200
Subject: [Rd] Start-up messages when Importing from a package
In-Reply-To: <alpine.LFD.2.02.1107231602100.31769@gannet.stats.ox.ac.uk>
References: <4E292DEB.5070403@jrc.ec.europa.eu>
	<alpine.LFD.2.02.1107231602100.31769@gannet.stats.ox.ac.uk>
Message-ID: <4E2D2E19.3080908@jrc.ec.europa.eu>

Prof Brian Ripley wrote:
> The message is from maptools (as it says), not sp.

Thank you very much prof. Ripley for pointing out my error.

> 
> No, there is nothing you can do when dependent packages put out 
> messages.  You could ask the maintainer not to put the message out when 
> rgeos is available ....
> 

I'll try.

All the best,

Mauricio


-- 
=======================================================
FLOODS Action
Land Management and Natural Hazards Unit
Institute for Environment and Sustainability (IES)
European Commission, Joint Research Centre (JRC)
webinfo    : http://floods.jrc.ec.europa.eu/
=======================================================
DISCLAIMER:
"The views expressed are purely those of the writer
and may not in any circumstances be regarded as stating
an official position of the European Commission."
=======================================================
Linux user #454569 -- Ubuntu user #17469
=======================================================
"There is only one pretty child in the world,
and every mother has it."
(Chinese Proverb)

> On Fri, 22 Jul 2011, Mauricio Zambrano-Bigiarini wrote:
> 
>> Dear List,
>>
>> I'm building a package  that uses 2 functions of the 'sp' package, and 
>> I declared them in my NAMESPACE file as follow:
>>
>>
>> importFrom("sp", proj4string, coordinates)
>>
>>
>> The package is built without any problem, but when I load the package, 
>> I get the following message:
>>
>> "
>> Loading required package: sp
>>
>>        Note: polygon geometry computations in maptools
>>        depend on the package gpclib, which has a
>>        restricted licence. It is disabled by default;
>>        to enable gpclib, type gpclibPermit()
>>
>> Checking rgeos availability as gpclib substitute:
>> TRUE
>> "
>>
>>
>> However, if I only load the 'sp' package (in a different R session), I 
>> don't get any warning message.
>>
>>
>> Is there any way I can prevent the warning message appears when I load 
>> my package ?
>>
>>
>> Thanks in advance for any help,
>>
>>
>> Mauricio Zambrano-Bigiairni
>> -- 
>> =======================================================
>> FLOODS Action
>> Land Management and Natural Hazards Unit
>> Institute for Environment and Sustainability (IES)
>> European Commission, Joint Research Centre (JRC)
>> webinfo    : http://floods.jrc.ec.europa.eu/
>> =======================================================
>> DISCLAIMER:\ "The views expressed are purely those of th...{{dropped:11}}
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>


From bbolker at gmail.com  Mon Jul 25 17:45:07 2011
From: bbolker at gmail.com (Ben Bolker)
Date: Mon, 25 Jul 2011 11:45:07 -0400
Subject: [Rd] plot.function documentation/export?
Message-ID: <4E2D8F83.5010707@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1


  I recently suggested to someone (
http://stackoverflow.com/questions/6789055/r-inconsistency-why-add-t-sometimes-works-and-sometimes-not-in-the-plot-functi/6789098#6789098
) that the should use methods("plot") or methods(class="function") to
locate the documentation on the plot method for objects of class
"function", but they pointed out that these don't actually work.

  I can't figure out why not: src/library/graphics/man/curve.Rd contains
the line

\method{plot}{function}(x, y = 0, to = 1, from = y, xlim = NULL, ylab =
NULL, \dots)

and src/library/graphics/DESCRIPTION contains

S3method(plot, "function")


 [presumably the extra quotes are in there because function is a
reserved word?]

 I'm not sure where else the information should be.  Searching around in
the code tree for information on tail.function (which is listed in the
methods:

> methods(class="function")
[1] as.list.function head.function*   print.function   tail.function*

I find the same S3method syntax, so I guess the quotation marks aren't
the problem ...

  Any ideas?


    Ben Bolker



> sessionInfo()
R version 2.13.1 (2011-07-08)
Platform: i486-pc-linux-gnu (32-bit)

-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.10 (GNU/Linux)
Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org/

iEYEARECAAYFAk4tj4MACgkQc5UpGjwzenOKhgCeONfuyOCw+9Fe+REaMPWjUbF8
6ikAniQrf5J9bn9f3Kga1xUr5SVcLtot
=Ju9G
-----END PGP SIGNATURE-----


From radford at cs.toronto.edu  Mon Jul 25 17:53:23 2011
From: radford at cs.toronto.edu (Radford Neal)
Date: Mon, 25 Jul 2011 11:53:23 -0400
Subject: [Rd] Best practices for writing R functions (really copying)
Message-ID: <20110725155323.GA15964@cs.toronto.edu>

Gabriel Becker writes:

  AFAIK R does not automatically copy function arguments. R actually tries
  very hard to avoid copying while maintaining "pass by value" functionality.

  ... R only copies data when you modify an object, not
  when you simply pass it to a function.

This is a bit misleading.  R tries to avoid copying by maintaining a
count of how many references there are to an object, so that x[i] <- 9
can be done without a copy if x is the only reference to the vector.
However, it never decrements such counts.  As a result, simply passing
x to a function that accesses but does not change it will result in x
being copied if x[i] is changed after that function returns.  An
exception is that this usually isn't the case if x is passed to a
primitive function.  But note that not all standard functions are 
technically "primitive".

The end result is that it's rather difficult to tell when copying will
be done.  Try the following test, for example:

  cat("a: "); print(system.time( { A <- matrix(c(1.0,1.1),50000,1000); 0 } ))
  cat("b: "); print(system.time( { A[1,1]<-7; 0 } ))
  cat("c: "); print(system.time( { B <- sqrt(A); 0 } ))
  cat("d: "); print(system.time( { A[1,1]<-7; 0 } ))
  cat("e: "); print(system.time( { B <- t(A); 0 } ))
  cat("f: "); print(system.time( { A[1,1]<-7; 0 } ))
  cat("g: "); print(system.time( { A[1,1]<-7; 0 } ))

You'll find that the time printed after b:, d:, and g: is near zero,
but that there is non-negligible time for f:.  This is because sqrt
is primitive but t is not, so the modification to A after the call
t(A) requires that a copy be made.

   Radford Neal


From matt at biostatmatt.com  Mon Jul 25 18:44:56 2011
From: matt at biostatmatt.com (Matt Shotwell)
Date: Mon, 25 Jul 2011 11:44:56 -0500
Subject: [Rd] Best practices for writing R functions (really copying)
In-Reply-To: <20110725155323.GA15964@cs.toronto.edu>
References: <20110725155323.GA15964@cs.toronto.edu>
Message-ID: <1311612296.13412.19.camel@pal>

Also consider subsetting:

cat("a: "); print(system.time( { A <- matrix(c(1.0,1.1),50000,1000); 0 } ))
cat("h: "); print(system.time( { sum(A[1:50000,1:1000]) } ))
cat("i: "); print(system.time( { sum(A[]) } ))
cat("j: "); print(system.time( { sum(A) } ))

In contrast with Python's NumPy array, the R array type has no concept
of 'viewing' the array contents in different ways. Instead, the contents
are copied or adjusted. Subsetting and matrix transposition are examples
of transformations that might be considered alternate 'views' of an
array. This is especially painful in the example above, because
A[1:5000,1:1000], A[], and A evaluate to identical() arrays. In case h:
the array is copied element-wise. In i: A is duplicate()d. In case j: A
is not copied.

Matt

On Mon, 2011-07-25 at 11:53 -0400, Radford Neal wrote:
> Gabriel Becker writes:
> 
>   AFAIK R does not automatically copy function arguments. R actually tries
>   very hard to avoid copying while maintaining "pass by value" functionality.
> 
>   ... R only copies data when you modify an object, not
>   when you simply pass it to a function.
> 
> This is a bit misleading.  R tries to avoid copying by maintaining a
> count of how many references there are to an object, so that x[i] <- 9
> can be done without a copy if x is the only reference to the vector.
> However, it never decrements such counts.  As a result, simply passing
> x to a function that accesses but does not change it will result in x
> being copied if x[i] is changed after that function returns.  An
> exception is that this usually isn't the case if x is passed to a
> primitive function.  But note that not all standard functions are 
> technically "primitive".
> 
> The end result is that it's rather difficult to tell when copying will
> be done.  Try the following test, for example:
> 
>   cat("a: "); print(system.time( { A <- matrix(c(1.0,1.1),50000,1000); 0 } ))
>   cat("b: "); print(system.time( { A[1,1]<-7; 0 } ))
>   cat("c: "); print(system.time( { B <- sqrt(A); 0 } ))
>   cat("d: "); print(system.time( { A[1,1]<-7; 0 } ))
>   cat("e: "); print(system.time( { B <- t(A); 0 } ))
>   cat("f: "); print(system.time( { A[1,1]<-7; 0 } ))
>   cat("g: "); print(system.time( { A[1,1]<-7; 0 } ))
> 
> You'll find that the time printed after b:, d:, and g: is near zero,
> but that there is non-negligible time for f:.  This is because sqrt
> is primitive but t is not, so the modification to A after the call
> t(A) requires that a copy be made.
> 
>    Radford Neal
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From pauljohn32 at gmail.com  Mon Jul 25 18:49:55 2011
From: pauljohn32 at gmail.com (Paul Johnson)
Date: Mon, 25 Jul 2011 11:49:55 -0500
Subject: [Rd] Randomness not due to seed
In-Reply-To: <1311081181602-3678082.post@n4.nabble.com>
References: <1311081181602-3678082.post@n4.nabble.com>
Message-ID: <CAErODj-YcvXq=BiDH_y84wSL=pUDH2KBqZuGiuryx6x3HwW4nw@mail.gmail.com>

On Tue, Jul 19, 2011 at 8:13 AM, jeroen00ms <jeroen.ooms at stat.ucla.edu> wrote:
> I am working on a reproducible computing platform for which I would like to
> be able to _exactly_ reproduce an R object. However, I am experiencing
> unexpected randomness in some calculations. I have a hard time finding out
> exactly how it occurs. The code below illustrates the issue.
>
> mylm1 <- lm(dist~speed, data=cars);
> mylm2 <- lm(dist~speed, data=cars);
> identical(mylm1, mylm2); #TRUE
>
> makelm <- function(){
> ? ? ? ?return(lm(dist~speed, data=cars));
> }
>
> mylm1 <- makelm();
> mylm2 <- makelm();
> identical(mylm1, mylm2); #FALSE
>
> When inspecting both objects there seem to be some rounding differences.
> Setting a seed does not make a difference. Is there any way I can remove
> this randomness and exactly reproduce the object every time?
>

William Dunlap was correct.  Observe in the sequence of comparisons
below, the difference in the "terms" object is causing the identical
to fail: Everything else associated with this model--the coefficients,
the r-square, cov matrix, etc, exactly match.


> mylm1 <- lm(dist~speed, data=cars);
> mylm2 <- lm(dist~speed, data=cars);
> identical(mylm1, mylm2); #TRUE
[1] TRUE
> makelm <- function(){
+        return(lm(dist~speed, data=cars));
+ }
> mylm1 <- makelm();
> mylm2 <- makelm();
> identical(mylm1, mylm2); #FALSE
[1] FALSE
> identical(coef(mylm1), coef(mylm2))
[1] TRUE
> identical(summary(mylm1), summary(mylm2))
[1] FALSE
> identical(coef(summary(mylm1)), coef(summary(mylm2)))
[1] TRUE
> all.equal(mylm1, mylm2)
[1] TRUE
> identical(summary(mylm1)$r.squared, summary(mylm2)$r.squared)
[1] TRUE
> identical(summary(mylm1)$adj.r.squared, summary(mylm2)$adj.r.squared)
[1] TRUE
> identical(summary(mylm1)$sigma, summary(mylm2)$sigma)
[1] TRUE
> identical(summary(mylm1)$fstatistic, summary(mylm2)$fstatistic)
[1] TRUE
> identical(summary(mylm1)$residuals, summary(mylm2)$residuals)
[1] TRUE
> identical(summary(mylm1)$cov.unscaled, summary(mylm2)$cov.unscaled)
[1] TRUE
> identical(summary(mylm1)$call, summary(mylm2)$call)
[1] TRUE
> identical(summary(mylm1)$terms, summary(mylm2)$terms)
[1] FALSE

> summary(mylm2)$terms
dist ~ speed
attr(,"variables")
list(dist, speed)
attr(,"factors")
      speed
dist      0
speed     1
attr(,"term.labels")
[1] "speed"
attr(,"order")
[1] 1
attr(,"intercept")
[1] 1
attr(,"response")
[1] 1
attr(,".Environment")
<environment: 0x1b76ae0>
attr(,"predvars")
list(dist, speed)
attr(,"dataClasses")
     dist     speed
"numeric" "numeric"
>
> summary(mylm1)$terms
dist ~ speed
attr(,"variables")
list(dist, speed)
attr(,"factors")
      speed
dist      0
speed     1
attr(,"term.labels")
[1] "speed"
attr(,"order")
[1] 1
attr(,"intercept")
[1] 1
attr(,"response")
[1] 1
attr(,".Environment")
<environment: 0x1cf06b8>
attr(,"predvars")
list(dist, speed)
attr(,"dataClasses")
     dist     speed
"numeric" "numeric"




-- 
Paul E. Johnson
Professor, Political Science
1541 Lilac Lane, Room 504
University of Kansas


From ligges at statistik.tu-dortmund.de  Mon Jul 25 18:55:11 2011
From: ligges at statistik.tu-dortmund.de (Uwe Ligges)
Date: Mon, 25 Jul 2011 18:55:11 +0200
Subject: [Rd] plot.function documentation/export?
In-Reply-To: <4E2D8F83.5010707@gmail.com>
References: <4E2D8F83.5010707@gmail.com>
Message-ID: <4E2D9FEF.3060805@statistik.tu-dortmund.de>



On 25.07.2011 17:45, Ben Bolker wrote:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
>
>    I recently suggested to someone (
> http://stackoverflow.com/questions/6789055/r-inconsistency-why-add-t-sometimes-works-and-sometimes-not-in-the-plot-functi/6789098#6789098
> ) that the should use methods("plot") or methods(class="function") to
> locate the documentation on the plot method for objects of class
> "function", but they pointed out that these don't actually work.
>
>    I can't figure out why not: src/library/graphics/man/curve.Rd contains
> the line
>
> \method{plot}{function}(x, y = 0, to = 1, from = y, xlim = NULL, ylab =
> NULL, \dots)
>
> and src/library/graphics/DESCRIPTION contains


you mean the following line is in NAMESPACE rather than DESCRIPTION.

> S3method(plot, "function")
>
>
>   [presumably the extra quotes are in there because function is a
> reserved word?]
>
>   I'm not sure where else the information should be.  Searching around in
> the code tree for information on tail.function (which is listed in the
> methods:
>
>> methods(class="function")
> [1] as.list.function head.function*   print.function   tail.function*
>
> I find the same S3method syntax, so I guess the quotation marks aren't
> the problem ...

?tail.function

tells us this one is from package "utils" and you can search for this 
function in the sources of the utils package

Or you could ask for

 > getAnywhere("tail.function")

and R tells you

A single object matching ?tail.function? was found
It was found in the following places
   registered S3 method for tail from namespace utils
   namespace:utils
[.....]

Best wishes,
Uwe





>
>    Any ideas?
>
>
>      Ben Bolker
>
>
>
>> sessionInfo()
> R version 2.13.1 (2011-07-08)
> Platform: i486-pc-linux-gnu (32-bit)
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v1.4.10 (GNU/Linux)
> Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org/
>
> iEYEARECAAYFAk4tj4MACgkQc5UpGjwzenOKhgCeONfuyOCw+9Fe+REaMPWjUbF8
> 6ikAniQrf5J9bn9f3Kga1xUr5SVcLtot
> =Ju9G
> -----END PGP SIGNATURE-----
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From bbolker at gmail.com  Mon Jul 25 19:01:59 2011
From: bbolker at gmail.com (Ben Bolker)
Date: Mon, 25 Jul 2011 13:01:59 -0400
Subject: [Rd] plot.function documentation/export?
In-Reply-To: <4E2D9FEF.3060805@statistik.tu-dortmund.de>
References: <4E2D8F83.5010707@gmail.com>
	<4E2D9FEF.3060805@statistik.tu-dortmund.de>
Message-ID: <4E2DA187.3010204@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

On 07/25/2011 12:55 PM, Uwe Ligges wrote:
> 
> 
> On 25.07.2011 17:45, Ben Bolker wrote:
> 
>    I recently suggested to someone (
> http://stackoverflow.com/questions/6789055/r-inconsistency-why-add-t-sometimes-works-and-sometimes-not-in-the-plot-functi/6789098#6789098
> 
> ) that the should use methods("plot") or methods(class="function") to
> locate the documentation on the plot method for objects of class
> "function", but they pointed out that these don't actually work.
> 
>    I can't figure out why not: src/library/graphics/man/curve.Rd contains
> the line
> 
> \method{plot}{function}(x, y = 0, to = 1, from = y, xlim = NULL, ylab =
> NULL, \dots)
> 
> and src/library/graphics/DESCRIPTION contains
> 
> 
>> you mean the following line is in NAMESPACE rather than DESCRIPTION.
> 
> S3method(plot, "function")

  Yes, sorry.

> 
> 
>   [presumably the extra quotes are in there because function is a
> reserved word?]
> 
>   I'm not sure where else the information should be.  Searching around in
> the code tree for information on tail.function (which is listed in the
> methods:
> 
>>>> methods(class="function")
> [1] as.list.function head.function*   print.function   tail.function*
> 
> I find the same S3method syntax, so I guess the quotation marks aren't
> the problem ...
> 
>> ?tail.function
> 
>> tells us this one is from package "utils" and you can search for this
>> function in the sources of the utils package
> 
>> Or you could ask for
> 
> getAnywhere("tail.function")
> 
>> and R tells you
> 
>> A single object matching tail.function was found
>> It was found in the following places
>>   registered S3 method for tail from namespace utils
>>   namespace:utils
>> [.....]
> 
>> Best wishes,
>> Uwe
> 
> 
> 

  Sorry, I didn't frame my question very clearly.  I can find
"tail.function" just fine, or I could if I wanted to.   What I don't
know is why methods("plot") and methods(class="function") don't list
"plot.function" even though its documentation and setup seem to be
similar to "tail.function", which *does* show up in
methods(class="function") ...

  cheers
    Ben Bolker


=========

  No plot.function listing in either of these ...

> library("graphics")
> methods("plot")
 [1] plot.acf*           plot.data.frame*    plot.decomposed.ts*
 [4] plot.default        plot.dendrogram*    plot.density
 [7] plot.ecdf           plot.factor*        plot.formula*
[10] plot.hclust*        plot.histogram*     plot.HoltWinters*
[13] plot.isoreg*        plot.lm             plot.medpolish*
[16] plot.mlm            plot.ppr*           plot.prcomp*
[19] plot.princomp*      plot.profile.nls*   plot.spec
[22] plot.spec.coherency plot.spec.phase     plot.stepfun
[25] plot.stl*           plot.table*         plot.ts
[28] plot.tskernel*      plot.TukeyHSD

   Non-visible functions are asterisked
> methods(class="function")
[1] as.list.function head.function*   print.function   tail.function*

   Non-visible functions are asterisked



-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.10 (GNU/Linux)
Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org/

iEYEARECAAYFAk4toYcACgkQc5UpGjwzenMyFACggRdP+48u++szSbV82S4HhTxj
MJcAnAsZ0iOXAsXtSeB8PZ4JmlgUgb9t
=2lyp
-----END PGP SIGNATURE-----


From pauljohn32 at gmail.com  Mon Jul 25 19:47:15 2011
From: pauljohn32 at gmail.com (Paul Johnson)
Date: Mon, 25 Jul 2011 12:47:15 -0500
Subject: [Rd] CRAN mirror size mushrooming; consider archiving some?
Message-ID: <CAErODj95THdYT0phbsF91hBeMoZr2ntM=pu_snKnzL6EUwkXZA@mail.gmail.com>

Hi, everybody

I'm setting up a new CRAN mirror and filled up the disk space the
server allotted me.  I asked for more, then filled that up.  Now the
system administrators want me to buy an $800 fiber channel card and a
storage device.  I'm going to do that, but it does make want to
suggest to you that this is a problem.

CRAN now is about 68GB, and about 3/4 of that is in the bin folder,
where one finds copies of compiled packages for macosx and windows.
If the administrators of CRAN would move the packages for R before,
say, 2.12, to long term storage, then mirror management would be a bit
more, well, manageable.

Moving the R for windows packages for, say, R 2.0 through 2.10 would
save some space, and possibly establish a useful precedent for the
long term.

Here's the bin/windows folder. Note it is expanding exponentially (or nearly so)

$ du --max-depth=1 | sort
1012644 ./2.6
103504  ./1.7
122200  ./1.8
1239876 ./2.7
1487024 ./2.8
15220   ./ATLAS
167668  ./1.9
17921604        .
1866196 ./2.9
204392  ./2.0
2207708 ./2.10
2340120 ./2.13
2356272 ./2.12
2403176 ./2.11
298620  ./2.1
364292  ./2.2
438044  ./2.3
595920  ./2.4
698064  ./2.5

-- 

-- 
Paul E. Johnson
Professor, Political Science
1541 Lilac Lane, Room 504
University of Kansas


From hb at biostat.ucsf.edu  Mon Jul 25 20:11:59 2011
From: hb at biostat.ucsf.edu (Henrik Bengtsson)
Date: Mon, 25 Jul 2011 11:11:59 -0700
Subject: [Rd] Best practices for writing R functions (really copying)
In-Reply-To: <20110725155323.GA15964@cs.toronto.edu>
References: <20110725155323.GA15964@cs.toronto.edu>
Message-ID: <CAFDcVCTURMVHD8BgbJ+z9Fiy4PZbyZMoUtJOyD+F-PeajvHwwQ@mail.gmail.com>

Use tracemem() instead, i.e.

> A <- matrix(c(1.0,1.1), nrow=5, ncol=10);
> tracemem(A);
[1] "<0x00000000047ab170"
> A[1,1] <- 7;
> B <- sqrt(A);
tracemem[0x00000000047ab170 -> 0x000000000552f338]:
> A[1,1] <- 7;
> B <- t(A);
> A[1,1] <- 7;
tracemem[0x00000000047ab170 -> 0x00000000057ba588]:
> A[1,1] <- 7;
> A[1,1] <- 7;

It looks like sqrt() creates the copy internally, which explains the difference.

However, it is true that even if a new copy is not needed/created
inside a function call, a function "touching" the object would trigger
downstream copies, e.g.

# Not touching the object:
> foo <- function(X) { 0 }
> B <- foo(A);
> A[1,1] <- 7;
> A[1,1] <- 7;

# Touching the object:
> bar <- function(X) { Y <- X; 0 }
> B <- bar(A);
> A[1,1] <- 7;
tracemem[0x00000000039b5538 -> 0x000000000402c448]:
> A[1,1] <- 7;

However however, try doing the same with a vector instead of matrix,
e.g. A <- 1:10, and/or assignment with A[1] <- 7 and you get a
different behavior.  The source code should explain why.  I leave it
at this.

My $.02

/Henrik

On Mon, Jul 25, 2011 at 8:53 AM, Radford Neal <radford at cs.toronto.edu> wrote:
> Gabriel Becker writes:
>
> ?AFAIK R does not automatically copy function arguments. R actually tries
> ?very hard to avoid copying while maintaining "pass by value" functionality.
>
> ?... R only copies data when you modify an object, not
> ?when you simply pass it to a function.
>
> This is a bit misleading. ?R tries to avoid copying by maintaining a
> count of how many references there are to an object, so that x[i] <- 9
> can be done without a copy if x is the only reference to the vector.
> However, it never decrements such counts. ?As a result, simply passing
> x to a function that accesses but does not change it will result in x
> being copied if x[i] is changed after that function returns. ?An
> exception is that this usually isn't the case if x is passed to a
> primitive function. ?But note that not all standard functions are
> technically "primitive".
>
> The end result is that it's rather difficult to tell when copying will
> be done. ?Try the following test, for example:
>
> ?cat("a: "); print(system.time( { A <- matrix(c(1.0,1.1),50000,1000); 0 } ))
> ?cat("b: "); print(system.time( { A[1,1]<-7; 0 } ))
> ?cat("c: "); print(system.time( { B <- sqrt(A); 0 } ))
> ?cat("d: "); print(system.time( { A[1,1]<-7; 0 } ))
> ?cat("e: "); print(system.time( { B <- t(A); 0 } ))
> ?cat("f: "); print(system.time( { A[1,1]<-7; 0 } ))
> ?cat("g: "); print(system.time( { A[1,1]<-7; 0 } ))
>
> You'll find that the time printed after b:, d:, and g: is near zero,
> but that there is non-negligible time for f:. ?This is because sqrt
> is primitive but t is not, so the modification to A after the call
> t(A) requires that a copy be made.
>
> ? Radford Neal
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From jmc at r-project.org  Tue Jul 26 02:45:14 2011
From: jmc at r-project.org (John Chambers)
Date: Mon, 25 Jul 2011 17:45:14 -0700
Subject: [Rd] Same class name, different package
In-Reply-To: <20013.5682.23561.615532@stat.math.ethz.ch>
References: <4E286203.8070104@r-project.org> <4E2C957F.7070803@r-project.org>
	<20013.5682.23561.615532@stat.math.ethz.ch>
Message-ID: <4E2E0E1A.4080107@r-project.org>

On 7/25/11 12:07 AM, Martin Maechler wrote:
>>>>>> John Chambers<jmc at r-project.org>
>>>>>>      on Sun, 24 Jul 2011 14:58:23 -0700 writes:
>
>      >  A point that has come up a couple of times with the new
>      >  test is that two classes from two packages may be "the
>      >  same class".  Should that turn on duplicate classes?
>
>      >  One situation where the answer seems to be No is when the
>      >  two classes are identical declarations of S3 classes, via
>      >  setOldClass().
>
>      >  A recent update (rev. 56492) tries to check for equivalent
>      >  classes, with some special leeway for that case, and does
>      >  not turn on the duplicate class flag.  It's not clear what
>      >  is really needed or wanted in all circumstances, so
>      >  further experience will be helpful.
>
>      >  If duplicate classes do exist, a utility
>      >  findDuplicateClasses(details = FALSE) will give the names
>      >  of the duplicated classes.  It's not yet exported so you
>      >  need to call methods:::findDuplicateClasses()
>
>      >  John
>
> I haven't yet looked into the many situations that are "out
> there" for CRAN and Bioconductor packages and am just speaking
> from my own S4-using perspective:
>
> I think
>
>    ImportClassesFrom(...)
>
> should be much more widely used, in order to diminish such class
> "conflicts".
> Wherever the new code produces warnings (does it?) about
> duplicate class names, it would be good to "advertize" the
> ImportClassesFrom()   clause for those cases where the two
> class definitions look to be identical.

No argument there.

But I think the situation is different for setOldClass() and for "real" 
S4 classes, with a warning more suitable in the second case.

With S3 classes, the scenario that will happen fairly often is:  Package 
A has an S3 class "foo"; Packages B and C both (independently) want to 
use/extend that class in S4 code.  Both will include setOldClass("foo") 
calls.

The problem here is that the two generated classes for "foo" will belong 
to packages B and C (there being no way in general to find where S3 
class "foo" is defined--indeed in a sense it's not defined at all).

Various approaches are possible, varying in ugliness.  One might be to 
associate all these converted S3 classes with a special pseudo-package. 
  Another, which I don't much like, is to ask the setOldClass() call to 
specify which package the S3 class comes from, and hope that all the 
packages in the above scenario make the same choice.

The short term approach will probably be to allow multiple identical 
setOldClass() effects without warning.  (The actual code as of today 
generates warning messages on all identical classes only if 
options(warn=1) has been set.)

John

>
> Martin
>
>
>      >  On 7/21/11 10:29 AM, John Chambers wrote:
>      >>  In principle, two separately developed packages could use
>      >>  the same class name, and a user could then attach both
>      >>  and attempt to use methods for both classes.
>      >>
>      >>  That has never worked, but some changes have been added
>      >>  to r-devel to handle this case.  The changes involve
>      >>  extending the "signature" class to include package
>      >>  information.  For compatibility, packages will need to be
>      >>  re-installed from a version of R labelled 56466 or later,
>      >>  although an attempt is made to fill in missing
>      >>  information.
>      >>
>      >>  John
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From ripley at stats.ox.ac.uk  Tue Jul 26 10:00:57 2011
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 26 Jul 2011 09:00:57 +0100
Subject: [Rd] --max-vsize
In-Reply-To: <87sjpzlb3e.fsf@cantab.net>
References: <87sjpzlb3e.fsf@cantab.net>
Message-ID: <alpine.LFD.2.02.1107260854551.17692@gannet.stats.ox.ac.uk>

Point 1 is as documented: you have exceeded the maximum integer and it 
does say that it gives NA.  So the only 'odd' is reporting that you 
did not read the documentation.

Point 2 is R not using the correct units for --max-vsize (it used the 
number of Vcells, as was once documented), and I have fixed.

But I do wonder why you are using --max-vsize: the documentation says 
it is very rarely needed, and I suspect that there are better ways to 
do this.

Also, you ignored the posting guide and did not tell us the 'at a 
minimum' information requested: what OS was this, and was it a 32- or 
64-bit R if a 64-bit OS?

I don't find reporting values of several GB as bytes very useful, but 
then mem.limits() is not useful to me either ....

On Thu, 21 Jul 2011, Christophe Rhodes wrote:

> Hi,
>
> In both R 2.13 and the SVN trunk, I observe odd behaviour with the
> --max-vsize command-line argument:
>
> 1. passing a largeish value (about 260M or greater) makes mem.limits()
>   report NA for the vsize limit; gc() continues to report a value...
>
> 2. ...but that value (and the actual limit) is wrong by a factor of 8.
>
> I attach a patch for issue 2, lightly tested.  I believe that fixing
> issue 1 involves changing the return convention of do_memlimits -- not
> returning a specialized integer vector, but a more general numeric; I
> wasn't confident to do that.
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From csr21 at cantab.net  Tue Jul 26 11:08:53 2011
From: csr21 at cantab.net (Christophe Rhodes)
Date: Tue, 26 Jul 2011 10:08:53 +0100
Subject: [Rd] --max-vsize
References: <87sjpzlb3e.fsf@cantab.net>
	<alpine.LFD.2.02.1107260854551.17692@gannet.stats.ox.ac.uk>
Message-ID: <874o29if2y.fsf@cantab.net>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> Point 1 is as documented: you have exceeded the maximum integer and it
> does say that it gives NA.  So the only 'odd' is reporting that you
> did not read the documentation.

I'm sorry; I thought that my message made it clear that I was aware that
the NA came from exceeding the maximum representable integer.  To
belatedly address the other information I failed to provide, I use R on
Linux, both 32-bit and 64-bit (with 64-bit R).

> Point 2 is R not using the correct units for --max-vsize (it used the
> number of Vcells, as was once documented), and I have fixed.

Thank you; I've read the changes and I think they meet my needs.  (I
will try to explain how/why I want to use larger-than-integer
mem.limits() below.  If there's a better or more supported way to
achieve what I want, that'd be fine too)

> But I do wonder why you are using --max-vsize: the documentation says
> it is very rarely needed, and I suspect that there are better ways to
> do this.

Here's the basic idea: I would like to be able to restrict R to a large
amount of memory (say 4GB, for the sake of argument), but in a way such
that I can increase that limit temporarily if it turns out to be
necessary for some reason.

The desire for a restriction is that I have found it fairly difficult to
predict in advance how much memory a given calculation or analysis is
going to take.  Part of that is my inexperience with R, leading to
hilarious thinkos, but I think that part of that difficulty to predict
is going to remain even as I gain experience.  I use R both on
multi-user systems and on single-user-multiple-use systems, and in both
cases it is usually bad if my R session causes the machine to swap;
usually that swapping is not the result of a desired computation -- most
often, it's from a straightforward mistake -- but it can take
substantial amounts of time for the machine to respond to aborts or kill
requests, and usually if the process grows enough to touch swap it will
continue growing beyond the swap limit too.

So, why not simply slap on an address-space ulimit instead (that being
the kind of ulimit in Linux that actually works...)?  Well, one reason
is that it then becomes necessary to estimate at the start of an R
session how much memory will be needed over the lifetime of that
session; guess too low, and at some point later (maybe days or even
weeks later) I might get a failure to allocate.  My options at that
stage would be to save the workspace and restart the session with a
higher limit, or attempt to delete enough things from the existing
workspace to allow the allocation to succeed.  (Have I missed anything?)
Saving and restarting will take substantial time (from writing ~4GB to
disk) while deleting things from the existing session involves cognitive
overhead that is irrelevant to my current investigation and may in any
case not succeed to free enough.

So, being able to raise the limit to something generally large for a
short time to perform a computation, get the results, and then lower the
limit again allows me to protect myself in general from overwhelming the
machine with mistaken computations, while also allowing in specific
cases the ability to dedicate more resources to a particular
computation.

> I don't find reporting values of several GB as bytes very useful, but
> then mem.limits() is not useful to me either ....

Ah, I'm not particularly interested in the reporting side of
mem.limits() :-); the setting side, on the other hand, very much so.

Thank you again for the fixes.

Best,

Christophe


From quintfall at googlemail.com  Tue Jul 26 17:26:35 2011
From: quintfall at googlemail.com (Thorsten)
Date: Tue, 26 Jul 2011 17:26:35 +0200
Subject: [Rd] R result objects as lists
Message-ID: <86sjpt9i6s.fsf@googlemail.com>

Hello List,
I want to communicate between a minimalistc lisp that has
only numbers, symbols (also used as strings) and lists as datatypes, and R.

It should be no problem to send command strings from the lisp process
to the R childprocess.  

I know, R is mostly implemented in Scheme, and I read recently, that
these special return objects of R are really lists under the
hood. Therefore my questions:

1. When I send a command from a lisp (that iks not elisp) to an R
subprocess, how can I recieve the R result object as a list (and not a
special R object)?

2. Apart from graphics - are all R result objects lists (or numbers or
strings)? That is, is it safe to assume that the result of an R call
will always be either a number, a string or a list (under the hood)?

Cheers
Thorsten


From ligges at statistik.tu-dortmund.de  Tue Jul 26 20:07:38 2011
From: ligges at statistik.tu-dortmund.de (Uwe Ligges)
Date: Tue, 26 Jul 2011 20:07:38 +0200
Subject: [Rd] CRAN mirror size mushrooming; consider archiving some?
In-Reply-To: <CAErODj95THdYT0phbsF91hBeMoZr2ntM=pu_snKnzL6EUwkXZA@mail.gmail.com>
References: <CAErODj95THdYT0phbsF91hBeMoZr2ntM=pu_snKnzL6EUwkXZA@mail.gmail.com>
Message-ID: <4E2F026A.1080507@statistik.tu-dortmund.de>



On 25.07.2011 19:47, Paul Johnson wrote:
> Hi, everybody
>
> I'm setting up a new CRAN mirror and filled up the disk space the
> server allotted me.  I asked for more, then filled that up.  Now the
> system administrators want me to buy an $800 fiber channel card and a
> storage device.  I'm going to do that, but it does make want to
> suggest to you that this is a problem.

Why? Just for the mirror? That's nonsense. A 6 year old outdated desktop 
machine (say upgraded to 2GB RAM) with a 1T harddisc for 50$ should be 
fine for your first tries. The bottleneck will probably be your network 
connection rather than the storage.


> CRAN now is about 68GB, and about 3/4 of that is in the bin folder,
> where one finds copies of compiled packages for macosx and windows.
> If the administrators of CRAN would move the packages for R before,
> say, 2.12, to long term storage, then mirror management would be a bit
> more, well, manageable.
>
> Moving the R for windows packages for, say, R 2.0 through 2.10 would
> save some space, and possibly establish a useful precedent for the
> long term.


That is right, but then users of R < 2.11.0 could no longer use 
install.packages() and friends. If we want to move stuff around in 
future, we may want to implement that in R first. We thought about 
removing old binaries before, but then disk space increased roughly as 
exponentially as repository space in the past and we decided to stay 
with it as is.


> Here's the bin/windows folder. Note it is expanding exponentially (or nearly so)

And you see that quite a lot of efforts were made during the last 
release cycles to reduce the amount of used memory (e.g. using better 
compression).

Best wishes,
Uwe



> $ du --max-depth=1 | sort
> 1012644 ./2.6
> 103504  ./1.7
> 122200  ./1.8
> 1239876 ./2.7
> 1487024 ./2.8
> 15220   ./ATLAS
> 167668  ./1.9
> 17921604        .
> 1866196 ./2.9
> 204392  ./2.0
> 2207708 ./2.10
> 2340120 ./2.13
> 2356272 ./2.12
> 2403176 ./2.11
> 298620  ./2.1
> 364292  ./2.2
> 438044  ./2.3
> 595920  ./2.4
> 698064  ./2.5
>


From ligges at statistik.tu-dortmund.de  Tue Jul 26 20:33:16 2011
From: ligges at statistik.tu-dortmund.de (Uwe Ligges)
Date: Tue, 26 Jul 2011 20:33:16 +0200
Subject: [Rd] plot.function documentation/export?
In-Reply-To: <4E2DA187.3010204@gmail.com>
References: <4E2D8F83.5010707@gmail.com>
	<4E2D9FEF.3060805@statistik.tu-dortmund.de>
	<4E2DA187.3010204@gmail.com>
Message-ID: <4E2F086C.5030004@statistik.tu-dortmund.de>

Now I see the difference: I was using R-devel and that worked as you 
expected.

Best,
Uwe Ligges

On 25.07.2011 19:01, Ben Bolker wrote:
> -----BEGIN PGP SIGNED MESSAGE-----
> Hash: SHA1
>
> On 07/25/2011 12:55 PM, Uwe Ligges wrote:
>>
>>
>> On 25.07.2011 17:45, Ben Bolker wrote:
>>
>>     I recently suggested to someone (
>> http://stackoverflow.com/questions/6789055/r-inconsistency-why-add-t-sometimes-works-and-sometimes-not-in-the-plot-functi/6789098#6789098
>>
>> ) that the should use methods("plot") or methods(class="function") to
>> locate the documentation on the plot method for objects of class
>> "function", but they pointed out that these don't actually work.
>>
>>     I can't figure out why not: src/library/graphics/man/curve.Rd contains
>> the line
>>
>> \method{plot}{function}(x, y = 0, to = 1, from = y, xlim = NULL, ylab =
>> NULL, \dots)
>>
>> and src/library/graphics/DESCRIPTION contains
>>
>>
>>> you mean the following line is in NAMESPACE rather than DESCRIPTION.
>>
>> S3method(plot, "function")
>
>    Yes, sorry.
>
>>
>>
>>    [presumably the extra quotes are in there because function is a
>> reserved word?]
>>
>>    I'm not sure where else the information should be.  Searching around in
>> the code tree for information on tail.function (which is listed in the
>> methods:
>>
>>>>> methods(class="function")
>> [1] as.list.function head.function*   print.function   tail.function*
>>
>> I find the same S3method syntax, so I guess the quotation marks aren't
>> the problem ...
>>
>>> ?tail.function
>>
>>> tells us this one is from package "utils" and you can search for this
>>> function in the sources of the utils package
>>
>>> Or you could ask for
>>
>> getAnywhere("tail.function")
>>
>>> and R tells you
>>
>>> A single object matching tail.function was found
>>> It was found in the following places
>>>    registered S3 method for tail from namespace utils
>>>    namespace:utils
>>> [.....]
>>
>>> Best wishes,
>>> Uwe
>>
>>
>>
>
>    Sorry, I didn't frame my question very clearly.  I can find
> "tail.function" just fine, or I could if I wanted to.   What I don't
> know is why methods("plot") and methods(class="function") don't list
> "plot.function" even though its documentation and setup seem to be
> similar to "tail.function", which *does* show up in
> methods(class="function") ...
>
>    cheers
>      Ben Bolker
>
>
> =========
>
>    No plot.function listing in either of these ...
>
>> library("graphics")
>> methods("plot")
>   [1] plot.acf*           plot.data.frame*    plot.decomposed.ts*
>   [4] plot.default        plot.dendrogram*    plot.density
>   [7] plot.ecdf           plot.factor*        plot.formula*
> [10] plot.hclust*        plot.histogram*     plot.HoltWinters*
> [13] plot.isoreg*        plot.lm             plot.medpolish*
> [16] plot.mlm            plot.ppr*           plot.prcomp*
> [19] plot.princomp*      plot.profile.nls*   plot.spec
> [22] plot.spec.coherency plot.spec.phase     plot.stepfun
> [25] plot.stl*           plot.table*         plot.ts
> [28] plot.tskernel*      plot.TukeyHSD
>
>     Non-visible functions are asterisked
>> methods(class="function")
> [1] as.list.function head.function*   print.function   tail.function*
>
>     Non-visible functions are asterisked
>
>
>
> -----BEGIN PGP SIGNATURE-----
> Version: GnuPG v1.4.10 (GNU/Linux)
> Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org/
>
> iEYEARECAAYFAk4toYcACgkQc5UpGjwzenMyFACggRdP+48u++szSbV82S4HhTxj
> MJcAnAsZ0iOXAsXtSeB8PZ4JmlgUgb9t
> =2lyp
> -----END PGP SIGNATURE-----


From bbolker at gmail.com  Tue Jul 26 20:48:50 2011
From: bbolker at gmail.com (Ben Bolker)
Date: Tue, 26 Jul 2011 14:48:50 -0400
Subject: [Rd] plot.function documentation/export?
In-Reply-To: <4E2F086C.5030004@statistik.tu-dortmund.de>
References: <4E2D8F83.5010707@gmail.com>
	<4E2D9FEF.3060805@statistik.tu-dortmund.de>
	<4E2DA187.3010204@gmail.com>
	<4E2F086C.5030004@statistik.tu-dortmund.de>
Message-ID: <4E2F0C12.9070809@gmail.com>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

  OK, I see that BDR did this on 2011-06-08 -- I was getting confused by
looking at the code of the development version but running the release
version.

  Thanks.

   Ben


On 07/26/2011 02:33 PM, Uwe Ligges wrote:
> Now I see the difference: I was using R-devel and that worked as you
> expected.
> 
> Best,
> Uwe Ligges
> 
> On 25.07.2011 19:01, Ben Bolker wrote:
> On 07/25/2011 12:55 PM, Uwe Ligges wrote:
>>>>
>>>>
>>>> On 25.07.2011 17:45, Ben Bolker wrote:
>>>>
>>>>     I recently suggested to someone (
>>>> http://stackoverflow.com/questions/6789055/r-inconsistency-why-add-t-sometimes-works-and-sometimes-not-in-the-plot-functi/6789098#6789098
>>>>
>>>>
>>>> ) that the should use methods("plot") or methods(class="function") to
>>>> locate the documentation on the plot method for objects of class
>>>> "function", but they pointed out that these don't actually work.
>>>>
>>>>     I can't figure out why not: src/library/graphics/man/curve.Rd
>>>> contains
>>>> the line
>>>>
>>>> \method{plot}{function}(x, y = 0, to = 1, from = y, xlim = NULL, ylab =
>>>> NULL, \dots)
>>>>
>>>> and src/library/graphics/DESCRIPTION contains
>>>>
>>>>
>>>>> you mean the following line is in NAMESPACE rather than DESCRIPTION.
>>>>
>>>> S3method(plot, "function")
> 
>    Yes, sorry.
> 
>>>>
>>>>
>>>>    [presumably the extra quotes are in there because function is a
>>>> reserved word?]
>>>>
>>>>    I'm not sure where else the information should be.  Searching
>>>> around in
>>>> the code tree for information on tail.function (which is listed in the
>>>> methods:
>>>>
>>>>>>> methods(class="function")
>>>> [1] as.list.function head.function*   print.function   tail.function*
>>>>
>>>> I find the same S3method syntax, so I guess the quotation marks aren't
>>>> the problem ...
>>>>
>>>>> ?tail.function
>>>>
>>>>> tells us this one is from package "utils" and you can search for this
>>>>> function in the sources of the utils package
>>>>
>>>>> Or you could ask for
>>>>
>>>> getAnywhere("tail.function")
>>>>
>>>>> and R tells you
>>>>
>>>>> A single object matching tail.function was found
>>>>> It was found in the following places
>>>>>    registered S3 method for tail from namespace utils
>>>>>    namespace:utils
>>>>> [.....]
>>>>
>>>>> Best wishes,
>>>>> Uwe
>>>>
>>>>
>>>>
> 
>    Sorry, I didn't frame my question very clearly.  I can find
> "tail.function" just fine, or I could if I wanted to.   What I don't
> know is why methods("plot") and methods(class="function") don't list
> "plot.function" even though its documentation and setup seem to be
> similar to "tail.function", which *does* show up in
> methods(class="function") ...
> 
>    cheers
>      Ben Bolker
> 
> 
> =========
> 
>    No plot.function listing in either of these ...
> 
>>>> library("graphics")
>>>> methods("plot")
>   [1] plot.acf*           plot.data.frame*    plot.decomposed.ts*
>   [4] plot.default        plot.dendrogram*    plot.density
>   [7] plot.ecdf           plot.factor*        plot.formula*
> [10] plot.hclust*        plot.histogram*     plot.HoltWinters*
> [13] plot.isoreg*        plot.lm             plot.medpolish*
> [16] plot.mlm            plot.ppr*           plot.prcomp*
> [19] plot.princomp*      plot.profile.nls*   plot.spec
> [22] plot.spec.coherency plot.spec.phase     plot.stepfun
> [25] plot.stl*           plot.table*         plot.ts
> [28] plot.tskernel*      plot.TukeyHSD
> 
>     Non-visible functions are asterisked
>>>> methods(class="function")
> [1] as.list.function head.function*   print.function   tail.function*
> 
>     Non-visible functions are asterisked
> 
> 
> 
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.10 (GNU/Linux)
Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org/

iEYEARECAAYFAk4vDBIACgkQc5UpGjwzenP54QCghWmpGf5gpmRVYqNxJ+gm41n4
ErgAoJlXroIs3DLIPnJ4qyEPy1izMrMl
=ptBG
-----END PGP SIGNATURE-----


From murdoch.duncan at gmail.com  Tue Jul 26 20:53:54 2011
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Tue, 26 Jul 2011 14:53:54 -0400
Subject: [Rd] R result objects as lists
In-Reply-To: <86sjpt9i6s.fsf@googlemail.com>
References: <86sjpt9i6s.fsf@googlemail.com>
Message-ID: <4E2F0D42.7050906@gmail.com>

On 26/07/2011 11:26 AM, Thorsten wrote:
> Hello List,
> I want to communicate between a minimalistc lisp that has
> only numbers, symbols (also used as strings) and lists as datatypes, and R.
>
> It should be no problem to send command strings from the lisp process
> to the R childprocess.
>
> I know, R is mostly implemented in Scheme,

No, the design of the original R interpreter was based on a Scheme 
interpreter, but it is mostly implemented in C.

>   and I read recently, that
> these special return objects of R are really lists under the
> hood. Therefore my questions:
>
> 1. When I send a command from a lisp (that iks not elisp) to an R
> subprocess, how can I recieve the R result object as a list (and not a
> special R object)?
>
> 2. Apart from graphics - are all R result objects lists (or numbers or
> strings)? That is, is it safe to assume that the result of an R call
> will always be either a number, a string or a list (under the hood)?

No, you need to treat the results as C structures under the hood.  Some 
are implemented as Lisp-like lists, but most are vectors with additional 
information about the type of object that is contained within (in a 
C-style array).

Duncan Murdoch


From Greg.Snow at imail.org  Tue Jul 26 23:31:56 2011
From: Greg.Snow at imail.org (Greg Snow)
Date: Tue, 26 Jul 2011 15:31:56 -0600
Subject: [Rd] default par
In-Reply-To: <BAY149-W36D4791B652A1054D0570DD54E0@phx.gbl>
References: <BAY149-W36D4791B652A1054D0570DD54E0@phx.gbl>
Message-ID: <B37C0A15B8FB3C468B5BC7EBC7DA14CC6349E11E71@LP-EXMBVS10.CO.IHC.COM>

For number 1, one option is to use the setHook function with the hook in plot.new.  Using this you can create a function that will be called before every new plot is created, your function could then call par with the options that you want, this will set the parameters on all devices.  However it could cause problems if you ever wanted to change those values for a plot, your call to par would be overwritten by the hook function.

For number 2, S-PLUS did have the default to warn when points were outside the plotting region, this was annoying when people intentionally used the limits to look at only part of the data, so I don't think it would be popular to bring back this behavior in general.  You can use the zoomplot function in the TeachingDemos package to expand the range of your current plot to show data that was outside the limits, or I believe that if you use ggplot2 the plots will be expanded automatically to include all the data (unless you limit the range in the call).  You could also write your own points or plot function that would check the range and give warnings then call the regular points or plot function.

-- 
Gregory (Greg) L. Snow Ph.D.
Statistical Data Center
Intermountain Healthcare
greg.snow at imail.org
801.408.8111


> -----Original Message-----
> From: r-devel-bounces at r-project.org [mailto:r-devel-bounces at r-
> project.org] On Behalf Of Berry Boessenkool
> Sent: Friday, July 22, 2011 7:47 AM
> To: r-devel at r-project.org
> Subject: [Rd] default par
> 
> 
> 
> Hello dear R-developers,
> 
> two questions on an otherwise magnificent program:
> 
> 1)
> Is there a way to set defaults for par differently than R offers
> normally?
> I for example would like to have las default to 1. (or in the same
> style, sometimes type in plot() could be "l" per default).
> 
> Tthe following post desribes pretty much exactly that:
> https://stat.ethz.ch/pipermail/r-help/2007-March/126646.html
> It was written four years ago, but it seems like there has been no real
> elegant solution.
> Did I just miss something there? If so, could someone give me an
> update?
> If not, is there a chance that such a feature? would be added to future
> R-versions?
> I could live with the idea to assign the par$element default in
> Rprofile.site.
> 
> 2)
> Would it appear sensible to have R give a warning, when points() is
> used, and some/all values are out of plotting range in the active
> device?
> It has happened some times that I needed quite a bit of time to figure
> out why nothing was plotted.
> Such a warning (or maybe even a beep?) would give users the clue to
> look at the values right away...
> (What I mean is this: ?? plot(1:10)? ; points(11,3)??? just in case
> it's unclear)
> 
> 
> Thanks ahead for pondering, and again: R ist the most beautiful thing I
> discovered in the last three years.
> Keep up the good work!
> 
> Berry
> 
> -------------------------------------
> Berry Boessenkool
> University of Potsdam, Germany
> -------------------------------------
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From hadley at rice.edu  Wed Jul 27 00:08:57 2011
From: hadley at rice.edu (Hadley Wickham)
Date: Tue, 26 Jul 2011 17:08:57 -0500
Subject: [Rd] CRAN mirror size mushrooming; consider archiving some?
In-Reply-To: <4E2F026A.1080507@statistik.tu-dortmund.de>
References: <CAErODj95THdYT0phbsF91hBeMoZr2ntM=pu_snKnzL6EUwkXZA@mail.gmail.com>
	<4E2F026A.1080507@statistik.tu-dortmund.de>
Message-ID: <CABdHhvFkbNTzJX5mSePZx9+Ja88cM_e=kr1yOMG5-GgpLiZpbw@mail.gmail.com>

>> I'm setting up a new CRAN mirror and filled up the disk space the
>> server allotted me. ?I asked for more, then filled that up. ?Now the
>> system administrators want me to buy an $800 fiber channel card and a
>> storage device. ?I'm going to do that, but it does make want to
>> suggest to you that this is a problem.
>
> Why? Just for the mirror? That's nonsense. A 6 year old outdated desktop
> machine (say upgraded to 2GB RAM) with a 1T harddisc for 50$ should be fine
> for your first tries. The bottleneck will probably be your network
> connection rather than the storage.

Another perspective is that it costs ~$10 / month to store 68 Gb of
data on amazon's S3.  And then you pay 12c / GB for download.

Hadley

-- 
Assistant Professor / Dobelman Family Junior Chair
Department of Statistics / Rice University
http://had.co.nz/


From jeffrey.ryan at lemnica.com  Wed Jul 27 00:17:43 2011
From: jeffrey.ryan at lemnica.com (Jeffrey Ryan)
Date: Tue, 26 Jul 2011 17:17:43 -0500
Subject: [Rd] CRAN mirror size mushrooming; consider archiving some?
In-Reply-To: <CABdHhvFkbNTzJX5mSePZx9+Ja88cM_e=kr1yOMG5-GgpLiZpbw@mail.gmail.com>
References: <CAErODj95THdYT0phbsF91hBeMoZr2ntM=pu_snKnzL6EUwkXZA@mail.gmail.com>
	<4E2F026A.1080507@statistik.tu-dortmund.de>
	<CABdHhvFkbNTzJX5mSePZx9+Ja88cM_e=kr1yOMG5-GgpLiZpbw@mail.gmail.com>
Message-ID: <CABDUZc_f_6QYUAL+sKqbOb6tw5iNp=RSeDq-bGZYBrs2Dq45-Q@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20110726/b164da47/attachment.pl>

From cubranic at stat.ubc.ca  Wed Jul 27 00:19:00 2011
From: cubranic at stat.ubc.ca (Davor Cubranic)
Date: Tue, 26 Jul 2011 15:19:00 -0700
Subject: [Rd] Best practices for writing R functions
In-Reply-To: <1311425861566-3688850.post@n4.nabble.com>
References: <1311341191873-3686674.post@n4.nabble.com>
	<4E2993CE.4020903@prodsyse.com>
	<CADwqtCONeEqXTTg0OyDj=OJWeQxNKBm79M=iBg1jdy6JtO_w0Q@mail.gmail.com>
	<6441154A9FF1CD4386AF4ABF141A056D265A9E9B@WMEXOSCD2-N1.bocad.bank-banque-canada.ca>
	<CABdHhvHvB4vV5BymZfSguxCUH745nd6mrAk8EDw8Kk3RPN-C2Q@mail.gmail.com>
	<1311425861566-3688850.post@n4.nabble.com>
Message-ID: <DC70CFDD-72C3-4010-9EEC-B5EB540AD85E@stat.ubc.ca>

On 2011-07-23, at 5:57 AM, Alireza Mahani wrote:

> Another trick to reduce verbosity of code (and focus on algorithm logic
> rather than boilerplate code) is to maintain a global copy of variables (in
> the global environment) which makes them visible to all functions (where
> appropriate, of course). Once the development and testing is finished, one
> can tidy things up and modify the function prototypes, add lines for
> unpacking lists inside functions, etc.

I think you'd be better off to stay away from such tricks. It's asking for trouble later on, because unless you have really good unit tests it is very easy to miss a variable during "tidying up" and end up with code that works fine in your development environment but is full of bugs once you distribute it to others.

Davor

From brian at braverock.com  Wed Jul 27 03:18:22 2011
From: brian at braverock.com (Brian G. Peterson)
Date: Tue, 26 Jul 2011 20:18:22 -0500
Subject: [Rd] Best practices for writing R functions
In-Reply-To: <DC70CFDD-72C3-4010-9EEC-B5EB540AD85E@stat.ubc.ca>
References: <1311341191873-3686674.post@n4.nabble.com>
	<4E2993CE.4020903@prodsyse.com>
	<CADwqtCONeEqXTTg0OyDj=OJWeQxNKBm79M=iBg1jdy6JtO_w0Q@mail.gmail.com>
	<6441154A9FF1CD4386AF4ABF141A056D265A9E9B@WMEXOSCD2-N1.bocad.bank-banque-canada.ca>
	<CABdHhvHvB4vV5BymZfSguxCUH745nd6mrAk8EDw8Kk3RPN-C2Q@mail.gmail.com>
	<1311425861566-3688850.post@n4.nabble.com>
	<DC70CFDD-72C3-4010-9EEC-B5EB540AD85E@stat.ubc.ca>
Message-ID: <1311729502.21063.138.camel@brian-desktop>

On Tue, 2011-07-26 at 15:19 -0700, Davor Cubranic wrote:
> On 2011-07-23, at 5:57 AM, Alireza Mahani wrote:
> 
> > Another trick to reduce verbosity of code (and focus on algorithm logic
> > rather than boilerplate code) is to maintain a global copy of variables (in
> > the global environment) which makes them visible to all functions (where
> > appropriate, of course). Once the development and testing is finished, one
> > can tidy things up and modify the function prototypes, add lines for
> > unpacking lists inside functions, etc.
> 
> I think you'd be better off to stay away from such tricks. It's asking for trouble later on, because unless you have really good unit tests it is very easy to miss a variable during "tidying up" and end up with code that works fine in your development environment but is full of bugs once you distribute it to others.

Isn't this specifically one of the things that environment are *for*?

Have your package/script/functions create an environment, and store
'loose variables' there.  Use get/assign to manage.  Don't
clutter .GlobalEnv.

-- 
Brian


From quintfall at googlemail.com  Tue Jul 26 23:10:01 2011
From: quintfall at googlemail.com (Thorsten)
Date: Tue, 26 Jul 2011 23:10:01 +0200
Subject: [Rd] R result objects as lists
References: <86sjpt9i6s.fsf@googlemail.com> <4E2F0D42.7050906@gmail.com>
Message-ID: <86k4b4aguu.fsf@googlemail.com>

Duncan Murdoch <murdoch.duncan at gmail.com> writes:

> No, you need to treat the results as C structures under the hood.
> Some are implemented as Lisp-like lists, but most are vectors with
> additional information about the type of object that is contained
> within (in a C-style array).

That makes things a bit more complicated (for me).
Thanks for the info
Thorsten


From ack.vandal at gmail.com  Fri Jul 29 00:56:27 2011
From: ack.vandal at gmail.com (Alexander James Rickett)
Date: Thu, 28 Jul 2011 15:56:27 -0700
Subject: [Rd] R CMD CHECK doens't run configure when testing install?
Message-ID: <960016D7-A6D2-44BC-9BBF-5D45EDD5E6E8@gmail.com>

I'm trying to get ready to submit a package to CRAN, but in order for the package to install on OS X, I need to temporarily set an environment variable.  I put this in the 'configure' script, and 'R CMD INSTALL MyPackage' works fine, but when I do 'R CMD CHECK MyPackage', and it tests installation, the configure script doesn't run and consequently the installation fails.  Should I be setting the variable another way?  It passes all the other checks, and it will install outside of check, so could I just submit it as is?

Thanks!

From ripley at stats.ox.ac.uk  Fri Jul 29 09:29:43 2011
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 29 Jul 2011 08:29:43 +0100 (BST)
Subject: [Rd] R CMD CHECK doens't run configure when testing install?
In-Reply-To: <960016D7-A6D2-44BC-9BBF-5D45EDD5E6E8@gmail.com>
References: <960016D7-A6D2-44BC-9BBF-5D45EDD5E6E8@gmail.com>
Message-ID: <alpine.LFD.2.02.1107290827150.32155@gannet.stats.ox.ac.uk>

Please don't spam multiple lists!

And follow the posting guide: you have not given the reproducible 
example asked for, there is no signature block ....

Of course you could waste people's time by submitting a package that 
does not past R CMD check, but you are asked to follow a set of checks 
in 'Writing R Extensions' and the CRAN workers do not have unlimited 
patience ....

On Thu, 28 Jul 2011, Alexander James Rickett wrote:

> I'm trying to get ready to submit a package to CRAN, but in order 
> for the package to install on OS X, I need to temporarily set an 
> environment variable.  I put this in the 'configure' script, and 'R 
> CMD INSTALL MyPackage' works fine, but when I do 'R CMD CHECK 
> MyPackage', and it tests installation, the configure script doesn't 
> run and consequently the installation fails.  Should I be setting 
> the variable another way?  It passes all the other checks, and it 
> will install outside of check, so could I just submit it as is?
>
> Thanks!
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From ack.vandal at gmail.com  Fri Jul 29 00:56:27 2011
From: ack.vandal at gmail.com (Alexander James Rickett)
Date: Thu, 28 Jul 2011 15:56:27 -0700
Subject: [Rd] R CMD CHECK doens't run configure when testing install?
	(Revised)
Message-ID: <960016D7-A6D2-44BC-9BBF-5D45EDD5E6E8@gmail.com>

I'm trying to get ready to submit a package to CRAN, but in order for the package to install on OS X, I need to temporarily set the environment variable NOAWT=1.  I put 'export NOAWT=1' in my package's 'configure' script, and 'R CMD INSTALL MyPackage' returns with no errors.   However when I do 'R CMD CHECK MyPackage', the installation test fails, because the configure script is not invoked.  If I set the variable manually in Terminal before I run CHECK, then it passes all the tests.

Is the configure script the right place to be setting this variable?

If so, why isn't configure being invoked as part of R CMD CHECK?

If not, where should I set the environmental variable NOAWT=1 such that the change will be reflected in the R CMD CHECK installation test?

Here is the source tar-ball of my package if you want to try for yourself.
http://www.cs.ucla.edu/~alexalex/R/src/contrib/DeducerText_0.1-0.tar.gz

Thanks.

--
Alex Rickett
ack.vandal at gmail.com

From pgilbert at bank-banque-canada.ca  Fri Jul 29 17:41:09 2011
From: pgilbert at bank-banque-canada.ca (Paul Gilbert)
Date: Fri, 29 Jul 2011 15:41:09 +0000
Subject: [Rd] R DDD fortran
Message-ID: <6441154A9FF1CD4386AF4ABF141A056D2A71A643@WMEXOSCD2-N1.bocad.bank-banque-canada.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20110729/7a00294c/attachment.pl>

From jeroen.ooms at stat.ucla.edu  Fri Jul 29 18:06:40 2011
From: jeroen.ooms at stat.ucla.edu (Jeroen Ooms)
Date: Fri, 29 Jul 2011 18:06:40 +0200
Subject: [Rd] [foreign] read.spss 'measure' attribute
Message-ID: <CABFfbXu6vKdX66j=h_Kw7G_+h=K89LfYiJg_oOAvo2V1BFvG_A@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20110729/90520fb2/attachment.pl>

From daniel.cegielka at gmail.com  Fri Jul 29 20:03:43 2011
From: daniel.cegielka at gmail.com (=?ISO-8859-2?Q?Daniel_Cegie=B3ka?=)
Date: Fri, 29 Jul 2011 20:03:43 +0200
Subject: [Rd] R DDD fortran
In-Reply-To: <6441154A9FF1CD4386AF4ABF141A056D2A71A643@WMEXOSCD2-N1.bocad.bank-banque-canada.ca>
References: <6441154A9FF1CD4386AF4ABF141A056D2A71A643@WMEXOSCD2-N1.bocad.bank-banque-canada.ca>
Message-ID: <CAPLrYESCtQv-khBKAqffgapMJ7mocw0NtAnOdAueB3_k-OC2Cw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20110729/626cfec1/attachment.pl>

From murdoch.duncan at gmail.com  Fri Jul 29 20:03:00 2011
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Fri, 29 Jul 2011 14:03:00 -0400
Subject: [Rd] R DDD fortran
In-Reply-To: <6441154A9FF1CD4386AF4ABF141A056D2A71A643@WMEXOSCD2-N1.bocad.bank-banque-canada.ca>
References: <6441154A9FF1CD4386AF4ABF141A056D2A71A643@WMEXOSCD2-N1.bocad.bank-banque-canada.ca>
Message-ID: <4E32F5D4.7090806@gmail.com>

On 29/07/2011 11:41 AM, Paul Gilbert wrote:
> Is there an easy way to compile a packages's fortran so that it has the information for ddd or gdb to follow the lines of the code? I seem to remember seeing something about this, but I cannot find it in Writing R Extensions. (I see about Valgrind, debugging a segfault, and some other tricks, but what I really want is to follow the fortran logic.)

On Windows, you set the environment variable DEBUG to T before 
compiling.   I believe that's the default on non-Windows platforms, but 
I imagine there are exceptions (perhaps all of them!)

Duncan Murdoch

> Thanks for any pointers,
> Paul
> ====================================================================================
>
> La version fran??aise suit le texte anglais.
>
> ------------------------------------------------------------------------------------
>
> This email may contain privileged and/or confidential information, and the Bank of
> Canada does not waive any related rights. Any distribution, use, or copying of this
> email or the information it contains by other than the intended recipient is
> unauthorized. If you received this email in error please delete it immediately from
> your system and notify the sender promptly by email that you have done so.
>
> ------------------------------------------------------------------------------------
>
> Le pr??sent courriel peut contenir de l'information privil??gi??e ou confidentielle.
> La Banque du Canada ne renonce pas aux droits qui s'y rapportent. Toute diffusion,
> utilisation ou copie de ce courriel ou des renseignements qu'il contient par une
> personne autre que le ou les destinataires d??sign??s est interdite. Si vous recevez
> ce courriel par erreur, veuillez le supprimer imm??diatement et envoyer sans d??lai ? 
> l'exp??diteur un message ??lectronique pour l'aviser que vous avez ??limin?? de votre
> ordinateur toute copie du courriel re??u.
>
> 	[[alternative HTML version deleted]]
>
>
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From murdoch.duncan at gmail.com  Fri Jul 29 20:11:47 2011
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Fri, 29 Jul 2011 14:11:47 -0400
Subject: [Rd] Front ends handling help.search() results?
Message-ID: <4E32F7E3.4080604@gmail.com>

Which front ends process the help.search() results to produce nice 
clickable lists, instead of the simple text display that's the default?  
I know the R.app gui on  Mac OS does; are there others?

I'm asking because one of the decisions made when R Core met in Vienna 
was to allow vignettes to be returned as results of help.search().  All 
of those front ends will need to be able to handle this.

Duncan Murdoch


From baptiste.auguie at googlemail.com  Sat Jul 30 01:20:14 2011
From: baptiste.auguie at googlemail.com (baptiste auguie)
Date: Sat, 30 Jul 2011 11:20:14 +1200
Subject: [Rd] package encoding warning
Message-ID: <CANLFJPreK32SDPprP5t6vWoNothE1zMJF-V1FKrYoQZRCKHc7w@mail.gmail.com>

Dear list,

I'd like to get rid off a couple of warnings that have appeared in
checking my package on CRAN (I did not find them on my local machine
before submission). What puzzles me is that different platforms return
different warnings, only one of which I recognise as my obvious
mistake. The check results are at
http://cran.r-project.org/web/checks/check_results_planar.html

r-devel-linux-x86_64-gcc-fedora
complains about unused variable in c++ code; this I understand as my
mistake, I'll fix it.

r-patched-windows-ix86+x86_64
is happy, no warning

r-release-macosx-ix86
checking examples ... WARNING
checking a package with encoding 'UTF-8' in an ASCII locale

This one really puzzles me: I have a Encoding: UTF-8 directive in
DESCRIPTION, precisely because I thought it would take care of
encoding issues such as with my surname. What does this mean, and what
should I do about it?

Similarly,
r-oldrel-macosx-ix86
checking whether package 'planar' can be installed ... WARNING
Found the following significant warnings:
Warning: 'DESCRIPTION' file has 'Encoding' field and re-encoding is not possible

has me perplexed.

Best regards,

baptiste


From ripley at stats.ox.ac.uk  Sat Jul 30 07:29:58 2011
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 30 Jul 2011 06:29:58 +0100 (BST)
Subject: [Rd] package encoding warning
In-Reply-To: <CANLFJPreK32SDPprP5t6vWoNothE1zMJF-V1FKrYoQZRCKHc7w@mail.gmail.com>
References: <CANLFJPreK32SDPprP5t6vWoNothE1zMJF-V1FKrYoQZRCKHc7w@mail.gmail.com>
Message-ID: <alpine.LFD.2.02.1107300621060.27240@gannet.stats.ox.ac.uk>

On Sat, 30 Jul 2011, baptiste auguie wrote:

> Dear list,
>
> I'd like to get rid off a couple of warnings that have appeared in
> checking my package on CRAN (I did not find them on my local machine
> before submission). What puzzles me is that different platforms return
> different warnings, only one of which I recognise as my obvious
> mistake. The check results are at
> http://cran.r-project.org/web/checks/check_results_planar.html
>
> r-devel-linux-x86_64-gcc-fedora
> complains about unused variable in c++ code; this I understand as my
> mistake, I'll fix it.
>
> r-patched-windows-ix86+x86_64
> is happy, no warning
>
> r-release-macosx-ix86
> checking examples ... WARNING
> checking a package with encoding 'UTF-8' in an ASCII locale
>
> This one really puzzles me: I have a Encoding: UTF-8 directive in
> DESCRIPTION, precisely because I thought it would take care of
> encoding issues such as with my surname.

Your 'thought' was unfounded: please re-read the relevant sections of 
the R manuals.

> What does this mean, and what should I do about it?

It means that your package cannot be checked correctly in that locale.
You can avoid it by following the recommendations to use only ASCII, 
as you did in this message (or at least, it was ASCII by the time it 
reached me).

>
> Similarly,
> r-oldrel-macosx-ix86
> checking whether package 'planar' can be installed ... WARNING
> Found the following significant warnings:
> Warning: 'DESCRIPTION' file has 'Encoding' field and re-encoding is not possible
>
> has me perplexed.

Same: non-ASCII UTF-8 strings cannot be represented in the C (aka 
ASCII) locale.

Now, I think it is a mistake to be checking in the C locale on Mac OS 
X as no end user will deliberately be using it, but that is the 
default for batch use on many OSes.  *But* the same encoding issues 
for European names will occur in, say, the Japanese Windows locale.


> Best regards,
>
> baptiste
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From baptiste.auguie at googlemail.com  Sat Jul 30 09:39:09 2011
From: baptiste.auguie at googlemail.com (baptiste auguie)
Date: Sat, 30 Jul 2011 19:39:09 +1200
Subject: [Rd] package encoding warning
In-Reply-To: <alpine.LFD.2.02.1107300621060.27240@gannet.stats.ox.ac.uk>
References: <CANLFJPreK32SDPprP5t6vWoNothE1zMJF-V1FKrYoQZRCKHc7w@mail.gmail.com>
	<alpine.LFD.2.02.1107300621060.27240@gannet.stats.ox.ac.uk>
Message-ID: <CANLFJPrN8V2Ox2Q0KKm2Zu3zO0yR6dFfz74tHihrNU=cU_uJOQ@mail.gmail.com>

Thank you -- since I don't understand the relevant discussion in
Writing R Extensions, I have now removed Encoding: UTF-8 from the
DESCRIPTION file; I also made sure that my surname and all text was in
plain ASCII.

Best regards,

baptiste


On 30 July 2011 17:29, Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> On Sat, 30 Jul 2011, baptiste auguie wrote:
>
>> Dear list,
>>
>> I'd like to get rid off a couple of warnings that have appeared in
>> checking my package on CRAN (I did not find them on my local machine
>> before submission). What puzzles me is that different platforms return
>> different warnings, only one of which I recognise as my obvious
>> mistake. The check results are at
>> http://cran.r-project.org/web/checks/check_results_planar.html
>>
>> r-devel-linux-x86_64-gcc-fedora
>> complains about unused variable in c++ code; this I understand as my
>> mistake, I'll fix it.
>>
>> r-patched-windows-ix86+x86_64
>> is happy, no warning
>>
>> r-release-macosx-ix86
>> checking examples ... WARNING
>> checking a package with encoding 'UTF-8' in an ASCII locale
>>
>> This one really puzzles me: I have a Encoding: UTF-8 directive in
>> DESCRIPTION, precisely because I thought it would take care of
>> encoding issues such as with my surname.
>
> Your 'thought' was unfounded: please re-read the relevant sections of the R
> manuals.
>
>> What does this mean, and what should I do about it?
>
> It means that your package cannot be checked correctly in that locale.
> You can avoid it by following the recommendations to use only ASCII, as you
> did in this message (or at least, it was ASCII by the time it reached me).
>
>>
>> Similarly,
>> r-oldrel-macosx-ix86
>> checking whether package 'planar' can be installed ... WARNING
>> Found the following significant warnings:
>> Warning: 'DESCRIPTION' file has 'Encoding' field and re-encoding is not
>> possible
>>
>> has me perplexed.
>
> Same: non-ASCII UTF-8 strings cannot be represented in the C (aka ASCII)
> locale.
>
> Now, I think it is a mistake to be checking in the C locale on Mac OS X as
> no end user will deliberately be using it, but that is the default for batch
> use on many OSes. ?*But* the same encoding issues for European names will
> occur in, say, the Japanese Windows locale.
>
>
>> Best regards,
>>
>> baptiste
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>
> --
> Brian D. Ripley, ? ? ? ? ? ? ? ? ?ripley at stats.ox.ac.uk
> Professor of Applied Statistics, ?http://www.stats.ox.ac.uk/~ripley/
> University of Oxford, ? ? ? ? ? ? Tel: ?+44 1865 272861 (self)
> 1 South Parks Road, ? ? ? ? ? ? ? ? ? ? +44 1865 272866 (PA)
> Oxford OX1 3TG, UK ? ? ? ? ? ? ? ?Fax: ?+44 1865 272595
>


From thomas.friedrichsmeier at ruhr-uni-bochum.de  Sat Jul 30 10:40:02 2011
From: thomas.friedrichsmeier at ruhr-uni-bochum.de (Thomas Friedrichsmeier)
Date: 30 Jul 2011 10:40:02 +0200
Subject: [Rd] Front ends handling help.search() results?
In-Reply-To: <4E32F7E3.4080604@gmail.com>
References: <4E32F7E3.4080604@gmail.com>
Message-ID: <201107301040.07068.thomas.friedrichsmeier@ruhr-uni-bochum.de>

On Friday 29 July 2011, Duncan Murdoch wrote:
> Which front ends process the help.search() results to produce nice
> clickable lists, instead of the simple text display that's the default?
> I know the R.app gui on  Mac OS does; are there others?

RKWard does, too. (More precisely: It has a UI for searching help files, which 
relies on help.search()).

> I'm asking because one of the decisions made when R Core met in Vienna
> was to allow vignettes to be returned as results of help.search().  All
> of those front ends will need to be able to handle this.

Will this affect the structure of the hsearch class?

Currently, our implementation for showing an item from the result list is to 
simply call help(help_type="html"), appropriately. What are the implications 
of vignettes for the behavior of help(), if any (with or without specification 
of help_type)?

Regards
Thomas
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 198 bytes
Desc: This is a digitally signed message part.
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20110730/86f739f3/attachment.bin>

From edd at debian.org  Sat Jul 30 15:46:18 2011
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sat, 30 Jul 2011 08:46:18 -0500
Subject: [Rd] R checks links broken
Message-ID: <20020.2858.679622.732957@max.nulle.part>


I was about to show the Rcpp build check results to a user when I noticed
that the relevant URL

   http://cran.r-project.org/web/checks/check_results_Rcpp.html
 
currently yields a 404 error. 

Spot checking for other package yields the same.  Looking at

   http://cran.r-project.org/web/

shows that no checks/ diretory can be found.  Did an NFS mount go down?

Cheers, Dirk

-- 
Gauss once played himself in a zero-sum game and won $50.
                      -- #11 at http://www.gaussfacts.com


From dwinsemius at comcast.net  Sat Jul 30 17:34:25 2011
From: dwinsemius at comcast.net (David Winsemius)
Date: Sat, 30 Jul 2011 11:34:25 -0400
Subject: [Rd] R checks links broken
In-Reply-To: <20020.2858.679622.732957@max.nulle.part>
References: <20020.2858.679622.732957@max.nulle.part>
Message-ID: <C5D43DC2-9F08-40AA-9E3B-37789F4A3313@comcast.net>


On Jul 30, 2011, at 9:46 AM, Dirk Eddelbuettel wrote:

>
> I was about to show the Rcpp build check results to a user when I  
> noticed
> that the relevant URL
>
>   http://cran.r-project.org/web/checks/check_results_Rcpp.html
>
> currently yields a 404 error.

It's there now.

>
> Spot checking for other package yields the same.  Looking at
>
>   http://cran.r-project.org/web/

Also up.

>
> shows that no checks/ diretory can be found.  Did an NFS mount go  
> down?
>
> Cheers, Dirk
>
> -- 
> Gauss once played himself in a zero-sum game and won $50.
>                      -- #11 at http://www.gaussfacts.com
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

David Winsemius, MD
West Hartford, CT


From murdoch.duncan at gmail.com  Sat Jul 30 17:51:57 2011
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Sat, 30 Jul 2011 11:51:57 -0400
Subject: [Rd] Front ends handling help.search() results?
In-Reply-To: <201107301040.07068.thomas.friedrichsmeier@ruhr-uni-bochum.de>
References: <4E32F7E3.4080604@gmail.com>
	<201107301040.07068.thomas.friedrichsmeier@ruhr-uni-bochum.de>
Message-ID: <4E34289D.4030806@gmail.com>

On 11-07-30 4:40 AM, Thomas Friedrichsmeier wrote:
> On Friday 29 July 2011, Duncan Murdoch wrote:
>> Which front ends process the help.search() results to produce nice
>> clickable lists, instead of the simple text display that's the default?
>> I know the R.app gui on  Mac OS does; are there others?
>
> RKWard does, too. (More precisely: It has a UI for searching help files, which
> relies on help.search()).
>
>> I'm asking because one of the decisions made when R Core met in Vienna
>> was to allow vignettes to be returned as results of help.search().  All
>> of those front ends will need to be able to handle this.
>
> Will this affect the structure of the hsearch class?
>
> Currently, our implementation for showing an item from the result list is to
> simply call help(help_type="html"), appropriately. What are the implications
> of vignettes for the behavior of help(), if any (with or without specification
> of help_type)?
>

Yes, there will be additional components added to it.  You can mostly 
ignore them, but not completely:

I will add an additional column named "Type" to the "matches" component 
of the help.search() results.  It will contain "help" for results that 
are man pages, "vignette" for results that are vignettes, and "demo" for 
results that are demos.  The quick fix is to just ignore any results 
other than "help" results; then you'll get the same display as before.

I will also add components to the result corresponding to arguments 
passed to help.search:  "agrep", "ignore.case", "package", "lib.loc",
and "types".  (The last of these is a character vector saying which 
types of documentation were requested, i.e. some combination of "help", 
"vignette", "demo".)  These can be ignored, or used to regenerate the 
same results as the original.  (The dynamic help system regenerates 
results for display.)

I have not committed this code yet, because the new types of entries 
could mess up existing front ends and I wanted to give people some 
warning.  I expect I'll commit (to R-devel only) within a day or two.

Duncan Murdoch


From thomas.friedrichsmeier at ruhr-uni-bochum.de  Sun Jul 31 16:34:52 2011
From: thomas.friedrichsmeier at ruhr-uni-bochum.de (Thomas Friedrichsmeier)
Date: 31 Jul 2011 16:34:52 +0200
Subject: [Rd] Front ends handling help.search() results?
In-Reply-To: <4E34289D.4030806@gmail.com>
References: <4E32F7E3.4080604@gmail.com>
	<201107301040.07068.thomas.friedrichsmeier@ruhr-uni-bochum.de>
	<4E34289D.4030806@gmail.com>
Message-ID: <201107311634.55821.thomas.friedrichsmeier@ruhr-uni-bochum.de>

On Saturday 30 July 2011, Duncan Murdoch wrote:
> I have not committed this code yet, because the new types of entries
> could mess up existing front ends and I wanted to give people some
> warning.  I expect I'll commit (to R-devel only) within a day or two.

Many thanks for the warning! I have added (untested) support for the new 
result types in RKWard.

Regards
Thomas
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 198 bytes
Desc: This is a digitally signed message part.
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20110731/d6884de3/attachment.bin>

From hb at biostat.ucsf.edu  Sun Jul 31 18:32:55 2011
From: hb at biostat.ucsf.edu (Henrik Bengtsson)
Date: Sun, 31 Jul 2011 09:32:55 -0700
Subject: [Rd] Front ends handling help.search() results?
In-Reply-To: <4E32F7E3.4080604@gmail.com>
References: <4E32F7E3.4080604@gmail.com>
Message-ID: <CAFDcVCSqET-X08R-kf-ZZbaEYa1YAtG55M0Lg7iyfd6cYhdcFg@mail.gmail.com>

The R.rsp package has a proof of concept page:

1. library("R.rsp")
2. browseRsp(path="/R/help/")
3. At the bottom, there is a "help.search" form.  Type in search term,
e.g. "svd".

This is all web browser based, so it should be able to handle compiled
vignettes (as long as there are valid paths/URL paths).

/Henrik

On Fri, Jul 29, 2011 at 11:11 AM, Duncan Murdoch
<murdoch.duncan at gmail.com> wrote:
> Which front ends process the help.search() results to produce nice clickable
> lists, instead of the simple text display that's the default? ?I know the
> R.app gui on ?Mac OS does; are there others?
>
> I'm asking because one of the decisions made when R Core met in Vienna was
> to allow vignettes to be returned as results of help.search(). ?All of those
> front ends will need to be able to handle this.
>
> Duncan Murdoch
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From jmc at r-project.org  Sun Jul 31 20:27:44 2011
From: jmc at r-project.org (John Chambers)
Date: Sun, 31 Jul 2011 11:27:44 -0700
Subject: [Rd] Reference classes: assignments to fields
Message-ID: <4E359EA0.4060700@r-project.org>

In R-devel, a recent change (Rev. 56572) makes assignments to fields in 
reference classes consistent with assignments to slots in S4 classes, 
when the field was declared with a class in the call to setRefClass().

The value assigned must come from the declared class for the field, if 
any, or from a subclass of that class.  Previously, if the field had a 
declared class the value for assignment was unconditionally coerced to 
that class.

The added test may produce new error messages, e.g., if you declare a 
field "integer" and assign a numeric, such as 1 rather than 1L.

John


From asaguiar at spsconsultoria.com  Sun Jul 31 23:45:49 2011
From: asaguiar at spsconsultoria.com (Alexandre Aguiar)
Date: Sun, 31 Jul 2011 18:45:49 -0300
Subject: [Rd] example package for devel newcomers
Message-ID: <201107311845.50264@spsconsultoria.com>

Hi,

I'd like to know whether there is a package (or more, of course) regarded 
as a good example that could be used also as an instructional tool for 
newcomers to R extensions development.

Thanks.

-- 


Alexandre

--
Alexandre Santos Aguiar, MD, SCT
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 198 bytes
Desc: This is a digitally signed message part.
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20110731/e55ffca6/attachment.bin>

