From murdoch.duncan at gmail.com  Mon May  1 20:21:59 2017
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Mon, 1 May 2017 14:21:59 -0400
Subject: [Rd] Any progress on write.csv fileEncoding for UTF-16 and
 UTF-32 ?
In-Reply-To: <6dee41ed-b1c1-4d4d-0eb1-03f3e7e0d759@gmail.com>
References: <000101d2c143$bd644380$382cca80$@bigpond.com>
 <6dee41ed-b1c1-4d4d-0eb1-03f3e7e0d759@gmail.com>
Message-ID: <9dce0e1b-27d8-60ca-9df3-3227aab0a3f8@gmail.com>

On 30/04/2017 12:23 PM, Duncan Murdoch wrote:
> No, I don't think anyone is working on this.
>
> There's a fairly simple workaround for the UTF-16 and UTF-32 iconv
> issues:  don't attempt to produce character vectors, produce raw vectors
> instead. (The "toRaw" argument to iconv() asks for this.) Raw vectors
> can contain embedded nulls.  Character vectors can't, because
> internally, R is using 8 bit C strings, and the nulls are string
> terminators.
>
> I don't know how difficult it would be to fix the write.table problems.

I've now taken a look, and it appears as if it's not too hard.  I'll see 
if I can work out a patch that I trust.

Duncan Murdoch

>
> Duncan Murdoch
>
> On 29/04/2017 7:53 PM, Jack Kelley wrote:
>> "R version 3.4.0 (2017-04-21)"  on "x86_64-w64-mingw32" platform
>>
>> I am using CSVs and other text tables, and text in general (including
>> regular expressions), on Windows 10.
>> For me, that means dealing with Windows-1252 and UTF-8 encoding, with UTF-16
>> and UTF-32 as helpful curiosities.
>>
>> Something as simple as iconv ("\n", to = "UTF-16") causes an error, due to
>> an embedded nul.
>>
>> Then there is write.csv (or write.table) with its fileEncoding parameter:
>> not working correctly for UTF-16 and UTF-32.
>>
>> Of course, developers are aware of this, for example ?
>>
>> [Rd] iconv to UTF-16 encoding produces error due to embedded nulls
>> (write.table with fileEncoding param)
>> https://stat.ethz.ch/pipermail/r-devel/2016-February/072323.html
>>
>> iconv to UTF-16 encoding produces error due to embedded nulls (write.table
>> with fileEncoding param)
>> http://r.789695.n4.nabble.com/iconv-to-UTF-16-encoding-produces-error-due-to
>> -embedded-nulls-write-table-with-fileEncoding-param-td4717481.html
>>
>> ----------------------------------------------------------------------------
>> ------------------------
>>
>> Focussing on write.csv and UTF-16LE and UTF-16BE, it seems that a nul
>> character is omitted in each <CarriageReturn><LineFeed> pair.
>>
>> TEST SCRIPT
>> ----------------------------------------------------------------------------
>> ------------------------
>> remove (list = objects())
>>
>> print (sessionInfo())
>> cat ("---------------------------------\n\n")
>>
>> LE <- data.frame (
>>   want = c ("0d,00", "0a,00"),
>>   got  = c ("0d   ", "0a,00")
>> )
>>
>> BE <- data.frame (
>>   want = c ("00,0d", "00,0a"),
>>   got  = c ("00,0d", "   0a")
>> )
>>
>> write.csv (LE, "R_LE.csv", fileEncoding = "UTF-16LE", row.names = FALSE)
>> write.csv (BE, "R_BE.csv", fileEncoding = "UTF-16BE", row.names = FALSE)
>>
>> print (readBin ("R_LE.csv", "raw", 1000))
>> print (LE)
>> cat ("\n")
>>
>> print (readBin ("R_BE.csv", "raw", 1000))
>> print (BE)
>> cat ("\n")
>>
>> try (iconv ("\n", to = "UTF-8"))
>>
>> try (iconv ("\n", to = "UTF-16LE"))
>> try (iconv ("\n", to = "UTF-16BE"))
>> try (iconv ("\n", to = "UTF-16"))
>>
>> try (iconv ("\n", to = "UTF-32LE"))
>> try (iconv ("\n", to = "UTF-32BE"))
>> try (iconv ("\n", to = "UTF-32"))
>> ----------------------------------------------------------------------------
>> ------------------------
>>
>> TEST SCRIPT OUTPUT
>>
>>> source ("bug_encoding.R")
>> R version 3.4.0 (2017-04-21)
>> Platform: x86_64-w64-mingw32/x64 (64-bit)
>> Running under: Windows 10 x64 (build 14393)
>>
>> Matrix products: default
>>
>> locale:
>> [1] LC_COLLATE=English_Australia.1252  LC_CTYPE=English_Australia.1252
>> [3] LC_MONETARY=English_Australia.1252 LC_NUMERIC=C
>> [5] LC_TIME=English_Australia.1252
>>
>> attached base packages:
>> [1] stats     graphics  grDevices utils     datasets  methods   base
>>
>> loaded via a namespace (and not attached):
>> [1] compiler_3.4.0
>> ---------------------------------
>>
>>  [1] 22 00 77 00 61 00 6e 00 74 00 22 00 2c 00 22 00 67 00 6f 00 74 00 22 00
>> 0d
>> [26] 0a 00 22 00 30 00 64 00 2c 00 30 00 30 00 22 00 2c 00 22 00 30 00 64 00
>> 20
>> [51] 00 20 00 20 00 22 00 0d 0a 00 22 00 30 00 61 00 2c 00 30 00 30 00 22 00
>> 2c
>> [76] 00 22 00 30 00 61 00 2c 00 30 00 30 00 22 00 0d 0a 00
>>    want   got
>> 1 0d,00 0d
>> 2 0a,00 0a,00
>>
>>  [1] 00 22 00 77 00 61 00 6e 00 74 00 22 00 2c 00 22 00 67 00 6f 00 74 00 22
>> 00
>> [26] 0d 0a 00 22 00 30 00 30 00 2c 00 30 00 64 00 22 00 2c 00 22 00 30 00 30
>> 00
>> [51] 2c 00 30 00 64 00 22 00 0d 0a 00 22 00 30 00 30 00 2c 00 30 00 61 00 22
>> 00
>> [76] 2c 00 22 00 20 00 20 00 20 00 30 00 61 00 22 00 0d 0a
>>    want   got
>> 1 00,0d 00,0d
>> 2 00,0a    0a
>>
>> Error in iconv("\n", to = "UTF-16LE") : embedded nul in string: '\n\0'
>> Error in iconv("\n", to = "UTF-16BE") : embedded nul in string: '\0\n'
>> Error in iconv("\n", to = "UTF-16") : embedded nul in string: '??\0\n'
>> Error in iconv("\n", to = "UTF-32LE") :
>>   embedded nul in string: '\n\0\0\0'
>> Error in iconv("\n", to = "UTF-32BE") :
>>   embedded nul in string: '\0\0\0\n'
>> Error in iconv("\n", to = "UTF-32") :
>>   embedded nul in string: '\0\0??\0\0\0\n'
>>>
>> ----------------------------------------------------------------------------
>> ------------------------
>> Cheers -- Jack Kelley
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>


From Jack.Kelley at bigpond.com  Tue May  2 02:49:28 2017
From: Jack.Kelley at bigpond.com (Jack Kelley)
Date: Tue, 2 May 2017 10:49:28 +1000
Subject: [Rd] Any progress on write.csv fileEncoding for UTF-16 and
	UTF-32 ?
In-Reply-To: <9dce0e1b-27d8-60ca-9df3-3227aab0a3f8@gmail.com>
References: <000101d2c143$bd644380$382cca80$@bigpond.com>
 <6dee41ed-b1c1-4d4d-0eb1-03f3e7e0d759@gmail.com>
 <9dce0e1b-27d8-60ca-9df3-3227aab0a3f8@gmail.com>
Message-ID: <000001d2c2dd$f4aa2d00$ddfe8700$@bigpond.com>

Thanks for looking into this.

A few notes regarding all the UTF encodings on Windows 10 ...

The default eol for write.csv (via write.table) is "\n" and always gives
as.raw (c (0x0d, 0x0a)), that is, <Carriage Return> <Line Feed> as adjacent
bytes. This is fine for UTF-8 but wrong for UTF-16 and UTF-32.

EXAMPLE: Using UTF-32 for exaggeration (note also that 3 nul bytes are
missing in the final CR+LF):

df <- data.frame (x = 1:2, y = 3:4)

$`UTF-32LE`$default.eol$raw
 [1] 22 00 00 00 78 00 00 00 22 00 00 00 2c 00 00 00 22 00 00 00 79 00 00 00
22
[26] 00 00 00 0d 0a 00 00 00 31 00 00 00 2c 00 00 00 33 00 00 00 0d 0a 00 00
00
[51] 32 00 00 00 2c 00 00 00 34 00 00 00 0d 0a 00 00 00

$`UTF-32BE`$default.eol$raw
 [1] 00 00 00 22 00 00 00 78 00 00 00 22 00 00 00 2c 00 00 00 22 00 00 00 79
00
[26] 00 00 22 00 00 00 0d 0a 00 00 00 31 00 00 00 2c 00 00 00 33 00 00 00 0d
0a
[51] 00 00 00 32 00 00 00 2c 00 00 00 34 00 00 00 0d 0a

(Nevertheless, Microsoft Excel 2013 tolerates these CSVs!)

One trick/solution is to use eol = "\r" (that is, <Carriage Return> only).

Regards -- Jack Kelley

----------------------------------------------------------------------------
--------

remove (list = objects())
print (sessionInfo())
cat ("##########################################################\n\n")

ENCODING <- c (
  "UTF-8",
  "UTF-16LE", "UTF-16BE", "UTF-16",
  "UTF-32LE", "UTF-32BE", "UTF-32"
)

df <- data.frame (x = 1:2, y = 3:4)

csv <- structure (lapply (ENCODING, function (encoding) {
  csv <- sprintf ("df_%s.csv", encoding)
  write.csv (df, csv, fileEncoding = encoding, row.names = FALSE)
  list (default.eol = list (
    csv = csv, raw = readBin (csv, "raw", 1000))
  )
}), .Names = ENCODING)

EOL <- c (LF = "\n", CR = "\r", "CR+LF" = "\r\n")

CSV <- structure (lapply (ENCODING, function (encoding) {
  structure (
    lapply (names (EOL), function (EOL.name) {
      csv <- sprintf ("df_%s_eol=%s.csv", encoding, EOL.name)
      write.csv (
        df, csv, fileEncoding = encoding, row.names = FALSE,
        eol = EOL [EOL.name]
      )
      list (csv = csv, raw = readBin (csv, "raw", 1000))
  }), .Names = names (EOL))
}), .Names = ENCODING)

print (csv)
print (CSV)

----------------------------------------------------------------------------
----------------

-----Original Message-----
From: Duncan Murdoch [mailto:murdoch.duncan at gmail.com] 
Sent: Tuesday, 2 May 2017 04:22
To: Jack Kelley <Jack.Kelley at bigpond.com>; r-devel at r-project.org
Subject: Re: [Rd] Any progress on write.csv fileEncoding for UTF-16 and
UTF-32 ?

On 30/04/2017 12:23 PM, Duncan Murdoch wrote:
> No, I don't think anyone is working on this.
>
> There's a fairly simple workaround for the UTF-16 and UTF-32 iconv
> issues:  don't attempt to produce character vectors, produce raw vectors
> instead. (The "toRaw" argument to iconv() asks for this.) Raw vectors
> can contain embedded nulls.  Character vectors can't, because
> internally, R is using 8 bit C strings, and the nulls are string
> terminators.
>
> I don't know how difficult it would be to fix the write.table problems.

I've now taken a look, and it appears as if it's not too hard.  I'll see 
if I can work out a patch that I trust.

Duncan Murdoch

>
> Duncan Murdoch
>
> On 29/04/2017 7:53 PM, Jack Kelley wrote:
>> "R version 3.4.0 (2017-04-21)"  on "x86_64-w64-mingw32" platform
>> ... [rest omitted]


From Jack.Kelley at bigpond.com  Tue May  2 03:22:48 2017
From: Jack.Kelley at bigpond.com (Jack Kelley)
Date: Tue, 2 May 2017 11:22:48 +1000
Subject: [Rd] Any progress on write.csv fileEncoding for UTF-16 and
	UTF-32 ?
In-Reply-To: <9dce0e1b-27d8-60ca-9df3-3227aab0a3f8@gmail.com>
References: <000101d2c143$bd644380$382cca80$@bigpond.com>
 <6dee41ed-b1c1-4d4d-0eb1-03f3e7e0d759@gmail.com>
 <9dce0e1b-27d8-60ca-9df3-3227aab0a3f8@gmail.com>
Message-ID: <000101d2c2e2$9caff3a0$d60fdae0$@bigpond.com>

Correction to my previous post: Not just the final CR+LF...

Change
EXAMPLE: Using UTF-32 for exaggeration (note also that 3 nul bytes are
missing in the final CR+LF):
to
EXAMPLE: Using UTF-32 for exaggeration (note also that 3 nul bytes are
missing in *each* CR+LF):

-- Jack Kelley


From murdoch.duncan at gmail.com  Tue May  2 10:11:42 2017
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Tue, 2 May 2017 04:11:42 -0400
Subject: [Rd] Any progress on write.csv fileEncoding for UTF-16 and
 UTF-32 ?
In-Reply-To: <000001d2c2dd$f4aa2d00$ddfe8700$@bigpond.com>
References: <000101d2c143$bd644380$382cca80$@bigpond.com>
 <6dee41ed-b1c1-4d4d-0eb1-03f3e7e0d759@gmail.com>
 <9dce0e1b-27d8-60ca-9df3-3227aab0a3f8@gmail.com>
 <000001d2c2dd$f4aa2d00$ddfe8700$@bigpond.com>
Message-ID: <ee4a023a-2f5c-0c48-345a-fbcc20bee980@gmail.com>

On 01/05/2017 8:49 PM, Jack Kelley wrote:
> Thanks for looking into this.
>
> A few notes regarding all the UTF encodings on Windows 10 ...

This all stems from the ancient bad decision by Microsoft to translate 
LF characters to CR LF when writing text files.  R passes 0A or 0A 00 or 
0A 00 00 00 to the output routine (part of the C run-time), and it needs 
to figure out how many characters there are in those bytes in order to 
add the appropriate CR with the right width.

The default is 8 bit, so you get 0D 0A in current versions of R, 
regardless of the encoding.

There are ways to declare UTF-16LE (see 
https://msdn.microsoft.com/en-us/library/yeby3zcb.aspx, or Google 
"Windows fopen" if that moves), but no other wide encoding.  That's what 
I'm putting in place if you ask for UTF-16LE or UCS-2LE.  So far I'm not 
planning to handle UTF-16BE or UTF-32, because doing those would mean R 
would have to handle the translation of LF itself, and I'm too lazy to 
do that.

So far this is working for writes, but not reads.  I still have to track 
down what's going wrong there.

Duncan Murdoch

>
> The default eol for write.csv (via write.table) is "\n" and always gives
> as.raw (c (0x0d, 0x0a)), that is, <Carriage Return> <Line Feed> as adjacent
> bytes. This is fine for UTF-8 but wrong for UTF-16 and UTF-32.
>
> EXAMPLE: Using UTF-32 for exaggeration (note also that 3 nul bytes are
> missing in the final CR+LF):
>
> df <- data.frame (x = 1:2, y = 3:4)
>
> $`UTF-32LE`$default.eol$raw
>  [1] 22 00 00 00 78 00 00 00 22 00 00 00 2c 00 00 00 22 00 00 00 79 00 00 00
> 22
> [26] 00 00 00 0d 0a 00 00 00 31 00 00 00 2c 00 00 00 33 00 00 00 0d 0a 00 00
> 00
> [51] 32 00 00 00 2c 00 00 00 34 00 00 00 0d 0a 00 00 00
>
> $`UTF-32BE`$default.eol$raw
>  [1] 00 00 00 22 00 00 00 78 00 00 00 22 00 00 00 2c 00 00 00 22 00 00 00 79
> 00
> [26] 00 00 22 00 00 00 0d 0a 00 00 00 31 00 00 00 2c 00 00 00 33 00 00 00 0d
> 0a
> [51] 00 00 00 32 00 00 00 2c 00 00 00 34 00 00 00 0d 0a
>
> (Nevertheless, Microsoft Excel 2013 tolerates these CSVs!)
>
> One trick/solution is to use eol = "\r" (that is, <Carriage Return> only).
>
> Regards -- Jack Kelley
>
> ----------------------------------------------------------------------------
> --------
>
> remove (list = objects())
> print (sessionInfo())
> cat ("##########################################################\n\n")
>
> ENCODING <- c (
>   "UTF-8",
>   "UTF-16LE", "UTF-16BE", "UTF-16",
>   "UTF-32LE", "UTF-32BE", "UTF-32"
> )
>
> df <- data.frame (x = 1:2, y = 3:4)
>
> csv <- structure (lapply (ENCODING, function (encoding) {
>   csv <- sprintf ("df_%s.csv", encoding)
>   write.csv (df, csv, fileEncoding = encoding, row.names = FALSE)
>   list (default.eol = list (
>     csv = csv, raw = readBin (csv, "raw", 1000))
>   )
> }), .Names = ENCODING)
>
> EOL <- c (LF = "\n", CR = "\r", "CR+LF" = "\r\n")
>
> CSV <- structure (lapply (ENCODING, function (encoding) {
>   structure (
>     lapply (names (EOL), function (EOL.name) {
>       csv <- sprintf ("df_%s_eol=%s.csv", encoding, EOL.name)
>       write.csv (
>         df, csv, fileEncoding = encoding, row.names = FALSE,
>         eol = EOL [EOL.name]
>       )
>       list (csv = csv, raw = readBin (csv, "raw", 1000))
>   }), .Names = names (EOL))
> }), .Names = ENCODING)
>
> print (csv)
> print (CSV)
>
> ----------------------------------------------------------------------------
> ----------------
>
> -----Original Message-----
> From: Duncan Murdoch [mailto:murdoch.duncan at gmail.com]
> Sent: Tuesday, 2 May 2017 04:22
> To: Jack Kelley <Jack.Kelley at bigpond.com>; r-devel at r-project.org
> Subject: Re: [Rd] Any progress on write.csv fileEncoding for UTF-16 and
> UTF-32 ?
>
> On 30/04/2017 12:23 PM, Duncan Murdoch wrote:
>> No, I don't think anyone is working on this.
>>
>> There's a fairly simple workaround for the UTF-16 and UTF-32 iconv
>> issues:  don't attempt to produce character vectors, produce raw vectors
>> instead. (The "toRaw" argument to iconv() asks for this.) Raw vectors
>> can contain embedded nulls.  Character vectors can't, because
>> internally, R is using 8 bit C strings, and the nulls are string
>> terminators.
>>
>> I don't know how difficult it would be to fix the write.table problems.
>
> I've now taken a look, and it appears as if it's not too hard.  I'll see
> if I can work out a patch that I trust.
>
> Duncan Murdoch
>
>>
>> Duncan Murdoch
>>
>> On 29/04/2017 7:53 PM, Jack Kelley wrote:
>>> "R version 3.4.0 (2017-04-21)"  on "x86_64-w64-mingw32" platform
>>> ... [rest omitted]
>
>


From tomas.kalibera at gmail.com  Tue May  2 13:08:48 2017
From: tomas.kalibera at gmail.com (Tomas Kalibera)
Date: Tue, 2 May 2017 13:08:48 +0200
Subject: [Rd] Byte compilation with window<- in R3.4.0
In-Reply-To: <590BAC44-18AD-46D8-847D-B328E51DC84D@gmail.com>
References: <590BAC44-18AD-46D8-847D-B328E51DC84D@gmail.com>
Message-ID: <1f7ae57a-0f05-e568-f208-2b4545976ff5@gmail.com>

Thanks for the report, fixed in R-devel and R-patched.

Best
Tomas

On 04/30/2017 10:48 PM, Christoph Sax wrote:
> Hi,
>
> I am running into a problem when I use the window<- replacement function in R
> 3.4.0. It will lead to an error when it is called inside a loop, probably
> the result of the byte compiler now enabled by default.
>
> When I turn it off, it works again, as in older versions of R. I tested on Win,
> Linux and Mac, and the problem occurs everywhere.
>
> Here is a reproducible example:
>
> z <- AirPassengers
>
> # this works
> window(z, start =  c(1955, 1), end =  c(1955, 1)) <- NA
>
> # but this does not
> for (i in 1) {
>   window(z, start =  c(1955, 1), end =  c(1955, 1)) <- NA
> }
> # Error in stats::window(x = `*tmp*`, start = c(1955, 1), end = c(1955,  :
> #   object '*tmp*' not found
>
> # turning off the byte compiler makes it working again (as in older R versions)
> compiler::enableJIT(0)
> for (i in 1) {
>   window(z, start =  c(1955, 1), end =  c(1955, 1)) <- NA
> }
>
> Any help is very much appreciated.
>
> Thanks,
> Christoph
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From y_alperovych at yahoo.fr  Tue May  2 13:12:43 2017
From: y_alperovych at yahoo.fr (Yan Alperovych)
Date: Tue, 2 May 2017 11:12:43 +0000 (UTC)
Subject: [Rd] R 3.4 and mclapply assertion failure - is this a bug?
References: <1190710968.753603.1493723563590.ref@mail.yahoo.com>
Message-ID: <1190710968.753603.1493723563590@mail.yahoo.com>

Dear all,?
I am not sure if this is a bug, so I prefer to post it here before filing.
After upgrading to 3.4 I encounter the following message with mclapply:
Assertion failure at kmp_runtime.cpp(6480): __kmp_thread_pool == __null.OMP: Error #13: Assertion failure at kmp_runtime.cpp(6480).OMP: Hint: Please submit a bug report with this message, compile and run commands used, and machine configuration info including native compiler and operating system versions. Faster response will be obtained by including all program sources. For information on submitting this issue, please see?http://www.intel.com/software/products/support/.
The complete pdf of what console produces is here:https://www.dropbox.com/s/navugg28gqne14p/R%20Console.pdf?dl=0
The batch with commands to (hopefully) reproduce it is here:https://www.dropbox.com/s/8pd0ij3kowigdlc/20170430%20-%20R%20mclapply%20bug%20report.r?dl=0
The data for the reproducible example is here:
https://www.dropbox.com/s/2enoeapu7jgcxwd/z.Rdata?dl=0
Note that this behavior was not present in R 3.3.3 on the same machine and all the batch was working without any errors.
The sessionInfo():
R version 3.4.0 (2017-04-21)Platform: x86_64-apple-darwin15.6.0 (64-bit)Running under: macOS Sierra 10.12.4
Matrix products: defaultBLAS: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRblas.0.dylibLAPACK: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRlapack.dylib
locale:[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
attached base packages:[1] parallel ?compiler ?stats ? ? graphics ?grDevices utils ? ? datasets ?methods ? base ? ??
other attached packages:?[1] data.table_1.10.4 ? ? ?numbers_0.6-6 ? ? ? ? ?microbenchmark_1.4-2.1 zoo_1.8-0 ? ? ? ? ? ? ?doParallel_1.0.10 ? ? ?iterators_1.0.8 ? ? ???[7] foreach_1.4.3 ? ? ? ? ?RSclient_0.7-3 ? ? ? ? stringi_1.1.5 ? ? ? ? ?stringr_1.2.0 ? ? ? ? ?lubridate_1.6.0 ? ? ? ?plyr_1.8.4 ? ? ? ? ? ?
loaded via a namespace (and not attached):?[1] Rcpp_0.12.10 ? ? lattice_0.20-35 ?codetools_0.2-15 grid_3.4.0 ? ? ? gtable_0.2.0 ? ? magrittr_1.5 ? ? scales_0.4.1 ? ? ggplot2_2.2.1 ???[9] lazyeval_0.2.0 ? tools_3.4.0 ? ? ?munsell_0.4.3 ? ?colorspace_1.3-2 tibble_1.3.0 ? ?

Thank you in advance for help/hints,?Yan
	[[alternative HTML version deleted]]


From simon.urbanek at r-project.org  Tue May  2 19:09:36 2017
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Tue, 2 May 2017 13:09:36 -0400
Subject: [Rd] R 3.4 and mclapply assertion failure - is this a bug?
In-Reply-To: <1190710968.753603.1493723563590@mail.yahoo.com>
References: <1190710968.753603.1493723563590.ref@mail.yahoo.com>
 <1190710968.753603.1493723563590@mail.yahoo.com>
Message-ID: <8315FA34-455A-41E3-92E6-5B6AB715AD59@r-project.org>

Yan,

this looks like an interaction of OpenMP in data.table and forking - you may want to report that to Matt as he can shed more light on that. He has been complaining that we didn't enable OpenMP before 3.4 so he probably knows about the issue and how to fix it.

Cheers,
Simon


> On May 2, 2017, at 7:12 AM, Yan Alperovych via R-devel <r-devel at r-project.org> wrote:
> 
> Dear all, 
> I am not sure if this is a bug, so I prefer to post it here before filing.
> After upgrading to 3.4 I encounter the following message with mclapply:
> Assertion failure at kmp_runtime.cpp(6480): __kmp_thread_pool == __null.OMP: Error #13: Assertion failure at kmp_runtime.cpp(6480).OMP: Hint: Please submit a bug report with this message, compile and run commands used, and machine configuration info including native compiler and operating system versions. Faster response will be obtained by including all program sources. For information on submitting this issue, please see http://www.intel.com/software/products/support/.
> The complete pdf of what console produces is here:https://www.dropbox.com/s/navugg28gqne14p/R%20Console.pdf?dl=0
> The batch with commands to (hopefully) reproduce it is here:https://www.dropbox.com/s/8pd0ij3kowigdlc/20170430%20-%20R%20mclapply%20bug%20report.r?dl=0
> The data for the reproducible example is here:
> https://www.dropbox.com/s/2enoeapu7jgcxwd/z.Rdata?dl=0
> Note that this behavior was not present in R 3.3.3 on the same machine and all the batch was working without any errors.
> The sessionInfo():
> R version 3.4.0 (2017-04-21)Platform: x86_64-apple-darwin15.6.0 (64-bit)Running under: macOS Sierra 10.12.4
> Matrix products: defaultBLAS: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRblas.0.dylibLAPACK: /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRlapack.dylib
> locale:[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
> attached base packages:[1] parallel  compiler  stats     graphics  grDevices utils     datasets  methods   base     
> other attached packages: [1] data.table_1.10.4      numbers_0.6-6          microbenchmark_1.4-2.1 zoo_1.8-0              doParallel_1.0.10      iterators_1.0.8        [7] foreach_1.4.3          RSclient_0.7-3         stringi_1.1.5          stringr_1.2.0          lubridate_1.6.0        plyr_1.8.4            
> loaded via a namespace (and not attached): [1] Rcpp_0.12.10     lattice_0.20-35  codetools_0.2-15 grid_3.4.0       gtable_0.2.0     magrittr_1.5     scales_0.4.1     ggplot2_2.2.1    [9] lazyeval_0.2.0   tools_3.4.0      munsell_0.4.3    colorspace_1.3-2 tibble_1.3.0    
> 
> Thank you in advance for help/hints, Yan
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From alexandre.courtiol at gmail.com  Wed May  3 00:10:56 2017
From: alexandre.courtiol at gmail.com (Alexandre Courtiol)
Date: Wed, 3 May 2017 00:10:56 +0200
Subject: [Rd] potential bug in simulate.lm when gaussian(link != "identity")
Message-ID: <CAERMt4e91vsVzOy_UGm9O0fOhvCOh0uk4LGivx4KAU+9Z8nLgA@mail.gmail.com>

Dear all,

I think that there is a bug in the function simulate.lm() when called upon
a glm fitted with gaussian family with a link other than identity. The
variance of the simulated response is clearly off and an inspection to the
code of simulate.lm reveals that it is because the variance is divided by
the model weights (precisely, not the prior ones), which is not documented.

Can somebody file this bug for me (if you agree that this is a bug)?
Many thanks.
Alex

Simple demonstration:

set.seed(1L)
y <- 10 + rnorm(n = 100)
mean(y) ##  10.10889
var(y)  ##   0.8067621

mod_glm1  <- glm(y ~ 1, family = gaussian())
new.y1 <- simulate(mod_glm1)[, 1]
mean(new.y1) ## 10.07493
var(new.y1)  ##  0.7402303

mod_glm2  <- glm(y ~ 1, family = gaussian(link = "log"))
new.y2 <- simulate(mod_glm2)[, 1]
mean(new.y2) ## 10.11152
var(new.y2)  ##  0.008445062  ##### WRONG #####

mod_glm3 <- mod_glm2
mod_glm3$weights <- NULL
new.y3 <- simulate(mod_glm3)[, 1]
mean(new.y3) ## 10.15524
var(new.y3)  ##  0.7933856  ##### OK(?) #####


### my session ###

> sessionInfo()
R version 3.4.0 (2017-04-21)
Platform: x86_64-apple-darwin16.5.0 (64-bit)
Running under: macOS Sierra 10.12.4

Matrix products: default
BLAS:
/System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib
LAPACK:
/System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libLAPACK.dylib

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

loaded via a namespace (and not attached):
[1] compiler_3.4.0 tools_3.4.0

-- 
Alexandre Courtiol

http://sites.google.com/site/alexandrecourtiol/home

*"Science is the belief in the ignorance of experts"*, R. Feynman

	[[alternative HTML version deleted]]


From mattjdowle at gmail.com  Wed May  3 01:30:04 2017
From: mattjdowle at gmail.com (Matt Dowle)
Date: Tue, 2 May 2017 16:30:04 -0700
Subject: [Rd] R 3.4 and mclapply assertion failure - is this a bug?
Message-ID: <CAOuOy3evCW4ZUThh3So3RKZQFR8zQSn7mRGhDCcXoQhuL6KWvA@mail.gmail.com>

That's great news that OpenMP is now enabled in MacOS CRAN binaries! Thanks
Simon.

data.table is supposed to automatically switch down to single-threaded mode
when explicitly parallelized via the fork mechanism.   It does that via
functions registered in init.c as follows :

    pthread_atfork(&when_fork, &when_fork_end, NULL)

Tests 1705 and 1706 test this is working.   Those tests are passing ok on
CRAN currently including x86_64-apple-darwin15.6.0.

I'll work with Yan off list to see what the difference is.  Either the fork
catch isn't working in that environment for some reason, the use case is
different to the tests somehow or the fork catch is working but something
somewhere is asserting its surprise at being single threaded.  In the
meantime setDTthreads(1) can be called manually before the explicit
parallelism as a workaround.

Matt

On Tue, May 2, 2017 at 1:25 PM, Yan Alperovych <y_alperovych at yahoo.fr>
wrote:

> Dear Matt,
>
> I follow the Simon?s advice on this. Since R 3.4 the mclapply is producing
> a strange error which was not present in R 3.3.3. Simon?s answer is below
> and the relevant info (the console output, the code, and the sample data)
> are all below.
>
> Thank you very much for helping,
> Yan
>
> Begin forwarded message:
>
> *From: *Simon Urbanek <simon.urbanek at r-project.org>
> *Subject: **Re: [Rd] R 3.4 and mclapply assertion failure - is this a
> bug?*
> *Date: *May 2, 2017 at 7:09:36 PM GMT+2
> *To: *Yan Alperovych <y_alperovych at yahoo.fr>
> *Cc: *"r-devel at r-project.org" <r-devel at r-project.org>
>
> Yan,
>
> this looks like an interaction of OpenMP in data.table and forking - you
> may want to report that to Matt as he can shed more light on that. He has
> been complaining that we didn't enable OpenMP before 3.4 so he probably
> knows about the issue and how to fix it.
>
> Cheers,
> Simon
>
>
> On May 2, 2017, at 7:12 AM, Yan Alperovych via R-devel <
> r-devel at r-project.org> wrote:
>
> Dear all,
>
>
> I am not sure if this is a bug, so I prefer to post it here before filing.
>
>
> After upgrading to 3.4 I encounter the following message with mclapply:
>
>
> Assertion failure at kmp_runtime.cpp(6480): __kmp_thread_pool ==
> __null.OMP : Error #13: Assertion failure at kmp_runtime.cpp(6480).OMP:
> Hint: Please submit a bug report with this message, compile and run
> commands used, and machine configuration info including native compiler and
> operating system versions. Faster response will be obtained by including
> all program sources. For information on submitting this issue, please see
> http://www.intel.com/software/products/support/.
>
>
> The complete pdf of what console produces is here:
>
> https://www.dropbox.com/s/navugg28gqne14p/R%20Console.pdf?dl=0
>
>
> The batch with commands to (hopefully) reproduce it is here:
>
> https://www.dropbox.com/s/8pd0ij3kowigdlc/20170430%20-%
> 20R%20mclapply%20bug%20report.r?dl=0
>
>
> The data for the reproducible example is here:
> https://www.dropbox.com/s/2enoeapu7jgcxwd/z.Rdata?dl=0
>
>
> Note that this behavior was not present in R 3.3.3 on the same machine and
> all the batch was working without any errors.
>
>
> The sessionInfo():
> R version 3.4.0 (2017-04-21)
>
> Platform: x86_64-apple-darwin15.6.0 (64-bit)
>
> Running under: macOS Sierra 10.12.4
>
>
> Matrix products: default
>
>
> BLAS: /Library/Frameworks/R.framework/Versions/3.4/
> Resources/lib/libRblas.0.dylib
>
> LAPACK: /Library/Frameworks/R.framework/Versions/3.4/
> Resources/lib/libRlapack.dylib
>
>
> *locale:*
> *[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8*
>
> *attached base packages:*
> *[1] parallel  compiler  stats     graphics  grDevices utils     datasets
>  methods   base     *
>
> *other attached packages:*
> * [1] data.table_1.10.4      numbers_0.6-6          microbenchmark_1.4-2.1
> zoo_1.8-0              doParallel_1.0.10      iterators_1.0.8       *
> * [7] foreach_1.4.3          RSclient_0.7-3         stringi_1.1.5
>  stringr_1.2.0          lubridate_1.6.0        plyr_1.8.4            *
>
> *loaded via a namespace (and not attached):*
> * [1] Rcpp_0.12.10     lattice_0.20-35  codetools_0.2-15 grid_3.4.0
> gtable_0.2.0     magrittr_1.5     scales_0.4.1     ggplot2_2.2.1   *
> * [9] lazyeval_0.2.0   tools_3.4.0      munsell_0.4.3    colorspace_1.3-2
> tibble_1.3.0    *
>
>
>
> Thank you in advance for help/hints, Yan
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>
>
>

	[[alternative HTML version deleted]]


From hpages at fredhutch.org  Wed May  3 02:50:12 2017
From: hpages at fredhutch.org (=?UTF-8?B?SGVydsOpIFBhZ8Oocw==?=)
Date: Tue, 2 May 2017 17:50:12 -0700
Subject: [Rd] stopifnot() does not stop at first non-TRUE argument
Message-ID: <64acdaeb-2857-9017-68c9-9da08e29a617@fredhutch.org>

Hi,

It's surprising that stopifnot() keeps evaluating its arguments after
it reaches the first one that is not TRUE:

   > stopifnot(3 == 5, as.integer(2^32), a <- 12)
   Error: 3 == 5 is not TRUE
   In addition: Warning message:
   In stopifnot(3 == 5, as.integer(2^32), a <- 12) :
     NAs introduced by coercion to integer range
   > a
   [1] 12

The details section in its man page actually suggests that it should
stop at the first non-TRUE argument:

   ?stopifnot(A, B)? is conceptually equivalent to

    { if(any(is.na(A)) || !all(A)) stop(...);
      if(any(is.na(B)) || !all(B)) stop(...) }

Best,
H.

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fredhutch.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From pdalgd at gmail.com  Wed May  3 11:26:34 2017
From: pdalgd at gmail.com (peter dalgaard)
Date: Wed, 3 May 2017 11:26:34 +0200
Subject: [Rd] stopifnot() does not stop at first non-TRUE argument
In-Reply-To: <64acdaeb-2857-9017-68c9-9da08e29a617@fredhutch.org>
References: <64acdaeb-2857-9017-68c9-9da08e29a617@fredhutch.org>
Message-ID: <E5DADF15-58C6-4F73-BC56-C6404896982F@gmail.com>

The first line of stopifnot is

    n <- length(ll <- list(...))

which takes ALL arguments and forms a list of them. This implies evaluation, so explains the effect that you see.

To do it differently, you would have to do something like 

   dots <- match.call(expand.dots=FALSE)$...

and then explicitly evaluate each argument in turn in the caller frame. This amount of nonstandard evaluation sounds like it would incur a performance penalty, which could be undesirable.

If you want to enforce the order of evaluation, there is always

   stopifnot(A)
   stopifnot(B)

-pd

> On 3 May 2017, at 02:50 , Herv? Pag?s <hpages at fredhutch.org> wrote:
> 
> Hi,
> 
> It's surprising that stopifnot() keeps evaluating its arguments after
> it reaches the first one that is not TRUE:
> 
>  > stopifnot(3 == 5, as.integer(2^32), a <- 12)
>  Error: 3 == 5 is not TRUE
>  In addition: Warning message:
>  In stopifnot(3 == 5, as.integer(2^32), a <- 12) :
>    NAs introduced by coercion to integer range
>  > a
>  [1] 12
> 
> The details section in its man page actually suggests that it should
> stop at the first non-TRUE argument:
> 
>  ?stopifnot(A, B)? is conceptually equivalent to
> 
>   { if(any(is.na(A)) || !all(A)) stop(...);
>     if(any(is.na(B)) || !all(B)) stop(...) }
> 
> Best,
> H.
> 
> -- 
> Herv? Pag?s
> 
> Program in Computational Biology
> Division of Public Health Sciences
> Fred Hutchinson Cancer Research Center
> 1100 Fairview Ave. N, M1-B514
> P.O. Box 19024
> Seattle, WA 98109-1024
> 
> E-mail: hpages at fredhutch.org
> Phone:  (206) 667-5791
> Fax:    (206) 667-1319
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From infinity0 at pwned.gg  Wed May  3 15:10:00 2017
From: infinity0 at pwned.gg (Ximin Luo)
Date: Wed, 03 May 2017 13:10:00 +0000
Subject: [Rd] Generate reproducible output independently of the build
	path
In-Reply-To: <c71a9aa3-7eef-a75e-550b-8447e8bfba61@pwned.gg>
References: <c71a9aa3-7eef-a75e-550b-8447e8bfba61@pwned.gg>
Message-ID: <34a7418c-bd5d-dee4-7e56-480575452bdc@pwned.gg>

Ximin Luo:
> [..]
> 
> I've attached a patch (applies to both 3.3.3 and 3.4) that fixes this issue; however I know it's not perfect and would welcome feedback on how to make it acceptable to the R project.
> 

Hi all, attached is an updated version of the patch.

We've tested this on our jenkins infrastructure and it makes 463/478 Debian R packages reproducible:

https://tests.reproducible-builds.org/debian/issues/unstable/randomness_in_r_rdb_rds_databases_issue.html

The previous version of the patch was slightly flawed, it made 2 of these packages fail-to-build-from-source (r-bioc-biobase, r-cran-shinybs). This is fixed in the current patch attached, and these packages reproduce with it.

The remaining FTBFS (r-cran-randomfields) are due to incompatibilities between r-base 3.3.3 and 3.4.0, being discussed in Debian bug 861333, and are not caused by this patch.

The remaining 14 unreproducible packages are likely unreproducible due to issues specifically in those packages. For example r-cran-runit-0.4.31/man/checkFuncs.Rd contains an explicit absolute path, and making this relative fixes the unreproducibility. I have not yet checked the other packages.

> For example, I've tried to limit the effects of the patch only to the RDB loading/saving code, but I'm not familiar with the codebase so it would be good if someone could verify that I did this correctly. Then, ideally we would also add some tests to ensure that unreproduciblity does not crop back in "by accident". R code heavily relies on absolute paths, and I went down several dead-ends chasing and editing variables containing absolute paths, before I finally managed to get this working patch, so I suspect that without specific reproducibility tests, this issue might recur in the future.
> 

I've been talking with Dirk Eddelbuettel off-thread and he suggested that the rest of the patch could also be guarded by something like getOption("useRelativePath", bool).

It would be good if other members of R Core could comment and give me some more guidance along these lines. :)

> I've checked that the existing tests still pass, with this patch applied to the Debian package. I have some errors like:
> [..]
> :* checking whether the package can be loaded ... ERROR
> [..]

We also figured out that this was a previous issue with Debian R 3.3.3, the error goes away with 3.4.0 either patched or unpatched.

X

-- 
GPG: ed25519/56034877E1F87C35
GPG: rsa4096/1318EFAC5FBBDBCE
https://github.com/infinity0/pubkeys.git
-------------- next part --------------
A non-text attachment was scrubbed...
Name: r-base_reproducible-build-paths.patch
Type: text/x-diff
Size: 2487 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20170503/3e8b53c6/attachment.bin>

From murdoch.duncan at gmail.com  Wed May  3 19:21:58 2017
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Wed, 3 May 2017 13:21:58 -0400
Subject: [Rd] Any progress on write.csv fileEncoding for UTF-16 and
 UTF-32 ?
In-Reply-To: <ee4a023a-2f5c-0c48-345a-fbcc20bee980@gmail.com>
References: <000101d2c143$bd644380$382cca80$@bigpond.com>
 <6dee41ed-b1c1-4d4d-0eb1-03f3e7e0d759@gmail.com>
 <9dce0e1b-27d8-60ca-9df3-3227aab0a3f8@gmail.com>
 <000001d2c2dd$f4aa2d00$ddfe8700$@bigpond.com>
 <ee4a023a-2f5c-0c48-345a-fbcc20bee980@gmail.com>
Message-ID: <b8eaa2ab-6630-cf62-d9ef-fbb33422b244@gmail.com>

Now fixed in R-devel revision 72650.

Duncan Murdoch

On 02/05/2017 4:11 AM, Duncan Murdoch wrote:
> On 01/05/2017 8:49 PM, Jack Kelley wrote:
>> Thanks for looking into this.
>>
>> A few notes regarding all the UTF encodings on Windows 10 ...
>
> This all stems from the ancient bad decision by Microsoft to translate
> LF characters to CR LF when writing text files.  R passes 0A or 0A 00 or
> 0A 00 00 00 to the output routine (part of the C run-time), and it needs
> to figure out how many characters there are in those bytes in order to
> add the appropriate CR with the right width.
>
> The default is 8 bit, so you get 0D 0A in current versions of R,
> regardless of the encoding.
>
> There are ways to declare UTF-16LE (see
> https://msdn.microsoft.com/en-us/library/yeby3zcb.aspx, or Google
> "Windows fopen" if that moves), but no other wide encoding.  That's what
> I'm putting in place if you ask for UTF-16LE or UCS-2LE.  So far I'm not
> planning to handle UTF-16BE or UTF-32, because doing those would mean R
> would have to handle the translation of LF itself, and I'm too lazy to
> do that.
>
> So far this is working for writes, but not reads.  I still have to track
> down what's going wrong there.
>
> Duncan Murdoch
>
>>
>> The default eol for write.csv (via write.table) is "\n" and always gives
>> as.raw (c (0x0d, 0x0a)), that is, <Carriage Return> <Line Feed> as adjacent
>> bytes. This is fine for UTF-8 but wrong for UTF-16 and UTF-32.
>>
>> EXAMPLE: Using UTF-32 for exaggeration (note also that 3 nul bytes are
>> missing in the final CR+LF):
>>
>> df <- data.frame (x = 1:2, y = 3:4)
>>
>> $`UTF-32LE`$default.eol$raw
>>  [1] 22 00 00 00 78 00 00 00 22 00 00 00 2c 00 00 00 22 00 00 00 79 00 00 00
>> 22
>> [26] 00 00 00 0d 0a 00 00 00 31 00 00 00 2c 00 00 00 33 00 00 00 0d 0a 00 00
>> 00
>> [51] 32 00 00 00 2c 00 00 00 34 00 00 00 0d 0a 00 00 00
>>
>> $`UTF-32BE`$default.eol$raw
>>  [1] 00 00 00 22 00 00 00 78 00 00 00 22 00 00 00 2c 00 00 00 22 00 00 00 79
>> 00
>> [26] 00 00 22 00 00 00 0d 0a 00 00 00 31 00 00 00 2c 00 00 00 33 00 00 00 0d
>> 0a
>> [51] 00 00 00 32 00 00 00 2c 00 00 00 34 00 00 00 0d 0a
>>
>> (Nevertheless, Microsoft Excel 2013 tolerates these CSVs!)
>>
>> One trick/solution is to use eol = "\r" (that is, <Carriage Return> only).
>>
>> Regards -- Jack Kelley
>>
>> ----------------------------------------------------------------------------
>> --------
>>
>> remove (list = objects())
>> print (sessionInfo())
>> cat ("##########################################################\n\n")
>>
>> ENCODING <- c (
>>   "UTF-8",
>>   "UTF-16LE", "UTF-16BE", "UTF-16",
>>   "UTF-32LE", "UTF-32BE", "UTF-32"
>> )
>>
>> df <- data.frame (x = 1:2, y = 3:4)
>>
>> csv <- structure (lapply (ENCODING, function (encoding) {
>>   csv <- sprintf ("df_%s.csv", encoding)
>>   write.csv (df, csv, fileEncoding = encoding, row.names = FALSE)
>>   list (default.eol = list (
>>     csv = csv, raw = readBin (csv, "raw", 1000))
>>   )
>> }), .Names = ENCODING)
>>
>> EOL <- c (LF = "\n", CR = "\r", "CR+LF" = "\r\n")
>>
>> CSV <- structure (lapply (ENCODING, function (encoding) {
>>   structure (
>>     lapply (names (EOL), function (EOL.name) {
>>       csv <- sprintf ("df_%s_eol=%s.csv", encoding, EOL.name)
>>       write.csv (
>>         df, csv, fileEncoding = encoding, row.names = FALSE,
>>         eol = EOL [EOL.name]
>>       )
>>       list (csv = csv, raw = readBin (csv, "raw", 1000))
>>   }), .Names = names (EOL))
>> }), .Names = ENCODING)
>>
>> print (csv)
>> print (CSV)
>>
>> ----------------------------------------------------------------------------
>> ----------------
>>
>> -----Original Message-----
>> From: Duncan Murdoch [mailto:murdoch.duncan at gmail.com]
>> Sent: Tuesday, 2 May 2017 04:22
>> To: Jack Kelley <Jack.Kelley at bigpond.com>; r-devel at r-project.org
>> Subject: Re: [Rd] Any progress on write.csv fileEncoding for UTF-16 and
>> UTF-32 ?
>>
>> On 30/04/2017 12:23 PM, Duncan Murdoch wrote:
>>> No, I don't think anyone is working on this.
>>>
>>> There's a fairly simple workaround for the UTF-16 and UTF-32 iconv
>>> issues:  don't attempt to produce character vectors, produce raw vectors
>>> instead. (The "toRaw" argument to iconv() asks for this.) Raw vectors
>>> can contain embedded nulls.  Character vectors can't, because
>>> internally, R is using 8 bit C strings, and the nulls are string
>>> terminators.
>>>
>>> I don't know how difficult it would be to fix the write.table problems.
>>
>> I've now taken a look, and it appears as if it's not too hard.  I'll see
>> if I can work out a patch that I trust.
>>
>> Duncan Murdoch
>>
>>>
>>> Duncan Murdoch
>>>
>>> On 29/04/2017 7:53 PM, Jack Kelley wrote:
>>>> "R version 3.4.0 (2017-04-21)"  on "x86_64-w64-mingw32" platform
>>>> ... [rest omitted]
>>
>>
>


From hpages at fredhutch.org  Wed May  3 21:04:57 2017
From: hpages at fredhutch.org (=?UTF-8?B?SGVydsOpIFBhZ8Oocw==?=)
Date: Wed, 3 May 2017 12:04:57 -0700
Subject: [Rd] stopifnot() does not stop at first non-TRUE argument
In-Reply-To: <E5DADF15-58C6-4F73-BC56-C6404896982F@gmail.com>
References: <64acdaeb-2857-9017-68c9-9da08e29a617@fredhutch.org>
 <E5DADF15-58C6-4F73-BC56-C6404896982F@gmail.com>
Message-ID: <c2b3cb22-90bd-5372-e8b9-d1ea8b2ab828@fredhutch.org>

Not sure why the performance penalty of nonstandard evaluation would
be more of a concern here than for something like switch().

If that can't/won't be fixed, what about fixing the man page so it's
in sync with the current behavior?

Thanks,
H.

On 05/03/2017 02:26 AM, peter dalgaard wrote:
> The first line of stopifnot is
>
>     n <- length(ll <- list(...))
>
> which takes ALL arguments and forms a list of them. This implies evaluation, so explains the effect that you see.
>
> To do it differently, you would have to do something like
>
>    dots <- match.call(expand.dots=FALSE)$...
>
> and then explicitly evaluate each argument in turn in the caller frame. This amount of nonstandard evaluation sounds like it would incur a performance penalty, which could be undesirable.
>
> If you want to enforce the order of evaluation, there is always
>
>    stopifnot(A)
>    stopifnot(B)
>
> -pd
>
>> On 3 May 2017, at 02:50 , Herv? Pag?s <hpages at fredhutch.org> wrote:
>>
>> Hi,
>>
>> It's surprising that stopifnot() keeps evaluating its arguments after
>> it reaches the first one that is not TRUE:
>>
>>  > stopifnot(3 == 5, as.integer(2^32), a <- 12)
>>  Error: 3 == 5 is not TRUE
>>  In addition: Warning message:
>>  In stopifnot(3 == 5, as.integer(2^32), a <- 12) :
>>    NAs introduced by coercion to integer range
>>  > a
>>  [1] 12
>>
>> The details section in its man page actually suggests that it should
>> stop at the first non-TRUE argument:
>>
>>  ?stopifnot(A, B)? is conceptually equivalent to
>>
>>   { if(any(is.na(A)) || !all(A)) stop(...);
>>     if(any(is.na(B)) || !all(B)) stop(...) }
>>
>> Best,
>> H.
>>
>> --
>> Herv? Pag?s
>>
>> Program in Computational Biology
>> Division of Public Health Sciences
>> Fred Hutchinson Cancer Research Center
>> 1100 Fairview Ave. N, M1-B514
>> P.O. Box 19024
>> Seattle, WA 98109-1024
>>
>> E-mail: hpages at fredhutch.org
>> Phone:  (206) 667-5791
>> Fax:    (206) 667-1319
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Ddevel&d=DwIFaQ&c=eRAMFD45gAfqt84VtBcfhQ&r=BK7q3XeAvimeWdGbWY_wJYbW0WYiZvSXAJJKaaPhzWA&m=JwgKhKD2k-9Kedeh6pqu-A8x6UEV0INrcxcSGVGo3Tg&s=f7IKJIhpRNJMC3rZAkuI6-MTdL3GAKSV2wK0boFN5HY&e=
>

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fredhutch.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From hpages at fredhutch.org  Wed May  3 21:08:26 2017
From: hpages at fredhutch.org (=?UTF-8?B?SGVydsOpIFBhZ8Oocw==?=)
Date: Wed, 3 May 2017 12:08:26 -0700
Subject: [Rd] stopifnot() does not stop at first non-TRUE argument
In-Reply-To: <c2b3cb22-90bd-5372-e8b9-d1ea8b2ab828@fredhutch.org>
References: <64acdaeb-2857-9017-68c9-9da08e29a617@fredhutch.org>
 <E5DADF15-58C6-4F73-BC56-C6404896982F@gmail.com>
 <c2b3cb22-90bd-5372-e8b9-d1ea8b2ab828@fredhutch.org>
Message-ID: <8945f7e4-99ce-53fa-46ee-cce64078a6a8@fredhutch.org>

On 05/03/2017 12:04 PM, Herv? Pag?s wrote:
> Not sure why the performance penalty of nonstandard evaluation would
> be more of a concern here than for something like switch().

which is actually a primitive. So it seems that there is at least
another way to go than 'dots <- match.call(expand.dots=FALSE)$...'

Thanks,
H.

>
> If that can't/won't be fixed, what about fixing the man page so it's
> in sync with the current behavior?
>
> Thanks,
> H.
>
> On 05/03/2017 02:26 AM, peter dalgaard wrote:
>> The first line of stopifnot is
>>
>>     n <- length(ll <- list(...))
>>
>> which takes ALL arguments and forms a list of them. This implies
>> evaluation, so explains the effect that you see.
>>
>> To do it differently, you would have to do something like
>>
>>    dots <- match.call(expand.dots=FALSE)$...
>>
>> and then explicitly evaluate each argument in turn in the caller
>> frame. This amount of nonstandard evaluation sounds like it would
>> incur a performance penalty, which could be undesirable.
>>
>> If you want to enforce the order of evaluation, there is always
>>
>>    stopifnot(A)
>>    stopifnot(B)
>>
>> -pd
>>
>>> On 3 May 2017, at 02:50 , Herv? Pag?s <hpages at fredhutch.org> wrote:
>>>
>>> Hi,
>>>
>>> It's surprising that stopifnot() keeps evaluating its arguments after
>>> it reaches the first one that is not TRUE:
>>>
>>>  > stopifnot(3 == 5, as.integer(2^32), a <- 12)
>>>  Error: 3 == 5 is not TRUE
>>>  In addition: Warning message:
>>>  In stopifnot(3 == 5, as.integer(2^32), a <- 12) :
>>>    NAs introduced by coercion to integer range
>>>  > a
>>>  [1] 12
>>>
>>> The details section in its man page actually suggests that it should
>>> stop at the first non-TRUE argument:
>>>
>>>  ?stopifnot(A, B)? is conceptually equivalent to
>>>
>>>   { if(any(is.na(A)) || !all(A)) stop(...);
>>>     if(any(is.na(B)) || !all(B)) stop(...) }
>>>
>>> Best,
>>> H.
>>>
>>> --
>>> Herv? Pag?s
>>>
>>> Program in Computational Biology
>>> Division of Public Health Sciences
>>> Fred Hutchinson Cancer Research Center
>>> 1100 Fairview Ave. N, M1-B514
>>> P.O. Box 19024
>>> Seattle, WA 98109-1024
>>>
>>> E-mail: hpages at fredhutch.org
>>> Phone:  (206) 667-5791
>>> Fax:    (206) 667-1319
>>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Ddevel&d=DwIFaQ&c=eRAMFD45gAfqt84VtBcfhQ&r=BK7q3XeAvimeWdGbWY_wJYbW0WYiZvSXAJJKaaPhzWA&m=JwgKhKD2k-9Kedeh6pqu-A8x6UEV0INrcxcSGVGo3Tg&s=f7IKJIhpRNJMC3rZAkuI6-MTdL3GAKSV2wK0boFN5HY&e=
>>>
>>
>

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fredhutch.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From kasperdanielhansen at gmail.com  Thu May  4 14:57:05 2017
From: kasperdanielhansen at gmail.com (Kasper Daniel Hansen)
Date: Thu, 4 May 2017 08:57:05 -0400
Subject: [Rd] complex tests failure
Message-ID: <CAC2h7uvigzjy-BdKp3qanmvY+Lp-yFukDpD11cQ_0e+M8k26Dw@mail.gmail.com>

For a while I have been getting that the complex tests fails on RHEL 6.
The specific issue has to do with tanh (see below for full output from
complex.Rout.fail).

This is both with the stock compiler (GCC 4.4.7) and a compiler supplied
through the conda project (GCC 4.8.5).  The compiler supplied through conda
ends up linking R to certain system files, so the binary is not completely
independent (although most dynamically linked libraries are coming from the
conda installation).

A search on R-devel reveals a discussion in April on an issue reported on
Windows with a bug in tanh in old versions of the GNU C standard library;
this seems relevant.  The discussion by Martin Maechler suggest "using R's
internal substitute".  So how do I enable this?  Or does this requires
updating the C standard library?

** From complex.Rout.fail

> stopifnot(identical(tanh(356+0i), 1+0i))
Error: identical(tanh(356 + (0+0i)), 1 + (0+0i)) is not TRUE
In addition: Warning message:
In tanh(356 + (0+0i)) : NaNs produced in function "tanh"
Execution halted

Best,
Kasper

	[[alternative HTML version deleted]]


From tomas.kalibera at gmail.com  Thu May  4 15:12:04 2017
From: tomas.kalibera at gmail.com (Tomas Kalibera)
Date: Thu, 4 May 2017 15:12:04 +0200
Subject: [Rd] complex tests failure
In-Reply-To: <CAC2h7uvigzjy-BdKp3qanmvY+Lp-yFukDpD11cQ_0e+M8k26Dw@mail.gmail.com>
References: <CAC2h7uvigzjy-BdKp3qanmvY+Lp-yFukDpD11cQ_0e+M8k26Dw@mail.gmail.com>
Message-ID: <2b323dcb-f7c8-6798-b08f-c199ca82df7e@gmail.com>


As a quick fix, you can undefine HAVE_CTANH in complex.c, somewhere 
after including config.h
An internal substitute, which is implemented inside complex.c, will be used.

Best
Tomas



On 05/04/2017 02:57 PM, Kasper Daniel Hansen wrote:
> For a while I have been getting that the complex tests fails on RHEL 6.
> The specific issue has to do with tanh (see below for full output from
> complex.Rout.fail).
>
> This is both with the stock compiler (GCC 4.4.7) and a compiler supplied
> through the conda project (GCC 4.8.5).  The compiler supplied through conda
> ends up linking R to certain system files, so the binary is not completely
> independent (although most dynamically linked libraries are coming from the
> conda installation).
>
> A search on R-devel reveals a discussion in April on an issue reported on
> Windows with a bug in tanh in old versions of the GNU C standard library;
> this seems relevant.  The discussion by Martin Maechler suggest "using R's
> internal substitute".  So how do I enable this?  Or does this requires
> updating the C standard library?
>
> ** From complex.Rout.fail
>
>> stopifnot(identical(tanh(356+0i), 1+0i))
> Error: identical(tanh(356 + (0+0i)), 1 + (0+0i)) is not TRUE
> In addition: Warning message:
> In tanh(356 + (0+0i)) : NaNs produced in function "tanh"
> Execution halted
>
> Best,
> Kasper
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From kasperdanielhansen at gmail.com  Thu May  4 15:23:48 2017
From: kasperdanielhansen at gmail.com (Kasper Daniel Hansen)
Date: Thu, 4 May 2017 09:23:48 -0400
Subject: [Rd] complex tests failure
In-Reply-To: <2b323dcb-f7c8-6798-b08f-c199ca82df7e@gmail.com>
References: <CAC2h7uvigzjy-BdKp3qanmvY+Lp-yFukDpD11cQ_0e+M8k26Dw@mail.gmail.com>
 <2b323dcb-f7c8-6798-b08f-c199ca82df7e@gmail.com>
Message-ID: <CAC2h7uvD5oBVFHFCv_i9-As_MKYvHtvMmTtYSQyGW910+1n+Xg@mail.gmail.com>

Thanks.

I assume there is no way to control this via. environment variables or
configure settings?  Obviously that would be great for something like this
which affects tests and seems to be a known problem for older C standard
libraries.

Best,
Kasper

On Thu, May 4, 2017 at 9:12 AM, Tomas Kalibera <tomas.kalibera at gmail.com>
wrote:

>
> As a quick fix, you can undefine HAVE_CTANH in complex.c, somewhere after
> including config.h
> An internal substitute, which is implemented inside complex.c, will be
> used.
>
> Best
> Tomas
>
>
>
>
> On 05/04/2017 02:57 PM, Kasper Daniel Hansen wrote:
>
>> For a while I have been getting that the complex tests fails on RHEL 6.
>> The specific issue has to do with tanh (see below for full output from
>> complex.Rout.fail).
>>
>> This is both with the stock compiler (GCC 4.4.7) and a compiler supplied
>> through the conda project (GCC 4.8.5).  The compiler supplied through
>> conda
>> ends up linking R to certain system files, so the binary is not completely
>> independent (although most dynamically linked libraries are coming from
>> the
>> conda installation).
>>
>> A search on R-devel reveals a discussion in April on an issue reported on
>> Windows with a bug in tanh in old versions of the GNU C standard library;
>> this seems relevant.  The discussion by Martin Maechler suggest "using R's
>> internal substitute".  So how do I enable this?  Or does this requires
>> updating the C standard library?
>>
>> ** From complex.Rout.fail
>>
>> stopifnot(identical(tanh(356+0i), 1+0i))
>>>
>> Error: identical(tanh(356 + (0+0i)), 1 + (0+0i)) is not TRUE
>> In addition: Warning message:
>> In tanh(356 + (0+0i)) : NaNs produced in function "tanh"
>> Execution halted
>>
>> Best,
>> Kasper
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>
>
>

	[[alternative HTML version deleted]]


From tomas.kalibera at gmail.com  Thu May  4 15:49:47 2017
From: tomas.kalibera at gmail.com (Tomas Kalibera)
Date: Thu, 4 May 2017 15:49:47 +0200
Subject: [Rd] complex tests failure
In-Reply-To: <CAC2h7uvD5oBVFHFCv_i9-As_MKYvHtvMmTtYSQyGW910+1n+Xg@mail.gmail.com>
References: <CAC2h7uvigzjy-BdKp3qanmvY+Lp-yFukDpD11cQ_0e+M8k26Dw@mail.gmail.com>
 <2b323dcb-f7c8-6798-b08f-c199ca82df7e@gmail.com>
 <CAC2h7uvD5oBVFHFCv_i9-As_MKYvHtvMmTtYSQyGW910+1n+Xg@mail.gmail.com>
Message-ID: <d07219e9-dbdc-42a1-fa1a-cc5c70e99570@gmail.com>


There is no way to control this at runtime.
We will probably have to add a configure test.

Best,
Tomas

On 05/04/2017 03:23 PM, Kasper Daniel Hansen wrote:
> Thanks.
>
> I assume there is no way to control this via. environment variables or 
> configure settings?  Obviously that would be great for something like 
> this which affects tests and seems to be a known problem for older C 
> standard libraries.
>
> Best,
> Kasper
>
> On Thu, May 4, 2017 at 9:12 AM, Tomas Kalibera 
> <tomas.kalibera at gmail.com <mailto:tomas.kalibera at gmail.com>> wrote:
>
>
>     As a quick fix, you can undefine HAVE_CTANH in complex.c,
>     somewhere after including config.h
>     An internal substitute, which is implemented inside complex.c,
>     will be used.
>
>     Best
>     Tomas
>
>
>
>
>     On 05/04/2017 02:57 PM, Kasper Daniel Hansen wrote:
>
>         For a while I have been getting that the complex tests fails
>         on RHEL 6.
>         The specific issue has to do with tanh (see below for full
>         output from
>         complex.Rout.fail).
>
>         This is both with the stock compiler (GCC 4.4.7) and a
>         compiler supplied
>         through the conda project (GCC 4.8.5).  The compiler supplied
>         through conda
>         ends up linking R to certain system files, so the binary is
>         not completely
>         independent (although most dynamically linked libraries are
>         coming from the
>         conda installation).
>
>         A search on R-devel reveals a discussion in April on an issue
>         reported on
>         Windows with a bug in tanh in old versions of the GNU C
>         standard library;
>         this seems relevant.  The discussion by Martin Maechler
>         suggest "using R's
>         internal substitute".  So how do I enable this?  Or does this
>         requires
>         updating the C standard library?
>
>         ** From complex.Rout.fail
>
>             stopifnot(identical(tanh(356+0i), 1+0i))
>
>         Error: identical(tanh(356 + (0+0i)), 1 + (0+0i)) is not TRUE
>         In addition: Warning message:
>         In tanh(356 + (0+0i)) : NaNs produced in function "tanh"
>         Execution halted
>
>         Best,
>         Kasper
>
>                 [[alternative HTML version deleted]]
>
>         ______________________________________________
>         R-devel at r-project.org <mailto:R-devel at r-project.org> mailing list
>         https://stat.ethz.ch/mailman/listinfo/r-devel
>         <https://stat.ethz.ch/mailman/listinfo/r-devel>
>
>
>
>


	[[alternative HTML version deleted]]


From nick.brown at free.fr  Thu May  4 16:28:53 2017
From: nick.brown at free.fr (Nick Brown)
Date: Thu, 4 May 2017 16:28:53 +0200 (CEST)
Subject: [Rd] lm() gives different results to lm.ridge() and SPSS
In-Reply-To: <66541776.179959727.1493907460059.JavaMail.root@zimbra61-e11.priv.proxad.net>
Message-ID: <1366533592.180004667.1493908133902.JavaMail.root@zimbra61-e11.priv.proxad.net>

Hallo, 

I hope I am posting to the right place. I was advised to try this list by Ben Bolker (https://twitter.com/bolkerb/status/859909918446497795). I also posted this question to StackOverflow (http://stackoverflow.com/questions/43771269/lm-gives-different-results-from-lm-ridgelambda-0). I am a relative newcomer to R, but I wrote my first program in 1975 and have been paid to program in about 15 different languages, so I have some general background knowledge. 


I have a regression from which I extract the coefficients like this: 
lm(y ~ x1 * x2, data=ds)$coef 
That gives: x1=0.40, x2=0.37, x1*x2=0.09 



When I do the same regression in SPSS, I get: 
beta(x1)=0.40, beta(x2)=0.37, beta(x1*x2)=0.14. 
So the main effects are in agreement, but there is quite a difference in the coefficient for the interaction. 


X1 and X2 are correlated about .75 (yes, yes, I know - this model wasn't my idea, but it got published), so there is quite possibly something going on with collinearity. So I thought I'd try lm.ridge() to see if I can get an idea of where the problems are occurring. 


The starting point is to run lm.ridge() with lambda=0 (i.e., no ridge penalty) and check we get the same results as with lm(): 
lm.ridge(y ~ x1 * x2, lambda=0, data=ds)$coef 
x1=0.40, x2=0.37, x1*x2=0.14 
So lm.ridge() agrees with SPSS, but not with lm(). (Of course, lambda=0 is the default, so it can be omitted; I can alternate between including or deleting ".ridge" in the function call, and watch the coefficient for the interaction change.) 



What seems slightly strange to me here is that I assumed that lm.ridge() just piggybacks on lm() anyway, so in the specific case where lambda=0 and there is no "ridging" to do, I'd expect exactly the same results. 


Unfortunately there are 34,000 cases in the dataset, so a "minimal" reprex will not be easy to make, but I can share the data via Dropbox or something if that would help. 



I appreciate that when there is strong collinearity then all bets are off in terms of what the betas mean, but I would really expect lm() and lm.ridge() to give the same results. (I would be happy to ignore SPSS, but for the moment it's part of the majority!) 



Thanks for reading, 
Nick 


	[[alternative HTML version deleted]]


From murdoch.duncan at gmail.com  Thu May  4 19:15:25 2017
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Thu, 4 May 2017 13:15:25 -0400
Subject: [Rd] lm() gives different results to lm.ridge() and SPSS
In-Reply-To: <1366533592.180004667.1493908133902.JavaMail.root@zimbra61-e11.priv.proxad.net>
References: <1366533592.180004667.1493908133902.JavaMail.root@zimbra61-e11.priv.proxad.net>
Message-ID: <9aac4ad6-39e9-9f16-8c18-fd74b89ce687@gmail.com>

On 04/05/2017 10:28 AM, Nick Brown wrote:
> Hallo,
>
> I hope I am posting to the right place. I was advised to try this list by Ben Bolker (https://twitter.com/bolkerb/status/859909918446497795). I also posted this question to StackOverflow (http://stackoverflow.com/questions/43771269/lm-gives-different-results-from-lm-ridgelambda-0). I am a relative newcomer to R, but I wrote my first program in 1975 and have been paid to program in about 15 different languages, so I have some general background knowledge.
>
>
> I have a regression from which I extract the coefficients like this:
> lm(y ~ x1 * x2, data=ds)$coef
> That gives: x1=0.40, x2=0.37, x1*x2=0.09
>
>
>
> When I do the same regression in SPSS, I get:
> beta(x1)=0.40, beta(x2)=0.37, beta(x1*x2)=0.14.
> So the main effects are in agreement, but there is quite a difference in the coefficient for the interaction.

I don't know about this instance, but a common cause of this sort of 
difference is a different parametrization.  If that's the case, then 
predictions in the two systems would match, even if coefficients don't.

Duncan Murdoch

>
>
> X1 and X2 are correlated about .75 (yes, yes, I know - this model wasn't my idea, but it got published), so there is quite possibly something going on with collinearity. So I thought I'd try lm.ridge() to see if I can get an idea of where the problems are occurring.
>
>
> The starting point is to run lm.ridge() with lambda=0 (i.e., no ridge penalty) and check we get the same results as with lm():
> lm.ridge(y ~ x1 * x2, lambda=0, data=ds)$coef
> x1=0.40, x2=0.37, x1*x2=0.14
> So lm.ridge() agrees with SPSS, but not with lm(). (Of course, lambda=0 is the default, so it can be omitted; I can alternate between including or deleting ".ridge" in the function call, and watch the coefficient for the interaction change.)
>
>
>
> What seems slightly strange to me here is that I assumed that lm.ridge() just piggybacks on lm() anyway, so in the specific case where lambda=0 and there is no "ridging" to do, I'd expect exactly the same results.
>
>
> Unfortunately there are 34,000 cases in the dataset, so a "minimal" reprex will not be easy to make, but I can share the data via Dropbox or something if that would help.
>
>
>
> I appreciate that when there is strong collinearity then all bets are off in terms of what the betas mean, but I would really expect lm() and lm.ridge() to give the same results. (I would be happy to ignore SPSS, but for the moment it's part of the majority!)
>
>
>
> Thanks for reading,
> Nick
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From pdalgd at gmail.com  Thu May  4 19:21:10 2017
From: pdalgd at gmail.com (peter dalgaard)
Date: Thu, 4 May 2017 19:21:10 +0200
Subject: [Rd] lm() gives different results to lm.ridge() and SPSS
In-Reply-To: <1366533592.180004667.1493908133902.JavaMail.root@zimbra61-e11.priv.proxad.net>
References: <1366533592.180004667.1493908133902.JavaMail.root@zimbra61-e11.priv.proxad.net>
Message-ID: <A3B42B64-72E5-4179-AE79-CA595A8B4669@gmail.com>

Um, the link to StackOverflow does not seem to contain the same question. It does contain a stern warning not to use the $coef component of lm.ridge...

Is it perhaps the case that x1 and x2 have already been scaled to have standard deviation 1? In that case, x1*x2 won't be.

Also notice that SPSS tends to use "Beta" for standardized regression coefficients, and (AFAIR) "b" for the regular ones.

-pd

> On 4 May 2017, at 16:28 , Nick Brown <nick.brown at free.fr> wrote:
> 
> Hallo, 
> 
> I hope I am posting to the right place. I was advised to try this list by Ben Bolker (https://twitter.com/bolkerb/status/859909918446497795). I also posted this question to StackOverflow (http://stackoverflow.com/questions/43771269/lm-gives-different-results-from-lm-ridgelambda-0). I am a relative newcomer to R, but I wrote my first program in 1975 and have been paid to program in about 15 different languages, so I have some general background knowledge. 
> 
> 
> I have a regression from which I extract the coefficients like this: 
> lm(y ~ x1 * x2, data=ds)$coef 
> That gives: x1=0.40, x2=0.37, x1*x2=0.09 
> 
> 
> 
> When I do the same regression in SPSS, I get: 
> beta(x1)=0.40, beta(x2)=0.37, beta(x1*x2)=0.14. 
> So the main effects are in agreement, but there is quite a difference in the coefficient for the interaction. 
> 
> 
> X1 and X2 are correlated about .75 (yes, yes, I know - this model wasn't my idea, but it got published), so there is quite possibly something going on with collinearity. So I thought I'd try lm.ridge() to see if I can get an idea of where the problems are occurring. 
> 
> 
> The starting point is to run lm.ridge() with lambda=0 (i.e., no ridge penalty) and check we get the same results as with lm(): 
> lm.ridge(y ~ x1 * x2, lambda=0, data=ds)$coef 
> x1=0.40, x2=0.37, x1*x2=0.14 
> So lm.ridge() agrees with SPSS, but not with lm(). (Of course, lambda=0 is the default, so it can be omitted; I can alternate between including or deleting ".ridge" in the function call, and watch the coefficient for the interaction change.) 
> 
> 
> 
> What seems slightly strange to me here is that I assumed that lm.ridge() just piggybacks on lm() anyway, so in the specific case where lambda=0 and there is no "ridging" to do, I'd expect exactly the same results. 
> 
> 
> Unfortunately there are 34,000 cases in the dataset, so a "minimal" reprex will not be easy to make, but I can share the data via Dropbox or something if that would help. 
> 
> 
> 
> I appreciate that when there is strong collinearity then all bets are off in terms of what the betas mean, but I would really expect lm() and lm.ridge() to give the same results. (I would be happy to ignore SPSS, but for the moment it's part of the majority!) 
> 
> 
> 
> Thanks for reading, 
> Nick 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From sbonner6 at uwo.ca  Thu May  4 19:07:33 2017
From: sbonner6 at uwo.ca (Simon Bonner)
Date: Thu, 4 May 2017 17:07:33 +0000
Subject: [Rd] lm() gives different results to lm.ridge() and SPSS
In-Reply-To: <1366533592.180004667.1493908133902.JavaMail.root@zimbra61-e11.priv.proxad.net>
References: <66541776.179959727.1493907460059.JavaMail.root@zimbra61-e11.priv.proxad.net>
 <1366533592.180004667.1493908133902.JavaMail.root@zimbra61-e11.priv.proxad.net>
Message-ID: <DM5PR1101MB2364BD86D29D697CD32DC148E4EA0@DM5PR1101MB2364.namprd11.prod.outlook.com>

Hi Nick,

I think that the problem here is your use of $coef to extract the coefficients of the ridge regression. The help for lm.ridge states that coef is a "matrix of coefficients, one row for each value of lambda. Note that these are not on the original scale and are for use by the coef method."

I ran a small test with simulated data, code is copied below, and indeed the output from lm.ridge differs depending on whether the coefficients are accessed via $coef or via the coefficients() function. The latter does produce results that match the output from lm.

I hope that helps.

Cheers,

Simon

## Load packages
library(MASS)

## Set seed 
set.seed(8888)

## Set parameters
n <- 100
beta <- c(1,0,1)
sigma <- .5
rho <- .75

## Simulate correlated covariates
Sigma <- matrix(c(1,rho,rho,1),ncol=2)
X <- mvrnorm(n,c(0,0),Sigma=Sigma)

## Simulate data
mu <- beta[1] + X %*% beta[-1]
y <- rnorm(n,mu,sigma)

## Fit model with lm()
fit1 <- lm(y ~ X)

## Fit model with lm.ridge()
fit2 <- lm.ridge(y ~ X)

## Compare coefficients
cbind(fit1$coefficients,c(NA,fit2$coef),coefficients(fit2))

                   [,1]        [,2]        [,3]
(Intercept)  0.99276001          NA  0.99276001
X1          -0.03980772 -0.04282391 -0.03980772
X2           1.11167179  1.06200476  1.11167179

--

Simon Bonner
Assistant Professor of Environmetrics/ Director MMASc 
Department of Statistical and Actuarial Sciences/Department of Biology 
University of Western Ontario

Office: Western Science Centre rm 276

Email: sbonner6 at uwo.ca | Telephone: 519-661-2111 x88205 | Fax: 519-661-3813
Twitter: @bonnerstatslab | Website: http://simon.bonners.ca/bonner-lab/wpblog/

> -----Original Message-----
> From: R-devel [mailto:r-devel-bounces at r-project.org] On Behalf Of Nick
> Brown
> Sent: May 4, 2017 10:29 AM
> To: r-devel at r-project.org
> Subject: [Rd] lm() gives different results to lm.ridge() and SPSS
> 
> Hallo,
> 
> I hope I am posting to the right place. I was advised to try this list by Ben Bolker
> (https://twitter.com/bolkerb/status/859909918446497795). I also posted this
> question to StackOverflow
> (http://stackoverflow.com/questions/43771269/lm-gives-different-results-
> from-lm-ridgelambda-0). I am a relative newcomer to R, but I wrote my first
> program in 1975 and have been paid to program in about 15 different
> languages, so I have some general background knowledge.
> 
> 
> I have a regression from which I extract the coefficients like this:
> lm(y ~ x1 * x2, data=ds)$coef
> That gives: x1=0.40, x2=0.37, x1*x2=0.09
> 
> 
> 
> When I do the same regression in SPSS, I get:
> beta(x1)=0.40, beta(x2)=0.37, beta(x1*x2)=0.14.
> So the main effects are in agreement, but there is quite a difference in the
> coefficient for the interaction.
> 
> 
> X1 and X2 are correlated about .75 (yes, yes, I know - this model wasn't my
> idea, but it got published), so there is quite possibly something going on with
> collinearity. So I thought I'd try lm.ridge() to see if I can get an idea of where
> the problems are occurring.
> 
> 
> The starting point is to run lm.ridge() with lambda=0 (i.e., no ridge penalty) and
> check we get the same results as with lm():
> lm.ridge(y ~ x1 * x2, lambda=0, data=ds)$coef
> x1=0.40, x2=0.37, x1*x2=0.14
> So lm.ridge() agrees with SPSS, but not with lm(). (Of course, lambda=0 is the
> default, so it can be omitted; I can alternate between including or deleting
> ".ridge" in the function call, and watch the coefficient for the interaction
> change.)
> 
> 
> 
> What seems slightly strange to me here is that I assumed that lm.ridge() just
> piggybacks on lm() anyway, so in the specific case where lambda=0 and there
> is no "ridging" to do, I'd expect exactly the same results.
> 
> 
> Unfortunately there are 34,000 cases in the dataset, so a "minimal" reprex will
> not be easy to make, but I can share the data via Dropbox or something if that
> would help.
> 
> 
> 
> I appreciate that when there is strong collinearity then all bets are off in terms
> of what the betas mean, but I would really expect lm() and lm.ridge() to give
> the same results. (I would be happy to ignore SPSS, but for the moment it's
> part of the majority!)
> 
> 
> 
> Thanks for reading,
> Nick
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From nick.brown at free.fr  Fri May  5 01:58:32 2017
From: nick.brown at free.fr (Nick Brown)
Date: Fri, 5 May 2017 01:58:32 +0200 (CEST)
Subject: [Rd] lm() gives different results to lm.ridge() and SPSS
In-Reply-To: <DM5PR1101MB2364BD86D29D697CD32DC148E4EA0@DM5PR1101MB2364.namprd11.prod.outlook.com>
Message-ID: <2110578518.181762030.1493942312870.JavaMail.root@zimbra61-e11.priv.proxad.net>

Hi Simon, 

Yes, if I uses coefficients() I get the same results for lm() and lm.ridge(). So that's consistent, at least. 

Interestingly, the "wrong" number I get from lm.ridge()$coef agrees with the value from SPSS to 5dp, which is an interesting coincidence if these numbers have no particular external meaning in lm.ridge(). 

Kind regards, 
Nick 

----- Original Message -----

From: "Simon Bonner" <sbonner6 at uwo.ca> 
To: "Nick Brown" <nick.brown at free.fr>, r-devel at r-project.org 
Sent: Thursday, 4 May, 2017 7:07:33 PM 
Subject: RE: [Rd] lm() gives different results to lm.ridge() and SPSS 

Hi Nick, 

I think that the problem here is your use of $coef to extract the coefficients of the ridge regression. The help for lm.ridge states that coef is a "matrix of coefficients, one row for each value of lambda. Note that these are not on the original scale and are for use by the coef method." 

I ran a small test with simulated data, code is copied below, and indeed the output from lm.ridge differs depending on whether the coefficients are accessed via $coef or via the coefficients() function. The latter does produce results that match the output from lm. 

I hope that helps. 

Cheers, 

Simon 

## Load packages 
library(MASS) 

## Set seed 
set.seed(8888) 

## Set parameters 
n <- 100 
beta <- c(1,0,1) 
sigma <- .5 
rho <- .75 

## Simulate correlated covariates 
Sigma <- matrix(c(1,rho,rho,1),ncol=2) 
X <- mvrnorm(n,c(0,0),Sigma=Sigma) 

## Simulate data 
mu <- beta[1] + X %*% beta[-1] 
y <- rnorm(n,mu,sigma) 

## Fit model with lm() 
fit1 <- lm(y ~ X) 

## Fit model with lm.ridge() 
fit2 <- lm.ridge(y ~ X) 

## Compare coefficients 
cbind(fit1$coefficients,c(NA,fit2$coef),coefficients(fit2)) 

[,1] [,2] [,3] 
(Intercept) 0.99276001 NA 0.99276001 
X1 -0.03980772 -0.04282391 -0.03980772 
X2 1.11167179 1.06200476 1.11167179 

-- 

Simon Bonner 
Assistant Professor of Environmetrics/ Director MMASc 
Department of Statistical and Actuarial Sciences/Department of Biology 
University of Western Ontario 

Office: Western Science Centre rm 276 

Email: sbonner6 at uwo.ca | Telephone: 519-661-2111 x88205 | Fax: 519-661-3813 
Twitter: @bonnerstatslab | Website: http://simon.bonners.ca/bonner-lab/wpblog/ 

> -----Original Message----- 
> From: R-devel [mailto:r-devel-bounces at r-project.org] On Behalf Of Nick 
> Brown 
> Sent: May 4, 2017 10:29 AM 
> To: r-devel at r-project.org 
> Subject: [Rd] lm() gives different results to lm.ridge() and SPSS 
> 
> Hallo, 
> 
> I hope I am posting to the right place. I was advised to try this list by Ben Bolker 
> (https://twitter.com/bolkerb/status/859909918446497795). I also posted this 
> question to StackOverflow 
> (http://stackoverflow.com/questions/43771269/lm-gives-different-results- 
> from-lm-ridgelambda-0). I am a relative newcomer to R, but I wrote my first 
> program in 1975 and have been paid to program in about 15 different 
> languages, so I have some general background knowledge. 
> 
> 
> I have a regression from which I extract the coefficients like this: 
> lm(y ~ x1 * x2, data=ds)$coef 
> That gives: x1=0.40, x2=0.37, x1*x2=0.09 
> 
> 
> 
> When I do the same regression in SPSS, I get: 
> beta(x1)=0.40, beta(x2)=0.37, beta(x1*x2)=0.14. 
> So the main effects are in agreement, but there is quite a difference in the 
> coefficient for the interaction. 
> 
> 
> X1 and X2 are correlated about .75 (yes, yes, I know - this model wasn't my 
> idea, but it got published), so there is quite possibly something going on with 
> collinearity. So I thought I'd try lm.ridge() to see if I can get an idea of where 
> the problems are occurring. 
> 
> 
> The starting point is to run lm.ridge() with lambda=0 (i.e., no ridge penalty) and 
> check we get the same results as with lm(): 
> lm.ridge(y ~ x1 * x2, lambda=0, data=ds)$coef 
> x1=0.40, x2=0.37, x1*x2=0.14 
> So lm.ridge() agrees with SPSS, but not with lm(). (Of course, lambda=0 is the 
> default, so it can be omitted; I can alternate between including or deleting 
> ".ridge" in the function call, and watch the coefficient for the interaction 
> change.) 
> 
> 
> 
> What seems slightly strange to me here is that I assumed that lm.ridge() just 
> piggybacks on lm() anyway, so in the specific case where lambda=0 and there 
> is no "ridging" to do, I'd expect exactly the same results. 
> 
> 
> Unfortunately there are 34,000 cases in the dataset, so a "minimal" reprex will 
> not be easy to make, but I can share the data via Dropbox or something if that 
> would help. 
> 
> 
> 
> I appreciate that when there is strong collinearity then all bets are off in terms 
> of what the betas mean, but I would really expect lm() and lm.ridge() to give 
> the same results. (I would be happy to ignore SPSS, but for the moment it's 
> part of the majority!) 
> 
> 
> 
> Thanks for reading, 
> Nick 
> 
> 
> [[alternative HTML version deleted]] 
> 
> ______________________________________________ 
> R-devel at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-devel 


	[[alternative HTML version deleted]]


From pdalgd at gmail.com  Fri May  5 10:02:10 2017
From: pdalgd at gmail.com (peter dalgaard)
Date: Fri, 5 May 2017 10:02:10 +0200
Subject: [Rd] lm() gives different results to lm.ridge() and SPSS
In-Reply-To: <2110578518.181762030.1493942312870.JavaMail.root@zimbra61-e11.priv.proxad.net>
References: <2110578518.181762030.1493942312870.JavaMail.root@zimbra61-e11.priv.proxad.net>
Message-ID: <46B7555E-BF46-41AE-8F38-5BC3C5457AF2@gmail.com>

I asked you before, but in case you missed it: Are you looking at the right place in SPSS output?

The UNstandardized coefficients should be comparable to R, i.e. the "B" column, not "Beta".

-pd

> On 5 May 2017, at 01:58 , Nick Brown <nick.brown at free.fr> wrote:
> 
> Hi Simon, 
> 
> Yes, if I uses coefficients() I get the same results for lm() and lm.ridge(). So that's consistent, at least. 
> 
> Interestingly, the "wrong" number I get from lm.ridge()$coef agrees with the value from SPSS to 5dp, which is an interesting coincidence if these numbers have no particular external meaning in lm.ridge(). 
> 
> Kind regards, 
> Nick 
> 
> ----- Original Message -----
> 
> From: "Simon Bonner" <sbonner6 at uwo.ca> 
> To: "Nick Brown" <nick.brown at free.fr>, r-devel at r-project.org 
> Sent: Thursday, 4 May, 2017 7:07:33 PM 
> Subject: RE: [Rd] lm() gives different results to lm.ridge() and SPSS 
> 
> Hi Nick, 
> 
> I think that the problem here is your use of $coef to extract the coefficients of the ridge regression. The help for lm.ridge states that coef is a "matrix of coefficients, one row for each value of lambda. Note that these are not on the original scale and are for use by the coef method." 
> 
> I ran a small test with simulated data, code is copied below, and indeed the output from lm.ridge differs depending on whether the coefficients are accessed via $coef or via the coefficients() function. The latter does produce results that match the output from lm. 
> 
> I hope that helps. 
> 
> Cheers, 
> 
> Simon 
> 
> ## Load packages 
> library(MASS) 
> 
> ## Set seed 
> set.seed(8888) 
> 
> ## Set parameters 
> n <- 100 
> beta <- c(1,0,1) 
> sigma <- .5 
> rho <- .75 
> 
> ## Simulate correlated covariates 
> Sigma <- matrix(c(1,rho,rho,1),ncol=2) 
> X <- mvrnorm(n,c(0,0),Sigma=Sigma) 
> 
> ## Simulate data 
> mu <- beta[1] + X %*% beta[-1] 
> y <- rnorm(n,mu,sigma) 
> 
> ## Fit model with lm() 
> fit1 <- lm(y ~ X) 
> 
> ## Fit model with lm.ridge() 
> fit2 <- lm.ridge(y ~ X) 
> 
> ## Compare coefficients 
> cbind(fit1$coefficients,c(NA,fit2$coef),coefficients(fit2)) 
> 
> [,1] [,2] [,3] 
> (Intercept) 0.99276001 NA 0.99276001 
> X1 -0.03980772 -0.04282391 -0.03980772 
> X2 1.11167179 1.06200476 1.11167179 
> 
> -- 
> 
> Simon Bonner 
> Assistant Professor of Environmetrics/ Director MMASc 
> Department of Statistical and Actuarial Sciences/Department of Biology 
> University of Western Ontario 
> 
> Office: Western Science Centre rm 276 
> 
> Email: sbonner6 at uwo.ca | Telephone: 519-661-2111 x88205 | Fax: 519-661-3813 
> Twitter: @bonnerstatslab | Website: http://simon.bonners.ca/bonner-lab/wpblog/ 
> 
>> -----Original Message----- 
>> From: R-devel [mailto:r-devel-bounces at r-project.org] On Behalf Of Nick 
>> Brown 
>> Sent: May 4, 2017 10:29 AM 
>> To: r-devel at r-project.org 
>> Subject: [Rd] lm() gives different results to lm.ridge() and SPSS 
>> 
>> Hallo, 
>> 
>> I hope I am posting to the right place. I was advised to try this list by Ben Bolker 
>> (https://twitter.com/bolkerb/status/859909918446497795). I also posted this 
>> question to StackOverflow 
>> (http://stackoverflow.com/questions/43771269/lm-gives-different-results- 
>> from-lm-ridgelambda-0). I am a relative newcomer to R, but I wrote my first 
>> program in 1975 and have been paid to program in about 15 different 
>> languages, so I have some general background knowledge. 
>> 
>> 
>> I have a regression from which I extract the coefficients like this: 
>> lm(y ~ x1 * x2, data=ds)$coef 
>> That gives: x1=0.40, x2=0.37, x1*x2=0.09 
>> 
>> 
>> 
>> When I do the same regression in SPSS, I get: 
>> beta(x1)=0.40, beta(x2)=0.37, beta(x1*x2)=0.14. 
>> So the main effects are in agreement, but there is quite a difference in the 
>> coefficient for the interaction. 
>> 
>> 
>> X1 and X2 are correlated about .75 (yes, yes, I know - this model wasn't my 
>> idea, but it got published), so there is quite possibly something going on with 
>> collinearity. So I thought I'd try lm.ridge() to see if I can get an idea of where 
>> the problems are occurring. 
>> 
>> 
>> The starting point is to run lm.ridge() with lambda=0 (i.e., no ridge penalty) and 
>> check we get the same results as with lm(): 
>> lm.ridge(y ~ x1 * x2, lambda=0, data=ds)$coef 
>> x1=0.40, x2=0.37, x1*x2=0.14 
>> So lm.ridge() agrees with SPSS, but not with lm(). (Of course, lambda=0 is the 
>> default, so it can be omitted; I can alternate between including or deleting 
>> ".ridge" in the function call, and watch the coefficient for the interaction 
>> change.) 
>> 
>> 
>> 
>> What seems slightly strange to me here is that I assumed that lm.ridge() just 
>> piggybacks on lm() anyway, so in the specific case where lambda=0 and there 
>> is no "ridging" to do, I'd expect exactly the same results. 
>> 
>> 
>> Unfortunately there are 34,000 cases in the dataset, so a "minimal" reprex will 
>> not be easy to make, but I can share the data via Dropbox or something if that 
>> would help. 
>> 
>> 
>> 
>> I appreciate that when there is strong collinearity then all bets are off in terms 
>> of what the betas mean, but I would really expect lm() and lm.ridge() to give 
>> the same results. (I would be happy to ignore SPSS, but for the moment it's 
>> part of the majority!) 
>> 
>> 
>> 
>> Thanks for reading, 
>> Nick 
>> 
>> 
>> [[alternative HTML version deleted]] 
>> 
>> ______________________________________________ 
>> R-devel at r-project.org mailing list 
>> https://stat.ethz.ch/mailman/listinfo/r-devel 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From nick.brown at free.fr  Fri May  5 10:40:29 2017
From: nick.brown at free.fr (Nick Brown)
Date: Fri, 5 May 2017 10:40:29 +0200 (CEST)
Subject: [Rd] lm() gives different results to lm.ridge() and SPSS
In-Reply-To: <46B7555E-BF46-41AE-8F38-5BC3C5457AF2@gmail.com>
Message-ID: <1710905310.182975967.1493973629624.JavaMail.root@zimbra61-e11.priv.proxad.net>

Hi, 

Here is (I hope) all the relevant output from R. 

> mean(s1$ZDEPRESSION, na.rm=T) [1] -1.041546e-16 > mean(s1$ZDIVERSITY_PA, na.rm=T) [1] -9.660583e-16 > mean(s1$ZMEAN_PA, na.rm=T) [1] -5.430282e-15 > lm.ridge(ZDEPRESSION ~ ZMEAN_PA * ZDIVERSITY_PA, data=s1)$coef ZMEAN_PA          ZDIVERSITY_PA ZMEAN_PA:ZDIVERSITY_PA 
            -0.3962254             -0.3636026             -0.1425772      ## This is what I thought was the problem originally. :-) 


> coefficients(lm(ZDEPRESSION ~ ZMEAN_PA * ZDIVERSITY_PA, data=s1)) (Intercept)               ZMEAN_PA          ZDIVERSITY_PA ZMEAN_PA:ZDIVERSITY_PA 
            0.07342198            -0.39650356            -0.36569488            -0.09435788 > coefficients(lm.ridge(ZDEPRESSION ~ ZMEAN_PA * ZDIVERSITY_PA, data=s1)) ZMEAN_PA          ZDIVERSITY_PA ZMEAN_PA:ZDIVERSITY_PA 
            0.07342198            -0.39650356            -0.36569488            -0.09435788 The equivalent from SPSS is attached. The unstandardized coefficients in SPSS look nothing like those in R. The standardized coefficients in SPSS match the lm.ridge()$coef numbers very closely indeed, suggesting that the same algorithm may be in use. 

I have put the dataset file, which is the untouched original I received from the authors, in this Dropbox folder: https://www.dropbox.com/sh/xsebjy55ius1ysb/AADwYUyV1bl6-iAw7ACuF1_La?dl=0. You can read it into R with this code (one variable needs to be standardized and centered; everything else is already in the file): 

s1 <- read.csv("Emodiversity_Study1.csv", stringsAsFactors=FALSE) s1$ZDEPRESSION <- scale(s1$DEPRESSION) 
Hey, maybe R is fine and I've stumbled on a bug in SPSS? If so, I'm sure IBM will want to fix it quickly (ha ha ha). 

Nick 



----- Original Message -----

From: "peter dalgaard" <pdalgd at gmail.com> 
To: "Nick Brown" <nick.brown at free.fr> 
Cc: "Simon Bonner" <sbonner6 at uwo.ca>, r-devel at r-project.org 
Sent: Friday, 5 May, 2017 10:02:10 AM 
Subject: Re: [Rd] lm() gives different results to lm.ridge() and SPSS 

I asked you before, but in case you missed it: Are you looking at the right place in SPSS output? 

The UNstandardized coefficients should be comparable to R, i.e. the "B" column, not "Beta". 

-pd 

> On 5 May 2017, at 01:58 , Nick Brown <nick.brown at free.fr> wrote: 
> 
> Hi Simon, 
> 
> Yes, if I uses coefficients() I get the same results for lm() and lm.ridge(). So that's consistent, at least. 
> 
> Interestingly, the "wrong" number I get from lm.ridge()$coef agrees with the value from SPSS to 5dp, which is an interesting coincidence if these numbers have no particular external meaning in lm.ridge(). 
> 
> Kind regards, 
> Nick 
> 
> ----- Original Message ----- 
> 
> From: "Simon Bonner" <sbonner6 at uwo.ca> 
> To: "Nick Brown" <nick.brown at free.fr>, r-devel at r-project.org 
> Sent: Thursday, 4 May, 2017 7:07:33 PM 
> Subject: RE: [Rd] lm() gives different results to lm.ridge() and SPSS 
> 
> Hi Nick, 
> 
> I think that the problem here is your use of $coef to extract the coefficients of the ridge regression. The help for lm.ridge states that coef is a "matrix of coefficients, one row for each value of lambda. Note that these are not on the original scale and are for use by the coef method." 
> 
> I ran a small test with simulated data, code is copied below, and indeed the output from lm.ridge differs depending on whether the coefficients are accessed via $coef or via the coefficients() function. The latter does produce results that match the output from lm. 
> 
> I hope that helps. 
> 
> Cheers, 
> 
> Simon 
> 
> ## Load packages 
> library(MASS) 
> 
> ## Set seed 
> set.seed(8888) 
> 
> ## Set parameters 
> n <- 100 
> beta <- c(1,0,1) 
> sigma <- .5 
> rho <- .75 
> 
> ## Simulate correlated covariates 
> Sigma <- matrix(c(1,rho,rho,1),ncol=2) 
> X <- mvrnorm(n,c(0,0),Sigma=Sigma) 
> 
> ## Simulate data 
> mu <- beta[1] + X %*% beta[-1] 
> y <- rnorm(n,mu,sigma) 
> 
> ## Fit model with lm() 
> fit1 <- lm(y ~ X) 
> 
> ## Fit model with lm.ridge() 
> fit2 <- lm.ridge(y ~ X) 
> 
> ## Compare coefficients 
> cbind(fit1$coefficients,c(NA,fit2$coef),coefficients(fit2)) 
> 
> [,1] [,2] [,3] 
> (Intercept) 0.99276001 NA 0.99276001 
> X1 -0.03980772 -0.04282391 -0.03980772 
> X2 1.11167179 1.06200476 1.11167179 
> 
> -- 
> 
> Simon Bonner 
> Assistant Professor of Environmetrics/ Director MMASc 
> Department of Statistical and Actuarial Sciences/Department of Biology 
> University of Western Ontario 
> 
> Office: Western Science Centre rm 276 
> 
> Email: sbonner6 at uwo.ca | Telephone: 519-661-2111 x88205 | Fax: 519-661-3813 
> Twitter: @bonnerstatslab | Website: http://simon.bonners.ca/bonner-lab/wpblog/ 
> 
>> -----Original Message----- 
>> From: R-devel [mailto:r-devel-bounces at r-project.org] On Behalf Of Nick 
>> Brown 
>> Sent: May 4, 2017 10:29 AM 
>> To: r-devel at r-project.org 
>> Subject: [Rd] lm() gives different results to lm.ridge() and SPSS 
>> 
>> Hallo, 
>> 
>> I hope I am posting to the right place. I was advised to try this list by Ben Bolker 
>> (https://twitter.com/bolkerb/status/859909918446497795). I also posted this 
>> question to StackOverflow 
>> (http://stackoverflow.com/questions/43771269/lm-gives-different-results- 
>> from-lm-ridgelambda-0). I am a relative newcomer to R, but I wrote my first 
>> program in 1975 and have been paid to program in about 15 different 
>> languages, so I have some general background knowledge. 
>> 
>> 
>> I have a regression from which I extract the coefficients like this: 
>> lm(y ~ x1 * x2, data=ds)$coef 
>> That gives: x1=0.40, x2=0.37, x1*x2=0.09 
>> 
>> 
>> 
>> When I do the same regression in SPSS, I get: 
>> beta(x1)=0.40, beta(x2)=0.37, beta(x1*x2)=0.14. 
>> So the main effects are in agreement, but there is quite a difference in the 
>> coefficient for the interaction. 
>> 
>> 
>> X1 and X2 are correlated about .75 (yes, yes, I know - this model wasn't my 
>> idea, but it got published), so there is quite possibly something going on with 
>> collinearity. So I thought I'd try lm.ridge() to see if I can get an idea of where 
>> the problems are occurring. 
>> 
>> 
>> The starting point is to run lm.ridge() with lambda=0 (i.e., no ridge penalty) and 
>> check we get the same results as with lm(): 
>> lm.ridge(y ~ x1 * x2, lambda=0, data=ds)$coef 
>> x1=0.40, x2=0.37, x1*x2=0.14 
>> So lm.ridge() agrees with SPSS, but not with lm(). (Of course, lambda=0 is the 
>> default, so it can be omitted; I can alternate between including or deleting 
>> ".ridge" in the function call, and watch the coefficient for the interaction 
>> change.) 
>> 
>> 
>> 
>> What seems slightly strange to me here is that I assumed that lm.ridge() just 
>> piggybacks on lm() anyway, so in the specific case where lambda=0 and there 
>> is no "ridging" to do, I'd expect exactly the same results. 
>> 
>> 
>> Unfortunately there are 34,000 cases in the dataset, so a "minimal" reprex will 
>> not be easy to make, but I can share the data via Dropbox or something if that 
>> would help. 
>> 
>> 
>> 
>> I appreciate that when there is strong collinearity then all bets are off in terms 
>> of what the betas mean, but I would really expect lm() and lm.ridge() to give 
>> the same results. (I would be happy to ignore SPSS, but for the moment it's 
>> part of the majority!) 
>> 
>> 
>> 
>> Thanks for reading, 
>> Nick 
>> 
>> 
>> [[alternative HTML version deleted]] 
>> 
>> ______________________________________________ 
>> R-devel at r-project.org mailing list 
>> https://stat.ethz.ch/mailman/listinfo/r-devel 
> 
> 
> [[alternative HTML version deleted]] 
> 
> ______________________________________________ 
> R-devel at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-devel 

-- 
Peter Dalgaard, Professor, 
Center for Statistics, Copenhagen Business School 
Solbjerg Plads 3, 2000 Frederiksberg, Denmark 
Phone: (+45)38153501 
Office: A 4.23 
Email: pd.mes at cbs.dk Priv: PDalgd at gmail.com 










-------------- next part --------------
A non-text attachment was scrubbed...
Name: SPSS.png
Type: image/png
Size: 38704 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20170505/24adf071/attachment.png>

From Ajay.Deonarine at aig.com  Fri May  5 14:24:30 2017
From: Ajay.Deonarine at aig.com (Deonarine, Ajay)
Date: Fri, 5 May 2017 12:24:30 +0000
Subject: [Rd] Possible bug in how POSIXct is printed
Message-ID: <E28F74B4F99F4F46804BCA91B0E7427628CE4B9B@PWGSMSLIVMBS02.mail.aig.net>

Greetings R-devel group.  When dealing with Inf dates, as.POSIXct seems to return Inf, but is printed as NA:

> x1 <- as.POSIXct(Inf, origin = '1970-01-01')
> print(x1)
[1] NA
> is.na(x1)
[1] FALSE
> is.infinite(x1)
[1] TRUE
>

POSIXlt at least evaluates and prints the result consistently:

> x1 <- as.POSIXlt(Inf, origin = '1970-01-01')
> print(x1)
[1] NA
> is.na(x1)
[1] TRUE
>

I think the cause is due to format.POSIXct function calling format.POSIXlt, hence printing as NA, but actually storing an Inf.  I don't have an opinion on what the actual result of as.POSIXct(Inf) should be, as long as what's printed matches that value.

Thank you

	[[alternative HTML version deleted]]


From wolfgang.viechtbauer at maastrichtuniversity.nl  Fri May  5 14:43:57 2017
From: wolfgang.viechtbauer at maastrichtuniversity.nl (Viechtbauer Wolfgang (SP))
Date: Fri, 5 May 2017 12:43:57 +0000
Subject: [Rd] lm() gives different results to lm.ridge() and SPSS
In-Reply-To: <1710905310.182975967.1493973629624.JavaMail.root@zimbra61-e11.priv.proxad.net>
References: <46B7555E-BF46-41AE-8F38-5BC3C5457AF2@gmail.com>
 <1710905310.182975967.1493973629624.JavaMail.root@zimbra61-e11.priv.proxad.net>
Message-ID: <b1a09b3bb148407e89fd52699018f786@UM-MAIL3215.unimaas.nl>

I had no problems running regression models in SPSS and R that yielded the same results for these data.

The difference you are observing is from fitting different models. In R, you fitted:

res <- lm(DEPRESSION ~ ZMEAN_PA * ZDIVERSITY_PA, data=dat)
summary(res)

The interaction term is the product of ZMEAN_PA and ZDIVERSITY_PA. This is not a standardized variable itself and not the same as "ZINTER_PA_C" in the png you showed, which is not a variable in the dataset, but can be created with:

dat$ZINTER_PA_C <- with(dat, scale(ZMEAN_PA * ZDIVERSITY_PA))

If you want the same results as in SPSS, then you need to fit:

res <- lm(DEPRESSION ~ ZMEAN_PA + ZDIVERSITY_PA + ZINTER_PA_C, data=dat)
summary(res)

This yields:

Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept)    6.41041    0.01722  372.21   <2e-16 ***
ZMEAN_PA      -1.62726    0.04200  -38.74   <2e-16 ***
ZDIVERSITY_PA -1.50082    0.07447  -20.15   <2e-16 ***
ZINTER_PA_C   -0.58955    0.05288  -11.15   <2e-16 ***
---
Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1

Exactly the same as in the png.

Peter already mentioned this as a possible reason for the discrepancy: https://stat.ethz.ch/pipermail/r-devel/2017-May/074191.html ("Is it perhaps the case that x1 and x2 have already been scaled to have standard deviation 1? In that case, x1*x2 won't be.")

Best,
Wolfgang

-----Original Message-----
From: R-devel [mailto:r-devel-bounces at r-project.org] On Behalf Of Nick Brown
Sent: Friday, May 05, 2017 10:40
To: peter dalgaard
Cc: r-devel at r-project.org
Subject: Re: [Rd] lm() gives different results to lm.ridge() and SPSS

Hi, 

Here is (I hope) all the relevant output from R. 

> mean(s1$ZDEPRESSION, na.rm=T) [1] -1.041546e-16 > mean(s1$ZDIVERSITY_PA, na.rm=T) [1] -9.660583e-16 > mean(s1$ZMEAN_PA, na.rm=T) [1] -5.430282e-15 > lm.ridge(ZDEPRESSION ~ ZMEAN_PA * ZDIVERSITY_PA, data=s1)$coef ZMEAN_PA          ZDIVERSITY_PA ZMEAN_PA:ZDIVERSITY_PA 
            -0.3962254             -0.3636026             -0.1425772      ## This is what I thought was the problem originally. :-) 


> coefficients(lm(ZDEPRESSION ~ ZMEAN_PA * ZDIVERSITY_PA, data=s1)) (Intercept)               ZMEAN_PA          ZDIVERSITY_PA ZMEAN_PA:ZDIVERSITY_PA 
            0.07342198            -0.39650356            -0.36569488            -0.09435788 > coefficients(lm.ridge(ZDEPRESSION ~ ZMEAN_PA * ZDIVERSITY_PA, data=s1)) ZMEAN_PA          ZDIVERSITY_PA ZMEAN_PA:ZDIVERSITY_PA 
            0.07342198            -0.39650356            -0.36569488            -0.09435788 The equivalent from SPSS is attached. The unstandardized coefficients in SPSS look nothing like those in R. The standardized coefficients in SPSS match the lm.ridge()$coef numbers very closely indeed, suggesting that the same algorithm may be in use. 

I have put the dataset file, which is the untouched original I received from the authors, in this Dropbox folder: https://www.dropbox.com/sh/xsebjy55ius1ysb/AADwYUyV1bl6-iAw7ACuF1_La?dl=0. You can read it into R with this code (one variable needs to be standardized and centered; everything else is already in the file): 

s1 <- read.csv("Emodiversity_Study1.csv", stringsAsFactors=FALSE) s1$ZDEPRESSION <- scale(s1$DEPRESSION) 
Hey, maybe R is fine and I've stumbled on a bug in SPSS? If so, I'm sure IBM will want to fix it quickly (ha ha ha). 

Nick 

----- Original Message -----

From: "peter dalgaard" <pdalgd at gmail.com> 
To: "Nick Brown" <nick.brown at free.fr> 
Cc: "Simon Bonner" <sbonner6 at uwo.ca>, r-devel at r-project.org 
Sent: Friday, 5 May, 2017 10:02:10 AM 
Subject: Re: [Rd] lm() gives different results to lm.ridge() and SPSS 

I asked you before, but in case you missed it: Are you looking at the right place in SPSS output? 

The UNstandardized coefficients should be comparable to R, i.e. the "B" column, not "Beta". 

-pd 

> On 5 May 2017, at 01:58 , Nick Brown <nick.brown at free.fr> wrote: 
> 
> Hi Simon, 
> 
> Yes, if I uses coefficients() I get the same results for lm() and lm.ridge(). So that's consistent, at least. 
> 
> Interestingly, the "wrong" number I get from lm.ridge()$coef agrees with the value from SPSS to 5dp, which is an interesting coincidence if these numbers have no particular external meaning in lm.ridge(). 
> 
> Kind regards, 
> Nick 
> 
> ----- Original Message ----- 
> 
> From: "Simon Bonner" <sbonner6 at uwo.ca> 
> To: "Nick Brown" <nick.brown at free.fr>, r-devel at r-project.org 
> Sent: Thursday, 4 May, 2017 7:07:33 PM 
> Subject: RE: [Rd] lm() gives different results to lm.ridge() and SPSS 
> 
> Hi Nick, 
> 
> I think that the problem here is your use of $coef to extract the coefficients of the ridge regression. The help for lm.ridge states that coef is a "matrix of coefficients, one row for each value of lambda. Note that these are not on the original scale and are for use by the coef method." 
> 
> I ran a small test with simulated data, code is copied below, and indeed the output from lm.ridge differs depending on whether the coefficients are accessed via $coef or via the coefficients() function. The latter does produce results that match the output from lm. 
> 
> I hope that helps. 
> 
> Cheers, 
> 
> Simon 
> 
> ## Load packages 
> library(MASS) 
> 
> ## Set seed 
> set.seed(8888) 
> 
> ## Set parameters 
> n <- 100 
> beta <- c(1,0,1) 
> sigma <- .5 
> rho <- .75 
> 
> ## Simulate correlated covariates 
> Sigma <- matrix(c(1,rho,rho,1),ncol=2) 
> X <- mvrnorm(n,c(0,0),Sigma=Sigma) 
> 
> ## Simulate data 
> mu <- beta[1] + X %*% beta[-1] 
> y <- rnorm(n,mu,sigma) 
> 
> ## Fit model with lm() 
> fit1 <- lm(y ~ X) 
> 
> ## Fit model with lm.ridge() 
> fit2 <- lm.ridge(y ~ X) 
> 
> ## Compare coefficients 
> cbind(fit1$coefficients,c(NA,fit2$coef),coefficients(fit2)) 
> 
> [,1] [,2] [,3] 
> (Intercept) 0.99276001 NA 0.99276001 
> X1 -0.03980772 -0.04282391 -0.03980772 
> X2 1.11167179 1.06200476 1.11167179 
> 
> -- 
> 
> Simon Bonner 
> Assistant Professor of Environmetrics/ Director MMASc 
> Department of Statistical and Actuarial Sciences/Department of Biology 
> University of Western Ontario 
> 
> Office: Western Science Centre rm 276 
> 
> Email: sbonner6 at uwo.ca | Telephone: 519-661-2111 x88205 | Fax: 519-661-3813 
> Twitter: @bonnerstatslab | Website: http://simon.bonners.ca/bonner-lab/wpblog/ 
> 
>> -----Original Message----- 
>> From: R-devel [mailto:r-devel-bounces at r-project.org] On Behalf Of Nick 
>> Brown 
>> Sent: May 4, 2017 10:29 AM 
>> To: r-devel at r-project.org 
>> Subject: [Rd] lm() gives different results to lm.ridge() and SPSS 
>> 
>> Hallo, 
>> 
>> I hope I am posting to the right place. I was advised to try this list by Ben Bolker 
>> (https://twitter.com/bolkerb/status/859909918446497795). I also posted this 
>> question to StackOverflow 
>> (http://stackoverflow.com/questions/43771269/lm-gives-different-results- 
>> from-lm-ridgelambda-0). I am a relative newcomer to R, but I wrote my first 
>> program in 1975 and have been paid to program in about 15 different 
>> languages, so I have some general background knowledge. 
>> 
>> I have a regression from which I extract the coefficients like this: 
>> lm(y ~ x1 * x2, data=ds)$coef 
>> That gives: x1=0.40, x2=0.37, x1*x2=0.09 
>> 
>> When I do the same regression in SPSS, I get: 
>> beta(x1)=0.40, beta(x2)=0.37, beta(x1*x2)=0.14. 
>> So the main effects are in agreement, but there is quite a difference in the 
>> coefficient for the interaction. 
>> 
>> X1 and X2 are correlated about .75 (yes, yes, I know - this model wasn't my 
>> idea, but it got published), so there is quite possibly something going on with 
>> collinearity. So I thought I'd try lm.ridge() to see if I can get an idea of where 
>> the problems are occurring. 
>> 
>> The starting point is to run lm.ridge() with lambda=0 (i.e., no ridge penalty) and 
>> check we get the same results as with lm(): 
>> lm.ridge(y ~ x1 * x2, lambda=0, data=ds)$coef 
>> x1=0.40, x2=0.37, x1*x2=0.14 
>> So lm.ridge() agrees with SPSS, but not with lm(). (Of course, lambda=0 is the 
>> default, so it can be omitted; I can alternate between including or deleting 
>> ".ridge" in the function call, and watch the coefficient for the interaction 
>> change.) 
>> 
>> What seems slightly strange to me here is that I assumed that lm.ridge() just 
>> piggybacks on lm() anyway, so in the specific case where lambda=0 and there 
>> is no "ridging" to do, I'd expect exactly the same results. 
>> 
>> Unfortunately there are 34,000 cases in the dataset, so a "minimal" reprex will 
>> not be easy to make, but I can share the data via Dropbox or something if that 
>> would help. 
>> 
>> I appreciate that when there is strong collinearity then all bets are off in terms 
>> of what the betas mean, but I would really expect lm() and lm.ridge() to give 
>> the same results. (I would be happy to ignore SPSS, but for the moment it's 
>> part of the majority!) 
>> 
>> Thanks for reading, 
>> Nick 

From tomas.kalibera at gmail.com  Fri May  5 15:23:40 2017
From: tomas.kalibera at gmail.com (Tomas Kalibera)
Date: Fri, 5 May 2017 15:23:40 +0200
Subject: [Rd] complex tests failure
In-Reply-To: <d07219e9-dbdc-42a1-fa1a-cc5c70e99570@gmail.com>
References: <CAC2h7uvigzjy-BdKp3qanmvY+Lp-yFukDpD11cQ_0e+M8k26Dw@mail.gmail.com>
 <2b323dcb-f7c8-6798-b08f-c199ca82df7e@gmail.com>
 <CAC2h7uvD5oBVFHFCv_i9-As_MKYvHtvMmTtYSQyGW910+1n+Xg@mail.gmail.com>
 <d07219e9-dbdc-42a1-fa1a-cc5c70e99570@gmail.com>
Message-ID: <edac4326-f88b-5ce0-5c3e-c650823e8874@gmail.com>

Thanks for the report, handled in configure in 72661 (R-devel).
I'll also port to R-patched.

Best
Tomas

On 05/04/2017 03:49 PM, Tomas Kalibera wrote:
>
> There is no way to control this at runtime.
> We will probably have to add a configure test.
>
> Best,
> Tomas
>
> On 05/04/2017 03:23 PM, Kasper Daniel Hansen wrote:
>> Thanks.
>>
>> I assume there is no way to control this via. environment variables 
>> or configure settings?  Obviously that would be great for something 
>> like this which affects tests and seems to be a known problem for 
>> older C standard libraries.
>>
>> Best,
>> Kasper
>>
>> On Thu, May 4, 2017 at 9:12 AM, Tomas Kalibera 
>> <tomas.kalibera at gmail.com <mailto:tomas.kalibera at gmail.com>> wrote:
>>
>>
>>     As a quick fix, you can undefine HAVE_CTANH in complex.c,
>>     somewhere after including config.h
>>     An internal substitute, which is implemented inside complex.c,
>>     will be used.
>>
>>     Best
>>     Tomas
>>
>>
>>
>>
>>     On 05/04/2017 02:57 PM, Kasper Daniel Hansen wrote:
>>
>>         For a while I have been getting that the complex tests fails
>>         on RHEL 6.
>>         The specific issue has to do with tanh (see below for full
>>         output from
>>         complex.Rout.fail).
>>
>>         This is both with the stock compiler (GCC 4.4.7) and a
>>         compiler supplied
>>         through the conda project (GCC 4.8.5).  The compiler supplied
>>         through conda
>>         ends up linking R to certain system files, so the binary is
>>         not completely
>>         independent (although most dynamically linked libraries are
>>         coming from the
>>         conda installation).
>>
>>         A search on R-devel reveals a discussion in April on an issue
>>         reported on
>>         Windows with a bug in tanh in old versions of the GNU C
>>         standard library;
>>         this seems relevant.  The discussion by Martin Maechler
>>         suggest "using R's
>>         internal substitute".  So how do I enable this?  Or does this
>>         requires
>>         updating the C standard library?
>>
>>         ** From complex.Rout.fail
>>
>>             stopifnot(identical(tanh(356+0i), 1+0i))
>>
>>         Error: identical(tanh(356 + (0+0i)), 1 + (0+0i)) is not TRUE
>>         In addition: Warning message:
>>         In tanh(356 + (0+0i)) : NaNs produced in function "tanh"
>>         Execution halted
>>
>>         Best,
>>         Kasper
>>
>>                 [[alternative HTML version deleted]]
>>
>>         ______________________________________________
>>         R-devel at r-project.org <mailto:R-devel at r-project.org> mailing list
>>         https://stat.ethz.ch/mailman/listinfo/r-devel
>>         <https://stat.ethz.ch/mailman/listinfo/r-devel>
>>
>>
>>
>>
>


	[[alternative HTML version deleted]]


From pdalgd at gmail.com  Fri May  5 15:33:29 2017
From: pdalgd at gmail.com (peter dalgaard)
Date: Fri, 5 May 2017 15:33:29 +0200
Subject: [Rd] lm() gives different results to lm.ridge() and SPSS
In-Reply-To: <b1a09b3bb148407e89fd52699018f786@UM-MAIL3215.unimaas.nl>
References: <46B7555E-BF46-41AE-8F38-5BC3C5457AF2@gmail.com>
 <1710905310.182975967.1493973629624.JavaMail.root@zimbra61-e11.priv.proxad.net>
 <b1a09b3bb148407e89fd52699018f786@UM-MAIL3215.unimaas.nl>
Message-ID: <15506037-17F7-4B45-9299-327BBCB27347@gmail.com>

Thanks, I was getting to try this, but got side tracked by actual work...

Your analysis reproduces the SPSS unscaled estimates. It still remains to figure out how Nick got

> 
coefficients(lm(ZDEPRESSION ~ ZMEAN_PA * ZDIVERSITY_PA, data=s1))

           (Intercept)               ZMEAN_PA          ZDIVERSITY_PA ZMEAN_PA:ZDIVERSITY_PA 
            0.07342198            -0.39650356            -0.36569488            -0.09435788 


which does not match your output. I suspect that ZMEAN_PA and ZDIVERSITY_PA were scaled for this analysis (but the interaction term still obviously is not). I conjecture that something in the vicinity of

res <- lm(DEPRESSION ~ scale(ZMEAN_PA) + scale(ZDIVERSITY_PA) + scale(ZMEAN_PA * ZDIVERSITY_PA), data=dat)
summary(res)

would reproduce the SPSS Beta values.


> On 5 May 2017, at 14:43 , Viechtbauer Wolfgang (SP) <wolfgang.viechtbauer at maastrichtuniversity.nl> wrote:
> 
> I had no problems running regression models in SPSS and R that yielded the same results for these data.
> 
> The difference you are observing is from fitting different models. In R, you fitted:
> 
> res <- lm(DEPRESSION ~ ZMEAN_PA * ZDIVERSITY_PA, data=dat)
> summary(res)
> 
> The interaction term is the product of ZMEAN_PA and ZDIVERSITY_PA. This is not a standardized variable itself and not the same as "ZINTER_PA_C" in the png you showed, which is not a variable in the dataset, but can be created with:
> 
> dat$ZINTER_PA_C <- with(dat, scale(ZMEAN_PA * ZDIVERSITY_PA))
> 
> If you want the same results as in SPSS, then you need to fit:
> 
> res <- lm(DEPRESSION ~ ZMEAN_PA + ZDIVERSITY_PA + ZINTER_PA_C, data=dat)
> summary(res)
> 
> This yields:
> 
> Coefficients:
>              Estimate Std. Error t value Pr(>|t|)    
> (Intercept)    6.41041    0.01722  372.21   <2e-16 ***
> ZMEAN_PA      -1.62726    0.04200  -38.74   <2e-16 ***
> ZDIVERSITY_PA -1.50082    0.07447  -20.15   <2e-16 ***
> ZINTER_PA_C   -0.58955    0.05288  -11.15   <2e-16 ***
> ---
> Signif. codes:  0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> 
> Exactly the same as in the png.
> 
> Peter already mentioned this as a possible reason for the discrepancy: https://stat.ethz.ch/pipermail/r-devel/2017-May/074191.html ("Is it perhaps the case that x1 and x2 have already been scaled to have standard deviation 1? In that case, x1*x2 won't be.")
> 
> Best,
> Wolfgang
> 
> -----Original Message-----
> From: R-devel [mailto:r-devel-bounces at r-project.org] On Behalf Of Nick Brown
> Sent: Friday, May 05, 2017 10:40
> To: peter dalgaard
> Cc: r-devel at r-project.org
> Subject: Re: [Rd] lm() gives different results to lm.ridge() and SPSS
> 
> Hi, 
> 
> Here is (I hope) all the relevant output from R. 
> 
>> mean(s1$ZDEPRESSION, na.rm=T) [1] -1.041546e-16 > mean(s1$ZDIVERSITY_PA, na.rm=T) [1] -9.660583e-16 > mean(s1$ZMEAN_PA, na.rm=T) [1] -5.430282e-15 > lm.ridge(ZDEPRESSION ~ ZMEAN_PA * ZDIVERSITY_PA, data=s1)$coef ZMEAN_PA          ZDIVERSITY_PA ZMEAN_PA:ZDIVERSITY_PA 
>            -0.3962254             -0.3636026             -0.1425772      ## This is what I thought was the problem originally. :-) 
> 
> 
>> coefficients(lm(ZDEPRESSION ~ ZMEAN_PA * ZDIVERSITY_PA, data=s1)) (Intercept)               ZMEAN_PA          ZDIVERSITY_PA ZMEAN_PA:ZDIVERSITY_PA 
>            0.07342198            -0.39650356            -0.36569488            -0.09435788 > coefficients(lm.ridge(ZDEPRESSION ~ ZMEAN_PA * ZDIVERSITY_PA, data=s1)) ZMEAN_PA          ZDIVERSITY_PA ZMEAN_PA:ZDIVERSITY_PA 
>            0.07342198            -0.39650356            -0.36569488            -0.09435788 The equivalent from SPSS is attached. The unstandardized coefficients in SPSS look nothing like those in R. The standardized coefficients in SPSS match the lm.ridge()$coef numbers very closely indeed, suggesting that the same algorithm may be in use. 
> 
> I have put the dataset file, which is the untouched original I received from the authors, in this Dropbox folder: https://www.dropbox.com/sh/xsebjy55ius1ysb/AADwYUyV1bl6-iAw7ACuF1_La?dl=0. You can read it into R with this code (one variable needs to be standardized and centered; everything else is already in the file): 
> 
> s1 <- read.csv("Emodiversity_Study1.csv", stringsAsFactors=FALSE) s1$ZDEPRESSION <- scale(s1$DEPRESSION) 
> Hey, maybe R is fine and I've stumbled on a bug in SPSS? If so, I'm sure IBM will want to fix it quickly (ha ha ha). 
> 
> Nick 
> 
> ----- Original Message -----
> 
> From: "peter dalgaard" <pdalgd at gmail.com> 
> To: "Nick Brown" <nick.brown at free.fr> 
> Cc: "Simon Bonner" <sbonner6 at uwo.ca>, r-devel at r-project.org 
> Sent: Friday, 5 May, 2017 10:02:10 AM 
> Subject: Re: [Rd] lm() gives different results to lm.ridge() and SPSS 
> 
> I asked you before, but in case you missed it: Are you looking at the right place in SPSS output? 
> 
> The UNstandardized coefficients should be comparable to R, i.e. the "B" column, not "Beta". 
> 
> -pd 
> 
>> On 5 May 2017, at 01:58 , Nick Brown <nick.brown at free.fr> wrote: 
>> 
>> Hi Simon, 
>> 
>> Yes, if I uses coefficients() I get the same results for lm() and lm.ridge(). So that's consistent, at least. 
>> 
>> Interestingly, the "wrong" number I get from lm.ridge()$coef agrees with the value from SPSS to 5dp, which is an interesting coincidence if these numbers have no particular external meaning in lm.ridge(). 
>> 
>> Kind regards, 
>> Nick 
>> 
>> ----- Original Message ----- 
>> 
>> From: "Simon Bonner" <sbonner6 at uwo.ca> 
>> To: "Nick Brown" <nick.brown at free.fr>, r-devel at r-project.org 
>> Sent: Thursday, 4 May, 2017 7:07:33 PM 
>> Subject: RE: [Rd] lm() gives different results to lm.ridge() and SPSS 
>> 
>> Hi Nick, 
>> 
>> I think that the problem here is your use of $coef to extract the coefficients of the ridge regression. The help for lm.ridge states that coef is a "matrix of coefficients, one row for each value of lambda. Note that these are not on the original scale and are for use by the coef method." 
>> 
>> I ran a small test with simulated data, code is copied below, and indeed the output from lm.ridge differs depending on whether the coefficients are accessed via $coef or via the coefficients() function. The latter does produce results that match the output from lm. 
>> 
>> I hope that helps. 
>> 
>> Cheers, 
>> 
>> Simon 
>> 
>> ## Load packages 
>> library(MASS) 
>> 
>> ## Set seed 
>> set.seed(8888) 
>> 
>> ## Set parameters 
>> n <- 100 
>> beta <- c(1,0,1) 
>> sigma <- .5 
>> rho <- .75 
>> 
>> ## Simulate correlated covariates 
>> Sigma <- matrix(c(1,rho,rho,1),ncol=2) 
>> X <- mvrnorm(n,c(0,0),Sigma=Sigma) 
>> 
>> ## Simulate data 
>> mu <- beta[1] + X %*% beta[-1] 
>> y <- rnorm(n,mu,sigma) 
>> 
>> ## Fit model with lm() 
>> fit1 <- lm(y ~ X) 
>> 
>> ## Fit model with lm.ridge() 
>> fit2 <- lm.ridge(y ~ X) 
>> 
>> ## Compare coefficients 
>> cbind(fit1$coefficients,c(NA,fit2$coef),coefficients(fit2)) 
>> 
>> [,1] [,2] [,3] 
>> (Intercept) 0.99276001 NA 0.99276001 
>> X1 -0.03980772 -0.04282391 -0.03980772 
>> X2 1.11167179 1.06200476 1.11167179 
>> 
>> -- 
>> 
>> Simon Bonner 
>> Assistant Professor of Environmetrics/ Director MMASc 
>> Department of Statistical and Actuarial Sciences/Department of Biology 
>> University of Western Ontario 
>> 
>> Office: Western Science Centre rm 276 
>> 
>> Email: sbonner6 at uwo.ca | Telephone: 519-661-2111 x88205 | Fax: 519-661-3813 
>> Twitter: @bonnerstatslab | Website: http://simon.bonners.ca/bonner-lab/wpblog/ 
>> 
>>> -----Original Message----- 
>>> From: R-devel [mailto:r-devel-bounces at r-project.org] On Behalf Of Nick 
>>> Brown 
>>> Sent: May 4, 2017 10:29 AM 
>>> To: r-devel at r-project.org 
>>> Subject: [Rd] lm() gives different results to lm.ridge() and SPSS 
>>> 
>>> Hallo, 
>>> 
>>> I hope I am posting to the right place. I was advised to try this list by Ben Bolker 
>>> (https://twitter.com/bolkerb/status/859909918446497795). I also posted this 
>>> question to StackOverflow 
>>> (http://stackoverflow.com/questions/43771269/lm-gives-different-results- 
>>> from-lm-ridgelambda-0). I am a relative newcomer to R, but I wrote my first 
>>> program in 1975 and have been paid to program in about 15 different 
>>> languages, so I have some general background knowledge. 
>>> 
>>> I have a regression from which I extract the coefficients like this: 
>>> lm(y ~ x1 * x2, data=ds)$coef 
>>> That gives: x1=0.40, x2=0.37, x1*x2=0.09 
>>> 
>>> When I do the same regression in SPSS, I get: 
>>> beta(x1)=0.40, beta(x2)=0.37, beta(x1*x2)=0.14. 
>>> So the main effects are in agreement, but there is quite a difference in the 
>>> coefficient for the interaction. 
>>> 
>>> X1 and X2 are correlated about .75 (yes, yes, I know - this model wasn't my 
>>> idea, but it got published), so there is quite possibly something going on with 
>>> collinearity. So I thought I'd try lm.ridge() to see if I can get an idea of where 
>>> the problems are occurring. 
>>> 
>>> The starting point is to run lm.ridge() with lambda=0 (i.e., no ridge penalty) and 
>>> check we get the same results as with lm(): 
>>> lm.ridge(y ~ x1 * x2, lambda=0, data=ds)$coef 
>>> x1=0.40, x2=0.37, x1*x2=0.14 
>>> So lm.ridge() agrees with SPSS, but not with lm(). (Of course, lambda=0 is the 
>>> default, so it can be omitted; I can alternate between including or deleting 
>>> ".ridge" in the function call, and watch the coefficient for the interaction 
>>> change.) 
>>> 
>>> What seems slightly strange to me here is that I assumed that lm.ridge() just 
>>> piggybacks on lm() anyway, so in the specific case where lambda=0 and there 
>>> is no "ridging" to do, I'd expect exactly the same results. 
>>> 
>>> Unfortunately there are 34,000 cases in the dataset, so a "minimal" reprex will 
>>> not be easy to make, but I can share the data via Dropbox or something if that 
>>> would help. 
>>> 
>>> I appreciate that when there is strong collinearity then all bets are off in terms 
>>> of what the betas mean, but I would really expect lm() and lm.ridge() to give 
>>> the same results. (I would be happy to ignore SPSS, but for the moment it's 
>>> part of the majority!) 
>>> 
>>> Thanks for reading, 
>>> Nick 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From nick.brown at free.fr  Fri May  5 15:40:59 2017
From: nick.brown at free.fr (Nick Brown)
Date: Fri, 5 May 2017 15:40:59 +0200 (CEST)
Subject: [Rd] lm() gives different results to lm.ridge() and SPSS
In-Reply-To: <15506037-17F7-4B45-9299-327BBCB27347@gmail.com>
Message-ID: <1426740519.184137599.1493991659840.JavaMail.root@zimbra61-e11.priv.proxad.net>

>I conjecture that something in the vicinity of 
> res <- lm(DEPRESSION ~ scale(ZMEAN_PA) + scale(ZDIVERSITY_PA) + scale(ZMEAN_PA * ZDIVERSITY_PA), data=dat) 
>summary(res) 
> would reproduce the SPSS Beta values. 

Yes, that works. Thanks! 

----- Original Message -----

From: "peter dalgaard" <pdalgd at gmail.com> 
To: "Viechtbauer Wolfgang (SP)" <wolfgang.viechtbauer at maastrichtuniversity.nl>, "Nick Brown" <nick.brown at free.fr> 
Cc: r-devel at r-project.org 
Sent: Friday, 5 May, 2017 3:33:29 PM 
Subject: Re: [Rd] lm() gives different results to lm.ridge() and SPSS 

Thanks, I was getting to try this, but got side tracked by actual work... 

Your analysis reproduces the SPSS unscaled estimates. It still remains to figure out how Nick got 

> 
coefficients(lm(ZDEPRESSION ~ ZMEAN_PA * ZDIVERSITY_PA, data=s1)) 

(Intercept) ZMEAN_PA ZDIVERSITY_PA ZMEAN_PA:ZDIVERSITY_PA 
0.07342198 -0.39650356 -0.36569488 -0.09435788 


which does not match your output. I suspect that ZMEAN_PA and ZDIVERSITY_PA were scaled for this analysis (but the interaction term still obviously is not). I conjecture that something in the vicinity of 

res <- lm(DEPRESSION ~ scale(ZMEAN_PA) + scale(ZDIVERSITY_PA) + scale(ZMEAN_PA * ZDIVERSITY_PA), data=dat) 
summary(res) 

would reproduce the SPSS Beta values. 


> On 5 May 2017, at 14:43 , Viechtbauer Wolfgang (SP) <wolfgang.viechtbauer at maastrichtuniversity.nl> wrote: 
> 
> I had no problems running regression models in SPSS and R that yielded the same results for these data. 
> 
> The difference you are observing is from fitting different models. In R, you fitted: 
> 
> res <- lm(DEPRESSION ~ ZMEAN_PA * ZDIVERSITY_PA, data=dat) 
> summary(res) 
> 
> The interaction term is the product of ZMEAN_PA and ZDIVERSITY_PA. This is not a standardized variable itself and not the same as "ZINTER_PA_C" in the png you showed, which is not a variable in the dataset, but can be created with: 
> 
> dat$ZINTER_PA_C <- with(dat, scale(ZMEAN_PA * ZDIVERSITY_PA)) 
> 
> If you want the same results as in SPSS, then you need to fit: 
> 
> res <- lm(DEPRESSION ~ ZMEAN_PA + ZDIVERSITY_PA + ZINTER_PA_C, data=dat) 
> summary(res) 
> 
> This yields: 
> 
> Coefficients: 
> Estimate Std. Error t value Pr(>|t|) 
> (Intercept) 6.41041 0.01722 372.21 <2e-16 *** 
> ZMEAN_PA -1.62726 0.04200 -38.74 <2e-16 *** 
> ZDIVERSITY_PA -1.50082 0.07447 -20.15 <2e-16 *** 
> ZINTER_PA_C -0.58955 0.05288 -11.15 <2e-16 *** 
> --- 
> Signif. codes: 0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 
> 
> Exactly the same as in the png. 
> 
> Peter already mentioned this as a possible reason for the discrepancy: https://stat.ethz.ch/pipermail/r-devel/2017-May/074191.html ("Is it perhaps the case that x1 and x2 have already been scaled to have standard deviation 1? In that case, x1*x2 won't be.") 
> 
> Best, 
> Wolfgang 
> 
> -----Original Message----- 
> From: R-devel [mailto:r-devel-bounces at r-project.org] On Behalf Of Nick Brown 
> Sent: Friday, May 05, 2017 10:40 
> To: peter dalgaard 
> Cc: r-devel at r-project.org 
> Subject: Re: [Rd] lm() gives different results to lm.ridge() and SPSS 
> 
> Hi, 
> 
> Here is (I hope) all the relevant output from R. 
> 
>> mean(s1$ZDEPRESSION, na.rm=T) [1] -1.041546e-16 > mean(s1$ZDIVERSITY_PA, na.rm=T) [1] -9.660583e-16 > mean(s1$ZMEAN_PA, na.rm=T) [1] -5.430282e-15 > lm.ridge(ZDEPRESSION ~ ZMEAN_PA * ZDIVERSITY_PA, data=s1)$coef ZMEAN_PA ZDIVERSITY_PA ZMEAN_PA:ZDIVERSITY_PA 
> -0.3962254 -0.3636026 -0.1425772 ## This is what I thought was the problem originally. :-) 
> 
> 
>> coefficients(lm(ZDEPRESSION ~ ZMEAN_PA * ZDIVERSITY_PA, data=s1)) (Intercept) ZMEAN_PA ZDIVERSITY_PA ZMEAN_PA:ZDIVERSITY_PA 
> 0.07342198 -0.39650356 -0.36569488 -0.09435788 > coefficients(lm.ridge(ZDEPRESSION ~ ZMEAN_PA * ZDIVERSITY_PA, data=s1)) ZMEAN_PA ZDIVERSITY_PA ZMEAN_PA:ZDIVERSITY_PA 
> 0.07342198 -0.39650356 -0.36569488 -0.09435788 The equivalent from SPSS is attached. The unstandardized coefficients in SPSS look nothing like those in R. The standardized coefficients in SPSS match the lm.ridge()$coef numbers very closely indeed, suggesting that the same algorithm may be in use. 
> 
> I have put the dataset file, which is the untouched original I received from the authors, in this Dropbox folder: https://www.dropbox.com/sh/xsebjy55ius1ysb/AADwYUyV1bl6-iAw7ACuF1_La?dl=0. You can read it into R with this code (one variable needs to be standardized and centered; everything else is already in the file): 
> 
> s1 <- read.csv("Emodiversity_Study1.csv", stringsAsFactors=FALSE) s1$ZDEPRESSION <- scale(s1$DEPRESSION) 
> Hey, maybe R is fine and I've stumbled on a bug in SPSS? If so, I'm sure IBM will want to fix it quickly (ha ha ha). 
> 
> Nick 
> 
> ----- Original Message ----- 
> 
> From: "peter dalgaard" <pdalgd at gmail.com> 
> To: "Nick Brown" <nick.brown at free.fr> 
> Cc: "Simon Bonner" <sbonner6 at uwo.ca>, r-devel at r-project.org 
> Sent: Friday, 5 May, 2017 10:02:10 AM 
> Subject: Re: [Rd] lm() gives different results to lm.ridge() and SPSS 
> 
> I asked you before, but in case you missed it: Are you looking at the right place in SPSS output? 
> 
> The UNstandardized coefficients should be comparable to R, i.e. the "B" column, not "Beta". 
> 
> -pd 
> 
>> On 5 May 2017, at 01:58 , Nick Brown <nick.brown at free.fr> wrote: 
>> 
>> Hi Simon, 
>> 
>> Yes, if I uses coefficients() I get the same results for lm() and lm.ridge(). So that's consistent, at least. 
>> 
>> Interestingly, the "wrong" number I get from lm.ridge()$coef agrees with the value from SPSS to 5dp, which is an interesting coincidence if these numbers have no particular external meaning in lm.ridge(). 
>> 
>> Kind regards, 
>> Nick 
>> 
>> ----- Original Message ----- 
>> 
>> From: "Simon Bonner" <sbonner6 at uwo.ca> 
>> To: "Nick Brown" <nick.brown at free.fr>, r-devel at r-project.org 
>> Sent: Thursday, 4 May, 2017 7:07:33 PM 
>> Subject: RE: [Rd] lm() gives different results to lm.ridge() and SPSS 
>> 
>> Hi Nick, 
>> 
>> I think that the problem here is your use of $coef to extract the coefficients of the ridge regression. The help for lm.ridge states that coef is a "matrix of coefficients, one row for each value of lambda. Note that these are not on the original scale and are for use by the coef method." 
>> 
>> I ran a small test with simulated data, code is copied below, and indeed the output from lm.ridge differs depending on whether the coefficients are accessed via $coef or via the coefficients() function. The latter does produce results that match the output from lm. 
>> 
>> I hope that helps. 
>> 
>> Cheers, 
>> 
>> Simon 
>> 
>> ## Load packages 
>> library(MASS) 
>> 
>> ## Set seed 
>> set.seed(8888) 
>> 
>> ## Set parameters 
>> n <- 100 
>> beta <- c(1,0,1) 
>> sigma <- .5 
>> rho <- .75 
>> 
>> ## Simulate correlated covariates 
>> Sigma <- matrix(c(1,rho,rho,1),ncol=2) 
>> X <- mvrnorm(n,c(0,0),Sigma=Sigma) 
>> 
>> ## Simulate data 
>> mu <- beta[1] + X %*% beta[-1] 
>> y <- rnorm(n,mu,sigma) 
>> 
>> ## Fit model with lm() 
>> fit1 <- lm(y ~ X) 
>> 
>> ## Fit model with lm.ridge() 
>> fit2 <- lm.ridge(y ~ X) 
>> 
>> ## Compare coefficients 
>> cbind(fit1$coefficients,c(NA,fit2$coef),coefficients(fit2)) 
>> 
>> [,1] [,2] [,3] 
>> (Intercept) 0.99276001 NA 0.99276001 
>> X1 -0.03980772 -0.04282391 -0.03980772 
>> X2 1.11167179 1.06200476 1.11167179 
>> 
>> -- 
>> 
>> Simon Bonner 
>> Assistant Professor of Environmetrics/ Director MMASc 
>> Department of Statistical and Actuarial Sciences/Department of Biology 
>> University of Western Ontario 
>> 
>> Office: Western Science Centre rm 276 
>> 
>> Email: sbonner6 at uwo.ca | Telephone: 519-661-2111 x88205 | Fax: 519-661-3813 
>> Twitter: @bonnerstatslab | Website: http://simon.bonners.ca/bonner-lab/wpblog/ 
>> 
>>> -----Original Message----- 
>>> From: R-devel [mailto:r-devel-bounces at r-project.org] On Behalf Of Nick 
>>> Brown 
>>> Sent: May 4, 2017 10:29 AM 
>>> To: r-devel at r-project.org 
>>> Subject: [Rd] lm() gives different results to lm.ridge() and SPSS 
>>> 
>>> Hallo, 
>>> 
>>> I hope I am posting to the right place. I was advised to try this list by Ben Bolker 
>>> (https://twitter.com/bolkerb/status/859909918446497795). I also posted this 
>>> question to StackOverflow 
>>> (http://stackoverflow.com/questions/43771269/lm-gives-different-results- 
>>> from-lm-ridgelambda-0). I am a relative newcomer to R, but I wrote my first 
>>> program in 1975 and have been paid to program in about 15 different 
>>> languages, so I have some general background knowledge. 
>>> 
>>> I have a regression from which I extract the coefficients like this: 
>>> lm(y ~ x1 * x2, data=ds)$coef 
>>> That gives: x1=0.40, x2=0.37, x1*x2=0.09 
>>> 
>>> When I do the same regression in SPSS, I get: 
>>> beta(x1)=0.40, beta(x2)=0.37, beta(x1*x2)=0.14. 
>>> So the main effects are in agreement, but there is quite a difference in the 
>>> coefficient for the interaction. 
>>> 
>>> X1 and X2 are correlated about .75 (yes, yes, I know - this model wasn't my 
>>> idea, but it got published), so there is quite possibly something going on with 
>>> collinearity. So I thought I'd try lm.ridge() to see if I can get an idea of where 
>>> the problems are occurring. 
>>> 
>>> The starting point is to run lm.ridge() with lambda=0 (i.e., no ridge penalty) and 
>>> check we get the same results as with lm(): 
>>> lm.ridge(y ~ x1 * x2, lambda=0, data=ds)$coef 
>>> x1=0.40, x2=0.37, x1*x2=0.14 
>>> So lm.ridge() agrees with SPSS, but not with lm(). (Of course, lambda=0 is the 
>>> default, so it can be omitted; I can alternate between including or deleting 
>>> ".ridge" in the function call, and watch the coefficient for the interaction 
>>> change.) 
>>> 
>>> What seems slightly strange to me here is that I assumed that lm.ridge() just 
>>> piggybacks on lm() anyway, so in the specific case where lambda=0 and there 
>>> is no "ridging" to do, I'd expect exactly the same results. 
>>> 
>>> Unfortunately there are 34,000 cases in the dataset, so a "minimal" reprex will 
>>> not be easy to make, but I can share the data via Dropbox or something if that 
>>> would help. 
>>> 
>>> I appreciate that when there is strong collinearity then all bets are off in terms 
>>> of what the betas mean, but I would really expect lm() and lm.ridge() to give 
>>> the same results. (I would be happy to ignore SPSS, but for the moment it's 
>>> part of the majority!) 
>>> 
>>> Thanks for reading, 
>>> Nick 
> ______________________________________________ 
> R-devel at r-project.org mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-devel 

-- 
Peter Dalgaard, Professor, 
Center for Statistics, Copenhagen Business School 
Solbjerg Plads 3, 2000 Frederiksberg, Denmark 
Phone: (+45)38153501 
Office: A 4.23 
Email: pd.mes at cbs.dk Priv: PDalgd at gmail.com 











	[[alternative HTML version deleted]]


From antonink at idi.ntnu.no  Fri May  5 19:00:09 2017
From: antonink at idi.ntnu.no (Antonin Klima)
Date: Fri, 5 May 2017 19:00:09 +0200
Subject: [Rd] A few suggestions and perspectives from a PhD student
Message-ID: <FCBC5362-8436-487D-95A0-65CB5924BD29@idi.ntnu.no>

Dear Sir or Madam,

I am in 2nd year of my PhD in bioinformatics, after taking my Master?s in computer science, and have been using R heavily during my PhD. As such, I have put together a list of certain features in R that, in my opinion, would be beneficial to add, or could be improved. The first two are already implemented in packages, but given that it is implemented as user-defined operators, it greatly restricts its usefulness. I hope you will find my suggestions interesting. If you find time, I will welcome any feedback as to whether you find the suggestions useful, or why you do not think they should be implemented. I will also welcome if you enlighten me with any features I might be unaware of, that might solve the issues I have pointed out below.

1) piping
Currently available in package magrittr, piping makes the code better readable by having the line start at its natural starting point, and following with functions that are applied - in order. The readability of several nested calls with a number of parameters each is almost zero, it?s almost as if one would need to come up with the solution himself. Pipeline in comparison is very straightforward, especially together with the point (2).

The package here works rather good nevertheless, the shortcomings of piping not being native are not quite as severe as in point (2). Nevertheless, an intuitive symbol such as | would be helpful, and it sometimes bothers me that I have to parenthesize anonymous function, which would probably not be required in a native pipe-operator, much like it is not required in f.ex. lapply. That is,
1:5 %>% function(x) x+2
should be totally fine

2) currying
Currently available in package Curry. The idea is that, having a function such as foo = function(x, y) x+y, one would like to write for example lapply(foo(3), 1:5), and have the interpreter figure out ok, foo(3) does not make a value result, but it can still give a function result - a function of y. This would be indeed most useful for various apply functions, rather than writing function(x) foo(3,x).

I suggest that currying would make the code easier to write, and more readable, especially when using apply functions. One might imagine that there could be some confusion with such a feature, especially from people unfamiliar with functional programming, although R already does take function as first-order arguments, so it could be just fine. But one could address it with special syntax, such as $foo(3) [$foo(x=3)] for partial application.  The current currying package has very limited usefulness, as, being limited by the user-defined operator framework, it only rarely can contribute to less code/more readability. Compare yourself:
$foo(x=3) vs foo %<% 3
goo = function(a,b,c)
$goo(b=3) vs goo %><% list(b=3)

Moreover, one would often like currying to have highest priority. For example, when piping:
data %>% foo %>% foo1 %<% 3
if one wants to do data %>% foo %>% $foo(x=3)

3) Code executable only when running the script itself
Whereas the first two suggestions are somewhat stealing from Haskell and the like, this suggestion would be stealing from Python. I?m building quite a complicated pipeline, using S4 classes. After defining the class and its methods, I also define how to build the class to my likings, based on my input data, using various now-defined methods. So I end up having a list of command line arguments to process, and the way to create the class instance based on them. If I write it to the class file, however, I end up running the code when it is sourced from the next step in the pipeline, that needs the previous class definitions.

A feature such as pythonic ?if __name__ == __main__? would thus be useful. As it is, I had to create run scripts as separate files. Which is actually not so terrible, given the class and its methods often span a few hundred lines, but still.

4) non-exported global variables
I also find it lacking, that I seem to be unable to create constants that would not get passed to files that source the class definition. That is, if class1 features global constant CONSTANT=3, then if class2 sources class1, it will also include the constant. This 1) clutters the namespace when running the code interactively, 2) potentially overwrites the constants in case of nameclash. Some kind of export/nonexport variable syntax, or symbolic import, or namespace would be useful. I know if I converted it to a package I would get at least something like a namespace, but still.

I understand that the variable cannot just not be imported, in general, as the functions will generally rely on it (otherwise it wouldn?t have to be there). But one could consider hiding it in an implicit namespace for the file, for example.

5) S4 methods with same name, for different classes
Say I have an S4 class called datasetSingle, and another S4 class called datasetMulti, which gathers up a number of datasetSingle classes, and adds some extra functionality on top. The datasetSingle class may have a method replicates, that returns a named vector assigning replicate number to experiment names of the dataset. But I would also like to have a function with the same name for the datasetMulti class, that returns for data frame, or list, covering replicate numbers for all the datasets included.

But then, I need to setGeneric for the method. But if I set generic before both implementations, I will reset the generic in the second call, losing the definition for ?replicates? for datasetSingle. Skipping this in the code for datasetMulti means that 1) I have to remember that I had the function defined for datasetSingle, 2) if I remove the function or change its name in datasetSingle, I now have to change the datasetMulti class file too. Moreover, if I would like to have a different generic for the datasetMulti version, I have to change it not in datasetMulti class file, but in the datasetSingle file, where it might not make much sense. In this case, I wanted to have another argument ?datasets?, which would return the replicates only for the datasets specified, rather than for all.

I made a wrapper that could circumvent the first issue, but the second issue is not easy to circumvent.

6) Many parameters freeze S4 method calls
If I specify ca over 6 parameters for an S4 method, I would often get a ?freeze? on the method call. The process would eat up a lot of memory before going into the call, upon which it would execute the call as normal (if it didn?t run out of memory or I didn?t run out of patience). Subsequent calls of the method would not include this overhead. The amount of memory this could take could be in gigabytes, and the time in minutes. I suspect this might be due to generating an entry in call table for each accepted signature. It can be circumvented, but sure isn?t a behaviour one would expect.

7) Default values for S4 methods
It would seem that it is not possible to set up default parameters for an S4 method in a usual way of definiton = function (x, y=5). I resorted to making class unions with ?missing? for signatures on the call, with the call starting with if(missing(param)) param=DEFAULT_VALUE, but it certainly does not improve readability or ease of coding.


Thank you for your time if you have finished reading thus far. :) Looking forward to any answer.

Yours Sincerely,
Antonin Klima


From istazahn at gmail.com  Fri May  5 19:55:00 2017
From: istazahn at gmail.com (Ista Zahn)
Date: Fri, 5 May 2017 13:55:00 -0400
Subject: [Rd] A few suggestions and perspectives from a PhD student
In-Reply-To: <FCBC5362-8436-487D-95A0-65CB5924BD29@idi.ntnu.no>
References: <FCBC5362-8436-487D-95A0-65CB5924BD29@idi.ntnu.no>
Message-ID: <CA+vqiLFQG+f_jZSZvuJNoSky66fgO4EXxwR6HY_D+64HMxktyA@mail.gmail.com>

On Fri, May 5, 2017 at 1:00 PM, Antonin Klima <antonink at idi.ntnu.no> wrote:
> Dear Sir or Madam,
>
> I am in 2nd year of my PhD in bioinformatics, after taking my Master?s in computer science, and have been using R heavily during my PhD. As such, I have put together a list of certain features in R that, in my opinion, would be beneficial to add, or could be improved. The first two are already implemented in packages, but given that it is implemented as user-defined operators, it greatly restricts its usefulness.

Why do you think being implemented in a contributed package restricts
the usefulness of a feature?

I hope you will find my suggestions interesting. If you find time, I
will welcome any feedback as to whether you find the suggestions
useful, or why you do not think they should be implemented. I will
also welcome if you enlighten me with any features I might be unaware
of, that might solve the issues I have pointed out below.
>
> 1) piping
> Currently available in package magrittr, piping makes the code better readable by having the line start at its natural starting point, and following with functions that are applied - in order. The readability of several nested calls with a number of parameters each is almost zero, it?s almost as if one would need to come up with the solution himself. Pipeline in comparison is very straightforward, especially together with the point (2).

You may be surprised to learn that not everyone thinks pipes are a
good idea. Personally I see some advantages, but there is also a big
downside with is that they mess up the call stack and make tracking
down errors via traceback() more difficult.

There is a simple alternative to pipes already built in to R that
gives you some of the advantages of %>% without messing up the call
stack.  Using Hadley's famous "little bunny foo foo" example:

foo_foo <- little_bunny()

## nesting (it is rough)
bop(
  scoop(
    hop(foo_foo, through = forest),
    up = field_mice
  ),
  on = head
)

## magrittr
foo_foo %>%
  hop(through = forest) %>%
  scoop(up = field_mouse) %>%
  bop(on = head)

## regular R assignment
foo_foo -> .
  hop(., through = forest) -> .
  scoop(., up = field_mouse) -> .
  bop(., on = head)

This is more limited that magrittr's %>%, but it gives you a lot of
the advantages without the disadvantages.

>
> The package here works rather good nevertheless, the shortcomings of piping not being native are not quite as severe as in point (2). Nevertheless, an intuitive symbol such as | would be helpful, and it sometimes bothers me that I have to parenthesize anonymous function, which would probably not be required in a native pipe-operator, much like it is not required in f.ex. lapply. That is,
> 1:5 %>% function(x) x+2
> should be totally fine

That seems pretty small-potatoes to me.

>
> 2) currying
> Currently available in package Curry. The idea is that, having a function such as foo = function(x, y) x+y, one would like to write for example lapply(foo(3), 1:5), and have the interpreter figure out ok, foo(3) does not make a value result, but it can still give a function result - a function of y. This would be indeed most useful for various apply functions, rather than writing function(x) foo(3,x).

You can already do

lapply(1:5, foo, y = 3)

(assuming that the first argument to foo is named "y")

I'm stopping here since I don't have anything useful to say about your
subsequent points.

Best,
Ista

>
> I suggest that currying would make the code easier to write, and more readable, especially when using apply functions. One might imagine that there could be some confusion with such a feature, especially from people unfamiliar with functional programming, although R already does take function as first-order arguments, so it could be just fine. But one could address it with special syntax, such as $foo(3) [$foo(x=3)] for partial application.  The current currying package has very limited usefulness, as, being limited by the user-defined operator framework, it only rarely can contribute to less code/more readability. Compare yourself:
> $foo(x=3) vs foo %<% 3
> goo = function(a,b,c)
> $goo(b=3) vs goo %><% list(b=3)
>
> Moreover, one would often like currying to have highest priority. For example, when piping:
> data %>% foo %>% foo1 %<% 3
> if one wants to do data %>% foo %>% $foo(x=3)
>
> 3) Code executable only when running the script itself
> Whereas the first two suggestions are somewhat stealing from Haskell and the like, this suggestion would be stealing from Python. I?m building quite a complicated pipeline, using S4 classes. After defining the class and its methods, I also define how to build the class to my likings, based on my input data, using various now-defined methods. So I end up having a list of command line arguments to process, and the way to create the class instance based on them. If I write it to the class file, however, I end up running the code when it is sourced from the next step in the pipeline, that needs the previous class definitions.
>
> A feature such as pythonic ?if __name__ == __main__? would thus be useful. As it is, I had to create run scripts as separate files. Which is actually not so terrible, given the class and its methods often span a few hundred lines, but still.
>
> 4) non-exported global variables
> I also find it lacking, that I seem to be unable to create constants that would not get passed to files that source the class definition. That is, if class1 features global constant CONSTANT=3, then if class2 sources class1, it will also include the constant. This 1) clutters the namespace when running the code interactively, 2) potentially overwrites the constants in case of nameclash. Some kind of export/nonexport variable syntax, or symbolic import, or namespace would be useful. I know if I converted it to a package I would get at least something like a namespace, but still.
>
> I understand that the variable cannot just not be imported, in general, as the functions will generally rely on it (otherwise it wouldn?t have to be there). But one could consider hiding it in an implicit namespace for the file, for example.
>
> 5) S4 methods with same name, for different classes
> Say I have an S4 class called datasetSingle, and another S4 class called datasetMulti, which gathers up a number of datasetSingle classes, and adds some extra functionality on top. The datasetSingle class may have a method replicates, that returns a named vector assigning replicate number to experiment names of the dataset. But I would also like to have a function with the same name for the datasetMulti class, that returns for data frame, or list, covering replicate numbers for all the datasets included.
>
> But then, I need to setGeneric for the method. But if I set generic before both implementations, I will reset the generic in the second call, losing the definition for ?replicates? for datasetSingle. Skipping this in the code for datasetMulti means that 1) I have to remember that I had the function defined for datasetSingle, 2) if I remove the function or change its name in datasetSingle, I now have to change the datasetMulti class file too. Moreover, if I would like to have a different generic for the datasetMulti version, I have to change it not in datasetMulti class file, but in the datasetSingle file, where it might not make much sense. In this case, I wanted to have another argument ?datasets?, which would return the replicates only for the datasets specified, rather than for all.
>
> I made a wrapper that could circumvent the first issue, but the second issue is not easy to circumvent.
>
> 6) Many parameters freeze S4 method calls
> If I specify ca over 6 parameters for an S4 method, I would often get a ?freeze? on the method call. The process would eat up a lot of memory before going into the call, upon which it would execute the call as normal (if it didn?t run out of memory or I didn?t run out of patience). Subsequent calls of the method would not include this overhead. The amount of memory this could take could be in gigabytes, and the time in minutes. I suspect this might be due to generating an entry in call table for each accepted signature. It can be circumvented, but sure isn?t a behaviour one would expect.
>
> 7) Default values for S4 methods
> It would seem that it is not possible to set up default parameters for an S4 method in a usual way of definiton = function (x, y=5). I resorted to making class unions with ?missing? for signatures on the call, with the call starting with if(missing(param)) param=DEFAULT_VALUE, but it certainly does not improve readability or ease of coding.
>
>
> Thank you for your time if you have finished reading thus far. :) Looking forward to any answer.
>
> Yours Sincerely,
> Antonin Klima
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From jfox at mcmaster.ca  Fri May  5 20:22:53 2017
From: jfox at mcmaster.ca (Fox, John)
Date: Fri, 5 May 2017 18:22:53 +0000
Subject: [Rd] lm() gives different results to lm.ridge() and SPSS
In-Reply-To: <11345_1493991675_v45DfEL9003599_1426740519.184137599.1493991659840.JavaMail.root@zimbra61-e11.priv.proxad.net>
References: <15506037-17F7-4B45-9299-327BBCB27347@gmail.com>
 <11345_1493991675_v45DfEL9003599_1426740519.184137599.1493991659840.JavaMail.root@zimbra61-e11.priv.proxad.net>
Message-ID: <D5323A89.71B2%jfox@mcmaster.ca>

Dear Nick,


On 2017-05-05, 9:40 AM, "R-devel on behalf of Nick Brown"
<r-devel-bounces at r-project.org on behalf of nick.brown at free.fr> wrote:

>>I conjecture that something in the vicinity of
>> res <- lm(DEPRESSION ~ scale(ZMEAN_PA) + scale(ZDIVERSITY_PA) +
>>scale(ZMEAN_PA * ZDIVERSITY_PA), data=dat)
>>summary(res) 
>> would reproduce the SPSS Beta values.
>
>Yes, that works. Thanks!

That you have to work hard in R to match the SPSS results isn?t such a bad
thing when you factor in the observation that standardizing the
interaction regressor, ZMEAN_PA * ZDIVERSITY_PA, separately from each of
its components, ZMEAN_PA and ZDIVERSITY_PA, is nonsense.

Best,
 John

-------------------------------------
John Fox, Professor
McMaster University
Hamilton, Ontario, Canada
Web: http://socserv.mcmaster.ca/jfox/


> 
>
>----- Original Message -----
>
>From: "peter dalgaard" <pdalgd at gmail.com>
>To: "Viechtbauer Wolfgang (SP)"
><wolfgang.viechtbauer at maastrichtuniversity.nl>, "Nick Brown"
><nick.brown at free.fr>
>Cc: r-devel at r-project.org
>Sent: Friday, 5 May, 2017 3:33:29 PM
>Subject: Re: [Rd] lm() gives different results to lm.ridge() and SPSS
>
>Thanks, I was getting to try this, but got side tracked by actual work...
>
>Your analysis reproduces the SPSS unscaled estimates. It still remains to
>figure out how Nick got
>
>> 
>coefficients(lm(ZDEPRESSION ~ ZMEAN_PA * ZDIVERSITY_PA, data=s1))
>
>(Intercept) ZMEAN_PA ZDIVERSITY_PA ZMEAN_PA:ZDIVERSITY_PA
>0.07342198 -0.39650356 -0.36569488 -0.09435788
>
>
>which does not match your output. I suspect that ZMEAN_PA and
>ZDIVERSITY_PA were scaled for this analysis (but the interaction term
>still obviously is not). I conjecture that something in the vicinity of
>
>res <- lm(DEPRESSION ~ scale(ZMEAN_PA) + scale(ZDIVERSITY_PA) +
>scale(ZMEAN_PA * ZDIVERSITY_PA), data=dat)
>summary(res) 
>
>would reproduce the SPSS Beta values.
>
>
>> On 5 May 2017, at 14:43 , Viechtbauer Wolfgang (SP)
>><wolfgang.viechtbauer at maastrichtuniversity.nl> wrote:
>> 
>> I had no problems running regression models in SPSS and R that yielded
>>the same results for these data.
>> 
>> The difference you are observing is from fitting different models. In
>>R, you fitted: 
>> 
>> res <- lm(DEPRESSION ~ ZMEAN_PA * ZDIVERSITY_PA, data=dat)
>> summary(res) 
>> 
>> The interaction term is the product of ZMEAN_PA and ZDIVERSITY_PA. This
>>is not a standardized variable itself and not the same as "ZINTER_PA_C"
>>in the png you showed, which is not a variable in the dataset, but can
>>be created with: 
>> 
>> dat$ZINTER_PA_C <- with(dat, scale(ZMEAN_PA * ZDIVERSITY_PA))
>> 
>> If you want the same results as in SPSS, then you need to fit:
>> 
>> res <- lm(DEPRESSION ~ ZMEAN_PA + ZDIVERSITY_PA + ZINTER_PA_C,
>>data=dat) 
>> summary(res) 
>> 
>> This yields: 
>> 
>> Coefficients: 
>> Estimate Std. Error t value Pr(>|t|)
>> (Intercept) 6.41041 0.01722 372.21 <2e-16 ***
>> ZMEAN_PA -1.62726 0.04200 -38.74 <2e-16 ***
>> ZDIVERSITY_PA -1.50082 0.07447 -20.15 <2e-16 ***
>> ZINTER_PA_C -0.58955 0.05288 -11.15 <2e-16 ***
>> --- 
>> Signif. codes: 0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>> 
>> Exactly the same as in the png.
>> 
>> Peter already mentioned this as a possible reason for the discrepancy:
>>https://stat.ethz.ch/pipermail/r-devel/2017-May/074191.html ("Is it
>>perhaps the case that x1 and x2 have already been scaled to have
>>standard deviation 1? In that case, x1*x2 won't be.")
>> 
>> Best, 
>> Wolfgang 
>> 
>> -----Original Message-----
>> From: R-devel [mailto:r-devel-bounces at r-project.org] On Behalf Of Nick
>>Brown 
>> Sent: Friday, May 05, 2017 10:40
>> To: peter dalgaard
>> Cc: r-devel at r-project.org
>> Subject: Re: [Rd] lm() gives different results to lm.ridge() and SPSS
>> 
>> Hi, 
>> 
>> Here is (I hope) all the relevant output from R.
>> 
>>> mean(s1$ZDEPRESSION, na.rm=T) [1] -1.041546e-16 >
>>>mean(s1$ZDIVERSITY_PA, na.rm=T) [1] -9.660583e-16 > mean(s1$ZMEAN_PA,
>>>na.rm=T) [1] -5.430282e-15 > lm.ridge(ZDEPRESSION ~ ZMEAN_PA *
>>>ZDIVERSITY_PA, data=s1)$coef ZMEAN_PA ZDIVERSITY_PA
>>>ZMEAN_PA:ZDIVERSITY_PA
>> -0.3962254 -0.3636026 -0.1425772 ## This is what I thought was the
>>problem originally. :-)
>> 
>> 
>>> coefficients(lm(ZDEPRESSION ~ ZMEAN_PA * ZDIVERSITY_PA, data=s1))
>>>(Intercept) ZMEAN_PA ZDIVERSITY_PA ZMEAN_PA:ZDIVERSITY_PA
>> 0.07342198 -0.39650356 -0.36569488 -0.09435788 >
>>coefficients(lm.ridge(ZDEPRESSION ~ ZMEAN_PA * ZDIVERSITY_PA, data=s1))
>>ZMEAN_PA ZDIVERSITY_PA ZMEAN_PA:ZDIVERSITY_PA
>> 0.07342198 -0.39650356 -0.36569488 -0.09435788 The equivalent from SPSS
>>is attached. The unstandardized coefficients in SPSS look nothing like
>>those in R. The standardized coefficients in SPSS match the
>>lm.ridge()$coef numbers very closely indeed, suggesting that the same
>>algorithm may be in use.
>> 
>> I have put the dataset file, which is the untouched original I received
>>from the authors, in this Dropbox folder:
>>https://www.dropbox.com/sh/xsebjy55ius1ysb/AADwYUyV1bl6-iAw7ACuF1_La?dl=0
>>. You can read it into R with this code (one variable needs to be
>>standardized and centered; everything else is already in the file):
>> 
>> s1 <- read.csv("Emodiversity_Study1.csv", stringsAsFactors=FALSE)
>>s1$ZDEPRESSION <- scale(s1$DEPRESSION)
>> Hey, maybe R is fine and I've stumbled on a bug in SPSS? If so, I'm
>>sure IBM will want to fix it quickly (ha ha ha).
>> 
>> Nick 
>> 
>> ----- Original Message -----
>> 
>> From: "peter dalgaard" <pdalgd at gmail.com>
>> To: "Nick Brown" <nick.brown at free.fr>
>> Cc: "Simon Bonner" <sbonner6 at uwo.ca>, r-devel at r-project.org
>> Sent: Friday, 5 May, 2017 10:02:10 AM
>> Subject: Re: [Rd] lm() gives different results to lm.ridge() and SPSS
>> 
>> I asked you before, but in case you missed it: Are you looking at the
>>right place in SPSS output?
>> 
>> The UNstandardized coefficients should be comparable to R, i.e. the "B"
>>column, not "Beta".
>> 
>> -pd 
>> 
>>> On 5 May 2017, at 01:58 , Nick Brown <nick.brown at free.fr> wrote:
>>> 
>>> Hi Simon, 
>>> 
>>> Yes, if I uses coefficients() I get the same results for lm() and
>>>lm.ridge(). So that's consistent, at least.
>>> 
>>> Interestingly, the "wrong" number I get from lm.ridge()$coef agrees
>>>with the value from SPSS to 5dp, which is an interesting coincidence if
>>>these numbers have no particular external meaning in lm.ridge().
>>> 
>>> Kind regards, 
>>> Nick 
>>> 
>>> ----- Original Message -----
>>> 
>>> From: "Simon Bonner" <sbonner6 at uwo.ca>
>>> To: "Nick Brown" <nick.brown at free.fr>, r-devel at r-project.org
>>> Sent: Thursday, 4 May, 2017 7:07:33 PM
>>> Subject: RE: [Rd] lm() gives different results to lm.ridge() and SPSS
>>> 
>>> Hi Nick, 
>>> 
>>> I think that the problem here is your use of $coef to extract the
>>>coefficients of the ridge regression. The help for lm.ridge states that
>>>coef is a "matrix of coefficients, one row for each value of lambda.
>>>Note that these are not on the original scale and are for use by the
>>>coef method." 
>>> 
>>> I ran a small test with simulated data, code is copied below, and
>>>indeed the output from lm.ridge differs depending on whether the
>>>coefficients are accessed via $coef or via the coefficients() function.
>>>The latter does produce results that match the output from lm.
>>> 
>>> I hope that helps.
>>> 
>>> Cheers, 
>>> 
>>> Simon 
>>> 
>>> ## Load packages
>>> library(MASS) 
>>> 
>>> ## Set seed 
>>> set.seed(8888) 
>>> 
>>> ## Set parameters
>>> n <- 100 
>>> beta <- c(1,0,1)
>>> sigma <- .5 
>>> rho <- .75 
>>> 
>>> ## Simulate correlated covariates
>>> Sigma <- matrix(c(1,rho,rho,1),ncol=2)
>>> X <- mvrnorm(n,c(0,0),Sigma=Sigma)
>>> 
>>> ## Simulate data
>>> mu <- beta[1] + X %*% beta[-1]
>>> y <- rnorm(n,mu,sigma)
>>> 
>>> ## Fit model with lm()
>>> fit1 <- lm(y ~ X)
>>> 
>>> ## Fit model with lm.ridge()
>>> fit2 <- lm.ridge(y ~ X)
>>> 
>>> ## Compare coefficients
>>> cbind(fit1$coefficients,c(NA,fit2$coef),coefficients(fit2))
>>> 
>>> [,1] [,2] [,3] 
>>> (Intercept) 0.99276001 NA 0.99276001
>>> X1 -0.03980772 -0.04282391 -0.03980772
>>> X2 1.11167179 1.06200476 1.11167179
>>> 
>>> -- 
>>> 
>>> Simon Bonner 
>>> Assistant Professor of Environmetrics/ Director MMASc
>>> Department of Statistical and Actuarial Sciences/Department of Biology
>>> University of Western Ontario
>>> 
>>> Office: Western Science Centre rm 276
>>> 
>>> Email: sbonner6 at uwo.ca | Telephone: 519-661-2111 x88205 | Fax:
>>>519-661-3813 
>>> Twitter: @bonnerstatslab | Website:
>>>http://simon.bonners.ca/bonner-lab/wpblog/
>>> 
>>>> -----Original Message-----
>>>> From: R-devel [mailto:r-devel-bounces at r-project.org] On Behalf Of
>>>>Nick 
>>>> Brown 
>>>> Sent: May 4, 2017 10:29 AM
>>>> To: r-devel at r-project.org
>>>> Subject: [Rd] lm() gives different results to lm.ridge() and SPSS
>>>> 
>>>> Hallo, 
>>>> 
>>>> I hope I am posting to the right place. I was advised to try this
>>>>list by Ben Bolker
>>>> (https://twitter.com/bolkerb/status/859909918446497795). I also
>>>>posted this 
>>>> question to StackOverflow
>>>> 
>>>>(http://stackoverflow.com/questions/43771269/lm-gives-different-results
>>>>- 
>>>> from-lm-ridgelambda-0). I am a relative newcomer to R, but I wrote my
>>>>first 
>>>> program in 1975 and have been paid to program in about 15 different
>>>> languages, so I have some general background knowledge.
>>>> 
>>>> I have a regression from which I extract the coefficients like this:
>>>> lm(y ~ x1 * x2, data=ds)$coef
>>>> That gives: x1=0.40, x2=0.37, x1*x2=0.09
>>>> 
>>>> When I do the same regression in SPSS, I get:
>>>> beta(x1)=0.40, beta(x2)=0.37, beta(x1*x2)=0.14.
>>>> So the main effects are in agreement, but there is quite a difference
>>>>in the 
>>>> coefficient for the interaction.
>>>> 
>>>> X1 and X2 are correlated about .75 (yes, yes, I know - this model
>>>>wasn't my 
>>>> idea, but it got published), so there is quite possibly something
>>>>going on with 
>>>> collinearity. So I thought I'd try lm.ridge() to see if I can get an
>>>>idea of where 
>>>> the problems are occurring.
>>>> 
>>>> The starting point is to run lm.ridge() with lambda=0 (i.e., no ridge
>>>>penalty) and 
>>>> check we get the same results as with lm():
>>>> lm.ridge(y ~ x1 * x2, lambda=0, data=ds)$coef
>>>> x1=0.40, x2=0.37, x1*x2=0.14
>>>> So lm.ridge() agrees with SPSS, but not with lm(). (Of course,
>>>>lambda=0 is the
>>>> default, so it can be omitted; I can alternate between including or
>>>>deleting 
>>>> ".ridge" in the function call, and watch the coefficient for the
>>>>interaction 
>>>> change.) 
>>>> 
>>>> What seems slightly strange to me here is that I assumed that
>>>>lm.ridge() just
>>>> piggybacks on lm() anyway, so in the specific case where lambda=0 and
>>>>there 
>>>> is no "ridging" to do, I'd expect exactly the same results.
>>>> 
>>>> Unfortunately there are 34,000 cases in the dataset, so a "minimal"
>>>>reprex will 
>>>> not be easy to make, but I can share the data via Dropbox or
>>>>something if that
>>>> would help. 
>>>> 
>>>> I appreciate that when there is strong collinearity then all bets are
>>>>off in terms 
>>>> of what the betas mean, but I would really expect lm() and lm.ridge()
>>>>to give 
>>>> the same results. (I would be happy to ignore SPSS, but for the
>>>>moment it's 
>>>> part of the majority!)
>>>> 
>>>> Thanks for reading,
>>>> Nick 
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>
>-- 
>Peter Dalgaard, Professor,
>Center for Statistics, Copenhagen Business School
>Solbjerg Plads 3, 2000 Frederiksberg, Denmark
>Phone: (+45)38153501
>Office: A 4.23 
>Email: pd.mes at cbs.dk Priv: PDalgd at gmail.com
>
>
>
>
>
>
>
>
>
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-devel at r-project.org mailing list
>https://stat.ethz.ch/mailman/listinfo/r-devel


From ggrothendieck at gmail.com  Fri May  5 22:33:14 2017
From: ggrothendieck at gmail.com (Gabor Grothendieck)
Date: Fri, 5 May 2017 16:33:14 -0400
Subject: [Rd] A few suggestions and perspectives from a PhD student
In-Reply-To: <FCBC5362-8436-487D-95A0-65CB5924BD29@idi.ntnu.no>
References: <FCBC5362-8436-487D-95A0-65CB5924BD29@idi.ntnu.no>
Message-ID: <CAP01uRk1NzjRjs80hHj3mJ+n1iaESfxj00t88fSkt_wS2pV1rg@mail.gmail.com>

Regarding the anonymous-function-in-a-pipeline point one can already
do this which does use brackets but even so it involves fewer
characters than the example shown.  Here { . * 2 } is basically a
lambda whose argument is dot. Would this be sufficient?

  library(magrittr)

  1.5 %>% { . * 2 }
  ## [1] 3

Regarding currying note that with magrittr Ista's code could be written as:

  1:5 %>% lapply(foo, y = 3)

or at the expense of slightly more verbosity:

  1:5 %>% Map(f = . %>% foo(y = 3))


On Fri, May 5, 2017 at 1:00 PM, Antonin Klima <antonink at idi.ntnu.no> wrote:
> Dear Sir or Madam,
>
> I am in 2nd year of my PhD in bioinformatics, after taking my Master?s in computer science, and have been using R heavily during my PhD. As such, I have put together a list of certain features in R that, in my opinion, would be beneficial to add, or could be improved. The first two are already implemented in packages, but given that it is implemented as user-defined operators, it greatly restricts its usefulness. I hope you will find my suggestions interesting. If you find time, I will welcome any feedback as to whether you find the suggestions useful, or why you do not think they should be implemented. I will also welcome if you enlighten me with any features I might be unaware of, that might solve the issues I have pointed out below.
>
> 1) piping
> Currently available in package magrittr, piping makes the code better readable by having the line start at its natural starting point, and following with functions that are applied - in order. The readability of several nested calls with a number of parameters each is almost zero, it?s almost as if one would need to come up with the solution himself. Pipeline in comparison is very straightforward, especially together with the point (2).
>
> The package here works rather good nevertheless, the shortcomings of piping not being native are not quite as severe as in point (2). Nevertheless, an intuitive symbol such as | would be helpful, and it sometimes bothers me that I have to parenthesize anonymous function, which would probably not be required in a native pipe-operator, much like it is not required in f.ex. lapply. That is,
> 1:5 %>% function(x) x+2
> should be totally fine
>
> 2) currying
> Currently available in package Curry. The idea is that, having a function such as foo = function(x, y) x+y, one would like to write for example lapply(foo(3), 1:5), and have the interpreter figure out ok, foo(3) does not make a value result, but it can still give a function result - a function of y. This would be indeed most useful for various apply functions, rather than writing function(x) foo(3,x).
>
> I suggest that currying would make the code easier to write, and more readable, especially when using apply functions. One might imagine that there could be some confusion with such a feature, especially from people unfamiliar with functional programming, although R already does take function as first-order arguments, so it could be just fine. But one could address it with special syntax, such as $foo(3) [$foo(x=3)] for partial application.  The current currying package has very limited usefulness, as, being limited by the user-defined operator framework, it only rarely can contribute to less code/more readability. Compare yourself:
> $foo(x=3) vs foo %<% 3
> goo = function(a,b,c)
> $goo(b=3) vs goo %><% list(b=3)
>
> Moreover, one would often like currying to have highest priority. For example, when piping:
> data %>% foo %>% foo1 %<% 3
> if one wants to do data %>% foo %>% $foo(x=3)
>
> 3) Code executable only when running the script itself
> Whereas the first two suggestions are somewhat stealing from Haskell and the like, this suggestion would be stealing from Python. I?m building quite a complicated pipeline, using S4 classes. After defining the class and its methods, I also define how to build the class to my likings, based on my input data, using various now-defined methods. So I end up having a list of command line arguments to process, and the way to create the class instance based on them. If I write it to the class file, however, I end up running the code when it is sourced from the next step in the pipeline, that needs the previous class definitions.
>
> A feature such as pythonic ?if __name__ == __main__? would thus be useful. As it is, I had to create run scripts as separate files. Which is actually not so terrible, given the class and its methods often span a few hundred lines, but still.
>
> 4) non-exported global variables
> I also find it lacking, that I seem to be unable to create constants that would not get passed to files that source the class definition. That is, if class1 features global constant CONSTANT=3, then if class2 sources class1, it will also include the constant. This 1) clutters the namespace when running the code interactively, 2) potentially overwrites the constants in case of nameclash. Some kind of export/nonexport variable syntax, or symbolic import, or namespace would be useful. I know if I converted it to a package I would get at least something like a namespace, but still.
>
> I understand that the variable cannot just not be imported, in general, as the functions will generally rely on it (otherwise it wouldn?t have to be there). But one could consider hiding it in an implicit namespace for the file, for example.
>
> 5) S4 methods with same name, for different classes
> Say I have an S4 class called datasetSingle, and another S4 class called datasetMulti, which gathers up a number of datasetSingle classes, and adds some extra functionality on top. The datasetSingle class may have a method replicates, that returns a named vector assigning replicate number to experiment names of the dataset. But I would also like to have a function with the same name for the datasetMulti class, that returns for data frame, or list, covering replicate numbers for all the datasets included.
>
> But then, I need to setGeneric for the method. But if I set generic before both implementations, I will reset the generic in the second call, losing the definition for ?replicates? for datasetSingle. Skipping this in the code for datasetMulti means that 1) I have to remember that I had the function defined for datasetSingle, 2) if I remove the function or change its name in datasetSingle, I now have to change the datasetMulti class file too. Moreover, if I would like to have a different generic for the datasetMulti version, I have to change it not in datasetMulti class file, but in the datasetSingle file, where it might not make much sense. In this case, I wanted to have another argument ?datasets?, which would return the replicates only for the datasets specified, rather than for all.
>
> I made a wrapper that could circumvent the first issue, but the second issue is not easy to circumvent.
>
> 6) Many parameters freeze S4 method calls
> If I specify ca over 6 parameters for an S4 method, I would often get a ?freeze? on the method call. The process would eat up a lot of memory before going into the call, upon which it would execute the call as normal (if it didn?t run out of memory or I didn?t run out of patience). Subsequent calls of the method would not include this overhead. The amount of memory this could take could be in gigabytes, and the time in minutes. I suspect this might be due to generating an entry in call table for each accepted signature. It can be circumvented, but sure isn?t a behaviour one would expect.
>
> 7) Default values for S4 methods
> It would seem that it is not possible to set up default parameters for an S4 method in a usual way of definiton = function (x, y=5). I resorted to making class unions with ?missing? for signatures on the call, with the call starting with if(missing(param)) param=DEFAULT_VALUE, but it certainly does not improve readability or ease of coding.
>
>
> Thank you for your time if you have finished reading thus far. :) Looking forward to any answer.
>
> Yours Sincerely,
> Antonin Klima
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel



-- 
Statistics & Software Consulting
GKX Group, GKX Associates Inc.
tel: 1-877-GKX-GROUP
email: ggrothendieck at gmail.com


From wolfgang.viechtbauer at maastrichtuniversity.nl  Fri May  5 22:34:40 2017
From: wolfgang.viechtbauer at maastrichtuniversity.nl (Viechtbauer Wolfgang (SP))
Date: Fri, 5 May 2017 20:34:40 +0000
Subject: [Rd] lm() gives different results to lm.ridge() and SPSS
In-Reply-To: <D5323A89.71B2%jfox@mcmaster.ca>
References: <15506037-17F7-4B45-9299-327BBCB27347@gmail.com>
 <11345_1493991675_v45DfEL9003599_1426740519.184137599.1493991659840.JavaMail.root@zimbra61-e11.priv.proxad.net>
 <D5323A89.71B2%jfox@mcmaster.ca>
Message-ID: <e244698b19134da2bdb69e61b755188e@UM-MAIL3215.unimaas.nl>

Totally agree that standardizing the interaction term is nonsense. But in all fairness, SPSS doesn't do that. In fact, the 'REGRESSION' command in SPSS doesn't compute any interaction terms -- one has to first compute them 'by hand' and then add them to the model like any other variable. So somebody worked extra hard to standardize that interaction term in SPSS as well :/

Best,
Wolfgang

-----Original Message-----
From: R-devel [mailto:r-devel-bounces at r-project.org] On Behalf Of Fox, John
Sent: Friday, May 05, 2017 20:23
To: Nick Brown; peter dalgaard
Cc: r-devel at r-project.org
Subject: Re: [Rd] lm() gives different results to lm.ridge() and SPSS

Dear Nick,

On 2017-05-05, 9:40 AM, "R-devel on behalf of Nick Brown"
<r-devel-bounces at r-project.org on behalf of nick.brown at free.fr> wrote:

>>I conjecture that something in the vicinity of
>> res <- lm(DEPRESSION ~ scale(ZMEAN_PA) + scale(ZDIVERSITY_PA) +
>>scale(ZMEAN_PA * ZDIVERSITY_PA), data=dat)
>>summary(res) 
>> would reproduce the SPSS Beta values.
>
>Yes, that works. Thanks!

That you have to work hard in R to match the SPSS results isn?t such a bad
thing when you factor in the observation that standardizing the
interaction regressor, ZMEAN_PA * ZDIVERSITY_PA, separately from each of
its components, ZMEAN_PA and ZDIVERSITY_PA, is nonsense.

Best,
 John

-------------------------------------
John Fox, Professor
McMaster University
Hamilton, Ontario, Canada
Web: http://socserv.mcmaster.ca/jfox/

>----- Original Message -----
>
>From: "peter dalgaard" <pdalgd at gmail.com>
>To: "Viechtbauer Wolfgang (SP)"
><wolfgang.viechtbauer at maastrichtuniversity.nl>, "Nick Brown"
><nick.brown at free.fr>
>Cc: r-devel at r-project.org
>Sent: Friday, 5 May, 2017 3:33:29 PM
>Subject: Re: [Rd] lm() gives different results to lm.ridge() and SPSS
>
>Thanks, I was getting to try this, but got side tracked by actual work...
>
>Your analysis reproduces the SPSS unscaled estimates. It still remains to
>figure out how Nick got
>
>> 
>coefficients(lm(ZDEPRESSION ~ ZMEAN_PA * ZDIVERSITY_PA, data=s1))
>
>(Intercept) ZMEAN_PA ZDIVERSITY_PA ZMEAN_PA:ZDIVERSITY_PA
>0.07342198 -0.39650356 -0.36569488 -0.09435788
>
>
>which does not match your output. I suspect that ZMEAN_PA and
>ZDIVERSITY_PA were scaled for this analysis (but the interaction term
>still obviously is not). I conjecture that something in the vicinity of
>
>res <- lm(DEPRESSION ~ scale(ZMEAN_PA) + scale(ZDIVERSITY_PA) +
>scale(ZMEAN_PA * ZDIVERSITY_PA), data=dat)
>summary(res) 
>
>would reproduce the SPSS Beta values.
>
>> On 5 May 2017, at 14:43 , Viechtbauer Wolfgang (SP)
>><wolfgang.viechtbauer at maastrichtuniversity.nl> wrote:
>> 
>> I had no problems running regression models in SPSS and R that yielded
>>the same results for these data.
>> 
>> The difference you are observing is from fitting different models. In
>>R, you fitted: 
>> 
>> res <- lm(DEPRESSION ~ ZMEAN_PA * ZDIVERSITY_PA, data=dat)
>> summary(res) 
>> 
>> The interaction term is the product of ZMEAN_PA and ZDIVERSITY_PA. This
>>is not a standardized variable itself and not the same as "ZINTER_PA_C"
>>in the png you showed, which is not a variable in the dataset, but can
>>be created with: 
>> 
>> dat$ZINTER_PA_C <- with(dat, scale(ZMEAN_PA * ZDIVERSITY_PA))
>> 
>> If you want the same results as in SPSS, then you need to fit:
>> 
>> res <- lm(DEPRESSION ~ ZMEAN_PA + ZDIVERSITY_PA + ZINTER_PA_C,
>>data=dat) 
>> summary(res) 
>> 
>> This yields: 
>> 
>> Coefficients: 
>> Estimate Std. Error t value Pr(>|t|)
>> (Intercept) 6.41041 0.01722 372.21 <2e-16 ***
>> ZMEAN_PA -1.62726 0.04200 -38.74 <2e-16 ***
>> ZDIVERSITY_PA -1.50082 0.07447 -20.15 <2e-16 ***
>> ZINTER_PA_C -0.58955 0.05288 -11.15 <2e-16 ***
>> --- 
>> Signif. codes: 0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>> 
>> Exactly the same as in the png.
>> 
>> Peter already mentioned this as a possible reason for the discrepancy:
>>https://stat.ethz.ch/pipermail/r-devel/2017-May/074191.html ("Is it
>>perhaps the case that x1 and x2 have already been scaled to have
>>standard deviation 1? In that case, x1*x2 won't be.")
>> 
>> Best, 
>> Wolfgang 

From wolfgang.viechtbauer at maastrichtuniversity.nl  Fri May  5 22:41:11 2017
From: wolfgang.viechtbauer at maastrichtuniversity.nl (Viechtbauer Wolfgang (SP))
Date: Fri, 5 May 2017 20:41:11 +0000
Subject: [Rd] lm() gives different results to lm.ridge() and SPSS
References: <15506037-17F7-4B45-9299-327BBCB27347@gmail.com>
 <11345_1493991675_v45DfEL9003599_1426740519.184137599.1493991659840.JavaMail.root@zimbra61-e11.priv.proxad.net>
 <D5323A89.71B2%jfox@mcmaster.ca> 
Message-ID: <06404cd358504a04a066f9671f045c8e@UM-MAIL3215.unimaas.nl>

Well, one correction -- the 'standardized coefficients' that SPSS shows are based on standardizing all variables separately (so x1, x2, and x1*x2 are all standardized). So with respect to that, the criticism certainly stands.

-----Original Message-----
From: Viechtbauer Wolfgang (SP) 
Sent: Friday, May 05, 2017 22:35
To: r-devel at r-project.org
Subject: RE: [Rd] lm() gives different results to lm.ridge() and SPSS

Totally agree that standardizing the interaction term is nonsense. But in all fairness, SPSS doesn't do that. In fact, the 'REGRESSION' command in SPSS doesn't compute any interaction terms -- one has to first compute them 'by hand' and then add them to the model like any other variable. So somebody worked extra hard to standardize that interaction term in SPSS as well :/

Best,
Wolfgang

-----Original Message-----
From: R-devel [mailto:r-devel-bounces at r-project.org] On Behalf Of Fox, John
Sent: Friday, May 05, 2017 20:23
To: Nick Brown; peter dalgaard
Cc: r-devel at r-project.org
Subject: Re: [Rd] lm() gives different results to lm.ridge() and SPSS

Dear Nick,

On 2017-05-05, 9:40 AM, "R-devel on behalf of Nick Brown"
<r-devel-bounces at r-project.org on behalf of nick.brown at free.fr> wrote:

>>I conjecture that something in the vicinity of
>> res <- lm(DEPRESSION ~ scale(ZMEAN_PA) + scale(ZDIVERSITY_PA) +
>>scale(ZMEAN_PA * ZDIVERSITY_PA), data=dat)
>>summary(res) 
>> would reproduce the SPSS Beta values.
>
>Yes, that works. Thanks!

That you have to work hard in R to match the SPSS results isn?t such a bad
thing when you factor in the observation that standardizing the
interaction regressor, ZMEAN_PA * ZDIVERSITY_PA, separately from each of
its components, ZMEAN_PA and ZDIVERSITY_PA, is nonsense.

Best,
 John

-------------------------------------
John Fox, Professor
McMaster University
Hamilton, Ontario, Canada
Web: http://socserv.mcmaster.ca/jfox/

>----- Original Message -----
>
>From: "peter dalgaard" <pdalgd at gmail.com>
>To: "Viechtbauer Wolfgang (SP)"
><wolfgang.viechtbauer at maastrichtuniversity.nl>, "Nick Brown"
><nick.brown at free.fr>
>Cc: r-devel at r-project.org
>Sent: Friday, 5 May, 2017 3:33:29 PM
>Subject: Re: [Rd] lm() gives different results to lm.ridge() and SPSS
>
>Thanks, I was getting to try this, but got side tracked by actual work...
>
>Your analysis reproduces the SPSS unscaled estimates. It still remains to
>figure out how Nick got
>
>> 
>coefficients(lm(ZDEPRESSION ~ ZMEAN_PA * ZDIVERSITY_PA, data=s1))
>
>(Intercept) ZMEAN_PA ZDIVERSITY_PA ZMEAN_PA:ZDIVERSITY_PA
>0.07342198 -0.39650356 -0.36569488 -0.09435788
>
>
>which does not match your output. I suspect that ZMEAN_PA and
>ZDIVERSITY_PA were scaled for this analysis (but the interaction term
>still obviously is not). I conjecture that something in the vicinity of
>
>res <- lm(DEPRESSION ~ scale(ZMEAN_PA) + scale(ZDIVERSITY_PA) +
>scale(ZMEAN_PA * ZDIVERSITY_PA), data=dat)
>summary(res) 
>
>would reproduce the SPSS Beta values.
>
>> On 5 May 2017, at 14:43 , Viechtbauer Wolfgang (SP)
>><wolfgang.viechtbauer at maastrichtuniversity.nl> wrote:
>> 
>> I had no problems running regression models in SPSS and R that yielded
>>the same results for these data.
>> 
>> The difference you are observing is from fitting different models. In
>>R, you fitted: 
>> 
>> res <- lm(DEPRESSION ~ ZMEAN_PA * ZDIVERSITY_PA, data=dat)
>> summary(res) 
>> 
>> The interaction term is the product of ZMEAN_PA and ZDIVERSITY_PA. This
>>is not a standardized variable itself and not the same as "ZINTER_PA_C"
>>in the png you showed, which is not a variable in the dataset, but can
>>be created with: 
>> 
>> dat$ZINTER_PA_C <- with(dat, scale(ZMEAN_PA * ZDIVERSITY_PA))
>> 
>> If you want the same results as in SPSS, then you need to fit:
>> 
>> res <- lm(DEPRESSION ~ ZMEAN_PA + ZDIVERSITY_PA + ZINTER_PA_C,
>>data=dat) 
>> summary(res) 
>> 
>> This yields: 
>> 
>> Coefficients: 
>> Estimate Std. Error t value Pr(>|t|)
>> (Intercept) 6.41041 0.01722 372.21 <2e-16 ***
>> ZMEAN_PA -1.62726 0.04200 -38.74 <2e-16 ***
>> ZDIVERSITY_PA -1.50082 0.07447 -20.15 <2e-16 ***
>> ZINTER_PA_C -0.58955 0.05288 -11.15 <2e-16 ***
>> --- 
>> Signif. codes: 0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
>> 
>> Exactly the same as in the png.
>> 
>> Peter already mentioned this as a possible reason for the discrepancy:
>>https://stat.ethz.ch/pipermail/r-devel/2017-May/074191.html ("Is it
>>perhaps the case that x1 and x2 have already been scaled to have
>>standard deviation 1? In that case, x1*x2 won't be.")
>> 
>> Best, 
>> Wolfgang 

From nick.brown at free.fr  Sat May  6 01:49:18 2017
From: nick.brown at free.fr (Nick Brown)
Date: Sat, 6 May 2017 01:49:18 +0200 (CEST)
Subject: [Rd] lm() gives different results to lm.ridge() and SPSS
In-Reply-To: <D5323A89.71B2%jfox@mcmaster.ca>
Message-ID: <1463049899.185952899.1494028158769.JavaMail.root@zimbra61-e11.priv.proxad.net>

Hi John, 

Thanks for the comment... but that appears to mean that SPSS has a big problem. I have always been told that to include an interaction term in a regression, the only way is to do the multiplication by hand. But then it seems to be impossible to stop SPSS from re-standardizing the variable that corresponds to the interaction term. Am I missing something? Is there a way to perform the regression with the interaction in SPSS without computing the interaction as a separate variable? 

Best, 
Nick 

----- Original Message -----

From: "John Fox" <jfox at mcmaster.ca> 
To: "Nick Brown" <nick.brown at free.fr>, "peter dalgaard" <pdalgd at gmail.com> 
Cc: r-devel at r-project.org 
Sent: Friday, 5 May, 2017 8:22:53 PM 
Subject: Re: [Rd] lm() gives different results to lm.ridge() and SPSS 

Dear Nick, 


On 2017-05-05, 9:40 AM, "R-devel on behalf of Nick Brown" 
<r-devel-bounces at r-project.org on behalf of nick.brown at free.fr> wrote: 

>>I conjecture that something in the vicinity of 
>> res <- lm(DEPRESSION ~ scale(ZMEAN_PA) + scale(ZDIVERSITY_PA) + 
>>scale(ZMEAN_PA * ZDIVERSITY_PA), data=dat) 
>>summary(res) 
>> would reproduce the SPSS Beta values. 
> 
>Yes, that works. Thanks! 

That you have to work hard in R to match the SPSS results isn?t such a bad 
thing when you factor in the observation that standardizing the 
interaction regressor, ZMEAN_PA * ZDIVERSITY_PA, separately from each of 
its components, ZMEAN_PA and ZDIVERSITY_PA, is nonsense. 

Best, 
John 

------------------------------------- 
John Fox, Professor 
McMaster University 
Hamilton, Ontario, Canada 
Web: http://socserv.mcmaster.ca/jfox/ 


> 
> 
>----- Original Message ----- 
> 
>From: "peter dalgaard" <pdalgd at gmail.com> 
>To: "Viechtbauer Wolfgang (SP)" 
><wolfgang.viechtbauer at maastrichtuniversity.nl>, "Nick Brown" 
><nick.brown at free.fr> 
>Cc: r-devel at r-project.org 
>Sent: Friday, 5 May, 2017 3:33:29 PM 
>Subject: Re: [Rd] lm() gives different results to lm.ridge() and SPSS 
> 
>Thanks, I was getting to try this, but got side tracked by actual work...
> 
>Your analysis reproduces the SPSS unscaled estimates. It still remains to
>figure out how Nick got 
> 
>> 
>coefficients(lm(ZDEPRESSION ~ ZMEAN_PA * ZDIVERSITY_PA, data=s1)) 
> 
>(Intercept) ZMEAN_PA ZDIVERSITY_PA ZMEAN_PA:ZDIVERSITY_PA 
>0.07342198 -0.39650356 -0.36569488 -0.09435788 
> 
> 
>which does not match your output. I suspect that ZMEAN_PA and 
>ZDIVERSITY_PA were scaled for this analysis (but the interaction term 
>still obviously is not). I conjecture that something in the vicinity of 
> 
>res <- lm(DEPRESSION ~ scale(ZMEAN_PA) + scale(ZDIVERSITY_PA) + 
>scale(ZMEAN_PA * ZDIVERSITY_PA), data=dat) 
>summary(res) 
> 
>would reproduce the SPSS Beta values. 
> 
> 
>> On 5 May 2017, at 14:43 , Viechtbauer Wolfgang (SP) 
>><wolfgang.viechtbauer at maastrichtuniversity.nl> wrote: 
>> 
>> I had no problems running regression models in SPSS and R that yielded 
>>the same results for these data. 
>> 
>> The difference you are observing is from fitting different models. In 
>>R, you fitted: 
>> 
>> res <- lm(DEPRESSION ~ ZMEAN_PA * ZDIVERSITY_PA, data=dat) 
>> summary(res) 
>> 
>> The interaction term is the product of ZMEAN_PA and ZDIVERSITY_PA. This
>>is not a standardized variable itself and not the same as "ZINTER_PA_C" 
>>in the png you showed, which is not a variable in the dataset, but can 
>>be created with: 
>> 
>> dat$ZINTER_PA_C <- with(dat, scale(ZMEAN_PA * ZDIVERSITY_PA)) 
>> 
>> If you want the same results as in SPSS, then you need to fit: 
>> 
>> res <- lm(DEPRESSION ~ ZMEAN_PA + ZDIVERSITY_PA + ZINTER_PA_C, 
>>data=dat) 
>> summary(res) 
>> 
>> This yields: 
>> 
>> Coefficients: 
>> Estimate Std. Error t value Pr(>|t|) 
>> (Intercept) 6.41041 0.01722 372.21 <2e-16 *** 
>> ZMEAN_PA -1.62726 0.04200 -38.74 <2e-16 *** 
>> ZDIVERSITY_PA -1.50082 0.07447 -20.15 <2e-16 *** 
>> ZINTER_PA_C -0.58955 0.05288 -11.15 <2e-16 *** 
>> --- 
>> Signif. codes: 0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1 
>> 
>> Exactly the same as in the png. 
>> 
>> Peter already mentioned this as a possible reason for the discrepancy: 
>>https://stat.ethz.ch/pipermail/r-devel/2017-May/074191.html ("Is it 
>>perhaps the case that x1 and x2 have already been scaled to have 
>>standard deviation 1? In that case, x1*x2 won't be.") 
>> 
>> Best, 
>> Wolfgang 
>> 
>> -----Original Message----- 
>> From: R-devel [mailto:r-devel-bounces at r-project.org] On Behalf Of Nick 
>>Brown 
>> Sent: Friday, May 05, 2017 10:40 
>> To: peter dalgaard 
>> Cc: r-devel at r-project.org 
>> Subject: Re: [Rd] lm() gives different results to lm.ridge() and SPSS 
>> 
>> Hi, 
>> 
>> Here is (I hope) all the relevant output from R. 
>> 
>>> mean(s1$ZDEPRESSION, na.rm=T) [1] -1.041546e-16 > 
>>>mean(s1$ZDIVERSITY_PA, na.rm=T) [1] -9.660583e-16 > mean(s1$ZMEAN_PA,
>>>na.rm=T) [1] -5.430282e-15 > lm.ridge(ZDEPRESSION ~ ZMEAN_PA * 
>>>ZDIVERSITY_PA, data=s1)$coef ZMEAN_PA ZDIVERSITY_PA 
>>>ZMEAN_PA:ZDIVERSITY_PA 
>> -0.3962254 -0.3636026 -0.1425772 ## This is what I thought was the 
>>problem originally. :-) 
>> 
>> 
>>> coefficients(lm(ZDEPRESSION ~ ZMEAN_PA * ZDIVERSITY_PA, data=s1)) 
>>>(Intercept) ZMEAN_PA ZDIVERSITY_PA ZMEAN_PA:ZDIVERSITY_PA 
>> 0.07342198 -0.39650356 -0.36569488 -0.09435788 > 
>>coefficients(lm.ridge(ZDEPRESSION ~ ZMEAN_PA * ZDIVERSITY_PA, data=s1))
>>ZMEAN_PA ZDIVERSITY_PA ZMEAN_PA:ZDIVERSITY_PA 
>> 0.07342198 -0.39650356 -0.36569488 -0.09435788 The equivalent from SPSS
>>is attached. The unstandardized coefficients in SPSS look nothing like 
>>those in R. The standardized coefficients in SPSS match the 
>>lm.ridge()$coef numbers very closely indeed, suggesting that the same 
>>algorithm may be in use. 
>> 
>> I have put the dataset file, which is the untouched original I received
>>from the authors, in this Dropbox folder: 
>>https://www.dropbox.com/sh/xsebjy55ius1ysb/AADwYUyV1bl6-iAw7ACuF1_La?dl=0 
>>. You can read it into R with this code (one variable needs to be 
>>standardized and centered; everything else is already in the file): 
>> 
>> s1 <- read.csv("Emodiversity_Study1.csv", stringsAsFactors=FALSE) 
>>s1$ZDEPRESSION <- scale(s1$DEPRESSION) 
>> Hey, maybe R is fine and I've stumbled on a bug in SPSS? If so, I'm 
>>sure IBM will want to fix it quickly (ha ha ha). 
>> 
>> Nick 
>> 
>> ----- Original Message ----- 
>> 
>> From: "peter dalgaard" <pdalgd at gmail.com> 
>> To: "Nick Brown" <nick.brown at free.fr> 
>> Cc: "Simon Bonner" <sbonner6 at uwo.ca>, r-devel at r-project.org 
>> Sent: Friday, 5 May, 2017 10:02:10 AM 
>> Subject: Re: [Rd] lm() gives different results to lm.ridge() and SPSS 
>> 
>> I asked you before, but in case you missed it: Are you looking at the 
>>right place in SPSS output? 
>> 
>> The UNstandardized coefficients should be comparable to R, i.e. the "B"
>>column, not "Beta". 
>> 
>> -pd 
>> 
>>> On 5 May 2017, at 01:58 , Nick Brown <nick.brown at free.fr> wrote: 
>>> 
>>> Hi Simon, 
>>> 
>>> Yes, if I uses coefficients() I get the same results for lm() and 
>>>lm.ridge(). So that's consistent, at least. 
>>> 
>>> Interestingly, the "wrong" number I get from lm.ridge()$coef agrees 
>>>with the value from SPSS to 5dp, which is an interesting coincidence if
>>>these numbers have no particular external meaning in lm.ridge(). 
>>> 
>>> Kind regards, 
>>> Nick 
>>> 
>>> ----- Original Message ----- 
>>> 
>>> From: "Simon Bonner" <sbonner6 at uwo.ca> 
>>> To: "Nick Brown" <nick.brown at free.fr>, r-devel at r-project.org 
>>> Sent: Thursday, 4 May, 2017 7:07:33 PM 
>>> Subject: RE: [Rd] lm() gives different results to lm.ridge() and SPSS 
>>> 
>>> Hi Nick, 
>>> 
>>> I think that the problem here is your use of $coef to extract the 
>>>coefficients of the ridge regression. The help for lm.ridge states that
>>>coef is a "matrix of coefficients, one row for each value of lambda. 
>>>Note that these are not on the original scale and are for use by the 
>>>coef method." 
>>> 
>>> I ran a small test with simulated data, code is copied below, and 
>>>indeed the output from lm.ridge differs depending on whether the 
>>>coefficients are accessed via $coef or via the coefficients() function.
>>>The latter does produce results that match the output from lm. 
>>> 
>>> I hope that helps. 
>>> 
>>> Cheers, 
>>> 
>>> Simon 
>>> 
>>> ## Load packages 
>>> library(MASS) 
>>> 
>>> ## Set seed 
>>> set.seed(8888) 
>>> 
>>> ## Set parameters 
>>> n <- 100 
>>> beta <- c(1,0,1) 
>>> sigma <- .5 
>>> rho <- .75 
>>> 
>>> ## Simulate correlated covariates 
>>> Sigma <- matrix(c(1,rho,rho,1),ncol=2) 
>>> X <- mvrnorm(n,c(0,0),Sigma=Sigma) 
>>> 
>>> ## Simulate data 
>>> mu <- beta[1] + X %*% beta[-1] 
>>> y <- rnorm(n,mu,sigma) 
>>> 
>>> ## Fit model with lm() 
>>> fit1 <- lm(y ~ X) 
>>> 
>>> ## Fit model with lm.ridge() 
>>> fit2 <- lm.ridge(y ~ X) 
>>> 
>>> ## Compare coefficients 
>>> cbind(fit1$coefficients,c(NA,fit2$coef),coefficients(fit2)) 
>>> 
>>> [,1] [,2] [,3] 
>>> (Intercept) 0.99276001 NA 0.99276001 
>>> X1 -0.03980772 -0.04282391 -0.03980772 
>>> X2 1.11167179 1.06200476 1.11167179 
>>> 
>>> -- 
>>> 
>>> Simon Bonner 
>>> Assistant Professor of Environmetrics/ Director MMASc 
>>> Department of Statistical and Actuarial Sciences/Department of Biology
>>> University of Western Ontario 
>>> 
>>> Office: Western Science Centre rm 276 
>>> 
>>> Email: sbonner6 at uwo.ca | Telephone: 519-661-2111 x88205 | Fax: 
>>>519-661-3813 
>>> Twitter: @bonnerstatslab | Website: 
>>>http://simon.bonners.ca/bonner-lab/wpblog/ 
>>> 
>>>> -----Original Message----- 
>>>> From: R-devel [mailto:r-devel-bounces at r-project.org] On Behalf Of 
>>>>Nick 
>>>> Brown 
>>>> Sent: May 4, 2017 10:29 AM 
>>>> To: r-devel at r-project.org 
>>>> Subject: [Rd] lm() gives different results to lm.ridge() and SPSS 
>>>> 
>>>> Hallo, 
>>>> 
>>>> I hope I am posting to the right place. I was advised to try this 
>>>>list by Ben Bolker 
>>>> (https://twitter.com/bolkerb/status/859909918446497795). I also 
>>>>posted this 
>>>> question to StackOverflow 
>>>> 
>>>>(http://stackoverflow.com/questions/43771269/lm-gives-different-results
>>>>- 
>>>> from-lm-ridgelambda-0). I am a relative newcomer to R, but I wrote my
>>>>first 
>>>> program in 1975 and have been paid to program in about 15 different 
>>>> languages, so I have some general background knowledge. 
>>>> 
>>>> I have a regression from which I extract the coefficients like this: 
>>>> lm(y ~ x1 * x2, data=ds)$coef 
>>>> That gives: x1=0.40, x2=0.37, x1*x2=0.09 
>>>> 
>>>> When I do the same regression in SPSS, I get: 
>>>> beta(x1)=0.40, beta(x2)=0.37, beta(x1*x2)=0.14. 
>>>> So the main effects are in agreement, but there is quite a difference
>>>>in the 
>>>> coefficient for the interaction. 
>>>> 
>>>> X1 and X2 are correlated about .75 (yes, yes, I know - this model 
>>>>wasn't my 
>>>> idea, but it got published), so there is quite possibly something 
>>>>going on with 
>>>> collinearity. So I thought I'd try lm.ridge() to see if I can get an 
>>>>idea of where 
>>>> the problems are occurring. 
>>>> 
>>>> The starting point is to run lm.ridge() with lambda=0 (i.e., no ridge 
>>>>penalty) and 
>>>> check we get the same results as with lm(): 
>>>> lm.ridge(y ~ x1 * x2, lambda=0, data=ds)$coef 
>>>> x1=0.40, x2=0.37, x1*x2=0.14 
>>>> So lm.ridge() agrees with SPSS, but not with lm(). (Of course, 
>>>>lambda=0 is the 
>>>> default, so it can be omitted; I can alternate between including or 
>>>>deleting 
>>>> ".ridge" in the function call, and watch the coefficient for the 
>>>>interaction 
>>>> change.) 
>>>> 
>>>> What seems slightly strange to me here is that I assumed that 
>>>>lm.ridge() just 
>>>> piggybacks on lm() anyway, so in the specific case where lambda=0 and 
>>>>there 
>>>> is no "ridging" to do, I'd expect exactly the same results. 
>>>> 
>>>> Unfortunately there are 34,000 cases in the dataset, so a "minimal" 
>>>>reprex will 
>>>> not be easy to make, but I can share the data via Dropbox or 
>>>>something if that 
>>>> would help. 
>>>> 
>>>> I appreciate that when there is strong collinearity then all bets are
>>>>off in terms 
>>>> of what the betas mean, but I would really expect lm() and lm.ridge()
>>>>to give 
>>>> the same results. (I would be happy to ignore SPSS, but for the 
>>>>moment it's 
>>>> part of the majority!) 
>>>> 
>>>> Thanks for reading, 
>>>> Nick 
>> ______________________________________________ 
>> R-devel at r-project.org mailing list 
>> https://stat.ethz.ch/mailman/listinfo/r-devel 
> 
>-- 
>Peter Dalgaard, Professor, 
>Center for Statistics, Copenhagen Business School 
>Solbjerg Plads 3, 2000 Frederiksberg, Denmark 
>Phone: (+45)38153501 
>Office: A 4.23 
>Email: pd.mes at cbs.dk Priv: PDalgd at gmail.com 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> 
> [[alternative HTML version deleted]] 
> 
>______________________________________________ 
>R-devel at r-project.org mailing list 
>https://stat.ethz.ch/mailman/listinfo/r-devel 



	[[alternative HTML version deleted]]


From pdalgd at gmail.com  Sat May  6 10:57:59 2017
From: pdalgd at gmail.com (peter dalgaard)
Date: Sat, 6 May 2017 10:57:59 +0200
Subject: [Rd] lm() gives different results to lm.ridge() and SPSS
In-Reply-To: <1463049899.185952899.1494028158769.JavaMail.root@zimbra61-e11.priv.proxad.net>
References: <1463049899.185952899.1494028158769.JavaMail.root@zimbra61-e11.priv.proxad.net>
Message-ID: <25BCCE85-8274-4B67-B3F3-AC5F214A2427@gmail.com>


> On 6 May 2017, at 01:49 , Nick Brown <nick.brown at free.fr> wrote:
> 
> Hi John,
> 
> Thanks for the comment... but that appears to mean that SPSS has a big problem.  I have always been told that to include an interaction term in a regression, the only way is to do the multiplication by hand.  But then it seems to be impossible to stop SPSS from re-standardizing the variable that corresponds to the interaction term.  Am I missing something?  Is there a way to perform the regression with the interaction in SPSS without computing the interaction as a separate variable?

Just look at the unstandardized coefficients in SPSS. The standardized ones are of some usefulness, but it is limited in the case of syntesized regressors like product terms. I imagine that the interpretation also goes whacky for squared terms, dummy coded groupings, etc.

(Does SPSS really still not have an automated way of generating interaction terms? 1977 called.... googling.... Looks like GLM understands them, REGRESS not.)

-pd

> 
> Best,
> Nick
> 
> From: "John Fox" <jfox at mcmaster.ca>
> To: "Nick Brown" <nick.brown at free.fr>, "peter dalgaard" <pdalgd at gmail.com>
> Cc: r-devel at r-project.org
> Sent: Friday, 5 May, 2017 8:22:53 PM
> Subject: Re: [Rd] lm() gives different results to lm.ridge() and SPSS
> 
> Dear Nick,
> 
> 
> On 2017-05-05, 9:40 AM, "R-devel on behalf of Nick Brown"
> <r-devel-bounces at r-project.org on behalf of nick.brown at free.fr> wrote:
> 
> >>I conjecture that something in the vicinity of
> >> res <- lm(DEPRESSION ~ scale(ZMEAN_PA) + scale(ZDIVERSITY_PA) +
> >>scale(ZMEAN_PA * ZDIVERSITY_PA), data=dat)
> >>summary(res) 
> >> would reproduce the SPSS Beta values.
> >
> >Yes, that works. Thanks!
> 
> That you have to work hard in R to match the SPSS results isn?t such a bad
> thing when you factor in the observation that standardizing the
> interaction regressor, ZMEAN_PA * ZDIVERSITY_PA, separately from each of
> its components, ZMEAN_PA and ZDIVERSITY_PA, is nonsense.
> 
> Best,
>  John
> 
> -------------------------------------
> John Fox, Professor
> McMaster University
> Hamilton, Ontario, Canada
> Web: http://socserv.mcmaster.ca/jfox/
> 
> 
> > 
> >
> >----- Original Message -----
> >
> >From: "peter dalgaard" <pdalgd at gmail.com>
> >To: "Viechtbauer Wolfgang (SP)"
> ><wolfgang.viechtbauer at maastrichtuniversity.nl>, "Nick Brown"
> ><nick.brown at free.fr>
> >Cc: r-devel at r-project.org
> >Sent: Friday, 5 May, 2017 3:33:29 PM
> >Subject: Re: [Rd] lm() gives different results to lm.ridge() and SPSS
> >
> >Thanks, I was getting to try this, but got side tracked by actual work...
> >
> >Your analysis reproduces the SPSS unscaled estimates. It still remains to
> >figure out how Nick got
> >
> >> 
> >coefficients(lm(ZDEPRESSION ~ ZMEAN_PA * ZDIVERSITY_PA, data=s1))
> >
> >(Intercept) ZMEAN_PA ZDIVERSITY_PA ZMEAN_PA:ZDIVERSITY_PA
> >0.07342198 -0.39650356 -0.36569488 -0.09435788
> >
> >
> >which does not match your output. I suspect that ZMEAN_PA and
> >ZDIVERSITY_PA were scaled for this analysis (but the interaction term
> >still obviously is not). I conjecture that something in the vicinity of
> >
> >res <- lm(DEPRESSION ~ scale(ZMEAN_PA) + scale(ZDIVERSITY_PA) +
> >scale(ZMEAN_PA * ZDIVERSITY_PA), data=dat)
> >summary(res) 
> >
> >would reproduce the SPSS Beta values.
> >
> >
> >> On 5 May 2017, at 14:43 , Viechtbauer Wolfgang (SP)
> >><wolfgang.viechtbauer at maastrichtuniversity.nl> wrote:
> >> 
> >> I had no problems running regression models in SPSS and R that yielded
> >>the same results for these data.
> >> 
> >> The difference you are observing is from fitting different models. In
> >>R, you fitted: 
> >> 
> >> res <- lm(DEPRESSION ~ ZMEAN_PA * ZDIVERSITY_PA, data=dat)
> >> summary(res) 
> >> 
> >> The interaction term is the product of ZMEAN_PA and ZDIVERSITY_PA. This
> >>is not a standardized variable itself and not the same as "ZINTER_PA_C"
> >>in the png you showed, which is not a variable in the dataset, but can
> >>be created with: 
> >> 
> >> dat$ZINTER_PA_C <- with(dat, scale(ZMEAN_PA * ZDIVERSITY_PA))
> >> 
> >> If you want the same results as in SPSS, then you need to fit:
> >> 
> >> res <- lm(DEPRESSION ~ ZMEAN_PA + ZDIVERSITY_PA + ZINTER_PA_C,
> >>data=dat) 
> >> summary(res) 
> >> 
> >> This yields: 
> >> 
> >> Coefficients: 
> >> Estimate Std. Error t value Pr(>|t|)
> >> (Intercept) 6.41041 0.01722 372.21 <2e-16 ***
> >> ZMEAN_PA -1.62726 0.04200 -38.74 <2e-16 ***
> >> ZDIVERSITY_PA -1.50082 0.07447 -20.15 <2e-16 ***
> >> ZINTER_PA_C -0.58955 0.05288 -11.15 <2e-16 ***
> >> --- 
> >> Signif. codes: 0 ?***? 0.001 ?**? 0.01 ?*? 0.05 ?.? 0.1 ? ? 1
> >> 
> >> Exactly the same as in the png.
> >> 
> >> Peter already mentioned this as a possible reason for the discrepancy:
> >>https://stat.ethz.ch/pipermail/r-devel/2017-May/074191.html ("Is it
> >>perhaps the case that x1 and x2 have already been scaled to have
> >>standard deviation 1? In that case, x1*x2 won't be.")
> >> 
> >> Best, 
> >> Wolfgang 
> >> 
> >> -----Original Message-----
> >> From: R-devel [mailto:r-devel-bounces at r-project.org] On Behalf Of Nick
> >>Brown 
> >> Sent: Friday, May 05, 2017 10:40
> >> To: peter dalgaard
> >> Cc: r-devel at r-project.org
> >> Subject: Re: [Rd] lm() gives different results to lm.ridge() and SPSS
> >> 
> >> Hi, 
> >> 
> >> Here is (I hope) all the relevant output from R.
> >> 
> >>> mean(s1$ZDEPRESSION, na.rm=T) [1] -1.041546e-16 >
> >>>mean(s1$ZDIVERSITY_PA, na.rm=T) [1] -9.660583e-16 > mean(s1$ZMEAN_PA,
> >>>na.rm=T) [1] -5.430282e-15 > lm.ridge(ZDEPRESSION ~ ZMEAN_PA *
> >>>ZDIVERSITY_PA, data=s1)$coef ZMEAN_PA ZDIVERSITY_PA
> >>>ZMEAN_PA:ZDIVERSITY_PA
> >> -0.3962254 -0.3636026 -0.1425772 ## This is what I thought was the
> >>problem originally. :-)
> >> 
> >> 
> >>> coefficients(lm(ZDEPRESSION ~ ZMEAN_PA * ZDIVERSITY_PA, data=s1))
> >>>(Intercept) ZMEAN_PA ZDIVERSITY_PA ZMEAN_PA:ZDIVERSITY_PA
> >> 0.07342198 -0.39650356 -0.36569488 -0.09435788 >
> >>coefficients(lm.ridge(ZDEPRESSION ~ ZMEAN_PA * ZDIVERSITY_PA, data=s1))
> >>ZMEAN_PA ZDIVERSITY_PA ZMEAN_PA:ZDIVERSITY_PA
> >> 0.07342198 -0.39650356 -0.36569488 -0.09435788 The equivalent from SPSS
> >>is attached. The unstandardized coefficients in SPSS look nothing like
> >>those in R. The standardized coefficients in SPSS match the
> >>lm.ridge()$coef numbers very closely indeed, suggesting that the same
> >>algorithm may be in use.
> >> 
> >> I have put the dataset file, which is the untouched original I received
> >>from the authors, in this Dropbox folder:
> >>https://www.dropbox.com/sh/xsebjy55ius1ysb/AADwYUyV1bl6-iAw7ACuF1_La?dl=0
> >>. You can read it into R with this code (one variable needs to be
> >>standardized and centered; everything else is already in the file):
> >> 
> >> s1 <- read.csv("Emodiversity_Study1.csv", stringsAsFactors=FALSE)
> >>s1$ZDEPRESSION <- scale(s1$DEPRESSION)
> >> Hey, maybe R is fine and I've stumbled on a bug in SPSS? If so, I'm
> >>sure IBM will want to fix it quickly (ha ha ha).
> >> 
> >> Nick 
> >> 
> >> ----- Original Message -----
> >> 
> >> From: "peter dalgaard" <pdalgd at gmail.com>
> >> To: "Nick Brown" <nick.brown at free.fr>
> >> Cc: "Simon Bonner" <sbonner6 at uwo.ca>, r-devel at r-project.org
> >> Sent: Friday, 5 May, 2017 10:02:10 AM
> >> Subject: Re: [Rd] lm() gives different results to lm.ridge() and SPSS
> >> 
> >> I asked you before, but in case you missed it: Are you looking at the
> >>right place in SPSS output?
> >> 
> >> The UNstandardized coefficients should be comparable to R, i.e. the "B"
> >>column, not "Beta".
> >> 
> >> -pd 
> >> 
> >>> On 5 May 2017, at 01:58 , Nick Brown <nick.brown at free.fr> wrote:
> >>> 
> >>> Hi Simon, 
> >>> 
> >>> Yes, if I uses coefficients() I get the same results for lm() and
> >>>lm.ridge(). So that's consistent, at least.
> >>> 
> >>> Interestingly, the "wrong" number I get from lm.ridge()$coef agrees
> >>>with the value from SPSS to 5dp, which is an interesting coincidence if
> >>>these numbers have no particular external meaning in lm.ridge().
> >>> 
> >>> Kind regards, 
> >>> Nick 
> >>> 
> >>> ----- Original Message -----
> >>> 
> >>> From: "Simon Bonner" <sbonner6 at uwo.ca>
> >>> To: "Nick Brown" <nick.brown at free.fr>, r-devel at r-project.org
> >>> Sent: Thursday, 4 May, 2017 7:07:33 PM
> >>> Subject: RE: [Rd] lm() gives different results to lm.ridge() and SPSS
> >>> 
> >>> Hi Nick, 
> >>> 
> >>> I think that the problem here is your use of $coef to extract the
> >>>coefficients of the ridge regression. The help for lm.ridge states that
> >>>coef is a "matrix of coefficients, one row for each value of lambda.
> >>>Note that these are not on the original scale and are for use by the
> >>>coef method." 
> >>> 
> >>> I ran a small test with simulated data, code is copied below, and
> >>>indeed the output from lm.ridge differs depending on whether the
> >>>coefficients are accessed via $coef or via the coefficients() function.
> >>>The latter does produce results that match the output from lm.
> >>> 
> >>> I hope that helps.
> >>> 
> >>> Cheers, 
> >>> 
> >>> Simon 
> >>> 
> >>> ## Load packages
> >>> library(MASS) 
> >>> 
> >>> ## Set seed 
> >>> set.seed(8888) 
> >>> 
> >>> ## Set parameters
> >>> n <- 100 
> >>> beta <- c(1,0,1)
> >>> sigma <- .5 
> >>> rho <- .75 
> >>> 
> >>> ## Simulate correlated covariates
> >>> Sigma <- matrix(c(1,rho,rho,1),ncol=2)
> >>> X <- mvrnorm(n,c(0,0),Sigma=Sigma)
> >>> 
> >>> ## Simulate data
> >>> mu <- beta[1] + X %*% beta[-1]
> >>> y <- rnorm(n,mu,sigma)
> >>> 
> >>> ## Fit model with lm()
> >>> fit1 <- lm(y ~ X)
> >>> 
> >>> ## Fit model with lm.ridge()
> >>> fit2 <- lm.ridge(y ~ X)
> >>> 
> >>> ## Compare coefficients
> >>> cbind(fit1$coefficients,c(NA,fit2$coef),coefficients(fit2))
> >>> 
> >>> [,1] [,2] [,3] 
> >>> (Intercept) 0.99276001 NA 0.99276001
> >>> X1 -0.03980772 -0.04282391 -0.03980772
> >>> X2 1.11167179 1.06200476 1.11167179
> >>> 
> >>> -- 
> >>> 
> >>> Simon Bonner 
> >>> Assistant Professor of Environmetrics/ Director MMASc
> >>> Department of Statistical and Actuarial Sciences/Department of Biology
> >>> University of Western Ontario
> >>> 
> >>> Office: Western Science Centre rm 276
> >>> 
> >>> Email: sbonner6 at uwo.ca | Telephone: 519-661-2111 x88205 | Fax:
> >>>519-661-3813 
> >>> Twitter: @bonnerstatslab | Website:
> >>>http://simon.bonners.ca/bonner-lab/wpblog/
> >>> 
> >>>> -----Original Message-----
> >>>> From: R-devel [mailto:r-devel-bounces at r-project.org] On Behalf Of
> >>>>Nick 
> >>>> Brown 
> >>>> Sent: May 4, 2017 10:29 AM
> >>>> To: r-devel at r-project.org
> >>>> Subject: [Rd] lm() gives different results to lm.ridge() and SPSS
> >>>> 
> >>>> Hallo, 
> >>>> 
> >>>> I hope I am posting to the right place. I was advised to try this
> >>>>list by Ben Bolker
> >>>> (https://twitter.com/bolkerb/status/859909918446497795). I also
> >>>>posted this 
> >>>> question to StackOverflow
> >>>> 
> >>>>(http://stackoverflow.com/questions/43771269/lm-gives-different-results
> >>>>- 
> >>>> from-lm-ridgelambda-0). I am a relative newcomer to R, but I wrote my
> >>>>first 
> >>>> program in 1975 and have been paid to program in about 15 different
> >>>> languages, so I have some general background knowledge.
> >>>> 
> >>>> I have a regression from which I extract the coefficients like this:
> >>>> lm(y ~ x1 * x2, data=ds)$coef
> >>>> That gives: x1=0.40, x2=0.37, x1*x2=0.09
> >>>> 
> >>>> When I do the same regression in SPSS, I get:
> >>>> beta(x1)=0.40, beta(x2)=0.37, beta(x1*x2)=0.14.
> >>>> So the main effects are in agreement, but there is quite a difference
> >>>>in the 
> >>>> coefficient for the interaction.
> >>>> 
> >>>> X1 and X2 are correlated about .75 (yes, yes, I know - this model
> >>>>wasn't my 
> >>>> idea, but it got published), so there is quite possibly something
> >>>>going on with 
> >>>> collinearity. So I thought I'd try lm.ridge() to see if I can get an
> >>>>idea of where 
> >>>> the problems are occurring.
> >>>> 
> >>>> The starting point is to run lm.ridge() with lambda=0 (i.e., no ridge
> >>>>penalty) and 
> >>>> check we get the same results as with lm():
> >>>> lm.ridge(y ~ x1 * x2, lambda=0, data=ds)$coef
> >>>> x1=0.40, x2=0.37, x1*x2=0.14
> >>>> So lm.ridge() agrees with SPSS, but not with lm(). (Of course,
> >>>>lambda=0 is the
> >>>> default, so it can be omitted; I can alternate between including or
> >>>>deleting 
> >>>> ".ridge" in the function call, and watch the coefficient for the
> >>>>interaction 
> >>>> change.) 
> >>>> 
> >>>> What seems slightly strange to me here is that I assumed that
> >>>>lm.ridge() just
> >>>> piggybacks on lm() anyway, so in the specific case where lambda=0 and
> >>>>there 
> >>>> is no "ridging" to do, I'd expect exactly the same results.
> >>>> 
> >>>> Unfortunately there are 34,000 cases in the dataset, so a "minimal"
> >>>>reprex will 
> >>>> not be easy to make, but I can share the data via Dropbox or
> >>>>something if that
> >>>> would help. 
> >>>> 
> >>>> I appreciate that when there is strong collinearity then all bets are
> >>>>off in terms 
> >>>> of what the betas mean, but I would really expect lm() and lm.ridge()
> >>>>to give 
> >>>> the same results. (I would be happy to ignore SPSS, but for the
> >>>>moment it's 
> >>>> part of the majority!)
> >>>> 
> >>>> Thanks for reading,
> >>>> Nick 
> >> ______________________________________________
> >> R-devel at r-project.org mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-devel
> >
> >-- 
> >Peter Dalgaard, Professor,
> >Center for Statistics, Copenhagen Business School
> >Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> >Phone: (+45)38153501
> >Office: A 4.23 
> >Email: pd.mes at cbs.dk Priv: PDalgd at gmail.com
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >        [[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-devel at r-project.org mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From pperry at stern.nyu.edu  Sat May  6 17:36:09 2017
From: pperry at stern.nyu.edu (Patrick Perry)
Date: Sat, 06 May 2017 17:36:09 +0200
Subject: [Rd] xrealloc namespace conflict
Message-ID: <590DED69.30404@stern.nyu.edu>

I have a package on CRAN now (corpus-0.3.1) that is currently failing 
tests on Linux, but passing on all other architectures:

https://cran.r-project.org/web/checks/check_results_corpus.html

I believe that the issue arrises from a namespace class between 
"xrealloc", which my package provides for internal use, but which R also 
seems to provide (possibly as part of TRE in src/extra/tre/xmalloc.c). 
It looks like my package is not picking up my custom xrealloc, but using 
an xrealloc provided by R.

Besides the fact that I am linking to the wrong xrealloc, I think my 
tests are failing for the same reason that the following code segfaults 
on Linux (Debian, with R 3.4.0):

test <- inline::cfunction(language='C',
     otherdefs='void *xmalloc(size_t); void *xrealloc(void *, size_t);',
     body = 'void *ptr = xmalloc(256); xrealloc(ptr, 0); return 
R_NilValue;')
test()
## xrealloc: out of virtual memory

It seems that the R xrealloc doesn't like being given a size of 0, even 
though this behavior is well-defined for realloc (it should free the 
memory). Based on my failing CRAN tests, it looks like this is a 
Linux-specific issue.

Is there a way to modify my Makevars to force the linker to choose my 
version of xrealloc for my package-specific code? My current Makevars 
are at https://github.com/patperry/r-corpus/blob/master/src/Makevars

Thanks in advance for any help.


Patrick


	[[alternative HTML version deleted]]


From kasperdanielhansen at gmail.com  Sun May  7 20:40:02 2017
From: kasperdanielhansen at gmail.com (Kasper Daniel Hansen)
Date: Sun, 7 May 2017 14:40:02 -0400
Subject: [Rd] complex tests failure
In-Reply-To: <edac4326-f88b-5ce0-5c3e-c650823e8874@gmail.com>
References: <CAC2h7uvigzjy-BdKp3qanmvY+Lp-yFukDpD11cQ_0e+M8k26Dw@mail.gmail.com>
 <2b323dcb-f7c8-6798-b08f-c199ca82df7e@gmail.com>
 <CAC2h7uvD5oBVFHFCv_i9-As_MKYvHtvMmTtYSQyGW910+1n+Xg@mail.gmail.com>
 <d07219e9-dbdc-42a1-fa1a-cc5c70e99570@gmail.com>
 <edac4326-f88b-5ce0-5c3e-c650823e8874@gmail.com>
Message-ID: <CAC2h7uumZPNNhq7QSxV+S23EiSvkFnCtoUkS=1te3XUdS+LOcA@mail.gmail.com>

Great! Works for me on RHEL6.

Best,
Kasper

On Fri, May 5, 2017 at 9:23 AM, Tomas Kalibera <tomas.kalibera at gmail.com>
wrote:

> Thanks for the report, handled in configure in 72661 (R-devel).
> I'll also port to R-patched.
>
> Best
> Tomas
>
>
> On 05/04/2017 03:49 PM, Tomas Kalibera wrote:
>
>
> There is no way to control this at runtime.
> We will probably have to add a configure test.
>
> Best,
> Tomas
>
> On 05/04/2017 03:23 PM, Kasper Daniel Hansen wrote:
>
> Thanks.
>
> I assume there is no way to control this via. environment variables or
> configure settings?  Obviously that would be great for something like this
> which affects tests and seems to be a known problem for older C standard
> libraries.
>
> Best,
> Kasper
>
> On Thu, May 4, 2017 at 9:12 AM, Tomas Kalibera <tomas.kalibera at gmail.com>
> wrote:
>
>>
>> As a quick fix, you can undefine HAVE_CTANH in complex.c, somewhere after
>> including config.h
>> An internal substitute, which is implemented inside complex.c, will be
>> used.
>>
>> Best
>> Tomas
>>
>>
>>
>>
>> On 05/04/2017 02:57 PM, Kasper Daniel Hansen wrote:
>>
>>> For a while I have been getting that the complex tests fails on RHEL 6.
>>> The specific issue has to do with tanh (see below for full output from
>>> complex.Rout.fail).
>>>
>>> This is both with the stock compiler (GCC 4.4.7) and a compiler supplied
>>> through the conda project (GCC 4.8.5).  The compiler supplied through
>>> conda
>>> ends up linking R to certain system files, so the binary is not
>>> completely
>>> independent (although most dynamically linked libraries are coming from
>>> the
>>> conda installation).
>>>
>>> A search on R-devel reveals a discussion in April on an issue reported on
>>> Windows with a bug in tanh in old versions of the GNU C standard library;
>>> this seems relevant.  The discussion by Martin Maechler suggest "using
>>> R's
>>> internal substitute".  So how do I enable this?  Or does this requires
>>> updating the C standard library?
>>>
>>> ** From complex.Rout.fail
>>>
>>> stopifnot(identical(tanh(356+0i), 1+0i))
>>>>
>>> Error: identical(tanh(356 + (0+0i)), 1 + (0+0i)) is not TRUE
>>> In addition: Warning message:
>>> In tanh(356 + (0+0i)) : NaNs produced in function "tanh"
>>> Execution halted
>>>
>>> Best,
>>> Kasper
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>
>>
>>
>>
>
>
>

	[[alternative HTML version deleted]]


From spencer.graves at prodsyse.com  Sun May  7 21:56:34 2017
From: spencer.graves at prodsyse.com (Spencer Graves)
Date: Sun, 7 May 2017 14:56:34 -0500
Subject: [Rd] deparse(substitute(x)) fails in implied call to an S3 print
	method
Message-ID: <d57cd70d-50e6-5fae-e9a2-0566b367c9c8@prodsyse.com>

In an implied call to an S3 print method, deparse(substitute(x)) returns 
"x", regardless of the name of object in .GlobalEnv, as indicated in the 
following:


 > Xnamed <- 1
 > class(Xnamed) <- 'name.x'
 > print.name.x <- function(x, ...){
+   namex <- deparse(substitute(x))
+   cat('How can I get the name of x in .GlobalEnv?\n',
+       'deparse(substitute(x)) gives only ', namex, '\n')
+ }
 > Xnamed
How can I get the name of x in .GlobalEnv?
  deparse(substitute(x)) gives only  x


       My real application is print.findFn{sos}, which displays in a web 
browser.  If the results of multiple searches are "printed", the name of 
the object in .GlobalEnv could help the user keep track of what was done.

       Thanks,
       Spencer Graves


 > sessionInfo()
R version 3.4.0 (2017-04-21)
Platform: x86_64-apple-darwin15.6.0 (64-bit)
Running under: macOS Sierra 10.12.4

Matrix products: default
BLAS: 
/System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib
LAPACK: 
/Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRlapack.dylib

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods base

loaded via a namespace (and not attached):
[1] compiler_3.4.0 tools_3.4.0


From murdoch.duncan at gmail.com  Sun May  7 22:46:12 2017
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Sun, 7 May 2017 16:46:12 -0400
Subject: [Rd] deparse(substitute(x)) fails in implied call to an S3
 print method
In-Reply-To: <d57cd70d-50e6-5fae-e9a2-0566b367c9c8@prodsyse.com>
References: <d57cd70d-50e6-5fae-e9a2-0566b367c9c8@prodsyse.com>
Message-ID: <caf36374-138d-a6f7-15e4-3ece7e80117d@gmail.com>

On 07/05/2017 3:56 PM, Spencer Graves wrote:
> In an implied call to an S3 print method, deparse(substitute(x)) returns
> "x", regardless of the name of object in .GlobalEnv, as indicated in the
> following:
>
>
>  > Xnamed <- 1
>  > class(Xnamed) <- 'name.x'
>  > print.name.x <- function(x, ...){
> +   namex <- deparse(substitute(x))
> +   cat('How can I get the name of x in .GlobalEnv?\n',
> +       'deparse(substitute(x)) gives only ', namex, '\n')
> + }
>  > Xnamed
> How can I get the name of x in .GlobalEnv?
>   deparse(substitute(x)) gives only  x
>
>
>        My real application is print.findFn{sos}, which displays in a web
> browser.  If the results of multiple searches are "printed", the name of
> the object in .GlobalEnv could help the user keep track of what was done.

I don't think there's any way around this.  Auto-printing of Xnamed 
isn't equivalent to print(Xnamed); it doesn't happen until after Xnamed 
is evaluated.  It assigns the result to a local variable called x, and 
calls print(x).  (All of this is done in C code, and at that point I 
don't think the original expression "Xnamed" is available any more.)

For your application, you may have to allow users to attach names to the 
object.  I don't know how you're creating those things, but part of the 
work could save a name as an attribute "SOSname", and then the print 
method would use that.

Duncan Murdoch


>
>        Thanks,
>        Spencer Graves
>
>
>  > sessionInfo()
> R version 3.4.0 (2017-04-21)
> Platform: x86_64-apple-darwin15.6.0 (64-bit)
> Running under: macOS Sierra 10.12.4
>
> Matrix products: default
> BLAS:
> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib
> LAPACK:
> /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRlapack.dylib
>
> locale:
> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods base
>
> loaded via a namespace (and not attached):
> [1] compiler_3.4.0 tools_3.4.0
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From spencer.graves at prodsyse.com  Sun May  7 22:52:19 2017
From: spencer.graves at prodsyse.com (Spencer Graves)
Date: Sun, 7 May 2017 15:52:19 -0500
Subject: [Rd] deparse(substitute(x)) fails in implied call to an S3
 print method
In-Reply-To: <caf36374-138d-a6f7-15e4-3ece7e80117d@gmail.com>
References: <d57cd70d-50e6-5fae-e9a2-0566b367c9c8@prodsyse.com>
 <caf36374-138d-a6f7-15e4-3ece7e80117d@gmail.com>
Message-ID: <af171f5a-4bc7-7d7f-edde-138041c8bb9a@prodsyse.com>



On 2017-05-07 3:46 PM, Duncan Murdoch wrote:
> On 07/05/2017 3:56 PM, Spencer Graves wrote:
>> In an implied call to an S3 print method, deparse(substitute(x)) returns
>> "x", regardless of the name of object in .GlobalEnv, as indicated in the
>> following:
>>
>>
>>  > Xnamed <- 1
>>  > class(Xnamed) <- 'name.x'
>>  > print.name.x <- function(x, ...){
>> +   namex <- deparse(substitute(x))
>> +   cat('How can I get the name of x in .GlobalEnv?\n',
>> +       'deparse(substitute(x)) gives only ', namex, '\n')
>> + }
>>  > Xnamed
>> How can I get the name of x in .GlobalEnv?
>>   deparse(substitute(x)) gives only  x
>>
>>
>>        My real application is print.findFn{sos}, which displays in a web
>> browser.  If the results of multiple searches are "printed", the name of
>> the object in .GlobalEnv could help the user keep track of what was 
>> done.
>
> I don't think there's any way around this.  Auto-printing of Xnamed 
> isn't equivalent to print(Xnamed); it doesn't happen until after 
> Xnamed is evaluated.  It assigns the result to a local variable called 
> x, and calls print(x).  (All of this is done in C code, and at that 
> point I don't think the original expression "Xnamed" is available any 
> more.)
>
> For your application, you may have to allow users to attach names to 
> the object.  I don't know how you're creating those things, but part 
> of the work could save a name as an attribute "SOSname", and then the 
> print method would use that.


       Thanks for the reply.  I'm already saving match.call() and 
displaying that.  That's adequate, but I was hoping to do better.


        Spencer Graves

>
> Duncan Murdoch
>
>
>>
>>        Thanks,
>>        Spencer Graves
>>
>>
>>  > sessionInfo()
>> R version 3.4.0 (2017-04-21)
>> Platform: x86_64-apple-darwin15.6.0 (64-bit)
>> Running under: macOS Sierra 10.12.4
>>
>> Matrix products: default
>> BLAS:
>> /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib 
>>
>> LAPACK:
>> /Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRlapack.dylib 
>>
>>
>> locale:
>> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
>>
>> attached base packages:
>> [1] stats     graphics  grDevices utils     datasets  methods base
>>
>> loaded via a namespace (and not attached):
>> [1] compiler_3.4.0 tools_3.4.0
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>


From antonink at idi.ntnu.no  Mon May  8 14:08:06 2017
From: antonink at idi.ntnu.no (Antonin Klima)
Date: Mon, 8 May 2017 14:08:06 +0200
Subject: [Rd] A few suggestions and perspectives from a PhD student
In-Reply-To: <CAP01uRk1NzjRjs80hHj3mJ+n1iaESfxj00t88fSkt_wS2pV1rg@mail.gmail.com>
References: <FCBC5362-8436-487D-95A0-65CB5924BD29@idi.ntnu.no>
 <CAP01uRk1NzjRjs80hHj3mJ+n1iaESfxj00t88fSkt_wS2pV1rg@mail.gmail.com>
Message-ID: <AE0E15A4-5A91-4E00-ABF8-6463635E45D4@idi.ntnu.no>

Thanks for the answers,

I?m aware of the ?.? option, just wanted to give a very simple example.

But the lapply ??' parameter use has eluded me and thanks for enlightening me. 

What do you mean by messing up the call stack. As far as I understand it, piping should translate into same code as deep nesting. So then I only see a tiny downside for debugging here. No loss of time/space efficiency or anything. With a change of inadvertent error in your example, coming from the fact that a variable is being reused and noone now checks for me whether it is being passed between the lines. And with having to specify the variable every single time. For me, that solution is clearly inferior.

Too bad you didn?t find my other comments interesting though.

>Why do you think being implemented in a contributed package restricts
>the usefulness of a feature?

I guess it depends on your philosophy. It may not restrict it per say, although it would make a lot of sense to me reusing the bash-style ?|' and have a shorter, more readable version. One has extra dependence on a package for an item that fits the language so well that it should be its part.  It is without doubt my most used operator at least. Going to some of my folders I found 101 uses in 750 lines, and 132 uses in 3303 lines. I would compare it to having a computer game being really good with a fan-created mod, but lacking otherwise. :) 

So to me, it makes sense that if there is no doubt that a feature improves the language, and especially if people extensively use it through a package already, it should be part of the ?standard?. Question is whether it is indeed very popular, and whether you share my view. But that?s now up to you, I just wanted to point it out I guess.

Best Regards,
Antonin

> On 05 May 2017, at 22:33, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> 
> Regarding the anonymous-function-in-a-pipeline point one can already
> do this which does use brackets but even so it involves fewer
> characters than the example shown.  Here { . * 2 } is basically a
> lambda whose argument is dot. Would this be sufficient?
> 
>  library(magrittr)
> 
>  1.5 %>% { . * 2 }
>  ## [1] 3
> 
> Regarding currying note that with magrittr Ista's code could be written as:
> 
>  1:5 %>% lapply(foo, y = 3)
> 
> or at the expense of slightly more verbosity:
> 
>  1:5 %>% Map(f = . %>% foo(y = 3))
> 
> 
> On Fri, May 5, 2017 at 1:00 PM, Antonin Klima <antonink at idi.ntnu.no> wrote:
>> Dear Sir or Madam,
>> 
>> I am in 2nd year of my PhD in bioinformatics, after taking my Master?s in computer science, and have been using R heavily during my PhD. As such, I have put together a list of certain features in R that, in my opinion, would be beneficial to add, or could be improved. The first two are already implemented in packages, but given that it is implemented as user-defined operators, it greatly restricts its usefulness. I hope you will find my suggestions interesting. If you find time, I will welcome any feedback as to whether you find the suggestions useful, or why you do not think they should be implemented. I will also welcome if you enlighten me with any features I might be unaware of, that might solve the issues I have pointed out below.
>> 
>> 1) piping
>> Currently available in package magrittr, piping makes the code better readable by having the line start at its natural starting point, and following with functions that are applied - in order. The readability of several nested calls with a number of parameters each is almost zero, it?s almost as if one would need to come up with the solution himself. Pipeline in comparison is very straightforward, especially together with the point (2).
>> 
>> The package here works rather good nevertheless, the shortcomings of piping not being native are not quite as severe as in point (2). Nevertheless, an intuitive symbol such as | would be helpful, and it sometimes bothers me that I have to parenthesize anonymous function, which would probably not be required in a native pipe-operator, much like it is not required in f.ex. lapply. That is,
>> 1:5 %>% function(x) x+2
>> should be totally fine
>> 
>> 2) currying
>> Currently available in package Curry. The idea is that, having a function such as foo = function(x, y) x+y, one would like to write for example lapply(foo(3), 1:5), and have the interpreter figure out ok, foo(3) does not make a value result, but it can still give a function result - a function of y. This would be indeed most useful for various apply functions, rather than writing function(x) foo(3,x).
>> 
>> I suggest that currying would make the code easier to write, and more readable, especially when using apply functions. One might imagine that there could be some confusion with such a feature, especially from people unfamiliar with functional programming, although R already does take function as first-order arguments, so it could be just fine. But one could address it with special syntax, such as $foo(3) [$foo(x=3)] for partial application.  The current currying package has very limited usefulness, as, being limited by the user-defined operator framework, it only rarely can contribute to less code/more readability. Compare yourself:
>> $foo(x=3) vs foo %<% 3
>> goo = function(a,b,c)
>> $goo(b=3) vs goo %><% list(b=3)
>> 
>> Moreover, one would often like currying to have highest priority. For example, when piping:
>> data %>% foo %>% foo1 %<% 3
>> if one wants to do data %>% foo %>% $foo(x=3)
>> 
>> 3) Code executable only when running the script itself
>> Whereas the first two suggestions are somewhat stealing from Haskell and the like, this suggestion would be stealing from Python. I?m building quite a complicated pipeline, using S4 classes. After defining the class and its methods, I also define how to build the class to my likings, based on my input data, using various now-defined methods. So I end up having a list of command line arguments to process, and the way to create the class instance based on them. If I write it to the class file, however, I end up running the code when it is sourced from the next step in the pipeline, that needs the previous class definitions.
>> 
>> A feature such as pythonic ?if __name__ == __main__? would thus be useful. As it is, I had to create run scripts as separate files. Which is actually not so terrible, given the class and its methods often span a few hundred lines, but still.
>> 
>> 4) non-exported global variables
>> I also find it lacking, that I seem to be unable to create constants that would not get passed to files that source the class definition. That is, if class1 features global constant CONSTANT=3, then if class2 sources class1, it will also include the constant. This 1) clutters the namespace when running the code interactively, 2) potentially overwrites the constants in case of nameclash. Some kind of export/nonexport variable syntax, or symbolic import, or namespace would be useful. I know if I converted it to a package I would get at least something like a namespace, but still.
>> 
>> I understand that the variable cannot just not be imported, in general, as the functions will generally rely on it (otherwise it wouldn?t have to be there). But one could consider hiding it in an implicit namespace for the file, for example.
>> 
>> 5) S4 methods with same name, for different classes
>> Say I have an S4 class called datasetSingle, and another S4 class called datasetMulti, which gathers up a number of datasetSingle classes, and adds some extra functionality on top. The datasetSingle class may have a method replicates, that returns a named vector assigning replicate number to experiment names of the dataset. But I would also like to have a function with the same name for the datasetMulti class, that returns for data frame, or list, covering replicate numbers for all the datasets included.
>> 
>> But then, I need to setGeneric for the method. But if I set generic before both implementations, I will reset the generic in the second call, losing the definition for ?replicates? for datasetSingle. Skipping this in the code for datasetMulti means that 1) I have to remember that I had the function defined for datasetSingle, 2) if I remove the function or change its name in datasetSingle, I now have to change the datasetMulti class file too. Moreover, if I would like to have a different generic for the datasetMulti version, I have to change it not in datasetMulti class file, but in the datasetSingle file, where it might not make much sense. In this case, I wanted to have another argument ?datasets?, which would return the replicates only for the datasets specified, rather than for all.
>> 
>> I made a wrapper that could circumvent the first issue, but the second issue is not easy to circumvent.
>> 
>> 6) Many parameters freeze S4 method calls
>> If I specify ca over 6 parameters for an S4 method, I would often get a ?freeze? on the method call. The process would eat up a lot of memory before going into the call, upon which it would execute the call as normal (if it didn?t run out of memory or I didn?t run out of patience). Subsequent calls of the method would not include this overhead. The amount of memory this could take could be in gigabytes, and the time in minutes. I suspect this might be due to generating an entry in call table for each accepted signature. It can be circumvented, but sure isn?t a behaviour one would expect.
>> 
>> 7) Default values for S4 methods
>> It would seem that it is not possible to set up default parameters for an S4 method in a usual way of definiton = function (x, y=5). I resorted to making class unions with ?missing? for signatures on the call, with the call starting with if(missing(param)) param=DEFAULT_VALUE, but it certainly does not improve readability or ease of coding.
>> 
>> 
>> Thank you for your time if you have finished reading thus far. :) Looking forward to any answer.
>> 
>> Yours Sincerely,
>> Antonin Klima
>> 
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
> 
> 
> 
> -- 
> Statistics & Software Consulting
> GKX Group, GKX Associates Inc.
> tel: 1-877-GKX-GROUP
> email: ggrothendieck at gmail.com


From istazahn at gmail.com  Mon May  8 16:37:37 2017
From: istazahn at gmail.com (Ista Zahn)
Date: Mon, 8 May 2017 10:37:37 -0400
Subject: [Rd] A few suggestions and perspectives from a PhD student
In-Reply-To: <AE0E15A4-5A91-4E00-ABF8-6463635E45D4@idi.ntnu.no>
References: <FCBC5362-8436-487D-95A0-65CB5924BD29@idi.ntnu.no>
 <CAP01uRk1NzjRjs80hHj3mJ+n1iaESfxj00t88fSkt_wS2pV1rg@mail.gmail.com>
 <AE0E15A4-5A91-4E00-ABF8-6463635E45D4@idi.ntnu.no>
Message-ID: <CA+vqiLG=mDFOiWScr4TCVjkwszXo+38ZTFB4T-7UvqbKzwy+7Q@mail.gmail.com>

On Mon, May 8, 2017 at 8:08 AM, Antonin Klima <antonink at idi.ntnu.no> wrote:
> Thanks for the answers,
>
> I?m aware of the ?.? option, just wanted to give a very simple example.
>
> But the lapply ??' parameter use has eluded me and thanks for enlightening me.
>
> What do you mean by messing up the call stack. As far as I understand it, piping should translate into same code as deep nesting.

Perhaps, but then magrittr is not really a pipe. Here is a simple example

library(magrittr)
data.frame(x = 1) %>%
    subset(y == 1)
traceback()

> Error in eval(e, x, parent.frame()) : object 'y' not found
> 12: eval(e, x, parent.frame())
11: eval(e, x, parent.frame())
10: subset.data.frame(., y == 1)
9: subset(., y == 1)
8: function_list[[k]](value)
7: withVisible(function_list[[k]](value))
6: freduce(value, `_function_list`)
5: `_fseq`(`_lhs`)
4: eval(quote(`_fseq`(`_lhs`)), env, env)
3: eval(quote(`_fseq`(`_lhs`)), env, env)
2: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))
1: data.frame(x = 1) %>% subset(y == 1)
>

subset(data.frame(x = 1),
      y == 1)
traceback()

> Error in eval(e, x, parent.frame()) : object 'y' not found
> 4: eval(e, x, parent.frame())
3: eval(e, x, parent.frame())
2: subset.data.frame(data.frame(x = 1), y == 1)
1: subset(data.frame(x = 1), y == 1)
>

It does pollute the call stack, making debugging harder.

 So then I only see a tiny downside for debugging here. No loss of
time/space efficiency or anything. With a change of inadvertent error
in your example, coming from the fact that a variable is being reused
and noone now checks for me whether it is being passed between the
lines. And with having to specify the variable every single time. For
me, that solution is clearly inferior.

There are tradeoffs. As demonstrated above, the pipe is clearly
inferior in that it is doing a lot of complicated stuff under the
hood, and when you try to traceback() through the call stack you have
to sift through all that complicated stuff. That's a pretty big
drawback in my opinion.

>
> Too bad you didn?t find my other comments interesting though.

I did not say that.

>
>>Why do you think being implemented in a contributed package restricts
>>the usefulness of a feature?
>
> I guess it depends on your philosophy. It may not restrict it per say, although it would make a lot of sense to me reusing the bash-style ?|' and have a shorter, more readable version. One has extra dependence on a package for an item that fits the language so well that it should be its part.  It is without doubt my most used operator at least. Going to some of my folders I found 101 uses in 750 lines, and 132 uses in 3303 lines. I would compare it to having a computer game being really good with a fan-created mod, but lacking otherwise. :)

One of the key strengths of R is that packages are not akin to "fan
created mods". They are a central and necessary part of the R system.

>
> So to me, it makes sense that if there is no doubt that a feature improves the language, and especially if people extensively use it through a package already, it should be part of the ?standard?. Question is whether it is indeed very popular, and whether you share my view. But that?s now up to you, I just wanted to point it out I guess.

>
> Best Regards,
> Antonin
>
>> On 05 May 2017, at 22:33, Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
>>
>> Regarding the anonymous-function-in-a-pipeline point one can already
>> do this which does use brackets but even so it involves fewer
>> characters than the example shown.  Here { . * 2 } is basically a
>> lambda whose argument is dot. Would this be sufficient?
>>
>>  library(magrittr)
>>
>>  1.5 %>% { . * 2 }
>>  ## [1] 3
>>
>> Regarding currying note that with magrittr Ista's code could be written as:
>>
>>  1:5 %>% lapply(foo, y = 3)
>>
>> or at the expense of slightly more verbosity:
>>
>>  1:5 %>% Map(f = . %>% foo(y = 3))
>>
>>
>> On Fri, May 5, 2017 at 1:00 PM, Antonin Klima <antonink at idi.ntnu.no> wrote:
>>> Dear Sir or Madam,
>>>
>>> I am in 2nd year of my PhD in bioinformatics, after taking my Master?s in computer science, and have been using R heavily during my PhD. As such, I have put together a list of certain features in R that, in my opinion, would be beneficial to add, or could be improved. The first two are already implemented in packages, but given that it is implemented as user-defined operators, it greatly restricts its usefulness. I hope you will find my suggestions interesting. If you find time, I will welcome any feedback as to whether you find the suggestions useful, or why you do not think they should be implemented. I will also welcome if you enlighten me with any features I might be unaware of, that might solve the issues I have pointed out below.
>>>
>>> 1) piping
>>> Currently available in package magrittr, piping makes the code better readable by having the line start at its natural starting point, and following with functions that are applied - in order. The readability of several nested calls with a number of parameters each is almost zero, it?s almost as if one would need to come up with the solution himself. Pipeline in comparison is very straightforward, especially together with the point (2).
>>>
>>> The package here works rather good nevertheless, the shortcomings of piping not being native are not quite as severe as in point (2). Nevertheless, an intuitive symbol such as | would be helpful, and it sometimes bothers me that I have to parenthesize anonymous function, which would probably not be required in a native pipe-operator, much like it is not required in f.ex. lapply. That is,
>>> 1:5 %>% function(x) x+2
>>> should be totally fine
>>>
>>> 2) currying
>>> Currently available in package Curry. The idea is that, having a function such as foo = function(x, y) x+y, one would like to write for example lapply(foo(3), 1:5), and have the interpreter figure out ok, foo(3) does not make a value result, but it can still give a function result - a function of y. This would be indeed most useful for various apply functions, rather than writing function(x) foo(3,x).
>>>
>>> I suggest that currying would make the code easier to write, and more readable, especially when using apply functions. One might imagine that there could be some confusion with such a feature, especially from people unfamiliar with functional programming, although R already does take function as first-order arguments, so it could be just fine. But one could address it with special syntax, such as $foo(3) [$foo(x=3)] for partial application.  The current currying package has very limited usefulness, as, being limited by the user-defined operator framework, it only rarely can contribute to less code/more readability. Compare yourself:
>>> $foo(x=3) vs foo %<% 3
>>> goo = function(a,b,c)
>>> $goo(b=3) vs goo %><% list(b=3)
>>>
>>> Moreover, one would often like currying to have highest priority. For example, when piping:
>>> data %>% foo %>% foo1 %<% 3
>>> if one wants to do data %>% foo %>% $foo(x=3)
>>>
>>> 3) Code executable only when running the script itself
>>> Whereas the first two suggestions are somewhat stealing from Haskell and the like, this suggestion would be stealing from Python. I?m building quite a complicated pipeline, using S4 classes. After defining the class and its methods, I also define how to build the class to my likings, based on my input data, using various now-defined methods. So I end up having a list of command line arguments to process, and the way to create the class instance based on them. If I write it to the class file, however, I end up running the code when it is sourced from the next step in the pipeline, that needs the previous class definitions.
>>>
>>> A feature such as pythonic ?if __name__ == __main__? would thus be useful. As it is, I had to create run scripts as separate files. Which is actually not so terrible, given the class and its methods often span a few hundred lines, but still.
>>>
>>> 4) non-exported global variables
>>> I also find it lacking, that I seem to be unable to create constants that would not get passed to files that source the class definition. That is, if class1 features global constant CONSTANT=3, then if class2 sources class1, it will also include the constant. This 1) clutters the namespace when running the code interactively, 2) potentially overwrites the constants in case of nameclash. Some kind of export/nonexport variable syntax, or symbolic import, or namespace would be useful. I know if I converted it to a package I would get at least something like a namespace, but still.
>>>
>>> I understand that the variable cannot just not be imported, in general, as the functions will generally rely on it (otherwise it wouldn?t have to be there). But one could consider hiding it in an implicit namespace for the file, for example.
>>>
>>> 5) S4 methods with same name, for different classes
>>> Say I have an S4 class called datasetSingle, and another S4 class called datasetMulti, which gathers up a number of datasetSingle classes, and adds some extra functionality on top. The datasetSingle class may have a method replicates, that returns a named vector assigning replicate number to experiment names of the dataset. But I would also like to have a function with the same name for the datasetMulti class, that returns for data frame, or list, covering replicate numbers for all the datasets included.
>>>
>>> But then, I need to setGeneric for the method. But if I set generic before both implementations, I will reset the generic in the second call, losing the definition for ?replicates? for datasetSingle. Skipping this in the code for datasetMulti means that 1) I have to remember that I had the function defined for datasetSingle, 2) if I remove the function or change its name in datasetSingle, I now have to change the datasetMulti class file too. Moreover, if I would like to have a different generic for the datasetMulti version, I have to change it not in datasetMulti class file, but in the datasetSingle file, where it might not make much sense. In this case, I wanted to have another argument ?datasets?, which would return the replicates only for the datasets specified, rather than for all.
>>>
>>> I made a wrapper that could circumvent the first issue, but the second issue is not easy to circumvent.
>>>
>>> 6) Many parameters freeze S4 method calls
>>> If I specify ca over 6 parameters for an S4 method, I would often get a ?freeze? on the method call. The process would eat up a lot of memory before going into the call, upon which it would execute the call as normal (if it didn?t run out of memory or I didn?t run out of patience). Subsequent calls of the method would not include this overhead. The amount of memory this could take could be in gigabytes, and the time in minutes. I suspect this might be due to generating an entry in call table for each accepted signature. It can be circumvented, but sure isn?t a behaviour one would expect.
>>>
>>> 7) Default values for S4 methods
>>> It would seem that it is not possible to set up default parameters for an S4 method in a usual way of definiton = function (x, y=5). I resorted to making class unions with ?missing? for signatures on the call, with the call starting with if(missing(param)) param=DEFAULT_VALUE, but it certainly does not improve readability or ease of coding.
>>>
>>>
>>> Thank you for your time if you have finished reading thus far. :) Looking forward to any answer.
>>>
>>> Yours Sincerely,
>>> Antonin Klima
>>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>>
>>
>> --
>> Statistics & Software Consulting
>> GKX Group, GKX Associates Inc.
>> tel: 1-877-GKX-GROUP
>> email: ggrothendieck at gmail.com
>


From h.wickham at gmail.com  Tue May  9 00:29:09 2017
From: h.wickham at gmail.com (Hadley Wickham)
Date: Mon, 8 May 2017 17:29:09 -0500
Subject: [Rd] A few suggestions and perspectives from a PhD student
In-Reply-To: <CA+vqiLG=mDFOiWScr4TCVjkwszXo+38ZTFB4T-7UvqbKzwy+7Q@mail.gmail.com>
References: <FCBC5362-8436-487D-95A0-65CB5924BD29@idi.ntnu.no>
 <CAP01uRk1NzjRjs80hHj3mJ+n1iaESfxj00t88fSkt_wS2pV1rg@mail.gmail.com>
 <AE0E15A4-5A91-4E00-ABF8-6463635E45D4@idi.ntnu.no>
 <CA+vqiLG=mDFOiWScr4TCVjkwszXo+38ZTFB4T-7UvqbKzwy+7Q@mail.gmail.com>
Message-ID: <CABdHhvHusDqyGOxp0jd7KeQvYRu1P+NiHtYZVk0Q173CnL6-sA@mail.gmail.com>

> There are tradeoffs. As demonstrated above, the pipe is clearly
> inferior in that it is doing a lot of complicated stuff under the
> hood, and when you try to traceback() through the call stack you have
> to sift through all that complicated stuff. That's a pretty big
> drawback in my opinion.

To be precise, that is a problem with the current implementation of
the pipe. It's not a limitation of the pipe per se.

Hadley

-- 
http://hadley.nz


From kirill.mueller at ivt.baug.ethz.ch  Tue May  9 09:42:56 2017
From: kirill.mueller at ivt.baug.ethz.ch (=?UTF-8?Q?Kirill_M=c3=bcller?=)
Date: Tue, 9 May 2017 09:42:56 +0200
Subject: [Rd] source(), parse(), and foreign UTF-8 characters
Message-ID: <30a59534-7d44-c469-c5d8-08a81b519504@ivt.baug.ethz.ch>

Hi


I'm having trouble sourcing or parsing a UTF-8 file that contains 
characters that are not representable in the current locale ("foreign 
characters") on Windows. The source() function stops with an error, the 
parse() function reencodes all foreign characters using the <U+xxxx> 
notation. I have added a reproducible example below the message.

This seems well within the bounds of documented behavior, although the 
documentation to source() could mention that the file can't contain 
foreign characters. Still, I'd prefer if UTF-8 "just worked" in R, and 
I'm willing to invest substantial time to help with that. Before 
starting to write a detailed proposal, I feel that I need a better 
understanding of the problem, and I'm grateful for any feedback you 
might have.

I have looked into character encodings in the context of the dplyr 
package, and I have observed the following behavior:

- Strings are treated preferentially in the native encoding
- Only upon specific request (via translateCharUTF8() or enc2utf8() or 
...), they are translated to UTF-8 and marked as such
- On UTF-8 systems, strings are never marked as UTF-8
- ASCII strings are marked as ASCII internally, but this information 
doesn't seem to be available, e.g., Encoding() returns "unknown" for 
such strings
- Most functions in R are encoding-agnostic: they work the same 
regardless if they receive a native or UTF-8 encoded string if they are 
properly tagged
- One important difference are symbols, which must be in the native 
encoding (and are always converted to native encoding, using <U+xxxx> 
escapes)
- I/O is centered around the native encoding, e.g., writeLines() always 
reencodes to the native encoding
- There is the "bytes" encoding which avoids reencoding.

I haven't looked into serialization or plot devices yet.

The conclusion to the "UTF-8 manifesto" [1] suggests "... to use UTF-8 
narrow strings everywhere and convert them back and forth when using 
platform APIs that don?t support UTF-8 ...". (It is written in the 
context of the UTF-16 encoding used internally on Windows, but seems to 
apply just the same here for the native encoding.) I think that Unicode 
support in R could be greatly improved if we follow these guidelines. 
This seems to mean:

- Convert strings to UTF-8 as soon as possible, and mark them as such 
(also on systems where UTF-8 is the native encoding)
- Translate to native only upon specific request, e.g., in calls to API 
functions or perhaps for .C()
- Use UTF-8 for symbols
- Avoid the forced round-trip to the native encoding in I/O functions 
and for parsing (but still read/write native by default)
- Carefully look into serialization and plot devices
- Add helper functions that simplify mundane tasks such as 
reading/writing a UTF-8 encoded file

I'm sure I've missed many potential pitfalls, your input is greatly 
appreciated. Thanks for your attention.

Further ressources: A write-up by Prof. Ripley [2], a section in R-ints 
[3], a blog post by Ista Zahn [4], a StackOverflow search [5].


Best regards

Kirill



[1] http://utf8everywhere.org/#conclusions

[2] https://developer.r-project.org/Encodings_and_R.html

[3] 
https://cran.r-project.org/doc/manuals/r-devel/R-ints.html#Encodings-for-CHARSXPs

[3] 
http://people.fas.harvard.edu/~izahn/posts/reading-data-with-non-native-encoding-in-r/

[4] 
http://stackoverflow.com/search?tab=votes&q=%5br%5d%20encoding%20windows%20is%3aquestion



# Use one of the following:
id <- "Gl\u00fcck"
id <- "\u5e78\u798f"
id <- "\u0441\u0447\u0430\u0441\u0442\u044c\u0435"
id <- "\ud589\ubcf5"

file_contents <- paste0('"', id, '"')
Encoding(file_contents)
raw_file_contents <- charToRaw(file_contents)

path <- tempfile(fileext = ".R")
writeBin(raw_file_contents, path)
file.size(path)
length(raw_file_contents)

# Escapes the string
parse(text = file_contents)

# Throws an error
print(source(path, encoding = "UTF-8"))


From berger at mpiib-berlin.mpg.de  Tue May  9 09:47:47 2017
From: berger at mpiib-berlin.mpg.de (Hilmar Berger)
Date: Tue, 9 May 2017 09:47:47 +0200
Subject: [Rd] A few suggestions and perspectives from a PhD student
In-Reply-To: <CA+vqiLG=mDFOiWScr4TCVjkwszXo+38ZTFB4T-7UvqbKzwy+7Q@mail.gmail.com>
References: <FCBC5362-8436-487D-95A0-65CB5924BD29@idi.ntnu.no>
 <CAP01uRk1NzjRjs80hHj3mJ+n1iaESfxj00t88fSkt_wS2pV1rg@mail.gmail.com>
 <AE0E15A4-5A91-4E00-ABF8-6463635E45D4@idi.ntnu.no>
 <CA+vqiLG=mDFOiWScr4TCVjkwszXo+38ZTFB4T-7UvqbKzwy+7Q@mail.gmail.com>
Message-ID: <4fdf0b2c-0393-aab3-dc93-84345baa7f67@mpiib-berlin.mpg.de>

Hi,

On 08/05/17 16:37, Ista Zahn wrote:
> One of the key strengths of R is that packages are not akin to "fan
> created mods". They are a central and necessary part of the R system.
>
I would tend to disagree here. R packages are in their majority not 
maintained by the core R developers. Concepts, features and lifetime 
depend mainly on the maintainers of the package (even though in theory 
GPL will allow to somebody to take over anytime). Several packages that 
are critical for processing big data and providing "modern" 
visualizations introduce concepts quite different from the legacy S/R 
language. I do feel that in a way, current core R shows strongly its 
origin in S, while modern concepts (e.g. data.table, dplyr, ggplot, ...) 
are often only available via extension packages. This is fine if one 
considers R to be a statistical toolkit; as a programming language, 
however, it introduces inconsistencies and uncertainties which could be 
avoided if some of the "modern" parts (including language concepts) 
could be more integrated in core-R.

Best regards,
Hilmar

-- 
Dr. Hilmar Berger, MD
Max Planck Institute for Infection Biology
Charit?platz 1
D-10117 Berlin
GERMANY

Phone:  + 49 30 28460 430
Fax:    + 49 30 28460 401
  
E-Mail: berger at mpiib-berlin.mpg.de
Web   : www.mpiib-berlin.mpg.de


From jorismeys at gmail.com  Tue May  9 11:22:48 2017
From: jorismeys at gmail.com (Joris Meys)
Date: Tue, 9 May 2017 11:22:48 +0200
Subject: [Rd] A few suggestions and perspectives from a PhD student
In-Reply-To: <4fdf0b2c-0393-aab3-dc93-84345baa7f67@mpiib-berlin.mpg.de>
References: <FCBC5362-8436-487D-95A0-65CB5924BD29@idi.ntnu.no>
 <CAP01uRk1NzjRjs80hHj3mJ+n1iaESfxj00t88fSkt_wS2pV1rg@mail.gmail.com>
 <AE0E15A4-5A91-4E00-ABF8-6463635E45D4@idi.ntnu.no>
 <CA+vqiLG=mDFOiWScr4TCVjkwszXo+38ZTFB4T-7UvqbKzwy+7Q@mail.gmail.com>
 <4fdf0b2c-0393-aab3-dc93-84345baa7f67@mpiib-berlin.mpg.de>
Message-ID: <CAO1zAVYhYz3ZuWAYH3z=C1-F2q5W-aUwPAhBWROxw5viNDiW5g@mail.gmail.com>

On Tue, May 9, 2017 at 9:47 AM, Hilmar Berger <berger at mpiib-berlin.mpg.de>
wrote:

> Hi,
>
> On 08/05/17 16:37, Ista Zahn wrote:
>
>> One of the key strengths of R is that packages are not akin to "fan
>> created mods". They are a central and necessary part of the R system.
>>
>> I would tend to disagree here. R packages are in their majority not
> maintained by the core R developers. Concepts, features and lifetime depend
> mainly on the maintainers of the package (even though in theory GPL will
> allow to somebody to take over anytime). Several packages that are critical
> for processing big data and providing "modern" visualizations introduce
> concepts quite different from the legacy S/R language. I do feel that in a
> way, current core R shows strongly its origin in S, while modern concepts
> (e.g. data.table, dplyr, ggplot, ...) are often only available via
> extension packages. This is fine if one considers R to be a statistical
> toolkit; as a programming language, however, it introduces inconsistencies
> and uncertainties which could be avoided if some of the "modern" parts
> (including language concepts) could be more integrated in core-R.
>
> Best regards,
> Hilmar
>

And I would tend to disagree here. R is build upon the paradigm of a
functional programming language, and falls in the same group as clojure,
haskell and the likes. It is a turing complete programming language on its
own. That's quite a bit more than "a statistical toolkit". You can say that
about eg the macro language of SPSS, but not about R.

Second, there's little "modern" about the ideas behind the tidyverse.
Piping is about as old as unix itself. The grammar of graphics, on which
ggplot is based, stems from the SYStat graphics system from the nineties.
Hadley and colleagues did (and do) a great job implementing these ideas in
R, but the ideas do have a respectable age.

Third, there's a lot of nonstandard evaluation going on in all these
packages. Using them inside your own functions requires serious attention
(eg the difference between aes() and aes_() in ggplot2). Actually, even
though I definitely see the merits of these packages in data analysis, the
tidyverse feels like a (clean and powerful) macro language on top of R. And
that's good, but that doesn't mean these parts are essential to transform R
into a programming language. Rather the contrary actually: too heavily
relying on these packages does complicate things when you start to develop
your own packages in R.

Forth, the tidyverse masks quite some native R functions. Obviously they
took great care in keeping the functionality as close as one would expect,
but that's not always the case. The lag() function of dplyr() masks an S3
generic from the stats package for example. So if you work with time series
in the stats package, loading the tidyverse gives you trouble.

Fifth, many of the tidyverse packages are a version 0.x.y : they're still
in beta development and their functionality might (and will) change.
Functions disappear, arguments are called different, tags change,... Often
the changes improve the packages, but they did break older code for me more
than once. You can't expect the R core team to incorporate something that
is bound to change.

Last but not least, the tidyverse actually sometimes works against new R
users. At least R users that go beyond the classic data workflow. I
literally rewrote some code -from a consultant- that abused the _ply
functions to create nested loops. Removing all that stuff and rewriting the
code using a simple list in combination with a simple for-loop, sped up the
code with a factor 150. That has nothing to do with dplyr, it's very fast.
That has everything to do with that person having a hammer and thinking
everything he sees is a nail. The tidyverse is no reason to not learn the
concepts of the language it's built upon.

The one thing I would like to see though, is the adaptation of the
statistical toolkit so that it can work with data.table and tibble objects
directly, as opposed to having to convert to a data.frame once you start
building the models. And I believe that eventually there will be a
replacement for the data.frame that increases R's performance and lessens
its burden on the memory.

So all in all, I do admire the tidyverse and how it speeds up data
preparation for analysis. But tidyverse is a powerful data toolkit, not a
programming language. And it won't make R a programming language either.
Because R is already.

Cheers
Joris

>
> --
> Dr. Hilmar Berger, MD
> Max Planck Institute for Infection Biology
> Charit?platz 1
> D-10117 Berlin
> GERMANY
>
> Phone:  + 49 30 28460 430
> Fax:    + 49 30 28460 401
>  E-Mail: berger at mpiib-berlin.mpg.de
> Web   : www.mpiib-berlin.mpg.de
>
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>



-- 
Joris Meys
Statistical consultant

Ghent University
Faculty of Bioscience Engineering
Department of Mathematical Modelling, Statistics and Bio-Informatics

tel :  +32 (0)9 264 61 79
Joris.Meys at Ugent.be
-------------------------------
Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php

	[[alternative HTML version deleted]]


From lionel at rstudio.com  Tue May  9 11:56:20 2017
From: lionel at rstudio.com (Lionel Henry)
Date: Tue, 9 May 2017 11:56:20 +0200
Subject: [Rd] A few suggestions and perspectives from a PhD student
In-Reply-To: <CAO1zAVYhYz3ZuWAYH3z=C1-F2q5W-aUwPAhBWROxw5viNDiW5g@mail.gmail.com>
References: <FCBC5362-8436-487D-95A0-65CB5924BD29@idi.ntnu.no>
 <CAP01uRk1NzjRjs80hHj3mJ+n1iaESfxj00t88fSkt_wS2pV1rg@mail.gmail.com>
 <AE0E15A4-5A91-4E00-ABF8-6463635E45D4@idi.ntnu.no>
 <CA+vqiLG=mDFOiWScr4TCVjkwszXo+38ZTFB4T-7UvqbKzwy+7Q@mail.gmail.com>
 <4fdf0b2c-0393-aab3-dc93-84345baa7f67@mpiib-berlin.mpg.de>
 <CAO1zAVYhYz3ZuWAYH3z=C1-F2q5W-aUwPAhBWROxw5viNDiW5g@mail.gmail.com>
Message-ID: <CAJf4E3ogWA3cvvUutqTbrjnA7OeFvemJR3vu3fWDWMAY8LXR3Q@mail.gmail.com>

> Third, there's a lot of nonstandard evaluation going on in all these
> packages. Using them inside your own functions requires serious attention
> (eg the difference between aes() and aes_() in ggplot2). Actually, even
> though I definitely see the merits of these packages in data analysis, the
> tidyverse feels like a (clean and powerful) macro language on top of R.

That is going to change as we have put a lot of effort into learning
how to deal with capturing functions. See the tidyeval framework which
will enable full and flexible programmability of tidyverse grammars.

That said I agree that data analysis and package programming often
require different sets of tools.

Lionel


From berger at mpiib-berlin.mpg.de  Tue May  9 12:31:38 2017
From: berger at mpiib-berlin.mpg.de (Hilmar Berger)
Date: Tue, 9 May 2017 12:31:38 +0200
Subject: [Rd] A few suggestions and perspectives from a PhD student
In-Reply-To: <CAO1zAVYhYz3ZuWAYH3z=C1-F2q5W-aUwPAhBWROxw5viNDiW5g@mail.gmail.com>
References: <FCBC5362-8436-487D-95A0-65CB5924BD29@idi.ntnu.no>
 <CAP01uRk1NzjRjs80hHj3mJ+n1iaESfxj00t88fSkt_wS2pV1rg@mail.gmail.com>
 <AE0E15A4-5A91-4E00-ABF8-6463635E45D4@idi.ntnu.no>
 <CA+vqiLG=mDFOiWScr4TCVjkwszXo+38ZTFB4T-7UvqbKzwy+7Q@mail.gmail.com>
 <4fdf0b2c-0393-aab3-dc93-84345baa7f67@mpiib-berlin.mpg.de>
 <CAO1zAVYhYz3ZuWAYH3z=C1-F2q5W-aUwPAhBWROxw5viNDiW5g@mail.gmail.com>
Message-ID: <82884e27-9876-2ed7-19e0-20737afc2dc2@mpiib-berlin.mpg.de>



On 09/05/17 11:22, Joris Meys wrote:
>
>
> On Tue, May 9, 2017 at 9:47 AM, Hilmar Berger 
> <berger at mpiib-berlin.mpg.de <mailto:berger at mpiib-berlin.mpg.de>> wrote:
>
>     Hi,
>
>     On 08/05/17 16:37, Ista Zahn wrote:
>
>         One of the key strengths of R is that packages are not akin to
>         "fan
>         created mods". They are a central and necessary part of the R
>         system.
>
>     I would tend to disagree here. R packages are in their majority
>     not maintained by the core R developers. Concepts, features and
>     lifetime depend mainly on the maintainers of the package (even
>     though in theory GPL will allow to somebody to take over anytime).
>     Several packages that are critical for processing big data and
>     providing "modern" visualizations introduce concepts quite
>     different from the legacy S/R language. I do feel that in a way,
>     current core R shows strongly its origin in S, while modern
>     concepts (e.g. data.table, dplyr, ggplot, ...) are often only
>     available via extension packages. This is fine if one considers R
>     to be a statistical toolkit; as a programming language, however,
>     it introduces inconsistencies and uncertainties which could be
>     avoided if some of the "modern" parts (including language
>     concepts) could be more integrated in core-R.
>
>     Best regards,
>     Hilmar
>
>
> And I would tend to disagree here. R is build upon the paradigm of a 
> functional programming language, and falls in the same group as 
> clojure, haskell and the likes. It is a turing complete programming 
> language on its own. That's quite a bit more than "a statistical 
> toolkit". You can say that about eg the macro language of SPSS, but 
> not about R.
>
My point was that inconsistencies are harder to tolerate when using R as 
a programming language as opposed to a toolkit that just has to do a job.
> Second, there's little "modern" about the ideas behind the tidyverse. 
> Piping is about as old as unix itself. The grammar of graphics, on 
> which ggplot is based, stems from the SYStat graphics system from the 
> nineties. Hadley and colleagues did (and do) a great job implementing 
> these ideas in R, but the ideas do have a respectable age.
Those ideas seem still to be more modern than e.g. stock R graphics 
designed probably in the seventies or eighties. Which still do their job 
for lots and lots of applications, however, the fact that many newer 
packages use ggplot in stead of plot() forces users to learn and use 
different paradigms for things so simple as drawing a line.

I also would like to make clear that I do not advocate for including the 
whole tidyverse in core R. I just believe that having core concepts well 
supported in core R instead of implemented in a package might make 
things more consistent. E.g. method chaining ("%>%") is a core language 
feature in many languages.
>
> The one thing I would like to see though, is the adaptation of the 
> statistical toolkit so that it can work with data.table and tibble 
> objects directly, as opposed to having to convert to a data.frame once 
> you start building the models. And I believe that eventually there 
> will be a replacement for the data.frame that increases R's 
> performance and lessens its burden on the memory.
>
Which is a perfect example of what I mean: improved functionality should 
find their way into core R at some time point, replacing or extending 
outdated functionality. Otherwise, I don't know how hard it will be to 
develop 21st century methods on top of a 1980s/90s language core. 
Although I admit that the R developers are doing a great job to make it 
possible.

Best,
Hilmar

> So all in all, I do admire the tidyverse and how it speeds up data 
> preparation for analysis. But tidyverse is a powerful data toolkit, 
> not a programming language. And it won't make R a programming language 
> either. Because R is already.
>
> Cheers
> Joris
>
>
>     -- 
>     Dr. Hilmar Berger, MD
>     Max Planck Institute for Infection Biology
>     Charit?platz 1
>     D-10117 Berlin
>     GERMANY
>
>     Phone: + 49 30 28460 430 <tel:%2B%2049%2030%2028460%20430>
>     Fax: + 49 30 28460 401 <tel:%2B%2049%2030%2028460%20401>
>      E-Mail: berger at mpiib-berlin.mpg.de
>     <mailto:berger at mpiib-berlin.mpg.de>
>     Web   : www.mpiib-berlin.mpg.de <http://www.mpiib-berlin.mpg.de>
>
>
>     ______________________________________________
>     R-devel at r-project.org <mailto:R-devel at r-project.org> mailing list
>     https://stat.ethz.ch/mailman/listinfo/r-devel
>     <https://stat.ethz.ch/mailman/listinfo/r-devel>
>
>
>
>
> -- 
> Joris Meys
> Statistical consultant
>
> Ghent University
> Faculty of Bioscience Engineering
> Department of Mathematical Modelling, Statistics and Bio-Informatics
>
> tel :  +32 (0)9 264 61 79
> Joris.Meys at Ugent.be
> -------------------------------
> Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php

-- 
Dr. Hilmar Berger, MD
Max Planck Institute for Infection Biology
Charit?platz 1
D-10117 Berlin
GERMANY

Phone:  + 49 30 28460 430
Fax:    + 49 30 28460 401
  
E-Mail: berger at mpiib-berlin.mpg.de
Web   : www.mpiib-berlin.mpg.de


	[[alternative HTML version deleted]]


From georges.anastasiou at gmail.com  Tue May  9 12:19:35 2017
From: georges.anastasiou at gmail.com (George Anastasiou)
Date: Tue, 9 May 2017 13:19:35 +0300
Subject: [Rd] Potential Bug in convolve {stats}
Message-ID: <CAN+0Jxvwr1QmBCJB5dMfR8KzAXjeK7oaVXHCKAP4gp7txJnVZg@mail.gmail.com>

Dear All,

I think there is a bug in the convolve function of stats package. Running
the following:

a <- convolve(c(1,1,1,1), 1, type="filter")
a

the answer is:

[1] 1 1

whereas it should be:

[1] 1 1 1 1

Looking at the code of convolve, the bug is on line 22 at:

[-c(1L:n1, (n - n1 + 1L):n)]/n


which is not correct when the second input argument has only one element
(n1=0).

When I run R.version I get the following output:

platform       i686-pc-linux-gnu
arch           i686
os             linux-gnu
system         i686, linux-gnu
status
major          3
minor          4.0
year           2017
month          04
day            21
svn rev        72570
language       R
version.string R version 3.4.0 (2017-04-21)
nickname       You Stupid Darkness


Best,

George Anastasiou

	[[alternative HTML version deleted]]


From murdoch.duncan at gmail.com  Tue May  9 13:19:37 2017
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Tue, 9 May 2017 07:19:37 -0400
Subject: [Rd] source(), parse(), and foreign UTF-8 characters
In-Reply-To: <30a59534-7d44-c469-c5d8-08a81b519504@ivt.baug.ethz.ch>
References: <30a59534-7d44-c469-c5d8-08a81b519504@ivt.baug.ethz.ch>
Message-ID: <1559895a-43b3-2c22-8e15-dfae64d81a2d@gmail.com>

On 09/05/2017 3:42 AM, Kirill M?ller wrote:
> Hi
>
>
> I'm having trouble sourcing or parsing a UTF-8 file that contains
> characters that are not representable in the current locale ("foreign
> characters") on Windows. The source() function stops with an error, the
> parse() function reencodes all foreign characters using the <U+xxxx>
> notation. I have added a reproducible example below the message.
>
> This seems well within the bounds of documented behavior, although the
> documentation to source() could mention that the file can't contain
> foreign characters. Still, I'd prefer if UTF-8 "just worked" in R, and
> I'm willing to invest substantial time to help with that. Before
> starting to write a detailed proposal, I feel that I need a better
> understanding of the problem, and I'm grateful for any feedback you
> might have.
>
> I have looked into character encodings in the context of the dplyr
> package, and I have observed the following behavior:
>
> - Strings are treated preferentially in the native encoding
> - Only upon specific request (via translateCharUTF8() or enc2utf8() or
> ...), they are translated to UTF-8 and marked as such
> - On UTF-8 systems, strings are never marked as UTF-8
> - ASCII strings are marked as ASCII internally, but this information
> doesn't seem to be available, e.g., Encoding() returns "unknown" for
> such strings
> - Most functions in R are encoding-agnostic: they work the same
> regardless if they receive a native or UTF-8 encoded string if they are
> properly tagged
> - One important difference are symbols, which must be in the native
> encoding (and are always converted to native encoding, using <U+xxxx>
> escapes)
> - I/O is centered around the native encoding, e.g., writeLines() always
> reencodes to the native encoding
> - There is the "bytes" encoding which avoids reencoding.
>
> I haven't looked into serialization or plot devices yet.
>
> The conclusion to the "UTF-8 manifesto" [1] suggests "... to use UTF-8
> narrow strings everywhere and convert them back and forth when using
> platform APIs that don?t support UTF-8 ...". (It is written in the
> context of the UTF-16 encoding used internally on Windows, but seems to
> apply just the same here for the native encoding.) I think that Unicode
> support in R could be greatly improved if we follow these guidelines.
> This seems to mean:
>
> - Convert strings to UTF-8 as soon as possible, and mark them as such
> (also on systems where UTF-8 is the native encoding)
> - Translate to native only upon specific request, e.g., in calls to API
> functions or perhaps for .C()
> - Use UTF-8 for symbols
> - Avoid the forced round-trip to the native encoding in I/O functions
> and for parsing (but still read/write native by default)
> - Carefully look into serialization and plot devices
> - Add helper functions that simplify mundane tasks such as
> reading/writing a UTF-8 encoded file

Those are good long term goals, though I think the effort is easier than 
you think.  Rather than attempting to do it all at once, you should look 
for ways to do it gradually and submit self-contained patches.  In many 
cases it doesn't matter if strings are left in the local encoding, 
because the encoding doesn't matter.  The problems arise when UTF-8 
strings are converted to the local encoding before it's necessary, 
because that's a lossy conversion.  So a simple way to proceed is to 
identify where these conversions occur, and remove them one-by-one.

Currently I'm working on bug 16098, "Windows doesn't handle high Unicode 
code points".  It doesn't require many changes at all to handle input of 
those characters; all the remaining issues are avoiding the problems you 
identify above.  The origin of the issue is the fact that in Windows 
wchar_t is only 16 bits (not big enough to hold all Unicode code 
points).  As far as I know, Windows has no standard type to hold a 
Unicode code point, most of the run-time functions still use the 16 bit 
wchar_t.

I think once that bug is dealt with, 90+% of the remaining issues could 
be solved by avoiding translateChar on Windows.  This could be done by 
avoiding it everywhere, or by acting as though Windows is running in a 
UTF-8 locale until you actually need to write to a file.  Other systems 
tend to have UTF-8 locales in common use, so they're already fine.

You offered to spend time on this.  I'd appreciate some checks of the 
patch I'm developing for 16098, and also some research into how certain 
things (e.g. the iswprint function) are handled on Windows.

Duncan Murdoch
>
> I'm sure I've missed many potential pitfalls, your input is greatly
> appreciated. Thanks for your attention.
>
> Further ressources: A write-up by Prof. Ripley [2], a section in R-ints
> [3], a blog post by Ista Zahn [4], a StackOverflow search [5].
>
>
> Best regards
>
> Kirill
>
>
>
> [1] http://utf8everywhere.org/#conclusions
>
> [2] https://developer.r-project.org/Encodings_and_R.html
>
> [3]
> https://cran.r-project.org/doc/manuals/r-devel/R-ints.html#Encodings-for-CHARSXPs
>
> [3]
> http://people.fas.harvard.edu/~izahn/posts/reading-data-with-non-native-encoding-in-r/
>
> [4]
> http://stackoverflow.com/search?tab=votes&q=%5br%5d%20encoding%20windows%20is%3aquestion
>
>
>
> # Use one of the following:
> id <- "Gl\u00fcck"
> id <- "\u5e78\u798f"
> id <- "\u0441\u0447\u0430\u0441\u0442\u044c\u0435"
> id <- "\ud589\ubcf5"
>
> file_contents <- paste0('"', id, '"')
> Encoding(file_contents)
> raw_file_contents <- charToRaw(file_contents)
>
> path <- tempfile(fileext = ".R")
> writeBin(raw_file_contents, path)
> file.size(path)
> length(raw_file_contents)
>
> # Escapes the string
> parse(text = file_contents)
>
> # Throws an error
> print(source(path, encoding = "UTF-8"))
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From dutangc at gmail.com  Tue May  9 13:44:20 2017
From: dutangc at gmail.com (Christophe Dutang)
Date: Tue, 9 May 2017 13:44:20 +0200
Subject: [Rd] registering Fortran routines in R packages
Message-ID: <1CDAFD25-E227-42EA-A689-87B3036434AB@gmail.com>

Dear list,

I?m trying to register Fortran routines in randtoolbox (in srt/init.c file), see https://r-forge.r-project.org/scm/viewvc.php/pkg/randtoolbox/src/init.c?view=markup&root=rmetrics. 

Reading https://cran.r-project.org/doc/manuals/r-release/R-exts.html#Registering-native-routines and looking at what is done in stats package, I first thought that the following code will do the job:

static const R_FortranMethodDef FortEntries[] = {
 {"halton", (DL_FUNC) &F77_NAME(HALTON),  7},
 {"sobol", (DL_FUNC) &F77_NAME(SOBOL),  11},
 {NULL, NULL, 0}
};

But I got error messages when building : use of undeclared identifier ?SOBOL_?. I also tried in lower case sobol and halton.

Looking at expm package https://r-forge.r-project.org/scm/viewvc.php/pkg/src/init.c?view=markup&revision=94&root=expm, I try  

static const R_FortranMethodDef FortEntries[] = {
 {"halton", (DL_FUNC) &F77_SUB(HALTON),  7},
 {"sobol", (DL_FUNC) &F77_SUB(SOBOL),  11},
 {NULL, NULL, 0}
};

But the problem remains the same.

Is there a way to have header file for Fortran codes? how to declare routines defined in my Fortran file src/LowDiscrepancy.f?

Any help appreciated

Regards, Christophe
---------------------------------------
Christophe Dutang
LMM, UdM, Le Mans, France
web: http://dutangc.free.fr


From bhh at xs4all.nl  Tue May  9 14:32:42 2017
From: bhh at xs4all.nl (Berend Hasselman)
Date: Tue, 9 May 2017 14:32:42 +0200
Subject: [Rd] registering Fortran routines in R packages
In-Reply-To: <1CDAFD25-E227-42EA-A689-87B3036434AB@gmail.com>
References: <1CDAFD25-E227-42EA-A689-87B3036434AB@gmail.com>
Message-ID: <93D3A75D-290F-49B3-9E37-FD88F78E2BEF@xs4all.nl>


> On 9 May 2017, at 13:44, Christophe Dutang <dutangc at gmail.com> wrote:
> 
> Dear list,
> 
> I?m trying to register Fortran routines in randtoolbox (in srt/init.c file), see https://r-forge.r-project.org/scm/viewvc.php/pkg/randtoolbox/src/init.c?view=markup&root=rmetrics. 
> 
> Reading https://cran.r-project.org/doc/manuals/r-release/R-exts.html#Registering-native-routines and looking at what is done in stats package, I first thought that the following code will do the job:
> 
> static const R_FortranMethodDef FortEntries[] = {
> {"halton", (DL_FUNC) &F77_NAME(HALTON),  7},
> {"sobol", (DL_FUNC) &F77_NAME(SOBOL),  11},
> {NULL, NULL, 0}
> };
> 
> But I got error messages when building : use of undeclared identifier ?SOBOL_?. I also tried in lower case sobol and halton.
> 
> Looking at expm package https://r-forge.r-project.org/scm/viewvc.php/pkg/src/init.c?view=markup&revision=94&root=expm, I try  
> 
> static const R_FortranMethodDef FortEntries[] = {
> {"halton", (DL_FUNC) &F77_SUB(HALTON),  7},
> {"sobol", (DL_FUNC) &F77_SUB(SOBOL),  11},
> {NULL, NULL, 0}
> };
> 
> But the problem remains the same.
> 
> Is there a way to have header file for Fortran codes? how to declare routines defined in my Fortran file src/LowDiscrepancy.f?
> 

lowercase routine names? manual does mention that.

Berend Hasselman


> Any help appreciated
> 
> Regards, Christophe
> ---------------------------------------
> Christophe Dutang
> LMM, UdM, Le Mans, France
> web: http://dutangc.free.fr
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From alexandre.courtiol at gmail.com  Tue May  9 16:40:46 2017
From: alexandre.courtiol at gmail.com (Alexandre Courtiol)
Date: Tue, 9 May 2017 16:40:46 +0200
Subject: [Rd] Bug simulate.lm() --> needs credential to report it
Message-ID: <CAERMt4dSfb11BJtPSNLEp-O31tkJkuJ5rZzH_4qBfWLR9Y03oQ@mail.gmail.com>

Dear R developers,

I did not get any reply concerning my email from last week concerning the
bug I found in stats::simulate.lm(). The bug shows up when called upon a
GLM with family gaussian(). I am confident it is a genuine bug related to a
mix-up between weights and prior weights that only impacts the gaussian
family (other families have their own simulate functions defined
elsewhere).

I cannot add the bug in Bugzilla as I have no credential.
Could someone please help me to get credentials so that I can add the bug
in bugzilla?

Thanks a lot,

Simple demonstration for the bug:

set.seed(1L)
y <- 10 + rnorm(n = 100)
mean(y) ##  10.10889
var(y)  ##   0.8067621

mod_glm  <- glm(y ~ 1, family = gaussian(link = "log"))
new.y <- simulate(mod_glm)[, 1]
mean(new.y) ## 10.10553
var(new.y)  ##  0.007243695  ##### WRONG #####

mod_glm$weights <- mod_glm$prior.weights  ## ugly hack showing where the
issue is
new.y <- simulate(mod_glm)[, 1]
mean(new.y) ## 10.13554
var(new.y)  ##  0.8629975  ##### OK #####


-- 
Alexandre Courtiol

http://sites.google.com/site/alexandrecourtiol/home

*"Science is the belief in the ignorance of experts"*, R. Feynman

	[[alternative HTML version deleted]]


From wdunlap at tibco.com  Tue May  9 23:06:09 2017
From: wdunlap at tibco.com (William Dunlap)
Date: Tue, 9 May 2017 14:06:09 -0700
Subject: [Rd] R-3.3.3/R-3.4.0 change in sys.call(sys.parent())
Message-ID: <CAF8bMcZm2EvJLTUH6x51Fxi+HQyHD0zBMKTfTOsvQfnVtDKBGw@mail.gmail.com>

Some formula methods for S3 generic functions use the idiom
    returnValue$call <- sys.call(sys.parent())
to show how to recreate the returned object or to use as a label on a
plot.  It is often followed by
     returnValue$call[[1]] <- quote(myName)
E.g., I see it in packages "latticeExtra" and "leaps", and I suspect it
used in "lattice" as well.

This idiom has not done good things for quite a while (ever?) but I noticed
while running tests that it acts differently in R-3.4.0 than in R-3.3.3.
Neither the old or new behavior is nice.  E.g., in R-3.3.3 we get

> parseEval <- function(text, envir) eval(parse(text=text), envir=envir)
> parseEval('lattice::xyplot(mpg~hp, data=datasets::mtcars)$call',
envir=new.env())
xyplot(expr, envir, enclos)

and

> evalInEnvir <- function(call, envir) eval(call, envir=envir)
> evalInEnvir(quote(lattice::xyplot(mpg~hp, data=datasets::mtcars)$call),
envir=new.env())
xyplot(expr, envir, enclos)

while in R-3.4.0 we get
> parseEval <- function(text, envir) eval(parse(text=text), envir=envir)
> parseEval('lattice::xyplot(mpg~hp, data=datasets::mtcars)$call',
envir=new.env())
xyplot(parse(text = text), envir = envir)

and

> evalInEnvir <- function(call, envir) eval(call, envir=envir)
> evalInEnvir(quote(lattice::xyplot(mpg~hp, data=datasets::mtcars)$call),
envir=new.env())
xyplot(call, envir = envir)

Should these packages be be fixed up to use just sys.call()?

Bill Dunlap
TIBCO Software
wdunlap tibco.com

	[[alternative HTML version deleted]]


From kirill.mueller at ivt.baug.ethz.ch  Tue May  9 23:46:28 2017
From: kirill.mueller at ivt.baug.ethz.ch (=?UTF-8?Q?Kirill_M=c3=bcller?=)
Date: Tue, 9 May 2017 23:46:28 +0200
Subject: [Rd] source(), parse(), and foreign UTF-8 characters
In-Reply-To: <1559895a-43b3-2c22-8e15-dfae64d81a2d@gmail.com>
References: <30a59534-7d44-c469-c5d8-08a81b519504@ivt.baug.ethz.ch>
 <1559895a-43b3-2c22-8e15-dfae64d81a2d@gmail.com>
Message-ID: <0497e811-9a4d-01ba-138f-cf65d913dd4f@ivt.baug.ethz.ch>

On 09.05.2017 13:19, Duncan Murdoch wrote:
> On 09/05/2017 3:42 AM, Kirill M?ller wrote:
>> Hi
>>
>>
>> I'm having trouble sourcing or parsing a UTF-8 file that contains
>> characters that are not representable in the current locale ("foreign
>> characters") on Windows. The source() function stops with an error, the
>> parse() function reencodes all foreign characters using the <U+xxxx>
>> notation. I have added a reproducible example below the message.
>>
>> This seems well within the bounds of documented behavior, although the
>> documentation to source() could mention that the file can't contain
>> foreign characters. Still, I'd prefer if UTF-8 "just worked" in R, and
>> I'm willing to invest substantial time to help with that. Before
>> starting to write a detailed proposal, I feel that I need a better
>> understanding of the problem, and I'm grateful for any feedback you
>> might have.
>>
>> I have looked into character encodings in the context of the dplyr
>> package, and I have observed the following behavior:
>>
>> - Strings are treated preferentially in the native encoding
>> - Only upon specific request (via translateCharUTF8() or enc2utf8() or
>> ...), they are translated to UTF-8 and marked as such
>> - On UTF-8 systems, strings are never marked as UTF-8
>> - ASCII strings are marked as ASCII internally, but this information
>> doesn't seem to be available, e.g., Encoding() returns "unknown" for
>> such strings
>> - Most functions in R are encoding-agnostic: they work the same
>> regardless if they receive a native or UTF-8 encoded string if they are
>> properly tagged
>> - One important difference are symbols, which must be in the native
>> encoding (and are always converted to native encoding, using <U+xxxx>
>> escapes)
>> - I/O is centered around the native encoding, e.g., writeLines() always
>> reencodes to the native encoding
>> - There is the "bytes" encoding which avoids reencoding.
>>
>> I haven't looked into serialization or plot devices yet.
>>
>> The conclusion to the "UTF-8 manifesto" [1] suggests "... to use UTF-8
>> narrow strings everywhere and convert them back and forth when using
>> platform APIs that don?t support UTF-8 ...". (It is written in the
>> context of the UTF-16 encoding used internally on Windows, but seems to
>> apply just the same here for the native encoding.) I think that Unicode
>> support in R could be greatly improved if we follow these guidelines.
>> This seems to mean:
>>
>> - Convert strings to UTF-8 as soon as possible, and mark them as such
>> (also on systems where UTF-8 is the native encoding)
>> - Translate to native only upon specific request, e.g., in calls to API
>> functions or perhaps for .C()
>> - Use UTF-8 for symbols
>> - Avoid the forced round-trip to the native encoding in I/O functions
>> and for parsing (but still read/write native by default)
>> - Carefully look into serialization and plot devices
>> - Add helper functions that simplify mundane tasks such as
>> reading/writing a UTF-8 encoded file
>
> Those are good long term goals, though I think the effort is easier 
> than you think.  Rather than attempting to do it all at once, you 
> should look for ways to do it gradually and submit self-contained 
> patches.  In many cases it doesn't matter if strings are left in the 
> local encoding, because the encoding doesn't matter.  The problems 
> arise when UTF-8 strings are converted to the local encoding before 
> it's necessary, because that's a lossy conversion.  So a simple way to 
> proceed is to identify where these conversions occur, and remove them 
> one-by-one.
Thanks, Duncan, this looks like a good start indeed. Did you really mean 
to say "the effort is easier than I think"? It would be great if I had 
overestimated the effort, I seldom do. That said, I'd be grateful if you 
could review/integrate/... future patches of mine towards parsing and 
sourcing of UTF-8 files with foreign characters, this problem seems to 
be self-contained (but perhaps not that easy).

I still think symbols should be in UTF-8, and this change might be 
difficult to split into smaller changes, especially if taking into 
account serialization and other potential pitfalls.

>
> Currently I'm working on bug 16098, "Windows doesn't handle high 
> Unicode code points".  It doesn't require many changes at all to 
> handle input of those characters; all the remaining issues are 
> avoiding the problems you identify above.  The origin of the issue is 
> the fact that in Windows wchar_t is only 16 bits (not big enough to 
> hold all Unicode code points).  As far as I know, Windows has no 
> standard type to hold a Unicode code point, most of the run-time 
> functions still use the 16 bit wchar_t.
I didn't mention non-BMP characters, they are an important issue as well.

>
> I think once that bug is dealt with, 90+% of the remaining issues 
> could be solved by avoiding translateChar on Windows.  This could be 
> done by avoiding it everywhere, or by acting as though Windows is 
> running in a UTF-8 locale until you actually need to write to a file.  
> Other systems tend to have UTF-8 locales in common use, so they're 
> already fine.
I'd argue against platform-specific switches. "grep translateChar" has 
found more than 500 hits on R-devel, and I suspect that checking each of 
them will take some time, one way or another.
>
> You offered to spend time on this.  I'd appreciate some checks of the 
> patch I'm developing for 16098, and also some research into how 
> certain things (e.g. the iswprint function) are handled on Windows.
I have looked in SVN, but couldn't find commits authored by you related 
to this bug in any of the branches. Could you please point me at your 
patch, or send it to me? Thanks!


-Kirill


>
> Duncan Murdoch
>>
>> I'm sure I've missed many potential pitfalls, your input is greatly
>> appreciated. Thanks for your attention.
>>
>> Further ressources: A write-up by Prof. Ripley [2], a section in R-ints
>> [3], a blog post by Ista Zahn [4], a StackOverflow search [5].
>>
>>
>> Best regards
>>
>> Kirill
>>
>>
>>
>> [1] http://utf8everywhere.org/#conclusions
>>
>> [2] https://developer.r-project.org/Encodings_and_R.html
>>
>> [3]
>> https://cran.r-project.org/doc/manuals/r-devel/R-ints.html#Encodings-for-CHARSXPs 
>>
>>
>> [3]
>> http://people.fas.harvard.edu/~izahn/posts/reading-data-with-non-native-encoding-in-r/ 
>>
>>
>> [4]
>> http://stackoverflow.com/search?tab=votes&q=%5br%5d%20encoding%20windows%20is%3aquestion 
>>
>>
>>
>>
>> # Use one of the following:
>> id <- "Gl\u00fcck"
>> id <- "\u5e78\u798f"
>> id <- "\u0441\u0447\u0430\u0441\u0442\u044c\u0435"
>> id <- "\ud589\ubcf5"
>>
>> file_contents <- paste0('"', id, '"')
>> Encoding(file_contents)
>> raw_file_contents <- charToRaw(file_contents)
>>
>> path <- tempfile(fileext = ".R")
>> writeBin(raw_file_contents, path)
>> file.size(path)
>> length(raw_file_contents)
>>
>> # Escapes the string
>> parse(text = file_contents)
>>
>> # Throws an error
>> print(source(path, encoding = "UTF-8"))
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>


From murdoch.duncan at gmail.com  Wed May 10 00:08:55 2017
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Tue, 9 May 2017 18:08:55 -0400
Subject: [Rd] source(), parse(), and foreign UTF-8 characters
In-Reply-To: <0497e811-9a4d-01ba-138f-cf65d913dd4f@ivt.baug.ethz.ch>
References: <30a59534-7d44-c469-c5d8-08a81b519504@ivt.baug.ethz.ch>
 <1559895a-43b3-2c22-8e15-dfae64d81a2d@gmail.com>
 <0497e811-9a4d-01ba-138f-cf65d913dd4f@ivt.baug.ethz.ch>
Message-ID: <8dba16cb-d430-54ae-11bf-69c71763e883@gmail.com>

On 09/05/2017 5:46 PM, Kirill M?ller wrote:
> On 09.05.2017 13:19, Duncan Murdoch wrote:
>> On 09/05/2017 3:42 AM, Kirill M?ller wrote:
>>> Hi
>>>
>>>
>>> I'm having trouble sourcing or parsing a UTF-8 file that contains
>>> characters that are not representable in the current locale ("foreign
>>> characters") on Windows. The source() function stops with an error, the
>>> parse() function reencodes all foreign characters using the <U+xxxx>
>>> notation. I have added a reproducible example below the message.
>>>
>>> This seems well within the bounds of documented behavior, although the
>>> documentation to source() could mention that the file can't contain
>>> foreign characters. Still, I'd prefer if UTF-8 "just worked" in R, and
>>> I'm willing to invest substantial time to help with that. Before
>>> starting to write a detailed proposal, I feel that I need a better
>>> understanding of the problem, and I'm grateful for any feedback you
>>> might have.
>>>
>>> I have looked into character encodings in the context of the dplyr
>>> package, and I have observed the following behavior:
>>>
>>> - Strings are treated preferentially in the native encoding
>>> - Only upon specific request (via translateCharUTF8() or enc2utf8() or
>>> ...), they are translated to UTF-8 and marked as such
>>> - On UTF-8 systems, strings are never marked as UTF-8
>>> - ASCII strings are marked as ASCII internally, but this information
>>> doesn't seem to be available, e.g., Encoding() returns "unknown" for
>>> such strings
>>> - Most functions in R are encoding-agnostic: they work the same
>>> regardless if they receive a native or UTF-8 encoded string if they are
>>> properly tagged
>>> - One important difference are symbols, which must be in the native
>>> encoding (and are always converted to native encoding, using <U+xxxx>
>>> escapes)
>>> - I/O is centered around the native encoding, e.g., writeLines() always
>>> reencodes to the native encoding
>>> - There is the "bytes" encoding which avoids reencoding.
>>>
>>> I haven't looked into serialization or plot devices yet.
>>>
>>> The conclusion to the "UTF-8 manifesto" [1] suggests "... to use UTF-8
>>> narrow strings everywhere and convert them back and forth when using
>>> platform APIs that don?t support UTF-8 ...". (It is written in the
>>> context of the UTF-16 encoding used internally on Windows, but seems to
>>> apply just the same here for the native encoding.) I think that Unicode
>>> support in R could be greatly improved if we follow these guidelines.
>>> This seems to mean:
>>>
>>> - Convert strings to UTF-8 as soon as possible, and mark them as such
>>> (also on systems where UTF-8 is the native encoding)
>>> - Translate to native only upon specific request, e.g., in calls to API
>>> functions or perhaps for .C()
>>> - Use UTF-8 for symbols
>>> - Avoid the forced round-trip to the native encoding in I/O functions
>>> and for parsing (but still read/write native by default)
>>> - Carefully look into serialization and plot devices
>>> - Add helper functions that simplify mundane tasks such as
>>> reading/writing a UTF-8 encoded file
>>
>> Those are good long term goals, though I think the effort is easier
>> than you think.  Rather than attempting to do it all at once, you
>> should look for ways to do it gradually and submit self-contained
>> patches.  In many cases it doesn't matter if strings are left in the
>> local encoding, because the encoding doesn't matter.  The problems
>> arise when UTF-8 strings are converted to the local encoding before
>> it's necessary, because that's a lossy conversion.  So a simple way to
>> proceed is to identify where these conversions occur, and remove them
>> one-by-one.
> Thanks, Duncan, this looks like a good start indeed. Did you really mean
> to say "the effort is easier than I think"? It would be great if I had
> overestimated the effort, I seldom do. That said, I'd be grateful if you
> could review/integrate/... future patches of mine towards parsing and
> sourcing of UTF-8 files with foreign characters, this problem seems to
> be self-contained (but perhaps not that easy).

I'll definitely look at small ones.  I'm not sure I'll have enough time 
to do really big ones, so it's best to try to break things up into small 
bites.

>
> I still think symbols should be in UTF-8, and this change might be
> difficult to split into smaller changes, especially if taking into
> account serialization and other potential pitfalls.
>
>>
>> Currently I'm working on bug 16098, "Windows doesn't handle high
>> Unicode code points".  It doesn't require many changes at all to
>> handle input of those characters; all the remaining issues are
>> avoiding the problems you identify above.  The origin of the issue is
>> the fact that in Windows wchar_t is only 16 bits (not big enough to
>> hold all Unicode code points).  As far as I know, Windows has no
>> standard type to hold a Unicode code point, most of the run-time
>> functions still use the 16 bit wchar_t.
> I didn't mention non-BMP characters, they are an important issue as well.
>
>>
>> I think once that bug is dealt with, 90+% of the remaining issues
>> could be solved by avoiding translateChar on Windows.  This could be
>> done by avoiding it everywhere, or by acting as though Windows is
>> running in a UTF-8 locale until you actually need to write to a file.
>> Other systems tend to have UTF-8 locales in common use, so they're
>> already fine.
> I'd argue against platform-specific switches. "grep translateChar" has
> found more than 500 hits on R-devel, and I suspect that checking each of
> them will take some time, one way or another.
>>
>> You offered to spend time on this.  I'd appreciate some checks of the
>> patch I'm developing for 16098, and also some research into how
>> certain things (e.g. the iswprint function) are handled on Windows.
> I have looked in SVN, but couldn't find commits authored by you related
> to this bug in any of the branches. Could you please point me at your
> patch, or send it to me? Thanks!

Not committed yet; I'll send you something tomorrow.  Generally we try 
to leave R in a good state after every commit, so I don't tend to commit 
patches until I'm happy with them.  I started on this one a couple of 
years ago, didn't like how it was going, and went on to easier things; 
I've come back to it very recently.

Duncan Murdoch


From dutangc at gmail.com  Wed May 10 08:08:24 2017
From: dutangc at gmail.com (Christophe Dutang)
Date: Wed, 10 May 2017 08:08:24 +0200
Subject: [Rd] registering Fortran routines in R packages
In-Reply-To: <93D3A75D-290F-49B3-9E37-FD88F78E2BEF@xs4all.nl>
References: <1CDAFD25-E227-42EA-A689-87B3036434AB@gmail.com>
 <93D3A75D-290F-49B3-9E37-FD88F78E2BEF@xs4all.nl>
Message-ID: <1072BC5F-750D-4DF8-8B7B-7D8E665EDE00@gmail.com>

Thanks for your email.

I try to change the name in lowercase but it conflicts with a C implementation also named halton. So I rename the C function halton2() and sobol2() while the Fortran function are HALTON() and SOBOL() (I also try lower case in the Fortran code). Unfortunately, it does not help since I get

init.c:97:25: error: use of undeclared identifier 'halton_'; did you mean 'halton2'?
  {"halton", (DL_FUNC) &F77_SUB(halton),  7},

My current solution is to comment FortEntries array and use R_useDynamicSymbols(dll, TRUE) for a dynamic search of Fortran routines.

Regards, Christophe
---------------------------------------
Christophe Dutang
LMM, UdM, Le Mans, France
web: http://dutangc.free.fr <http://dutangc.free.fr/>
> Le 9 mai 2017 ? 14:32, Berend Hasselman <bhh at xs4all.nl> a ?crit :
> 
> 
>> On 9 May 2017, at 13:44, Christophe Dutang <dutangc at gmail.com> wrote:
>> 
>> Dear list,
>> 
>> I?m trying to register Fortran routines in randtoolbox (in srt/init.c file), see https://r-forge.r-project.org/scm/viewvc.php/pkg/randtoolbox/src/init.c?view=markup&root=rmetrics. 
>> 
>> Reading https://cran.r-project.org/doc/manuals/r-release/R-exts.html#Registering-native-routines and looking at what is done in stats package, I first thought that the following code will do the job:
>> 
>> static const R_FortranMethodDef FortEntries[] = {
>> {"halton", (DL_FUNC) &F77_NAME(HALTON),  7},
>> {"sobol", (DL_FUNC) &F77_NAME(SOBOL),  11},
>> {NULL, NULL, 0}
>> };
>> 
>> But I got error messages when building : use of undeclared identifier ?SOBOL_?. I also tried in lower case sobol and halton.
>> 
>> Looking at expm package https://r-forge.r-project.org/scm/viewvc.php/pkg/src/init.c?view=markup&revision=94&root=expm, I try  
>> 
>> static const R_FortranMethodDef FortEntries[] = {
>> {"halton", (DL_FUNC) &F77_SUB(HALTON),  7},
>> {"sobol", (DL_FUNC) &F77_SUB(SOBOL),  11},
>> {NULL, NULL, 0}
>> };
>> 
>> But the problem remains the same.
>> 
>> Is there a way to have header file for Fortran codes? how to declare routines defined in my Fortran file src/LowDiscrepancy.f?
>> 
> 
> lowercase routine names? manual does mention that.
> 
> Berend Hasselman
> 
> 
>> Any help appreciated
>> 
>> Regards, Christophe
>> ---------------------------------------
>> Christophe Dutang
>> LMM, UdM, Le Mans, France
>> web: http://dutangc.free.fr
>> 
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
> 


	[[alternative HTML version deleted]]


From bhh at xs4all.nl  Wed May 10 08:48:52 2017
From: bhh at xs4all.nl (Berend Hasselman)
Date: Wed, 10 May 2017 08:48:52 +0200
Subject: [Rd] registering Fortran routines in R packages
In-Reply-To: <1072BC5F-750D-4DF8-8B7B-7D8E665EDE00@gmail.com>
References: <1CDAFD25-E227-42EA-A689-87B3036434AB@gmail.com>
 <93D3A75D-290F-49B3-9E37-FD88F78E2BEF@xs4all.nl>
 <1072BC5F-750D-4DF8-8B7B-7D8E665EDE00@gmail.com>
Message-ID: <696AFC2F-DD3D-4E5E-BC4E-12B87087CA6F@xs4all.nl>

Christophe, 

> On 10 May 2017, at 08:08, Christophe Dutang <dutangc at gmail.com> wrote:
> 
> Thanks for your email.
> 
> I try to change the name in lowercase but it conflicts with a C implementation also named halton. So I rename the C function halton2() and sobol2() while the Fortran function are HALTON() and SOBOL() (I also try lower case in the Fortran code). Unfortunately, it does not help since I get
> 
> init.c:97:25: error: use of undeclared identifier 'halton_'; did you mean 'halton2'?
>   {"halton", (DL_FUNC) &F77_SUB(halton),  7},
> 
> My current solution is to comment FortEntries array and use R_useDynamicSymbols(dll, TRUE) for a dynamic search of Fortran routines.

Have a look at my package geigen and its init.c.
Could it be that you are missing extern declarations for the Fortran routines?


Berend


From jari.oksanen at oulu.fi  Wed May 10 08:56:57 2017
From: jari.oksanen at oulu.fi (Jari Oksanen)
Date: Wed, 10 May 2017 06:56:57 +0000
Subject: [Rd] registering Fortran routines in R packages
In-Reply-To: <696AFC2F-DD3D-4E5E-BC4E-12B87087CA6F@xs4all.nl>
References: <1CDAFD25-E227-42EA-A689-87B3036434AB@gmail.com>
 <93D3A75D-290F-49B3-9E37-FD88F78E2BEF@xs4all.nl>
 <1072BC5F-750D-4DF8-8B7B-7D8E665EDE00@gmail.com>,
 <696AFC2F-DD3D-4E5E-BC4E-12B87087CA6F@xs4all.nl>
Message-ID: <1494399417326.77831@oulu.fi>

Have you tried using tools:::package_native_routine_registration_skeleton()? If you don't like its output, you can easily edit its results and still avoid most pitfalls.

Cheers, Jari Oksanen
________________________________________
From: R-devel <r-devel-bounces at r-project.org> on behalf of Berend Hasselman <bhh at xs4all.nl>
Sent: 10 May 2017 09:48
To: Christophe Dutang
Cc: r-devel at r-project.org
Subject: Re: [Rd] registering Fortran routines in R packages

Christophe,

> On 10 May 2017, at 08:08, Christophe Dutang <dutangc at gmail.com> wrote:
>
> Thanks for your email.
>
> I try to change the name in lowercase but it conflicts with a C implementation also named halton. So I rename the C function halton2() and sobol2() while the Fortran function are HALTON() and SOBOL() (I also try lower case in the Fortran code). Unfortunately, it does not help since I get
>
> init.c:97:25: error: use of undeclared identifier 'halton_'; did you mean 'halton2'?
>   {"halton", (DL_FUNC) &F77_SUB(halton),  7},
>
> My current solution is to comment FortEntries array and use R_useDynamicSymbols(dll, TRUE) for a dynamic search of Fortran routines.

Have a look at my package geigen and its init.c.
Could it be that you are missing extern declarations for the Fortran routines?


Berend

______________________________________________
R-devel at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-devel


From bhh at xs4all.nl  Wed May 10 08:58:47 2017
From: bhh at xs4all.nl (Berend Hasselman)
Date: Wed, 10 May 2017 08:58:47 +0200
Subject: [Rd] registering Fortran routines in R packages
In-Reply-To: <696AFC2F-DD3D-4E5E-BC4E-12B87087CA6F@xs4all.nl>
References: <1CDAFD25-E227-42EA-A689-87B3036434AB@gmail.com>
 <93D3A75D-290F-49B3-9E37-FD88F78E2BEF@xs4all.nl>
 <1072BC5F-750D-4DF8-8B7B-7D8E665EDE00@gmail.com>
 <696AFC2F-DD3D-4E5E-BC4E-12B87087CA6F@xs4all.nl>
Message-ID: <3741DD7A-38B9-491F-B3D7-3C0E5BCB41AA@xs4all.nl>


> On 10 May 2017, at 08:48, Berend Hasselman <bhh at xs4all.nl> wrote:
> 
> Christophe, 
> 
>> On 10 May 2017, at 08:08, Christophe Dutang <dutangc at gmail.com> wrote:
>> 
>> Thanks for your email.
>> 
>> I try to change the name in lowercase but it conflicts with a C implementation also named halton. So I rename the C function halton2() and sobol2() while the Fortran function are HALTON() and SOBOL() (I also try lower case in the Fortran code). Unfortunately, it does not help since I get
>> 
>> init.c:97:25: error: use of undeclared identifier 'halton_'; did you mean 'halton2'?
>>  {"halton", (DL_FUNC) &F77_SUB(halton),  7},
>> 
>> My current solution is to comment FortEntries array and use R_useDynamicSymbols(dll, TRUE) for a dynamic search of Fortran routines.
> 
> Have a look at my package geigen and its init.c.
> Could it be that you are missing extern declarations for the Fortran routines?

I ran a development version of R some time ago with this 

tools:::package_native_routine_registration_skeleton("geigen",con="./geigen_report.txt")

to generate a skeleton init.c. I used that without modification.

Berend


From dutangc at gmail.com  Wed May 10 10:56:46 2017
From: dutangc at gmail.com (Christophe Dutang)
Date: Wed, 10 May 2017 10:56:46 +0200
Subject: [Rd] registering Fortran routines in R packages
In-Reply-To: <1494399417326.77831@oulu.fi>
References: <1CDAFD25-E227-42EA-A689-87B3036434AB@gmail.com>
 <93D3A75D-290F-49B3-9E37-FD88F78E2BEF@xs4all.nl>
 <1072BC5F-750D-4DF8-8B7B-7D8E665EDE00@gmail.com>
 <696AFC2F-DD3D-4E5E-BC4E-12B87087CA6F@xs4all.nl>
 <1494399417326.77831@oulu.fi>
Message-ID: <736AE6CB-6F64-4562-BA5C-2B2E1322D9D0@gmail.com>

Thanks Jari and Berend.

I was not aware of that function. I do use it : indeed it may extern declarations for Fortran routines. see https://r-forge.r-project.org/scm/viewvc.php/pkg/randtoolbox/src/init.c?view=markup&root=rmetrics <https://r-forge.r-project.org/scm/viewvc.php/pkg/randtoolbox/src/init.c?view=markup&root=rmetrics>

However I get a new error when building 

Error: package or namespace load failed for ?randtoolbox? in dyn.load(file, DLLpath = DLLpath, ...):
 impossible de charger l'objet partag? '/Library/Frameworks/R.framework/Versions/3.4/Resources/library/randtoolbox/libs/randtoolbox.so':
  dlopen(/Library/Frameworks/R.framework/Versions/3.4/Resources/library/randtoolbox/libs/randtoolbox.so, 6): Symbol not found: _halton_f_
  Referenced from: /Library/Frameworks/R.framework/Versions/3.4/Resources/library/randtoolbox/libs/randtoolbox.so
  Expected in: flat namespace
 in /Library/Frameworks/R.framework/Versions/3.4/Resources/library/randtoolbox/libs/randtoolbox.so
Erreur : le chargement a ?chou?
Ex?cution arr?t?e
ERROR: loading failed

Note that I change the name of Fortran routines to halton_f and sobol_f to avoid conflict with C version also renamed halton_c and sobol_c.

In the NAMESPACE, I put at the top useDynLib(randtoolbox, .registration = TRUE). 

Berend, I do look at your package? I still don?t figure out why it works for you!

Regards, Christophe

---------------------------------------
Christophe Dutang
LMM, UdM, Le Mans, France
web: http://dutangc.free.fr <http://dutangc.free.fr/>
> Le 10 mai 2017 ? 08:56, Jari Oksanen <jari.oksanen at oulu.fi> a ?crit :
> 
> Have you tried using tools:::package_native_routine_registration_skeleton()? If you don't like its output, you can easily edit its results and still avoid most pitfalls.
> 
> Cheers, Jari Oksanen
> ________________________________________
> From: R-devel <r-devel-bounces at r-project.org> on behalf of Berend Hasselman <bhh at xs4all.nl>
> Sent: 10 May 2017 09:48
> To: Christophe Dutang
> Cc: r-devel at r-project.org
> Subject: Re: [Rd] registering Fortran routines in R packages
> 
> Christophe,
> 
>> On 10 May 2017, at 08:08, Christophe Dutang <dutangc at gmail.com> wrote:
>> 
>> Thanks for your email.
>> 
>> I try to change the name in lowercase but it conflicts with a C implementation also named halton. So I rename the C function halton2() and sobol2() while the Fortran function are HALTON() and SOBOL() (I also try lower case in the Fortran code). Unfortunately, it does not help since I get
>> 
>> init.c:97:25: error: use of undeclared identifier 'halton_'; did you mean 'halton2'?
>>  {"halton", (DL_FUNC) &F77_SUB(halton),  7},
>> 
>> My current solution is to comment FortEntries array and use R_useDynamicSymbols(dll, TRUE) for a dynamic search of Fortran routines.
> 
> Have a look at my package geigen and its init.c.
> Could it be that you are missing extern declarations for the Fortran routines?
> 
> 
> Berend
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


	[[alternative HTML version deleted]]


From tomas.kalibera at gmail.com  Wed May 10 21:58:43 2017
From: tomas.kalibera at gmail.com (Tomas Kalibera)
Date: Wed, 10 May 2017 21:58:43 +0200
Subject: [Rd] R-3.3.3/R-3.4.0 change in sys.call(sys.parent())
In-Reply-To: <CAF8bMcZm2EvJLTUH6x51Fxi+HQyHD0zBMKTfTOsvQfnVtDKBGw@mail.gmail.com>
References: <CAF8bMcZm2EvJLTUH6x51Fxi+HQyHD0zBMKTfTOsvQfnVtDKBGw@mail.gmail.com>
Message-ID: <d79e0914-bcff-55ff-b8cf-ad1ee7cf4d48@gmail.com>


The difference in the outputs between 3.3 and 3.4 is in how call 
expressions are selected in presence of .Internals. R is asked for a 
call expression for "eval". In 3.3 one gets the arguments for the call 
expression from the .Internal that implements eval. In 3.4 one gets the 
arguments for the call expression from the closure wrapper of "eval", 
which is less surprising. See e.g.

(3.4)
 > evalq()
Error in evalq() : argument is missing, with no default

vs

(3.3)
 > evalq()
Error in eval(substitute(expr), envir, enclos) :
   argument is missing, with no default

(and yes, these examples work with sys.call() and lattice originally 
used it in xyplot - perhaps it'd be best to submit a bug report/issue 
for lattice)

Tomas


On 05/09/2017 11:06 PM, William Dunlap via R-devel wrote:
> Some formula methods for S3 generic functions use the idiom
>      returnValue$call <- sys.call(sys.parent())
> to show how to recreate the returned object or to use as a label on a
> plot.  It is often followed by
>       returnValue$call[[1]] <- quote(myName)
> E.g., I see it in packages "latticeExtra" and "leaps", and I suspect it
> used in "lattice" as well.
>
> This idiom has not done good things for quite a while (ever?) but I noticed
> while running tests that it acts differently in R-3.4.0 than in R-3.3.3.
> Neither the old or new behavior is nice.  E.g., in R-3.3.3 we get
>
>> parseEval <- function(text, envir) eval(parse(text=text), envir=envir)
>> parseEval('lattice::xyplot(mpg~hp, data=datasets::mtcars)$call',
> envir=new.env())
> xyplot(expr, envir, enclos)
>
> and
>
>> evalInEnvir <- function(call, envir) eval(call, envir=envir)
>> evalInEnvir(quote(lattice::xyplot(mpg~hp, data=datasets::mtcars)$call),
> envir=new.env())
> xyplot(expr, envir, enclos)
>
> while in R-3.4.0 we get
>> parseEval <- function(text, envir) eval(parse(text=text), envir=envir)
>> parseEval('lattice::xyplot(mpg~hp, data=datasets::mtcars)$call',
> envir=new.env())
> xyplot(parse(text = text), envir = envir)
>
> and
>
>> evalInEnvir <- function(call, envir) eval(call, envir=envir)
>> evalInEnvir(quote(lattice::xyplot(mpg~hp, data=datasets::mtcars)$call),
> envir=new.env())
> xyplot(call, envir = envir)
>
> Should these packages be be fixed up to use just sys.call()?
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From douglas-langbehn at uiowa.edu  Wed May 10 22:59:30 2017
From: douglas-langbehn at uiowa.edu (Langbehn, Douglas)
Date: Wed, 10 May 2017 20:59:30 +0000
Subject: [Rd] bug report: nlme model-fitting crashes with R 3.4.0
Message-ID: <106CAE40B5287C47982478041DB6FAC79962728C@HC-MAILBOXC1-N6.healthcare.uiowa.edu>

lme() and gls() models from the nlme package are all crashing with R.3.4.0. Identical code ran correctly, without error in R 3.3.3 and earlier versions.  The behavior is easily demonstrated using one of the examples form the lme() help file, along with two simple variants. I have commented the errors generated by these calls, as well as the lines of code generating them, in the code example below.

As of today, this bug had not been reported on the R Bugzilla page. I could not submit this report directly to the page because I am not a member, and , as explained in the "Reporting Bugs" link from the  R home page, membership has now been closed due to spamming problems..

############################################################################
library(nlme)
#Using version 3.1-131
#Windows 7 64-bit operating system

fm2 <- lme(distance ~ age + Sex, data = Orthodont, random = ~ 1)

# Error in array(c(rep(1, p), .C(inner_perc_table, as.double(X), as.integer(unlist(grps)),  :
# object 'inner_perc_table' not found
#
#Upon debugging, this error is thrown with line 135 of lme.formula() code.
#
#fixDF <- getFixDF(X, grps, attr(lmeSt, "conLin")$dims$ngrps,

lme(distance ~ age + Sex, data = Orthodont, random = ~ 1|Subject)

# Error in array(c(rep(1, p), .C(inner_perc_table, as.double(X), as.integer(unlist(grps)),  :
# object 'inner_perc_table' not found

gls(distance ~ age + Sex, data = Orthodont,
    correlation = corCompSymm( form = ~ 1 | Subject))

# Error in corMatrix.corCompSymm(object) :
# object 'compSymm_matList' not found
#
#Upon debugging, the error is thrown by line 60 of gls code
#
#glsSt <- Initialize(glsSt, dataMod, glsEstControl)

R.version

# _
# platform       x86_64-w64-mingw32
# arch           x86_64
# os             mingw32
# system         x86_64, mingw32
# status
# major          3
# minor          4.0
# year           2017
# month          04
# day            21
# svn rev        72570
# language       R
# version.string R version 3.4.0 (2017-04-21)
# nickname       You Stupid Darkness

########################################################################
Douglas R Langbehn MD, PhD
Professor
Dept. of Psychiatry and Biostatistics (secondary)
University of Iowa Carver College of Medicine
douglas-langbehn at uiowa.edu



________________________________
Notice: This UI Health Care e-mail (including attachments) is covered by the Electronic Communications Privacy Act, 18 U.S.C. 2510-2521 and is intended only for the use of the individual or entity to which it is addressed, and may contain information that is privileged, confidential, and exempt from disclosure under applicable law. If you are not the intended recipient, any dissemination, distribution or copying of this communication is strictly prohibited. If you have received this communication in error, please notify the sender immediately and delete or destroy all copies of the original message and attachments thereto. Email sent to or from UI Health Care may be retained as required by law or regulation. Thank you.
________________________________

	[[alternative HTML version deleted]]


From deepayan.sarkar at gmail.com  Thu May 11 10:09:58 2017
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Thu, 11 May 2017 13:39:58 +0530
Subject: [Rd] R-3.3.3/R-3.4.0 change in sys.call(sys.parent())
In-Reply-To: <CAF8bMcZm2EvJLTUH6x51Fxi+HQyHD0zBMKTfTOsvQfnVtDKBGw@mail.gmail.com>
References: <CAF8bMcZm2EvJLTUH6x51Fxi+HQyHD0zBMKTfTOsvQfnVtDKBGw@mail.gmail.com>
Message-ID: <CADfFDC5=_di=1QV4FiuH0Ap2pJoP-6uBmDgbuEH=uMLUmMrViA@mail.gmail.com>

On Wed, May 10, 2017 at 2:36 AM, William Dunlap via R-devel
<r-devel at r-project.org> wrote:
> Some formula methods for S3 generic functions use the idiom
>     returnValue$call <- sys.call(sys.parent())
> to show how to recreate the returned object or to use as a label on a
> plot.  It is often followed by
>      returnValue$call[[1]] <- quote(myName)
> E.g., I see it in packages "latticeExtra" and "leaps", and I suspect it
> used in "lattice" as well.
>
> This idiom has not done good things for quite a while (ever?) but I noticed
> while running tests that it acts differently in R-3.4.0 than in R-3.3.3.
> Neither the old or new behavior is nice.  E.g., in R-3.3.3 we get
>
>> parseEval <- function(text, envir) eval(parse(text=text), envir=envir)
>> parseEval('lattice::xyplot(mpg~hp, data=datasets::mtcars)$call',
> envir=new.env())
> xyplot(expr, envir, enclos)
>
> and
>
>> evalInEnvir <- function(call, envir) eval(call, envir=envir)
>> evalInEnvir(quote(lattice::xyplot(mpg~hp, data=datasets::mtcars)$call),
> envir=new.env())
> xyplot(expr, envir, enclos)
>
> while in R-3.4.0 we get
>> parseEval <- function(text, envir) eval(parse(text=text), envir=envir)
>> parseEval('lattice::xyplot(mpg~hp, data=datasets::mtcars)$call',
> envir=new.env())
> xyplot(parse(text = text), envir = envir)
>
> and
>
>> evalInEnvir <- function(call, envir) eval(call, envir=envir)
>> evalInEnvir(quote(lattice::xyplot(mpg~hp, data=datasets::mtcars)$call),
> envir=new.env())
> xyplot(call, envir = envir)
>
> Should these packages be be fixed up to use just sys.call()?

I admit to not understanding these things very well, but I'll try to
explain why I ended up with the usage I have. The main use of the
$call component within lattice is to use it in the summary method, as
in:

> summary(xyplot(mpg~hp, data=mtcars))

Call:
xyplot(mpg ~ hp, data = mtcars)

Number of observations:
[1] 32

Here is a minimal approximation to what I need: Here foo() and bar()
are generics producing objects of class "foobar", bar() calls foo()
with one argument changed, and the print() method for "foobar" is just
supposed to print the call that produced it:

########

foo <- function(x, ...) UseMethod("foo")
bar <- function(x, ...) UseMethod("bar")
print.foobar <- function(x, ...) print(x$call)

## Using plain sys.call():

foo.formula <- function(x, ...)
{
    ans <- structure(list(), class = "foobar")
    ans$call <- sys.call()
    ans
}

bar.formula <- function(x, ..., panel)
{
    foo.formula(x, ..., panel = panel.bar)
}

foo.table <- function(x, ...)
{
    ans <- foo.formula(Freq ~ Var1,
                       as.data.frame.table(x), ...)
    ans
}

## I would get

foo(y ~ x)
# foo.formula(y ~ x)

bar(y ~ x)
# foo.formula(x, ..., panel = panel.bar)

foo(as.table(1:10))
# foo.formula(Freq ~ Var1, as.data.frame.table(x), ...)

## The last two are improved by

foo.formula <- function(x, ...)
{
    ans <- structure(list(), class = "foobar")
    ans$call <- sys.call(sys.parent())
    ans
}

bar(y ~ x)
## bar.formula(y ~ x)

foo(as.table(1:10))
## foo.table(as.table(1:10))

########

Adding

ans$call[[1]] <- quote(foo)

(or quote(bar) in bar.formula) is needed to replace the unexported
method name (foo.formula) with the generic name (foo), but that's
probably not the problem.

With this approach in lattice,

p <- some.function(...)
eval(p$call)

usually works, but not always, if I remember correctly.

I'm happy to consider more robust solutions. Maybe I just need to have a

...$call <- sys.call()

statement in every method?

-Deepayan


From lepennec at cmap.polytechnique.fr  Thu May 11 10:17:24 2017
From: lepennec at cmap.polytechnique.fr (Erwan Le Pennec)
Date: Thu, 11 May 2017 10:17:24 +0200
Subject: [Rd] bug report: nlme model-fitting crashes with R 3.4.0
In-Reply-To: <106CAE40B5287C47982478041DB6FAC79962728C@HC-MAILBOXC1-N6.healthcare.uiowa.edu>
References: <106CAE40B5287C47982478041DB6FAC79962728C@HC-MAILBOXC1-N6.healthcare.uiowa.edu>
Message-ID: <0048a68b-8676-9ec3-76fc-671a5c156996@cmap.polytechnique.fr>

     Dear all,

     I've stumbled a similar issue with the package cluster when 
compiling the 3.4.0 version with the settings of Fedora RPM specs. 
Compiling R with the default setting of configure yields a version that 
works for cluster... and nlme.

     I did not find the exact option that was the cause of this issue 
but I'm willing to help.

     Erwan

PS: This is the reason why R is still at version 3.3.3 on the Fedora 
distribution.

On 10/05/17 22:59, Langbehn, Douglas wrote:
> lme() and gls() models from the nlme package are all crashing with R.3.4.0. Identical code ran correctly, without error in R 3.3.3 and earlier versions.  The behavior is easily demonstrated using one of the examples form the lme() help file, along with two simple variants. I have commented the errors generated by these calls, as well as the lines of code generating them, in the code example below.
>
> As of today, this bug had not been reported on the R Bugzilla page. I could not submit this report directly to the page because I am not a member, and , as explained in the "Reporting Bugs" link from the  R home page, membership has now been closed due to spamming problems..
>
> ############################################################################
> library(nlme)
> #Using version 3.1-131
> #Windows 7 64-bit operating system
>
> fm2 <- lme(distance ~ age + Sex, data = Orthodont, random = ~ 1)
>
> # Error in array(c(rep(1, p), .C(inner_perc_table, as.double(X), as.integer(unlist(grps)),  :
> # object 'inner_perc_table' not found
> #
> #Upon debugging, this error is thrown with line 135 of lme.formula() code.
> #
> #fixDF <- getFixDF(X, grps, attr(lmeSt, "conLin")$dims$ngrps,
>
> lme(distance ~ age + Sex, data = Orthodont, random = ~ 1|Subject)
>
> # Error in array(c(rep(1, p), .C(inner_perc_table, as.double(X), as.integer(unlist(grps)),  :
> # object 'inner_perc_table' not found
>
> gls(distance ~ age + Sex, data = Orthodont,
>      correlation = corCompSymm( form = ~ 1 | Subject))
>
> # Error in corMatrix.corCompSymm(object) :
> # object 'compSymm_matList' not found
> #
> #Upon debugging, the error is thrown by line 60 of gls code
> #
> #glsSt <- Initialize(glsSt, dataMod, glsEstControl)
>
> R.version
>
> # _
> # platform       x86_64-w64-mingw32
> # arch           x86_64
> # os             mingw32
> # system         x86_64, mingw32
> # status
> # major          3
> # minor          4.0
> # year           2017
> # month          04
> # day            21
> # svn rev        72570
> # language       R
> # version.string R version 3.4.0 (2017-04-21)
> # nickname       You Stupid Darkness
>
> ########################################################################
> Douglas R Langbehn MD, PhD
> Professor
> Dept. of Psychiatry and Biostatistics (secondary)
> University of Iowa Carver College of Medicine
> douglas-langbehn at uiowa.edu
>
>
>
> ________________________________
> Notice: This UI Health Care e-mail (including attachments) is covered by the Electronic Communications Privacy Act, 18 U.S.C. 2510-2521 and is intended only for the use of the individual or entity to which it is addressed, and may contain information that is privileged, confidential, and exempt from disclosure under applicable law. If you are not the intended recipient, any dissemination, distribution or copying of this communication is strictly prohibited. If you have received this communication in error, please notify the sender immediately and delete or destroy all copies of the original message and attachments thereto. Email sent to or from UI Health Care may be retained as required by law or regulation. Thank you.
> ________________________________
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From pperry at stern.nyu.edu  Thu May 11 12:16:51 2017
From: pperry at stern.nyu.edu (Patrick Perry)
Date: Thu, 11 May 2017 12:16:51 +0200
Subject: [Rd] xrealloc namespace conflict
In-Reply-To: <590DED69.30404@stern.nyu.edu>
References: <590DED69.30404@stern.nyu.edu>
Message-ID: <59143A13.4090600@stern.nyu.edu>

I've done a bit more investigation into this issue. Here is my current 
understanding of the situation:

1. I have a package on CRAN (corpus-0.3.1) that passes tests on all 
platforms except for Linux.
2. My package defines a C function, "xrealloc", for internal use.
3. The libreadline library that R links to defines a different version 
of "xrealloc".
4. On Linux, when I load my package, references to "xrealloc" get linked 
to the libreadline version of that function.
5. This causes one of my tests to fail, calling exit(2), because the 
libreadline version of xrealloc does not allow calls of the form 
"xrealloc(ptr, 0)".

I can work around this issue pretty easily, by either renaming my 
version of xrealloc to something else, or by avoiding calls of the form 
"xrealloc(ptr, 0)". So, this is not a major issue, but it's a little 
unsettling to see this behavior when my package does not explicitly link 
to or use anything from libreadline.

Is there a reason this behavior is only manifesting on Linux? Is there 
something wrong with the way I'm compiling my package on that platform? 
Is this just some quirk about the way R is loading dynamic libraries on 
Linux? I'd appreciate any insight into the issue.


Patrick

p.s. Here are some references:

My package Makevars are at 
https://github.com/patperry/r-corpus/blob/master/src/Makevars ; my 
version of "xrealloc" is in corpus/src/xalloc.c

You can see the source for the libreadline xrealloc at 
https://github.com/JuliaLang/readline/blob/master/xmalloc.c#L67


Patrick Perry wrote:
>
> I have a package on CRAN now (corpus-0.3.1) that is currently failing
> tests on Linux, but passing on all other architectures:
>
> https://cran.r-project.org/web/checks/check_results_corpus.html
>
> I believe that the issue arrises from a namespace class between
> "xrealloc", which my package provides for internal use, but which R
> also seems to provide (possibly as part of TRE in
> src/extra/tre/xmalloc.c). It looks like my package is not picking up
> my custom xrealloc, but using an xrealloc provided by R.
>
> Besides the fact that I am linking to the wrong xrealloc, I think my
> tests are failing for the same reason that the following code
> segfaults on Linux (Debian, with R 3.4.0):
>
> test <- inline::cfunction(language='C',
>     otherdefs='void *xmalloc(size_t); void *xrealloc(void *, size_t);',
>     body = 'void *ptr = xmalloc(256); xrealloc(ptr, 0); return
> R_NilValue;')
> test()
> ## xrealloc: out of virtual memory
>
> It seems that the R xrealloc doesn't like being given a size of 0,
> even though this behavior is well-defined for realloc (it should free
> the memory). Based on my failing CRAN tests, it looks like this is a
> Linux-specific issue.
>
> Is there a way to modify my Makevars to force the linker to choose my
> version of xrealloc for my package-specific code? My current Makevars
> are at https://github.com/patperry/r-corpus/blob/master/src/Makevars
>
> Thanks in advance for any help.
>
>
> Patrick


> Patrick Perry <mailto:pperry at stern.nyu.edu>
> May 6, 2017 at 5:36 PM
> I have a package on CRAN now (corpus-0.3.1) that is currently failing 
> tests on Linux, but passing on all other architectures:
>
> https://cran.r-project.org/web/checks/check_results_corpus.html
>
> I believe that the issue arrises from a namespace class between 
> "xrealloc", which my package provides for internal use, but which R 
> also seems to provide (possibly as part of TRE in 
> src/extra/tre/xmalloc.c). It looks like my package is not picking up 
> my custom xrealloc, but using an xrealloc provided by R.
>
> Besides the fact that I am linking to the wrong xrealloc, I think my 
> tests are failing for the same reason that the following code 
> segfaults on Linux (Debian, with R 3.4.0):
>
> test <- inline::cfunction(language='C',
>     otherdefs='void *xmalloc(size_t); void *xrealloc(void *, size_t);',
>     body = 'void *ptr = xmalloc(256); xrealloc(ptr, 0); return 
> R_NilValue;')
> test()
> ## xrealloc: out of virtual memory
>
> It seems that the R xrealloc doesn't like being given a size of 0, 
> even though this behavior is well-defined for realloc (it should free 
> the memory). Based on my failing CRAN tests, it looks like this is a 
> Linux-specific issue.
>
> Is there a way to modify my Makevars to force the linker to choose my 
> version of xrealloc for my package-specific code? My current Makevars 
> are at https://github.com/patperry/r-corpus/blob/master/src/Makevars
>
> Thanks in advance for any help.
>
>
> Patrick
>


	[[alternative HTML version deleted]]


From edd at debian.org  Thu May 11 13:29:50 2017
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 11 May 2017 06:29:50 -0500
Subject: [Rd] xrealloc namespace conflict
In-Reply-To: <59143A13.4090600@stern.nyu.edu>
References: <590DED69.30404@stern.nyu.edu>
	<59143A13.4090600@stern.nyu.edu>
Message-ID: <22804.19246.514394.430012@max.eddelbuettel.com>


On 11 May 2017 at 12:16, Patrick Perry wrote:
| I've done a bit more investigation into this issue. Here is my current 
| understanding of the situation:
| 
| 1. I have a package on CRAN (corpus-0.3.1) that passes tests on all 
| platforms except for Linux.
| 2. My package defines a C function, "xrealloc", for internal use.
| 3. The libreadline library that R links to defines a different version 
| of "xrealloc".
| 4. On Linux, when I load my package, references to "xrealloc" get linked 
| to the libreadline version of that function.
| 5. This causes one of my tests to fail, calling exit(2), because the 
| libreadline version of xrealloc does not allow calls of the form 
| "xrealloc(ptr, 0)".
| 
| I can work around this issue pretty easily, by either renaming my 
| version of xrealloc to something else, or by avoiding calls of the form 
| "xrealloc(ptr, 0)". So, this is not a major issue, but it's a little 
| unsettling to see this behavior when my package does not explicitly link 
| to or use anything from libreadline.
| 
| Is there a reason this behavior is only manifesting on Linux? Is there 
| something wrong with the way I'm compiling my package on that platform? 
| Is this just some quirk about the way R is loading dynamic libraries on 
| Linux? I'd appreciate any insight into the issue.

It may just be the flat namespace and linking order. AFAIK there is nothing
in C preventing so maybe you 'just got lucky' on the other platforms. See eg
http://stackoverflow.com/questions/7998770/

But then I don't use pure C that after anymore ... and in C++ you could just
wrap a namespace around your code, avoiding the issue.


Dirk

-- 
http://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From edd at debian.org  Thu May 11 13:37:46 2017
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 11 May 2017 06:37:46 -0500
Subject: [Rd] bug report: nlme model-fitting crashes with R 3.4.0
In-Reply-To: <0048a68b-8676-9ec3-76fc-671a5c156996@cmap.polytechnique.fr>
References: <106CAE40B5287C47982478041DB6FAC79962728C@HC-MAILBOXC1-N6.healthcare.uiowa.edu>
 <0048a68b-8676-9ec3-76fc-671a5c156996@cmap.polytechnique.fr>
Message-ID: <22804.19722.546929.314493@max.eddelbuettel.com>


On 11 May 2017 at 10:17, Erwan Le Pennec wrote:
|      Dear all,
| 
|      I've stumbled a similar issue with the package cluster when 
| compiling the 3.4.0 version with the settings of Fedora RPM specs. 
| Compiling R with the default setting of configure yields a version that 
| works for cluster... and nlme.
| 
|      I did not find the exact option that was the cause of this issue 
| but I'm willing to help.
| 
|      Erwan
| 
| PS: This is the reason why R is still at version 3.3.3 on the Fedora 
| distribution.
| 
| On 10/05/17 22:59, Langbehn, Douglas wrote:
| > lme() and gls() models from the nlme package are all crashing with R.3.4.0. Identical code ran correctly, without error in R 3.3.3 and earlier versions.  The behavior is easily demonstrated using one of the examples form the lme() help file, along with two simple variants. I have commented the errors generated by these calls, as well as the lines of code generating them, in the code example below.
| >
| > As of today, this bug had not been reported on the R Bugzilla page. I could not submit this report directly to the page because I am not a member, and , as explained in the "Reporting Bugs" link from the  R home page, membership has now been closed due to spamming problems..
| >
| > ############################################################################
| > library(nlme)
| > #Using version 3.1-131
| > #Windows 7 64-bit operating system
| >
| > fm2 <- lme(distance ~ age + Sex, data = Orthodont, random = ~ 1)
| >
| > # Error in array(c(rep(1, p), .C(inner_perc_table, as.double(X), as.integer(unlist(grps)),  :
| > # object 'inner_perc_table' not found

That is a known issue with R 3.4.0 -- see NEWS.

Packages using .C and .Fortran _must_ be recompiled for R 3.4.0. If and when
you do, the example will work again.

Dirk

-- 
http://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From plummerm at iarc.fr  Thu May 11 14:23:33 2017
From: plummerm at iarc.fr (Martyn Plummer)
Date: Thu, 11 May 2017 12:23:33 +0000
Subject: [Rd] bug report: nlme model-fitting crashes with R 3.4.0
In-Reply-To: <22804.19722.546929.314493@max.eddelbuettel.com>
References: <106CAE40B5287C47982478041DB6FAC79962728C@HC-MAILBOXC1-N6.healthcare.uiowa.edu>
 <0048a68b-8676-9ec3-76fc-671a5c156996@cmap.polytechnique.fr>
 <22804.19722.546929.314493@max.eddelbuettel.com>
Message-ID: <1494505413.12031.135.camel@iarc.fr>

On Thu, 2017-05-11 at 06:37 -0500, Dirk Eddelbuettel wrote:
> On 11 May 2017 at 10:17, Erwan Le Pennec wrote:
> > ?????Dear all,
> > 
> > ?????I've stumbled a similar issue with the package cluster when?
> > compiling the 3.4.0 version with the settings of Fedora RPM specs.?
> > Compiling R with the default setting of configure yields a version
> > that?
> > works for cluster... and nlme.
> > 
> > ?????I did not find the exact option that was the cause of this
> > issue?
> > but I'm willing to help.
> > 
> > ?????Erwan
> > 
> > PS: This is the reason why R is still at version 3.3.3 on the
> > Fedora?
> > distribution.
> > 
> > On 10/05/17 22:59, Langbehn, Douglas wrote:
> > > lme() and gls() models from the nlme package are all crashing
> > > with R.3.4.0. Identical code ran correctly, without error in R
> > > 3.3.3 and earlier versions.??The behavior is easily demonstrated
> > > using one of the examples form the lme() help file, along with
> > > two simple variants. I have commented the errors generated by
> > > these calls, as well as the lines of code generating them, in the
> > > code example below.
> > > 
> > > As of today, this bug had not been reported on the R Bugzilla
> > > page. I could not submit this report directly to the page because
> > > I am not a member, and , as explained in the "Reporting Bugs"
> > > link from the??R home page, membership has now been closed due to
> > > spamming problems..
> > > 
> > > #################################################################
> > > ###########
> > > library(nlme)
> > > #Using version 3.1-131
> > > #Windows 7 64-bit operating system
> > > 
> > > fm2 <- lme(distance ~ age + Sex, data = Orthodont, random = ~ 1)
> > > 
> > > # Error in array(c(rep(1, p), .C(inner_perc_table, as.double(X),
> > > as.integer(unlist(grps)),??:
> > > # object 'inner_perc_table' not found
> 
> That is a known issue with R 3.4.0 -- see NEWS.
> 
> Packages using .C and .Fortran _must_ be recompiled for R 3.4.0. If
> and when
> you do, the example will work again.
> 
> Dirk

However, the issue raised by Erwan on Fedora is a real bug which
affects at least two recommended packages. I know the cause of the
problem and am trying to find out how many packages are affected.

Martyn

From wdunlap at tibco.com  Thu May 11 16:33:53 2017
From: wdunlap at tibco.com (William Dunlap)
Date: Thu, 11 May 2017 07:33:53 -0700
Subject: [Rd] R-3.3.3/R-3.4.0 change in sys.call(sys.parent())
In-Reply-To: <CADfFDC5=_di=1QV4FiuH0Ap2pJoP-6uBmDgbuEH=uMLUmMrViA@mail.gmail.com>
References: <CAF8bMcZm2EvJLTUH6x51Fxi+HQyHD0zBMKTfTOsvQfnVtDKBGw@mail.gmail.com>
 <CADfFDC5=_di=1QV4FiuH0Ap2pJoP-6uBmDgbuEH=uMLUmMrViA@mail.gmail.com>
Message-ID: <CAF8bMcbhsSciSFeyEiegYtCijHvn8ZChDX10S2WEv1mnhw0eLA@mail.gmail.com>

Here is a case where the current scheme fails:

  > with(datasets::mtcars, xyplot(mpg~wt|gear)$call)
  xyplot(substitute(expr), data, enclos = parent.frame())


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Thu, May 11, 2017 at 1:09 AM, Deepayan Sarkar <deepayan.sarkar at gmail.com>
wrote:

> On Wed, May 10, 2017 at 2:36 AM, William Dunlap via R-devel
> <r-devel at r-project.org> wrote:
> > Some formula methods for S3 generic functions use the idiom
> >     returnValue$call <- sys.call(sys.parent())
> > to show how to recreate the returned object or to use as a label on a
> > plot.  It is often followed by
> >      returnValue$call[[1]] <- quote(myName)
> > E.g., I see it in packages "latticeExtra" and "leaps", and I suspect it
> > used in "lattice" as well.
> >
> > This idiom has not done good things for quite a while (ever?) but I
> noticed
> > while running tests that it acts differently in R-3.4.0 than in R-3.3.3.
> > Neither the old or new behavior is nice.  E.g., in R-3.3.3 we get
> >
> >> parseEval <- function(text, envir) eval(parse(text=text), envir=envir)
> >> parseEval('lattice::xyplot(mpg~hp, data=datasets::mtcars)$call',
> > envir=new.env())
> > xyplot(expr, envir, enclos)
> >
> > and
> >
> >> evalInEnvir <- function(call, envir) eval(call, envir=envir)
> >> evalInEnvir(quote(lattice::xyplot(mpg~hp, data=datasets::mtcars)$call),
> > envir=new.env())
> > xyplot(expr, envir, enclos)
> >
> > while in R-3.4.0 we get
> >> parseEval <- function(text, envir) eval(parse(text=text), envir=envir)
> >> parseEval('lattice::xyplot(mpg~hp, data=datasets::mtcars)$call',
> > envir=new.env())
> > xyplot(parse(text = text), envir = envir)
> >
> > and
> >
> >> evalInEnvir <- function(call, envir) eval(call, envir=envir)
> >> evalInEnvir(quote(lattice::xyplot(mpg~hp, data=datasets::mtcars)$call),
> > envir=new.env())
> > xyplot(call, envir = envir)
> >
> > Should these packages be be fixed up to use just sys.call()?
>
> I admit to not understanding these things very well, but I'll try to
> explain why I ended up with the usage I have. The main use of the
> $call component within lattice is to use it in the summary method, as
> in:
>
> > summary(xyplot(mpg~hp, data=mtcars))
>
> Call:
> xyplot(mpg ~ hp, data = mtcars)
>
> Number of observations:
> [1] 32
>
> Here is a minimal approximation to what I need: Here foo() and bar()
> are generics producing objects of class "foobar", bar() calls foo()
> with one argument changed, and the print() method for "foobar" is just
> supposed to print the call that produced it:
>
> ########
>
> foo <- function(x, ...) UseMethod("foo")
> bar <- function(x, ...) UseMethod("bar")
> print.foobar <- function(x, ...) print(x$call)
>
> ## Using plain sys.call():
>
> foo.formula <- function(x, ...)
> {
>     ans <- structure(list(), class = "foobar")
>     ans$call <- sys.call()
>     ans
> }
>
> bar.formula <- function(x, ..., panel)
> {
>     foo.formula(x, ..., panel = panel.bar)
> }
>
> foo.table <- function(x, ...)
> {
>     ans <- foo.formula(Freq ~ Var1,
>                        as.data.frame.table(x), ...)
>     ans
> }
>
> ## I would get
>
> foo(y ~ x)
> # foo.formula(y ~ x)
>
> bar(y ~ x)
> # foo.formula(x, ..., panel = panel.bar)
>
> foo(as.table(1:10))
> # foo.formula(Freq ~ Var1, as.data.frame.table(x), ...)
>
> ## The last two are improved by
>
> foo.formula <- function(x, ...)
> {
>     ans <- structure(list(), class = "foobar")
>     ans$call <- sys.call(sys.parent())
>     ans
> }
>
> bar(y ~ x)
> ## bar.formula(y ~ x)
>
> foo(as.table(1:10))
> ## foo.table(as.table(1:10))
>
> ########
>
> Adding
>
> ans$call[[1]] <- quote(foo)
>
> (or quote(bar) in bar.formula) is needed to replace the unexported
> method name (foo.formula) with the generic name (foo), but that's
> probably not the problem.
>
> With this approach in lattice,
>
> p <- some.function(...)
> eval(p$call)
>
> usually works, but not always, if I remember correctly.
>
> I'm happy to consider more robust solutions. Maybe I just need to have a
>
> ...$call <- sys.call()
>
> statement in every method?
>
> -Deepayan
>

	[[alternative HTML version deleted]]


From plummerm at iarc.fr  Thu May 11 18:19:51 2017
From: plummerm at iarc.fr (Martyn Plummer)
Date: Thu, 11 May 2017 16:19:51 +0000
Subject: [Rd] bug report: nlme model-fitting crashes with R 3.4.0
In-Reply-To: <1494505413.12031.135.camel@iarc.fr>
References: <106CAE40B5287C47982478041DB6FAC79962728C@HC-MAILBOXC1-N6.healthcare.uiowa.edu>
 <0048a68b-8676-9ec3-76fc-671a5c156996@cmap.polytechnique.fr>
 <22804.19722.546929.314493@max.eddelbuettel.com>
 <1494505413.12031.135.camel@iarc.fr>
Message-ID: <1494519584.10161.2.camel@iarc.fr>

On Thu, 2017-05-11 at 12:23 +0000, Martyn Plummer wrote:
> On Thu, 2017-05-11 at 06:37 -0500, Dirk Eddelbuettel wrote:
> > On 11 May 2017 at 10:17, Erwan Le Pennec wrote:
> > > ?????Dear all,
> > > 
> > > ?????I've stumbled a similar issue with the package cluster when?
> > > compiling the 3.4.0 version with the settings of Fedora RPM
> > > specs.?
> > > Compiling R with the default setting of configure yields a
> > > version
> > > that?
> > > works for cluster... and nlme.
> > > 
> > > ?????I did not find the exact option that was the cause of this
> > > issue?
> > > but I'm willing to help.
> > > 
> > > ?????Erwan
> > > 
> > > PS: This is the reason why R is still at version 3.3.3 on the
> > > Fedora?
> > > distribution.
> > > 
> > > On 10/05/17 22:59, Langbehn, Douglas wrote:
> > > > lme() and gls() models from the nlme package are all crashing
> > > > with R.3.4.0. Identical code ran correctly, without error in R
> > > > 3.3.3 and earlier versions.??The behavior is easily
> > > > demonstrated
> > > > using one of the examples form the lme() help file, along with
> > > > two simple variants. I have commented the errors generated by
> > > > these calls, as well as the lines of code generating them, in
> > > > the
> > > > code example below.
> > > > 
> > > > As of today, this bug had not been reported on the R Bugzilla
> > > > page. I could not submit this report directly to the page
> > > > because
> > > > I am not a member, and , as explained in the "Reporting Bugs"
> > > > link from the??R home page, membership has now been closed due
> > > > to
> > > > spamming problems..
> > > > 
> > > > ###############################################################
> > > > ##
> > > > ###########
> > > > library(nlme)
> > > > #Using version 3.1-131
> > > > #Windows 7 64-bit operating system
> > > > 
> > > > fm2 <- lme(distance ~ age + Sex, data = Orthodont, random = ~
> > > > 1)
> > > > 
> > > > # Error in array(c(rep(1, p), .C(inner_perc_table,
> > > > as.double(X),
> > > > as.integer(unlist(grps)),??:
> > > > # object 'inner_perc_table' not found
> > 
> > That is a known issue with R 3.4.0 -- see NEWS.
> > 
> > Packages using .C and .Fortran _must_ be recompiled for R 3.4.0. If
> > and when
> > you do, the example will work again.
> > 
> > Dirk
> 
> However, the issue raised by Erwan on Fedora is a real bug which
> affects at least two recommended packages. I know the cause of the
> problem and am trying to find out how many packages are affected.

Sorry for the false alarm. Dirk is right and the problem is the binary
incompatibility between 3.4.0 and 3.3.3.

If you try building the R source RPM with R 3.4.0 *without using a
chroot* then you get interference from the installed version of R (i.e.
3.3.3) when the %check section of the spec file is run. This is not a
bug in the spec file because you are not supposed to build an SRPM this
way.

If you build the SRPM *using mock* (the chroot tool used by Red Hat)
then the build fails for a completely different reason. The chroot is
not set up with time zone information and this triggers one of the
regression tests.

I'm filing a bug report with Red Hat. So hopefully we will see RPMs of
R 3.4.0 for Fedora soon.

Martyn



> Martyn
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

From deepayan.sarkar at gmail.com  Fri May 12 09:25:32 2017
From: deepayan.sarkar at gmail.com (Deepayan Sarkar)
Date: Fri, 12 May 2017 12:55:32 +0530
Subject: [Rd] R-3.3.3/R-3.4.0 change in sys.call(sys.parent())
In-Reply-To: <CAF8bMcbhsSciSFeyEiegYtCijHvn8ZChDX10S2WEv1mnhw0eLA@mail.gmail.com>
References: <CAF8bMcZm2EvJLTUH6x51Fxi+HQyHD0zBMKTfTOsvQfnVtDKBGw@mail.gmail.com>
 <CADfFDC5=_di=1QV4FiuH0Ap2pJoP-6uBmDgbuEH=uMLUmMrViA@mail.gmail.com>
 <CAF8bMcbhsSciSFeyEiegYtCijHvn8ZChDX10S2WEv1mnhw0eLA@mail.gmail.com>
Message-ID: <CADfFDC5bHJgJqSbdqeuoZ_wAduBLN6xO_a_FhPEewLVryK7gzg@mail.gmail.com>

On Thu, May 11, 2017 at 8:03 PM, William Dunlap <wdunlap at tibco.com> wrote:
> Here is a case where the current scheme fails:
>
>   > with(datasets::mtcars, xyplot(mpg~wt|gear)$call)
>   xyplot(substitute(expr), data, enclos = parent.frame())

Right, thanks. So I guess I can't avoid setting $call inside every method.

-Deepayan

> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
> On Thu, May 11, 2017 at 1:09 AM, Deepayan Sarkar <deepayan.sarkar at gmail.com>
> wrote:
>>
>> On Wed, May 10, 2017 at 2:36 AM, William Dunlap via R-devel
>> <r-devel at r-project.org> wrote:
>> > Some formula methods for S3 generic functions use the idiom
>> >     returnValue$call <- sys.call(sys.parent())
>> > to show how to recreate the returned object or to use as a label on a
>> > plot.  It is often followed by
>> >      returnValue$call[[1]] <- quote(myName)
>> > E.g., I see it in packages "latticeExtra" and "leaps", and I suspect it
>> > used in "lattice" as well.
>> >
>> > This idiom has not done good things for quite a while (ever?) but I
>> > noticed
>> > while running tests that it acts differently in R-3.4.0 than in R-3.3.3.
>> > Neither the old or new behavior is nice.  E.g., in R-3.3.3 we get
>> >
>> >> parseEval <- function(text, envir) eval(parse(text=text), envir=envir)
>> >> parseEval('lattice::xyplot(mpg~hp, data=datasets::mtcars)$call',
>> > envir=new.env())
>> > xyplot(expr, envir, enclos)
>> >
>> > and
>> >
>> >> evalInEnvir <- function(call, envir) eval(call, envir=envir)
>> >> evalInEnvir(quote(lattice::xyplot(mpg~hp, data=datasets::mtcars)$call),
>> > envir=new.env())
>> > xyplot(expr, envir, enclos)
>> >
>> > while in R-3.4.0 we get
>> >> parseEval <- function(text, envir) eval(parse(text=text), envir=envir)
>> >> parseEval('lattice::xyplot(mpg~hp, data=datasets::mtcars)$call',
>> > envir=new.env())
>> > xyplot(parse(text = text), envir = envir)
>> >
>> > and
>> >
>> >> evalInEnvir <- function(call, envir) eval(call, envir=envir)
>> >> evalInEnvir(quote(lattice::xyplot(mpg~hp, data=datasets::mtcars)$call),
>> > envir=new.env())
>> > xyplot(call, envir = envir)
>> >
>> > Should these packages be be fixed up to use just sys.call()?
>>
>> I admit to not understanding these things very well, but I'll try to
>> explain why I ended up with the usage I have. The main use of the
>> $call component within lattice is to use it in the summary method, as
>> in:
>>
>> > summary(xyplot(mpg~hp, data=mtcars))
>>
>> Call:
>> xyplot(mpg ~ hp, data = mtcars)
>>
>> Number of observations:
>> [1] 32
>>
>> Here is a minimal approximation to what I need: Here foo() and bar()
>> are generics producing objects of class "foobar", bar() calls foo()
>> with one argument changed, and the print() method for "foobar" is just
>> supposed to print the call that produced it:
>>
>> ########
>>
>> foo <- function(x, ...) UseMethod("foo")
>> bar <- function(x, ...) UseMethod("bar")
>> print.foobar <- function(x, ...) print(x$call)
>>
>> ## Using plain sys.call():
>>
>> foo.formula <- function(x, ...)
>> {
>>     ans <- structure(list(), class = "foobar")
>>     ans$call <- sys.call()
>>     ans
>> }
>>
>> bar.formula <- function(x, ..., panel)
>> {
>>     foo.formula(x, ..., panel = panel.bar)
>> }
>>
>> foo.table <- function(x, ...)
>> {
>>     ans <- foo.formula(Freq ~ Var1,
>>                        as.data.frame.table(x), ...)
>>     ans
>> }
>>
>> ## I would get
>>
>> foo(y ~ x)
>> # foo.formula(y ~ x)
>>
>> bar(y ~ x)
>> # foo.formula(x, ..., panel = panel.bar)
>>
>> foo(as.table(1:10))
>> # foo.formula(Freq ~ Var1, as.data.frame.table(x), ...)
>>
>> ## The last two are improved by
>>
>> foo.formula <- function(x, ...)
>> {
>>     ans <- structure(list(), class = "foobar")
>>     ans$call <- sys.call(sys.parent())
>>     ans
>> }
>>
>> bar(y ~ x)
>> ## bar.formula(y ~ x)
>>
>> foo(as.table(1:10))
>> ## foo.table(as.table(1:10))
>>
>> ########
>>
>> Adding
>>
>> ans$call[[1]] <- quote(foo)
>>
>> (or quote(bar) in bar.formula) is needed to replace the unexported
>> method name (foo.formula) with the generic name (foo), but that's
>> probably not the problem.
>>
>> With this approach in lattice,
>>
>> p <- some.function(...)
>> eval(p$call)
>>
>> usually works, but not always, if I remember correctly.
>>
>> I'm happy to consider more robust solutions. Maybe I just need to have a
>>
>> ...$call <- sys.call()
>>
>> statement in every method?
>>
>> -Deepayan
>
>


From pperry at stern.nyu.edu  Fri May 12 15:50:23 2017
From: pperry at stern.nyu.edu (Patrick Perry)
Date: Fri, 12 May 2017 15:50:23 +0200
Subject: [Rd] xrealloc namespace conflict
In-Reply-To: <22804.19246.514394.430012@max.eddelbuettel.com>
References: <590DED69.30404@stern.nyu.edu> <59143A13.4090600@stern.nyu.edu>
 <22804.19246.514394.430012@max.eddelbuettel.com>
Message-ID: <5915BD9F.2000101@stern.nyu.edu>

Thanks for the response. It looks like there is a dlopen flag that will 
fix the issue on Linux (RTLD_DEEPBIND); on Mac OS, symbols get resolved 
by searching in the library first, so the flag isn't needed. I don't 
know what the default behavior is on other platforms. See, e.g. 
https://software.intel.com/en-us/articles/ensuring-shared-library-uses-intel-math-functions 
.

The use of RTLD_DEEPBIND is not portable, and "should only be used as a 
last resort" according to Ulrich Drepper: 
https://software.intel.com/sites/default/files/m/a/1/e/dsohowto.pdf .

I'll probably end up prefixing all of my functions with "corpus_" to 
avoid future namespace clashers.


Patrick


Dirk Eddelbuettel wrote:
>
> On 11 May 2017 at 12:16, Patrick Perry wrote:
> | I've done a bit more investigation into this issue. Here is my current
> | understanding of the situation:
> |
> | 1. I have a package on CRAN (corpus-0.3.1) that passes tests on all
> | platforms except for Linux.
> | 2. My package defines a C function, "xrealloc", for internal use.
> | 3. The libreadline library that R links to defines a different version
> | of "xrealloc".
> | 4. On Linux, when I load my package, references to "xrealloc" get linked
> | to the libreadline version of that function.
> | 5. This causes one of my tests to fail, calling exit(2), because the
> | libreadline version of xrealloc does not allow calls of the form
> | "xrealloc(ptr, 0)".
> |
> | I can work around this issue pretty easily, by either renaming my
> | version of xrealloc to something else, or by avoiding calls of the form
> | "xrealloc(ptr, 0)". So, this is not a major issue, but it's a little
> | unsettling to see this behavior when my package does not explicitly link
> | to or use anything from libreadline.
> |
> | Is there a reason this behavior is only manifesting on Linux? Is there
> | something wrong with the way I'm compiling my package on that platform?
> | Is this just some quirk about the way R is loading dynamic libraries on
> | Linux? I'd appreciate any insight into the issue.
>
> It may just be the flat namespace and linking order. AFAIK there is 
> nothing
> in C preventing so maybe you 'just got lucky' on the other platforms. 
> See eg
> http://stackoverflow.com/questions/7998770/
>
> But then I don't use pure C that after anymore ... and in C++ you 
> could just
> wrap a namespace around your code, avoiding the issue.
>
>
> Dirk


> Dirk Eddelbuettel <mailto:edd at debian.org>
> May 11, 2017 at 1:29 PM
> On 11 May 2017 at 12:16, Patrick Perry wrote:
> | I've done a bit more investigation into this issue. Here is my current
> | understanding of the situation:
> |
> | 1. I have a package on CRAN (corpus-0.3.1) that passes tests on all
> | platforms except for Linux.
> | 2. My package defines a C function, "xrealloc", for internal use.
> | 3. The libreadline library that R links to defines a different version
> | of "xrealloc".
> | 4. On Linux, when I load my package, references to "xrealloc" get 
> linked
> | to the libreadline version of that function.
> | 5. This causes one of my tests to fail, calling exit(2), because the
> | libreadline version of xrealloc does not allow calls of the form
> | "xrealloc(ptr, 0)".
> |
> | I can work around this issue pretty easily, by either renaming my
> | version of xrealloc to something else, or by avoiding calls of the form
> | "xrealloc(ptr, 0)". So, this is not a major issue, but it's a little
> | unsettling to see this behavior when my package does not explicitly 
> link
> | to or use anything from libreadline.
> |
> | Is there a reason this behavior is only manifesting on Linux? Is there
> | something wrong with the way I'm compiling my package on that platform?
> | Is this just some quirk about the way R is loading dynamic libraries on
> | Linux? I'd appreciate any insight into the issue.
>
> It may just be the flat namespace and linking order. AFAIK there is 
> nothing
> in C preventing so maybe you 'just got lucky' on the other platforms. 
> See eg
> http://stackoverflow.com/questions/7998770/
>
> But then I don't use pure C that after anymore ... and in C++ you 
> could just
> wrap a namespace around your code, avoiding the issue.
>
>
> Dirk
>
> Patrick Perry <mailto:pperry at stern.nyu.edu>
> May 11, 2017 at 12:16 PM
> I've done a bit more investigation into this issue. Here is my current 
> understanding of the situation:
>
> 1. I have a package on CRAN (corpus-0.3.1) that passes tests on all 
> platforms except for Linux.
> 2. My package defines a C function, "xrealloc", for internal use.
> 3. The libreadline library that R links to defines a different version 
> of "xrealloc".
> 4. On Linux, when I load my package, references to "xrealloc" get 
> linked to the libreadline version of that function.
> 5. This causes one of my tests to fail, calling exit(2), because the 
> libreadline version of xrealloc does not allow calls of the form 
> "xrealloc(ptr, 0)".
>
> I can work around this issue pretty easily, by either renaming my 
> version of xrealloc to something else, or by avoiding calls of the 
> form "xrealloc(ptr, 0)". So, this is not a major issue, but it's a 
> little unsettling to see this behavior when my package does not 
> explicitly link to or use anything from libreadline.
>
> Is there a reason this behavior is only manifesting on Linux? Is there 
> something wrong with the way I'm compiling my package on that 
> platform? Is this just some quirk about the way R is loading dynamic 
> libraries on Linux? I'd appreciate any insight into the issue.
>
>
> Patrick
>
> p.s. Here are some references:
>
> My package Makevars are at 
> https://github.com/patperry/r-corpus/blob/master/src/Makevars ; my 
> version of "xrealloc" is in corpus/src/xalloc.c
>
> You can see the source for the libreadline xrealloc at 
> https://github.com/JuliaLang/readline/blob/master/xmalloc.c#L67
>
>
> Patrick Perry wrote:
>>
>> I have a package on CRAN now (corpus-0.3.1) that is currently failing
>> tests on Linux, but passing on all other architectures:
>>
>> https://cran.r-project.org/web/checks/check_results_corpus.html
>>
>> I believe that the issue arrises from a namespace class between
>> "xrealloc", which my package provides for internal use, but which R
>> also seems to provide (possibly as part of TRE in
>> src/extra/tre/xmalloc.c). It looks like my package is not picking up
>> my custom xrealloc, but using an xrealloc provided by R.
>>
>> Besides the fact that I am linking to the wrong xrealloc, I think my
>> tests are failing for the same reason that the following code
>> segfaults on Linux (Debian, with R 3.4.0):
>>
>> test <- inline::cfunction(language='C',
>>     otherdefs='void *xmalloc(size_t); void *xrealloc(void *, size_t);',
>>     body = 'void *ptr = xmalloc(256); xrealloc(ptr, 0); return
>> R_NilValue;')
>> test()
>> ## xrealloc: out of virtual memory
>>
>> It seems that the R xrealloc doesn't like being given a size of 0,
>> even though this behavior is well-defined for realloc (it should free
>> the memory). Based on my failing CRAN tests, it looks like this is a
>> Linux-specific issue.
>>
>> Is there a way to modify my Makevars to force the linker to choose my
>> version of xrealloc for my package-specific code? My current Makevars
>> are at https://github.com/patperry/r-corpus/blob/master/src/Makevars
>>
>> Thanks in advance for any help.
>>
>>
>> Patrick
>
>
>> Patrick Perry <mailto:pperry at stern.nyu.edu>
>> May 6, 2017 at 5:36 PM
>> I have a package on CRAN now (corpus-0.3.1) that is currently failing 
>> tests on Linux, but passing on all other architectures:
>>
>> https://cran.r-project.org/web/checks/check_results_corpus.html
>>
>> I believe that the issue arrises from a namespace class between 
>> "xrealloc", which my package provides for internal use, but which R 
>> also seems to provide (possibly as part of TRE in 
>> src/extra/tre/xmalloc.c). It looks like my package is not picking up 
>> my custom xrealloc, but using an xrealloc provided by R.
>>
>> Besides the fact that I am linking to the wrong xrealloc, I think my 
>> tests are failing for the same reason that the following code 
>> segfaults on Linux (Debian, with R 3.4.0):
>>
>> test <- inline::cfunction(language='C',
>>     otherdefs='void *xmalloc(size_t); void *xrealloc(void *, size_t);',
>>     body = 'void *ptr = xmalloc(256); xrealloc(ptr, 0); return 
>> R_NilValue;')
>> test()
>> ## xrealloc: out of virtual memory
>>
>> It seems that the R xrealloc doesn't like being given a size of 0, 
>> even though this behavior is well-defined for realloc (it should free 
>> the memory). Based on my failing CRAN tests, it looks like this is a 
>> Linux-specific issue.
>>
>> Is there a way to modify my Makevars to force the linker to choose my 
>> version of xrealloc for my package-specific code? My current Makevars 
>> are at https://github.com/patperry/r-corpus/blob/master/src/Makevars
>>
>> Thanks in advance for any help.
>>
>>
>> Patrick
>>
>
> Patrick Perry <mailto:pperry at stern.nyu.edu>
> May 6, 2017 at 5:36 PM
> I have a package on CRAN now (corpus-0.3.1) that is currently failing 
> tests on Linux, but passing on all other architectures:
>
> https://cran.r-project.org/web/checks/check_results_corpus.html
>
> I believe that the issue arrises from a namespace class between 
> "xrealloc", which my package provides for internal use, but which R 
> also seems to provide (possibly as part of TRE in 
> src/extra/tre/xmalloc.c). It looks like my package is not picking up 
> my custom xrealloc, but using an xrealloc provided by R.
>
> Besides the fact that I am linking to the wrong xrealloc, I think my 
> tests are failing for the same reason that the following code 
> segfaults on Linux (Debian, with R 3.4.0):
>
> test <- inline::cfunction(language='C',
>     otherdefs='void *xmalloc(size_t); void *xrealloc(void *, size_t);',
>     body = 'void *ptr = xmalloc(256); xrealloc(ptr, 0); return 
> R_NilValue;')
> test()
> ## xrealloc: out of virtual memory
>
> It seems that the R xrealloc doesn't like being given a size of 0, 
> even though this behavior is well-defined for realloc (it should free 
> the memory). Based on my failing CRAN tests, it looks like this is a 
> Linux-specific issue.
>
> Is there a way to modify my Makevars to force the linker to choose my 
> version of xrealloc for my package-specific code? My current Makevars 
> are at https://github.com/patperry/r-corpus/blob/master/src/Makevars
>
> Thanks in advance for any help.
>
>
> Patrick
>


	[[alternative HTML version deleted]]


From maechler at stat.math.ethz.ch  Mon May 15 10:39:05 2017
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 15 May 2017 10:39:05 +0200
Subject: [Rd] stopifnot() does not stop at first non-TRUE argument
In-Reply-To: <8945f7e4-99ce-53fa-46ee-cce64078a6a8@fredhutch.org>
References: <64acdaeb-2857-9017-68c9-9da08e29a617@fredhutch.org>
 <E5DADF15-58C6-4F73-BC56-C6404896982F@gmail.com>
 <c2b3cb22-90bd-5372-e8b9-d1ea8b2ab828@fredhutch.org>
 <8945f7e4-99ce-53fa-46ee-cce64078a6a8@fredhutch.org>
Message-ID: <22809.26921.473223.395550@stat.math.ethz.ch>

>>>>> Herv? Pag?s <hpages at fredhutch.org>
>>>>>     on Wed, 3 May 2017 12:08:26 -0700 writes:

    > On 05/03/2017 12:04 PM, Herv? Pag?s wrote:
    >> Not sure why the performance penalty of nonstandard evaluation would
    >> be more of a concern here than for something like switch().

    > which is actually a primitive. So it seems that there is at least
    > another way to go than 'dots <- match.call(expand.dots=FALSE)$...'

    > Thanks, H.

    >> 
    >> If that can't/won't be fixed, what about fixing the man page so it's
    >> in sync with the current behavior?
    >> 
    >> Thanks, H.

Being back from vacations,...
I agree that something should be done here, if not to the code than at
least to the man page.

For now, I'd like to look a bit longer into a possible change to the function.
Peter mentioned a NSE way to fix the problem and you mentioned switch().

Originally, stopifnot() was only a few lines of code and meant to be
"self-explaining" by just reading its definition, and I really would like
to not walk too much away from that original idea.
How did you (Herve) think to use  switch()  here?



    >> On 05/03/2017 02:26 AM, peter dalgaard wrote:
    >>> The first line of stopifnot is
    >>> 
    >>> n <- length(ll <- list(...))
    >>> 
    >>> which takes ALL arguments and forms a list of them. This implies
    >>> evaluation, so explains the effect that you see.
    >>> 
    >>> To do it differently, you would have to do something like
    >>> 
    >>> dots <- match.call(expand.dots=FALSE)$...
    >>> 
    >>> and then explicitly evaluate each argument in turn in the caller
    >>> frame. This amount of nonstandard evaluation sounds like it would
    >>> incur a performance penalty, which could be undesirable.
    >>> 
    >>> If you want to enforce the order of evaluation, there is always
    >>> 
    >>> stopifnot(A) stopifnot(B)
    >>> 
    >>> -pd
    >>> 
    >>>> On 3 May 2017, at 02:50 , Herv? Pag?s <hpages at fredhutch.org>
    >>>> wrote:
    >>>> 
    >>>> Hi,
    >>>> 
    >>>> It's surprising that stopifnot() keeps evaluating its arguments
    >>>> after it reaches the first one that is not TRUE:
    >>>> 
    >>>> > stopifnot(3 == 5, as.integer(2^32), a <- 12) Error: 3 == 5 is
    >>>> not TRUE In addition: Warning message: In stopifnot(3 == 5,
    >>>> as.integer(2^32), a <- 12) : NAs introduced by coercion to integer
    >>>> range > a [1] 12
    >>>> 
    >>>> The details section in its man page actually suggests that it
    >>>> should stop at the first non-TRUE argument:
    >>>> 
    >>>> ?stopifnot(A, B)? is conceptually equivalent to
    >>>> 
    >>>> { if(any(is.na(A)) || !all(A)) stop(...); if(any(is.na(B)) ||
    >>>> !all(B)) stop(...) }
    >>>> 
    >>>> Best, H.
    >>>> 
    >>>> --
    >>>> Herv? Pag?s
    >>>> 
    >>>> Program in Computational Biology Division of Public Health
    >>>> Sciences Fred Hutchinson Cancer Research Center 1100 Fairview
    >>>> Ave. N, M1-B514 P.O. Box 19024 Seattle, WA 98109-1024
    >>>> 
    >>>> E-mail: hpages at fredhutch.org Phone: (206) 667-5791 Fax: (206)
    >>>> 667-1319
    >>>> 
    >>>> ______________________________________________
    >>>> R-devel at r-project.org mailing list
    >>>> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Ddevel&d=DwIFaQ&c=eRAMFD45gAfqt84VtBcfhQ&r=BK7q3XeAvimeWdGbWY_wJYbW0WYiZvSXAJJKaaPhzWA&m=JwgKhKD2k-9Kedeh6pqu-A8x6UEV0INrcxcSGVGo3Tg&s=f7IKJIhpRNJMC3rZAkuI6-MTdL3GAKSV2wK0boFN5HY&e=
    >>>> 
    >>> 
    >> 

    > -- Herv? Pag?s

    > Program in Computational Biology Division of Public Health Sciences
    > Fred Hutchinson Cancer Research Center 1100 Fairview Ave. N,
    > M1-B514 P.O. Box 19024 Seattle, WA 98109-1024

    > E-mail: hpages at fredhutch.org Phone: (206) 667-5791 Fax: (206)
    > 667-1319

    > ______________________________________________
    > R-devel at r-project.org mailing list
    > https://stat.ethz.ch/mailman/listinfo/r-devel


From sokol at insa-toulouse.fr  Mon May 15 12:48:41 2017
From: sokol at insa-toulouse.fr (Serguei Sokol)
Date: Mon, 15 May 2017 12:48:41 +0200
Subject: [Rd] stopifnot() does not stop at first non-TRUE argument
In-Reply-To: <22809.26921.473223.395550@stat.math.ethz.ch>
References: <64acdaeb-2857-9017-68c9-9da08e29a617@fredhutch.org>
 <E5DADF15-58C6-4F73-BC56-C6404896982F@gmail.com>
 <c2b3cb22-90bd-5372-e8b9-d1ea8b2ab828@fredhutch.org>
 <8945f7e4-99ce-53fa-46ee-cce64078a6a8@fredhutch.org>
 <22809.26921.473223.395550@stat.math.ethz.ch>
Message-ID: <8fdb66b1-f785-ed55-6e40-a4ef8138e637@insa-toulouse.fr>

Hello,

I am a new on this list, so I introduce myself very briefly:
my background is applied mathematics, more precisely scientific calculus
applied for modeling metabolic systems, I am author/maintainer of
few packages (Deriv, rmumps, arrApply).

Now, on the subject of this discussion, I must say that I don't really understand
Peter's argument:

     >>> To do it differently, you would have to do something like
     >>>
     >>> dots <- match.call(expand.dots=FALSE)$...
     >>>
     >>> and then explicitly evaluate each argument in turn in the caller
     >>> frame. This amount of nonstandard evaluation sounds like it would
     >>> incur a performance penalty, which could be undesirable.
The first line of the current stopifnot()
n <- length(ll <- list(...))
already evaluates _all_ of the arguments
in the caller frame. So to do the same only
on a part of them (till the first FALSE or NA occurs)
cannot be more penalizing than the current version, right?

I attach here a slightly modified version called stopifnot_new()
which works in accordance with the man page and
where there are only two additional calls: parent.frame() and eval().
I don't think it can be considered as real performance penalty
as the same or bigger amount of (implicit) evaluations was
already done in the current version:

> source("stopifnot_new.R")
> stopifnot_new(3 == 5, as.integer(2^32), a <- 12)
Error: 3 == 5 is not TRUE
> a
Error: object 'a' not found

Best,
Serguei.


Le 15/05/2017 ? 10:39, Martin Maechler a ?crit :
>>>>>> Herv? Pag?s <hpages at fredhutch.org>
>>>>>>      on Wed, 3 May 2017 12:08:26 -0700 writes:
>      > On 05/03/2017 12:04 PM, Herv? Pag?s wrote:
>      >> Not sure why the performance penalty of nonstandard evaluation would
>      >> be more of a concern here than for something like switch().
>
>      > which is actually a primitive. So it seems that there is at least
>      > another way to go than 'dots <- match.call(expand.dots=FALSE)$...'
>
>      > Thanks, H.
>
>      >>
>      >> If that can't/won't be fixed, what about fixing the man page so it's
>      >> in sync with the current behavior?
>      >>
>      >> Thanks, H.
>
> Being back from vacations,...
> I agree that something should be done here, if not to the code than at
> least to the man page.
>
> For now, I'd like to look a bit longer into a possible change to the function.
> Peter mentioned a NSE way to fix the problem and you mentioned switch().
>
> Originally, stopifnot() was only a few lines of code and meant to be
> "self-explaining" by just reading its definition, and I really would like
> to not walk too much away from that original idea.
> How did you (Herve) think to use  switch()  here?
>
>
>
>      >> On 05/03/2017 02:26 AM, peter dalgaard wrote:
>      >>> The first line of stopifnot is
>      >>>
>      >>> n <- length(ll <- list(...))
>      >>>
>      >>> which takes ALL arguments and forms a list of them. This implies
>      >>> evaluation, so explains the effect that you see.
>      >>>
>      >>> To do it differently, you would have to do something like
>      >>>
>      >>> dots <- match.call(expand.dots=FALSE)$...
>      >>>
>      >>> and then explicitly evaluate each argument in turn in the caller
>      >>> frame. This amount of nonstandard evaluation sounds like it would
>      >>> incur a performance penalty, which could be undesirable.
>      >>>
>      >>> If you want to enforce the order of evaluation, there is always
>      >>>
>      >>> stopifnot(A) stopifnot(B)
>      >>>
>      >>> -pd
>      >>>
>      >>>> On 3 May 2017, at 02:50 , Herv? Pag?s <hpages at fredhutch.org>
>      >>>> wrote:
>      >>>>
>      >>>> Hi,
>      >>>>
>      >>>> It's surprising that stopifnot() keeps evaluating its arguments
>      >>>> after it reaches the first one that is not TRUE:
>      >>>>
>      >>>> > stopifnot(3 == 5, as.integer(2^32), a <- 12) Error: 3 == 5 is
>      >>>> not TRUE In addition: Warning message: In stopifnot(3 == 5,
>      >>>> as.integer(2^32), a <- 12) : NAs introduced by coercion to integer
>      >>>> range > a [1] 12
>      >>>>
>      >>>> The details section in its man page actually suggests that it
>      >>>> should stop at the first non-TRUE argument:
>      >>>>
>      >>>> ?stopifnot(A, B)? is conceptually equivalent to
>      >>>>
>      >>>> { if(any(is.na(A)) || !all(A)) stop(...); if(any(is.na(B)) ||
>      >>>> !all(B)) stop(...) }
>      >>>>
>      >>>> Best, H.
>      >>>>
>      >>>> --
>      >>>> Herv? Pag?s
>      >>>>
>      >>>> Program in Computational Biology Division of Public Health
>      >>>> Sciences Fred Hutchinson Cancer Research Center 1100 Fairview
>      >>>> Ave. N, M1-B514 P.O. Box 19024 Seattle, WA 98109-1024
>      >>>>
>      >>>> E-mail: hpages at fredhutch.org Phone: (206) 667-5791 Fax: (206)
>      >>>> 667-1319
>      >>>>
>      >>>> ______________________________________________
>      >>>> R-devel at r-project.org mailing list
>      >>>> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Ddevel&d=DwIFaQ&c=eRAMFD45gAfqt84VtBcfhQ&r=BK7q3XeAvimeWdGbWY_wJYbW0WYiZvSXAJJKaaPhzWA&m=JwgKhKD2k-9Kedeh6pqu-A8x6UEV0INrcxcSGVGo3Tg&s=f7IKJIhpRNJMC3rZAkuI6-MTdL3GAKSV2wK0boFN5HY&e=
>      >>>>
>      >>>
>      >>
>
>      > -- Herv? Pag?s
>
>      > Program in Computational Biology Division of Public Health Sciences
>      > Fred Hutchinson Cancer Research Center 1100 Fairview Ave. N,
>      > M1-B514 P.O. Box 19024 Seattle, WA 98109-1024
>
>      > E-mail: hpages at fredhutch.org Phone: (206) 667-5791 Fax: (206)
>      > 667-1319
>
>      > ______________________________________________
>      > R-devel at r-project.org mailing list
>      > https://stat.ethz.ch/mailman/listinfo/r-devel
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From sokol at insa-toulouse.fr  Mon May 15 13:14:34 2017
From: sokol at insa-toulouse.fr (Serguei Sokol)
Date: Mon, 15 May 2017 13:14:34 +0200
Subject: [Rd] stopifnot() does not stop at first non-TRUE argument
In-Reply-To: <8fdb66b1-f785-ed55-6e40-a4ef8138e637@insa-toulouse.fr>
References: <64acdaeb-2857-9017-68c9-9da08e29a617@fredhutch.org>
 <E5DADF15-58C6-4F73-BC56-C6404896982F@gmail.com>
 <c2b3cb22-90bd-5372-e8b9-d1ea8b2ab828@fredhutch.org>
 <8945f7e4-99ce-53fa-46ee-cce64078a6a8@fredhutch.org>
 <22809.26921.473223.395550@stat.math.ethz.ch>
 <8fdb66b1-f785-ed55-6e40-a4ef8138e637@insa-toulouse.fr>
Message-ID: <26bd9210-e40c-0967-7abb-5f724b09c034@insa-toulouse.fr>

I see in the archives that the attachment cannot pass.
So, here is the code:
8<----
stopifnot_new <- function (...)
{
     mc <- match.call()
     n <- length(mc)-1
     if (n == 0L)
         return(invisible())
     Dparse <- function(call, cutoff = 60L) {
         ch <- deparse(call, width.cutoff = cutoff)
         if (length(ch) > 1L)
             paste(ch[1L], "....")
         else ch
     }
     head <- function(x, n = 6L) x[seq_len(if (n < 0L) max(length(x) +
         n, 0L) else min(n, length(x)))]
     abbrev <- function(ae, n = 3L) paste(c(head(ae, n), if (length(ae) >
         n) "...."), collapse = "\n  ")
     pfr <- parent.frame()
     for (i in 1L:n) {
         cl.i <- mc[[i + 1L]]
         r <- eval(cl.i, pfr)
         if (!(is.logical(r) && !anyNA(r) && all(r))) {
             msg <- if (is.call(cl.i) && identical(cl.i[[1]], quote(all.equal)) &&
                 (is.null(ni <- names(cl.i)) || length(cl.i) == 3L ||
                     length(cl.i <- cl.i[!nzchar(ni)]) == 3L))
                 sprintf(gettext("%s and %s are not equal:\n  %s"),
                     Dparse(cl.i[[2]]), Dparse(cl.i[[3]]), abbrev(r))
             else sprintf(ngettext(length(r), "%s is not TRUE", "%s are not all TRUE"),
                 Dparse(cl.i))
             stop(msg, call. = FALSE, domain = NA)
         }
     }
     invisible()
}
8<----

Best,
Serguei.

Le 15/05/2017 ? 12:48, Serguei Sokol a ?crit :
> Hello,
>
> I am a new on this list, so I introduce myself very briefly:
> my background is applied mathematics, more precisely scientific calculus
> applied for modeling metabolic systems, I am author/maintainer of
> few packages (Deriv, rmumps, arrApply).
>
> Now, on the subject of this discussion, I must say that I don't really understand
> Peter's argument:
>
>     >>> To do it differently, you would have to do something like
>     >>>
>     >>> dots <- match.call(expand.dots=FALSE)$...
>     >>>
>     >>> and then explicitly evaluate each argument in turn in the caller
>     >>> frame. This amount of nonstandard evaluation sounds like it would
>     >>> incur a performance penalty, which could be undesirable.
> The first line of the current stopifnot()
> n <- length(ll <- list(...))
> already evaluates _all_ of the arguments
> in the caller frame. So to do the same only
> on a part of them (till the first FALSE or NA occurs)
> cannot be more penalizing than the current version, right?
>
> I attach here a slightly modified version called stopifnot_new()
> which works in accordance with the man page and
> where there are only two additional calls: parent.frame() and eval().
> I don't think it can be considered as real performance penalty
> as the same or bigger amount of (implicit) evaluations was
> already done in the current version:
>
>> source("stopifnot_new.R")
>> stopifnot_new(3 == 5, as.integer(2^32), a <- 12)
> Error: 3 == 5 is not TRUE
>> a
> Error: object 'a' not found
>
> Best,
> Serguei.
>
>
> Le 15/05/2017 ? 10:39, Martin Maechler a ?crit :
>>>>>>> Herv? Pag?s <hpages at fredhutch.org>
>>>>>>>      on Wed, 3 May 2017 12:08:26 -0700 writes:
>>      > On 05/03/2017 12:04 PM, Herv? Pag?s wrote:
>>      >> Not sure why the performance penalty of nonstandard evaluation would
>>      >> be more of a concern here than for something like switch().
>>
>>      > which is actually a primitive. So it seems that there is at least
>>      > another way to go than 'dots <- match.call(expand.dots=FALSE)$...'
>>
>>      > Thanks, H.
>>
>>      >>
>>      >> If that can't/won't be fixed, what about fixing the man page so it's
>>      >> in sync with the current behavior?
>>      >>
>>      >> Thanks, H.
>>
>> Being back from vacations,...
>> I agree that something should be done here, if not to the code than at
>> least to the man page.
>>
>> For now, I'd like to look a bit longer into a possible change to the function.
>> Peter mentioned a NSE way to fix the problem and you mentioned switch().
>>
>> Originally, stopifnot() was only a few lines of code and meant to be
>> "self-explaining" by just reading its definition, and I really would like
>> to not walk too much away from that original idea.
>> How did you (Herve) think to use  switch()  here?
>>
>>
>>
>>      >> On 05/03/2017 02:26 AM, peter dalgaard wrote:
>>      >>> The first line of stopifnot is
>>      >>>
>>      >>> n <- length(ll <- list(...))
>>      >>>
>>      >>> which takes ALL arguments and forms a list of them. This implies
>>      >>> evaluation, so explains the effect that you see.
>>      >>>
>>      >>> To do it differently, you would have to do something like
>>      >>>
>>      >>> dots <- match.call(expand.dots=FALSE)$...
>>      >>>
>>      >>> and then explicitly evaluate each argument in turn in the caller
>>      >>> frame. This amount of nonstandard evaluation sounds like it would
>>      >>> incur a performance penalty, which could be undesirable.
>>      >>>
>>      >>> If you want to enforce the order of evaluation, there is always
>>      >>>
>>      >>> stopifnot(A) stopifnot(B)
>>      >>>
>>      >>> -pd
>>      >>>
>>      >>>> On 3 May 2017, at 02:50 , Herv? Pag?s <hpages at fredhutch.org>
>>      >>>> wrote:
>>      >>>>
>>      >>>> Hi,
>>      >>>>
>>      >>>> It's surprising that stopifnot() keeps evaluating its arguments
>>      >>>> after it reaches the first one that is not TRUE:
>>      >>>>
>>      >>>> > stopifnot(3 == 5, as.integer(2^32), a <- 12) Error: 3 == 5 is
>>      >>>> not TRUE In addition: Warning message: In stopifnot(3 == 5,
>>      >>>> as.integer(2^32), a <- 12) : NAs introduced by coercion to integer
>>      >>>> range > a [1] 12
>>      >>>>
>>      >>>> The details section in its man page actually suggests that it
>>      >>>> should stop at the first non-TRUE argument:
>>      >>>>
>>      >>>> ?stopifnot(A, B)? is conceptually equivalent to
>>      >>>>
>>      >>>> { if(any(is.na(A)) || !all(A)) stop(...); if(any(is.na(B)) ||
>>      >>>> !all(B)) stop(...) }
>>      >>>>
>>      >>>> Best, H.
>>      >>>>
>>      >>>> --
>>      >>>> Herv? Pag?s
>>      >>>>
>>      >>>> Program in Computational Biology Division of Public Health
>>      >>>> Sciences Fred Hutchinson Cancer Research Center 1100 Fairview
>>      >>>> Ave. N, M1-B514 P.O. Box 19024 Seattle, WA 98109-1024
>>      >>>>
>>      >>>> E-mail: hpages at fredhutch.org Phone: (206) 667-5791 Fax: (206)
>>      >>>> 667-1319
>>      >>>>
>>      >>>> ______________________________________________
>>      >>>> R-devel at r-project.org mailing list
>>      >>>> 
>> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Ddevel&d=DwIFaQ&c=eRAMFD45gAfqt84VtBcfhQ&r=BK7q3XeAvimeWdGbWY_wJYbW0WYiZvSXAJJKaaPhzWA&m=JwgKhKD2k-9Kedeh6pqu-A8x6UEV0INrcxcSGVGo3Tg&s=f7IKJIhpRNJMC3rZAkuI6-MTdL3GAKSV2wK0boFN5HY&e=
>>      >>>>
>>      >>>
>>      >>
>>
>>      > -- Herv? Pag?s
>>
>>      > Program in Computational Biology Division of Public Health Sciences
>>      > Fred Hutchinson Cancer Research Center 1100 Fairview Ave. N,
>>      > M1-B514 P.O. Box 19024 Seattle, WA 98109-1024
>>
>>      > E-mail: hpages at fredhutch.org Phone: (206) 667-5791 Fax: (206)
>>      > 667-1319
>>
>>      > ______________________________________________
>>      > R-devel at r-project.org mailing list
>>      > https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From jeroen at berkeley.edu  Mon May 15 14:07:28 2017
From: jeroen at berkeley.edu (Jeroen Ooms)
Date: Mon, 15 May 2017 14:07:28 +0200
Subject: [Rd] Error messages in replayPlot()
Message-ID: <CABFfbXtAaM0O0dtFFdLYBk40w1ADaoUz4UJVMRBOVmfgma6YPA@mail.gmail.com>

I was wondering if there is something that can be done to improve
error messages when replaying a recorded plot. For example a graphics
device that is too small usually results in a helpful error message:

 png(height = 100)
 plot(1)
 # Error in plot.new() : figure margins too large
 dev.off()

However when this happens when replaying a recorded plot, the error
message is not so helpful.

 myplot <- evaluate::evaluate("plot(1)")[[2]]
 png(height = 100)
 replayPlot(myplot)
 # Error in replayPlot(x) : invalid graphics state
 dev.off()

A more informative error message that hints at what exactly is invalid
about the graphics state would be very helpful in this case.


From maechler at stat.math.ethz.ch  Mon May 15 15:37:34 2017
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 15 May 2017 15:37:34 +0200
Subject: [Rd] stopifnot() does not stop at first non-TRUE argument
In-Reply-To: <26bd9210-e40c-0967-7abb-5f724b09c034@insa-toulouse.fr>
References: <64acdaeb-2857-9017-68c9-9da08e29a617@fredhutch.org>
 <E5DADF15-58C6-4F73-BC56-C6404896982F@gmail.com>
 <c2b3cb22-90bd-5372-e8b9-d1ea8b2ab828@fredhutch.org>
 <8945f7e4-99ce-53fa-46ee-cce64078a6a8@fredhutch.org>
 <22809.26921.473223.395550@stat.math.ethz.ch>
 <8fdb66b1-f785-ed55-6e40-a4ef8138e637@insa-toulouse.fr>
 <26bd9210-e40c-0967-7abb-5f724b09c034@insa-toulouse.fr>
Message-ID: <22809.44830.153073.675527@stat.math.ethz.ch>

>>>>> Serguei Sokol <sokol at insa-toulouse.fr>
>>>>>     on Mon, 15 May 2017 13:14:34 +0200 writes:

    > I see in the archives that the attachment cannot pass.
    > So, here is the code:

    [....... MM: I needed to reformat etc to match closely to
     the current source code which is in
     	 https://svn.r-project.org/R/trunk/src/library/base/R/stop.R
     or its corresponding github mirror
        https://github.com/wch/r-source/blob/trunk/src/library/base/R/stop.R
    ]

    > Best,
    > Serguei.

Yes, something like that seems even simpler than Peter's
suggestion...

It currently breaks 'make check' in the R sources,
specifically in tests/reg-tests-2.R (lines 6574 ff),
the new code now gives

  > ## error messages from (C-level) evalList
  > tst <- function(y) { stopifnot(is.numeric(y)); y+ 1 }
  > try(tst())
  Error in eval(cl.i, pfr) : argument "y" is missing, with no default

whereas previously it gave

  Error in stopifnot(is.numeric(y)) : 
     argument "y" is missing, with no default


But I think that change (of call stack in such an error case) is
unavoidable and not a big problem.

--

I'm still curious about Herv?'s idea on using  switch()  for the
issue.

Martin


    > Le 15/05/2017 ? 12:48, Serguei Sokol a ?crit :
    >> Hello,
    >> 
    >> I am a new on this list, so I introduce myself very briefly:
    >> my background is applied mathematics, more precisely scientific calculus
    >> applied for modeling metabolic systems, I am author/maintainer of
    >> few packages (Deriv, rmumps, arrApply).
    >> 
    >> Now, on the subject of this discussion, I must say that I don't really understand
    >> Peter's argument:
    >> 
    >> >>> To do it differently, you would have to do something like
    >> >>>
    >> >>> dots <- match.call(expand.dots=FALSE)$...
    >> >>>
    >> >>> and then explicitly evaluate each argument in turn in the caller
    >> >>> frame. This amount of nonstandard evaluation sounds like it would
    >> >>> incur a performance penalty, which could be undesirable.
    >> The first line of the current stopifnot()
    >> n <- length(ll <- list(...))
    >> already evaluates _all_ of the arguments
    >> in the caller frame. So to do the same only
    >> on a part of them (till the first FALSE or NA occurs)
    >> cannot be more penalizing than the current version, right?
    >> 
    >> I attach here a slightly modified version called stopifnot_new()
    >> which works in accordance with the man page and
    >> where there are only two additional calls: parent.frame() and eval().
    >> I don't think it can be considered as real performance penalty
    >> as the same or bigger amount of (implicit) evaluations was
    >> already done in the current version:
    >> 
    >>> source("stopifnot_new.R")
    >>> stopifnot_new(3 == 5, as.integer(2^32), a <- 12)
    >> Error: 3 == 5 is not TRUE
    >>> a
    >> Error: object 'a' not found
    >> 
    >> Best,
    >> Serguei.
    >> 
    >> 
    >> Le 15/05/2017 ? 10:39, Martin Maechler a ?crit :
    >>>>>>>> Herv? Pag?s <hpages at fredhutch.org>
    >>>>>>>> on Wed, 3 May 2017 12:08:26 -0700 writes:
    >>> > On 05/03/2017 12:04 PM, Herv? Pag?s wrote:
    >>> >> Not sure why the performance penalty of nonstandard evaluation would
    >>> >> be more of a concern here than for something like switch().
    >>> 
    >>> > which is actually a primitive. So it seems that there is at least
    >>> > another way to go than 'dots <- match.call(expand.dots=FALSE)$...'
    >>> 
    >>> > Thanks, H.
    >>> 
    >>> >>
    >>> >> If that can't/won't be fixed, what about fixing the man page so it's
    >>> >> in sync with the current behavior?
    >>> >>
    >>> >> Thanks, H.
    >>> 
    >>> Being back from vacations,...
    >>> I agree that something should be done here, if not to the code than at
    >>> least to the man page.
    >>> 
    >>> For now, I'd like to look a bit longer into a possible change to the function.
    >>> Peter mentioned a NSE way to fix the problem and you mentioned switch().
    >>> 
    >>> Originally, stopifnot() was only a few lines of code and meant to be
    >>> "self-explaining" by just reading its definition, and I really would like
    >>> to not walk too much away from that original idea.
    >>> How did you (Herve) think to use  switch()  here?
    >>> 
    >>> 
    >>> 
    >>> >> On 05/03/2017 02:26 AM, peter dalgaard wrote:
    >>> >>> The first line of stopifnot is
    >>> >>>
    >>> >>> n <- length(ll <- list(...))
    >>> >>>
    >>> >>> which takes ALL arguments and forms a list of them. This implies
    >>> >>> evaluation, so explains the effect that you see.
    >>> >>>
    >>> >>> To do it differently, you would have to do something like
    >>> >>>
    >>> >>> dots <- match.call(expand.dots=FALSE)$...
    >>> >>>
    >>> >>> and then explicitly evaluate each argument in turn in the caller
    >>> >>> frame. This amount of nonstandard evaluation sounds like it would
    >>> >>> incur a performance penalty, which could be undesirable.
    >>> >>>
    >>> >>> If you want to enforce the order of evaluation, there is always
    >>> >>>
    >>> >>> stopifnot(A) stopifnot(B)
    >>> >>>
    >>> >>> -pd
    >>> >>>
    >>> >>>> On 3 May 2017, at 02:50 , Herv? Pag?s <hpages at fredhutch.org>
    >>> >>>> wrote:
    >>> >>>>
    >>> >>>> Hi,
    >>> >>>>
    >>> >>>> It's surprising that stopifnot() keeps evaluating its arguments
    >>> >>>> after it reaches the first one that is not TRUE:
    >>> >>>>
    >>> >>>> > stopifnot(3 == 5, as.integer(2^32), a <- 12) Error: 3 == 5 is
    >>> >>>> not TRUE In addition: Warning message: In stopifnot(3 == 5,
    >>> >>>> as.integer(2^32), a <- 12) : NAs introduced by coercion to integer
    >>> >>>> range > a [1] 12
    >>> >>>>
    >>> >>>> The details section in its man page actually suggests that it
    >>> >>>> should stop at the first non-TRUE argument:
    >>> >>>>
    >>> >>>> ?stopifnot(A, B)? is conceptually equivalent to
    >>> >>>>
    >>> >>>> { if(any(is.na(A)) || !all(A)) stop(...); if(any(is.na(B)) ||
    >>> >>>> !all(B)) stop(...) }
    >>> >>>>
    >>> >>>> Best, H.
    >>> >>>>
    >>> >>>> --
    >>> >>>> Herv? Pag?s
    >>> >>>>
    >>> >>>> Program in Computational Biology Division of Public Health
    >>> >>>> Sciences Fred Hutchinson Cancer Research Center 1100 Fairview
    >>> >>>> Ave. N, M1-B514 P.O. Box 19024 Seattle, WA 98109-1024
    >>> >>>>
    >>> >>>> E-mail: hpages at fredhutch.org Phone: (206) 667-5791 Fax: (206)
    >>> >>>> 667-1319
    >>> >>>>
    >>> >>>> ______________________________________________
    >>> >>>> R-devel at r-project.org mailing list
    >>> >>>> 
    >>> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Ddevel&d=DwIFaQ&c=eRAMFD45gAfqt84VtBcfhQ&r=BK7q3XeAvimeWdGbWY_wJYbW0WYiZvSXAJJKaaPhzWA&m=JwgKhKD2k-9Kedeh6pqu-A8x6UEV0INrcxcSGVGo3Tg&s=f7IKJIhpRNJMC3rZAkuI6-MTdL3GAKSV2wK0boFN5HY&e=
    >>> >>>>
    >>> >>>
    >>> >>
    >>> 
    >>> > -- Herv? Pag?s
    >>> 
    >>> > Program in Computational Biology Division of Public Health Sciences
    >>> > Fred Hutchinson Cancer Research Center 1100 Fairview Ave. N,
    >>> > M1-B514 P.O. Box 19024 Seattle, WA 98109-1024
    >>> 
    >>> > E-mail: hpages at fredhutch.org Phone: (206) 667-5791 Fax: (206)
    >>> > 667-1319
    >>> 
    >>> > ______________________________________________
    >>> > R-devel at r-project.org mailing list
    >>> > https://stat.ethz.ch/mailman/listinfo/r-devel
    >>> 
    >>> ______________________________________________
    >>> R-devel at r-project.org mailing list
    >>> https://stat.ethz.ch/mailman/listinfo/r-devel
    >> 

    > ______________________________________________
    > R-devel at r-project.org mailing list
    > https://stat.ethz.ch/mailman/listinfo/r-devel


From pdalgd at gmail.com  Mon May 15 16:28:42 2017
From: pdalgd at gmail.com (peter dalgaard)
Date: Mon, 15 May 2017 16:28:42 +0200
Subject: [Rd] stopifnot() does not stop at first non-TRUE argument
In-Reply-To: <22809.44830.153073.675527@stat.math.ethz.ch>
References: <64acdaeb-2857-9017-68c9-9da08e29a617@fredhutch.org>
 <E5DADF15-58C6-4F73-BC56-C6404896982F@gmail.com>
 <c2b3cb22-90bd-5372-e8b9-d1ea8b2ab828@fredhutch.org>
 <8945f7e4-99ce-53fa-46ee-cce64078a6a8@fredhutch.org>
 <22809.26921.473223.395550@stat.math.ethz.ch>
 <8fdb66b1-f785-ed55-6e40-a4ef8138e637@insa-toulouse.fr>
 <26bd9210-e40c-0967-7abb-5f724b09c034@insa-toulouse.fr>
 <22809.44830.153073.675527@stat.math.ethz.ch>
Message-ID: <703806C3-8EC7-46D7-BE92-46C61B1BDE13@gmail.com>

I think Herv?'s idea was just that if switch can evaluate arguments selectively, so can stopifnot(). But switch() is .Primitive, so does it from C. 

I think it is almost a no-brainer to implement a sequential stopifnot if dropping to C code is allowed. In R it gets trickier, but how about this:

Stopifnot <- function(...)
{
  n <- length(match.call()) - 1
  for (i in 1:n)
  {
    nm <- as.name(paste0("..",i))
    if (!eval(nm)) stop("not all true")
  }
}
Stopifnot(2+2==4)
Stopifnot(2+2==5, print("Hey!!!") == "Hey!!!")
Stopifnot(2+2==4, print("Hey!!!") == "Hey!!!")
Stopifnot(T,T,T,T,T,T,T,T,T,T,T,T,T,T,T,T,F,T)


> On 15 May 2017, at 15:37 , Martin Maechler <maechler at stat.math.ethz.ch> wrote:
> 
> I'm still curious about Herv?'s idea on using  switch()  for the
> issue.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From sokol at insa-toulouse.fr  Mon May 15 16:32:20 2017
From: sokol at insa-toulouse.fr (Serguei Sokol)
Date: Mon, 15 May 2017 16:32:20 +0200
Subject: [Rd] stopifnot() does not stop at first non-TRUE argument
In-Reply-To: <22809.44830.153073.675527@stat.math.ethz.ch>
References: <64acdaeb-2857-9017-68c9-9da08e29a617@fredhutch.org>
 <E5DADF15-58C6-4F73-BC56-C6404896982F@gmail.com>
 <c2b3cb22-90bd-5372-e8b9-d1ea8b2ab828@fredhutch.org>
 <8945f7e4-99ce-53fa-46ee-cce64078a6a8@fredhutch.org>
 <22809.26921.473223.395550@stat.math.ethz.ch>
 <8fdb66b1-f785-ed55-6e40-a4ef8138e637@insa-toulouse.fr>
 <26bd9210-e40c-0967-7abb-5f724b09c034@insa-toulouse.fr>
 <22809.44830.153073.675527@stat.math.ethz.ch>
Message-ID: <477f9e1a-4f7c-a902-e744-da4a36eb5a4f@insa-toulouse.fr>

Le 15/05/2017 ? 15:37, Martin Maechler a ?crit :
>>>>>> Serguei Sokol <sokol at insa-toulouse.fr>
>>>>>>      on Mon, 15 May 2017 13:14:34 +0200 writes:
>      > I see in the archives that the attachment cannot pass.
>      > So, here is the code:
>
>      [....... MM: I needed to reformat etc to match closely to
>       the current source code which is in
>       	 https://svn.r-project.org/R/trunk/src/library/base/R/stop.R
>       or its corresponding github mirror
>          https://github.com/wch/r-source/blob/trunk/src/library/base/R/stop.R
>      ]
>
>      > Best,
>      > Serguei.
>
> Yes, something like that seems even simpler than Peter's
> suggestion...
>
> It currently breaks 'make check' in the R sources,
> specifically in tests/reg-tests-2.R (lines 6574 ff),
> the new code now gives
>
>    > ## error messages from (C-level) evalList
>    > tst <- function(y) { stopifnot(is.numeric(y)); y+ 1 }
>    > try(tst())
>    Error in eval(cl.i, pfr) : argument "y" is missing, with no default
>
> whereas previously it gave
>
>    Error in stopifnot(is.numeric(y)) :
>       argument "y" is missing, with no default
>
>
> But I think that change (of call stack in such an error case) is
> unavoidable and not a big problem.
It can be avoided but at price of customizing error() and warning() calls with something like:
wrn <- function(w) {w$call <- cl.i; warning(w)}
err <- function(e) {e$call <- cl.i; stop(e)}
...
tryCatch(r <- eval(cl.i, pfr), warning=wrn, error=err)

Serguei.

>
> --
>
> I'm still curious about Herv?'s idea on using  switch()  for the
> issue.
>
> Martin
>
>
>      > Le 15/05/2017 ? 12:48, Serguei Sokol a ?crit :
>      >> Hello,
>      >>
>      >> I am a new on this list, so I introduce myself very briefly:
>      >> my background is applied mathematics, more precisely scientific calculus
>      >> applied for modeling metabolic systems, I am author/maintainer of
>      >> few packages (Deriv, rmumps, arrApply).
>      >>
>      >> Now, on the subject of this discussion, I must say that I don't really understand
>      >> Peter's argument:
>      >>
>      >> >>> To do it differently, you would have to do something like
>      >> >>>
>      >> >>> dots <- match.call(expand.dots=FALSE)$...
>      >> >>>
>      >> >>> and then explicitly evaluate each argument in turn in the caller
>      >> >>> frame. This amount of nonstandard evaluation sounds like it would
>      >> >>> incur a performance penalty, which could be undesirable.
>      >> The first line of the current stopifnot()
>      >> n <- length(ll <- list(...))
>      >> already evaluates _all_ of the arguments
>      >> in the caller frame. So to do the same only
>      >> on a part of them (till the first FALSE or NA occurs)
>      >> cannot be more penalizing than the current version, right?
>      >>
>      >> I attach here a slightly modified version called stopifnot_new()
>      >> which works in accordance with the man page and
>      >> where there are only two additional calls: parent.frame() and eval().
>      >> I don't think it can be considered as real performance penalty
>      >> as the same or bigger amount of (implicit) evaluations was
>      >> already done in the current version:
>      >>
>      >>> source("stopifnot_new.R")
>      >>> stopifnot_new(3 == 5, as.integer(2^32), a <- 12)
>      >> Error: 3 == 5 is not TRUE
>      >>> a
>      >> Error: object 'a' not found
>      >>
>      >> Best,
>      >> Serguei.
>      >>
>      >>
>      >> Le 15/05/2017 ? 10:39, Martin Maechler a ?crit :
>      >>>>>>>> Herv? Pag?s <hpages at fredhutch.org>
>      >>>>>>>> on Wed, 3 May 2017 12:08:26 -0700 writes:
>      >>> > On 05/03/2017 12:04 PM, Herv? Pag?s wrote:
>      >>> >> Not sure why the performance penalty of nonstandard evaluation would
>      >>> >> be more of a concern here than for something like switch().
>      >>>
>      >>> > which is actually a primitive. So it seems that there is at least
>      >>> > another way to go than 'dots <- match.call(expand.dots=FALSE)$...'
>      >>>
>      >>> > Thanks, H.
>      >>>
>      >>> >>
>      >>> >> If that can't/won't be fixed, what about fixing the man page so it's
>      >>> >> in sync with the current behavior?
>      >>> >>
>      >>> >> Thanks, H.
>      >>>
>      >>> Being back from vacations,...
>      >>> I agree that something should be done here, if not to the code than at
>      >>> least to the man page.
>      >>>
>      >>> For now, I'd like to look a bit longer into a possible change to the function.
>      >>> Peter mentioned a NSE way to fix the problem and you mentioned switch().
>      >>>
>      >>> Originally, stopifnot() was only a few lines of code and meant to be
>      >>> "self-explaining" by just reading its definition, and I really would like
>      >>> to not walk too much away from that original idea.
>      >>> How did you (Herve) think to use  switch()  here?
>      >>>
>      >>>
>      >>>
>      >>> >> On 05/03/2017 02:26 AM, peter dalgaard wrote:
>      >>> >>> The first line of stopifnot is
>      >>> >>>
>      >>> >>> n <- length(ll <- list(...))
>      >>> >>>
>      >>> >>> which takes ALL arguments and forms a list of them. This implies
>      >>> >>> evaluation, so explains the effect that you see.
>      >>> >>>
>      >>> >>> To do it differently, you would have to do something like
>      >>> >>>
>      >>> >>> dots <- match.call(expand.dots=FALSE)$...
>      >>> >>>
>      >>> >>> and then explicitly evaluate each argument in turn in the caller
>      >>> >>> frame. This amount of nonstandard evaluation sounds like it would
>      >>> >>> incur a performance penalty, which could be undesirable.
>      >>> >>>
>      >>> >>> If you want to enforce the order of evaluation, there is always
>      >>> >>>
>      >>> >>> stopifnot(A) stopifnot(B)
>      >>> >>>
>      >>> >>> -pd
>      >>> >>>
>      >>> >>>> On 3 May 2017, at 02:50 , Herv? Pag?s <hpages at fredhutch.org>
>      >>> >>>> wrote:
>      >>> >>>>
>      >>> >>>> Hi,
>      >>> >>>>
>      >>> >>>> It's surprising that stopifnot() keeps evaluating its arguments
>      >>> >>>> after it reaches the first one that is not TRUE:
>      >>> >>>>
>      >>> >>>> > stopifnot(3 == 5, as.integer(2^32), a <- 12) Error: 3 == 5 is
>      >>> >>>> not TRUE In addition: Warning message: In stopifnot(3 == 5,
>      >>> >>>> as.integer(2^32), a <- 12) : NAs introduced by coercion to integer
>      >>> >>>> range > a [1] 12
>      >>> >>>>
>      >>> >>>> The details section in its man page actually suggests that it
>      >>> >>>> should stop at the first non-TRUE argument:
>      >>> >>>>
>      >>> >>>> ?stopifnot(A, B)? is conceptually equivalent to
>      >>> >>>>
>      >>> >>>> { if(any(is.na(A)) || !all(A)) stop(...); if(any(is.na(B)) ||
>      >>> >>>> !all(B)) stop(...) }
>      >>> >>>>
>      >>> >>>> Best, H.
>      >>> >>>>
>      >>> >>>> --
>      >>> >>>> Herv? Pag?s
>      >>> >>>>
>      >>> >>>> Program in Computational Biology Division of Public Health
>      >>> >>>> Sciences Fred Hutchinson Cancer Research Center 1100 Fairview
>      >>> >>>> Ave. N, M1-B514 P.O. Box 19024 Seattle, WA 98109-1024
>      >>> >>>>
>      >>> >>>> E-mail: hpages at fredhutch.org Phone: (206) 667-5791 Fax: (206)
>      >>> >>>> 667-1319
>      >>> >>>>
>      >>> >>>> ______________________________________________
>      >>> >>>> R-devel at r-project.org mailing list
>      >>> >>>>
>      >>> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Ddevel&d=DwIFaQ&c=eRAMFD45gAfqt84VtBcfhQ&r=BK7q3XeAvimeWdGbWY_wJYbW0WYiZvSXAJJKaaPhzWA&m=JwgKhKD2k-9Kedeh6pqu-A8x6UEV0INrcxcSGVGo3Tg&s=f7IKJIhpRNJMC3rZAkuI6-MTdL3GAKSV2wK0boFN5HY&e=
>      >>> >>>>
>      >>> >>>
>      >>> >>
>      >>>
>      >>> > -- Herv? Pag?s
>      >>>
>      >>> > Program in Computational Biology Division of Public Health Sciences
>      >>> > Fred Hutchinson Cancer Research Center 1100 Fairview Ave. N,
>      >>> > M1-B514 P.O. Box 19024 Seattle, WA 98109-1024
>      >>>
>      >>> > E-mail: hpages at fredhutch.org Phone: (206) 667-5791 Fax: (206)
>      >>> > 667-1319
>      >>>
>      >>> > ______________________________________________
>      >>> > R-devel at r-project.org mailing list
>      >>> > https://stat.ethz.ch/mailman/listinfo/r-devel
>      >>>
>      >>> ______________________________________________
>      >>> R-devel at r-project.org mailing list
>      >>> https://stat.ethz.ch/mailman/listinfo/r-devel
>      >>
>
>      > ______________________________________________
>      > R-devel at r-project.org mailing list
>      > https://stat.ethz.ch/mailman/listinfo/r-devel
>


From maechler at stat.math.ethz.ch  Mon May 15 17:04:40 2017
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 15 May 2017 17:04:40 +0200
Subject: [Rd] stopifnot() does not stop at first non-TRUE argument
In-Reply-To: <703806C3-8EC7-46D7-BE92-46C61B1BDE13@gmail.com>
References: <64acdaeb-2857-9017-68c9-9da08e29a617@fredhutch.org>
 <E5DADF15-58C6-4F73-BC56-C6404896982F@gmail.com>
 <c2b3cb22-90bd-5372-e8b9-d1ea8b2ab828@fredhutch.org>
 <8945f7e4-99ce-53fa-46ee-cce64078a6a8@fredhutch.org>
 <22809.26921.473223.395550@stat.math.ethz.ch>
 <8fdb66b1-f785-ed55-6e40-a4ef8138e637@insa-toulouse.fr>
 <26bd9210-e40c-0967-7abb-5f724b09c034@insa-toulouse.fr>
 <22809.44830.153073.675527@stat.math.ethz.ch>
 <703806C3-8EC7-46D7-BE92-46C61B1BDE13@gmail.com>
Message-ID: <22809.50056.401373.410919@stat.math.ethz.ch>

>>>>> peter dalgaard <pdalgd at gmail.com>
>>>>>     on Mon, 15 May 2017 16:28:42 +0200 writes:

    > I think Herv?'s idea was just that if switch can evaluate arguments selectively, so can stopifnot(). But switch() is .Primitive, so does it from C. 

if he just meant that, then "yes, of course" (but not so interesting).

    > I think it is almost a no-brainer to implement a sequential stopifnot if dropping to C code is allowed. In R it gets trickier, but how about this:

Something like this, yes, that's close to what Serguei Sokol had proposed
(and of course I *do*  want to keep the current sophistication
 of stopifnot(), so this is really too simple)

    > Stopifnot <- function(...)
    > {
    > n <- length(match.call()) - 1
    > for (i in 1:n)
    > {
    > nm <- as.name(paste0("..",i))
    > if (!eval(nm)) stop("not all true")
    > }
    > }
    > Stopifnot(2+2==4)
    > Stopifnot(2+2==5, print("Hey!!!") == "Hey!!!")
    > Stopifnot(2+2==4, print("Hey!!!") == "Hey!!!")
    > Stopifnot(T,T,T,T,T,T,T,T,T,T,T,T,T,T,T,T,F,T)


    >> On 15 May 2017, at 15:37 , Martin Maechler <maechler at stat.math.ethz.ch> wrote:
    >> 
    >> I'm still curious about Herv?'s idea on using  switch()  for the
    >> issue.

    > -- 
    > Peter Dalgaard, Professor,
    > Center for Statistics, Copenhagen Business School
    > Solbjerg Plads 3, 2000 Frederiksberg, Denmark
    > Phone: (+45)38153501
    > Office: A 4.23
    > Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From pdalgd at gmail.com  Mon May 15 17:34:48 2017
From: pdalgd at gmail.com (peter dalgaard)
Date: Mon, 15 May 2017 17:34:48 +0200
Subject: [Rd] stopifnot() does not stop at first non-TRUE argument
In-Reply-To: <22809.50056.401373.410919@stat.math.ethz.ch>
References: <64acdaeb-2857-9017-68c9-9da08e29a617@fredhutch.org>
 <E5DADF15-58C6-4F73-BC56-C6404896982F@gmail.com>
 <c2b3cb22-90bd-5372-e8b9-d1ea8b2ab828@fredhutch.org>
 <8945f7e4-99ce-53fa-46ee-cce64078a6a8@fredhutch.org>
 <22809.26921.473223.395550@stat.math.ethz.ch>
 <8fdb66b1-f785-ed55-6e40-a4ef8138e637@insa-toulouse.fr>
 <26bd9210-e40c-0967-7abb-5f724b09c034@insa-toulouse.fr>
 <22809.44830.153073.675527@stat.math.ethz.ch>
 <703806C3-8EC7-46D7-BE92-46C61B1BDE13@gmail.com>
 <22809.50056.401373.410919@stat.math.ethz.ch>
Message-ID: <7BE7067A-1278-486B-879D-5EB67D83DE2C@gmail.com>

However, it doesn't look much of a hassle to fuse my suggestion into the current stopifnot: Basically, just use eval(as.name(paste0("..",i))) instead of ll[[i]] and base the initial calculation of n on match.call() rather than on list(...).

-pd


> On 15 May 2017, at 17:04 , Martin Maechler <maechler at stat.math.ethz.ch> wrote:
> 
>>>>>> peter dalgaard <pdalgd at gmail.com>
>>>>>>    on Mon, 15 May 2017 16:28:42 +0200 writes:
> 
>> I think Herv?'s idea was just that if switch can evaluate arguments selectively, so can stopifnot(). But switch() is .Primitive, so does it from C. 
> 
> if he just meant that, then "yes, of course" (but not so interesting).
> 
>> I think it is almost a no-brainer to implement a sequential stopifnot if dropping to C code is allowed. In R it gets trickier, but how about this:
> 
> Something like this, yes, that's close to what Serguei Sokol had proposed
> (and of course I *do*  want to keep the current sophistication
> of stopifnot(), so this is really too simple)
> 
>> Stopifnot <- function(...)
>> {
>> n <- length(match.call()) - 1
>> for (i in 1:n)
>> {
>> nm <- as.name(paste0("..",i))
>> if (!eval(nm)) stop("not all true")
>> }
>> }
>> Stopifnot(2+2==4)
>> Stopifnot(2+2==5, print("Hey!!!") == "Hey!!!")
>> Stopifnot(2+2==4, print("Hey!!!") == "Hey!!!")
>> Stopifnot(T,T,T,T,T,T,T,T,T,T,T,T,T,T,T,T,F,T)
> 
> 
>>> On 15 May 2017, at 15:37 , Martin Maechler <maechler at stat.math.ethz.ch> wrote:
>>> 
>>> I'm still curious about Herv?'s idea on using  switch()  for the
>>> issue.
> 
>> -- 
>> Peter Dalgaard, Professor,
>> Center for Statistics, Copenhagen Business School
>> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
>> Phone: (+45)38153501
>> Office: A 4.23
>> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> 
> 
> 
> 
> 
> 
> 
> 

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From maechler at stat.math.ethz.ch  Mon May 15 17:44:06 2017
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 15 May 2017 17:44:06 +0200
Subject: [Rd] stopifnot() does not stop at first non-TRUE argument
In-Reply-To: <477f9e1a-4f7c-a902-e744-da4a36eb5a4f@insa-toulouse.fr>
References: <64acdaeb-2857-9017-68c9-9da08e29a617@fredhutch.org>
 <E5DADF15-58C6-4F73-BC56-C6404896982F@gmail.com>
 <c2b3cb22-90bd-5372-e8b9-d1ea8b2ab828@fredhutch.org>
 <8945f7e4-99ce-53fa-46ee-cce64078a6a8@fredhutch.org>
 <22809.26921.473223.395550@stat.math.ethz.ch>
 <8fdb66b1-f785-ed55-6e40-a4ef8138e637@insa-toulouse.fr>
 <26bd9210-e40c-0967-7abb-5f724b09c034@insa-toulouse.fr>
 <22809.44830.153073.675527@stat.math.ethz.ch>
 <477f9e1a-4f7c-a902-e744-da4a36eb5a4f@insa-toulouse.fr>
Message-ID: <22809.52422.219712.66882@stat.math.ethz.ch>

>>>>> Serguei Sokol <sokol at insa-toulouse.fr>
>>>>>     on Mon, 15 May 2017 16:32:20 +0200 writes:

    > Le 15/05/2017 ? 15:37, Martin Maechler a ?crit :
    >>>>>>> Serguei Sokol <sokol at insa-toulouse.fr>
    >>>>>>> on Mon, 15 May 2017 13:14:34 +0200 writes:
    >> > I see in the archives that the attachment cannot pass.
    >> > So, here is the code:
    >> 
    >> [....... MM: I needed to reformat etc to match closely to
    >> the current source code which is in
    >> https://svn.r-project.org/R/trunk/src/library/base/R/stop.R
    >> or its corresponding github mirror
    >> https://github.com/wch/r-source/blob/trunk/src/library/base/R/stop.R
    >> ]
    >> 
    >> > Best,
    >> > Serguei.
    >> 
    >> Yes, something like that seems even simpler than Peter's
    >> suggestion...
    >> 
    >> It currently breaks 'make check' in the R sources,
    >> specifically in tests/reg-tests-2.R (lines 6574 ff),
    >> the new code now gives
    >> 
    >> > ## error messages from (C-level) evalList
    >> > tst <- function(y) { stopifnot(is.numeric(y)); y+ 1 }
    >> > try(tst())
    >> Error in eval(cl.i, pfr) : argument "y" is missing, with no default
    >> 
    >> whereas previously it gave
    >> 
    >> Error in stopifnot(is.numeric(y)) :
    >> argument "y" is missing, with no default
    >> 
    >> 
    >> But I think that change (of call stack in such an error case) is
    >> unavoidable and not a big problem.

    > It can be avoided but at price of customizing error() and warning() calls with something like:
    > wrn <- function(w) {w$call <- cl.i; warning(w)}
    > err <- function(e) {e$call <- cl.i; stop(e)}
    > ...
    > tryCatch(r <- eval(cl.i, pfr), warning=wrn, error=err)

    > Serguei.

Well, a good idea, but the 'warning' case is more complicated
(and the above incorrect): I do want the warning there, but
_not_ return the warning, but rather, the result of eval() :
So this needs even more sophistication, using  withCallingHandlers(.)
and maybe that really get's too sophisticated and no
more "readable" to 99.9% of the R users ... ?

I now do append my current version -- in case some may want to
comment or improve further.

Martin

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: stopifnot_new.R.~3~
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20170515/4d595547/attachment.ksh>

From sbibauw at gmail.com  Mon May 15 17:59:32 2017
From: sbibauw at gmail.com (Serge Bibauw)
Date: Mon, 15 May 2017 11:59:32 -0400
Subject: [Rd] =?utf-8?q?=5Bbug=5D_droplevels=28=29_also_drop_object_attrib?=
 =?utf-8?b?dXRlcyAoY29tbWVudOKApik=?=
Message-ID: <f72658b5-4f45-4936-afb9-a4b4b78dc9a1@Spark>

Hi,

Just reporting a small bug? not really a big deal, but I don?t think that is intended: droplevels()?also drops all object?s attributes.

Example:

> > test <- c("hello", "something", "hi")
> > test <- factor(test)
> > comment(test) <- "this is a test"
> > attr(test, "description") <- "this is another test"
> > attributes(test)
> $levels
> [1] "hello" ? ? "hi" ? ? ? ?"something"
>
> $class
> [1] "factor"
>
> $comment
> [1] "this is a test"
>
> $description
> [1] "this is another test"
>
> > test <- droplevels(test)
> > attributes(test)
> $levels
> [1] "hello" ? ? "hi" ? ? ? ?"something"
>
> $class
> [1] "factor"


Serge

	[[alternative HTML version deleted]]


From sokol at insa-toulouse.fr  Mon May 15 19:24:53 2017
From: sokol at insa-toulouse.fr (Serguei Sokol)
Date: Mon, 15 May 2017 19:24:53 +0200
Subject: [Rd] stopifnot() does not stop at first non-TRUE argument
In-Reply-To: <22809.52422.219712.66882@stat.math.ethz.ch>
References: <64acdaeb-2857-9017-68c9-9da08e29a617@fredhutch.org>
 <E5DADF15-58C6-4F73-BC56-C6404896982F@gmail.com>
 <c2b3cb22-90bd-5372-e8b9-d1ea8b2ab828@fredhutch.org>
 <8945f7e4-99ce-53fa-46ee-cce64078a6a8@fredhutch.org>
 <22809.26921.473223.395550@stat.math.ethz.ch>
 <8fdb66b1-f785-ed55-6e40-a4ef8138e637@insa-toulouse.fr>
 <26bd9210-e40c-0967-7abb-5f724b09c034@insa-toulouse.fr>
 <22809.44830.153073.675527@stat.math.ethz.ch>
 <477f9e1a-4f7c-a902-e744-da4a36eb5a4f@insa-toulouse.fr>
 <22809.52422.219712.66882@stat.math.ethz.ch>
Message-ID: <d7076a2d-9140-f58c-4613-b1ef60e3b015@insa-toulouse.fr>

Le 15/05/2017 ? 17:44, Martin Maechler a ?crit :
...
> So this needs even more sophistication, using withCallingHandlers(.)
> and maybe that really get's too sophisticated and no
> more "readable" to 99.9% of the R users ... ?
I'd say the current version is of minimal sophistication to reach
both the doc and test requirements.

Serguei.


From luke-tierney at uiowa.edu  Mon May 15 19:41:13 2017
From: luke-tierney at uiowa.edu (luke-tierney at uiowa.edu)
Date: Mon, 15 May 2017 12:41:13 -0500 (CDT)
Subject: [Rd] stopifnot() does not stop at first non-TRUE argument
In-Reply-To: <22809.52422.219712.66882@stat.math.ethz.ch>
References: <64acdaeb-2857-9017-68c9-9da08e29a617@fredhutch.org>
 <E5DADF15-58C6-4F73-BC56-C6404896982F@gmail.com>
 <c2b3cb22-90bd-5372-e8b9-d1ea8b2ab828@fredhutch.org>
 <8945f7e4-99ce-53fa-46ee-cce64078a6a8@fredhutch.org>
 <22809.26921.473223.395550@stat.math.ethz.ch>
 <8fdb66b1-f785-ed55-6e40-a4ef8138e637@insa-toulouse.fr>
 <26bd9210-e40c-0967-7abb-5f724b09c034@insa-toulouse.fr>
 <22809.44830.153073.675527@stat.math.ethz.ch>
 <477f9e1a-4f7c-a902-e744-da4a36eb5a4f@insa-toulouse.fr>
 <22809.52422.219712.66882@stat.math.ethz.ch>
Message-ID: <alpine.LFD.2.20.1705151236510.27291@itasca.stat.uiowa.edu>

This is getting pretty convoluted.

The current behavior is consistent with the description at the top of
the help page -- it does not promise to stop evaluation once the first
non-TRUE is found.  That seems OK to me -- if you want sequencing you
can use

stopifnot(A)
stopifnot(B)

or

stopifnot(A && B)

I could see an argument for a change that in the multiple argumetn
case reports _all_ that fail; that would seem more useful to me than
twisting the code into knots.

Best,

luke

On Mon, 15 May 2017, Martin Maechler wrote:

>>>>>> Serguei Sokol <sokol at insa-toulouse.fr>
>>>>>>     on Mon, 15 May 2017 16:32:20 +0200 writes:
>
>    > Le 15/05/2017 ? 15:37, Martin Maechler a ?crit :
>    >>>>>>> Serguei Sokol <sokol at insa-toulouse.fr>
>    >>>>>>> on Mon, 15 May 2017 13:14:34 +0200 writes:
>    >> > I see in the archives that the attachment cannot pass.
>    >> > So, here is the code:
>    >>
>    >> [....... MM: I needed to reformat etc to match closely to
>    >> the current source code which is in
>    >> https://svn.r-project.org/R/trunk/src/library/base/R/stop.R
>    >> or its corresponding github mirror
>    >> https://github.com/wch/r-source/blob/trunk/src/library/base/R/stop.R
>    >> ]
>    >>
>    >> > Best,
>    >> > Serguei.
>    >>
>    >> Yes, something like that seems even simpler than Peter's
>    >> suggestion...
>    >>
>    >> It currently breaks 'make check' in the R sources,
>    >> specifically in tests/reg-tests-2.R (lines 6574 ff),
>    >> the new code now gives
>    >>
>    >> > ## error messages from (C-level) evalList
>    >> > tst <- function(y) { stopifnot(is.numeric(y)); y+ 1 }
>    >> > try(tst())
>    >> Error in eval(cl.i, pfr) : argument "y" is missing, with no default
>    >>
>    >> whereas previously it gave
>    >>
>    >> Error in stopifnot(is.numeric(y)) :
>    >> argument "y" is missing, with no default
>    >>
>    >>
>    >> But I think that change (of call stack in such an error case) is
>    >> unavoidable and not a big problem.
>
>    > It can be avoided but at price of customizing error() and warning() calls with something like:
>    > wrn <- function(w) {w$call <- cl.i; warning(w)}
>    > err <- function(e) {e$call <- cl.i; stop(e)}
>    > ...
>    > tryCatch(r <- eval(cl.i, pfr), warning=wrn, error=err)
>
>    > Serguei.
>
> Well, a good idea, but the 'warning' case is more complicated
> (and the above incorrect): I do want the warning there, but
> _not_ return the warning, but rather, the result of eval() :
> So this needs even more sophistication, using  withCallingHandlers(.)
> and maybe that really get's too sophisticated and no
> more "readable" to 99.9% of the R users ... ?
>
> I now do append my current version -- in case some may want to
> comment or improve further.
>
> Martin
>
>

-- 
Luke Tierney
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu

From hpages at fredhutch.org  Tue May 16 01:54:46 2017
From: hpages at fredhutch.org (=?UTF-8?B?SGVydsOpIFBhZ8Oocw==?=)
Date: Mon, 15 May 2017 16:54:46 -0700
Subject: [Rd] stopifnot() does not stop at first non-TRUE argument
In-Reply-To: <alpine.LFD.2.20.1705151236510.27291@itasca.stat.uiowa.edu>
References: <64acdaeb-2857-9017-68c9-9da08e29a617@fredhutch.org>
 <E5DADF15-58C6-4F73-BC56-C6404896982F@gmail.com>
 <c2b3cb22-90bd-5372-e8b9-d1ea8b2ab828@fredhutch.org>
 <8945f7e4-99ce-53fa-46ee-cce64078a6a8@fredhutch.org>
 <22809.26921.473223.395550@stat.math.ethz.ch>
 <8fdb66b1-f785-ed55-6e40-a4ef8138e637@insa-toulouse.fr>
 <26bd9210-e40c-0967-7abb-5f724b09c034@insa-toulouse.fr>
 <22809.44830.153073.675527@stat.math.ethz.ch>
 <477f9e1a-4f7c-a902-e744-da4a36eb5a4f@insa-toulouse.fr>
 <22809.52422.219712.66882@stat.math.ethz.ch>
 <alpine.LFD.2.20.1705151236510.27291@itasca.stat.uiowa.edu>
Message-ID: <3d296d7c-6fc0-0175-7bc0-57638cc3998b@fredhutch.org>

Hi,

On 05/15/2017 10:41 AM, luke-tierney at uiowa.edu wrote:
> This is getting pretty convoluted.
>
> The current behavior is consistent with the description at the top of
> the help page -- it does not promise to stop evaluation once the first
> non-TRUE is found.  That seems OK to me -- if you want sequencing you
> can use
>
> stopifnot(A)
> stopifnot(B)
>
> or
>
> stopifnot(A && B)

My main use case for using stopifnot() is argument checking. In that
context, I like the conciseness of

   stopifnot(
     A,
     B,
     ...
   )

I think it's a common use case (and a pretty natural thing to do) to
order/organize the expressions in a way such that it only makes sense
to continue evaluating if all was OK so far e.g.

   stopifnot(
     is.numeric(x),
     length(x) == 1,
     is.na(x)
   )

At least that's how things are organized in the stopifnot() calls that
accumulated in my code over the years. That's because I was convinced
that evaluation would stop at the first non-true expression (as
suggested by the man page). Until recently when I got a warning issued
by an expression located *after* the first non-true expression. This
was pretty unexpected/confusing!

If I can't rely on this "sequencing" feature, I guess I can always
do

   stopifnot(A)
   stopifnot(B)
   ...

but I loose the conciseness of calling stopifnot() only once.
I could also use

   stopifnot(A && B && ...)

but then I loose the conciseness of the error message i.e. it's going
to be something like

   Error: A && B && ... is not TRUE

which can be pretty long/noisy compared to the message that reports
only the 1st error.

Conciseness/readability of the single call to stopifnot() and
conciseness of the error message are the features that made me
adopt stopifnot() in the 1st place. If stopifnot() cannot be revisited
to do "sequencing" then that means I will need to revisit all my calls
to stopifnot().

>
> I could see an argument for a change that in the multiple argumetn
> case reports _all_ that fail; that would seem more useful to me than
> twisting the code into knots.

Why not. Still better than the current situation. But only if that
semantic seems more useful to people. Would be sad if usefulness
of one semantic or the other was decided based on trickiness of
implementation.

Thanks,
H.

>
> Best,
>
> luke
>
> On Mon, 15 May 2017, Martin Maechler wrote:
>
>>>>>>> Serguei Sokol <sokol at insa-toulouse.fr>
>>>>>>>     on Mon, 15 May 2017 16:32:20 +0200 writes:
>>
>>    > Le 15/05/2017 ? 15:37, Martin Maechler a ?crit :
>>    >>>>>>> Serguei Sokol <sokol at insa-toulouse.fr>
>>    >>>>>>> on Mon, 15 May 2017 13:14:34 +0200 writes:
>>    >> > I see in the archives that the attachment cannot pass.
>>    >> > So, here is the code:
>>    >>
>>    >> [....... MM: I needed to reformat etc to match closely to
>>    >> the current source code which is in
>>    >>
>> https://urldefense.proofpoint.com/v2/url?u=https-3A__svn.r-2Dproject.org_R_trunk_src_library_base_R_stop.R&d=DwIFAw&c=eRAMFD45gAfqt84VtBcfhQ&r=BK7q3XeAvimeWdGbWY_wJYbW0WYiZvSXAJJKaaPhzWA&m=t9fJDOl9YG2zB-GF0wQXrXJTsW2jxTxMHE-qZfLGzHU&s=KGsvpXrXpHCFTdbLM9ci3sBNO9C3ocsgEqHMvZKvV9I&e=
>>    >> or its corresponding github mirror
>>    >>
>> https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_wch_r-2Dsource_blob_trunk_src_library_base_R_stop.R&d=DwIFAw&c=eRAMFD45gAfqt84VtBcfhQ&r=BK7q3XeAvimeWdGbWY_wJYbW0WYiZvSXAJJKaaPhzWA&m=t9fJDOl9YG2zB-GF0wQXrXJTsW2jxTxMHE-qZfLGzHU&s=7Z5bPVWdGPpY2KLnXQP6c-_8s86CpKe0ZYkCfqjfxY0&e=
>>    >> ]
>>    >>
>>    >> > Best,
>>    >> > Serguei.
>>    >>
>>    >> Yes, something like that seems even simpler than Peter's
>>    >> suggestion...
>>    >>
>>    >> It currently breaks 'make check' in the R sources,
>>    >> specifically in tests/reg-tests-2.R (lines 6574 ff),
>>    >> the new code now gives
>>    >>
>>    >> > ## error messages from (C-level) evalList
>>    >> > tst <- function(y) { stopifnot(is.numeric(y)); y+ 1 }
>>    >> > try(tst())
>>    >> Error in eval(cl.i, pfr) : argument "y" is missing, with no default
>>    >>
>>    >> whereas previously it gave
>>    >>
>>    >> Error in stopifnot(is.numeric(y)) :
>>    >> argument "y" is missing, with no default
>>    >>
>>    >>
>>    >> But I think that change (of call stack in such an error case) is
>>    >> unavoidable and not a big problem.
>>
>>    > It can be avoided but at price of customizing error() and
>> warning() calls with something like:
>>    > wrn <- function(w) {w$call <- cl.i; warning(w)}
>>    > err <- function(e) {e$call <- cl.i; stop(e)}
>>    > ...
>>    > tryCatch(r <- eval(cl.i, pfr), warning=wrn, error=err)
>>
>>    > Serguei.
>>
>> Well, a good idea, but the 'warning' case is more complicated
>> (and the above incorrect): I do want the warning there, but
>> _not_ return the warning, but rather, the result of eval() :
>> So this needs even more sophistication, using  withCallingHandlers(.)
>> and maybe that really get's too sophisticated and no
>> more "readable" to 99.9% of the R users ... ?
>>
>> I now do append my current version -- in case some may want to
>> comment or improve further.
>>
>> Martin
>>
>>
>

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fredhutch.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From hpages at fredhutch.org  Tue May 16 01:57:05 2017
From: hpages at fredhutch.org (=?UTF-8?B?SGVydsOpIFBhZ8Oocw==?=)
Date: Mon, 15 May 2017 16:57:05 -0700
Subject: [Rd] stopifnot() does not stop at first non-TRUE argument
In-Reply-To: <703806C3-8EC7-46D7-BE92-46C61B1BDE13@gmail.com>
References: <64acdaeb-2857-9017-68c9-9da08e29a617@fredhutch.org>
 <E5DADF15-58C6-4F73-BC56-C6404896982F@gmail.com>
 <c2b3cb22-90bd-5372-e8b9-d1ea8b2ab828@fredhutch.org>
 <8945f7e4-99ce-53fa-46ee-cce64078a6a8@fredhutch.org>
 <22809.26921.473223.395550@stat.math.ethz.ch>
 <8fdb66b1-f785-ed55-6e40-a4ef8138e637@insa-toulouse.fr>
 <26bd9210-e40c-0967-7abb-5f724b09c034@insa-toulouse.fr>
 <22809.44830.153073.675527@stat.math.ethz.ch>
 <703806C3-8EC7-46D7-BE92-46C61B1BDE13@gmail.com>
Message-ID: <8ed5a1c1-4f11-3305-17b1-3326b99e9480@fredhutch.org>

On 05/15/2017 07:28 AM, peter dalgaard wrote:
> I think Herv?'s idea was just that if switch can evaluate arguments selectively, so can stopifnot().

Yep.

Thanks,
H.

> But switch() is .Primitive, so does it from C.
>
> I think it is almost a no-brainer to implement a sequential stopifnot if dropping to C code is allowed. In R it gets trickier, but how about this:
>
> Stopifnot <- function(...)
> {
>   n <- length(match.call()) - 1
>   for (i in 1:n)
>   {
>     nm <- as.name(paste0("..",i))
>     if (!eval(nm)) stop("not all true")
>   }
> }
> Stopifnot(2+2==4)
> Stopifnot(2+2==5, print("Hey!!!") == "Hey!!!")
> Stopifnot(2+2==4, print("Hey!!!") == "Hey!!!")
> Stopifnot(T,T,T,T,T,T,T,T,T,T,T,T,T,T,T,T,F,T)
>
>
>> On 15 May 2017, at 15:37 , Martin Maechler <maechler at stat.math.ethz.ch> wrote:
>>
>> I'm still curious about Herv?'s idea on using  switch()  for the
>> issue.
>

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fredhutch.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From paul at stat.auckland.ac.nz  Tue May 16 03:53:14 2017
From: paul at stat.auckland.ac.nz (Paul Murrell)
Date: Tue, 16 May 2017 13:53:14 +1200
Subject: [Rd] Error messages in replayPlot()
In-Reply-To: <CABFfbXtAaM0O0dtFFdLYBk40w1ADaoUz4UJVMRBOVmfgma6YPA@mail.gmail.com>
References: <CABFfbXtAaM0O0dtFFdLYBk40w1ADaoUz4UJVMRBOVmfgma6YPA@mail.gmail.com>
Message-ID: <bbdb1737-2697-8134-f804-4cc59c0bbc69@stat.auckland.ac.nz>

Hi

The "figure margins too large" message is suppressed on replay because 
that replay code is also played when resizing a graphics device (so if 
it was printed to the console you could get millions of error messages 
as you resized a window) - on replay, the message is drawn on the 
graphics device instead (take a look at the PNG that your second example 
creates), but that is not necessarily easy to access.

Paul

On 16/05/17 00:07, Jeroen Ooms wrote:
> I was wondering if there is something that can be done to improve
> error messages when replaying a recorded plot. For example a graphics
> device that is too small usually results in a helpful error message:
>
>  png(height = 100)
>  plot(1)
>  # Error in plot.new() : figure margins too large
>  dev.off()
>
> However when this happens when replaying a recorded plot, the error
> message is not so helpful.
>
>  myplot <- evaluate::evaluate("plot(1)")[[2]]
>  png(height = 100)
>  replayPlot(myplot)
>  # Error in replayPlot(x) : invalid graphics state
>  dev.off()
>
> A more informative error message that hints at what exactly is invalid
> about the graphics state would be very helpful in this case.
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/


From maechler at stat.math.ethz.ch  Tue May 16 11:01:23 2017
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 16 May 2017 11:01:23 +0200
Subject: [Rd]
	=?utf-8?q?=5Bbug=5D_droplevels=28=29_also_drop_object_attrib?=
	=?utf-8?b?dXRlcyAoY29tbWVudOKApik=?=
In-Reply-To: <f72658b5-4f45-4936-afb9-a4b4b78dc9a1@Spark>
References: <f72658b5-4f45-4936-afb9-a4b4b78dc9a1@Spark>
Message-ID: <22810.49123.10886.612295@stat.math.ethz.ch>

>>>>> Serge Bibauw <sbibauw at gmail.com>
>>>>>     on Mon, 15 May 2017 11:59:32 -0400 writes:

    > Hi,

    > Just reporting a small bug? not really a big deal, but I don?t think that is intended: droplevels()?also drops all object?s attributes.

Yes.  The help page for droplevels (or the simple definition of
'droplevels.factor') clearly indicate that the method for
factors is really just a call to   factor(x, exclude = *)

and that _is_ quite an important base function whose semantic
should not be changed lightly. Still, let's continue :

Looking a bit, I see that the current behavior of factor() {and
hence droplevels} has been unchanged in this respect  for the
whole history of R, well, at least for more than 17 years (R 1.0.1, April 2000).

I'd agree there _is_ a bug, at least in the documentation which
does *not* mention that currently, all attributes are dropped but "names",
"levels" (and "class").

OTOH, factor() would only need a small change to make it
preserve all attributes (but "class" and "levels" which are set explicitly).

I'm sure this will break some checks in some packages.
Is it worth it?

e.g., our own R  QC checks currently check (the printing of) the
following (in tests/reg-tests-2.R ):

    > ## some tests of factor matrices
    > A <- factor(7:12)
    > dim(A) <- c(2, 3)
    > A
	 [,1] [,2] [,3]
    [1,] 7    9    11  
    [2,] 8    10   12  
    Levels: 7 8 9 10 11 12
    > str(A)
     factor [1:2, 1:3] 7 8 9 10 ...
     - attr(*, "levels")= chr [1:6] "7" "8" "9" "10" ...
    > A[, 1:2]
	 [,1] [,2]
    [1,] 7    9   
    [2,] 8    10  
    Levels: 7 8 9 10 11 12
    > A[, 1:2, drop=TRUE]
    [1] 7  8  9  10
    Levels: 7 8 9 10

with the proposed change to factor(),
the last call would change its result:

    > A[, 1:2, drop=TRUE]
	 [,1] [,2]
    [1,] 7    9   
    [2,] 8    10  
    Levels: 7 8 9 10

because 'drop=TRUE' calls factor(..) and that would also
preserve the "dim" attribute.
I would think that the changed behavior _is_ better, and is also
according to documentation, because the help page for
 [.factor
explains that 'drop = TRUE' drops levels, but _not_ that it
transforms a factor matrix into a factor (vector).


Martin


    > Example:

    >> > test <- c("hello", "something", "hi")
    >> > test <- factor(test)
    >> > comment(test) <- "this is a test"
    >> > attr(test, "description") <- "this is another test"
    >> > attributes(test)
    >> $levels
    >> [1] "hello" ? ? "hi" ? ? ? ?"something"
    >> 
    >> $class
    >> [1] "factor"
    >> 
    >> $comment
    >> [1] "this is a test"
    >> 
    >> $description
    >> [1] "this is another test"
    >> 
    >> > test <- droplevels(test)
    >> > attributes(test)
    >> $levels
    >> [1] "hello" ? ? "hi" ? ? ? ?"something"
    >> 
    >> $class
    >> [1] "factor"


    > Serge


From maechler at stat.math.ethz.ch  Tue May 16 11:31:42 2017
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 16 May 2017 11:31:42 +0200
Subject: [Rd] stopifnot() does not stop at first non-TRUE argument
In-Reply-To: <3d296d7c-6fc0-0175-7bc0-57638cc3998b@fredhutch.org>
References: <64acdaeb-2857-9017-68c9-9da08e29a617@fredhutch.org>
 <E5DADF15-58C6-4F73-BC56-C6404896982F@gmail.com>
 <c2b3cb22-90bd-5372-e8b9-d1ea8b2ab828@fredhutch.org>
 <8945f7e4-99ce-53fa-46ee-cce64078a6a8@fredhutch.org>
 <22809.26921.473223.395550@stat.math.ethz.ch>
 <8fdb66b1-f785-ed55-6e40-a4ef8138e637@insa-toulouse.fr>
 <26bd9210-e40c-0967-7abb-5f724b09c034@insa-toulouse.fr>
 <22809.44830.153073.675527@stat.math.ethz.ch>
 <477f9e1a-4f7c-a902-e744-da4a36eb5a4f@insa-toulouse.fr>
 <22809.52422.219712.66882@stat.math.ethz.ch>
 <alpine.LFD.2.20.1705151236510.27291@itasca.stat.uiowa.edu>
 <3d296d7c-6fc0-0175-7bc0-57638cc3998b@fredhutch.org>
Message-ID: <22810.50942.577206.452639@stat.math.ethz.ch>

>>>>> Herv? Pag?s <hpages at fredhutch.org>
>>>>>     on Mon, 15 May 2017 16:54:46 -0700 writes:

    > Hi,
    > On 05/15/2017 10:41 AM, luke-tierney at uiowa.edu wrote:
    >> This is getting pretty convoluted.
    >> 
    >> The current behavior is consistent with the description at the top of
    >> the help page -- it does not promise to stop evaluation once the first
    >> non-TRUE is found.  That seems OK to me -- if you want sequencing you
    >> can use
    >> 
    >> stopifnot(A)
    >> stopifnot(B)
    >> 
    >> or
    >> 
    >> stopifnot(A && B)

    > My main use case for using stopifnot() is argument checking. In that
    > context, I like the conciseness of

    > stopifnot(
    > A,
    > B,
    > ...
    > )

    > I think it's a common use case (and a pretty natural thing to do) to
    > order/organize the expressions in a way such that it only makes sense
    > to continue evaluating if all was OK so far e.g.

    > stopifnot(
    > is.numeric(x),
    > length(x) == 1,
    > is.na(x)
    > )

I agree.  And that's how I have used stopifnot() in many cases
myself, sometimes even more "extremely" than the above example,
using assertions that only make sense if previous assertions
were fulfilled, such as

    stopifnot(is.numeric(n), length(n) == 1, n == round(n), n >= 0)

or in the Matrix package, first checking some class properties
and then things that only make sense for objects with those properties.


    > At least that's how things are organized in the stopifnot() calls that
    > accumulated in my code over the years. That's because I was convinced
    > that evaluation would stop at the first non-true expression (as
    > suggested by the man page). Until recently when I got a warning issued
    > by an expression located *after* the first non-true expression. This
    > was pretty unexpected/confusing!

    > If I can't rely on this "sequencing" feature, I guess I can always
    > do

    > stopifnot(A)
    > stopifnot(B)
    > ...

    > but I loose the conciseness of calling stopifnot() only once.
    > I could also use

    > stopifnot(A && B && ...)

    > but then I loose the conciseness of the error message i.e. it's going
    > to be something like

    > Error: A && B && ... is not TRUE

    > which can be pretty long/noisy compared to the message that reports
    > only the 1st error.


    > Conciseness/readability of the single call to stopifnot() and
    > conciseness of the error message are the features that made me
    > adopt stopifnot() in the 1st place. 

Yes, and that had been my design goal when I created it.

I do tend agree with  Herv? and Serguei here.

    > If stopifnot() cannot be revisited
    > to do "sequencing" then that means I will need to revisit all my calls
    > to stopifnot().

    >> 
    >> I could see an argument for a change that in the multiple argumetn
    >> case reports _all_ that fail; that would seem more useful to me than
    >> twisting the code into knots.

Interesting... but really differing from the current documentation,

    > Why not. Still better than the current situation. But only if that
    > semantic seems more useful to people. Would be sad if usefulness
    > of one semantic or the other was decided based on trickiness of
    > implementation.

Well, the trickiness  should definitely play a role.
Apart from functionality and semantics, long term maintenance
and code readibility, even elegance have shown to be very
important aspects of good code in ca 30 years of S and R programming.

OTOH, as mentioned above, the creation of good error messages
has been an important design goal of  stopifnot()  and hence I'm
willing to accept the extra complexity of "patching up" the call
used in the error / warning messages.

Also, as a change to what I posted yesterday, I now plan to follow
Peter Dalgaard's suggestion of using
             eval( ..<n> ) 
instead of   eval(cl[[i]], envir = <parent.frame(.)>)
as there may be cases where the former behaves better in lazy
evaluation situations.
(Other opinions on that ?)

Martin

    > Thanks,
    > H.

    >> 
    >> Best,
    >> 
    >> luke
    >> 
    >> On Mon, 15 May 2017, Martin Maechler wrote:
    >> 
    >>>>>>>> Serguei Sokol <sokol at insa-toulouse.fr>
    >>>>>>>> on Mon, 15 May 2017 16:32:20 +0200 writes:
    >>> 
    >>> > Le 15/05/2017 ? 15:37, Martin Maechler a ?crit :
    >>> >>>>>>> Serguei Sokol <sokol at insa-toulouse.fr>
    >>> >>>>>>> on Mon, 15 May 2017 13:14:34 +0200 writes:
    >>> >> > I see in the archives that the attachment cannot pass.
    >>> >> > So, here is the code:
    >>> >>
    >>> >> [....... MM: I needed to reformat etc to match closely to
    >>> >> the current source code which is in
    >>> >>
    >>> https://urldefense.proofpoint.com/v2/url?u=https-3A__svn.r-2Dproject.org_R_trunk_src_library_base_R_stop.R&d=DwIFAw&c=eRAMFD45gAfqt84VtBcfhQ&r=BK7q3XeAvimeWdGbWY_wJYbW0WYiZvSXAJJKaaPhzWA&m=t9fJDOl9YG2zB-GF0wQXrXJTsW2jxTxMHE-qZfLGzHU&s=KGsvpXrXpHCFTdbLM9ci3sBNO9C3ocsgEqHMvZKvV9I&e=
    >>> >> or its corresponding github mirror
    >>> >>
    >>> https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_wch_r-2Dsource_blob_trunk_src_library_base_R_stop.R&d=DwIFAw&c=eRAMFD45gAfqt84VtBcfhQ&r=BK7q3XeAvimeWdGbWY_wJYbW0WYiZvSXAJJKaaPhzWA&m=t9fJDOl9YG2zB-GF0wQXrXJTsW2jxTxMHE-qZfLGzHU&s=7Z5bPVWdGPpY2KLnXQP6c-_8s86CpKe0ZYkCfqjfxY0&e=
    >>> >> ]
    >>> >>
    >>> >> > Best,
    >>> >> > Serguei.
    >>> >>
    >>> >> Yes, something like that seems even simpler than Peter's
    >>> >> suggestion...
    >>> >>
    >>> >> It currently breaks 'make check' in the R sources,
    >>> >> specifically in tests/reg-tests-2.R (lines 6574 ff),
    >>> >> the new code now gives
    >>> >>
    >>> >> > ## error messages from (C-level) evalList
    >>> >> > tst <- function(y) { stopifnot(is.numeric(y)); y+ 1 }
    >>> >> > try(tst())
    >>> >> Error in eval(cl.i, pfr) : argument "y" is missing, with no default
    >>> >>
    >>> >> whereas previously it gave
    >>> >>
    >>> >> Error in stopifnot(is.numeric(y)) :
    >>> >> argument "y" is missing, with no default
    >>> >>
    >>> >>
    >>> >> But I think that change (of call stack in such an error case) is
    >>> >> unavoidable and not a big problem.
    >>> 
    >>> > It can be avoided but at price of customizing error() and
    >>> warning() calls with something like:
    >>> > wrn <- function(w) {w$call <- cl.i; warning(w)}
    >>> > err <- function(e) {e$call <- cl.i; stop(e)}
    >>> > ...
    >>> > tryCatch(r <- eval(cl.i, pfr), warning=wrn, error=err)
    >>> 
    >>> > Serguei.
    >>> 
    >>> Well, a good idea, but the 'warning' case is more complicated
    >>> (and the above incorrect): I do want the warning there, but
    >>> _not_ return the warning, but rather, the result of eval() :
    >>> So this needs even more sophistication, using  withCallingHandlers(.)
    >>> and maybe that really get's too sophisticated and no
    >>> more "readable" to 99.9% of the R users ... ?
    >>> 
    >>> I now do append my current version -- in case some may want to
    >>> comment or improve further.
    >>> 
    >>> Martin


From sokol at insa-toulouse.fr  Tue May 16 11:37:10 2017
From: sokol at insa-toulouse.fr (Serguei Sokol)
Date: Tue, 16 May 2017 11:37:10 +0200
Subject: [Rd] stopifnot() does not stop at first non-TRUE argument
In-Reply-To: <alpine.LFD.2.20.1705151236510.27291@itasca.stat.uiowa.edu>
References: <64acdaeb-2857-9017-68c9-9da08e29a617@fredhutch.org>
 <E5DADF15-58C6-4F73-BC56-C6404896982F@gmail.com>
 <c2b3cb22-90bd-5372-e8b9-d1ea8b2ab828@fredhutch.org>
 <8945f7e4-99ce-53fa-46ee-cce64078a6a8@fredhutch.org>
 <22809.26921.473223.395550@stat.math.ethz.ch>
 <8fdb66b1-f785-ed55-6e40-a4ef8138e637@insa-toulouse.fr>
 <26bd9210-e40c-0967-7abb-5f724b09c034@insa-toulouse.fr>
 <22809.44830.153073.675527@stat.math.ethz.ch>
 <477f9e1a-4f7c-a902-e744-da4a36eb5a4f@insa-toulouse.fr>
 <22809.52422.219712.66882@stat.math.ethz.ch>
 <alpine.LFD.2.20.1705151236510.27291@itasca.stat.uiowa.edu>
Message-ID: <495de2cf-1633-e3bf-8c61-5f30029399e6@insa-toulouse.fr>

Le 15/05/2017 ? 19:41, luke-tierney at uiowa.edu a ?crit :
> This is getting pretty convoluted.
>
> The current behavior is consistent with the description at the top of
> the help page -- it does not promise to stop evaluation once the first
> non-TRUE is found.
Hm... we can read in the man page :
?stopifnot(A, B)? is conceptually equivalent to

       { if(any(is.na(A)) || !all(A)) stop(...);
         if(any(is.na(B)) || !all(B)) stop(...) }
and this behavior does promise to stop at first non-TRUE value
without evaluation of the rest of conditions.

Sergue?.


From Olivier.Renaud at unige.ch  Tue May 16 09:47:47 2017
From: Olivier.Renaud at unige.ch (Olivier Renaud)
Date: Tue, 16 May 2017 09:47:47 +0200
Subject: [Rd] Wish for arima function: add a data argument and a
 formula-type for regressors
Message-ID: <4454faa0-3ddf-8761-53d5-ff66e7d04673@unige.ch>

Hi,

Using arima on data that are in a data frame, especially when adding 
xreg, would be much easier if the arima function contained

1) a "data=" argument

2) the possibility to include the covariate(s) in a formula style.

Ideally the call could be something like

 > arima(symptome, order=c(1,0,0), xreg=~trait01*mesure0, data=anxiete)

( or arima(symptome~trait01*mesure0, order=c(1,0,0), data=anxiete)       )

instead of present:

 > anxiete$interact = anxiete$trait01*anxiete$mesure0
 > arima(anxiete$symptome, order=c(1,0,0), xreg=anxiete[, c("trait01", 
"mesure0", "interact")])


Background: Especially in psychology, so-called single case analyses 
consist often in a the interaction effect of treatment and usual 
training effect, with typically arma type of error, resulting in the 
above model. Typically, all the needed data are in a data.frame .

An additional advantage concerns the names of the coefficient in the 
output: if only one regressor:

>arima(anxiete$symptome, order=c(1,0,0), xreg=anxiete[, c("trait01")]) [...]
Coefficients:
          ar1  intercept  anxiete[, c("trait01")]
       0.5649    33.8623                  -8.1225
s.e.  0.1073     0.5969                   0.8052

but the name convention changes with several regressors:

>arima(anxiete$symptome, order=c(1,0,0), xreg=anxiete[, c("trait01", 
"mesure0", "interact")]) [...]
Coefficients:
          ar1  intercept  trait01  mesure0  interact
       0.2715    34.1363  -5.5777   0.0075   -0.1809
s.e.  0.1211     0.6685   0.9009   0.0342    0.0490



-- 
Prof. Olivier Renaud                  http://www.unige.ch/fapse/mad/
Methodology & Data Analysis - Psychology Dept - University of Geneva
UniMail, Office 4138  -  40, Bd du Pont d'Arve   -  CH-1211 Geneva 4


	[[alternative HTML version deleted]]


From sokol at insa-toulouse.fr  Tue May 16 12:58:00 2017
From: sokol at insa-toulouse.fr (Serguei Sokol)
Date: Tue, 16 May 2017 12:58:00 +0200
Subject: [Rd] tweaking Sys.timezone()
Message-ID: <cc29218b-a0e8-cdc1-e10e-973e8067f119@insa-toulouse.fr>

Hi,

On my system (Linux, Mageia 5) Sys.timezone() returns NA but
with a minor tweak it could work as expected, i.e. returning "Europe/Paris".

Here is the problem. At some moment it does

  lt <- normalizePath("/etc/localtime")

On my system /etc/localtime is a symlink pointing to
/usr/share/zoneinfo/Europe/Paris. So far so good.
With the next two operations the good answer should be found:

  if (grepl(pat <- "^/usr/share/zoneinfo/", lt)) sub(pat, "", lt)

Unfortunately, on my system "/usr/share" is also a simlink
so lt resolves to "/home/local/usr_share/zoneinfo/Europe/Paris"
and not to "/usr/share/zoneinfo/Europe/Paris".
So the test above fails.
As the keyword in this story is zoneinfo, could we modify the
pat to look as

   if (grepl(pat <- "^.*/zoneinfo/", lt)) sub(pat, "", lt)

?
In this way, we don't make assumption where exactly
"zoneinfo/*" resides. We have found it, no matter where, so use it.

Hoping it could find its way into a next R release.

Best,
Serguei.


From luke-tierney at uiowa.edu  Tue May 16 16:40:19 2017
From: luke-tierney at uiowa.edu (luke-tierney at uiowa.edu)
Date: Tue, 16 May 2017 09:40:19 -0500 (CDT)
Subject: [Rd] stopifnot() does not stop at first non-TRUE argument
In-Reply-To: <495de2cf-1633-e3bf-8c61-5f30029399e6@insa-toulouse.fr>
References: <64acdaeb-2857-9017-68c9-9da08e29a617@fredhutch.org>
 <E5DADF15-58C6-4F73-BC56-C6404896982F@gmail.com>
 <c2b3cb22-90bd-5372-e8b9-d1ea8b2ab828@fredhutch.org>
 <8945f7e4-99ce-53fa-46ee-cce64078a6a8@fredhutch.org>
 <22809.26921.473223.395550@stat.math.ethz.ch>
 <8fdb66b1-f785-ed55-6e40-a4ef8138e637@insa-toulouse.fr>
 <26bd9210-e40c-0967-7abb-5f724b09c034@insa-toulouse.fr>
 <22809.44830.153073.675527@stat.math.ethz.ch>
 <477f9e1a-4f7c-a902-e744-da4a36eb5a4f@insa-toulouse.fr>
 <22809.52422.219712.66882@stat.math.ethz.ch>
 <alpine.LFD.2.20.1705151236510.27291@itasca.stat.uiowa.edu>
 <495de2cf-1633-e3bf-8c61-5f30029399e6@insa-toulouse.fr>
Message-ID: <alpine.DEB.2.20.1705160938100.3019@luke-Latitude>

On Tue, 16 May 2017, Serguei Sokol wrote:

> Le 15/05/2017 ? 19:41, luke-tierney at uiowa.edu a ?crit :
>> This is getting pretty convoluted.
>> 
>> The current behavior is consistent with the description at the top of
>> the help page -- it does not promise to stop evaluation once the first
>> non-TRUE is found.
> Hm... we can read in the man page :
> ?stopifnot(A, B)? is conceptually equivalent to
>
>      { if(any(is.na(A)) || !all(A)) stop(...);
>        if(any(is.na(B)) || !all(B)) stop(...) }
> and this behavior does promise to stop at first non-TRUE value
> without evaluation of the rest of conditions.

Yes: that is why I explicitly referenced the description at the top of
the page.

Changing the 'conceptually equivalent' bit to reflect what is
happening is easy.  The changes being discussed, and their long term
maintenance, ar not.

Best,

luke


>
> Sergue?.
>

-- 
Luke Tierney
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu

From luke-tierney at uiowa.edu  Tue May 16 16:49:56 2017
From: luke-tierney at uiowa.edu (luke-tierney at uiowa.edu)
Date: Tue, 16 May 2017 09:49:56 -0500 (CDT)
Subject: [Rd] stopifnot() does not stop at first non-TRUE argument
In-Reply-To: <22810.50942.577206.452639@stat.math.ethz.ch>
References: <64acdaeb-2857-9017-68c9-9da08e29a617@fredhutch.org>
 <E5DADF15-58C6-4F73-BC56-C6404896982F@gmail.com>
 <c2b3cb22-90bd-5372-e8b9-d1ea8b2ab828@fredhutch.org>
 <8945f7e4-99ce-53fa-46ee-cce64078a6a8@fredhutch.org>
 <22809.26921.473223.395550@stat.math.ethz.ch>
 <8fdb66b1-f785-ed55-6e40-a4ef8138e637@insa-toulouse.fr>
 <26bd9210-e40c-0967-7abb-5f724b09c034@insa-toulouse.fr>
 <22809.44830.153073.675527@stat.math.ethz.ch>
 <477f9e1a-4f7c-a902-e744-da4a36eb5a4f@insa-toulouse.fr>
 <22809.52422.219712.66882@stat.math.ethz.ch>
 <alpine.LFD.2.20.1705151236510.27291@itasca.stat.uiowa.edu>
 <3d296d7c-6fc0-0175-7bc0-57638cc3998b@fredhutch.org>
 <22810.50942.577206.452639@stat.math.ethz.ch>
Message-ID: <alpine.DEB.2.20.1705160942240.3019@luke-Latitude>

On Tue, 16 May 2017, Martin Maechler wrote:

>>>>>> Herv? Pag?s <hpages at fredhutch.org>
>>>>>>     on Mon, 15 May 2017 16:54:46 -0700 writes:
>
>    > Hi,
>    > On 05/15/2017 10:41 AM, luke-tierney at uiowa.edu wrote:
>    >> This is getting pretty convoluted.
>    >>
>    >> The current behavior is consistent with the description at the top of
>    >> the help page -- it does not promise to stop evaluation once the first
>    >> non-TRUE is found.  That seems OK to me -- if you want sequencing you
>    >> can use
>    >>
>    >> stopifnot(A)
>    >> stopifnot(B)
>    >>
>    >> or
>    >>
>    >> stopifnot(A && B)
>
>    > My main use case for using stopifnot() is argument checking. In that
>    > context, I like the conciseness of
>
>    > stopifnot(
>    > A,
>    > B,
>    > ...
>    > )
>
>    > I think it's a common use case (and a pretty natural thing to do) to
>    > order/organize the expressions in a way such that it only makes sense
>    > to continue evaluating if all was OK so far e.g.
>
>    > stopifnot(
>    > is.numeric(x),
>    > length(x) == 1,
>    > is.na(x)
>    > )
>
> I agree.  And that's how I have used stopifnot() in many cases
> myself, sometimes even more "extremely" than the above example,
> using assertions that only make sense if previous assertions
> were fulfilled, such as
>
>    stopifnot(is.numeric(n), length(n) == 1, n == round(n), n >= 0)
>
> or in the Matrix package, first checking some class properties
> and then things that only make sense for objects with those properties.
>
>
>    > At least that's how things are organized in the stopifnot() calls that
>    > accumulated in my code over the years. That's because I was convinced
>    > that evaluation would stop at the first non-true expression (as
>    > suggested by the man page). Until recently when I got a warning issued
>    > by an expression located *after* the first non-true expression. This
>    > was pretty unexpected/confusing!
>
>    > If I can't rely on this "sequencing" feature, I guess I can always
>    > do
>
>    > stopifnot(A)
>    > stopifnot(B)
>    > ...
>
>    > but I loose the conciseness of calling stopifnot() only once.
>    > I could also use
>
>    > stopifnot(A && B && ...)
>
>    > but then I loose the conciseness of the error message i.e. it's going
>    > to be something like
>
>    > Error: A && B && ... is not TRUE
>
>    > which can be pretty long/noisy compared to the message that reports
>    > only the 1st error.
>
>
>    > Conciseness/readability of the single call to stopifnot() and
>    > conciseness of the error message are the features that made me
>    > adopt stopifnot() in the 1st place.
>
> Yes, and that had been my design goal when I created it.
>
> I do tend agree with  Herv? and Serguei here.
>
>    > If stopifnot() cannot be revisited
>    > to do "sequencing" then that means I will need to revisit all my calls
>    > to stopifnot().
>
>    >>
>    >> I could see an argument for a change that in the multiple argumetn
>    >> case reports _all_ that fail; that would seem more useful to me than
>    >> twisting the code into knots.
>
> Interesting... but really differing from the current documentation,
>
>    > Why not. Still better than the current situation. But only if that
>    > semantic seems more useful to people. Would be sad if usefulness
>    > of one semantic or the other was decided based on trickiness of
>    > implementation.
>
> Well, the trickiness  should definitely play a role.
> Apart from functionality and semantics, long term maintenance
> and code readibility, even elegance have shown to be very
> important aspects of good code in ca 30 years of S and R programming.
>
> OTOH, as mentioned above, the creation of good error messages
> has been an important design goal of  stopifnot()  and hence I'm
> willing to accept the extra complexity of "patching up" the call
> used in the error / warning messages.
>
> Also, as a change to what I posted yesterday, I now plan to follow
> Peter Dalgaard's suggestion of using
>             eval( ..<n> )
> instead of   eval(cl[[i]], envir = <parent.frame(.)>)
> as there may be cases where the former behaves better in lazy
> evaluation situations.
> (Other opinions on that ?)

If you go this route it would be useful to step back and think about
whether there might be some useful primitives to add to make this
easier, such as

- provide a dotsLength function for computing the number arguments
   captured in a ... argument

- providing a dotsElt function for extracting the i-the element
   instead of going through the eval(sprintf("..%d", i)) construct.

- maybe something for extracting the expression for the i-th argument.

The might be more generally useful and make the code more readable and
maintainable.

Best,

luke

>
> Martin
>
>    > Thanks,
>    > H.
>
>    >>
>    >> Best,
>    >>
>    >> luke
>    >>
>    >> On Mon, 15 May 2017, Martin Maechler wrote:
>    >>
>    >>>>>>>> Serguei Sokol <sokol at insa-toulouse.fr>
>    >>>>>>>> on Mon, 15 May 2017 16:32:20 +0200 writes:
>    >>>
>    >>> > Le 15/05/2017 ? 15:37, Martin Maechler a ?crit :
>    >>> >>>>>>> Serguei Sokol <sokol at insa-toulouse.fr>
>    >>> >>>>>>> on Mon, 15 May 2017 13:14:34 +0200 writes:
>    >>> >> > I see in the archives that the attachment cannot pass.
>    >>> >> > So, here is the code:
>    >>> >>
>    >>> >> [....... MM: I needed to reformat etc to match closely to
>    >>> >> the current source code which is in
>    >>> >>
>    >>> https://urldefense.proofpoint.com/v2/url?u=https-3A__svn.r-2Dproject.org_R_trunk_src_library_base_R_stop.R&d=DwIFAw&c=eRAMFD45gAfqt84VtBcfhQ&r=BK7q3XeAvimeWdGbWY_wJYbW0WYiZvSXAJJKaaPhzWA&m=t9fJDOl9YG2zB-GF0wQXrXJTsW2jxTxMHE-qZfLGzHU&s=KGsvpXrXpHCFTdbLM9ci3sBNO9C3ocsgEqHMvZKvV9I&e=
>    >>> >> or its corresponding github mirror
>    >>> >>
>    >>> https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_wch_r-2Dsource_blob_trunk_src_library_base_R_stop.R&d=DwIFAw&c=eRAMFD45gAfqt84VtBcfhQ&r=BK7q3XeAvimeWdGbWY_wJYbW0WYiZvSXAJJKaaPhzWA&m=t9fJDOl9YG2zB-GF0wQXrXJTsW2jxTxMHE-qZfLGzHU&s=7Z5bPVWdGPpY2KLnXQP6c-_8s86CpKe0ZYkCfqjfxY0&e=
>    >>> >> ]
>    >>> >>
>    >>> >> > Best,
>    >>> >> > Serguei.
>    >>> >>
>    >>> >> Yes, something like that seems even simpler than Peter's
>    >>> >> suggestion...
>    >>> >>
>    >>> >> It currently breaks 'make check' in the R sources,
>    >>> >> specifically in tests/reg-tests-2.R (lines 6574 ff),
>    >>> >> the new code now gives
>    >>> >>
>    >>> >> > ## error messages from (C-level) evalList
>    >>> >> > tst <- function(y) { stopifnot(is.numeric(y)); y+ 1 }
>    >>> >> > try(tst())
>    >>> >> Error in eval(cl.i, pfr) : argument "y" is missing, with no default
>    >>> >>
>    >>> >> whereas previously it gave
>    >>> >>
>    >>> >> Error in stopifnot(is.numeric(y)) :
>    >>> >> argument "y" is missing, with no default
>    >>> >>
>    >>> >>
>    >>> >> But I think that change (of call stack in such an error case) is
>    >>> >> unavoidable and not a big problem.
>    >>>
>    >>> > It can be avoided but at price of customizing error() and
>    >>> warning() calls with something like:
>    >>> > wrn <- function(w) {w$call <- cl.i; warning(w)}
>    >>> > err <- function(e) {e$call <- cl.i; stop(e)}
>    >>> > ...
>    >>> > tryCatch(r <- eval(cl.i, pfr), warning=wrn, error=err)
>    >>>
>    >>> > Serguei.
>    >>>
>    >>> Well, a good idea, but the 'warning' case is more complicated
>    >>> (and the above incorrect): I do want the warning there, but
>    >>> _not_ return the warning, but rather, the result of eval() :
>    >>> So this needs even more sophistication, using  withCallingHandlers(.)
>    >>> and maybe that really get's too sophisticated and no
>    >>> more "readable" to 99.9% of the R users ... ?
>    >>>
>    >>> I now do append my current version -- in case some may want to
>    >>> comment or improve further.
>    >>>
>    >>> Martin
>

-- 
Luke Tierney
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu

From suharto_anggono at yahoo.com  Tue May 16 18:37:45 2017
From: suharto_anggono at yahoo.com (Suharto Anggono Suharto Anggono)
Date: Tue, 16 May 2017 16:37:45 +0000 (UTC)
Subject: [Rd] stopifnot() does not stop at first non-TRUE argument
References: <1034131318.560887.1494952665524.ref@mail.yahoo.com>
Message-ID: <1034131318.560887.1494952665524@mail.yahoo.com>

switch(i, ...)
extracts 'i'-th argument in '...'. It is like
eval(as.name(paste0("..", i))) .

Just mentioning other things:
- For 'n',
n <- nargs()
can be used.
- sys.call() can be used in place of match.call() .
---------------------------
>>>>> peter dalgaard <pdalgd at gmail.com>
>>>>>     on Mon, 15 May 2017 16:28:42 +0200 writes:

    > I think Herv?'s idea was just that if switch can evaluate arguments selectively, so can stopifnot(). But switch() is .Primitive, so does it from C. 

if he just meant that, then "yes, of course" (but not so interesting).

    > I think it is almost a no-brainer to implement a sequential stopifnot if dropping to C code is allowed. In R it gets trickier, but how about this:

Something like this, yes, that's close to what Serguei Sokol had proposed
(and of course I *do*  want to keep the current sophistication
 of stopifnot(), so this is really too simple)

    > Stopifnot <- function(...)
    > {
    > n <- length(match.call()) - 1
    > for (i in 1:n)
    > {
    > nm <- as.name(paste0("..",i))
    > if (!eval(nm)) stop("not all true")
    > }
    > }
    > Stopifnot(2+2==4)
    > Stopifnot(2+2==5, print("Hey!!!") == "Hey!!!")
    > Stopifnot(2+2==4, print("Hey!!!") == "Hey!!!")
    > Stopifnot(T,T,T,T,T,T,T,T,T,T,T,T,T,T,T,T,F,T)


    >> On 15 May 2017, at 15:37 , Martin Maechler <maechler at stat.math.ethz.ch> wrote:
    >> 
    >> I'm still curious about Herv?'s idea on using  switch()  for the
    >> issue.

    > -- 
    > Peter Dalgaard, Professor,
    > Center for Statistics, Copenhagen Business School
    > Solbjerg Plads 3, 2000 Frederiksberg, Denmark
    > Phone: (+45)38153501
    > Office: A 4.23
    > Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From pdalgd at gmail.com  Tue May 16 18:59:46 2017
From: pdalgd at gmail.com (peter dalgaard)
Date: Tue, 16 May 2017 18:59:46 +0200
Subject: [Rd] stopifnot() does not stop at first non-TRUE argument
In-Reply-To: <1034131318.560887.1494952665524@mail.yahoo.com>
References: <1034131318.560887.1494952665524.ref@mail.yahoo.com>
 <1034131318.560887.1494952665524@mail.yahoo.com>
Message-ID: <10E55245-5B37-48FC-A3D0-0B87979DD5D1@gmail.com>


> On 16 May 2017, at 18:37 , Suharto Anggono Suharto Anggono via R-devel <r-devel at r-project.org> wrote:
> 
> switch(i, ...)
> extracts 'i'-th argument in '...'. It is like
> eval(as.name(paste0("..", i))) .

Hey, that's pretty neat! 

-pd

> 
> Just mentioning other things:
> - For 'n',
> n <- nargs()
> can be used.
> - sys.call() can be used in place of match.call() .
> ---------------------------
>>>>>> peter dalgaard <pdalgd at gmail.com>
>>>>>>    on Mon, 15 May 2017 16:28:42 +0200 writes:
> 
>> I think Herv?'s idea was just that if switch can evaluate arguments selectively, so can stopifnot(). But switch() is .Primitive, so does it from C. 
> 
> if he just meant that, then "yes, of course" (but not so interesting).
> 
>> I think it is almost a no-brainer to implement a sequential stopifnot if dropping to C code is allowed. In R it gets trickier, but how about this:
> 
> Something like this, yes, that's close to what Serguei Sokol had proposed
> (and of course I *do*  want to keep the current sophistication
> of stopifnot(), so this is really too simple)
> 
>> Stopifnot <- function(...)
>> {
>> n <- length(match.call()) - 1
>> for (i in 1:n)
>> {
>> nm <- as.name(paste0("..",i))
>> if (!eval(nm)) stop("not all true")
>> }
>> }
>> Stopifnot(2+2==4)
>> Stopifnot(2+2==5, print("Hey!!!") == "Hey!!!")
>> Stopifnot(2+2==4, print("Hey!!!") == "Hey!!!")
>> Stopifnot(T,T,T,T,T,T,T,T,T,T,T,T,T,T,T,T,F,T)
> 
> 
>>> On 15 May 2017, at 15:37 , Martin Maechler <maechler at stat.math.ethz.ch> wrote:
>>> 
>>> I'm still curious about Herv?'s idea on using  switch()  for the
>>> issue.
> 
>> -- 
>> Peter Dalgaard, Professor,
>> Center for Statistics, Copenhagen Business School
>> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
>> Phone: (+45)38153501
>> Office: A 4.23
>> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From maechler at stat.math.ethz.ch  Tue May 16 19:10:30 2017
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 16 May 2017 19:10:30 +0200
Subject: [Rd] stopifnot() does not stop at first non-TRUE argument
In-Reply-To: <alpine.DEB.2.20.1705160942240.3019@luke-Latitude>
References: <64acdaeb-2857-9017-68c9-9da08e29a617@fredhutch.org>
 <E5DADF15-58C6-4F73-BC56-C6404896982F@gmail.com>
 <c2b3cb22-90bd-5372-e8b9-d1ea8b2ab828@fredhutch.org>
 <8945f7e4-99ce-53fa-46ee-cce64078a6a8@fredhutch.org>
 <22809.26921.473223.395550@stat.math.ethz.ch>
 <8fdb66b1-f785-ed55-6e40-a4ef8138e637@insa-toulouse.fr>
 <26bd9210-e40c-0967-7abb-5f724b09c034@insa-toulouse.fr>
 <22809.44830.153073.675527@stat.math.ethz.ch>
 <477f9e1a-4f7c-a902-e744-da4a36eb5a4f@insa-toulouse.fr>
 <22809.52422.219712.66882@stat.math.ethz.ch>
 <alpine.LFD.2.20.1705151236510.27291@itasca.stat.uiowa.edu>
 <3d296d7c-6fc0-0175-7bc0-57638cc3998b@fredhutch.org>
 <22810.50942.577206.452639@stat.math.ethz.ch>
 <alpine.DEB.2.20.1705160942240.3019@luke-Latitude>
Message-ID: <22811.12934.755008.738297@stat.math.ethz.ch>

>>>>>   <luke-tierney at uiowa.edu>
>>>>>     on Tue, 16 May 2017 09:49:56 -0500 writes:

    > On Tue, 16 May 2017, Martin Maechler wrote:
    >>>>>>> Herv? Pag?s <hpages at fredhutch.org>
    >>>>>>> on Mon, 15 May 2017 16:54:46 -0700 writes:
    >> 
    >> > Hi,
    >> > On 05/15/2017 10:41 AM, luke-tierney at uiowa.edu wrote:
    >> >> This is getting pretty convoluted.
    >> >>
    >> >> The current behavior is consistent with the description at the top of
    >> >> the help page -- it does not promise to stop evaluation once the first
    >> >> non-TRUE is found.  That seems OK to me -- if you want sequencing you
    >> >> can use
    >> >>
    >> >> stopifnot(A)
    >> >> stopifnot(B)
    >> >>
    >> >> or
    >> >>
    >> >> stopifnot(A && B)
    >> 
    >> > My main use case for using stopifnot() is argument checking. In that
    >> > context, I like the conciseness of
    >> 
    >> > stopifnot(
    >> > A,
    >> > B,
    >> > ...
    >> > )
    >> 
    >> > I think it's a common use case (and a pretty natural thing to do) to
    >> > order/organize the expressions in a way such that it only makes sense
    >> > to continue evaluating if all was OK so far e.g.
    >> 
    >> > stopifnot(
    >> > is.numeric(x),
    >> > length(x) == 1,
    >> > is.na(x)
    >> > )
    >> 
    >> I agree.  And that's how I have used stopifnot() in many cases
    >> myself, sometimes even more "extremely" than the above example,
    >> using assertions that only make sense if previous assertions
    >> were fulfilled, such as
    >> 
    >> stopifnot(is.numeric(n), length(n) == 1, n == round(n), n >= 0)
    >> 
    >> or in the Matrix package, first checking some class properties
    >> and then things that only make sense for objects with those properties.
    >> 
    >> 
    >> > At least that's how things are organized in the stopifnot() calls that
    >> > accumulated in my code over the years. That's because I was convinced
    >> > that evaluation would stop at the first non-true expression (as
    >> > suggested by the man page). Until recently when I got a warning issued
    >> > by an expression located *after* the first non-true expression. This
    >> > was pretty unexpected/confusing!
    >> 
    >> > If I can't rely on this "sequencing" feature, I guess I can always
    >> > do
    >> 
    >> > stopifnot(A)
    >> > stopifnot(B)
    >> > ...
    >> 
    >> > but I loose the conciseness of calling stopifnot() only once.
    >> > I could also use
    >> 
    >> > stopifnot(A && B && ...)
    >> 
    >> > but then I loose the conciseness of the error message i.e. it's going
    >> > to be something like
    >> 
    >> > Error: A && B && ... is not TRUE
    >> 
    >> > which can be pretty long/noisy compared to the message that reports
    >> > only the 1st error.
    >> 
    >> 
    >> > Conciseness/readability of the single call to stopifnot() and
    >> > conciseness of the error message are the features that made me
    >> > adopt stopifnot() in the 1st place.
    >> 
    >> Yes, and that had been my design goal when I created it.
    >> 
    >> I do tend agree with  Herv? and Serguei here.
    >> 
    >> > If stopifnot() cannot be revisited
    >> > to do "sequencing" then that means I will need to revisit all my calls
    >> > to stopifnot().
    >> 
    >> >>
    >> >> I could see an argument for a change that in the multiple argumetn
    >> >> case reports _all_ that fail; that would seem more useful to me than
    >> >> twisting the code into knots.
    >> 
    >> Interesting... but really differing from the current documentation,
    >> 
    >> > Why not. Still better than the current situation. But only if that
    >> > semantic seems more useful to people. Would be sad if usefulness
    >> > of one semantic or the other was decided based on trickiness of
    >> > implementation.
    >> 
    >> Well, the trickiness  should definitely play a role.
    >> Apart from functionality and semantics, long term maintenance
    >> and code readibility, even elegance have shown to be very
    >> important aspects of good code in ca 30 years of S and R programming.
    >> 
    >> OTOH, as mentioned above, the creation of good error messages
    >> has been an important design goal of  stopifnot()  and hence I'm
    >> willing to accept the extra complexity of "patching up" the call
    >> used in the error / warning messages.
    >> 
    >> Also, as a change to what I posted yesterday, I now plan to follow
    >> Peter Dalgaard's suggestion of using
    >> eval( ..<n> )
    >> instead of   eval(cl[[i]], envir = <parent.frame(.)>)
    >> as there may be cases where the former behaves better in lazy
    >> evaluation situations.
    >> (Other opinions on that ?)

    > If you go this route it would be useful to step back and think about
    > whether there might be some useful primitives to add to make this
    > easier, such as

    > - provide a dotsLength function for computing the number arguments
    > captured in a ... argument

actually my current version did not use that anymore 
(and here we could use   nargs() )

    > - providing a dotsElt function for extracting the i-the element
    > instead of going through the eval(sprintf("..%d", i)) construct.

I've started to do that, ...

    > - maybe something for extracting the expression for the i-th argument.

well, yes, but that seems quite readable here, we use the whole
call anyway and loop, looking at each sequentially --- that part has not
been new!

    > The might be more generally useful and make the code more readable and
    > maintainable.

    > Best,

    > luke

    >> 
    >> Martin
    >> 
    >> > Thanks,
    >> > H.
    >> 
    >> >>
    >> >> Best,
    >> >>
    >> >> luke
    >> >>
    >> >> On Mon, 15 May 2017, Martin Maechler wrote:
    >> >>
    >> >>>>>>>> Serguei Sokol <sokol at insa-toulouse.fr>
    >> >>>>>>>> on Mon, 15 May 2017 16:32:20 +0200 writes:
    >> >>>
    >> >>> > Le 15/05/2017 ? 15:37, Martin Maechler a ?crit :
    >> >>> >>>>>>> Serguei Sokol <sokol at insa-toulouse.fr>
    >> >>> >>>>>>> on Mon, 15 May 2017 13:14:34 +0200 writes:
    >> >>> >> > I see in the archives that the attachment cannot pass.
    >> >>> >> > So, here is the code:
    >> >>> >>
    >> >>> >> [....... MM: I needed to reformat etc to match closely to
    >> >>> >> the current source code which is in
    >> >>> >>
    >> >>> https://urldefense.proofpoint.com/v2/url?u=https-3A__svn.r-2Dproject.org_R_trunk_src_library_base_R_stop.R&d=DwIFAw&c=eRAMFD45gAfqt84VtBcfhQ&r=BK7q3XeAvimeWdGbWY_wJYbW0WYiZvSXAJJKaaPhzWA&m=t9fJDOl9YG2zB-GF0wQXrXJTsW2jxTxMHE-qZfLGzHU&s=KGsvpXrXpHCFTdbLM9ci3sBNO9C3ocsgEqHMvZKvV9I&e=
    >> >>> >> or its corresponding github mirror
    >> >>> >>
    >> >>> https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_wch_r-2Dsource_blob_trunk_src_library_base_R_stop.R&d=DwIFAw&c=eRAMFD45gAfqt84VtBcfhQ&r=BK7q3XeAvimeWdGbWY_wJYbW0WYiZvSXAJJKaaPhzWA&m=t9fJDOl9YG2zB-GF0wQXrXJTsW2jxTxMHE-qZfLGzHU&s=7Z5bPVWdGPpY2KLnXQP6c-_8s86CpKe0ZYkCfqjfxY0&e=
    >> >>> >> ]
    >> >>> >>
    >> >>> >> > Best,
    >> >>> >> > Serguei.
    >> >>> >>
    >> >>> >> Yes, something like that seems even simpler than Peter's
    >> >>> >> suggestion...
    >> >>> >>
    >> >>> >> It currently breaks 'make check' in the R sources,
    >> >>> >> specifically in tests/reg-tests-2.R (lines 6574 ff),
    >> >>> >> the new code now gives
    >> >>> >>
    >> >>> >> > ## error messages from (C-level) evalList
    >> >>> >> > tst <- function(y) { stopifnot(is.numeric(y)); y+ 1 }
    >> >>> >> > try(tst())
    >> >>> >> Error in eval(cl.i, pfr) : argument "y" is missing, with no default
    >> >>> >>
    >> >>> >> whereas previously it gave
    >> >>> >>
    >> >>> >> Error in stopifnot(is.numeric(y)) :
    >> >>> >> argument "y" is missing, with no default
    >> >>> >>
    >> >>> >>
    >> >>> >> But I think that change (of call stack in such an error case) is
    >> >>> >> unavoidable and not a big problem.
    >> >>>
    >> >>> > It can be avoided but at price of customizing error() and
    >> >>> warning() calls with something like:
    >> >>> > wrn <- function(w) {w$call <- cl.i; warning(w)}
    >> >>> > err <- function(e) {e$call <- cl.i; stop(e)}
    >> >>> > ...
    >> >>> > tryCatch(r <- eval(cl.i, pfr), warning=wrn, error=err)
    >> >>>
    >> >>> > Serguei.
    >> >>>
    >> >>> Well, a good idea, but the 'warning' case is more complicated
    >> >>> (and the above incorrect): I do want the warning there, but
    >> >>> _not_ return the warning, but rather, the result of eval() :
    >> >>> So this needs even more sophistication, using  withCallingHandlers(.)
    >> >>> and maybe that really get's too sophisticated and no
    >> >>> more "readable" to 99.9% of the R users ... ?
    >> >>>
    >> >>> I now do append my current version -- in case some may want to
    >> >>> comment or improve further.
    >> >>>
    >> >>> Martin
    >> 

    > -- 
    > Luke Tierney
    > Ralph E. Wareham Professor of Mathematical Sciences
    > University of Iowa                  Phone:             319-335-3386
    > Department of Statistics and        Fax:               319-335-3017
    > Actuarial Science
    > 241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
    > Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu


From james.f.hester at gmail.com  Tue May 16 20:02:08 2017
From: james.f.hester at gmail.com (Jim Hester)
Date: Tue, 16 May 2017 14:02:08 -0400
Subject: [Rd] Consider increasing the size of HSIZE
Message-ID: <CAD6tx94dcke3ETvbkQh4EqvuvCJKcgxQk3cSkR=fHKyAO03qTA@mail.gmail.com>

The HSIZE constant, which sets the size of the hash table used to
store symbols is currently defined as `#define HSIZE 4119`. This value
was last increased in r5182 on 1999-07-15.

https://github.com/jimhester/hashsize#readme contains a code which
simulates a normal R workflow by loading a handful of packages. In the
example more than 20,000 symbols are included in the hash table,
resulting in a load factor of greater than 5. The histogram in the
linked repository shows the distribution of bucket sizes for the hash
table.

This high load factor means most queries into the hashtable result in
a collision, requiring an additional linear search of the linked list
for each bucket. Is is common for growable hash tables to increase
their size when the load factor is greater than .75, so I think it
would be of benefit to increase the HSIZE constant considerably; to
32768 or possibly 65536. This will result in increased memory
requirements for the hash table, but far fewer collisions.

To get an idea of the performance implications the repository includes
some benchmarks of looking up the first element in a given hash
bucket, and the last element (for buckets over 10 elements long). The
results are somewhat noisy. Because longer symbol names hashing the
name and performing string comparisons to searching the list tends to
dominate the time. But for symbols of similar length there is a 2X-4X
increase in lookup performance between retrieving the first element in
a bucket to retrieving the last (indicated by the `total` column in
the table).

Increasing the size of `HSIZE` seems like a easy way to improve the
performance of an operation that occurs thousands if not millions of
times for every R session, with very limited cost in memory.


From hpages at fredhutch.org  Tue May 16 20:26:28 2017
From: hpages at fredhutch.org (=?UTF-8?B?SGVydsOpIFBhZ8Oocw==?=)
Date: Tue, 16 May 2017 11:26:28 -0700
Subject: [Rd] stopifnot() does not stop at first non-TRUE argument
In-Reply-To: <10E55245-5B37-48FC-A3D0-0B87979DD5D1@gmail.com>
References: <1034131318.560887.1494952665524.ref@mail.yahoo.com>
 <1034131318.560887.1494952665524@mail.yahoo.com>
 <10E55245-5B37-48FC-A3D0-0B87979DD5D1@gmail.com>
Message-ID: <610e50b4-e38e-3339-24f0-91e726c26b4f@fredhutch.org>

On 05/16/2017 09:59 AM, peter dalgaard wrote:
>
>> On 16 May 2017, at 18:37 , Suharto Anggono Suharto Anggono via R-devel <r-devel at r-project.org> wrote:
>>
>> switch(i, ...)
>> extracts 'i'-th argument in '...'. It is like
>> eval(as.name(paste0("..", i))) .
>
> Hey, that's pretty neat!

Indeed! Seems like this topic is even more connected to switch()
than I anticipated...

H.

>
> -pd
>
>>
>> Just mentioning other things:
>> - For 'n',
>> n <- nargs()
>> can be used.
>> - sys.call() can be used in place of match.call() .
>> ---------------------------
>>>>>>> peter dalgaard <pdalgd at gmail.com>
>>>>>>>    on Mon, 15 May 2017 16:28:42 +0200 writes:
>>
>>> I think Herv?'s idea was just that if switch can evaluate arguments selectively, so can stopifnot(). But switch() is .Primitive, so does it from C.
>>
>> if he just meant that, then "yes, of course" (but not so interesting).
>>
>>> I think it is almost a no-brainer to implement a sequential stopifnot if dropping to C code is allowed. In R it gets trickier, but how about this:
>>
>> Something like this, yes, that's close to what Serguei Sokol had proposed
>> (and of course I *do*  want to keep the current sophistication
>> of stopifnot(), so this is really too simple)
>>
>>> Stopifnot <- function(...)
>>> {
>>> n <- length(match.call()) - 1
>>> for (i in 1:n)
>>> {
>>> nm <- as.name(paste0("..",i))
>>> if (!eval(nm)) stop("not all true")
>>> }
>>> }
>>> Stopifnot(2+2==4)
>>> Stopifnot(2+2==5, print("Hey!!!") == "Hey!!!")
>>> Stopifnot(2+2==4, print("Hey!!!") == "Hey!!!")
>>> Stopifnot(T,T,T,T,T,T,T,T,T,T,T,T,T,T,T,T,F,T)
>>
>>
>>>> On 15 May 2017, at 15:37 , Martin Maechler <maechler at stat.math.ethz.ch> wrote:
>>>>
>>>> I'm still curious about Herv?'s idea on using  switch()  for the
>>>> issue.
>>
>>> --
>>> Peter Dalgaard, Professor,
>>> Center for Statistics, Copenhagen Business School
>>> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
>>> Phone: (+45)38153501
>>> Office: A 4.23
>>> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Ddevel&d=DwIGaQ&c=eRAMFD45gAfqt84VtBcfhQ&r=BK7q3XeAvimeWdGbWY_wJYbW0WYiZvSXAJJKaaPhzWA&m=mLJLORFCunDiCafHllurGVVVHiMf85ExkM7B5DngfIk&s=helOsmplADBmY6Ct7r30onNuD8a6GKz6yuSgjPxljeU&e=
>

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fredhutch.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From kirill at altlinux.org  Wed May 17 02:35:39 2017
From: kirill at altlinux.org (Kirill Maslinsky)
Date: Wed, 17 May 2017 03:35:39 +0300
Subject: [Rd] problem running test on a system without /etc/localtime
Message-ID: <20170517003539.GA31461@imap.altlinux.org>

Hi all, 

A problem with tests while building R.

I'm packaging R for Sisyphus repository and package build environment,
by design, doesn't have /etc/localtime file present. This causes failure
with Sys.timeone during test run:

[builder at localhost tests]$ ../bin/R --vanilla < reg-tests-1d.R

> ## PR#17186 - Sys.timezone() on some Debian-derived platforms
> (S.t <- Sys.timezone())
Error in normalizePath("/etc/localtime") : 
  (converted from warning) path[1]="/etc/localtime": No such file or
  directory
  Calls: Sys.timezone -> normalizePath
  Execution halted

This is caused by this code:

> Sys.timezone
function (location = TRUE) 
{
    tz <- Sys.getenv("TZ", names = FALSE)
    if (!location || nzchar(tz)) 
        return(Sys.getenv("TZ", unset = NA_character_))
>>  lt <- normalizePath("/etc/localtime")
[remainder of the code skkipped]

File /etc/loclatime is optional and is not guaranteed to be present on
any platform. And anyway, it is a good idea to first check that file 
exists before calling normalizePath. 

Sure, this can be worked around by setting TZ environment variable, but
that causes tests to fail in another place:

[builder at localhost tests]$ TZ="GMT" ../bin/R --vanilla < reg-tests-1d.R

> ## format()ing invalid hand-constructed  POSIXlt  objects
> d <- as.POSIXlt("2016-12-06"); d$zone <- 1
> tools::assertError(format(d))
Error: Failed to get error in evaluating format(d)
Execution halted

It seems that the best solution will be to patch Sys.timezone.

-- 
KM


From edd at debian.org  Wed May 17 04:57:38 2017
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue, 16 May 2017 21:57:38 -0500
Subject: [Rd] problem running test on a system without /etc/localtime
In-Reply-To: <20170517003539.GA31461@imap.altlinux.org>
References: <20170517003539.GA31461@imap.altlinux.org>
Message-ID: <22811.48162.446462.802006@max.eddelbuettel.com>


On 17 May 2017 at 03:35, Kirill Maslinsky wrote:
| I'm packaging R for Sisyphus repository and package build environment,
| by design, doesn't have /etc/localtime file present. This causes failure
| with Sys.timeone during test run:
[...]
| It seems that the best solution will be to patch Sys.timezone.

The file-based approach was AFAIK never successfully standardized.

Setting a TZ is a defensible fallback.  At some point last year I got so
annoyed about this (and have the historical Debian attitude that a config
file may be preferable to a environment variable [ which I now think is wrong
for some things like TZ ]) I wrote the 'gettz' package.   Quick demo in a
Docker container with nothing set:

edd at max:~$ docker run --rm -ti r-base /bin/bash
root at f3848979cab4:/# echo $TZ
echo $TZ

root at f3848979cab4:/# R
R

R version 3.4.0 (2017-04-21) -- "You Stupid Darkness"
Copyright (C) 2017 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> Sys.getenv("TZ")  # as expected
Sys.getenv("TZ")  # as expected
[1] ""
> install.packages("gettz")
install.packages("gettz")
Installing package into ?/usr/local/lib/R/site-library?
(as ?lib? is unspecified)
trying URL 'https://cran.rstudio.com/src/contrib/gettz_0.0.3.tar.gz'
Content type 'application/x-gzip' length 9064 bytes
==================================================
downloaded 9064 bytes

* installing *source* package ?gettz? ...
** package ?gettz? successfully unpacked and MD5 sums checked
** libs
g++  -I/usr/share/R/include -DNDEBUG      -fpic  -g -O2 -fdebug-prefix-map=/build/r-base-3.4.0=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c gettz.cpp -o gettz.o
g++ -shared -L/usr/lib/R/lib -Wl,-z,relro -o gettz.so gettz.o -L/usr/lib/R/lib -lR
installing to /usr/local/lib/R/site-library/gettz/libs
** R
** preparing package for lazy loading
** help
*** installing help indices
** building package indices
** testing if installed package can be loaded
* DONE (gettz)

The downloaded source packages are in
	?/tmp/RtmpLvuVz8/downloaded_packages?
> gettz::gettz()
gettz::gettz()
[1] "Etc/UTC"
>


As I recall, R got patched for R 3.3.3 or R 3.4.0 to return "" in more cases.
gettz is a little smarter about looking in more locations that R was at the
time (and hence not dissimilar to what was suggested earlier today, but
operates at compiled-code level). It uses a trick I found on StackOverflow
(and which is credited in the package).

It is certainly not perfect, but it is "good enough" for the uses I had in
packages requiring some localtime information.

Dirk

-- 
http://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From henrik.bengtsson at gmail.com  Wed May 17 05:49:02 2017
From: henrik.bengtsson at gmail.com (Henrik Bengtsson)
Date: Tue, 16 May 2017 20:49:02 -0700
Subject: [Rd] problem running test on a system without /etc/localtime
In-Reply-To: <20170517003539.GA31461@imap.altlinux.org>
References: <20170517003539.GA31461@imap.altlinux.org>
Message-ID: <CAFDcVCTOOxBqaL0+m7Y8xHzZsDMVh7ZN1VFpuw9Y+SOXnbPi5A@mail.gmail.com>

On Tue, May 16, 2017 at 5:35 PM, Kirill Maslinsky <kirill at altlinux.org> wrote:
> Hi all,
>
> A problem with tests while building R.
>
> I'm packaging R for Sisyphus repository and package build environment,
> by design, doesn't have /etc/localtime file present. This causes failure
> with Sys.timeone during test run:
>
> [builder at localhost tests]$ ../bin/R --vanilla < reg-tests-1d.R
>
>> ## PR#17186 - Sys.timezone() on some Debian-derived platforms
>> (S.t <- Sys.timezone())
> Error in normalizePath("/etc/localtime") :
>   (converted from warning) path[1]="/etc/localtime": No such file or
>   directory
>   Calls: Sys.timezone -> normalizePath
>   Execution halted
>
> This is caused by this code:
>
>> Sys.timezone
> function (location = TRUE)
> {
>     tz <- Sys.getenv("TZ", names = FALSE)
>     if (!location || nzchar(tz))
>         return(Sys.getenv("TZ", unset = NA_character_))
>>>  lt <- normalizePath("/etc/localtime")
> [remainder of the code skkipped]
>
> File /etc/loclatime is optional and is not guaranteed to be present on
> any platform. And anyway, it is a good idea to first check that file
> exists before calling normalizePath.

Looking at the code
(https://github.com/wch/r-source/blob/R-3-4-branch/src/library/base/R/datetime.R#L26),
could it be that mustWork = FALSE (instead of the default NA) avoids
the warning causes this check error?

Index: src/library/base/R/datetime.R
===================================================================
--- src/library/base/R/datetime.R (revision 72684)
+++ src/library/base/R/datetime.R (working copy)
@@ -23,7 +23,7 @@
 {
     tz <- Sys.getenv("TZ", names = FALSE)
     if(!location || nzchar(tz)) return(Sys.getenv("TZ", unset = NA_character_))
-    lt <- normalizePath("/etc/localtime") # Linux, macOS, ...
+    lt <- normalizePath("/etc/localtime", mustWork = FALSE) # Linux, macOS, ...
     if (grepl(pat <- "^/usr/share/zoneinfo/", lt)) sub(pat, "", lt)
     else if (lt == "/etc/localtime" && file.exists("/etc/timezone") &&
      dir.exists("/usr/share/zoneinfo") &&

/Henrik

>
> Sure, this can be worked around by setting TZ environment variable, but
> that causes tests to fail in another place:
>
> [builder at localhost tests]$ TZ="GMT" ../bin/R --vanilla < reg-tests-1d.R
>
>> ## format()ing invalid hand-constructed  POSIXlt  objects
>> d <- as.POSIXlt("2016-12-06"); d$zone <- 1
>> tools::assertError(format(d))
> Error: Failed to get error in evaluating format(d)
> Execution halted
>
> It seems that the best solution will be to patch Sys.timezone.
>
> --
> KM
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From maechler at stat.math.ethz.ch  Wed May 17 11:29:16 2017
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 17 May 2017 11:29:16 +0200
Subject: [Rd] problem running test on a system without /etc/localtime
In-Reply-To: <CAFDcVCTOOxBqaL0+m7Y8xHzZsDMVh7ZN1VFpuw9Y+SOXnbPi5A@mail.gmail.com>
References: <20170517003539.GA31461@imap.altlinux.org>
 <CAFDcVCTOOxBqaL0+m7Y8xHzZsDMVh7ZN1VFpuw9Y+SOXnbPi5A@mail.gmail.com>
Message-ID: <22812.6124.879696.821362@stat.math.ethz.ch>

>>>>> Henrik Bengtsson <henrik.bengtsson at gmail.com>
>>>>>     on Tue, 16 May 2017 20:49:02 -0700 writes:

    > On Tue, May 16, 2017 at 5:35 PM, Kirill Maslinsky <kirill at altlinux.org> wrote:
    >> Hi all,
    >> 
    >> A problem with tests while building R.
    >> 
    >> I'm packaging R for Sisyphus repository and package build environment,
    >> by design, doesn't have /etc/localtime file present. This causes failure
    >> with Sys.timeone during test run:
    >> 
    >> [builder at localhost tests]$ ../bin/R --vanilla < reg-tests-1d.R
    >> 
    >>> ## PR#17186 - Sys.timezone() on some Debian-derived platforms
    >>> (S.t <- Sys.timezone())
    >> Error in normalizePath("/etc/localtime") :
    >> (converted from warning) path[1]="/etc/localtime": No such file or
    >> directory
    >> Calls: Sys.timezone -> normalizePath
    >> Execution halted
    >> 
    >> This is caused by this code:
    >> 
    >>> Sys.timezone
    >> function (location = TRUE)
    >> {
    >> tz <- Sys.getenv("TZ", names = FALSE)
    >> if (!location || nzchar(tz))
    >> return(Sys.getenv("TZ", unset = NA_character_))
    >>>> lt <- normalizePath("/etc/localtime")
    >> [remainder of the code skkipped]
    >> 
    >> File /etc/loclatime is optional and is not guaranteed to be present on
    >> any platform. And anyway, it is a good idea to first check that file
    >> exists before calling normalizePath.

    > Looking at the code
    > (https://github.com/wch/r-source/blob/R-3-4-branch/src/library/base/R/datetime.R#L26),
    > could it be that mustWork = FALSE (instead of the default NA) avoids
    > the warning causes this check error?

Good idea.

Kirill, could you apply the minimal patch to the sources and
report back ?


    > Index: src/library/base/R/datetime.R
    > ===================================================================
    > --- src/library/base/R/datetime.R (revision 72684)
    > +++ src/library/base/R/datetime.R (working copy)
    > @@ -23,7 +23,7 @@
    > {
    > tz <- Sys.getenv("TZ", names = FALSE)
    > if(!location || nzchar(tz)) return(Sys.getenv("TZ", unset = NA_character_))
    > -    lt <- normalizePath("/etc/localtime") # Linux, macOS, ...
    > +    lt <- normalizePath("/etc/localtime", mustWork = FALSE) # Linux, macOS, ...
    > if (grepl(pat <- "^/usr/share/zoneinfo/", lt)) sub(pat, "", lt)
    > else if (lt == "/etc/localtime" && file.exists("/etc/timezone") &&
    > dir.exists("/usr/share/zoneinfo") &&

    > /Henrik

    >> 
    >> Sure, this can be worked around by setting TZ environment variable, but
    >> that causes tests to fail in another place:
    >> 
    >> [builder at localhost tests]$ TZ="GMT" ../bin/R --vanilla < reg-tests-1d.R
    >> 
    >>> ## format()ing invalid hand-constructed  POSIXlt  objects
    >>> d <- as.POSIXlt("2016-12-06"); d$zone <- 1
    >>> tools::assertError(format(d))
    >> Error: Failed to get error in evaluating format(d)
    >> Execution halted
    >> 
    >> It seems that the best solution will be to patch Sys.timezone.
    >> 
    >> --
    >> KM
    >> 
    >> ______________________________________________
    >> R-devel at r-project.org mailing list
    >> https://stat.ethz.ch/mailman/listinfo/r-devel

    > ______________________________________________
    > R-devel at r-project.org mailing list
    > https://stat.ethz.ch/mailman/listinfo/r-devel


From JTELLERIA at external.gamesacorp.com  Wed May 17 09:50:23 2017
From: JTELLERIA at external.gamesacorp.com (TELLERIA RUIZ DE AGUIRRE, JUAN)
Date: Wed, 17 May 2017 07:50:23 +0000
Subject: [Rd] SUGGESTION: R Base Packages
Message-ID: <C7D958509BBBF44A8F8F9CBE5BE9400764C69570@EXDG04.usr.corp.gamesa.es>

Dear R Developers,

I am writing for doing the following suggestions:


*         Bugs:

o   Add in Bugzilla an Area in order to request membership for bug reporting, which shall itself include another area to introduce a little dissertation for justifying "Why someone would like to be a member of Bugzilla" for being able to report proper bugs, and avoid spam.

o   Now, new R serious members cannot report properly bugs.



*         That, in the R Base distribution, the following packages are included as default (pre-installed):

o   ggplot2.

o   dplyr.

o   tidyr.



As in 2017, these are essential, in my opinion, in R programming.


*         And that, as regards R Project:

o   Improve the CRAN & R Project web pages, giving them a more modern look and feel (Such as the Python.org webpage), in order to "sell" better R to the everybody.

o   Improve the basic GUI distributed with R, with some of the core features of RStudio Desktop (GPL). Or even make RStudio Desktop to replace the R default GUI, with another name, in its executable version.

o   Enable a Suggestion web page / forum in the R Project webpage in order to boost core R Open Innovation.

Thank you all.



	[[alternative HTML version deleted]]


From frederik at ofb.net  Wed May 17 21:57:22 2017
From: frederik at ofb.net (frederik at ofb.net)
Date: Wed, 17 May 2017 12:57:22 -0700
Subject: [Rd] SUGGESTION: R Base Packages
In-Reply-To: <C7D958509BBBF44A8F8F9CBE5BE9400764C69570@EXDG04.usr.corp.gamesa.es>
References: <C7D958509BBBF44A8F8F9CBE5BE9400764C69570@EXDG04.usr.corp.gamesa.es>
Message-ID: <20170517195722.GM25437@ofb.net>

Hello!

Thanks to Juan for his suggestions. I would like to voice my opinion
for and against some of these.

> o   Add in Bugzilla an Area in order to request membership for bug reporting, which shall itself include another area to introduce a little dissertation for justifying "Why someone would like to be a member of Bugzilla" for being able to report proper bugs, and avoid spam.
> 
> o   Now, new R serious members cannot report properly bugs.

I too was annoyed by the state of the bug reporting system when I
first started using R, and that was even before automatic account
creation was disabled. I think the situation could be improved. I
don't think it's a huge burden to tell bug reporters to ask for an
account via the R-devel mailing list. But from what I can see the bug
tracker doesn't make it clear that this has to be done. We should at
least post a message on the main page explaining how people can create
an account. This would be less work than what Juan is suggesting, but
I think sufficient for our needs.

> *         That, in the R Base distribution, the following packages are included as default (pre-installed):
> 
> o   ggplot2.
> 
> o   dplyr.
> 
> o   tidyr.

I think this is a terrible idea. I've only been using R for a few
years and while I find these "tidyverse" packages interesting and use
them on occasion, I've also concluded that they change too quickly to
be used in code that I want to stay working for a long time. They are
largely based on experimental concepts. Being able to change and
evolve is part of the strength of these packages. So I think putting
them in the R Base distribution would be bad for all parties.

> *         And that, as regards R Project:
> 
> o   Improve the CRAN & R Project web pages, giving them a more modern look and feel (Such as the Python.org webpage), in order to "sell" better R to the everybody.
> 
> o   Improve the basic GUI distributed with R, with some of the core features of RStudio Desktop (GPL). Or even make RStudio Desktop to replace the R default GUI, with another name, in its executable version.
> 
> o   Enable a Suggestion web page / forum in the R Project webpage in order to boost core R Open Innovation.

I think the R project web page looks great. It's simple and it loads
quickly and doesn't try to mesmerize people. I like R's command line
interface. It completes on symbols and files and I can easily use it
with my favorite editor and run it in the terminal of my choice. I
don't think that more effort should be put into developing bloated
GUIs which try to enforce a standard way of interacting with R.

I think a "forum" or bulletin board system would be a detraction from
the project and a distraction for the project leaders. Users have
Stack Exchange - it's better than any forum we could create, and it
takes care of itself.

That's my two cents, more or less.

Thanks,

Frederick


From kirill at altlinux.org  Wed May 17 20:01:59 2017
From: kirill at altlinux.org (Kirill Maslinsky)
Date: Wed, 17 May 2017 21:01:59 +0300
Subject: [Rd] problem running test on a system without /etc/localtime
In-Reply-To: <22812.6124.879696.821362@stat.math.ethz.ch>
References: <20170517003539.GA31461@imap.altlinux.org>
 <CAFDcVCTOOxBqaL0+m7Y8xHzZsDMVh7ZN1VFpuw9Y+SOXnbPi5A@mail.gmail.com>
 <22812.6124.879696.821362@stat.math.ethz.ch>
Message-ID: <20170517180159.GA12344@imap.altlinux.org>

On Wed, May 17, 2017 at 11:29:16AM +0200, Martin Maechler wrote:

[...]

> 
> 
>     > Index: src/library/base/R/datetime.R
>     > ===================================================================
>     > --- src/library/base/R/datetime.R (revision 72684)
>     > +++ src/library/base/R/datetime.R (working copy)
>     > @@ -23,7 +23,7 @@
>     > {
>     > tz <- Sys.getenv("TZ", names = FALSE)
>     > if(!location || nzchar(tz)) return(Sys.getenv("TZ", unset = NA_character_))
>     > -    lt <- normalizePath("/etc/localtime") # Linux, macOS, ...
>     > +    lt <- normalizePath("/etc/localtime", mustWork = FALSE) # Linux, macOS, ...
>     > if (grepl(pat <- "^/usr/share/zoneinfo/", lt)) sub(pat, "", lt)
>     > else if (lt == "/etc/localtime" && file.exists("/etc/timezone") &&
>     > dir.exists("/usr/share/zoneinfo") &&

Done, the patch successfully fixes warning but the test failed in the other way:

$ ../bin/R --vanilla < reg-tests-1d.R

> ## PR#17186 - Sys.timezone() on some Debian-derived platforms
> (S.t <- Sys.timezone())
[1] NA
> if(is.na(S.t) || !nzchar(S.t)) stop("could not get timezone")
Error: could not get timezone
Execution halted

Which is technically correct, timezone is not set on the system, but date
utility assumes UTC in this case:

$ date +%Z
UTC

I'm not sure if R should do the same. 


Probably, setting TZ environment variable is the right way to go in this
build environment, but then another failure should be fixed:

>     >> Sure, this can be worked around by setting TZ environment variable, but
>     >> that causes tests to fail in another place:
>     >> 
>     >> [builder at localhost tests]$ TZ="GMT" ../bin/R --vanilla < reg-tests-1d.R
>     >> 
>     >>> ## format()ing invalid hand-constructed  POSIXlt  objects
>     >>> d <- as.POSIXlt("2016-12-06"); d$zone <- 1
>     >>> tools::assertError(format(d))
>     >> Error: Failed to get error in evaluating format(d)
>     >> Execution halted

-- 
KM


From maechler at stat.math.ethz.ch  Thu May 18 10:03:28 2017
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 18 May 2017 10:03:28 +0200
Subject: [Rd] stopifnot() does not stop at first non-TRUE argument
In-Reply-To: <1034131318.560887.1494952665524@mail.yahoo.com>
References: <1034131318.560887.1494952665524.ref@mail.yahoo.com>
 <1034131318.560887.1494952665524@mail.yahoo.com>
Message-ID: <22813.21840.123266.967166@stat.math.ethz.ch>

>>>>> Suharto Anggono Suharto Anggono via R-devel <r-devel at r-project.org>
>>>>>     on Tue, 16 May 2017 16:37:45 +0000 writes:

    > switch(i, ...)
    > extracts 'i'-th argument in '...'. It is like
    > eval(as.name(paste0("..", i))) .

Yes, that's neat.

It is only almost the same:  in the case of illegal 'i'
the switch() version returns
    invisible(NULL)

whereas the version we'd want should signal an error, typically
the same error message as

  > t2 <- function(...) ..2
  > t2(1)
  Error in t2(1) (from #1) : the ... list does not contain 2 elements
  > 


    > Just mentioning other things:
    > - For 'n',
    > n <- nargs()
    > can be used.

I know .. [in this case, where '...' is the only formal argument of the function]

    > - sys.call() can be used in place of match.call() .

Hmm... in many cases, yes.... notably, as we do *not* want the
argument names here, I think you are right.


    > ---------------------------
>>>>> peter dalgaard <pdalgd at gmail.com>
>>>>>     on Mon, 15 May 2017 16:28:42 +0200 writes:

    >> I think Herv?'s idea was just that if switch can evaluate arguments selectively, so can stopifnot(). But switch() is .Primitive, so does it from C. 

    > if he just meant that, then "yes, of course" (but not so interesting).

    >> I think it is almost a no-brainer to implement a sequential stopifnot if dropping to C code is allowed. In R it gets trickier, but how about this:

    > Something like this, yes, that's close to what Serguei Sokol had proposed
    > (and of course I *do*  want to keep the current sophistication
    > of stopifnot(), so this is really too simple)

    >> Stopifnot <- function(...)
    >> {
    >> n <- length(match.call()) - 1
    >> for (i in 1:n)
    >> {
    >> nm <- as.name(paste0("..",i))
    >> if (!eval(nm)) stop("not all true")
    >> }
    >> }
    >> Stopifnot(2+2==4)
    >> Stopifnot(2+2==5, print("Hey!!!") == "Hey!!!")
    >> Stopifnot(2+2==4, print("Hey!!!") == "Hey!!!")
    >> Stopifnot(T,T,T,T,T,T,T,T,T,T,T,T,T,T,T,T,F,T)


    >>> On 15 May 2017, at 15:37 , Martin Maechler <maechler at stat.math.ethz.ch> wrote:
    >>> 
    >>> I'm still curious about Herv?'s idea on using  switch()  for the
    >>> issue.

    >> -- 
    >> Peter Dalgaard, Professor,
    >> Center for Statistics, Copenhagen Business School
    >> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
    >> Phone: (+45)38153501
    >> Office: A 4.23
    >> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com

    > ______________________________________________
    > R-devel at r-project.org mailing list
    > https://stat.ethz.ch/mailman/listinfo/r-devel


From p_connolly at slingshot.co.nz  Thu May 18 11:00:16 2017
From: p_connolly at slingshot.co.nz (Patrick Connolly)
Date: Thu, 18 May 2017 21:00:16 +1200
Subject: [Rd] [R] R-3.4.0 fails test
In-Reply-To: <ACA47D66-F469-4C75-8521-CB0DD76572C7@gmail.com>
References: <20170517074216.GA4553@slingshot.co.nz>
 <4a086fbb-61ed-d3a5-ce23-14bbaea5531f@gmail.com>
 <ACA47D66-F469-4C75-8521-CB0DD76572C7@gmail.com>
Message-ID: <20170518090016.GB4553@slingshot.co.nz>

On Wed, 17-May-2017 at 01:21PM +0200, Peter Dalgaard wrote:

|> 
|> Anyways, you might want to 
|> 
|> a) move the discussion to R-devel
|> b) include your platform (hardware, OS) and time zone info

System:    Host: MTA-V1-427894 Kernel: 3.19.0-32-generic x86_64 (64 bit gcc: 4.8.2)
           Desktop: KDE Plasma 4.14.2 (Qt 4.8.6) Distro: Linux Mint 17.3 Rosa
Machine:   System: innotek product: VirtualBox v: 1.2
           Mobo: Oracle model: VirtualBox v: 1.2 Bios: innotek v: VirtualBox date: 12/01/2006
CPU:       Quad core Intel Core i7-4790 (-MCP-) cache: 8192 KB
           flags: (lm nx sse sse2 sse3 sse4_1 sse4_2 ssse3) bmips: 28734
           clock speeds: max: 3591 MHz 1: 3591 MHz 2: 3591 MHz 3: 3591 MHz 4: 3591 MHz
Graphics:  Card: InnoTek Systemberatung VirtualBox Graphics Adapter bus-ID: 00:02.0
           Display Server: X.Org 1.17.1 drivers: vboxvideo (unloaded: fbdev,vesa) Resolution: 1920x1080 at 60.0hz
           GLX Renderer: Chromium GLX Version: 2.1 Chromium 1.9 Direct Rendering: Yes
Audio:     Card Intel 82801AA AC'97 Audio Controller driver: snd_intel8x0 ports: d100 d200 bus-ID: 00:05.0
           Sound: Advanced Linux Sound Architecture v: k3.19.0-32-generic
Network:   Card: Intel 82540EM Gigabit Ethernet Controller
           driver: e1000 v: 7.3.21-k8-NAPI port: d240 bus-ID: 00:08.0
           IF: eth0 state: up speed: 1000 Mbps duplex: full mac: <filter>
Drives:    HDD Total Size: 131.7GB (56.5% used) ID-1: /dev/sda model: VBOX_HARDDISK size: 131.7GB
Partition: ID-1: / size: 117G used: 66G (60%) fs: ext4 dev: /dev/sda1
           ID-2: swap-1 size: 4.16GB used: 0.00GB (0%) fs: swap dev: /dev/sda5
RAID:      No RAID devices: /proc/mdstat, md_mod kernel module present
Sensors:   None detected - is lm-sensors installed and configured?
Info:      Processes: 256 Uptime: 17:01 Memory: 3533.8/14955.3MB Init: Upstart runlevel: 2 Gcc sys: 4.8.4
           Client: Shell (bash 4.3.111) inxi: 2.2.28 


Time zone: NZST

|> c) run the offending code lines "by hand" and show us the values of format(dlt) and format(dct) so we can see what the problem is, something like
|> 
|> dlt <- structure(
|>     list(sec = 52, min = 59L, hour = 18L, mday = 6L, mon = 11L, year = 116L,
|>        wday = 2L, yday = 340L, isdst = 0L, zone = "CET", gmtoff = 3600L),
|>        class = c("POSIXlt", "POSIXt"), tzone = c("", "CET", "CEST"))
|> dlt$sec <- 10000 + 1:10 
|> dct <- as.POSIXct(dlt)
|> cbind(format(dlt), format(dct))

> cbind(format(dlt), format(dct))
      [,1]                  [,2]                 
 [1,] "2016-12-06 21:45:41" "2016-12-06 22:45:41"
 [2,] "2016-12-06 21:45:42" "2016-12-06 22:45:42"
 [3,] "2016-12-06 21:45:43" "2016-12-06 22:45:43"
 [4,] "2016-12-06 21:45:44" "2016-12-06 22:45:44"
 [5,] "2016-12-06 21:45:45" "2016-12-06 22:45:45"
 [6,] "2016-12-06 21:45:46" "2016-12-06 22:45:46"
 [7,] "2016-12-06 21:45:47" "2016-12-06 22:45:47"
 [8,] "2016-12-06 21:45:48" "2016-12-06 22:45:48"
 [9,] "2016-12-06 21:45:49" "2016-12-06 22:45:49"
[10,] "2016-12-06 21:45:50" "2016-12-06 22:45:50"
> 


-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.   
   ___    Patrick Connolly   
 {~._.~}                   Great minds discuss ideas    
 _( Y )_  	         Average minds discuss events 
(:_~*~_:)                  Small minds discuss people  
 (_)-(_)  	                      ..... Eleanor Roosevelt
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From JTELLERIA at external.gamesacorp.com  Thu May 18 09:18:01 2017
From: JTELLERIA at external.gamesacorp.com (TELLERIA RUIZ DE AGUIRRE, JUAN)
Date: Thu, 18 May 2017 07:18:01 +0000
Subject: [Rd] SUGGESTION: R Base Packages
In-Reply-To: <20170517195722.GM25437@ofb.net>
References: <C7D958509BBBF44A8F8F9CBE5BE9400764C69570@EXDG04.usr.corp.gamesa.es>
 <20170517195722.GM25437@ofb.net>
Message-ID: <C7D958509BBBF44A8F8F9CBE5BE9400764C695CA@EXDG04.usr.corp.gamesa.es>

Thank you Frederick for your comments:

They are really well justified.

> I think a "forum" or bulletin board system would be a detraction from the project and a distraction for the project leaders. Users have Stack Exchange - it's better than any forum we could create, and it 
> takes care of itself.

An excellent idea would be to add in the R Project Webpage a link to RSeek, in order to put all together. This can be done in a fast and easy way.

> I too was annoyed by the state of the bug reporting system when I first started using R, and that was even before automatic account creation was disabled. I think the situation could be improved. 
> I don't think it's a huge burden to tell bug reporters to ask for an account via the R-devel mailing list. But from what I can see the bug tracker doesn't make it clear that this has to be done. We should at 
> least post a message on the main page explaining how people can create an account. This would be less work than what Juan is suggesting, but I think sufficient for our needs.

Yes, clarifying in the RProject webpage that accounts could be requested for bug reporting in Bugzilla would be a great news, but it shall be really good justified.

> I think this is a terrible idea. I've only been using R for a few years and while I find these "tidyverse" packages interesting and use them on occasion, I've also concluded that they change too quickly to 
> be used in code that I want to stay working for a long time. They are largely based on experimental concepts. Being able to change and evolve is part of the strength of these packages. So I think putting 
> them in the R Base distribution would be bad for all parties.

Totally agree.

> I think the R project web page looks great. It's simple and it loads quickly and doesn't try to mesmerize people. I like R's command line interface. It completes on symbols and files and I can easily use it 
> with my favorite editor and run it in the terminal of my choice. I don't think that more effort should be put into developing bloated GUIs which try to enforce a standard way of interacting with R.

I agree, but, in my opinion,  the RGUI icons  shall have a little bit more modern look. Just a bit.

Thank you,
Juan


From pdalgd at gmail.com  Thu May 18 13:19:35 2017
From: pdalgd at gmail.com (peter dalgaard)
Date: Thu, 18 May 2017 13:19:35 +0200
Subject: [Rd] [R] R-3.4.0 fails test
In-Reply-To: <20170518090016.GB4553@slingshot.co.nz>
References: <20170517074216.GA4553@slingshot.co.nz>
 <4a086fbb-61ed-d3a5-ce23-14bbaea5531f@gmail.com>
 <ACA47D66-F469-4C75-8521-CB0DD76572C7@gmail.com>
 <20170518090016.GB4553@slingshot.co.nz>
Message-ID: <29540304-FC8B-4B49-B94B-E191DEFA3BCB@gmail.com>


> On 18 May 2017, at 11:00 , Patrick Connolly <p_connolly at slingshot.co.nz> wrote:
> 
> On Wed, 17-May-2017 at 01:21PM +0200, Peter Dalgaard wrote:
> 
> |> 
> |> Anyways, you might want to 
> |> 
> |> a) move the discussion to R-devel
> |> b) include your platform (hardware, OS) and time zone info
> 
> System:    Host: MTA-V1-427894 Kernel: 3.19.0-32-generic x86_64 (64 bit gcc: 4.8.2)
>           Desktop: KDE Plasma 4.14.2 (Qt 4.8.6) Distro: Linux Mint 17.3 Rosa

I suppose that'll do...


> Time zone: NZST



> 
> |> c) run the offending code lines "by hand" and show us the values of format(dlt) and format(dct) so we can see what the problem is, something like
> |> 
> |> dlt <- structure(
> |>     list(sec = 52, min = 59L, hour = 18L, mday = 6L, mon = 11L, year = 116L,
> |>        wday = 2L, yday = 340L, isdst = 0L, zone = "CET", gmtoff = 3600L),
> |>        class = c("POSIXlt", "POSIXt"), tzone = c("", "CET", "CEST"))
> |> dlt$sec <- 10000 + 1:10 
> |> dct <- as.POSIXct(dlt)
> |> cbind(format(dlt), format(dct))
> 
>> cbind(format(dlt), format(dct))
>      [,1]                  [,2]                 
> [1,] "2016-12-06 21:45:41" "2016-12-06 22:45:41"
> [2,] "2016-12-06 21:45:42" "2016-12-06 22:45:42"
> [3,] "2016-12-06 21:45:43" "2016-12-06 22:45:43"
> [4,] "2016-12-06 21:45:44" "2016-12-06 22:45:44"
> [5,] "2016-12-06 21:45:45" "2016-12-06 22:45:45"
> [6,] "2016-12-06 21:45:46" "2016-12-06 22:45:46"
> [7,] "2016-12-06 21:45:47" "2016-12-06 22:45:47"
> [8,] "2016-12-06 21:45:48" "2016-12-06 22:45:48"
> [9,] "2016-12-06 21:45:49" "2016-12-06 22:45:49"
> [10,] "2016-12-06 21:45:50" "2016-12-06 22:45:50"
>> 
> 


So exactly 1 hour out of whack. Is there a Daylight Saving Times issue, perchance?

-pd


> 
> -- 
> ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.   
>   ___    Patrick Connolly   
> {~._.~}                   Great minds discuss ideas    
> _( Y )_  	         Average minds discuss events 
> (:_~*~_:)                  Small minds discuss people  
> (_)-(_)  	                      ..... Eleanor Roosevelt
> 	  
> ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From jorismeys at gmail.com  Thu May 18 13:45:16 2017
From: jorismeys at gmail.com (Joris Meys)
Date: Thu, 18 May 2017 13:45:16 +0200
Subject: [Rd] [R] R-3.4.0 fails test
In-Reply-To: <29540304-FC8B-4B49-B94B-E191DEFA3BCB@gmail.com>
References: <20170517074216.GA4553@slingshot.co.nz>
 <4a086fbb-61ed-d3a5-ce23-14bbaea5531f@gmail.com>
 <ACA47D66-F469-4C75-8521-CB0DD76572C7@gmail.com>
 <20170518090016.GB4553@slingshot.co.nz>
 <29540304-FC8B-4B49-B94B-E191DEFA3BCB@gmail.com>
Message-ID: <CAO1zAVarefFWaW_UmN6gCN3ppVuaGhLO7ErYdpUJmbfJ6rb4XQ@mail.gmail.com>

This has to do with your own timezone. If I run that code on my computer,
both formats are correct. If I do this after

Sys.setenv(TZ = "UTC")

Then:

> cbind(format(dlt), format(dct))
      [,1]                  [,2]
 [1,] "2016-12-06 21:45:41" "2016-12-06 20:45:41"
 [2,] "2016-12-06 21:45:42" "2016-12-06 20:45:42"

The reason for that, is that dlt has a timezone set, but dct doesn't. To be
correct, it only takes the first value "", which indicates "Use the default
timezone of the locale".

> attr(dlt, "tzone")
[1] ""     "CET"  "CEST"
> attr(dct, "tzone")
[1] ""

The thing is, in POSIXlt the timezone attribute is stored together with the
actual values for hour, minute etc. in list format. Changing the timezone
doesn't change those values, but it will change the time itself:

> Sys.unsetenv("TZ")
> dlt2 <- dlt
> attr(dlt2,"tzone") <- "UTC"
> dlt2
 [1] "2016-12-06 21:45:41 UTC" "2016-12-06 21:45:42 UTC"
 [3] "2016-12-06 21:45:43 UTC" "2016-12-06 21:45:44 UTC"

in POSIXct the value doesn't change either, just the attribute. But this
value is the number of seconds since the origin. So the time itself doesn't
change, but you'll see a different hour.

> dct
 [1] "2016-12-06 21:45:41 CET" "2016-12-06 21:45:42 CET"
...
> attr(dct,"tzone") <- "UTC"
> dct
 [1] "2016-12-06 20:45:41 UTC" "2016-12-06 20:45:42 UTC"
 [3] "2016-12-06 20:45:43 UTC" "2016-12-06 20:45:44 UTC"

So what you see, is simply the result of your timezone settings on your
computer.

Cheers
Joris

On Thu, May 18, 2017 at 1:19 PM, peter dalgaard <pdalgd at gmail.com> wrote:

>
> > On 18 May 2017, at 11:00 , Patrick Connolly <p_connolly at slingshot.co.nz>
> wrote:
> >
> > On Wed, 17-May-2017 at 01:21PM +0200, Peter Dalgaard wrote:
> >
> > |>
> > |> Anyways, you might want to
> > |>
> > |> a) move the discussion to R-devel
> > |> b) include your platform (hardware, OS) and time zone info
> >
> > System:    Host: MTA-V1-427894 Kernel: 3.19.0-32-generic x86_64 (64 bit
> gcc: 4.8.2)
> >           Desktop: KDE Plasma 4.14.2 (Qt 4.8.6) Distro: Linux Mint 17.3
> Rosa
>
> I suppose that'll do...
>
>
> > Time zone: NZST
>
>
>
> >
> > |> c) run the offending code lines "by hand" and show us the values of
> format(dlt) and format(dct) so we can see what the problem is, something
> like
> > |>
> > |> dlt <- structure(
> > |>     list(sec = 52, min = 59L, hour = 18L, mday = 6L, mon = 11L, year
> = 116L,
> > |>        wday = 2L, yday = 340L, isdst = 0L, zone = "CET", gmtoff =
> 3600L),
> > |>        class = c("POSIXlt", "POSIXt"), tzone = c("", "CET", "CEST"))
> > |> dlt$sec <- 10000 + 1:10
> > |> dct <- as.POSIXct(dlt)
> > |> cbind(format(dlt), format(dct))
> >
> >> cbind(format(dlt), format(dct))
> >      [,1]                  [,2]
> > [1,] "2016-12-06 21:45:41" "2016-12-06 22:45:41"
> > [2,] "2016-12-06 21:45:42" "2016-12-06 22:45:42"
> > [3,] "2016-12-06 21:45:43" "2016-12-06 22:45:43"
> > [4,] "2016-12-06 21:45:44" "2016-12-06 22:45:44"
> > [5,] "2016-12-06 21:45:45" "2016-12-06 22:45:45"
> > [6,] "2016-12-06 21:45:46" "2016-12-06 22:45:46"
> > [7,] "2016-12-06 21:45:47" "2016-12-06 22:45:47"
> > [8,] "2016-12-06 21:45:48" "2016-12-06 22:45:48"
> > [9,] "2016-12-06 21:45:49" "2016-12-06 22:45:49"
> > [10,] "2016-12-06 21:45:50" "2016-12-06 22:45:50"
> >>
> >
>
>
> So exactly 1 hour out of whack. Is there a Daylight Saving Times issue,
> perchance?
>
> -pd
>
>
> >
> > --
> > ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.
> >   ___    Patrick Connolly
> > {~._.~}                   Great minds discuss ideas
> > _( Y )_                Average minds discuss events
> > (:_~*~_:)                  Small minds discuss people
> > (_)-(_)                             ..... Eleanor Roosevelt
> >
> > ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>



-- 
Joris Meys
Statistical consultant

Ghent University
Faculty of Bioscience Engineering
Department of Mathematical Modelling, Statistics and Bio-Informatics

tel :  +32 (0)9 264 61 79
Joris.Meys at Ugent.be
-------------------------------
Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php

	[[alternative HTML version deleted]]


From jorismeys at gmail.com  Thu May 18 13:47:28 2017
From: jorismeys at gmail.com (Joris Meys)
Date: Thu, 18 May 2017 13:47:28 +0200
Subject: [Rd] [R] R-3.4.0 fails test
In-Reply-To: <CAO1zAVarefFWaW_UmN6gCN3ppVuaGhLO7ErYdpUJmbfJ6rb4XQ@mail.gmail.com>
References: <20170517074216.GA4553@slingshot.co.nz>
 <4a086fbb-61ed-d3a5-ce23-14bbaea5531f@gmail.com>
 <ACA47D66-F469-4C75-8521-CB0DD76572C7@gmail.com>
 <20170518090016.GB4553@slingshot.co.nz>
 <29540304-FC8B-4B49-B94B-E191DEFA3BCB@gmail.com>
 <CAO1zAVarefFWaW_UmN6gCN3ppVuaGhLO7ErYdpUJmbfJ6rb4XQ@mail.gmail.com>
Message-ID: <CAO1zAVb2T3roWLU4FxhvjxJz31wRp8oYtCeHc2e8V8KOprP1DA@mail.gmail.com>

Correction: Also dlt uses the default timezone, but POSIXlt is not
recalculated whereas POSIXct is. Reason for that is the different way
values are stored (hours, minutes, seconds as opposed to minutes from
origin, as explained in my previous mail)

CHeers
Joris

On Thu, May 18, 2017 at 1:45 PM, Joris Meys <jorismeys at gmail.com> wrote:

> This has to do with your own timezone. If I run that code on my computer,
> both formats are correct. If I do this after
>
> Sys.setenv(TZ = "UTC")
>
> Then:
>
> > cbind(format(dlt), format(dct))
>       [,1]                  [,2]
>  [1,] "2016-12-06 21:45:41" "2016-12-06 20:45:41"
>  [2,] "2016-12-06 21:45:42" "2016-12-06 20:45:42"
>
> The reason for that, is that dlt has a timezone set, but dct doesn't. To
> be correct, it only takes the first value "", which indicates "Use the
> default timezone of the locale".
>
> > attr(dlt, "tzone")
> [1] ""     "CET"  "CEST"
> > attr(dct, "tzone")
> [1] ""
>
> The thing is, in POSIXlt the timezone attribute is stored together with
> the actual values for hour, minute etc. in list format. Changing the
> timezone doesn't change those values, but it will change the time itself:
>
> > Sys.unsetenv("TZ")
> > dlt2 <- dlt
> > attr(dlt2,"tzone") <- "UTC"
> > dlt2
>  [1] "2016-12-06 21:45:41 UTC" "2016-12-06 21:45:42 UTC"
>  [3] "2016-12-06 21:45:43 UTC" "2016-12-06 21:45:44 UTC"
>
> in POSIXct the value doesn't change either, just the attribute. But this
> value is the number of seconds since the origin. So the time itself doesn't
> change, but you'll see a different hour.
>
> > dct
>  [1] "2016-12-06 21:45:41 CET" "2016-12-06 21:45:42 CET"
> ...
> > attr(dct,"tzone") <- "UTC"
> > dct
>  [1] "2016-12-06 20:45:41 UTC" "2016-12-06 20:45:42 UTC"
>  [3] "2016-12-06 20:45:43 UTC" "2016-12-06 20:45:44 UTC"
>
> So what you see, is simply the result of your timezone settings on your
> computer.
>
> Cheers
> Joris
>
> On Thu, May 18, 2017 at 1:19 PM, peter dalgaard <pdalgd at gmail.com> wrote:
>
>>
>> > On 18 May 2017, at 11:00 , Patrick Connolly <p_connolly at slingshot.co.nz>
>> wrote:
>> >
>> > On Wed, 17-May-2017 at 01:21PM +0200, Peter Dalgaard wrote:
>> >
>> > |>
>> > |> Anyways, you might want to
>> > |>
>> > |> a) move the discussion to R-devel
>> > |> b) include your platform (hardware, OS) and time zone info
>> >
>> > System:    Host: MTA-V1-427894 Kernel: 3.19.0-32-generic x86_64 (64 bit
>> gcc: 4.8.2)
>> >           Desktop: KDE Plasma 4.14.2 (Qt 4.8.6) Distro: Linux Mint 17.3
>> Rosa
>>
>> I suppose that'll do...
>>
>>
>> > Time zone: NZST
>>
>>
>>
>> >
>> > |> c) run the offending code lines "by hand" and show us the values of
>> format(dlt) and format(dct) so we can see what the problem is, something
>> like
>> > |>
>> > |> dlt <- structure(
>> > |>     list(sec = 52, min = 59L, hour = 18L, mday = 6L, mon = 11L, year
>> = 116L,
>> > |>        wday = 2L, yday = 340L, isdst = 0L, zone = "CET", gmtoff =
>> 3600L),
>> > |>        class = c("POSIXlt", "POSIXt"), tzone = c("", "CET", "CEST"))
>> > |> dlt$sec <- 10000 + 1:10
>> > |> dct <- as.POSIXct(dlt)
>> > |> cbind(format(dlt), format(dct))
>> >
>> >> cbind(format(dlt), format(dct))
>> >      [,1]                  [,2]
>> > [1,] "2016-12-06 21:45:41" "2016-12-06 22:45:41"
>> > [2,] "2016-12-06 21:45:42" "2016-12-06 22:45:42"
>> > [3,] "2016-12-06 21:45:43" "2016-12-06 22:45:43"
>> > [4,] "2016-12-06 21:45:44" "2016-12-06 22:45:44"
>> > [5,] "2016-12-06 21:45:45" "2016-12-06 22:45:45"
>> > [6,] "2016-12-06 21:45:46" "2016-12-06 22:45:46"
>> > [7,] "2016-12-06 21:45:47" "2016-12-06 22:45:47"
>> > [8,] "2016-12-06 21:45:48" "2016-12-06 22:45:48"
>> > [9,] "2016-12-06 21:45:49" "2016-12-06 22:45:49"
>> > [10,] "2016-12-06 21:45:50" "2016-12-06 22:45:50"
>> >>
>> >
>>
>>
>> So exactly 1 hour out of whack. Is there a Daylight Saving Times issue,
>> perchance?
>>
>> -pd
>>
>>
>> >
>> > --
>> > ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.
>> ~.~.~.~.~.~.
>> >   ___    Patrick Connolly
>> > {~._.~}                   Great minds discuss ideas
>> > _( Y )_                Average minds discuss events
>> > (:_~*~_:)                  Small minds discuss people
>> > (_)-(_)                             ..... Eleanor Roosevelt
>> >
>> > ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.
>> ~.~.~.~.~.~.
>>
>> --
>> Peter Dalgaard, Professor,
>> Center for Statistics, Copenhagen Business School
>> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
>> Phone: (+45)38153501
>> Office: A 4.23
>> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>
>
>
> --
> Joris Meys
> Statistical consultant
>
> Ghent University
> Faculty of Bioscience Engineering
> Department of Mathematical Modelling, Statistics and Bio-Informatics
>
> tel :  +32 (0)9 264 61 79 <+32%209%20264%2061%2079>
> Joris.Meys at Ugent.be
> -------------------------------
> Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php
>



-- 
Joris Meys
Statistical consultant

Ghent University
Faculty of Bioscience Engineering
Department of Mathematical Modelling, Statistics and Bio-Informatics

tel :  +32 (0)9 264 61 79
Joris.Meys at Ugent.be
-------------------------------
Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php

	[[alternative HTML version deleted]]


From spencer.graves at prodsyse.com  Thu May 18 13:38:20 2017
From: spencer.graves at prodsyse.com (Spencer Graves)
Date: Thu, 18 May 2017 06:38:20 -0500
Subject: [Rd] SUGGESTION: R Base Packages
In-Reply-To: <C7D958509BBBF44A8F8F9CBE5BE9400764C695CA@EXDG04.usr.corp.gamesa.es>
References: <C7D958509BBBF44A8F8F9CBE5BE9400764C69570@EXDG04.usr.corp.gamesa.es>
 <20170517195722.GM25437@ofb.net>
 <C7D958509BBBF44A8F8F9CBE5BE9400764C695CA@EXDG04.usr.corp.gamesa.es>
Message-ID: <8e331367-367e-cccb-5d2b-6a8371a6bb6e@prodsyse.com>



On 2017-05-18 2:18 AM, TELLERIA RUIZ DE AGUIRRE, JUAN wrote:
> Thank you Frederick for your comments:
>
> They are really well justified.
>
>> I think a "forum" or bulletin board system would be a detraction from the project and a distraction for the project leaders. Users have Stack Exchange - it's better than any forum we could create, and it
>> takes care of itself.
> An excellent idea would be to add in the R Project Webpage a link to RSeek, in order to put all together. This can be done in a fast and easy way.
>
>> I too was annoyed by the state of the bug reporting system when I first started using R, and that was even before automatic account creation was disabled. I think the situation could be improved.
>> I don't think it's a huge burden to tell bug reporters to ask for an account via the R-devel mailing list. But from what I can see the bug tracker doesn't make it clear that this has to be done. We should at
>> least post a message on the main page explaining how people can create an account. This would be less work than what Juan is suggesting, but I think sufficient for our needs.
> Yes, clarifying in the RProject webpage that accounts could be requested for bug reporting in Bugzilla would be a great news, but it shall be really good justified.
>
>> I think this is a terrible idea. I've only been using R for a few years and while I find these "tidyverse" packages interesting and use them on occasion, I've also concluded that they change too quickly to
>> be used in code that I want to stay working for a long time. They are largely based on experimental concepts. Being able to change and evolve is part of the strength of these packages. So I think putting
>> them in the R Base distribution would be bad for all parties.
> Totally agree.
>
>> I think the R project web page looks great. It's simple and it loads quickly and doesn't try to mesmerize people.


       I like that description "mesmerize":  I despise web pages with 
anything that moves.  I don't surf the net for entertainment:  I want 
information.  Anything that moves is a distraction and makes it harder 
for me to find what I want.


        Spencer Graves


>> I like R's command line interface. It completes on symbols and files and I can easily use it
>> with my favorite editor and run it in the terminal of my choice. I don't think that more effort should be put into developing bloated GUIs which try to enforce a standard way of interacting with R.
> I agree, but, in my opinion,  the RGUI icons  shall have a little bit more modern look. Just a bit.
>
> Thank you,
> Juan
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From marc_schwartz at me.com  Thu May 18 14:05:14 2017
From: marc_schwartz at me.com (Marc Schwartz)
Date: Thu, 18 May 2017 07:05:14 -0500
Subject: [Rd] SUGGESTION: R Base Packages
In-Reply-To: <C7D958509BBBF44A8F8F9CBE5BE9400764C695CA@EXDG04.usr.corp.gamesa.es>
References: <C7D958509BBBF44A8F8F9CBE5BE9400764C69570@EXDG04.usr.corp.gamesa.es>
 <20170517195722.GM25437@ofb.net>
 <C7D958509BBBF44A8F8F9CBE5BE9400764C695CA@EXDG04.usr.corp.gamesa.es>
Message-ID: <B87995AD-4061-4BA8-B83A-AD497DC92448@me.com>


> On May 18, 2017, at 2:18 AM, TELLERIA RUIZ DE AGUIRRE, JUAN <JTELLERIA at external.gamesacorp.com> wrote:
> 
> Thank you Frederick for your comments:
> 
> They are really well justified.
> 
>> I think a "forum" or bulletin board system would be a detraction from the project and a distraction for the project leaders. Users have Stack Exchange - it's better than any forum we could create, and it 
>> takes care of itself.
> 
> An excellent idea would be to add in the R Project Webpage a link to RSeek, in order to put all together. This can be done in a fast and easy way.


Hi,

It is already there, in two places:

  https://www.r-project.org/search.html

and 

   https://www.r-project.org/help.html

both of those accessible from the navigation menu on the left side of the home page under "Search" and "Getting Help".

Regards,

Marc Schwartz


> 
>> I too was annoyed by the state of the bug reporting system when I first started using R, and that was even before automatic account creation was disabled. I think the situation could be improved. 
>> I don't think it's a huge burden to tell bug reporters to ask for an account via the R-devel mailing list. But from what I can see the bug tracker doesn't make it clear that this has to be done. We should at 
>> least post a message on the main page explaining how people can create an account. This would be less work than what Juan is suggesting, but I think sufficient for our needs.
> 
> Yes, clarifying in the RProject webpage that accounts could be requested for bug reporting in Bugzilla would be a great news, but it shall be really good justified.
> 
>> I think this is a terrible idea. I've only been using R for a few years and while I find these "tidyverse" packages interesting and use them on occasion, I've also concluded that they change too quickly to 
>> be used in code that I want to stay working for a long time. They are largely based on experimental concepts. Being able to change and evolve is part of the strength of these packages. So I think putting 
>> them in the R Base distribution would be bad for all parties.
> 
> Totally agree.
> 
>> I think the R project web page looks great. It's simple and it loads quickly and doesn't try to mesmerize people. I like R's command line interface. It completes on symbols and files and I can easily use it 
>> with my favorite editor and run it in the terminal of my choice. I don't think that more effort should be put into developing bloated GUIs which try to enforce a standard way of interacting with R.
> 
> I agree, but, in my opinion,  the RGUI icons  shall have a little bit more modern look. Just a bit.
> 
> Thank you,
> Juan


From pdalgd at gmail.com  Thu May 18 14:49:39 2017
From: pdalgd at gmail.com (peter dalgaard)
Date: Thu, 18 May 2017 14:49:39 +0200
Subject: [Rd] [R] R-3.4.0 fails test
In-Reply-To: <CAO1zAVb2T3roWLU4FxhvjxJz31wRp8oYtCeHc2e8V8KOprP1DA@mail.gmail.com>
References: <20170517074216.GA4553@slingshot.co.nz>
 <4a086fbb-61ed-d3a5-ce23-14bbaea5531f@gmail.com>
 <ACA47D66-F469-4C75-8521-CB0DD76572C7@gmail.com>
 <20170518090016.GB4553@slingshot.co.nz>
 <29540304-FC8B-4B49-B94B-E191DEFA3BCB@gmail.com>
 <CAO1zAVarefFWaW_UmN6gCN3ppVuaGhLO7ErYdpUJmbfJ6rb4XQ@mail.gmail.com>
 <CAO1zAVb2T3roWLU4FxhvjxJz31wRp8oYtCeHc2e8V8KOprP1DA@mail.gmail.com>
Message-ID: <18013A70-4C15-4A20-8626-578C91BF80DB@gmail.com>


> On 18 May 2017, at 13:47 , Joris Meys <jorismeys at gmail.com> wrote:
> 
> Correction: Also dlt uses the default timezone, but POSIXlt is not recalculated whereas POSIXct is. Reason for that is the different way values are stored (hours, minutes, seconds as opposed to minutes from origin, as explained in my previous mail)
> 

I would suspect that there is something more subtle going on, New Zealand time is 10, 11, or 12 hours from Central European, depending on time of year (10 in our Summer, 12 in theirs and 11 during the overlap at both ends, if you must know), and we are talking a 1 hour difference.  

However DST transitions were both in March/April, so that's not it. Maybe a POSIX[lc]t expert can comment?

-pd

> CHeers
> Joris
> 
> On Thu, May 18, 2017 at 1:45 PM, Joris Meys <jorismeys at gmail.com> wrote:
> This has to do with your own timezone. If I run that code on my computer, both formats are correct. If I do this after 
> 
> Sys.setenv(TZ = "UTC")
> 
> Then:
> 
> > cbind(format(dlt), format(dct))
>       [,1]                  [,2]                 
>  [1,] "2016-12-06 21:45:41" "2016-12-06 20:45:41"
>  [2,] "2016-12-06 21:45:42" "2016-12-06 20:45:42"
> 
> The reason for that, is that dlt has a timezone set, but dct doesn't. To be correct, it only takes the first value "", which indicates "Use the default timezone of the locale".
> 
> > attr(dlt, "tzone")
> [1] ""     "CET"  "CEST"
> > attr(dct, "tzone")
> [1] ""
> 
> The thing is, in POSIXlt the timezone attribute is stored together with the actual values for hour, minute etc. in list format. Changing the timezone doesn't change those values, but it will change the time itself:
> 
> > Sys.unsetenv("TZ")
> > dlt2 <- dlt
> > attr(dlt2,"tzone") <- "UTC"
> > dlt2
>  [1] "2016-12-06 21:45:41 UTC" "2016-12-06 21:45:42 UTC"
>  [3] "2016-12-06 21:45:43 UTC" "2016-12-06 21:45:44 UTC"
> 
> in POSIXct the value doesn't change either, just the attribute. But this value is the number of seconds since the origin. So the time itself doesn't change, but you'll see a different hour.
> 
> > dct
>  [1] "2016-12-06 21:45:41 CET" "2016-12-06 21:45:42 CET"
> ...
> > attr(dct,"tzone") <- "UTC"
> > dct
>  [1] "2016-12-06 20:45:41 UTC" "2016-12-06 20:45:42 UTC"
>  [3] "2016-12-06 20:45:43 UTC" "2016-12-06 20:45:44 UTC"
> 
> So what you see, is simply the result of your timezone settings on your computer.
> 
> Cheers
> Joris
> 
> On Thu, May 18, 2017 at 1:19 PM, peter dalgaard <pdalgd at gmail.com> wrote:
> 
> > On 18 May 2017, at 11:00 , Patrick Connolly <p_connolly at slingshot.co.nz> wrote:
> >
> > On Wed, 17-May-2017 at 01:21PM +0200, Peter Dalgaard wrote:
> >
> > |>
> > |> Anyways, you might want to
> > |>
> > |> a) move the discussion to R-devel
> > |> b) include your platform (hardware, OS) and time zone info
> >
> > System:    Host: MTA-V1-427894 Kernel: 3.19.0-32-generic x86_64 (64 bit gcc: 4.8.2)
> >           Desktop: KDE Plasma 4.14.2 (Qt 4.8.6) Distro: Linux Mint 17.3 Rosa
> 
> I suppose that'll do...
> 
> 
> > Time zone: NZST
> 
> 
> 
> >
> > |> c) run the offending code lines "by hand" and show us the values of format(dlt) and format(dct) so we can see what the problem is, something like
> > |>
> > |> dlt <- structure(
> > |>     list(sec = 52, min = 59L, hour = 18L, mday = 6L, mon = 11L, year = 116L,
> > |>        wday = 2L, yday = 340L, isdst = 0L, zone = "CET", gmtoff = 3600L),
> > |>        class = c("POSIXlt", "POSIXt"), tzone = c("", "CET", "CEST"))
> > |> dlt$sec <- 10000 + 1:10
> > |> dct <- as.POSIXct(dlt)
> > |> cbind(format(dlt), format(dct))
> >
> >> cbind(format(dlt), format(dct))
> >      [,1]                  [,2]
> > [1,] "2016-12-06 21:45:41" "2016-12-06 22:45:41"
> > [2,] "2016-12-06 21:45:42" "2016-12-06 22:45:42"
> > [3,] "2016-12-06 21:45:43" "2016-12-06 22:45:43"
> > [4,] "2016-12-06 21:45:44" "2016-12-06 22:45:44"
> > [5,] "2016-12-06 21:45:45" "2016-12-06 22:45:45"
> > [6,] "2016-12-06 21:45:46" "2016-12-06 22:45:46"
> > [7,] "2016-12-06 21:45:47" "2016-12-06 22:45:47"
> > [8,] "2016-12-06 21:45:48" "2016-12-06 22:45:48"
> > [9,] "2016-12-06 21:45:49" "2016-12-06 22:45:49"
> > [10,] "2016-12-06 21:45:50" "2016-12-06 22:45:50"
> >>
> >
> 
> 
> So exactly 1 hour out of whack. Is there a Daylight Saving Times issue, perchance?
> 
> -pd
> 
> 
> >
> > --
> > ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.
> >   ___    Patrick Connolly
> > {~._.~}                   Great minds discuss ideas
> > _( Y )_                Average minds discuss events
> > (:_~*~_:)                  Small minds discuss people
> > (_)-(_)                             ..... Eleanor Roosevelt
> >
> > ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.
> 
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 
> 
> 
> -- 
> Joris Meys
> Statistical consultant
> 
> Ghent University
> Faculty of Bioscience Engineering
> Department of Mathematical Modelling, Statistics and Bio-Informatics
> 
> tel :  +32 (0)9 264 61 79
> Joris.Meys at Ugent.be
> -------------------------------
> Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php
> 
> 
> 
> -- 
> Joris Meys
> Statistical consultant
> 
> Ghent University
> Faculty of Bioscience Engineering
> Department of Mathematical Modelling, Statistics and Bio-Informatics
> 
> tel :  +32 (0)9 264 61 79
> Joris.Meys at Ugent.be
> -------------------------------
> Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From plummerm at iarc.fr  Thu May 18 14:58:19 2017
From: plummerm at iarc.fr (Martyn Plummer)
Date: Thu, 18 May 2017 12:58:19 +0000
Subject: [Rd] [R] R-3.4.0 fails test
In-Reply-To: <18013A70-4C15-4A20-8626-578C91BF80DB@gmail.com>
References: <20170517074216.GA4553@slingshot.co.nz>
 <4a086fbb-61ed-d3a5-ce23-14bbaea5531f@gmail.com>
 <ACA47D66-F469-4C75-8521-CB0DD76572C7@gmail.com>
 <20170518090016.GB4553@slingshot.co.nz>
 <29540304-FC8B-4B49-B94B-E191DEFA3BCB@gmail.com>
 <CAO1zAVarefFWaW_UmN6gCN3ppVuaGhLO7ErYdpUJmbfJ6rb4XQ@mail.gmail.com>
 <CAO1zAVb2T3roWLU4FxhvjxJz31wRp8oYtCeHc2e8V8KOprP1DA@mail.gmail.com>,
 <18013A70-4C15-4A20-8626-578C91BF80DB@gmail.com>
Message-ID: <B736344B-9330-48FA-AB1D-17B76771673A@iarc.fr>



> On 18 May 2017, at 14:51, peter dalgaard <pdalgd at gmail.com> wrote:
> 
> 
>> On 18 May 2017, at 13:47 , Joris Meys <jorismeys at gmail.com> wrote:
>> 
>> Correction: Also dlt uses the default timezone, but POSIXlt is not recalculated whereas POSIXct is. Reason for that is the different way values are stored (hours, minutes, seconds as opposed to minutes from origin, as explained in my previous mail)
>> 
> 
> I would suspect that there is something more subtle going on, New Zealand time is 10, 11, or 12 hours from Central European, depending on time of year (10 in our Summer, 12 in theirs and 11 during the overlap at both ends, if you must know), and we are talking a 1 hour difference.  
> 
> However DST transitions were both in March/April, so that's not it. Maybe a POSIX[lc]t expert can comment?

If I change the month from December to June then I see the same phenomenon in my Europe/Paris time zone. The issue seems to be that, for the date chosen for the test, Summer/daylight savings time is in force in NZ and some other parts of the southern hemisphere , but not in the northern hemisphere.

Martyn

> -pd
> 
>> CHeers
>> Joris
>> 
>> On Thu, May 18, 2017 at 1:45 PM, Joris Meys <jorismeys at gmail.com> wrote:
>> This has to do with your own timezone. If I run that code on my computer, both formats are correct. If I do this after 
>> 
>> Sys.setenv(TZ = "UTC")
>> 
>> Then:
>> 
>>> cbind(format(dlt), format(dct))
>>      [,1]                  [,2]                 
>> [1,] "2016-12-06 21:45:41" "2016-12-06 20:45:41"
>> [2,] "2016-12-06 21:45:42" "2016-12-06 20:45:42"
>> 
>> The reason for that, is that dlt has a timezone set, but dct doesn't. To be correct, it only takes the first value "", which indicates "Use the default timezone of the locale".
>> 
>>> attr(dlt, "tzone")
>> [1] ""     "CET"  "CEST"
>>> attr(dct, "tzone")
>> [1] ""
>> 
>> The thing is, in POSIXlt the timezone attribute is stored together with the actual values for hour, minute etc. in list format. Changing the timezone doesn't change those values, but it will change the time itself:
>> 
>>> Sys.unsetenv("TZ")
>>> dlt2 <- dlt
>>> attr(dlt2,"tzone") <- "UTC"
>>> dlt2
>> [1] "2016-12-06 21:45:41 UTC" "2016-12-06 21:45:42 UTC"
>> [3] "2016-12-06 21:45:43 UTC" "2016-12-06 21:45:44 UTC"
>> 
>> in POSIXct the value doesn't change either, just the attribute. But this value is the number of seconds since the origin. So the time itself doesn't change, but you'll see a different hour.
>> 
>>> dct
>> [1] "2016-12-06 21:45:41 CET" "2016-12-06 21:45:42 CET"
>> ...
>>> attr(dct,"tzone") <- "UTC"
>>> dct
>> [1] "2016-12-06 20:45:41 UTC" "2016-12-06 20:45:42 UTC"
>> [3] "2016-12-06 20:45:43 UTC" "2016-12-06 20:45:44 UTC"
>> 
>> So what you see, is simply the result of your timezone settings on your computer.
>> 
>> Cheers
>> Joris
>> 
>>> On Thu, May 18, 2017 at 1:19 PM, peter dalgaard <pdalgd at gmail.com> wrote:
>>> 
>>> On 18 May 2017, at 11:00 , Patrick Connolly <p_connolly at slingshot.co.nz> wrote:
>>> 
>>> On Wed, 17-May-2017 at 01:21PM +0200, Peter Dalgaard wrote:
>>> 
>>> |>
>>> |> Anyways, you might want to
>>> |>
>>> |> a) move the discussion to R-devel
>>> |> b) include your platform (hardware, OS) and time zone info
>>> 
>>> System:    Host: MTA-V1-427894 Kernel: 3.19.0-32-generic x86_64 (64 bit gcc: 4.8.2)
>>>          Desktop: KDE Plasma 4.14.2 (Qt 4.8.6) Distro: Linux Mint 17.3 Rosa
>> 
>> I suppose that'll do...
>> 
>> 
>>> Time zone: NZST
>> 
>> 
>> 
>>> 
>>> |> c) run the offending code lines "by hand" and show us the values of format(dlt) and format(dct) so we can see what the problem is, something like
>>> |>
>>> |> dlt <- structure(
>>> |>     list(sec = 52, min = 59L, hour = 18L, mday = 6L, mon = 11L, year = 116L,
>>> |>        wday = 2L, yday = 340L, isdst = 0L, zone = "CET", gmtoff = 3600L),
>>> |>        class = c("POSIXlt", "POSIXt"), tzone = c("", "CET", "CEST"))
>>> |> dlt$sec <- 10000 + 1:10
>>> |> dct <- as.POSIXct(dlt)
>>> |> cbind(format(dlt), format(dct))
>>> 
>>>> cbind(format(dlt), format(dct))
>>>     [,1]                  [,2]
>>> [1,] "2016-12-06 21:45:41" "2016-12-06 22:45:41"
>>> [2,] "2016-12-06 21:45:42" "2016-12-06 22:45:42"
>>> [3,] "2016-12-06 21:45:43" "2016-12-06 22:45:43"
>>> [4,] "2016-12-06 21:45:44" "2016-12-06 22:45:44"
>>> [5,] "2016-12-06 21:45:45" "2016-12-06 22:45:45"
>>> [6,] "2016-12-06 21:45:46" "2016-12-06 22:45:46"
>>> [7,] "2016-12-06 21:45:47" "2016-12-06 22:45:47"
>>> [8,] "2016-12-06 21:45:48" "2016-12-06 22:45:48"
>>> [9,] "2016-12-06 21:45:49" "2016-12-06 22:45:49"
>>> [10,] "2016-12-06 21:45:50" "2016-12-06 22:45:50"
>>>> 
>>> 
>> 
>> 
>> So exactly 1 hour out of whack. Is there a Daylight Saving Times issue, perchance?
>> 
>> -pd
>> 
>> 
>>> 
>>> --
>>> ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.
>>>  ___    Patrick Connolly
>>> {~._.~}                   Great minds discuss ideas
>>> _( Y )_                Average minds discuss events
>>> (:_~*~_:)                  Small minds discuss people
>>> (_)-(_)                             ..... Eleanor Roosevelt
>>> 
>>> ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.
>> 
>> --
>> Peter Dalgaard, Professor,
>> Center for Statistics, Copenhagen Business School
>> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
>> Phone: (+45)38153501
>> Office: A 4.23
>> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>> 
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>> 
>> 
>> 
>> -- 
>> Joris Meys
>> Statistical consultant
>> 
>> Ghent University
>> Faculty of Bioscience Engineering
>> Department of Mathematical Modelling, Statistics and Bio-Informatics
>> 
>> tel :  +32 (0)9 264 61 79
>> Joris.Meys at Ugent.be
>> -------------------------------
>> Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php
>> 
>> 
>> 
>> -- 
>> Joris Meys
>> Statistical consultant
>> 
>> Ghent University
>> Faculty of Bioscience Engineering
>> Department of Mathematical Modelling, Statistics and Bio-Informatics
>> 
>> tel :  +32 (0)9 264 61 79
>> Joris.Meys at Ugent.be
>> -------------------------------
>> Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php
> 
> -- 
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
-----------------------------------------------------------------------
This message and its attachments are strictly confidenti...{{dropped:8}}


From pdalgd at gmail.com  Thu May 18 15:22:03 2017
From: pdalgd at gmail.com (peter dalgaard)
Date: Thu, 18 May 2017 15:22:03 +0200
Subject: [Rd] [R] R-3.4.0 fails test
In-Reply-To: <B736344B-9330-48FA-AB1D-17B76771673A@iarc.fr>
References: <20170517074216.GA4553@slingshot.co.nz>
 <4a086fbb-61ed-d3a5-ce23-14bbaea5531f@gmail.com>
 <ACA47D66-F469-4C75-8521-CB0DD76572C7@gmail.com>
 <20170518090016.GB4553@slingshot.co.nz>
 <29540304-FC8B-4B49-B94B-E191DEFA3BCB@gmail.com>
 <CAO1zAVarefFWaW_UmN6gCN3ppVuaGhLO7ErYdpUJmbfJ6rb4XQ@mail.gmail.com>
 <CAO1zAVb2T3roWLU4FxhvjxJz31wRp8oYtCeHc2e8V8KOprP1DA@mail.gmail.com>
 <18013A70-4C15-4A20-8626-578C91BF80DB@gmail.com>
 <B736344B-9330-48FA-AB1D-17B76771673A@iarc.fr>
Message-ID: <5854D32F-A43D-4002-8F0F-6DD0D7888B5C@gmail.com>


> On 18 May 2017, at 14:58 , Martyn Plummer <plummerM at iarc.fr> wrote:
> 
> 
> 
>> On 18 May 2017, at 14:51, peter dalgaard <pdalgd at gmail.com> wrote:
>> 
>> 
>>> On 18 May 2017, at 13:47 , Joris Meys <jorismeys at gmail.com> wrote:
>>> 
>>> Correction: Also dlt uses the default timezone, but POSIXlt is not recalculated whereas POSIXct is. Reason for that is the different way values are stored (hours, minutes, seconds as opposed to minutes from origin, as explained in my previous mail)
>>> 
>> 
>> I would suspect that there is something more subtle going on, New Zealand time is 10, 11, or 12 hours from Central European, depending on time of year (10 in our Summer, 12 in theirs and 11 during the overlap at both ends, if you must know), and we are talking a 1 hour difference.  
>> 
>> However DST transitions were both in March/April, so that's not it. Maybe a POSIX[lc]t expert can comment?
> 
> If I change the month from December to June then I see the same phenomenon in my Europe/Paris time zone. The issue seems to be that, for the date chosen for the test, Summer/daylight savings time is in force in NZ and some other parts of the southern hemisphere , but not in the northern hemisphere.
> 

Of course! I overlooked that the date in the test is the issue, not the current date. (Let's blame that on the fact that Summer seems to have finally arrived in Copenhagen...)

"svn praise" claims this test is due to Martin Maechler in r71742, so maybe he knows how to fix it. (I wonder if he just used the current date at the time, or actually thought that there would be no DST issues in December ;-) )

-pd


> Martyn
> 
>> -pd
>> 
>>> CHeers
>>> Joris
>>> 
>>> On Thu, May 18, 2017 at 1:45 PM, Joris Meys <jorismeys at gmail.com> wrote:
>>> This has to do with your own timezone. If I run that code on my computer, both formats are correct. If I do this after 
>>> 
>>> Sys.setenv(TZ = "UTC")
>>> 
>>> Then:
>>> 
>>>> cbind(format(dlt), format(dct))
>>>     [,1]                  [,2]                 
>>> [1,] "2016-12-06 21:45:41" "2016-12-06 20:45:41"
>>> [2,] "2016-12-06 21:45:42" "2016-12-06 20:45:42"
>>> 
>>> The reason for that, is that dlt has a timezone set, but dct doesn't. To be correct, it only takes the first value "", which indicates "Use the default timezone of the locale".
>>> 
>>>> attr(dlt, "tzone")
>>> [1] ""     "CET"  "CEST"
>>>> attr(dct, "tzone")
>>> [1] ""
>>> 
>>> The thing is, in POSIXlt the timezone attribute is stored together with the actual values for hour, minute etc. in list format. Changing the timezone doesn't change those values, but it will change the time itself:
>>> 
>>>> Sys.unsetenv("TZ")
>>>> dlt2 <- dlt
>>>> attr(dlt2,"tzone") <- "UTC"
>>>> dlt2
>>> [1] "2016-12-06 21:45:41 UTC" "2016-12-06 21:45:42 UTC"
>>> [3] "2016-12-06 21:45:43 UTC" "2016-12-06 21:45:44 UTC"
>>> 
>>> in POSIXct the value doesn't change either, just the attribute. But this value is the number of seconds since the origin. So the time itself doesn't change, but you'll see a different hour.
>>> 
>>>> dct
>>> [1] "2016-12-06 21:45:41 CET" "2016-12-06 21:45:42 CET"
>>> ...
>>>> attr(dct,"tzone") <- "UTC"
>>>> dct
>>> [1] "2016-12-06 20:45:41 UTC" "2016-12-06 20:45:42 UTC"
>>> [3] "2016-12-06 20:45:43 UTC" "2016-12-06 20:45:44 UTC"
>>> 
>>> So what you see, is simply the result of your timezone settings on your computer.
>>> 
>>> Cheers
>>> Joris
>>> 
>>>> On Thu, May 18, 2017 at 1:19 PM, peter dalgaard <pdalgd at gmail.com> wrote:
>>>> 
>>>> On 18 May 2017, at 11:00 , Patrick Connolly <p_connolly at slingshot.co.nz> wrote:
>>>> 
>>>> On Wed, 17-May-2017 at 01:21PM +0200, Peter Dalgaard wrote:
>>>> 
>>>> |>
>>>> |> Anyways, you might want to
>>>> |>
>>>> |> a) move the discussion to R-devel
>>>> |> b) include your platform (hardware, OS) and time zone info
>>>> 
>>>> System:    Host: MTA-V1-427894 Kernel: 3.19.0-32-generic x86_64 (64 bit gcc: 4.8.2)
>>>>         Desktop: KDE Plasma 4.14.2 (Qt 4.8.6) Distro: Linux Mint 17.3 Rosa
>>> 
>>> I suppose that'll do...
>>> 
>>> 
>>>> Time zone: NZST
>>> 
>>> 
>>> 
>>>> 
>>>> |> c) run the offending code lines "by hand" and show us the values of format(dlt) and format(dct) so we can see what the problem is, something like
>>>> |>
>>>> |> dlt <- structure(
>>>> |>     list(sec = 52, min = 59L, hour = 18L, mday = 6L, mon = 11L, year = 116L,
>>>> |>        wday = 2L, yday = 340L, isdst = 0L, zone = "CET", gmtoff = 3600L),
>>>> |>        class = c("POSIXlt", "POSIXt"), tzone = c("", "CET", "CEST"))
>>>> |> dlt$sec <- 10000 + 1:10
>>>> |> dct <- as.POSIXct(dlt)
>>>> |> cbind(format(dlt), format(dct))
>>>> 
>>>>> cbind(format(dlt), format(dct))
>>>>    [,1]                  [,2]
>>>> [1,] "2016-12-06 21:45:41" "2016-12-06 22:45:41"
>>>> [2,] "2016-12-06 21:45:42" "2016-12-06 22:45:42"
>>>> [3,] "2016-12-06 21:45:43" "2016-12-06 22:45:43"
>>>> [4,] "2016-12-06 21:45:44" "2016-12-06 22:45:44"
>>>> [5,] "2016-12-06 21:45:45" "2016-12-06 22:45:45"
>>>> [6,] "2016-12-06 21:45:46" "2016-12-06 22:45:46"
>>>> [7,] "2016-12-06 21:45:47" "2016-12-06 22:45:47"
>>>> [8,] "2016-12-06 21:45:48" "2016-12-06 22:45:48"
>>>> [9,] "2016-12-06 21:45:49" "2016-12-06 22:45:49"
>>>> [10,] "2016-12-06 21:45:50" "2016-12-06 22:45:50"
>>>>> 
>>>> 
>>> 
>>> 
>>> So exactly 1 hour out of whack. Is there a Daylight Saving Times issue, perchance?
>>> 
>>> -pd
>>> 
>>> 
>>>> 
>>>> --
>>>> ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.
>>>> ___    Patrick Connolly
>>>> {~._.~}                   Great minds discuss ideas
>>>> _( Y )_                Average minds discuss events
>>>> (:_~*~_:)                  Small minds discuss people
>>>> (_)-(_)                             ..... Eleanor Roosevelt
>>>> 
>>>> ~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.
>>> 
>>> --
>>> Peter Dalgaard, Professor,
>>> Center for Statistics, Copenhagen Business School
>>> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
>>> Phone: (+45)38153501
>>> Office: A 4.23
>>> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>>> 
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>> 
>>> 
>>> 
>>> -- 
>>> Joris Meys
>>> Statistical consultant
>>> 
>>> Ghent University
>>> Faculty of Bioscience Engineering
>>> Department of Mathematical Modelling, Statistics and Bio-Informatics
>>> 
>>> tel :  +32 (0)9 264 61 79
>>> Joris.Meys at Ugent.be
>>> -------------------------------
>>> Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php
>>> 
>>> 
>>> 
>>> -- 
>>> Joris Meys
>>> Statistical consultant
>>> 
>>> Ghent University
>>> Faculty of Bioscience Engineering
>>> Department of Mathematical Modelling, Statistics and Bio-Informatics
>>> 
>>> tel :  +32 (0)9 264 61 79
>>> Joris.Meys at Ugent.be
>>> -------------------------------
>>> Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php
>> 
>> -- 
>> Peter Dalgaard, Professor,
>> Center for Statistics, Copenhagen Business School
>> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
>> Phone: (+45)38153501
>> Office: A 4.23
>> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>> 
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
> -----------------------------------------------------------------------
> This message and its attachments are strictly confiden...{{dropped:25}}


From maechler at stat.math.ethz.ch  Thu May 18 17:46:08 2017
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 18 May 2017 17:46:08 +0200
Subject: [Rd] [R] R-3.4.0 fails test
In-Reply-To: <5854D32F-A43D-4002-8F0F-6DD0D7888B5C@gmail.com>
References: <20170517074216.GA4553@slingshot.co.nz>
 <4a086fbb-61ed-d3a5-ce23-14bbaea5531f@gmail.com>
 <ACA47D66-F469-4C75-8521-CB0DD76572C7@gmail.com>
 <20170518090016.GB4553@slingshot.co.nz>
 <29540304-FC8B-4B49-B94B-E191DEFA3BCB@gmail.com>
 <CAO1zAVarefFWaW_UmN6gCN3ppVuaGhLO7ErYdpUJmbfJ6rb4XQ@mail.gmail.com>
 <CAO1zAVb2T3roWLU4FxhvjxJz31wRp8oYtCeHc2e8V8KOprP1DA@mail.gmail.com>
 <18013A70-4C15-4A20-8626-578C91BF80DB@gmail.com>
 <B736344B-9330-48FA-AB1D-17B76771673A@iarc.fr>
 <5854D32F-A43D-4002-8F0F-6DD0D7888B5C@gmail.com>
Message-ID: <22813.49600.238510.855887@stat.math.ethz.ch>


> > On 18 May 2017, at 14:58 , Martyn Plummer <plummerM at iarc.fr> wrote:
> > 
> >> On 18 May 2017, at 14:51, peter dalgaard <pdalgd at gmail.com> wrote:

> >>> On 18 May 2017, at 13:47 , Joris Meys <jorismeys at gmail.com> wrote:
> >>> 
> >>> Correction: Also dlt uses the default timezone, but POSIXlt is not recalculated whereas POSIXct is. Reason for that is the different way values are stored (hours, minutes, seconds as opposed to minutes from origin, as explained in my previous mail)
> >>> 
> >> 
> >> I would suspect that there is something more subtle going on, New Zealand time is 10, 11, or 12 hours from Central European, depending on time of year (10 in our Summer, 12 in theirs and 11 during the overlap at both ends, if you must know), and we are talking a 1 hour difference.  
> >> 
> >> However DST transitions were both in March/April, so that's not it. Maybe a POSIX[lc]t expert can comment?
> > 
> > If I change the month from December to June then I see the same phenomenon in my Europe/Paris time zone. The issue seems to be that, for the date chosen for the test, Summer/daylight savings time is in force in NZ and some other parts of the southern hemisphere , but not in the northern hemisphere.
> > 

> Of course! I overlooked that the date in the test is the issue, not the current date. (Let's blame that on the fact that Summer seems to have finally arrived in Copenhagen...)

> "svn praise" claims this test is due to Martin Maechler in r71742, so maybe he knows how to fix it. (I wonder if he just used the current date at the time, or actually thought that there would be no DST issues in December ;-) )

> -pd

I think the former.  This was about fixing a segmentation fault problem
 (thread from Dec 6, 2016, on the R-devel mailing list, starting
  here: https://stat.ethz.ch/pipermail/r-devel/2016-December/073448.html )
mostly,  but the 2nd part of the regression test evidently needs
more thought than I had put there.

Being pretty "stretched" time wise currently, I'm happy for
timezone-portable propositions to change the test.

Martin


From suharto_anggono at yahoo.com  Thu May 18 18:27:09 2017
From: suharto_anggono at yahoo.com (Suharto Anggono Suharto Anggono)
Date: Thu, 18 May 2017 16:27:09 +0000 (UTC)
Subject: [Rd] stopifnot() does not stop at first non-TRUE argument
References: <812904062.672054.1495124829132.ref@mail.yahoo.com>
Message-ID: <812904062.672054.1495124829132@mail.yahoo.com>

>From an example in http://www.uni-muenster.de/ZIV.BennoSueselbeck/s-html/helpfiles/nargs.html , number of arguments in '...' can be obtained by
(function(...)nargs())(...) .

I now realize that sys.call() doesn't expand '...' when the function is called with '...'. It just returns the call as is. If 'stopifnot' uses sys.call() instead of match.call() , the following example behaves improperly:
g <- function(...) stopifnot(...)
g(TRUE, FALSE)

--------------------------------------------
On Thu, 18/5/17, Martin Maechler <maechler at stat.math.ethz.ch> wrote:

 Subject: Re: [Rd] stopifnot() does not stop at first non-TRUE argument

 Cc: r-devel at r-project.org
 Date: Thursday, 18 May, 2017, 3:03 PM
 
>>>>> Suharto Anggono Suharto Anggono via R-devel <r-devel at r-project.org>
>>>>>     on Tue, 16 May 2017 16:37:45 +0000 writes:

    > switch(i, ...)
    > extracts 'i'-th argument in '...'. It is like
    > eval(as.name(paste0("..", i))) .

Yes, that's neat.

It is only almost the same:  in the case of illegal 'i'
the switch() version returns
    invisible(NULL)

whereas the version we'd want should signal an error, typically
the same error message as

  > t2 <- function(...) ..2
  > t2(1)
  Error in t2(1) (from #1) : the ... list does not contain 2 elements
  > 


    > Just mentioning other things:
    > - For 'n',
    > n <- nargs()
    > can be used.

I know .. [in this case, where '...' is the only formal argument of the function]

    > - sys.call() can be used in place of match.call() .

Hmm... in many cases, yes.... notably, as we do *not* want the
argument names here, I think you are right.


From joyousjoyyy at gmail.com  Thu May 18 18:54:35 2017
From: joyousjoyyy at gmail.com (Joy)
Date: Thu, 18 May 2017 12:54:35 -0400
Subject: [Rd] Interpreting R memory profiling statistics from Rprof() and
	gc()
Message-ID: <CAPkgxdv+ZkPgyuO73QavJKzEC+Jaen-rmkaSsyk=pAkFLJeaiQ@mail.gmail.com>

Sorry, this might be a really basic question, but I'm trying to interpret
the results from memory profiling, and I have a few questions (marked by
*Q#*).

From the summaryRprof() documentation, it seems that the four columns of
statistics that are reported when setting memory.profiling=TRUE are
- vector memory in small blocks on the R heap
- vector memory in large blocks (from malloc)
- memory in nodes on the R heap
- number of calls to the internal function duplicate in the time interval
(*Q1:* Are the units of the first 3 stats in bytes?)

and from the gc() documentation, the two rows represent
- ?"Ncells"? (_cons cells_), usually 28 bytes each on 32-bit systems and 56
bytes on 64-bit systems,
- ?"Vcells"? (_vector cells_, 8 bytes each)
(*Q2:* how are Ncells and Vcells related to small heap/large heap/memory in
nodes?)

And I guess the question that lead to these other questions is - *Q3:* I'd
like to plot out the total amount of memory used over time, and I don't
think Rprofmem() give me what I'd like to know because, as I'm
understanding it, Rprofmem() records the amount of memory allocated with
each call, but this doesn't tell me the total amount of memory R is using,
or am I mistaken?

Thanks in advance!

Joy

	[[alternative HTML version deleted]]


From sbbrouwer at gmail.com  Thu May 18 22:50:52 2017
From: sbbrouwer at gmail.com (Sietse Brouwer)
Date: Thu, 18 May 2017 22:50:52 +0200
Subject: [Rd] Bug: floating point bug in nclass.FD can cause hist() to crash
Message-ID: <CAF=dkzxor=d5FLN_W_FSg5yYjEKFrQxuPiADdqQqqX_mzrDdzQ@mail.gmail.com>

Hello everybody,

This is a bug involving functions in core R package:
graphics::hist.default, grDevices::nclass.FD, and
base::pretty.default. It is not yet on Bugzilla. I cannot submit it
myself, as I do not have an account. Could somebody else add it for
me, perhaps? That would be much appreciated.

Kind regards,

Sietse
Sietse Brouwer


Summary
-------

Floating point errors can cause a data vector to have an ultra-small
inter-quartile range, which causes `grDevices::nclass.FD` to suggest
an absurdly large number of breaks to `graphics::hist(breaks="FD")`.
Because this large float becomes NA when converted to integer, hist's
call to `base::pretty` crashes.

How could nclass.FD, which has the job of suggesting a reasonable number of
classes, avoid suggesting an absurdly large number of classes when the
inter-quartile range is absurdly small compared to the range?


Steps to reproduce
------------------

    hist(c(1, 1, 1, 1 + 1e-15, 2), breaks="FD")


Observed behaviour
------------------

Running this code gives the following error message:

    Error in pretty.default(range(x), n = breaks, min.n = 1):
      invalid 'n' argument
    In addition: Warning message:
    In pretty.default(range(x), n = breaks, min.n = 1) :
      NAs introduced by coercion to integer range


Expected behaviour
------------------

That hist() should never crash when given valid numerical data. Specifically,
that it should be robust even to those rare datasets where (through floating
point inaccuracy) the inter-quartile range is tens of orders of magnitude
smaller than the range.


Analysis
--------

Dramatis personae:

* graphics::hist.default
  https://svn.r-project.org/R/trunk/src/library/graphics/R/hist.R

* grDevices::nclass.FD
  https://svn.r-project.org/R/trunk/src/library/grDevices/R/calc.R

* base::pretty.default
  https://svn.r-project.org/R/trunk/src/library/base/R/pretty.R

`nclass.FD` examines the inter-quartile range of `x`, and gets a positive, but
very small floating point value -- let's call it TINYFLOAT. It inserts this
ultra-low IQR into the `nclass` denominator, which means `nclass`
becoms a huge number -- let's call it BIGFLOAT. `nclass.FD` then returns this
huge value to `hist`.

Once `hist` has its 'number of breaks' suggestion, it feeds this
number to `pretty`:

    pretty(range(x), BIGFLOAT, min.n = 1)

`pretty`, in turn, calls

    .Internal(pretty(min(x), max(x), BIGFLOAT, min.n, shrink.sml,
        c(high.u.bias, u5.bias), eps.correct))

Which fails with the error and warning shown at start of this e-mail. (Invalid
'n' argument / NA's introduced by coercion to integer range.) My reading is
that .Internal tried to coerce BIGFLOAT to integer range and produced an NA,
and that (the C implementation of) `pretty`, in turn, choked when confronted
with NA.


From spencer.graves at prodsyse.com  Thu May 18 23:05:07 2017
From: spencer.graves at prodsyse.com (Spencer Graves)
Date: Thu, 18 May 2017 16:05:07 -0500
Subject: [Rd] Bug: floating point bug in nclass.FD can cause hist() to
 crash
In-Reply-To: <CAF=dkzxor=d5FLN_W_FSg5yYjEKFrQxuPiADdqQqqX_mzrDdzQ@mail.gmail.com>
References: <CAF=dkzxor=d5FLN_W_FSg5yYjEKFrQxuPiADdqQqqX_mzrDdzQ@mail.gmail.com>
Message-ID: <5378285b-fbba-8637-42df-f7ce5ef1dd82@prodsyse.com>

I just got the same error message with


 > sessionInfo()
R version 3.4.0 (2017-04-21)
Platform: x86_64-apple-darwin15.6.0 (64-bit)
Running under: macOS Sierra 10.12.4

Matrix products: default
BLAS: 
/System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib
LAPACK: 
/Library/Frameworks/R.framework/Versions/3.4/Resources/lib/libRlapack.dylib

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils
[5] datasets  methods   base

loaded via a namespace (and not attached):
[1] compiler_3.4.0 tools_3.4.0
 >

On 2017-05-18 3:50 PM, Sietse Brouwer wrote:
> Hello everybody,
>
> This is a bug involving functions in core R package:
> graphics::hist.default, grDevices::nclass.FD, and
> base::pretty.default. It is not yet on Bugzilla. I cannot submit it
> myself, as I do not have an account. Could somebody else add it for
> me, perhaps? That would be much appreciated.
>
> Kind regards,
>
> Sietse
> Sietse Brouwer
>
>
> Summary
> -------
>
> Floating point errors can cause a data vector to have an ultra-small
> inter-quartile range, which causes `grDevices::nclass.FD` to suggest
> an absurdly large number of breaks to `graphics::hist(breaks="FD")`.
> Because this large float becomes NA when converted to integer, hist's
> call to `base::pretty` crashes.
>
> How could nclass.FD, which has the job of suggesting a reasonable number of
> classes, avoid suggesting an absurdly large number of classes when the
> inter-quartile range is absurdly small compared to the range?
>
>
> Steps to reproduce
> ------------------
>
>      hist(c(1, 1, 1, 1 + 1e-15, 2), breaks="FD")
>
>
> Observed behaviour
> ------------------
>
> Running this code gives the following error message:
>
>      Error in pretty.default(range(x), n = breaks, min.n = 1):
>        invalid 'n' argument
>      In addition: Warning message:
>      In pretty.default(range(x), n = breaks, min.n = 1) :
>        NAs introduced by coercion to integer range
>
>
> Expected behaviour
> ------------------
>
> That hist() should never crash when given valid numerical data. Specifically,
> that it should be robust even to those rare datasets where (through floating
> point inaccuracy) the inter-quartile range is tens of orders of magnitude
> smaller than the range.
>
>
> Analysis
> --------
>
> Dramatis personae:
>
> * graphics::hist.default
>    https://svn.r-project.org/R/trunk/src/library/graphics/R/hist.R
>
> * grDevices::nclass.FD
>    https://svn.r-project.org/R/trunk/src/library/grDevices/R/calc.R
>
> * base::pretty.default
>    https://svn.r-project.org/R/trunk/src/library/base/R/pretty.R
>
> `nclass.FD` examines the inter-quartile range of `x`, and gets a positive, but
> very small floating point value -- let's call it TINYFLOAT. It inserts this
> ultra-low IQR into the `nclass` denominator, which means `nclass`
> becoms a huge number -- let's call it BIGFLOAT. `nclass.FD` then returns this
> huge value to `hist`.
>
> Once `hist` has its 'number of breaks' suggestion, it feeds this
> number to `pretty`:
>
>      pretty(range(x), BIGFLOAT, min.n = 1)
>
> `pretty`, in turn, calls
>
>      .Internal(pretty(min(x), max(x), BIGFLOAT, min.n, shrink.sml,
>          c(high.u.bias, u5.bias), eps.correct))
>
> Which fails with the error and warning shown at start of this e-mail. (Invalid
> 'n' argument / NA's introduced by coercion to integer range.) My reading is
> that .Internal tried to coerce BIGFLOAT to integer range and produced an NA,
> and that (the C implementation of) `pretty`, in turn, choked when confronted
> with NA.
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From p_connolly at slingshot.co.nz  Fri May 19 06:13:03 2017
From: p_connolly at slingshot.co.nz (Patrick Connolly)
Date: Fri, 19 May 2017 16:13:03 +1200
Subject: [Rd] [R] R-3.4.0 fails test
In-Reply-To: <22813.49600.238510.855887@stat.math.ethz.ch>
References: <4a086fbb-61ed-d3a5-ce23-14bbaea5531f@gmail.com>
 <ACA47D66-F469-4C75-8521-CB0DD76572C7@gmail.com>
 <20170518090016.GB4553@slingshot.co.nz>
 <29540304-FC8B-4B49-B94B-E191DEFA3BCB@gmail.com>
 <CAO1zAVarefFWaW_UmN6gCN3ppVuaGhLO7ErYdpUJmbfJ6rb4XQ@mail.gmail.com>
 <CAO1zAVb2T3roWLU4FxhvjxJz31wRp8oYtCeHc2e8V8KOprP1DA@mail.gmail.com>
 <18013A70-4C15-4A20-8626-578C91BF80DB@gmail.com>
 <B736344B-9330-48FA-AB1D-17B76771673A@iarc.fr>
 <5854D32F-A43D-4002-8F0F-6DD0D7888B5C@gmail.com>
 <22813.49600.238510.855887@stat.math.ethz.ch>
Message-ID: <20170519041303.GA5250@slingshot.co.nz>

On Thu, 18-May-2017 at 05:46PM +0200, Martin Maechler wrote:

|> 
.....

|> 
|> Being pretty "stretched" time wise currently, I'm happy for
|> timezone-portable propositions to change the test.

Meantime, anyone who lives where DST happpens in December who wants to
get through the remaining tests can avoid this one by changing the line

> stopifnot(length(fd) == 10, identical(fd, format(dct <- as.POSIXct(dlt))))
to
> stopifnot(length(fd) == 10, identical(fd, format(dct <- as.POSIXlt(dlt))))
                                                                ^^^^
(which effectively isn't testing anything much)

A less lazy way would be to comment out the relevant lines.


|> 
|> Martin

-- 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.   
   ___    Patrick Connolly   
 {~._.~}                   Great minds discuss ideas    
 _( Y )_  	         Average minds discuss events 
(:_~*~_:)                  Small minds discuss people  
 (_)-(_)  	                      ..... Eleanor Roosevelt
	  
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.


From plummerm at iarc.fr  Fri May 19 08:16:19 2017
From: plummerm at iarc.fr (Martyn Plummer)
Date: Fri, 19 May 2017 06:16:19 +0000
Subject: [Rd] [R] R-3.4.0 fails test
In-Reply-To: <20170519041303.GA5250@slingshot.co.nz>
References: <4a086fbb-61ed-d3a5-ce23-14bbaea5531f@gmail.com>
 <ACA47D66-F469-4C75-8521-CB0DD76572C7@gmail.com>
 <20170518090016.GB4553@slingshot.co.nz>
 <29540304-FC8B-4B49-B94B-E191DEFA3BCB@gmail.com>
 <CAO1zAVarefFWaW_UmN6gCN3ppVuaGhLO7ErYdpUJmbfJ6rb4XQ@mail.gmail.com>
 <CAO1zAVb2T3roWLU4FxhvjxJz31wRp8oYtCeHc2e8V8KOprP1DA@mail.gmail.com>
 <18013A70-4C15-4A20-8626-578C91BF80DB@gmail.com>
 <B736344B-9330-48FA-AB1D-17B76771673A@iarc.fr>
 <5854D32F-A43D-4002-8F0F-6DD0D7888B5C@gmail.com>
 <22813.49600.238510.855887@stat.math.ethz.ch>,
 <20170519041303.GA5250@slingshot.co.nz>
Message-ID: <1495174481092.24933@iarc.fr>

I have fixed this in R-devel and will port it over to the R release branch in due course.

The underlying issue is that the conversion from POSIXlt to POSIXct uses the local time zone and not the CET time zone. I believe this is a bug, but I will take up that discussion elsewhere.

Martyn
________________________________________
From: Patrick Connolly <p_connolly at slingshot.co.nz>
Sent: 19 May 2017 06:13
To: Martin Maechler
Cc: peter dalgaard; Martyn Plummer; Joris Meys; R-devel
Subject: Re: [Rd] [R] R-3.4.0 fails test

On Thu, 18-May-2017 at 05:46PM +0200, Martin Maechler wrote:

|>
.....

|>
|> Being pretty "stretched" time wise currently, I'm happy for
|> timezone-portable propositions to change the test.

Meantime, anyone who lives where DST happpens in December who wants to
get through the remaining tests can avoid this one by changing the line

> stopifnot(length(fd) == 10, identical(fd, format(dct <- as.POSIXct(dlt))))
to
> stopifnot(length(fd) == 10, identical(fd, format(dct <- as.POSIXlt(dlt))))
                                                                ^^^^
(which effectively isn't testing anything much)

A less lazy way would be to comment out the relevant lines.


|>
|> Martin

--
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.
   ___    Patrick Connolly
 {~._.~}                   Great minds discuss ideas
 _( Y )_                 Average minds discuss events
(:_~*~_:)                  Small minds discuss people
 (_)-(_)                              ..... Eleanor Roosevelt

~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.
-----------------------------------------------------------------------
This message and its attachments are strictly confidenti...{{dropped:8}}


From maechler at stat.math.ethz.ch  Fri May 19 14:31:23 2017
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 19 May 2017 14:31:23 +0200
Subject: [Rd] stopifnot() does not stop at first non-TRUE argument
In-Reply-To: <812904062.672054.1495124829132@mail.yahoo.com>
References: <812904062.672054.1495124829132.ref@mail.yahoo.com>
 <812904062.672054.1495124829132@mail.yahoo.com>
Message-ID: <22814.58779.91894.396576@stat.math.ethz.ch>

>>>>> Suharto Anggono Suharto Anggono via R-devel <r-devel at r-project.org>
>>>>>     on Thu, 18 May 2017 16:27:09 +0000 writes:

    >> From an example in

    >> http://www.uni-muenster.de/ZIV.BennoSueselbeck/s-html/helpfiles/nargs.html
    >> , number of arguments in '...' can be obtained by

    > (function(...)nargs())(...) .

neat and good.  Though really is not exactly "well readable".

In the mean time, there is   ...length()   in R-devel [somewhat experimentally]

    > I now realize that sys.call() doesn't expand '...' when
    > the function is called with '...'. It just returns the call as is.
yes.
    > If 'stopifnot' uses sys.call() instead of
    > match.call() , the following example behaves improperly: 

    > g <- function(...) stopifnot(...)
    > g(TRUE, FALSE)

Indeed.  Very improperly (it does not stop).

However, calling stopifnot() with a '...' passed from above is
not a very good idea anyway, because stopifnot has to assume it
is called with explicit expressions.
Hence we have

  > g <- function(...) stopifnot(...) ;  g(1 == 1, 3 < 1)
  Error: ..2 is not TRUE

{and to "fix" this, e.g., with an extra optional argument} would
 lead to more complications  which I really think we do not want}.

But the example does show we should keep match.call().
Martin

    > --------------------------------------------
    > On Thu, 18/5/17, Martin Maechler
    > <maechler at stat.math.ethz.ch> wrote:

    >  Subject: Re: [Rd] stopifnot() does not stop at first
    > non-TRUE argument

    >  Cc: r-devel at r-project.org Date: Thursday, 18 May, 2017,
    > 3:03 PM
 
>>>>> Suharto Anggono Suharto Anggono via R-devel <r-devel at r-project.org>
>>>>>     on Tue, 16 May 2017 16:37:45 +0000 writes:

    >> switch(i, ...)  extracts 'i'-th argument in '...'. It is
    >> like eval(as.name(paste0("..", i))) .

    > Yes, that's neat.

    > It is only almost the same: in the case of illegal 'i' the
    > switch() version returns invisible(NULL)

    > whereas the version we'd want should signal an error,
    > typically the same error message as

    >> t2 <- function(...) ..2 t2(1)
    >   Error in t2(1) (from #1) : the ... list does not contain
    > 2 elements
    >> 


    >> Just mentioning other things: - For 'n', n <- nargs() can
    >> be used.

    > I know .. [in this case, where '...' is the only formal
    > argument of the function]

    >> - sys.call() can be used in place of match.call() .

    > Hmm... in many cases, yes.... notably, as we do *not* want
    > the argument names here, I think you are right.

    > ______________________________________________
    > R-devel at r-project.org mailing list
    > https://stat.ethz.ch/mailman/listinfo/r-devel


From michaelchirico4 at gmail.com  Fri May 19 17:23:12 2017
From: michaelchirico4 at gmail.com (Michael Chirico)
Date: Fri, 19 May 2017 11:23:12 -0400
Subject: [Rd] Inconsistency in handling of numeric input with %d by sprintf
Message-ID: <CAPRVBcx3OjFK0G=hk1g8qChYiQ7cuZPpep-kju-_BD4MErhDuA@mail.gmail.com>

Consider

#as.numeric for emphasis
sprintf('%d', as.numeric(1))
# [1] "1"

vs.

sprintf('%d', NA_real_)

>  Error in sprintf("%d", NA_real_) :

   invalid format '%d'; use format %f, %e, %g or %a for numeric object
>

I understand the error is correct, but if it works for other numeric input,
why doesn't R just coerce NA_real_ to NA_integer_?

Michael Chirico

	[[alternative HTML version deleted]]


From zubinmeva at qbitlogic.com  Fri May 19 18:12:53 2017
From: zubinmeva at qbitlogic.com (Zubin Mevawalla)
Date: Fri, 19 May 2017 12:12:53 -0400
Subject: [Rd] Null pointer dereference?
Message-ID: <CANn+0XObL+1EHdGCObmRzNMRLmutW8OrPjpAryfSLHOhbxpQ5Q@mail.gmail.com>

I was curious if this was a real null pointer dereference issue in
R-devel/src/library/grDevices/src/devPS.c on line 1009?

1000: static type1fontinfo makeType1Font()
1001: {
1002:     type1fontinfo font = (Type1FontInfo *) malloc(sizeof(Type1FontInfo));
1003:     /*
1004:      * Initialise font->metrics.KernPairs to NULL
1005:      * so that we know NOT to free it if we fail to
1006:      * load this font and have to
1007:      * bail out and free this type1fontinfo
1008:      */
1009:     font->metrics.KernPairs = NULL;
1010:     if (!font)
1011: warning(_("failed to allocate Type 1 font info"));
1012:     return font;
1013: }

`font` is conceivably null because there is a null check on line 1010,
but is dereferenced on 1009.

CodeAi, an automated repair tool being developed at Qbit logic,
suggested an if-guard as a fix:

@@ -1006,9 +1006,7 @@ static type1fontinfo makeType1Font()
      * load this font and have to
      * bail out and free this type1fontinfo
      */
-    if(font) {
-        font->metrics.KernPairs = NULL;
-    }
+    font->metrics.KernPairs = NULL;
     if (!font)
        warning(_("failed to allocate Type 1 font info"));
     return font;

Could I submit this as a patch if it looks alright?

Thanks so much,

Zubin


From wdunlap at tibco.com  Fri May 19 18:30:42 2017
From: wdunlap at tibco.com (William Dunlap)
Date: Fri, 19 May 2017 09:30:42 -0700
Subject: [Rd] stopifnot() does not stop at first non-TRUE argument
In-Reply-To: <22814.58779.91894.396576@stat.math.ethz.ch>
References: <812904062.672054.1495124829132.ref@mail.yahoo.com>
 <812904062.672054.1495124829132@mail.yahoo.com>
 <22814.58779.91894.396576@stat.math.ethz.ch>
Message-ID: <CAF8bMcYUT9qZSJq6hppg6V_=BQAdqyx+zyXnvSUZbpQdB+hdtg@mail.gmail.com>

While you are fiddling with stopifnot(), please consider changing the form
of the error thrown so that it includes the caller's call.  The change
would be from something like
  stop( <<the message>> )
to
  stop(simpleError( <<the message>>, sys.call(-1)))

For the following code
  f <- function(x, y) {
      stopifnot(x > y)
      x - y
  }
  g <- function(x, y, z) {
      c(f(x, y), f(y, z))
  }
  g(6,3,4)
you would see
  Error in f(y, z) : x > y is not TRUE
instead of the less informative
  Error: x > y is not TRUE



Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Fri, May 19, 2017 at 5:31 AM, Martin Maechler <maechler at stat.math.ethz.ch
> wrote:

> >>>>> Suharto Anggono Suharto Anggono via R-devel <r-devel at r-project.org>
> >>>>>     on Thu, 18 May 2017 16:27:09 +0000 writes:
>
>     >> From an example in
>
>     >> http://www.uni-muenster.de/ZIV.BennoSueselbeck/s-html/
> helpfiles/nargs.html
>     >> , number of arguments in '...' can be obtained by
>
>     > (function(...)nargs())(...) .
>
> neat and good.  Though really is not exactly "well readable".
>
> In the mean time, there is   ...length()   in R-devel [somewhat
> experimentally]
>
>     > I now realize that sys.call() doesn't expand '...' when
>     > the function is called with '...'. It just returns the call as is.
> yes.
>     > If 'stopifnot' uses sys.call() instead of
>     > match.call() , the following example behaves improperly:
>
>     > g <- function(...) stopifnot(...)
>     > g(TRUE, FALSE)
>
> Indeed.  Very improperly (it does not stop).
>
> However, calling stopifnot() with a '...' passed from above is
> not a very good idea anyway, because stopifnot has to assume it
> is called with explicit expressions.
> Hence we have
>
>   > g <- function(...) stopifnot(...) ;  g(1 == 1, 3 < 1)
>   Error: ..2 is not TRUE
>
> {and to "fix" this, e.g., with an extra optional argument} would
>  lead to more complications  which I really think we do not want}.
>
> But the example does show we should keep match.call().
> Martin
>
>     > --------------------------------------------
>     > On Thu, 18/5/17, Martin Maechler
>     > <maechler at stat.math.ethz.ch> wrote:
>
>     >  Subject: Re: [Rd] stopifnot() does not stop at first
>     > non-TRUE argument
>
>     >  Cc: r-devel at r-project.org Date: Thursday, 18 May, 2017,
>     > 3:03 PM
>
> >>>>> Suharto Anggono Suharto Anggono via R-devel <r-devel at
> r-project.org>
> >>>>>     on Tue, 16 May 2017 16:37:45 +0000 writes:
>
>     >> switch(i, ...)  extracts 'i'-th argument in '...'. It is
>     >> like eval(as.name(paste0("..", i))) .
>
>     > Yes, that's neat.
>
>     > It is only almost the same: in the case of illegal 'i' the
>     > switch() version returns invisible(NULL)
>
>     > whereas the version we'd want should signal an error,
>     > typically the same error message as
>
>     >> t2 <- function(...) ..2 t2(1)
>     >   Error in t2(1) (from #1) : the ... list does not contain
>     > 2 elements
>     >>
>
>
>     >> Just mentioning other things: - For 'n', n <- nargs() can
>     >> be used.
>
>     > I know .. [in this case, where '...' is the only formal
>     > argument of the function]
>
>     >> - sys.call() can be used in place of match.call() .
>
>     > Hmm... in many cases, yes.... notably, as we do *not* want
>     > the argument names here, I think you are right.
>
>     > ______________________________________________
>     > R-devel at r-project.org mailing list
>     > https://stat.ethz.ch/mailman/listinfo/r-devel
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From tomas.kalibera at gmail.com  Fri May 19 19:07:31 2017
From: tomas.kalibera at gmail.com (Tomas Kalibera)
Date: Fri, 19 May 2017 19:07:31 +0200
Subject: [Rd] Null pointer dereference?
In-Reply-To: <CANn+0XObL+1EHdGCObmRzNMRLmutW8OrPjpAryfSLHOhbxpQ5Q@mail.gmail.com>
References: <CANn+0XObL+1EHdGCObmRzNMRLmutW8OrPjpAryfSLHOhbxpQ5Q@mail.gmail.com>
Message-ID: <821898ca-41d1-37dc-7edb-98a8847070b7@gmail.com>

Thanks, the tool is indeed right, this is a real error. Although it is 
unlikely to trigger and unlikely to cause new problems (R would fail 
soon anyway if out of memory), it is clearly something to be fixed and 
something to be classified as "true positive".

I've fixed this in a way that is consistent with coding style in that file.

Best,
Tomas

On 05/19/2017 06:12 PM, Zubin Mevawalla wrote:
> I was curious if this was a real null pointer dereference issue in
> R-devel/src/library/grDevices/src/devPS.c on line 1009?
>
> 1000: static type1fontinfo makeType1Font()
> 1001: {
> 1002:     type1fontinfo font = (Type1FontInfo *) malloc(sizeof(Type1FontInfo));
> 1003:     /*
> 1004:      * Initialise font->metrics.KernPairs to NULL
> 1005:      * so that we know NOT to free it if we fail to
> 1006:      * load this font and have to
> 1007:      * bail out and free this type1fontinfo
> 1008:      */
> 1009:     font->metrics.KernPairs = NULL;
> 1010:     if (!font)
> 1011: warning(_("failed to allocate Type 1 font info"));
> 1012:     return font;
> 1013: }
>
> `font` is conceivably null because there is a null check on line 1010,
> but is dereferenced on 1009.
>
> CodeAi, an automated repair tool being developed at Qbit logic,
> suggested an if-guard as a fix:
>
> @@ -1006,9 +1006,7 @@ static type1fontinfo makeType1Font()
>        * load this font and have to
>        * bail out and free this type1fontinfo
>        */
> -    if(font) {
> -        font->metrics.KernPairs = NULL;
> -    }
> +    font->metrics.KernPairs = NULL;
>       if (!font)
>          warning(_("failed to allocate Type 1 font info"));
>       return font;
>
> Could I submit this as a patch if it looks alright?
>
> Thanks so much,
>
> Zubin
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From kasperdanielhansen at gmail.com  Sat May 20 01:29:19 2017
From: kasperdanielhansen at gmail.com (Kasper Daniel Hansen)
Date: Fri, 19 May 2017 19:29:19 -0400
Subject: [Rd] test fails when requesting LC_CTYPE
Message-ID: <CAC2h7usJEYG_xt4LoMCkZmOO9S36gw=L0=jJzVkE36q53Qv8sA@mail.gmail.com>

On RedHat Enterprise Linux 6, the test below fails (this is using the stock
GCC 4.4.7) from R-devel r72707.  LC_CTYPE is unset when I run it, but
LANG=en_US.UTF-8

It also failed "yesterday" where as far as I recall the test code looked a
bit different.

Best,
Kasper

> ## Results differed by platform, but some gave incorrect results on
string 10.
>
>
> ## str() on large strings (in multibyte locales; changing locale may not
work everywhere
> oloc <- Sys.getlocale("LC_CTYPE")
> mbyte.lc <- {
+     if(.Platform$OS.type == "windows")
+       "English_United States.28605"
+     else if(grepl("[.]UTF-8$", oloc, ignore.case=TRUE)) # typically
nowadays
+       oloc
+     else
+       "C.UTF-8" # or rather "en_US.UTF-8" (? from  system("locale -a|
fgrep .UTF-8") )
+ }
> stopifnot(identical(Sys.setlocale("LC_CTYPE", mbyte.lc), mbyte.lc))
Error: identical(Sys.setlocale("LC_CTYPE", mbyte.lc), mbyte.lc) is not TRUE
In addition: Warning message:
In Sys.setlocale("LC_CTYPE", mbyte.lc) :
  OS reports request to set locale to "C.UTF-8" cannot be honored
Execution halted

	[[alternative HTML version deleted]]


From kasperdanielhansen at gmail.com  Sat May 20 02:09:24 2017
From: kasperdanielhansen at gmail.com (Kasper Daniel Hansen)
Date: Fri, 19 May 2017 20:09:24 -0400
Subject: [Rd] test fails when requesting LC_CTYPE
In-Reply-To: <CAC2h7usJEYG_xt4LoMCkZmOO9S36gw=L0=jJzVkE36q53Qv8sA@mail.gmail.com>
References: <CAC2h7usJEYG_xt4LoMCkZmOO9S36gw=L0=jJzVkE36q53Qv8sA@mail.gmail.com>
Message-ID: <CAC2h7uvwn9x68OpHgmnhYy0Zih=hZKMgp8YOxpjoWfx-UNEs0A@mail.gmail.com>

I rebuilt R with
  export LC_CTYPE=en_US.UTF-8
and the test still fail.  Surprisingly, when I run R from the bin directory
and execute the test code, it runs without error:

> oloc <- Sys.getlocale("LC_CTYPE")
> mbyte.lc <- {
+     if(.Platform$OS.type == "windows")
+       "English_United States.28605"
+     else if(grepl("[.]UTF-8$", oloc, ignore.case=TRUE)) # typically
nowadays
+       oloc
+     else
+       "C.UTF-8" # or rather "en_US.UTF-8" (? from  system("locale -a|
fgrep .UTF-8") )
+ }
> stopifnot(identical(Sys.setlocale("LC_CTYPE", mbyte.lc), mbyte.lc))
> oloc
[1] "en_US.UTF-8"
> mbyte.lc
[1] "en_US.UTF-8"


On Fri, May 19, 2017 at 7:29 PM, Kasper Daniel Hansen <
kasperdanielhansen at gmail.com> wrote:

> On RedHat Enterprise Linux 6, the test below fails (this is using the
> stock GCC 4.4.7) from R-devel r72707.  LC_CTYPE is unset when I run it, but
> LANG=en_US.UTF-8
>
> It also failed "yesterday" where as far as I recall the test code looked a
> bit different.
>
> Best,
> Kasper
>
> > ## Results differed by platform, but some gave incorrect results on
> string 10.
> >
> >
> > ## str() on large strings (in multibyte locales; changing locale may not
> work everywhere
> > oloc <- Sys.getlocale("LC_CTYPE")
> > mbyte.lc <- {
> +     if(.Platform$OS.type == "windows")
> +       "English_United States.28605"
> +     else if(grepl("[.]UTF-8$", oloc, ignore.case=TRUE)) # typically
> nowadays
> +       oloc
> +     else
> +       "C.UTF-8" # or rather "en_US.UTF-8" (? from  system("locale -a|
> fgrep .UTF-8") )
> + }
> > stopifnot(identical(Sys.setlocale("LC_CTYPE", mbyte.lc), mbyte.lc))
> Error: identical(Sys.setlocale("LC_CTYPE", mbyte.lc), mbyte.lc) is not
> TRUE
> In addition: Warning message:
> In Sys.setlocale("LC_CTYPE", mbyte.lc) :
>   OS reports request to set locale to "C.UTF-8" cannot be honored
> Execution halted
>

	[[alternative HTML version deleted]]


From rpbarry at alaska.edu  Sat May 20 11:14:20 2017
From: rpbarry at alaska.edu (Ronald Barry)
Date: Sat, 20 May 2017 01:14:20 -0800
Subject: [Rd] What to do re 'replacing previous import' errors
Message-ID: <CAOVYLHeoFeOKQdYa19FocOKCwnHu+K8+O-2+8zoPkXwOitSZNA@mail.gmail.com>

After looking at a few discussions of this problem online, I can't seem to
find a good solution.  When running a CHECK on a package I'm working on
(which passed all tests a few years ago), I get the following 'significant
warning':

Warning: replacing previous import ?spam::tail? by ?utils::tail? when
loading ?latticeDensity?
  Warning: replacing previous import ?spam::head? by ?utils::head? when
loading ?latticeDensity?

Humorously enough, I don't use the 'tail' function anywhere.  It seems to
be some sort of namespace clash between spam and utils.  In my namespace,
I'm importing spam and utils.  Is there a fix for this problem?  Thanks.

	[[alternative HTML version deleted]]


From murdoch.duncan at gmail.com  Sat May 20 11:50:52 2017
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Sat, 20 May 2017 05:50:52 -0400
Subject: [Rd] What to do re 'replacing previous import' errors
In-Reply-To: <CAOVYLHeoFeOKQdYa19FocOKCwnHu+K8+O-2+8zoPkXwOitSZNA@mail.gmail.com>
References: <CAOVYLHeoFeOKQdYa19FocOKCwnHu+K8+O-2+8zoPkXwOitSZNA@mail.gmail.com>
Message-ID: <3f532845-4835-6059-ce6e-6361367b2801@gmail.com>

On 20/05/2017 5:14 AM, Ronald Barry wrote:
> After looking at a few discussions of this problem online, I can't seem to
> find a good solution.  When running a CHECK on a package I'm working on
> (which passed all tests a few years ago), I get the following 'significant
> warning':
>
> Warning: replacing previous import ?spam::tail? by ?utils::tail? when
> loading ?latticeDensity?
>   Warning: replacing previous import ?spam::head? by ?utils::head? when
> loading ?latticeDensity?
>
> Humorously enough, I don't use the 'tail' function anywhere.  It seems to
> be some sort of namespace clash between spam and utils.  In my namespace,
> I'm importing spam and utils.  Is there a fix for this problem?  Thanks.

It's generally a good idea to import only the functions you need, not 
everything. If latticeDensity used

importFrom(utils, foo, bar)

etc., then it would only get a clash if you explicitly imported two 
things with the same name.  (You can do a rename when importing if you 
really want to do that.)

Duncan Murdoch


From maechler at stat.math.ethz.ch  Sat May 20 15:35:10 2017
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 20 May 2017 15:35:10 +0200
Subject: [Rd] test fails when requesting LC_CTYPE
In-Reply-To: <CAC2h7uvwn9x68OpHgmnhYy0Zih=hZKMgp8YOxpjoWfx-UNEs0A@mail.gmail.com>
References: <CAC2h7usJEYG_xt4LoMCkZmOO9S36gw=L0=jJzVkE36q53Qv8sA@mail.gmail.com>
 <CAC2h7uvwn9x68OpHgmnhYy0Zih=hZKMgp8YOxpjoWfx-UNEs0A@mail.gmail.com>
Message-ID: <22816.17934.988043.394635@stat.math.ethz.ch>

>>>>> Kasper Daniel Hansen <kasperdanielhansen at gmail.com>
>>>>>     on Fri, 19 May 2017 20:09:24 -0400 writes:

    > I rebuilt R with
    > export LC_CTYPE=en_US.UTF-8
    > and the test still fail.  Surprisingly, when I run R from the bin directory
    > and execute the test code, it runs without error:

  >> oloc <- Sys.getlocale("LC_CTYPE")
  >> mbyte.lc <- {
  > +     if(.Platform$OS.type == "windows")
  > +       "English_United States.28605"
  > +     else if(grepl("[.]UTF-8$", oloc, ignore.case=TRUE)) # typically nowadays
  > +       oloc
  > +     else
  > +       "C.UTF-8" # or rather "en_US.UTF-8" (? from  system("locale -a| fgrep .UTF-8") )
  > + }
  >> stopifnot(identical(Sys.setlocale("LC_CTYPE", mbyte.lc), mbyte.lc))
  >> oloc
  > [1] "en_US.UTF-8"
  >> mbyte.lc
  > [1] "en_US.UTF-8"

I had been making these changes in R-devel after offline
discussions with Linux users for which the original check (using "en_UK.UTF-8")
failed.

What I read below is suggesting that "C.UTF-8" is not okay
either, as a fallback.

It seems we should use "en_US.UTF-8" as fallback instead
(though I assume that won't work in North Korea).

I've committed a version that does that _and_ no longer stops
when that identical() does not give a 'TRUE'.

Martin

    > On Fri, May 19, 2017 at 7:29 PM, Kasper Daniel Hansen <
    > kasperdanielhansen at gmail.com> wrote:

    >> On RedHat Enterprise Linux 6, the test below fails (this is using the
    >> stock GCC 4.4.7) from R-devel r72707.  LC_CTYPE is unset when I run it, but
    >> LANG=en_US.UTF-8
    >> 
    >> It also failed "yesterday" where as far as I recall the test code looked a
    >> bit different.
    >> 
    >> Best,
    >> Kasper
    >> 
    >> > ## Results differed by platform, but some gave incorrect results on
    >> string 10.
    >> >
    >> >
    >> > ## str() on large strings (in multibyte locales; changing locale may not
    >> work everywhere
    >> > oloc <- Sys.getlocale("LC_CTYPE")
    >> > mbyte.lc <- {
    >> +     if(.Platform$OS.type == "windows")
    >> +       "English_United States.28605"
    >> +     else if(grepl("[.]UTF-8$", oloc, ignore.case=TRUE)) # typically
    >> nowadays
    >> +       oloc
    >> +     else
    >> +       "C.UTF-8" # or rather "en_US.UTF-8" (? from  system("locale -a|
    >> fgrep .UTF-8") )
    >> + }
    >> > stopifnot(identical(Sys.setlocale("LC_CTYPE", mbyte.lc), mbyte.lc))
    >> Error: identical(Sys.setlocale("LC_CTYPE", mbyte.lc), mbyte.lc) is not
    >> TRUE
    >> In addition: Warning message:
    >> In Sys.setlocale("LC_CTYPE", mbyte.lc) :
    >> OS reports request to set locale to "C.UTF-8" cannot be honored
    >> Execution halted
    >>


From maechler at stat.math.ethz.ch  Sat May 20 17:41:05 2017
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 20 May 2017 17:41:05 +0200
Subject: [Rd] stopifnot() does not stop at first non-TRUE argument
In-Reply-To: <CAF8bMcYUT9qZSJq6hppg6V_=BQAdqyx+zyXnvSUZbpQdB+hdtg@mail.gmail.com>
References: <812904062.672054.1495124829132.ref@mail.yahoo.com>
 <812904062.672054.1495124829132@mail.yahoo.com>
 <22814.58779.91894.396576@stat.math.ethz.ch>
 <CAF8bMcYUT9qZSJq6hppg6V_=BQAdqyx+zyXnvSUZbpQdB+hdtg@mail.gmail.com>
Message-ID: <22816.25489.288803.850541@stat.math.ethz.ch>

>>>>> William Dunlap <wdunlap at tibco.com>
>>>>>     on Fri, 19 May 2017 09:30:42 -0700 writes:

    > While you are fiddling with stopifnot(), please consider changing the form
    > of the error thrown so that it includes the caller's call.  The change
    > would be from something like
    >   stop( <<the message>> )
    > to
    >   stop(simpleError( <<the message>>, sys.call(-1)))

    > For the following code
    >   f <- function(x, y) {
    >     stopifnot(x > y)
    >     x - y
    >   }
    >   g <- function(x, y, z) {
    >      c(f(x, y), f(y, z))
    >   }
    >   g(6,3,4)

    > you would see
    >   Error in f(y, z) : x > y is not TRUE
    > instead of the less informative
    >   Error: x > y is not TRUE

well, yes, I have been fiddling .. ;-)

and your proposal above is quite remarkable!
I hadn't been aware of (the consequence of) this possibility.

I will do that change in addition to the planned ones, just to
make the changes slightly more modular.
(It will need a change in   tests/isas-tests.Rout.save  as
indeed, the _messages_ of non-toplevel  stopifnot() calls will
change too.
This may affect package checks check for the _wording_ of
stopifnot error messages [which may not be the best idea
.. though understandable for regression checks].

Martin


    > Bill Dunlap
    > TIBCO Software
    > wdunlap tibco.com

    > On Fri, May 19, 2017 at 5:31 AM, Martin Maechler <maechler at stat.math.ethz.ch
    >> wrote:

    >> >>>>> Suharto Anggono Suharto Anggono via R-devel <r-devel at r-project.org>
    >> >>>>>     on Thu, 18 May 2017 16:27:09 +0000 writes:
    >> 
    >> >> From an example in
    >> 
    >> >> http://www.uni-muenster.de/ZIV.BennoSueselbeck/s-html/
    >> helpfiles/nargs.html
    >> >> , number of arguments in '...' can be obtained by
    >> 
    >> > (function(...)nargs())(...) .
    >> 
    >> neat and good.  Though really is not exactly "well readable".
    >> 
    >> In the mean time, there is   ...length()   in R-devel [somewhat
    >> experimentally]
    >> 
    >> > I now realize that sys.call() doesn't expand '...' when
    >> > the function is called with '...'. It just returns the call as is.
    >> yes.
    >> > If 'stopifnot' uses sys.call() instead of
    >> > match.call() , the following example behaves improperly:
    >> 
    >> > g <- function(...) stopifnot(...)
    >> > g(TRUE, FALSE)
    >> 
    >> Indeed.  Very improperly (it does not stop).
    >> 
    >> However, calling stopifnot() with a '...' passed from above is
    >> not a very good idea anyway, because stopifnot has to assume it
    >> is called with explicit expressions.
    >> Hence we have
    >> 
    >> > g <- function(...) stopifnot(...) ;  g(1 == 1, 3 < 1)
    >> Error: ..2 is not TRUE
    >> 
    >> {and to "fix" this, e.g., with an extra optional argument} would
    >> lead to more complications  which I really think we do not want}.
    >> 
    >> But the example does show we should keep match.call().
    >> Martin
    >> 
    >> > --------------------------------------------
    >> > On Thu, 18/5/17, Martin Maechler
    >> > <maechler at stat.math.ethz.ch> wrote:
    >> 
    >> >  Subject: Re: [Rd] stopifnot() does not stop at first
    >> > non-TRUE argument
    >> 
    >> >  Cc: r-devel at r-project.org Date: Thursday, 18 May, 2017,
    >> > 3:03 PM
    >> 
    >> >>>>> Suharto Anggono Suharto Anggono via R-devel <r-devel at
    r-project.org> 
    >> >>>>>     on Tue, 16 May 2017 16:37:45 +0000 writes:
    >> 
    >> >> switch(i, ...)  extracts 'i'-th argument in '...'. It is
    >> >> like eval(as.name(paste0("..", i))) .
    >> 
    >> > Yes, that's neat.
    >> 
    >> > It is only almost the same: in the case of illegal 'i' the
    >> > switch() version returns invisible(NULL)
    >> 
    >> > whereas the version we'd want should signal an error,
    >> > typically the same error message as
    >> 
    >> >> t2 <- function(...) ..2 t2(1)
    >> >   Error in t2(1) (from #1) : the ... list does not contain
    >> > 2 elements
    >> >>
    >> 
    >> 
    >> >> Just mentioning other things: - For 'n', n <- nargs() can
    >> >> be used.
    >> 
    >> > I know .. [in this case, where '...' is the only formal
    >> > argument of the function]
    >> 
    >> >> - sys.call() can be used in place of match.call() .
    >> 
    >> > Hmm... in many cases, yes.... notably, as we do *not* want
    >> > the argument names here, I think you are right.
    >> 
    >> > ______________________________________________
    >> > R-devel at r-project.org mailing list
    >> > https://stat.ethz.ch/mailman/listinfo/r-devel
    >> 
    >> ______________________________________________
    >> R-devel at r-project.org mailing list
    >> https://stat.ethz.ch/mailman/listinfo/r-devel
    >> 

    > [[alternative HTML version deleted]]


From sbbrouwer at gmail.com  Sat May 20 18:20:30 2017
From: sbbrouwer at gmail.com (Sietse Brouwer)
Date: Sat, 20 May 2017 18:20:30 +0200
Subject: [Rd] Bug: floating point bug in nclass.FD can cause hist() to
	crash
In-Reply-To: <5378285b-fbba-8637-42df-f7ce5ef1dd82@prodsyse.com>
References: <CAF=dkzxor=d5FLN_W_FSg5yYjEKFrQxuPiADdqQqqX_mzrDdzQ@mail.gmail.com>
 <5378285b-fbba-8637-42df-f7ce5ef1dd82@prodsyse.com>
Message-ID: <CAF=dkzyrWT1ovJSWJ9oNJ3N-Pkc573tGM5auzCTc_WjBGu8+ZQ@mail.gmail.com>

Hi, all,

Sietse wrote:
> Floating point errors can cause a data vector to have an ultra-small
> inter-quartile range, which causes `grDevices::nclass.FD` to suggest
> an absurdly large number of breaks to `graphics::hist(breaks="FD")`.
> Because this large float becomes NA when converted to integer, hist's
> call to `base::pretty` crashes.

I have been provided with an account, and filed the bug at
https://bugs.r-project.org/bugzilla/show_bug.cgi?id=17274

Discussion continues there.

Cheers,
Sietse


From murdoch.duncan at gmail.com  Sun May 21 20:57:44 2017
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Sun, 21 May 2017 14:57:44 -0400
Subject: [Rd] [R] Somewhat obscure bug in R 3.4.0 building from source
In-Reply-To: <CAPvCojJqJYiU4LN2XvxH7LMtzqeAghnkztXJ2YnjU4w6UrUFNw@mail.gmail.com>
References: <CAPvCojJqJYiU4LN2XvxH7LMtzqeAghnkztXJ2YnjU4w6UrUFNw@mail.gmail.com>
Message-ID: <f24508d4-c232-2cc1-8302-b73e39a93869@gmail.com>

On 21/05/2017 10:30 AM, Peter Carbonetto wrote:
> Hi,
>
> I uncovered a bug in installing R 3.4.0 from source in Linux, following the
> standard procedure (configure; make; make install). Is this an appropriate
> place to report this bug? If not, can you please direct me to the
> appropriate place?

Generally R-devel is better; I've responded there.

>
> The error occurs only when I do "make clean" followed by "make" again; make
> works the first time.
>
> The error is a failure to build NEWS.pdf:
>
> Error in texi2dvi(file = file, pdf = TRUE, clean = clean, quiet = quiet,  :
>   pdflatex is not available
> Calls: <Anonymous> -> texi2pdf -> texi2dvi
> Execution halted
> make[1]: *** [NEWS.pdf] Error 1
> make: [docs] Error 2 (ignored)
>
> and can be reproduced wit the following sequence:
>
> ./configure
> make
> make clean
> make

We usually don't build in the source directory; see the second 
recommendation in the admin manual section 2.1.  So it's possible 
there's a bug triggered when you do that.  Can you try building in a 
separate directory?

Duncan Murdoch

>
> This suggests to me that perhaps "make clean" is not working.
>
> I'm happy to provide more details so that you are able to reproduce the bug.
>
> Thanks,
>
> Peter Carbonetto, Ph.D.
> Computational Staff Scientist, Statistics & Genetics
> Research Computing Center
> University of Chicago
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.
>


From pdalgd at gmail.com  Mon May 22 00:07:08 2017
From: pdalgd at gmail.com (Peter Dalgaard)
Date: Mon, 22 May 2017 00:07:08 +0200
Subject: [Rd] [R] Somewhat obscure bug in R 3.4.0 building from source
In-Reply-To: <f24508d4-c232-2cc1-8302-b73e39a93869@gmail.com>
References: <CAPvCojJqJYiU4LN2XvxH7LMtzqeAghnkztXJ2YnjU4w6UrUFNw@mail.gmail.com>
 <f24508d4-c232-2cc1-8302-b73e39a93869@gmail.com>
Message-ID: <863B1238-607F-45A7-A28B-0FDFF9CF150A@gmail.com>

Inline below...

> On 21 May 2017, at 20:57 , Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
> 
> On 21/05/2017 10:30 AM, Peter Carbonetto wrote:
>> Hi,
>> 
>> I uncovered a bug in installing R 3.4.0 from source in Linux, following the
>> standard procedure (configure; make; make install). Is this an appropriate
>> place to report this bug? If not, can you please direct me to the
>> appropriate place?
> 
> Generally R-devel is better; I've responded there.
> 
>> 
>> The error occurs only when I do "make clean" followed by "make" again; make
>> works the first time.
>> 
>> The error is a failure to build NEWS.pdf:
>> 
>> Error in texi2dvi(file = file, pdf = TRUE, clean = clean, quiet = quiet,  :
>>  pdflatex is not available
>> Calls: <Anonymous> -> texi2pdf -> texi2dvi
>> Execution halted
>> make[1]: *** [NEWS.pdf] Error 1
>> make: [docs] Error 2 (ignored)
>> 
>> and can be reproduced wit the following sequence:
>> 
>> ./configure
>> make
>> make clean
>> make
> 
> We usually don't build in the source directory; see the second recommendation in the admin manual section 2.1.  So it's possible there's a bug triggered when you do that.  Can you try building in a separate directory?

Notice that the error is that "pdflatex" is missing from your setup. We do, for the benefit of users with defective TeX installations supply a pre-built NEWS.pdf (and NEWS.html too) in the source tarballs. However, they are technically make targets and make clean will wipe them; in that case, you had better have the tools to rebuild them! 

-pd

> 
> Duncan Murdoch
> 
>> 
>> This suggests to me that perhaps "make clean" is not working.
>> 
>> I'm happy to provide more details so that you are able to reproduce the bug.
>> 
>> Thanks,
>> 
>> Peter Carbonetto, Ph.D.
>> Computational Staff Scientist, Statistics & Genetics
>> Research Computing Center
>> University of Chicago
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
>> and provide commented, minimal, self-contained, reproducible code.
>> 
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From kasperdanielhansen at gmail.com  Mon May 22 04:18:11 2017
From: kasperdanielhansen at gmail.com (Kasper Daniel Hansen)
Date: Sun, 21 May 2017 22:18:11 -0400
Subject: [Rd] test fails when requesting LC_CTYPE
In-Reply-To: <22816.17934.988043.394635@stat.math.ethz.ch>
References: <CAC2h7usJEYG_xt4LoMCkZmOO9S36gw=L0=jJzVkE36q53Qv8sA@mail.gmail.com>
 <CAC2h7uvwn9x68OpHgmnhYy0Zih=hZKMgp8YOxpjoWfx-UNEs0A@mail.gmail.com>
 <22816.17934.988043.394635@stat.math.ethz.ch>
Message-ID: <CAC2h7uuNzm5P2ZPz33h4HZbut8nAs5B7q=dK7ERiAqoqeNDDLQ@mail.gmail.com>

Thanks, works for me.

Best,
Kasper

On Sat, May 20, 2017 at 9:35 AM, Martin Maechler <maechler at stat.math.ethz.ch
> wrote:

> >>>>> Kasper Daniel Hansen <kasperdanielhansen at gmail.com>
> >>>>>     on Fri, 19 May 2017 20:09:24 -0400 writes:
>
>     > I rebuilt R with
>     > export LC_CTYPE=en_US.UTF-8
>     > and the test still fail.  Surprisingly, when I run R from the bin
> directory
>     > and execute the test code, it runs without error:
>
>   >> oloc <- Sys.getlocale("LC_CTYPE")
>   >> mbyte.lc <- {
>   > +     if(.Platform$OS.type == "windows")
>   > +       "English_United States.28605"
>   > +     else if(grepl("[.]UTF-8$", oloc, ignore.case=TRUE)) # typically
> nowadays
>   > +       oloc
>   > +     else
>   > +       "C.UTF-8" # or rather "en_US.UTF-8" (? from  system("locale
> -a| fgrep .UTF-8") )
>   > + }
>   >> stopifnot(identical(Sys.setlocale("LC_CTYPE", mbyte.lc), mbyte.lc))
>   >> oloc
>   > [1] "en_US.UTF-8"
>   >> mbyte.lc
>   > [1] "en_US.UTF-8"
>
> I had been making these changes in R-devel after offline
> discussions with Linux users for which the original check (using
> "en_UK.UTF-8")
> failed.
>
> What I read below is suggesting that "C.UTF-8" is not okay
> either, as a fallback.
>
> It seems we should use "en_US.UTF-8" as fallback instead
> (though I assume that won't work in North Korea).
>
> I've committed a version that does that _and_ no longer stops
> when that identical() does not give a 'TRUE'.
>
> Martin
>
>     > On Fri, May 19, 2017 at 7:29 PM, Kasper Daniel Hansen <
>     > kasperdanielhansen at gmail.com> wrote:
>
>     >> On RedHat Enterprise Linux 6, the test below fails (this is using
> the
>     >> stock GCC 4.4.7) from R-devel r72707.  LC_CTYPE is unset when I run
> it, but
>     >> LANG=en_US.UTF-8
>     >>
>     >> It also failed "yesterday" where as far as I recall the test code
> looked a
>     >> bit different.
>     >>
>     >> Best,
>     >> Kasper
>     >>
>     >> > ## Results differed by platform, but some gave incorrect results
> on
>     >> string 10.
>     >> >
>     >> >
>     >> > ## str() on large strings (in multibyte locales; changing locale
> may not
>     >> work everywhere
>     >> > oloc <- Sys.getlocale("LC_CTYPE")
>     >> > mbyte.lc <- {
>     >> +     if(.Platform$OS.type == "windows")
>     >> +       "English_United States.28605"
>     >> +     else if(grepl("[.]UTF-8$", oloc, ignore.case=TRUE)) #
> typically
>     >> nowadays
>     >> +       oloc
>     >> +     else
>     >> +       "C.UTF-8" # or rather "en_US.UTF-8" (? from  system("locale
> -a|
>     >> fgrep .UTF-8") )
>     >> + }
>     >> > stopifnot(identical(Sys.setlocale("LC_CTYPE", mbyte.lc), mbyte.lc
> ))
>     >> Error: identical(Sys.setlocale("LC_CTYPE", mbyte.lc), mbyte.lc) is
> not
>     >> TRUE
>     >> In addition: Warning message:
>     >> In Sys.setlocale("LC_CTYPE", mbyte.lc) :
>     >> OS reports request to set locale to "C.UTF-8" cannot be honored
>     >> Execution halted
>     >>
>

	[[alternative HTML version deleted]]


From sahil.kang at asilaycomputing.com  Mon May 22 10:25:51 2017
From: sahil.kang at asilaycomputing.com (Sahil Kang)
Date: Mon, 22 May 2017 01:25:51 -0700
Subject: [Rd] [PATCH] Ensure correct order of evaluation in macro
Message-ID: <fc0f3809-6832-92a8-607b-e9f845ebdcd8@asilaycomputing.com>

Hello,

I'd like to contribute this small patch (attached) that I think will 
help prevent some future bugs from occurring in paste.c.
By wrapping the macro's arguments in parentheses, we can ensure that the 
correct order of evaluation will take place during preprocessing.

To illustrate, we can use the == operator which has lower evaluation 
precedence than the < operator:
     * With the current macro, imax2(3==4, 1) expands to 0.
     * After applying this patch, imax2(3==4, 1) expands to 1 as expected.

Since I'm still relatively new to the mailing list, I've kept this patch 
small.
I did notice other macros that have this same issue, so I can start 
sending additional patches if this seems okay.

Thanks,
Sahil
-------------- next part --------------
A non-text attachment was scrubbed...
Name: patch.diff
Type: text/x-patch
Size: 363 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20170522/0fd87568/attachment.bin>

From peter.carbonetto at gmail.com  Mon May 22 19:16:30 2017
From: peter.carbonetto at gmail.com (Peter Carbonetto)
Date: Mon, 22 May 2017 12:16:30 -0500
Subject: [Rd] [R] Somewhat obscure bug in R 3.4.0 building from source
In-Reply-To: <863B1238-607F-45A7-A28B-0FDFF9CF150A@gmail.com>
References: <CAPvCojJqJYiU4LN2XvxH7LMtzqeAghnkztXJ2YnjU4w6UrUFNw@mail.gmail.com>
 <f24508d4-c232-2cc1-8302-b73e39a93869@gmail.com>
 <863B1238-607F-45A7-A28B-0FDFF9CF150A@gmail.com>
Message-ID: <CAPvCojJ=P6CoeokZWJKUM5KZLxhopRKZhQH-u5XTrBoSxda2qw@mail.gmail.com>

Hi Peter, Duncan & Bert,

Thank you kindly for the responses.

Indeed, doc/NEWS.pdf is included in the source distribution, and then
removed upon "make clean".

I thought that it might be useful to report this for your benefit, but on
closer inspection it appears that I'm getting errors that arise due to
incompatibilities in my texlive and texinfo installations. This is the
error I get when trying to build NEWS.pdf using "R CMD Rd2pdf":

R CMD Rd2pdf --output=NEWS.pdf NEWS.Rd
Error in texi2dvi(file = file, pdf = TRUE, clean = clean, quiet = quiet,  :
  Running 'texi2dvi' on 'Rd2.tex' failed.
Messages:
/software/texinfo-6.3-el7-x86_64/bin/texi2dvi: TeX neither supports
-recorder nor outputs \openout lines in its log file
Output:

I'm not sure what to make of this error exactly but perhaps it is
introduced by the latest version of texinfo (which seems to be a recurring
issue based on reading the help for texi2dvi in R):

texi2dvi --version
texi2dvi (GNU Texinfo 6.3) 7353

Peter

On Sun, May 21, 2017 at 5:07 PM, Peter Dalgaard <pdalgd at gmail.com> wrote:

> Inline below...
>
> > On 21 May 2017, at 20:57 , Duncan Murdoch <murdoch.duncan at gmail.com>
> wrote:
> >
> > On 21/05/2017 10:30 AM, Peter Carbonetto wrote:
> >> Hi,
> >>
> >> I uncovered a bug in installing R 3.4.0 from source in Linux, following
> the
> >> standard procedure (configure; make; make install). Is this an
> appropriate
> >> place to report this bug? If not, can you please direct me to the
> >> appropriate place?
> >
> > Generally R-devel is better; I've responded there.
> >
> >>
> >> The error occurs only when I do "make clean" followed by "make" again;
> make
> >> works the first time.
> >>
> >> The error is a failure to build NEWS.pdf:
> >>
> >> Error in texi2dvi(file = file, pdf = TRUE, clean = clean, quiet =
> quiet,  :
> >>  pdflatex is not available
> >> Calls: <Anonymous> -> texi2pdf -> texi2dvi
> >> Execution halted
> >> make[1]: *** [NEWS.pdf] Error 1
> >> make: [docs] Error 2 (ignored)
> >>
> >> and can be reproduced wit the following sequence:
> >>
> >> ./configure
> >> make
> >> make clean
> >> make
> >
> > We usually don't build in the source directory; see the second
> recommendation in the admin manual section 2.1.  So it's possible there's a
> bug triggered when you do that.  Can you try building in a separate
> directory?
>
> Notice that the error is that "pdflatex" is missing from your setup. We
> do, for the benefit of users with defective TeX installations supply a
> pre-built NEWS.pdf (and NEWS.html too) in the source tarballs. However,
> they are technically make targets and make clean will wipe them; in that
> case, you had better have the tools to rebuild them!
>
> -pd
>
> >
> > Duncan Murdoch
> >
> >>
> >> This suggests to me that perhaps "make clean" is not working.
> >>
> >> I'm happy to provide more details so that you are able to reproduce the
> bug.
> >>
> >> Thanks,
> >>
> >> Peter Carbonetto, Ph.D.
> >> Computational Staff Scientist, Statistics & Genetics
> >> Research Computing Center
> >> University of Chicago
> >>
> >>      [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide http://www.R-project.org/
> posting-guide.html
> >> and provide commented, minimal, self-contained, reproducible code.
> >>
> >
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
>
>
>
>
>
>
>
>
>

	[[alternative HTML version deleted]]


From ripley at stats.ox.ac.uk  Mon May 22 19:49:28 2017
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 22 May 2017 18:49:28 +0100
Subject: [Rd] [R] Somewhat obscure bug in R 3.4.0 building from source
In-Reply-To: <CAPvCojJ=P6CoeokZWJKUM5KZLxhopRKZhQH-u5XTrBoSxda2qw@mail.gmail.com>
References: <CAPvCojJqJYiU4LN2XvxH7LMtzqeAghnkztXJ2YnjU4w6UrUFNw@mail.gmail.com>
 <f24508d4-c232-2cc1-8302-b73e39a93869@gmail.com>
 <863B1238-607F-45A7-A28B-0FDFF9CF150A@gmail.com>
 <CAPvCojJ=P6CoeokZWJKUM5KZLxhopRKZhQH-u5XTrBoSxda2qw@mail.gmail.com>
Message-ID: <08e8a71b-242f-bca3-0ccf-6f4803bf4afe@stats.ox.ac.uk>

On 22/05/2017 18:16, Peter Carbonetto wrote:
> Hi Peter, Duncan & Bert,
> 
> Thank you kindly for the responses.
> 
> Indeed, doc/NEWS.pdf is included in the source distribution, and then
> removed upon "make clean".
> 
> I thought that it might be useful to report this for your benefit, but on
> closer inspection it appears that I'm getting errors that arise due to
> incompatibilities in my texlive and texinfo installations. This is the
> error I get when trying to build NEWS.pdf using "R CMD Rd2pdf":
> 
> R CMD Rd2pdf --output=NEWS.pdf NEWS.Rd
> Error in texi2dvi(file = file, pdf = TRUE, clean = clean, quiet = quiet,  :
>    Running 'texi2dvi' on 'Rd2.tex' failed.
> Messages:
> /software/texinfo-6.3-el7-x86_64/bin/texi2dvi: TeX neither supports
> -recorder nor outputs \openout lines in its log file
> Output:
> 
> I'm not sure what to make of this error exactly but perhaps it is
> introduced by the latest version of texinfo (which seems to be a recurring
> issue based on reading the help for texi2dvi in R):
> 
> texi2dvi --version
> texi2dvi (GNU Texinfo 6.3) 7353

Please do not speculate (see the posting guide and FAQ).

That version is thoroughly tested.  texinfo troubles were mainly in the 
transition to Perl ca 5.[012], and its slow adoption by distros: another 
holdback has been the transition to GPL-3-only licensing, AFAIR at 6.0.


> 
> Peter
> 
> On Sun, May 21, 2017 at 5:07 PM, Peter Dalgaard <pdalgd at gmail.com> wrote:
> 
>> Inline below...
>>
>>> On 21 May 2017, at 20:57 , Duncan Murdoch <murdoch.duncan at gmail.com>
>> wrote:
>>>
>>> On 21/05/2017 10:30 AM, Peter Carbonetto wrote:
>>>> Hi,
>>>>
>>>> I uncovered a bug in installing R 3.4.0 from source in Linux, following
>> the
>>>> standard procedure (configure; make; make install). Is this an
>> appropriate
>>>> place to report this bug? If not, can you please direct me to the
>>>> appropriate place?
>>>
>>> Generally R-devel is better; I've responded there.
>>>
>>>>
>>>> The error occurs only when I do "make clean" followed by "make" again;
>> make
>>>> works the first time.
>>>>
>>>> The error is a failure to build NEWS.pdf:
>>>>
>>>> Error in texi2dvi(file = file, pdf = TRUE, clean = clean, quiet =
>> quiet,  :
>>>>   pdflatex is not available
>>>> Calls: <Anonymous> -> texi2pdf -> texi2dvi
>>>> Execution halted
>>>> make[1]: *** [NEWS.pdf] Error 1
>>>> make: [docs] Error 2 (ignored)
>>>>
>>>> and can be reproduced wit the following sequence:
>>>>
>>>> ./configure
>>>> make
>>>> make clean
>>>> make
>>>
>>> We usually don't build in the source directory; see the second
>> recommendation in the admin manual section 2.1.  So it's possible there's a
>> bug triggered when you do that.  Can you try building in a separate
>> directory?
>>
>> Notice that the error is that "pdflatex" is missing from your setup. We
>> do, for the benefit of users with defective TeX installations supply a
>> pre-built NEWS.pdf (and NEWS.html too) in the source tarballs. However,
>> they are technically make targets and make clean will wipe them; in that
>> case, you had better have the tools to rebuild them!
>>
>> -pd
>>
>>>
>>> Duncan Murdoch
>>>
>>>>
>>>> This suggests to me that perhaps "make clean" is not working.
>>>>
>>>> I'm happy to provide more details so that you are able to reproduce the
>> bug.
>>>>
>>>> Thanks,
>>>>
>>>> Peter Carbonetto, Ph.D.
>>>> Computational Staff Scientist, Statistics & Genetics
>>>> Research Computing Center
>>>> University of Chicago
>>>>
>>>>       [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide http://www.R-project.org/
>> posting-guide.html
>>>> and provide commented, minimal, self-contained, reproducible code.
>>>>
>>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>> --
>> Peter Dalgaard, Professor,
>> Center for Statistics, Copenhagen Business School
>> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
>> Phone: (+45)38153501
>> Office: A 4.23
>> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Emeritus Professor of Applied Statistics, University of Oxford


From jorismeys at gmail.com  Tue May 23 14:39:48 2017
From: jorismeys at gmail.com (Joris Meys)
Date: Tue, 23 May 2017 14:39:48 +0200
Subject: [Rd] help pages base R not rendered correctly?
Message-ID: <CAO1zAVb=qu8_tyS0wd3Eq=xzrQePUy3Uc1u2z+16sLJgzkuS8Q@mail.gmail.com>

Hi all,

Don't know if this is a known issue, but I couldn't find anything so I
report anyway. When checking eg ?qr in both RStudio and the naked R IDE,
the help page is rendered incorrectly. More specifically, any use of
\bold{...} is printed as is, rather than interpreted as bold. Same happens
on ?svd.

According to the manual Writing R Extensions, this should still be
recognized. When I try to use it in the help pages of my own packages,
\bold{} is interpreted correctly.

No idea what is going wrong and it's not that important, but I found it
curious enough to report.

Cheers
Joris

-- 
Joris Meys
Statistical consultant

Ghent University
Faculty of Bioscience Engineering
Department of Mathematical Modelling, Statistics and Bio-Informatics

tel :  +32 (0)9 264 61 79
Joris.Meys at Ugent.be
-------------------------------
Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php

	[[alternative HTML version deleted]]


From murdoch.duncan at gmail.com  Tue May 23 15:10:02 2017
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Tue, 23 May 2017 09:10:02 -0400
Subject: [Rd] help pages base R not rendered correctly?
In-Reply-To: <CAO1zAVb=qu8_tyS0wd3Eq=xzrQePUy3Uc1u2z+16sLJgzkuS8Q@mail.gmail.com>
References: <CAO1zAVb=qu8_tyS0wd3Eq=xzrQePUy3Uc1u2z+16sLJgzkuS8Q@mail.gmail.com>
Message-ID: <28a56160-9077-48f0-955e-637a8af1463b@gmail.com>

On 23/05/2017 8:39 AM, Joris Meys wrote:
> Hi all,
>
> Don't know if this is a known issue, but I couldn't find anything so I
> report anyway. When checking eg ?qr in both RStudio and the naked R IDE,
> the help page is rendered incorrectly. More specifically, any use of
> \bold{...} is printed as is, rather than interpreted as bold. Same happens
> on ?svd.

In ?qr (and probably in ?svd), \bold is inside \eqn{}, so it is being 
interpreted as LaTeX markup rather than Rd markup, and since no 
alternative was given, is displayed as-is.

If you actually wanted it to display properly in HTML and ascii, the 
current \eqn{\bold{Ax} = \bold{b}} would have to be written as something 
like

\ifelse{latex}{\eqn{\bold{Ax} = \bold{b}}}{\bold{Ax} = \bold{b}}

which is pretty tedious to write, and it's not the only example on that 
page.  So I doubt if anyone will fix it.

>
> According to the manual Writing R Extensions, this should still be
> recognized. When I try to use it in the help pages of my own packages,
> \bold{} is interpreted correctly.

You missed the part about \eqn.

Duncan Murdoch

>
> No idea what is going wrong and it's not that important, but I found it
> curious enough to report.
>
> Cheers
> Joris
>


From jorismeys at gmail.com  Tue May 23 15:56:12 2017
From: jorismeys at gmail.com (Joris Meys)
Date: Tue, 23 May 2017 15:56:12 +0200
Subject: [Rd] help pages base R not rendered correctly?
In-Reply-To: <28a56160-9077-48f0-955e-637a8af1463b@gmail.com>
References: <CAO1zAVb=qu8_tyS0wd3Eq=xzrQePUy3Uc1u2z+16sLJgzkuS8Q@mail.gmail.com>
 <28a56160-9077-48f0-955e-637a8af1463b@gmail.com>
Message-ID: <CAO1zAVaGrkwS4oQrtMwcsLWsbPVcZj9rw8ZQkPj-YDRuBafvcw@mail.gmail.com>

Hi Duncan,

that explains, thank you. If nobody finds the time to fix that, I might
give it a shot myself this summer. Barbeque is overrated.

Cheers
Joris

On Tue, May 23, 2017 at 3:10 PM, Duncan Murdoch <murdoch.duncan at gmail.com>
wrote:

> On 23/05/2017 8:39 AM, Joris Meys wrote:
>
>> Hi all,
>>
>> Don't know if this is a known issue, but I couldn't find anything so I
>> report anyway. When checking eg ?qr in both RStudio and the naked R IDE,
>> the help page is rendered incorrectly. More specifically, any use of
>> \bold{...} is printed as is, rather than interpreted as bold. Same happens
>> on ?svd.
>>
>
> In ?qr (and probably in ?svd), \bold is inside \eqn{}, so it is being
> interpreted as LaTeX markup rather than Rd markup, and since no alternative
> was given, is displayed as-is.
>
> If you actually wanted it to display properly in HTML and ascii, the
> current \eqn{\bold{Ax} = \bold{b}} would have to be written as something
> like
>
> \ifelse{latex}{\eqn{\bold{Ax} = \bold{b}}}{\bold{Ax} = \bold{b}}
>
> which is pretty tedious to write, and it's not the only example on that
> page.  So I doubt if anyone will fix it.
>
>
>> According to the manual Writing R Extensions, this should still be
>> recognized. When I try to use it in the help pages of my own packages,
>> \bold{} is interpreted correctly.
>>
>
> You missed the part about \eqn.
>
> Duncan Murdoch
>
>
>
>> No idea what is going wrong and it's not that important, but I found it
>> curious enough to report.
>>
>> Cheers
>> Joris
>>
>>
>


-- 
Joris Meys
Statistical consultant

Ghent University
Faculty of Bioscience Engineering
Department of Mathematical Modelling, Statistics and Bio-Informatics

tel :  +32 (0)9 264 61 79
Joris.Meys at Ugent.be
-------------------------------
Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php

	[[alternative HTML version deleted]]


From pdalgd at gmail.com  Tue May 23 16:25:04 2017
From: pdalgd at gmail.com (peter dalgaard)
Date: Tue, 23 May 2017 16:25:04 +0200
Subject: [Rd] help pages base R not rendered correctly?
In-Reply-To: <CAO1zAVaGrkwS4oQrtMwcsLWsbPVcZj9rw8ZQkPj-YDRuBafvcw@mail.gmail.com>
References: <CAO1zAVb=qu8_tyS0wd3Eq=xzrQePUy3Uc1u2z+16sLJgzkuS8Q@mail.gmail.com>
 <28a56160-9077-48f0-955e-637a8af1463b@gmail.com>
 <CAO1zAVaGrkwS4oQrtMwcsLWsbPVcZj9rw8ZQkPj-YDRuBafvcw@mail.gmail.com>
Message-ID: <7E458D06-E233-49C0-BFC2-AB9C9DB33745@gmail.com>


> On 23 May 2017, at 15:56 , Joris Meys <jorismeys at gmail.com> wrote:
> 
> Hi Duncan,
> 
> that explains, thank you. If nobody finds the time to fix that, I might
> give it a shot myself this summer. Barbeque is overrated.

I beg to differ! Chances of rain are underestimated, though (in .be as in .dk, I suspect). 

;-)

-pd

> 
> Cheers
> Joris
> 

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From ecortens at mtroyal.ca  Tue May 23 16:53:34 2017
From: ecortens at mtroyal.ca (Evan Cortens)
Date: Tue, 23 May 2017 08:53:34 -0600
Subject: [Rd] Inconsistency in handling of numeric input with %d by
	sprintf
In-Reply-To: <CAPRVBcx3OjFK0G=hk1g8qChYiQ7cuZPpep-kju-_BD4MErhDuA@mail.gmail.com>
References: <CAPRVBcx3OjFK0G=hk1g8qChYiQ7cuZPpep-kju-_BD4MErhDuA@mail.gmail.com>
Message-ID: <CABKQe-bStQz+AOQ7SGMcdiNY9BNOuaz21j7b3Mtp6-tqtpdbPg@mail.gmail.com>

Hi Michael,

I posted something on this topic to R-devel several weeks ago, but never
got a response. My ultimate conclusion is that sprintf() isn't super
consistent in how it handles coercion: sometimes it'll coerce real to
integer without complaint, other times it won't. (My particular email had
to do with the vectors longer than 1 and their positioning vis-a-vis the
format string.) The safest thing is just to pass the right type. In this
case, sprintf('%d', as.integer(NA_real_)) works.

Best,

Evan

On Fri, May 19, 2017 at 9:23 AM, Michael Chirico <michaelchirico4 at gmail.com>
wrote:

> Consider
>
> #as.numeric for emphasis
> sprintf('%d', as.numeric(1))
> # [1] "1"
>
> vs.
>
> sprintf('%d', NA_real_)
>
> >  Error in sprintf("%d", NA_real_) :
>
>    invalid format '%d'; use format %f, %e, %g or %a for numeric object
> >
>
> I understand the error is correct, but if it works for other numeric input,
> why doesn't R just coerce NA_real_ to NA_integer_?
>
> Michael Chirico
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>



-- 
Evan Cortens, PhD
Institutional Analyst - Office of Institutional Analysis
Mount Royal University
403-440-6529

	[[alternative HTML version deleted]]


From sahil.kang at asilaycomputing.com  Tue May 23 17:47:08 2017
From: sahil.kang at asilaycomputing.com (Sahil Kang)
Date: Tue, 23 May 2017 08:47:08 -0700
Subject: [Rd] [PATCH] Ensure correct order of evaluation in macro
In-Reply-To: <fc0f3809-6832-92a8-607b-e9f845ebdcd8@asilaycomputing.com>
References: <fc0f3809-6832-92a8-607b-e9f845ebdcd8@asilaycomputing.com>
Message-ID: <2DFDED2E-822C-40F5-902A-B1F32F5571D4@asilaycomputing.com>

Hi Duncan,

Would you merge this patch?
I'm planning on sending larger patches in the next few days that fix other macros I've seen, but I figured? it'd be best to start with a smaller patch.

Thanks,
Sahil


From jorismeys at gmail.com  Tue May 23 17:53:24 2017
From: jorismeys at gmail.com (Joris Meys)
Date: Tue, 23 May 2017 17:53:24 +0200
Subject: [Rd] Inconsistency in handling of numeric input with %d by
	sprintf
In-Reply-To: <CABKQe-bStQz+AOQ7SGMcdiNY9BNOuaz21j7b3Mtp6-tqtpdbPg@mail.gmail.com>
References: <CAPRVBcx3OjFK0G=hk1g8qChYiQ7cuZPpep-kju-_BD4MErhDuA@mail.gmail.com>
 <CABKQe-bStQz+AOQ7SGMcdiNY9BNOuaz21j7b3Mtp6-tqtpdbPg@mail.gmail.com>
Message-ID: <CAO1zAVZNDbwMuV1v=fK6DMQqYt4o_4d6Vrme1jM_+BLnkNzsGQ@mail.gmail.com>

I initially thought this is "documented behaviour". ?sprintf says:

Numeric variables with __exactly integer__ values will be coerced to
integer. (emphasis mine).

Turns out this only works when the first value is numeric and not NA, as
shown by the following example:

> sprintf("%d", as.numeric(c(NA,1)))
Error in sprintf("%d", as.numeric(c(NA, 1))) :
  invalid format '%d'; use format %f, %e, %g or %a for numeric objects
> sprintf("%d", as.numeric(c(1,NA)))
[1] "1"  "NA"

So the safest thing is indeed passing the right type, but the behaviour is
indeed confusing. I checked this on both Windows and Debian, and on both
systems I get the exact same response.

Cheers
Joris

On Tue, May 23, 2017 at 4:53 PM, Evan Cortens <ecortens at mtroyal.ca> wrote:

> Hi Michael,
>
> I posted something on this topic to R-devel several weeks ago, but never
> got a response. My ultimate conclusion is that sprintf() isn't super
> consistent in how it handles coercion: sometimes it'll coerce real to
> integer without complaint, other times it won't. (My particular email had
> to do with the vectors longer than 1 and their positioning vis-a-vis the
> format string.) The safest thing is just to pass the right type. In this
> case, sprintf('%d', as.integer(NA_real_)) works.
>
> Best,
>
> Evan
>
> On Fri, May 19, 2017 at 9:23 AM, Michael Chirico <
> michaelchirico4 at gmail.com>
> wrote:
>
> > Consider
> >
> > #as.numeric for emphasis
> > sprintf('%d', as.numeric(1))
> > # [1] "1"
> >
> > vs.
> >
> > sprintf('%d', NA_real_)
> >
> > >  Error in sprintf("%d", NA_real_) :
> >
> >    invalid format '%d'; use format %f, %e, %g or %a for numeric object
> > >
> >
> > I understand the error is correct, but if it works for other numeric
> input,
> > why doesn't R just coerce NA_real_ to NA_integer_?
> >
> > Michael Chirico
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
> >
>
>
>
> --
> Evan Cortens, PhD
> Institutional Analyst - Office of Institutional Analysis
> Mount Royal University
> 403-440-6529
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>



-- 
Joris Meys
Statistical consultant

Ghent University
Faculty of Bioscience Engineering
Department of Mathematical Modelling, Statistics and Bio-Informatics

tel :  +32 (0)9 264 61 79
Joris.Meys at Ugent.be
-------------------------------
Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php

	[[alternative HTML version deleted]]


From michaelchirico4 at gmail.com  Tue May 23 18:00:03 2017
From: michaelchirico4 at gmail.com (Michael Chirico)
Date: Tue, 23 May 2017 12:00:03 -0400
Subject: [Rd] Inconsistency in handling of numeric input with %d by
	sprintf
In-Reply-To: <CAO1zAVZNDbwMuV1v=fK6DMQqYt4o_4d6Vrme1jM_+BLnkNzsGQ@mail.gmail.com>
References: <CAPRVBcx3OjFK0G=hk1g8qChYiQ7cuZPpep-kju-_BD4MErhDuA@mail.gmail.com>
 <CABKQe-bStQz+AOQ7SGMcdiNY9BNOuaz21j7b3Mtp6-tqtpdbPg@mail.gmail.com>
 <CAO1zAVZNDbwMuV1v=fK6DMQqYt4o_4d6Vrme1jM_+BLnkNzsGQ@mail.gmail.com>
Message-ID: <CAPRVBcxz3+n_r8N0TwWq0DC2ML274ahF68xemimNYbZz9RHbsQ@mail.gmail.com>

Astute observation. And of course we should be passing integer when we use
%d. It's an edge case in how we printed ITime objects in data.table:


On Tue, May 23, 2017 at 11:53 AM, Joris Meys <jorismeys at gmail.com> wrote:

> I initially thought this is "documented behaviour". ?sprintf says:
>
> Numeric variables with __exactly integer__ values will be coerced to
> integer. (emphasis mine).
>
> Turns out this only works when the first value is numeric and not NA, as
> shown by the following example:
>
> > sprintf("%d", as.numeric(c(NA,1)))
> Error in sprintf("%d", as.numeric(c(NA, 1))) :
>   invalid format '%d'; use format %f, %e, %g or %a for numeric objects
> > sprintf("%d", as.numeric(c(1,NA)))
> [1] "1"  "NA"
>
> So the safest thing is indeed passing the right type, but the behaviour is
> indeed confusing. I checked this on both Windows and Debian, and on both
> systems I get the exact same response.
>
> Cheers
> Joris
>
> On Tue, May 23, 2017 at 4:53 PM, Evan Cortens <ecortens at mtroyal.ca> wrote:
>
>> Hi Michael,
>>
>> I posted something on this topic to R-devel several weeks ago, but never
>> got a response. My ultimate conclusion is that sprintf() isn't super
>> consistent in how it handles coercion: sometimes it'll coerce real to
>> integer without complaint, other times it won't. (My particular email had
>> to do with the vectors longer than 1 and their positioning vis-a-vis the
>> format string.) The safest thing is just to pass the right type. In this
>> case, sprintf('%d', as.integer(NA_real_)) works.
>>
>> Best,
>>
>> Evan
>>
>> On Fri, May 19, 2017 at 9:23 AM, Michael Chirico <
>> michaelchirico4 at gmail.com>
>> wrote:
>>
>> > Consider
>> >
>> > #as.numeric for emphasis
>> > sprintf('%d', as.numeric(1))
>> > # [1] "1"
>> >
>> > vs.
>> >
>> > sprintf('%d', NA_real_)
>> >
>> > >  Error in sprintf("%d", NA_real_) :
>> >
>> >    invalid format '%d'; use format %f, %e, %g or %a for numeric object
>> > >
>> >
>> > I understand the error is correct, but if it works for other numeric
>> input,
>> > why doesn't R just coerce NA_real_ to NA_integer_?
>> >
>> > Michael Chirico
>> >
>> >         [[alternative HTML version deleted]]
>> >
>> > ______________________________________________
>> > R-devel at r-project.org mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-devel
>> >
>>
>>
>>
>> --
>> Evan Cortens, PhD
>> Institutional Analyst - Office of Institutional Analysis
>> Mount Royal University
>> 403-440-6529
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>
>
>
> --
> Joris Meys
> Statistical consultant
>
> Ghent University
> Faculty of Bioscience Engineering
> Department of Mathematical Modelling, Statistics and Bio-Informatics
>
> tel :  +32 (0)9 264 61 79 <+32%209%20264%2061%2079>
> Joris.Meys at Ugent.be
> -------------------------------
> Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php
>

	[[alternative HTML version deleted]]


From michaelchirico4 at gmail.com  Tue May 23 18:02:15 2017
From: michaelchirico4 at gmail.com (Michael Chirico)
Date: Tue, 23 May 2017 12:02:15 -0400
Subject: [Rd] Inconsistency in handling of numeric input with %d by
	sprintf
In-Reply-To: <CAPRVBcxz3+n_r8N0TwWq0DC2ML274ahF68xemimNYbZz9RHbsQ@mail.gmail.com>
References: <CAPRVBcx3OjFK0G=hk1g8qChYiQ7cuZPpep-kju-_BD4MErhDuA@mail.gmail.com>
 <CABKQe-bStQz+AOQ7SGMcdiNY9BNOuaz21j7b3Mtp6-tqtpdbPg@mail.gmail.com>
 <CAO1zAVZNDbwMuV1v=fK6DMQqYt4o_4d6Vrme1jM_+BLnkNzsGQ@mail.gmail.com>
 <CAPRVBcxz3+n_r8N0TwWq0DC2ML274ahF68xemimNYbZz9RHbsQ@mail.gmail.com>
Message-ID: <CAPRVBcw2YZ0cEpXCSjOKEfGHxN2MhqmM6TwtDJhQJjOBasbZ2g@mail.gmail.com>

https://github.com/Rdatatable/data.table/issues/2171

The fix was easy, it's just surprising to see the behavior change almost on
a whim. Just wanted to point it out in case this is unknown behavior, but
Evan seems to have found this as well.

On Tue, May 23, 2017 at 12:00 PM, Michael Chirico <michaelchirico4 at gmail.com
> wrote:

> Astute observation. And of course we should be passing integer when we use
> %d. It's an edge case in how we printed ITime objects in data.table:
>
>
> On Tue, May 23, 2017 at 11:53 AM, Joris Meys <jorismeys at gmail.com> wrote:
>
>> I initially thought this is "documented behaviour". ?sprintf says:
>>
>> Numeric variables with __exactly integer__ values will be coerced to
>> integer. (emphasis mine).
>>
>> Turns out this only works when the first value is numeric and not NA, as
>> shown by the following example:
>>
>> > sprintf("%d", as.numeric(c(NA,1)))
>> Error in sprintf("%d", as.numeric(c(NA, 1))) :
>>   invalid format '%d'; use format %f, %e, %g or %a for numeric objects
>> > sprintf("%d", as.numeric(c(1,NA)))
>> [1] "1"  "NA"
>>
>> So the safest thing is indeed passing the right type, but the behaviour
>> is indeed confusing. I checked this on both Windows and Debian, and on both
>> systems I get the exact same response.
>>
>> Cheers
>> Joris
>>
>> On Tue, May 23, 2017 at 4:53 PM, Evan Cortens <ecortens at mtroyal.ca>
>> wrote:
>>
>>> Hi Michael,
>>>
>>> I posted something on this topic to R-devel several weeks ago, but never
>>> got a response. My ultimate conclusion is that sprintf() isn't super
>>> consistent in how it handles coercion: sometimes it'll coerce real to
>>> integer without complaint, other times it won't. (My particular email had
>>> to do with the vectors longer than 1 and their positioning vis-a-vis the
>>> format string.) The safest thing is just to pass the right type. In this
>>> case, sprintf('%d', as.integer(NA_real_)) works.
>>>
>>> Best,
>>>
>>> Evan
>>>
>>> On Fri, May 19, 2017 at 9:23 AM, Michael Chirico <
>>> michaelchirico4 at gmail.com>
>>> wrote:
>>>
>>> > Consider
>>> >
>>> > #as.numeric for emphasis
>>> > sprintf('%d', as.numeric(1))
>>> > # [1] "1"
>>> >
>>> > vs.
>>> >
>>> > sprintf('%d', NA_real_)
>>> >
>>> > >  Error in sprintf("%d", NA_real_) :
>>> >
>>> >    invalid format '%d'; use format %f, %e, %g or %a for numeric object
>>> > >
>>> >
>>> > I understand the error is correct, but if it works for other numeric
>>> input,
>>> > why doesn't R just coerce NA_real_ to NA_integer_?
>>> >
>>> > Michael Chirico
>>> >
>>> >         [[alternative HTML version deleted]]
>>> >
>>> > ______________________________________________
>>> > R-devel at r-project.org mailing list
>>> > https://stat.ethz.ch/mailman/listinfo/r-devel
>>> >
>>>
>>>
>>>
>>> --
>>> Evan Cortens, PhD
>>> Institutional Analyst - Office of Institutional Analysis
>>> Mount Royal University
>>> 403-440-6529
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>
>>
>>
>>
>> --
>> Joris Meys
>> Statistical consultant
>>
>> Ghent University
>> Faculty of Bioscience Engineering
>> Department of Mathematical Modelling, Statistics and Bio-Informatics
>>
>> tel :  +32 (0)9 264 61 79 <+32%209%20264%2061%2079>
>> Joris.Meys at Ugent.be
>> -------------------------------
>> Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php
>>
>
>

	[[alternative HTML version deleted]]


From ecortens at mtroyal.ca  Tue May 23 18:32:20 2017
From: ecortens at mtroyal.ca (Evan Cortens)
Date: Tue, 23 May 2017 10:32:20 -0600
Subject: [Rd] Inconsistency in handling of numeric input with %d by
	sprintf
In-Reply-To: <CAPRVBcw2YZ0cEpXCSjOKEfGHxN2MhqmM6TwtDJhQJjOBasbZ2g@mail.gmail.com>
References: <CAPRVBcx3OjFK0G=hk1g8qChYiQ7cuZPpep-kju-_BD4MErhDuA@mail.gmail.com>
 <CABKQe-bStQz+AOQ7SGMcdiNY9BNOuaz21j7b3Mtp6-tqtpdbPg@mail.gmail.com>
 <CAO1zAVZNDbwMuV1v=fK6DMQqYt4o_4d6Vrme1jM_+BLnkNzsGQ@mail.gmail.com>
 <CAPRVBcxz3+n_r8N0TwWq0DC2ML274ahF68xemimNYbZz9RHbsQ@mail.gmail.com>
 <CAPRVBcw2YZ0cEpXCSjOKEfGHxN2MhqmM6TwtDJhQJjOBasbZ2g@mail.gmail.com>
Message-ID: <CABKQe-aTQHz-0+iS+kNngaZLzf61SXR7tDfXx9Yze+t9RaqTFw@mail.gmail.com>

Yes, what Joris posts about is exactly what I noted in my March 9th post to
R-devel. The behaviour is sort of documented, but not in the clearest
manner (in my opinion). Like I say, my ultimate conclusion was that the
silent coercion of numerics to integers by sprintf() was a handy
convenience, but not one that should be relied about to always work
predictably.

On Tue, May 23, 2017 at 10:02 AM, Michael Chirico <michaelchirico4 at gmail.com
> wrote:

> https://github.com/Rdatatable/data.table/issues/2171
>
> The fix was easy, it's just surprising to see the behavior change almost
> on a whim. Just wanted to point it out in case this is unknown behavior,
> but Evan seems to have found this as well.
>
> On Tue, May 23, 2017 at 12:00 PM, Michael Chirico <
> michaelchirico4 at gmail.com> wrote:
>
>> Astute observation. And of course we should be passing integer when we
>> use %d. It's an edge case in how we printed ITime objects in data.table:
>>
>>
>> On Tue, May 23, 2017 at 11:53 AM, Joris Meys <jorismeys at gmail.com> wrote:
>>
>>> I initially thought this is "documented behaviour". ?sprintf says:
>>>
>>> Numeric variables with __exactly integer__ values will be coerced to
>>> integer. (emphasis mine).
>>>
>>> Turns out this only works when the first value is numeric and not NA, as
>>> shown by the following example:
>>>
>>> > sprintf("%d", as.numeric(c(NA,1)))
>>> Error in sprintf("%d", as.numeric(c(NA, 1))) :
>>>   invalid format '%d'; use format %f, %e, %g or %a for numeric objects
>>> > sprintf("%d", as.numeric(c(1,NA)))
>>> [1] "1"  "NA"
>>>
>>> So the safest thing is indeed passing the right type, but the behaviour
>>> is indeed confusing. I checked this on both Windows and Debian, and on both
>>> systems I get the exact same response.
>>>
>>> Cheers
>>> Joris
>>>
>>> On Tue, May 23, 2017 at 4:53 PM, Evan Cortens <ecortens at mtroyal.ca>
>>> wrote:
>>>
>>>> Hi Michael,
>>>>
>>>> I posted something on this topic to R-devel several weeks ago, but never
>>>> got a response. My ultimate conclusion is that sprintf() isn't super
>>>> consistent in how it handles coercion: sometimes it'll coerce real to
>>>> integer without complaint, other times it won't. (My particular email
>>>> had
>>>> to do with the vectors longer than 1 and their positioning vis-a-vis the
>>>> format string.) The safest thing is just to pass the right type. In this
>>>> case, sprintf('%d', as.integer(NA_real_)) works.
>>>>
>>>> Best,
>>>>
>>>> Evan
>>>>
>>>> On Fri, May 19, 2017 at 9:23 AM, Michael Chirico <
>>>> michaelchirico4 at gmail.com>
>>>> wrote:
>>>>
>>>> > Consider
>>>> >
>>>> > #as.numeric for emphasis
>>>> > sprintf('%d', as.numeric(1))
>>>> > # [1] "1"
>>>> >
>>>> > vs.
>>>> >
>>>> > sprintf('%d', NA_real_)
>>>> >
>>>> > >  Error in sprintf("%d", NA_real_) :
>>>> >
>>>> >    invalid format '%d'; use format %f, %e, %g or %a for numeric object
>>>> > >
>>>> >
>>>> > I understand the error is correct, but if it works for other numeric
>>>> input,
>>>> > why doesn't R just coerce NA_real_ to NA_integer_?
>>>> >
>>>> > Michael Chirico
>>>> >
>>>> >         [[alternative HTML version deleted]]
>>>> >
>>>> > ______________________________________________
>>>> > R-devel at r-project.org mailing list
>>>> > https://stat.ethz.ch/mailman/listinfo/r-devel
>>>> >
>>>>
>>>>
>>>>
>>>> --
>>>> Evan Cortens, PhD
>>>> Institutional Analyst - Office of Institutional Analysis
>>>> Mount Royal University
>>>> 403-440-6529
>>>>
>>>>         [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-devel at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>
>>>
>>>
>>>
>>> --
>>> Joris Meys
>>> Statistical consultant
>>>
>>> Ghent University
>>> Faculty of Bioscience Engineering
>>> Department of Mathematical Modelling, Statistics and Bio-Informatics
>>>
>>> tel :  +32 (0)9 264 61 79 <+32%209%20264%2061%2079>
>>> Joris.Meys at Ugent.be
>>> -------------------------------
>>> Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php
>>>
>>
>>
>

	[[alternative HTML version deleted]]


From murdoch.duncan at gmail.com  Tue May 23 18:35:05 2017
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Tue, 23 May 2017 12:35:05 -0400
Subject: [Rd] [PATCH] Ensure correct order of evaluation in macro
In-Reply-To: <2DFDED2E-822C-40F5-902A-B1F32F5571D4@asilaycomputing.com>
References: <fc0f3809-6832-92a8-607b-e9f845ebdcd8@asilaycomputing.com>
 <2DFDED2E-822C-40F5-902A-B1F32F5571D4@asilaycomputing.com>
Message-ID: <708cb0eb-e0d9-d923-bd2c-c7c848ead4fc@gmail.com>

On 23/05/2017 11:47 AM, Sahil Kang wrote:
> Hi Duncan,
>
> Would you merge this patch?
> I'm planning on sending larger patches in the next few days that fix other macros I've seen, but I figured? it'd be best to start with a smaller patch.

No, I generally try to leave the macro stuff to others.

Duncan Murdoch


From rmcgehee at walleyetrading.net  Tue May 23 21:04:07 2017
From: rmcgehee at walleyetrading.net (Robert McGehee)
Date: Tue, 23 May 2017 19:04:07 +0000
Subject: [Rd] Allow dot in RHS of update.formula's old formula
Message-ID: <30D28A63376088428E8318DD67FD407F77AA5E@ny-mailstore1.walleyetrading.net>

Feature request:

I want to use update.formula to subtract an intercept (or other) term from a formula with a dot on the RHS. However, as this causes an error, I propose a patch below.

Thus, I want:
> update.formula(y ~ ., ~ . -1)
[1] y ~ . - 1

Instead I get this error:
Error in terms.formula(tmp, simplify = TRUE) : 
  '.' in formula and no 'data' argument

While the error message isn't especially helpful (as I *cannot* currently pass in a data argument), the problem is that terms.formula inside update.formula does not allow a dot in the RHS of 'old' unless either a 'data' argument is passed in or 'allowDotAsName=TRUE'. 

Thus, I'd like to suggest this change to update.formula to allow a dot in the RHS of old without (I believe) impacting any other behavior.

-    out <- formula(terms.formula(tmp, simplify = TRUE))
+    out <- formula(terms.formula(tmp, simplify = TRUE, allowDotAsName=TRUE))

If this is undesirable for some reason, then alternatively the dots argument of update.formula could be passed to terms.formula so the user could pass in either 'data' or 'allowDotAsName=TRUE' themselves (though as I cannot think of any reason the user would prefer 'allowDotAsName=FALSE', this is not my preference).

-    out <- formula(terms.formula(tmp, simplify = TRUE))
+    out <- formula(terms.formula(tmp, simplify = TRUE, ...))

>From my reading of the Details section of ?update.formula, it seems as if this suggestion is consistent with the current documentation, as no mention is made of dots in the RHS of 'old', and no mention is made of why this behavior should be otherwise prohibited. If neither change is desirable for some reason, then the update.formula documentation should at least point out this exception (e.g. "... and substitutes the _rhs_ of the 'old' formula for any occurrence of '.' on the right of 'new' *except if there is a dot in the _rhs_ of 'old'*."

Thanks, Robert


From nsosnov at microsoft.com  Tue May 23 23:02:33 2017
From: nsosnov at microsoft.com (Nathan Sosnovske)
Date: Tue, 23 May 2017 21:02:33 +0000
Subject: [Rd] Getting an R bugzilla account
Message-ID: <SN1PR21MB0061112D59B76A17EC7061BBB7F90@SN1PR21MB0061.namprd21.prod.outlook.com>

Hi All,

I have a fix to this bug ( https://bugs.r-project.org/bugzilla3/show_bug.cgi?id=16454) and would like to submit a patch to the bug report on Bugzilla. I'd also like to start going through some of the other Windows-specific issues and start fixing those. The bug submission instructions indicate that I should ask here for a Bugzilla account. Is that still the correct procedure?

Thanks!

Nathan


	[[alternative HTML version deleted]]


From plummerm at iarc.fr  Wed May 24 08:15:53 2017
From: plummerm at iarc.fr (Martyn Plummer)
Date: Wed, 24 May 2017 06:15:53 +0000
Subject: [Rd] Getting an R bugzilla account
In-Reply-To: <SN1PR21MB0061112D59B76A17EC7061BBB7F90@SN1PR21MB0061.namprd21.prod.outlook.com>
References: <SN1PR21MB0061112D59B76A17EC7061BBB7F90@SN1PR21MB0061.namprd21.prod.outlook.com>
Message-ID: <1495606449.23606.20.camel@iarc.fr>

Thanks for your help Nathan. I have added a bugzilla account for you.

Martyn

On Tue, 2017-05-23 at 21:02 +0000, Nathan Sosnovske via R-devel wrote:
> Hi All,
> 
> I have a fix to this bug ( https://bugs.r-project.org/bugzilla3/show_
> bug.cgi?id=16454) and would like to submit a patch to the bug report
> on Bugzilla. I'd also like to start going through some of the other
> Windows-specific issues and start fixing those. The bug submission
> instructions indicate that I should ask here for a Bugzilla account.
> Is that still the correct procedure?
> 
> Thanks!
> 
> Nathan
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

From hkawakat at gmail.com  Wed May 24 11:47:28 2017
From: hkawakat at gmail.com (Hiroyuki Kawakatsu)
Date: Wed, 24 May 2017 10:47:28 +0100
Subject: [Rd] reg-tests-1d.R fails in r72721
Message-ID: <20170524094728.ynpyldncoeipp3bq@godel.dcu.ie>

Hi,

I am failing make check in r72721 at the end of reg-tests-1d.R. The
relevant block of code is

## path.expand shouldn't translate to local encoding PR#17120
filename <- "\U9b3c.R"
print(Encoding(filename))
x1 <- path.expand(paste0("~/", filename))
print(Encoding(x1))
x2 <- paste0(path.expand("~/"), filename)
print(Encoding(x2))
stopifnot(identical( path.expand(paste0("~/", filename)), paste0(path.expand("~/"), filename)))
## Chinese character was changed to hex code 

Encoding(x1) is "unknown" while Encoding(x2) is "UTF-8". If I run
this code with R --vanilla, both are UTF-8 and the assertion
passes. What is make check doing differently? Or is there something
wrong with my setting/environment? Thanks,

h.
-- 
+---
| Hiroyuki Kawakatsu
| Business School, Dublin City University
| Dublin 9, Ireland. Tel +353 (0)1 700 7496


From mkim0710 at gmail.com  Wed May 24 04:24:33 2017
From: mkim0710 at gmail.com (M Kim)
Date: Tue, 23 May 2017 22:24:33 -0400
Subject: [Rd] precision of do_arith() in arithmetic.c
Message-ID: <CAPtC_u3aBjF-A98_iRpw9xrf14uq14YpA9tj9iBzHG887B8baQ@mail.gmail.com>

To the R development team:

First of all, thank you so much for maintaining wonderful R software.

Perhaps, Dr. Ahn has just reported an error on the wilcox.test() function,
and suggesting that an error may arise from abs() and rank().


I just had a quick check that the problem may come from the precision of
the results of arithmetic functions.


87.7-89.1+1.4
# > 87.7-89.1+1.4
# [1] 8.437695e-15

I checked that do_arith() in arithmetic.c is using double type (8 byte) for
PLUSOP, MINUSOP, TIMESOP, DIVOP etc.

https://github.com/wch/r-source/blob/f68b30e3b5479d84adbff516d48d4722a574dc82/src/main/arithmetic.c


I have two thoughts:

(1) in the rank() function, we may round at less than 14 decimal places by
default?

(2) using long double type (10 byte) instead of double type (8 byte) by
default in the arithmetic function could be helpful for preventing
embarassment?

Perhaps, R may provide the arithmetic function with various variable types,
if some application needs.


Thanks.

Min-hyung

	[[alternative HTML version deleted]]


From murdoch.duncan at gmail.com  Wed May 24 13:06:28 2017
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Wed, 24 May 2017 07:06:28 -0400
Subject: [Rd] reg-tests-1d.R fails in r72721
In-Reply-To: <20170524094728.ynpyldncoeipp3bq@godel.dcu.ie>
References: <20170524094728.ynpyldncoeipp3bq@godel.dcu.ie>
Message-ID: <cd5731cf-69ee-10da-1f76-9b671e77584c@gmail.com>

On 24/05/2017 5:47 AM, Hiroyuki Kawakatsu wrote:
> Hi,
>
> I am failing make check in r72721 at the end of reg-tests-1d.R. The
> relevant block of code is
>
> ## path.expand shouldn't translate to local encoding PR#17120
> filename <- "\U9b3c.R"
> print(Encoding(filename))
> x1 <- path.expand(paste0("~/", filename))
> print(Encoding(x1))
> x2 <- paste0(path.expand("~/"), filename)
> print(Encoding(x2))
> stopifnot(identical( path.expand(paste0("~/", filename)), paste0(path.expand("~/"), filename)))
> ## Chinese character was changed to hex code
>
> Encoding(x1) is "unknown" while Encoding(x2) is "UTF-8". If I run
> this code with R --vanilla, both are UTF-8 and the assertion
> passes. What is make check doing differently? Or is there something
> wrong with my setting/environment? Thanks,
>

I think the test is wrong because in the first case you are working in a 
locale where that character is representable.  In my locale it is not, 
so x1 is converted to UTF-8, and everything compares equal.

An explicit conversion of x1 to UTF-8 should fix this, i.e. replace

x1 <- path.expand(paste0("~/", filename))

with

x1 <- enc2utf8(path.expand(paste0("~/", filename)))

Could you try this and see if it helps?

Duncan Murdoch


From hkawakat at gmail.com  Wed May 24 13:59:00 2017
From: hkawakat at gmail.com (Hiroyuki Kawakatsu)
Date: Wed, 24 May 2017 12:59:00 +0100
Subject: [Rd] reg-tests-1d.R fails in r72721
In-Reply-To: <cd5731cf-69ee-10da-1f76-9b671e77584c@gmail.com>
References: <20170524094728.ynpyldncoeipp3bq@godel.dcu.ie>
 <cd5731cf-69ee-10da-1f76-9b671e77584c@gmail.com>
Message-ID: <20170524115900.xuxydwfoxdacq74b@godel.dcu.ie>

On 2017-05-24, Duncan Murdoch wrote:
> 
> I think the test is wrong because in the first case you are working in a
> locale where that character is representable.  In my locale it is not, so x1
> is converted to UTF-8, and everything compares equal.
> 
> An explicit conversion of x1 to UTF-8 should fix this, i.e. replace
> 
> x1 <- path.expand(paste0("~/", filename))
> 
> with
> 
> x1 <- enc2utf8(path.expand(paste0("~/", filename)))
> 
> Could you try this and see if it helps?

Nope:

> ## path.expand shouldn't translate to local encoding PR#17120
> filename <- "\U9b3c.R"
> 
> x11 <- path.expand(paste0("~/", filename))
> print(Encoding(x11))
[1] "unknown"
> x12 <- enc2utf8( path.expand(paste0("~/", filename)) )
> print(Encoding(x12))
[1] "unknown"
> x2 <- paste0(path.expand("~/"), filename)
> print(Encoding(x2))
[1] "UTF-8"
> 
> #stopifnot(identical(path.expand(paste0("~/", filename)),
> stopifnot(identical(enc2utf8( path.expand(paste0("~/", filename)) ),
+                   paste0(path.expand("~/"), filename)))
Error: identical(enc2utf8(path.expand(paste0("~/", filename))), paste0(path.expand("~/"),  .... is not TRUE
Execution halted

I forgot to report:

> sessionInfo()
R Under development (unstable) (2017-05-23 r72721)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Debian GNU/Linux 9 (stretch)

Matrix products: default
BLAS: /usr/local/share/R-devel/lib/libRblas.so
LAPACK: /usr/local/share/R-devel/lib/libRlapack.so

locale:
 [1] LC_CTYPE=en_GB.UTF-8       LC_NUMERIC=C              
 [3] LC_TIME=en_GB.UTF-8        LC_COLLATE=en_GB.UTF-8    
 [5] LC_MONETARY=en_GB.UTF-8    LC_MESSAGES=en_GB.UTF-8   
 [7] LC_PAPER=en_GB.UTF-8       LC_NAME=C                 
 [9] LC_ADDRESS=C               LC_TELEPHONE=C            
[11] LC_MEASUREMENT=en_GB.UTF-8 LC_IDENTIFICATION=C       

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

loaded via a namespace (and not attached):
[1] compiler_3.5.0

h.

-- 
+---
| Hiroyuki Kawakatsu
| Business School, Dublin City University
| Dublin 9, Ireland. Tel +353 (0)1 700 7496


From bhh at xs4all.nl  Wed May 24 14:08:03 2017
From: bhh at xs4all.nl (Berend Hasselman)
Date: Wed, 24 May 2017 14:08:03 +0200
Subject: [Rd] precision of do_arith() in arithmetic.c
In-Reply-To: <CAPtC_u3aBjF-A98_iRpw9xrf14uq14YpA9tj9iBzHG887B8baQ@mail.gmail.com>
References: <CAPtC_u3aBjF-A98_iRpw9xrf14uq14YpA9tj9iBzHG887B8baQ@mail.gmail.com>
Message-ID: <6620A708-0A09-4C25-BBDB-A3101CEC6716@xs4all.nl>


> On 24 May 2017, at 04:24, M Kim <mkim0710 at gmail.com> wrote:
> 
> To the R development team:
> 
> First of all, thank you so much for maintaining wonderful R software.
> 
> Perhaps, Dr. Ahn has just reported an error on the wilcox.test() function,
> and suggesting that an error may arise from abs() and rank().
> 
> 
> I just had a quick check that the problem may come from the precision of
> the results of arithmetic functions.
> 
> 
> 87.7-89.1+1.4
> # > 87.7-89.1+1.4
> # [1] 8.437695e-15
> 

R FAQ 7.31.
See https://cran.r-project.org/doc/FAQ/R-FAQ.html#Why-doesn_0027t-R-think-these-numbers-are-equal_003f

Berend

> I checked that do_arith() in arithmetic.c is using double type (8 byte) for
> PLUSOP, MINUSOP, TIMESOP, DIVOP etc.
> 
> https://github.com/wch/r-source/blob/f68b30e3b5479d84adbff516d48d4722a574dc82/src/main/arithmetic.c
> 
> 
> I have two thoughts:
> 
> (1) in the rank() function, we may round at less than 14 decimal places by
> default?
> 
> (2) using long double type (10 byte) instead of double type (8 byte) by
> default in the arithmetic function could be helpful for preventing
> embarassment?
> 
> Perhaps, R may provide the arithmetic function with various variable types,
> if some application needs.
> 
> 
> Thanks.
> 
> Min-hyung
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From murdoch.duncan at gmail.com  Wed May 24 14:35:26 2017
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Wed, 24 May 2017 08:35:26 -0400
Subject: [Rd] reg-tests-1d.R fails in r72721
In-Reply-To: <20170524115900.xuxydwfoxdacq74b@godel.dcu.ie>
References: <20170524094728.ynpyldncoeipp3bq@godel.dcu.ie>
 <cd5731cf-69ee-10da-1f76-9b671e77584c@gmail.com>
 <20170524115900.xuxydwfoxdacq74b@godel.dcu.ie>
Message-ID: <c50ef3bc-3bfe-4afe-416f-ae60346073a2@gmail.com>

On 24/05/2017 7:59 AM, Hiroyuki Kawakatsu wrote:
> On 2017-05-24, Duncan Murdoch wrote:
>>
>> I think the test is wrong because in the first case you are working in a
>> locale where that character is representable.  In my locale it is not, so x1
>> is converted to UTF-8, and everything compares equal.
>>
>> An explicit conversion of x1 to UTF-8 should fix this, i.e. replace
>>
>> x1 <- path.expand(paste0("~/", filename))
>>
>> with
>>
>> x1 <- enc2utf8(path.expand(paste0("~/", filename)))
>>
>> Could you try this and see if it helps?
>
> Nope:

Okay, how about if we weaken the test?  Instead of

stopifnot(identical(path.expand(paste0("~/", filename)),
                     paste0(path.expand("~/"), filename)))

try

stopifnot(path.expand(paste0("~/", filename)) ==
                       paste0(path.expand("~/"), filename))

Duncan Murdoch

>
>> ## path.expand shouldn't translate to local encoding PR#17120
>> filename <- "\U9b3c.R"
>>
>> x11 <- path.expand(paste0("~/", filename))
>> print(Encoding(x11))
> [1] "unknown"
>> x12 <- enc2utf8( path.expand(paste0("~/", filename)) )
>> print(Encoding(x12))
> [1] "unknown"
>> x2 <- paste0(path.expand("~/"), filename)
>> print(Encoding(x2))
> [1] "UTF-8"
>>
>> #stopifnot(identical(path.expand(paste0("~/", filename)),
>> stopifnot(identical(enc2utf8( path.expand(paste0("~/", filename)) ),
> +                   paste0(path.expand("~/"), filename)))
> Error: identical(enc2utf8(path.expand(paste0("~/", filename))), paste0(path.expand("~/"),  .... is not TRUE
> Execution halted
>
> I forgot to report:
>
>> sessionInfo()
> R Under development (unstable) (2017-05-23 r72721)
> Platform: x86_64-pc-linux-gnu (64-bit)
> Running under: Debian GNU/Linux 9 (stretch)
>
> Matrix products: default
> BLAS: /usr/local/share/R-devel/lib/libRblas.so
> LAPACK: /usr/local/share/R-devel/lib/libRlapack.so
>
> locale:
>  [1] LC_CTYPE=en_GB.UTF-8       LC_NUMERIC=C
>  [3] LC_TIME=en_GB.UTF-8        LC_COLLATE=en_GB.UTF-8
>  [5] LC_MONETARY=en_GB.UTF-8    LC_MESSAGES=en_GB.UTF-8
>  [7] LC_PAPER=en_GB.UTF-8       LC_NAME=C
>  [9] LC_ADDRESS=C               LC_TELEPHONE=C
> [11] LC_MEASUREMENT=en_GB.UTF-8 LC_IDENTIFICATION=C
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> loaded via a namespace (and not attached):
> [1] compiler_3.5.0
>
> h.
>


From hkawakat at gmail.com  Wed May 24 15:59:43 2017
From: hkawakat at gmail.com (Hiroyuki Kawakatsu)
Date: Wed, 24 May 2017 14:59:43 +0100
Subject: [Rd] reg-tests-1d.R fails in r72721
In-Reply-To: <c50ef3bc-3bfe-4afe-416f-ae60346073a2@gmail.com>
References: <20170524094728.ynpyldncoeipp3bq@godel.dcu.ie>
 <cd5731cf-69ee-10da-1f76-9b671e77584c@gmail.com>
 <20170524115900.xuxydwfoxdacq74b@godel.dcu.ie>
 <c50ef3bc-3bfe-4afe-416f-ae60346073a2@gmail.com>
Message-ID: <20170524135943.aj6hod6r2droq3ie@godel.dcu.ie>

On 2017-05-24, Duncan Murdoch wrote:
[...] 
> Okay, how about if we weaken the test?  
[...] 
> try
> 
> stopifnot(path.expand(paste0("~/", filename)) ==
>                       paste0(path.expand("~/"), filename))
> 

Nope:

> ## path.expand shouldn't translate to local encoding PR#17120
> filename <- "\U9b3c.R"
> 
> #stopifnot(identical(path.expand(paste0("~/", filename)),
> stopifnot(path.expand(paste0("~/", filename)) == 
+                   paste0(path.expand("~/"), filename))
Error: path.expand(paste0("~/", filename)) == paste0(path.expand("~/"),  .... is not TRUE
Execution halted

The problem is that path.expand(), or do_pathexpand() for
non-windoze calls translateChar() which in turn calls
translateToNative() which is unknown to make check (but not to R
--vanilla) under my setup. Once it is unknown, there seems to be no
way to force an encoding:

> ## path.expand shouldn't translate to local encoding PR#17120
> filename <- "\U9b3c.R"
> print(Encoding(filename))
[1] "UTF-8"
> 
> y1 <- paste0("~/", filename)
> print(Encoding(y1))
[1] "UTF-8"
> 
> y2 <- path.expand(y1)
> print(Encoding(y2))
[1] "unknown"
> 
> y3a <- iconv(y2, to="UTF-8")
> print(Encoding(y3a))
[1] "unknown"
> 
> y3b <- enc2utf8(y2)
> print(Encoding(y3b))
[1] "unknown"
> 
> Encoding(y2) <- "UTF-8"
> print(Encoding(y2))
[1] "unknown"
>

h.

-- 
+---
| Hiroyuki Kawakatsu
| Business School, Dublin City University
| Dublin 9, Ireland. Tel +353 (0)1 700 7496


From murdoch.duncan at gmail.com  Wed May 24 16:16:28 2017
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Wed, 24 May 2017 10:16:28 -0400
Subject: [Rd] reg-tests-1d.R fails in r72721
In-Reply-To: <20170524135943.aj6hod6r2droq3ie@godel.dcu.ie>
References: <20170524094728.ynpyldncoeipp3bq@godel.dcu.ie>
 <cd5731cf-69ee-10da-1f76-9b671e77584c@gmail.com>
 <20170524115900.xuxydwfoxdacq74b@godel.dcu.ie>
 <c50ef3bc-3bfe-4afe-416f-ae60346073a2@gmail.com>
 <20170524135943.aj6hod6r2droq3ie@godel.dcu.ie>
Message-ID: <6810b345-7d32-6668-e4fa-9c478b4c2600@gmail.com>

On 24/05/2017 9:59 AM, Hiroyuki Kawakatsu wrote:
> On 2017-05-24, Duncan Murdoch wrote:
> [...]
>> Okay, how about if we weaken the test?
> [...]
>> try
>>
>> stopifnot(path.expand(paste0("~/", filename)) ==
>>                       paste0(path.expand("~/"), filename))
>>
>
> Nope:
>
>> ## path.expand shouldn't translate to local encoding PR#17120
>> filename <- "\U9b3c.R"
>>
>> #stopifnot(identical(path.expand(paste0("~/", filename)),
>> stopifnot(path.expand(paste0("~/", filename)) ==
> +                   paste0(path.expand("~/"), filename))
> Error: path.expand(paste0("~/", filename)) == paste0(path.expand("~/"),  .... is not TRUE
> Execution halted

Thanks.  I've made that test conditional on running on Windows, and 
re-opened bug 17120.  I indicated that it's now a Unix-only bug.

This may be a first:  a case where R handles non-native characters 
better in Windows than it does in Unix.  I'm sure this will show up in a 
Microsoft ad soon :-).

Duncan Murdoch

> The problem is that path.expand(), or do_pathexpand() for
> non-windoze calls translateChar() which in turn calls
> translateToNative() which is unknown to make check (but not to R
> --vanilla) under my setup. Once it is unknown, there seems to be no
> way to force an encoding:
>
>> ## path.expand shouldn't translate to local encoding PR#17120
>> filename <- "\U9b3c.R"
>> print(Encoding(filename))
> [1] "UTF-8"
>>
>> y1 <- paste0("~/", filename)
>> print(Encoding(y1))
> [1] "UTF-8"
>>
>> y2 <- path.expand(y1)
>> print(Encoding(y2))
> [1] "unknown"
>>
>> y3a <- iconv(y2, to="UTF-8")
>> print(Encoding(y3a))
> [1] "unknown"
>>
>> y3b <- enc2utf8(y2)
>> print(Encoding(y3b))
> [1] "unknown"
>>
>> Encoding(y2) <- "UTF-8"
>> print(Encoding(y2))
> [1] "unknown"
>>
>
> h.
>


From nsosnov at microsoft.com  Wed May 24 23:18:54 2017
From: nsosnov at microsoft.com (Nathan Sosnovske)
Date: Wed, 24 May 2017 21:18:54 +0000
Subject: [Rd] Getting an R bugzilla account
In-Reply-To: <1495606449.23606.20.camel@iarc.fr>
References: <SN1PR21MB0061112D59B76A17EC7061BBB7F90@SN1PR21MB0061.namprd21.prod.outlook.com>
 <1495606449.23606.20.camel@iarc.fr>
Message-ID: <BY2PR21MB00523853B2A7FD175616B9B1B7FE0@BY2PR21MB0052.namprd21.prod.outlook.com>

Thank you for creating this! I'll go ahead and start working on bugs as I have time. I have a patch for the bug I linked below so you should see that soon. :)

Nathan

-----Original Message-----
From: Martyn Plummer [mailto:plummerm at iarc.fr] 
Sent: Tuesday, May 23, 2017 11:16 PM
To: r-devel at r-project.org; Nathan Sosnovske <nsosnov at microsoft.com>
Subject: Re: [Rd] Getting an R bugzilla account

Thanks for your help Nathan. I have added a bugzilla account for you.

Martyn

On Tue, 2017-05-23 at 21:02 +0000, Nathan Sosnovske via R-devel wrote:
> Hi All,
> 
> I have a fix to this bug ( 
> https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fbugs.
> r-project.org%2Fbugzilla3%2Fshow_&data=02%7C01%7Cnsosnov%40microsoft.c
> om%7Ce0541136ceb6429d9d0f08d4a26c1926%7C72f988bf86f141af91ab2d7cd011db
> 47%7C1%7C0%7C636312032542968195&sdata=oNadqT%2B%2Fm0pj86k8Z854lVSWWHUu
> t60YOaYmiaZfPZk%3D&reserved=0
> bug.cgi?id=16454) and would like to submit a patch to the bug report 
> on Bugzilla. I'd also like to start going through some of the other 
> Windows-specific issues and start fixing those. The bug submission 
> instructions indicate that I should ask here for a Bugzilla account.
> Is that still the correct procedure?
> 
> Thanks!
> 
> Nathan
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstat.ethz.ch%2Fmailman%2Flistinfo%2Fr-devel&data=02%7C01%7Cnsosnov%40microsoft.com%7Ce0541136ceb6429d9d0f08d4a26c1926%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C636312032542968195&sdata=nDdVG5QFWYItWdoKPq9XvYOGiwqq51ODtd53wuES5JY%3D&reserved=0

From sahil.kang at asilaycomputing.com  Thu May 25 06:17:43 2017
From: sahil.kang at asilaycomputing.com (Sahil Kang)
Date: Wed, 24 May 2017 21:17:43 -0700
Subject: [Rd] [PATCH] Ensure correct order of evaluation in macro
In-Reply-To: <708cb0eb-e0d9-d923-bd2c-c7c848ead4fc@gmail.com>
References: <fc0f3809-6832-92a8-607b-e9f845ebdcd8@asilaycomputing.com>
 <2DFDED2E-822C-40F5-902A-B1F32F5571D4@asilaycomputing.com>
 <708cb0eb-e0d9-d923-bd2c-c7c848ead4fc@gmail.com>
Message-ID: <b2ff2d3c-5bd5-5b08-c5c6-ec93346cf182@asilaycomputing.com>

Hi Martin,

Would you feel comfortable merging my small patch in?
I'd like to send additional patches that will improve these types of 
function macros so I'm hoping to start with this small change.

Thanks,
Sahil


From tomas.kalibera at gmail.com  Thu May 25 15:06:14 2017
From: tomas.kalibera at gmail.com (Tomas Kalibera)
Date: Thu, 25 May 2017 15:06:14 +0200
Subject: [Rd] Inconsistency in handling of numeric input with %d by
 sprintf
In-Reply-To: <CABKQe-aTQHz-0+iS+kNngaZLzf61SXR7tDfXx9Yze+t9RaqTFw@mail.gmail.com>
References: <CAPRVBcx3OjFK0G=hk1g8qChYiQ7cuZPpep-kju-_BD4MErhDuA@mail.gmail.com>
 <CABKQe-bStQz+AOQ7SGMcdiNY9BNOuaz21j7b3Mtp6-tqtpdbPg@mail.gmail.com>
 <CAO1zAVZNDbwMuV1v=fK6DMQqYt4o_4d6Vrme1jM_+BLnkNzsGQ@mail.gmail.com>
 <CAPRVBcxz3+n_r8N0TwWq0DC2ML274ahF68xemimNYbZz9RHbsQ@mail.gmail.com>
 <CAPRVBcw2YZ0cEpXCSjOKEfGHxN2MhqmM6TwtDJhQJjOBasbZ2g@mail.gmail.com>
 <CABKQe-aTQHz-0+iS+kNngaZLzf61SXR7tDfXx9Yze+t9RaqTFw@mail.gmail.com>
Message-ID: <8be81d2b-1b5c-39e0-c330-ef61479e47a2@gmail.com>

Thanks, fixed in 72737 (R-devel).

Best
Tomas

On 05/23/2017 06:32 PM, Evan Cortens wrote:
> Yes, what Joris posts about is exactly what I noted in my March 9th post to
> R-devel. The behaviour is sort of documented, but not in the clearest
> manner (in my opinion). Like I say, my ultimate conclusion was that the
> silent coercion of numerics to integers by sprintf() was a handy
> convenience, but not one that should be relied about to always work
> predictably.
>
> On Tue, May 23, 2017 at 10:02 AM, Michael Chirico <michaelchirico4 at gmail.com
>> wrote:
>> https://github.com/Rdatatable/data.table/issues/2171
>>
>> The fix was easy, it's just surprising to see the behavior change almost
>> on a whim. Just wanted to point it out in case this is unknown behavior,
>> but Evan seems to have found this as well.
>>
>> On Tue, May 23, 2017 at 12:00 PM, Michael Chirico <
>> michaelchirico4 at gmail.com> wrote:
>>
>>> Astute observation. And of course we should be passing integer when we
>>> use %d. It's an edge case in how we printed ITime objects in data.table:
>>>
>>>
>>> On Tue, May 23, 2017 at 11:53 AM, Joris Meys <jorismeys at gmail.com> wrote:
>>>
>>>> I initially thought this is "documented behaviour". ?sprintf says:
>>>>
>>>> Numeric variables with __exactly integer__ values will be coerced to
>>>> integer. (emphasis mine).
>>>>
>>>> Turns out this only works when the first value is numeric and not NA, as
>>>> shown by the following example:
>>>>
>>>>> sprintf("%d", as.numeric(c(NA,1)))
>>>> Error in sprintf("%d", as.numeric(c(NA, 1))) :
>>>>    invalid format '%d'; use format %f, %e, %g or %a for numeric objects
>>>>> sprintf("%d", as.numeric(c(1,NA)))
>>>> [1] "1"  "NA"
>>>>
>>>> So the safest thing is indeed passing the right type, but the behaviour
>>>> is indeed confusing. I checked this on both Windows and Debian, and on both
>>>> systems I get the exact same response.
>>>>
>>>> Cheers
>>>> Joris
>>>>
>>>> On Tue, May 23, 2017 at 4:53 PM, Evan Cortens <ecortens at mtroyal.ca>
>>>> wrote:
>>>>
>>>>> Hi Michael,
>>>>>
>>>>> I posted something on this topic to R-devel several weeks ago, but never
>>>>> got a response. My ultimate conclusion is that sprintf() isn't super
>>>>> consistent in how it handles coercion: sometimes it'll coerce real to
>>>>> integer without complaint, other times it won't. (My particular email
>>>>> had
>>>>> to do with the vectors longer than 1 and their positioning vis-a-vis the
>>>>> format string.) The safest thing is just to pass the right type. In this
>>>>> case, sprintf('%d', as.integer(NA_real_)) works.
>>>>>
>>>>> Best,
>>>>>
>>>>> Evan
>>>>>
>>>>> On Fri, May 19, 2017 at 9:23 AM, Michael Chirico <
>>>>> michaelchirico4 at gmail.com>
>>>>> wrote:
>>>>>
>>>>>> Consider
>>>>>>
>>>>>> #as.numeric for emphasis
>>>>>> sprintf('%d', as.numeric(1))
>>>>>> # [1] "1"
>>>>>>
>>>>>> vs.
>>>>>>
>>>>>> sprintf('%d', NA_real_)
>>>>>>
>>>>>>>   Error in sprintf("%d", NA_real_) :
>>>>>>     invalid format '%d'; use format %f, %e, %g or %a for numeric object
>>>>>> I understand the error is correct, but if it works for other numeric
>>>>> input,
>>>>>> why doesn't R just coerce NA_real_ to NA_integer_?
>>>>>>
>>>>>> Michael Chirico
>>>>>>
>>>>>>          [[alternative HTML version deleted]]
>>>>>>
>>>>>> ______________________________________________
>>>>>> R-devel at r-project.org mailing list
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>>>
>>>>>
>>>>>
>>>>> --
>>>>> Evan Cortens, PhD
>>>>> Institutional Analyst - Office of Institutional Analysis
>>>>> Mount Royal University
>>>>> 403-440-6529
>>>>>
>>>>>          [[alternative HTML version deleted]]
>>>>>
>>>>> ______________________________________________
>>>>> R-devel at r-project.org mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>>
>>>>
>>>>
>>>> --
>>>> Joris Meys
>>>> Statistical consultant
>>>>
>>>> Ghent University
>>>> Faculty of Bioscience Engineering
>>>> Department of Mathematical Modelling, Statistics and Bio-Informatics
>>>>
>>>> tel :  +32 (0)9 264 61 79 <+32%209%20264%2061%2079>
>>>> Joris.Meys at Ugent.be
>>>> -------------------------------
>>>> Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php
>>>>
>>>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From roman.kiselew at gmail.com  Fri May 26 14:59:18 2017
From: roman.kiselew at gmail.com (Roman Kiselev)
Date: Fri, 26 May 2017 14:59:18 +0200
Subject: [Rd] Bug in R 3.4.0: R CMD Sweave return value is 1 on success
 (instead of 0)
Message-ID: <061a10ee-8a44-fb53-39f7-a61b9cf37031@gmail.com>

Dear all,

after an update from R 3.3.x to R 3.4.0 I cannot build Sweave documents 
using make, because make checks the exit code of the `R CMD Sweave` and 
stops if it is not zero. I believe that this is a bug and that the 
return code should be 0 on success and any other value in case of error.

Regards
Roman Kiselev



	[[alternative HTML version deleted]]


From tomas.kalibera at gmail.com  Fri May 26 15:17:41 2017
From: tomas.kalibera at gmail.com (Tomas Kalibera)
Date: Fri, 26 May 2017 15:17:41 +0200
Subject: [Rd] Bug in R 3.4.0: R CMD Sweave return value is 1 on success
 (instead of 0)
In-Reply-To: <061a10ee-8a44-fb53-39f7-a61b9cf37031@gmail.com>
References: <061a10ee-8a44-fb53-39f7-a61b9cf37031@gmail.com>
Message-ID: <4f3d9c21-9a59-153b-5e60-cb7c66d3bb54@gmail.com>

This bug has been fixed in R-devel 72612 and R-patched 72614.

Best
Tomas

On 05/26/2017 02:59 PM, Roman Kiselev wrote:
> Dear all,
>
> after an update from R 3.3.x to R 3.4.0 I cannot build Sweave documents
> using make, because make checks the exit code of the `R CMD Sweave` and
> stops if it is not zero. I believe that this is a bug and that the
> return code should be 0 on success and any other value in case of error.
>
> Regards
> Roman Kiselev
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From glnbrntt at gmail.com  Sun May 28 03:28:26 2017
From: glnbrntt at gmail.com (GlenB)
Date: Sun, 28 May 2017 11:28:26 +1000
Subject: [Rd] stats::line() does not produce correct Tukey line when n mod 6
 is 2 or 3
Message-ID: <CAN7_cLjefc605D1ATS__MOV4gN7fKDBo5Nx_CNB_B9cDbAxc6w@mail.gmail.com>

Bug: stats::line() does not produce correct Tukey line when n mod 6 is 2 or
3

Example: line(1:9,1:9) should have intercept 0 and slope 1 but it gives
intercept -1 and slope 1.2

Trying line(1:i,1:i) across a range of i makes it clear there's a cycle of
length 6, with four of every six correct.

Bug has been present across many versions.

The machine I just tried it on just now has R3.2.3:

               _
platform       x86_64-w64-mingw32
arch           x86_64
os             mingw32
system         x86_64, mingw32
status
major          3
minor          2.3
year           2015
month          12
day            10
svn rev        69752
language       R
version.string R version 3.2.3 (2015-12-10)
nickname       Wooden Christmas-Tree

	[[alternative HTML version deleted]]


From jorismeys at gmail.com  Sun May 28 15:27:05 2017
From: jorismeys at gmail.com (Joris Meys)
Date: Sun, 28 May 2017 15:27:05 +0200
Subject: [Rd] stats::line() does not produce correct Tukey line when n
 mod 6 is 2 or 3
In-Reply-To: <CAN7_cLjefc605D1ATS__MOV4gN7fKDBo5Nx_CNB_B9cDbAxc6w@mail.gmail.com>
References: <CAN7_cLjefc605D1ATS__MOV4gN7fKDBo5Nx_CNB_B9cDbAxc6w@mail.gmail.com>
Message-ID: <CAO1zAVZJpWQsvcJBAb2dib8UUc7xCtQ1YfQrYK6DatoyjRS7ug@mail.gmail.com>

Can confirm this in R 3.4.0 :

end <- 6:100
res <- lapply(end, function(i) line(1:i,1:i))
absresid <- sapply(res, function(i) mean(abs(resid(i))))
plot(absresid, type = "h")
coefs <- sapply(res, coef)
plot(coefs[1,], coefs[2,])

> sessionInfo()
R version 3.4.0 (2017-04-21)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows >= 8 x64 (build 9200)

Matrix products: default

locale:
[1] LC_COLLATE=English_United Kingdom.1252  LC_CTYPE=English_United
Kingdom.1252
[3] LC_MONETARY=English_United Kingdom.1252
LC_NUMERIC=C
[5] LC_TIME=English_United Kingdom.1252

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

loaded via a namespace (and not attached):
[1] compiler_3.4.0 tools_3.4.0




On Sun, May 28, 2017 at 3:28 AM, GlenB <glnbrntt at gmail.com> wrote:

> Bug: stats::line() does not produce correct Tukey line when n mod 6 is 2 or
> 3
>
> Example: line(1:9,1:9) should have intercept 0 and slope 1 but it gives
> intercept -1 and slope 1.2
>
> Trying line(1:i,1:i) across a range of i makes it clear there's a cycle of
> length 6, with four of every six correct.
>
> Bug has been present across many versions.
>
> The machine I just tried it on just now has R3.2.3:
>
>                _
> platform       x86_64-w64-mingw32
> arch           x86_64
> os             mingw32
> system         x86_64, mingw32
> status
> major          3
> minor          2.3
> year           2015
> month          12
> day            10
> svn rev        69752
> language       R
> version.string R version 3.2.3 (2015-12-10)
> nickname       Wooden Christmas-Tree
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>



-- 
Joris Meys
Statistical consultant

Ghent University
Faculty of Bioscience Engineering
Department of Mathematical Modelling, Statistics and Bio-Informatics

tel :  +32 (0)9 264 61 79
Joris.Meys at Ugent.be
-------------------------------
Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php

	[[alternative HTML version deleted]]


From arne.henningsen at gmail.com  Sun May 28 17:37:30 2017
From: arne.henningsen at gmail.com (Arne Henningsen)
Date: Sun, 28 May 2017 17:37:30 +0200
Subject: [Rd] Rounding in print.summaryDefault()
Message-ID: <CAMTWbJi22S2+ioQbPYA=L12dDAdrr8BtV_L2pjJ2LgDBk-VLTA@mail.gmail.com>

Dear all

I am happy that summary.default() no longer rounds since R 3.4.0.

However, in R 3.4.0, in a few cases, print.summaryDefault() rounds the
mean value (and the median value) differently on my GNU/Linux machine
and on my colleague's MS-Windows machine. Here is a small (simplified)
reproducible example:

R> a <- 1234568.01 + c(0:1)
R> summary(a)

Output on MS-Windows (expected rounding of the mean value):
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
1234568 1234568 1234569 1234569 1234569 1234569

Output on GNU/Linux (unexpected rounding of the mean value):
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
1234568 1234568 1234568 1234568 1234569 1234569

The following code gives the same output on MS-Windows and on GNU/Linux:
R> print(summary(a), digits=9)
    Min.   1st Qu.    Median      Mean   3rd Qu.      Max.
1234568.0 1234568.3 1234568.5 1234568.5 1234568.8 1234569.0
R> summary(a)["Mean"]
  Mean
1234569
R> mean(a)
[1] 1234569
R> print(mean(a), digits=9)
[1] 1234568.51

Can these outputs be reproduced by other GNU/Linux and MS-Windows users?

If these differences can be reproduced by others: Should these
differences in the output on GNU/Linux and MS-Windows be considered as
a bug?

Does anybody know how one can avoid to get different roundings in the
outputs of summary() on different computers?

Best,
Arne

-- 
Arne Henningsen
http://www.arne-henningsen.name


From edd at debian.org  Sun May 28 17:51:22 2017
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 28 May 2017 10:51:22 -0500
Subject: [Rd] Rounding in print.summaryDefault()
In-Reply-To: <CAMTWbJi22S2+ioQbPYA=L12dDAdrr8BtV_L2pjJ2LgDBk-VLTA@mail.gmail.com>
References: <CAMTWbJi22S2+ioQbPYA=L12dDAdrr8BtV_L2pjJ2LgDBk-VLTA@mail.gmail.com>
Message-ID: <22826.61946.88892.195164@max.eddelbuettel.com>


On 28 May 2017 at 17:37, Arne Henningsen wrote:
| Dear all
| 
| I am happy that summary.default() no longer rounds since R 3.4.0.
| 
| However, in R 3.4.0, in a few cases, print.summaryDefault() rounds the
| mean value (and the median value) differently on my GNU/Linux machine
| and on my colleague's MS-Windows machine. Here is a small (simplified)
| reproducible example:
| 
| R> a <- 1234568.01 + c(0:1)
| R> summary(a)
| 
| Output on MS-Windows (expected rounding of the mean value):
|    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
| 1234568 1234568 1234569 1234569 1234569 1234569
| 
| Output on GNU/Linux (unexpected rounding of the mean value):
|    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
| 1234568 1234568 1234568 1234568 1234569 1234569

Not here:

R> a <- 1234568.01 + c(0:1)
R> a
[1] 1234568 1234569
R> summary(a)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
1234568 1234568 1234569 1234569 1234569 1234569 
R> Sys.info()
                                      sysname 
                                      "Linux" 
                                      release 
                           "4.8.0-39-generic" 
                                      version 
"#42-Ubuntu SMP Mon Feb 20 11:47:27 UTC 2017" 
                                     nodename 
                                        "max" 
                                      machine 
                                     "x86_64" 
                                        login 
                                        "edd" 
                                         user 
                                        "edd" 
                               effective_user 
                                        "edd" 
R> 

Ubuntu 16.10, 64bit.

Dirk
 
| The following code gives the same output on MS-Windows and on GNU/Linux:
| R> print(summary(a), digits=9)
|     Min.   1st Qu.    Median      Mean   3rd Qu.      Max.
| 1234568.0 1234568.3 1234568.5 1234568.5 1234568.8 1234569.0
| R> summary(a)["Mean"]
|   Mean
| 1234569
| R> mean(a)
| [1] 1234569
| R> print(mean(a), digits=9)
| [1] 1234568.51
| 
| Can these outputs be reproduced by other GNU/Linux and MS-Windows users?
| 
| If these differences can be reproduced by others: Should these
| differences in the output on GNU/Linux and MS-Windows be considered as
| a bug?
| 
| Does anybody know how one can avoid to get different roundings in the
| outputs of summary() on different computers?
| 
| Best,
| Arne
| 
| -- 
| Arne Henningsen
| http://www.arne-henningsen.name
| 
| ______________________________________________
| R-devel at r-project.org mailing list
| https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
http://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From jorismeys at gmail.com  Sun May 28 17:52:30 2017
From: jorismeys at gmail.com (Joris Meys)
Date: Sun, 28 May 2017 17:52:30 +0200
Subject: [Rd] Rounding in print.summaryDefault()
In-Reply-To: <CAMTWbJi22S2+ioQbPYA=L12dDAdrr8BtV_L2pjJ2LgDBk-VLTA@mail.gmail.com>
References: <CAMTWbJi22S2+ioQbPYA=L12dDAdrr8BtV_L2pjJ2LgDBk-VLTA@mail.gmail.com>
Message-ID: <CAO1zAVa3=PKv9HFLsFoyXKObNJ_PSr-15KH9zkWgF1ujmBZ4Bg@mail.gmail.com>

Weird, because the output of Windows is actually correct. The exact mean of
a is 1234568.51, which rounds to 1234569.

I can reproduce the Windows output on a Windows machine. My Debian server
has R 3.3.1 , and there I can reproduce your output using:

> summary(a, digits = 8)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
1234568 1234568 1234568 1234568 1234569 1234569

> sessionInfo()
R version 3.3.1 (2016-06-21)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Debian GNU/Linux 8 (jessie)

locale:
 [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
 [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
 [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8
 [7] LC_PAPER=en_US.UTF-8       LC_NAME=C
 [9] LC_ADDRESS=C               LC_TELEPHONE=C
[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base



On Sun, May 28, 2017 at 5:37 PM, Arne Henningsen <arne.henningsen at gmail.com>
wrote:

> Dear all
>
> I am happy that summary.default() no longer rounds since R 3.4.0.
>
> However, in R 3.4.0, in a few cases, print.summaryDefault() rounds the
> mean value (and the median value) differently on my GNU/Linux machine
> and on my colleague's MS-Windows machine. Here is a small (simplified)
> reproducible example:
>
> R> a <- 1234568.01 + c(0:1)
> R> summary(a)
>
> Output on MS-Windows (expected rounding of the mean value):
>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
> 1234568 1234568 1234569 1234569 1234569 1234569
>
> Output on GNU/Linux (unexpected rounding of the mean value):
>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
> 1234568 1234568 1234568 1234568 1234569 1234569
>
> The following code gives the same output on MS-Windows and on GNU/Linux:
> R> print(summary(a), digits=9)
>     Min.   1st Qu.    Median      Mean   3rd Qu.      Max.
> 1234568.0 1234568.3 1234568.5 1234568.5 1234568.8 1234569.0
> R> summary(a)["Mean"]
>   Mean
> 1234569
> R> mean(a)
> [1] 1234569
> R> print(mean(a), digits=9)
> [1] 1234568.51
>
> Can these outputs be reproduced by other GNU/Linux and MS-Windows users?
>
> If these differences can be reproduced by others: Should these
> differences in the output on GNU/Linux and MS-Windows be considered as
> a bug?
>
> Does anybody know how one can avoid to get different roundings in the
> outputs of summary() on different computers?
>
> Best,
> Arne
>
> --
> Arne Henningsen
> http://www.arne-henningsen.name
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>



-- 
Joris Meys
Statistical consultant

Ghent University
Faculty of Bioscience Engineering
Department of Mathematical Modelling, Statistics and Bio-Informatics

tel :  +32 (0)9 264 61 79
Joris.Meys at Ugent.be
-------------------------------
Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php

	[[alternative HTML version deleted]]


From jorismeys at gmail.com  Sun May 28 17:58:20 2017
From: jorismeys at gmail.com (Joris Meys)
Date: Sun, 28 May 2017 17:58:20 +0200
Subject: [Rd] Rounding in print.summaryDefault()
In-Reply-To: <22826.61946.88892.195164@max.eddelbuettel.com>
References: <CAMTWbJi22S2+ioQbPYA=L12dDAdrr8BtV_L2pjJ2LgDBk-VLTA@mail.gmail.com>
 <22826.61946.88892.195164@max.eddelbuettel.com>
Message-ID: <CAO1zAVYb0Nx6NdVv2Sqxr+HOR_FE8u54qDKxxoKnBZ-A4W=sDQ@mail.gmail.com>

Might this be related to the Linux version? I'm testing on one of our
university servers, and they tend to be deprived of regular updates
sometimes... (Dirk, sorry for sending you this twice.)

> Sys.info()
                                    sysname
                                    "Linux"
                                    release
                           "3.16.0-4-amd64"
                                    version
"#1 SMP Debian 3.16.7-ckt11-1 (2015-05-24)"
                                   nodename
                                  "ourServer"
                                    machine
                                   "x86_64"
                                      login
                                   "me"
                                       user
                                   "me"
                             effective_user
                                   "me"

On Sun, May 28, 2017 at 5:51 PM, Dirk Eddelbuettel <edd at debian.org> wrote:

>
> On 28 May 2017 at 17:37, Arne Henningsen wrote:
> | Dear all
> |
> | I am happy that summary.default() no longer rounds since R 3.4.0.
> |
> | However, in R 3.4.0, in a few cases, print.summaryDefault() rounds the
> | mean value (and the median value) differently on my GNU/Linux machine
> | and on my colleague's MS-Windows machine. Here is a small (simplified)
> | reproducible example:
> |
> | R> a <- 1234568.01 + c(0:1)
> | R> summary(a)
> |
> | Output on MS-Windows (expected rounding of the mean value):
> |    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
> | 1234568 1234568 1234569 1234569 1234569 1234569
> |
> | Output on GNU/Linux (unexpected rounding of the mean value):
> |    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
> | 1234568 1234568 1234568 1234568 1234569 1234569
>
> Not here:
>
> R> a <- 1234568.01 + c(0:1)
> R> a
> [1] 1234568 1234569
> R> summary(a)
>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
> 1234568 1234568 1234569 1234569 1234569 1234569
> R> Sys.info()
>                                       sysname
>                                       "Linux"
>                                       release
>                            "4.8.0-39-generic"
>                                       version
> "#42-Ubuntu SMP Mon Feb 20 11:47:27 UTC 2017"
>                                      nodename
>                                         "max"
>                                       machine
>                                      "x86_64"
>                                         login
>                                         "edd"
>                                          user
>                                         "edd"
>                                effective_user
>                                         "edd"
> R>
>
> Ubuntu 16.10, 64bit.
>
> Dirk
>
> | The following code gives the same output on MS-Windows and on GNU/Linux:
> | R> print(summary(a), digits=9)
> |     Min.   1st Qu.    Median      Mean   3rd Qu.      Max.
> | 1234568.0 1234568.3 1234568.5 1234568.5 1234568.8 1234569.0
> | R> summary(a)["Mean"]
> |   Mean
> | 1234569
> | R> mean(a)
> | [1] 1234569
> | R> print(mean(a), digits=9)
> | [1] 1234568.51
> |
> | Can these outputs be reproduced by other GNU/Linux and MS-Windows users?
> |
> | If these differences can be reproduced by others: Should these
> | differences in the output on GNU/Linux and MS-Windows be considered as
> | a bug?
> |
> | Does anybody know how one can avoid to get different roundings in the
> | outputs of summary() on different computers?
> |
> | Best,
> | Arne
> |
> | --
> | Arne Henningsen
> | http://www.arne-henningsen.name
> |
> | ______________________________________________
> | R-devel at r-project.org mailing list
> | https://stat.ethz.ch/mailman/listinfo/r-devel
>
> --
> http://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>



-- 
Joris Meys
Statistical consultant

Ghent University
Faculty of Bioscience Engineering
Department of Mathematical Modelling, Statistics and Bio-Informatics

tel :  +32 (0)9 264 61 79
Joris.Meys at Ugent.be
-------------------------------
Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php

	[[alternative HTML version deleted]]


From arne.henningsen at gmail.com  Sun May 28 19:15:09 2017
From: arne.henningsen at gmail.com (Arne Henningsen)
Date: Sun, 28 May 2017 19:15:09 +0200
Subject: [Rd] Rounding in print.summaryDefault()
In-Reply-To: <CAO1zAVYb0Nx6NdVv2Sqxr+HOR_FE8u54qDKxxoKnBZ-A4W=sDQ@mail.gmail.com>
References: <CAMTWbJi22S2+ioQbPYA=L12dDAdrr8BtV_L2pjJ2LgDBk-VLTA@mail.gmail.com>
 <22826.61946.88892.195164@max.eddelbuettel.com>
 <CAO1zAVYb0Nx6NdVv2Sqxr+HOR_FE8u54qDKxxoKnBZ-A4W=sDQ@mail.gmail.com>
Message-ID: <CAMTWbJhHKWAGTgDf37eFVqcCa-UZnD_6BxF46s9sU3DLmJWatw@mail.gmail.com>

Thanks for all your responses. The issue seems to be more complex than
I thought. Here is the output of Sys.info() and sessionInfo() on my
GNU/Linux machine (Ubuntu 16.04.02 LTS):

R> a <- 1234568.01 + c(0:1)
R> a
[1] 1234568 1234569
R> summary(a)
  Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
1234568 1234568 1234568 1234568 1234569 1234569
R> Sys.info()
                                        sysname
                                        "Linux"
                                        release
                      "4.5.0-040500rc6-generic"
                                        version
"#201602281230 SMP Sun Feb 28 17:33:02 UTC 2016"
                                       nodename
                             "arne-HP-EB-8560w"
                                        machine
                                       "x86_64"
                                          login
                                         "arne"
                                           user
                                         "arne"
                                 effective_user
                                         "arne"
R> sessionInfo()
R version 3.4.0 (2017-04-21)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Ubuntu 16.04.2 LTS

Matrix products: default
BLAS: /usr/lib/atlas-base/atlas/libblas.so.3.0
LAPACK: /usr/lib/atlas-base/atlas/liblapack.so.3.0

locale:
[1] LC_CTYPE=en_GB.UTF-8       LC_NUMERIC=C
[3] LC_TIME=da_DK.UTF-8        LC_COLLATE=en_GB.UTF-8
[5] LC_MONETARY=da_DK.UTF-8    LC_MESSAGES=en_GB.UTF-8
[7] LC_PAPER=en_GB.UTF-8       LC_NAME=C
[9] LC_ADDRESS=C               LC_TELEPHONE=C
[11] LC_MEASUREMENT=da_DK.UTF-8 LC_IDENTIFICATION=C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

loaded via a namespace (and not attached):
[1] compiler_3.4.0 tools_3.4.0


/Arne


On 28 May 2017 at 17:58, Joris Meys <jorismeys at gmail.com> wrote:
> Might this be related to the Linux version? I'm testing on one of our
> university servers, and they tend to be deprived of regular updates
> sometimes... (Dirk, sorry for sending you this twice.)
>
>> Sys.info()
>                                     sysname
>                                     "Linux"
>                                     release
>                            "3.16.0-4-amd64"
>                                     version
> "#1 SMP Debian 3.16.7-ckt11-1 (2015-05-24)"
>                                    nodename
>                                   "ourServer"
>                                     machine
>                                    "x86_64"
>                                       login
>                                    "me"
>                                        user
>                                    "me"
>                              effective_user
>                                    "me"
>
> On Sun, May 28, 2017 at 5:51 PM, Dirk Eddelbuettel <edd at debian.org> wrote:
>>
>>
>> On 28 May 2017 at 17:37, Arne Henningsen wrote:
>> | Dear all
>> |
>> | I am happy that summary.default() no longer rounds since R 3.4.0.
>> |
>> | However, in R 3.4.0, in a few cases, print.summaryDefault() rounds the
>> | mean value (and the median value) differently on my GNU/Linux machine
>> | and on my colleague's MS-Windows machine. Here is a small (simplified)
>> | reproducible example:
>> |
>> | R> a <- 1234568.01 + c(0:1)
>> | R> summary(a)
>> |
>> | Output on MS-Windows (expected rounding of the mean value):
>> |    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>> | 1234568 1234568 1234569 1234569 1234569 1234569
>> |
>> | Output on GNU/Linux (unexpected rounding of the mean value):
>> |    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>> | 1234568 1234568 1234568 1234568 1234569 1234569
>>
>> Not here:
>>
>> R> a <- 1234568.01 + c(0:1)
>> R> a
>> [1] 1234568 1234569
>> R> summary(a)
>>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>> 1234568 1234568 1234569 1234569 1234569 1234569
>> R> Sys.info()
>>                                       sysname
>>                                       "Linux"
>>                                       release
>>                            "4.8.0-39-generic"
>>                                       version
>> "#42-Ubuntu SMP Mon Feb 20 11:47:27 UTC 2017"
>>                                      nodename
>>                                         "max"
>>                                       machine
>>                                      "x86_64"
>>                                         login
>>                                         "edd"
>>                                          user
>>                                         "edd"
>>                                effective_user
>>                                         "edd"
>> R>
>>
>> Ubuntu 16.10, 64bit.
>>
>> Dirk
>>
>> | The following code gives the same output on MS-Windows and on GNU/Linux:
>> | R> print(summary(a), digits=9)
>> |     Min.   1st Qu.    Median      Mean   3rd Qu.      Max.
>> | 1234568.0 1234568.3 1234568.5 1234568.5 1234568.8 1234569.0
>> | R> summary(a)["Mean"]
>> |   Mean
>> | 1234569
>> | R> mean(a)
>> | [1] 1234569
>> | R> print(mean(a), digits=9)
>> | [1] 1234568.51
>> |
>> | Can these outputs be reproduced by other GNU/Linux and MS-Windows users?
>> |
>> | If these differences can be reproduced by others: Should these
>> | differences in the output on GNU/Linux and MS-Windows be considered as
>> | a bug?
>> |
>> | Does anybody know how one can avoid to get different roundings in the
>> | outputs of summary() on different computers?
>> |
>> | Best,
>> | Arne
>> |
>> | --
>> | Arne Henningsen
>> | http://www.arne-henningsen.name
>> |
>> | ______________________________________________
>> | R-devel at r-project.org mailing list
>> | https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>> --
>> http://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>
>
>
>
> --
> Joris Meys
> Statistical consultant
>
> Ghent University
> Faculty of Bioscience Engineering
> Department of Mathematical Modelling, Statistics and Bio-Informatics
>
> tel :  +32 (0)9 264 61 79
> Joris.Meys at Ugent.be
> -------------------------------
> Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php



-- 
Arne Henningsen
http://www.arne-henningsen.name


From murdoch.duncan at gmail.com  Mon May 29 00:40:14 2017
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Sun, 28 May 2017 18:40:14 -0400
Subject: [Rd] stats::line() does not produce correct Tukey line when n
 mod 6 is 2 or 3
In-Reply-To: <CAN7_cLjefc605D1ATS__MOV4gN7fKDBo5Nx_CNB_B9cDbAxc6w@mail.gmail.com>
References: <CAN7_cLjefc605D1ATS__MOV4gN7fKDBo5Nx_CNB_B9cDbAxc6w@mail.gmail.com>
Message-ID: <ffc3e473-622d-796f-a06a-6e1f92f8ab5f@gmail.com>

On 27/05/2017 9:28 PM, GlenB wrote:
> Bug: stats::line() does not produce correct Tukey line when n mod 6 is 2 or
> 3
>
> Example: line(1:9,1:9) should have intercept 0 and slope 1 but it gives
> intercept -1 and slope 1.2
>
> Trying line(1:i,1:i) across a range of i makes it clear there's a cycle of
> length 6, with four of every six correct.
>
> Bug has been present across many versions.
>
> The machine I just tried it on just now has R3.2.3:

If you look at the source (in src/library/stats/src/line.c), the 
explanation is clear:  the x value is chosen as the 1/6 quantile 
(according to a particular definition of quantile), and the y value is 
chosen as the median of the y values where x is less than or equal to 
the 1/3 quantile.  Those are different definitions (though I think they 
would be asymptotically equivalent under pretty weak assumptions), so 
it's not surprising the x value doesn't correspond perfectly to the y 
value, and the line ends up "wrong".

So is it a bug?  Well, that depends on Tukey's definition.  I don't have 
a copy of his book handy so I can't really say.  Maybe the R function is 
doing exactly what Tukey said it should, and that's not a bug.  Or maybe 
R is wrong.

Duncan Murdoch


From glnbrntt at gmail.com  Mon May 29 06:19:37 2017
From: glnbrntt at gmail.com (GlenB)
Date: Mon, 29 May 2017 14:19:37 +1000
Subject: [Rd] stats::line() does not produce correct Tukey line when n
 mod 6 is 2 or 3
In-Reply-To: <ffc3e473-622d-796f-a06a-6e1f92f8ab5f@gmail.com>
References: <CAN7_cLjefc605D1ATS__MOV4gN7fKDBo5Nx_CNB_B9cDbAxc6w@mail.gmail.com>
 <ffc3e473-622d-796f-a06a-6e1f92f8ab5f@gmail.com>
Message-ID: <CAN7_cLjmQFtZf9z-qJKdWRCaRY2uxfP8k5=QJsptz3U+TtP8oA@mail.gmail.com>

Tukey divides the points into three groups, not the x and y values
separately.

I'll try to get hold of the book for a direct quote, might take a couple of
days.



On Mon, May 29, 2017 at 8:40 AM, Duncan Murdoch <murdoch.duncan at gmail.com>
wrote:

> On 27/05/2017 9:28 PM, GlenB wrote:
>
>> Bug: stats::line() does not produce correct Tukey line when n mod 6 is 2
>> or
>> 3
>>
>> Example: line(1:9,1:9) should have intercept 0 and slope 1 but it gives
>> intercept -1 and slope 1.2
>>
>> Trying line(1:i,1:i) across a range of i makes it clear there's a cycle of
>> length 6, with four of every six correct.
>>
>> Bug has been present across many versions.
>>
>> The machine I just tried it on just now has R3.2.3:
>>
>
> If you look at the source (in src/library/stats/src/line.c), the
> explanation is clear:  the x value is chosen as the 1/6 quantile (according
> to a particular definition of quantile), and the y value is chosen as the
> median of the y values where x is less than or equal to the 1/3 quantile.
> Those are different definitions (though I think they would be
> asymptotically equivalent under pretty weak assumptions), so it's not
> surprising the x value doesn't correspond perfectly to the y value, and the
> line ends up "wrong".
>
> So is it a bug?  Well, that depends on Tukey's definition.  I don't have a
> copy of his book handy so I can't really say.  Maybe the R function is
> doing exactly what Tukey said it should, and that's not a bug.  Or maybe R
> is wrong.
>
> Duncan Murdoch
>
>

	[[alternative HTML version deleted]]


From glnbrntt at gmail.com  Mon May 29 07:21:08 2017
From: glnbrntt at gmail.com (GlenB)
Date: Mon, 29 May 2017 15:21:08 +1000
Subject: [Rd] stats::line() does not produce correct Tukey line when n
 mod 6 is 2 or 3
In-Reply-To: <CAN7_cLjmQFtZf9z-qJKdWRCaRY2uxfP8k5=QJsptz3U+TtP8oA@mail.gmail.com>
References: <CAN7_cLjefc605D1ATS__MOV4gN7fKDBo5Nx_CNB_B9cDbAxc6w@mail.gmail.com>
 <ffc3e473-622d-796f-a06a-6e1f92f8ab5f@gmail.com>
 <CAN7_cLjmQFtZf9z-qJKdWRCaRY2uxfP8k5=QJsptz3U+TtP8oA@mail.gmail.com>
Message-ID: <CAN7_cLjbz6S8u79jtx52XgfD1LcpO8CePYUhrunYqrzXHMMN8A@mail.gmail.com>

> Tukey divides the points into three groups, not the x and y values
separately.

> I'll try to get hold of the book for a direct quote, might take a couple
of days.

Ah well, I can't get it for a week. But the fact that it's often called
Tukey's three group line (try a search on *tukey three group line* and
you'll get plenty of hits) is pretty much a giveaway.


On Mon, May 29, 2017 at 2:19 PM, GlenB <glnbrntt at gmail.com> wrote:

> Tukey divides the points into three groups, not the x and y values
> separately.
>
> I'll try to get hold of the book for a direct quote, might take a couple
> of days.
>
>
>
> On Mon, May 29, 2017 at 8:40 AM, Duncan Murdoch <murdoch.duncan at gmail.com>
> wrote:
>
>> On 27/05/2017 9:28 PM, GlenB wrote:
>>
>>> Bug: stats::line() does not produce correct Tukey line when n mod 6 is 2
>>> or
>>> 3
>>>
>>> Example: line(1:9,1:9) should have intercept 0 and slope 1 but it gives
>>> intercept -1 and slope 1.2
>>>
>>> Trying line(1:i,1:i) across a range of i makes it clear there's a cycle
>>> of
>>> length 6, with four of every six correct.
>>>
>>> Bug has been present across many versions.
>>>
>>> The machine I just tried it on just now has R3.2.3:
>>>
>>
>> If you look at the source (in src/library/stats/src/line.c), the
>> explanation is clear:  the x value is chosen as the 1/6 quantile (according
>> to a particular definition of quantile), and the y value is chosen as the
>> median of the y values where x is less than or equal to the 1/3 quantile.
>> Those are different definitions (though I think they would be
>> asymptotically equivalent under pretty weak assumptions), so it's not
>> surprising the x value doesn't correspond perfectly to the y value, and the
>> line ends up "wrong".
>>
>> So is it a bug?  Well, that depends on Tukey's definition.  I don't have
>> a copy of his book handy so I can't really say.  Maybe the R function is
>> doing exactly what Tukey said it should, and that's not a bug.  Or maybe R
>> is wrong.
>>
>> Duncan Murdoch
>>
>>
>

	[[alternative HTML version deleted]]


From pdalgd at gmail.com  Mon May 29 10:02:03 2017
From: pdalgd at gmail.com (peter dalgaard)
Date: Mon, 29 May 2017 10:02:03 +0200
Subject: [Rd] stats::line() does not produce correct Tukey line when n
 mod 6 is 2 or 3
In-Reply-To: <CAN7_cLjbz6S8u79jtx52XgfD1LcpO8CePYUhrunYqrzXHMMN8A@mail.gmail.com>
References: <CAN7_cLjefc605D1ATS__MOV4gN7fKDBo5Nx_CNB_B9cDbAxc6w@mail.gmail.com>
 <ffc3e473-622d-796f-a06a-6e1f92f8ab5f@gmail.com>
 <CAN7_cLjmQFtZf9z-qJKdWRCaRY2uxfP8k5=QJsptz3U+TtP8oA@mail.gmail.com>
 <CAN7_cLjbz6S8u79jtx52XgfD1LcpO8CePYUhrunYqrzXHMMN8A@mail.gmail.com>
Message-ID: <024A7E7D-45F8-4C95-A9BD-DBEB1D085BFA@gmail.com>

A usually trustworthy R correspondent posted a pure R implementation on SO at some point in his lost youth:

https://stackoverflow.com/questions/3224731/john-tukey-median-median-or-resistant-line-statistical-test-for-r-and-line

This one does indeed generate the line of identity for the (1:9, 1:9) case, so I do suspect that we have a genuine scr*wup in line().

Notice, incidentally, that

> line(1:9+rnorm(9,,1e-1),1:9+rnorm(9,,1e-1))

Call:
line(1:9 + rnorm(9, , 0.1), 1:9 + rnorm(9, , 0.1))

Coefficients:
[1]  -0.9407   1.1948

I.e., it is not likely an issue with exact integers or perfect fit.

-pd



> On 29 May 2017, at 07:21 , GlenB <glnbrntt at gmail.com> wrote:
> 
>> Tukey divides the points into three groups, not the x and y values
> separately.
> 
>> I'll try to get hold of the book for a direct quote, might take a couple
> of days.
> 
> Ah well, I can't get it for a week. But the fact that it's often called
> Tukey's three group line (try a search on *tukey three group line* and
> you'll get plenty of hits) is pretty much a giveaway.
> 
> 
> On Mon, May 29, 2017 at 2:19 PM, GlenB <glnbrntt at gmail.com> wrote:
> 
>> Tukey divides the points into three groups, not the x and y values
>> separately.
>> 
>> I'll try to get hold of the book for a direct quote, might take a couple
>> of days.
>> 
>> 
>> 
>> On Mon, May 29, 2017 at 8:40 AM, Duncan Murdoch <murdoch.duncan at gmail.com>
>> wrote:
>> 
>>> On 27/05/2017 9:28 PM, GlenB wrote:
>>> 
>>>> Bug: stats::line() does not produce correct Tukey line when n mod 6 is 2
>>>> or
>>>> 3
>>>> 
>>>> Example: line(1:9,1:9) should have intercept 0 and slope 1 but it gives
>>>> intercept -1 and slope 1.2
>>>> 
>>>> Trying line(1:i,1:i) across a range of i makes it clear there's a cycle
>>>> of
>>>> length 6, with four of every six correct.
>>>> 
>>>> Bug has been present across many versions.
>>>> 
>>>> The machine I just tried it on just now has R3.2.3:
>>>> 
>>> 
>>> If you look at the source (in src/library/stats/src/line.c), the
>>> explanation is clear:  the x value is chosen as the 1/6 quantile (according
>>> to a particular definition of quantile), and the y value is chosen as the
>>> median of the y values where x is less than or equal to the 1/3 quantile.
>>> Those are different definitions (though I think they would be
>>> asymptotically equivalent under pretty weak assumptions), so it's not
>>> surprising the x value doesn't correspond perfectly to the y value, and the
>>> line ends up "wrong".
>>> 
>>> So is it a bug?  Well, that depends on Tukey's definition.  I don't have
>>> a copy of his book handy so I can't really say.  Maybe the R function is
>>> doing exactly what Tukey said it should, and that's not a bug.  Or maybe R
>>> is wrong.
>>> 
>>> Duncan Murdoch
>>> 
>>> 
>> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From sokol at insa-toulouse.fr  Mon May 29 12:21:25 2017
From: sokol at insa-toulouse.fr (Serguei Sokol)
Date: Mon, 29 May 2017 12:21:25 +0200
Subject: [Rd] stats::line() does not produce correct Tukey line when n
 mod 6 is 2 or 3
In-Reply-To: <024A7E7D-45F8-4C95-A9BD-DBEB1D085BFA@gmail.com>
References: <CAN7_cLjefc605D1ATS__MOV4gN7fKDBo5Nx_CNB_B9cDbAxc6w@mail.gmail.com>
 <ffc3e473-622d-796f-a06a-6e1f92f8ab5f@gmail.com>
 <CAN7_cLjmQFtZf9z-qJKdWRCaRY2uxfP8k5=QJsptz3U+TtP8oA@mail.gmail.com>
 <CAN7_cLjbz6S8u79jtx52XgfD1LcpO8CePYUhrunYqrzXHMMN8A@mail.gmail.com>
 <024A7E7D-45F8-4C95-A9BD-DBEB1D085BFA@gmail.com>
Message-ID: <94b65a5b-da9e-ec5c-e428-7c5dbf175ec1@insa-toulouse.fr>

The problem or actual R implementation relies on an assumption
that median(x[i] | x[i] <= quantile(x, 1/3)) == quantile(x, 1/6)
which reveals not to be true despite very trustful appearance.

If we continue with the example of x=y=1:9
then quantile(x, 1/6)=2.5 (here quantile() is taken in C-code sens, not R's one)
while median(y[i] | x[i] <= quantile(x, 1/3))=2
On the other sample's side we've got 7.5 and 8 for x and y respectively.
Hence the slope (8-2)/(7.5-2.5)=1.2

To get a correct version of this, one should calculate x robust points in the same way as the y's,
i.e. xb=median(x[i] | x[i] <= quantile(x, 1/3)) and xt=median(x[i] | x[i] >= quantile(x, 2/3))

Best,
Serguei.

Le 29/05/2017 ? 10:02, peter dalgaard a ?crit :
> A usually trustworthy R correspondent posted a pure R implementation on SO at some point in his lost youth:
>
> https://stackoverflow.com/questions/3224731/john-tukey-median-median-or-resistant-line-statistical-test-for-r-and-line
>
> This one does indeed generate the line of identity for the (1:9, 1:9) case, so I do suspect that we have a genuine scr*wup in line().
>
> Notice, incidentally, that
>
>> line(1:9+rnorm(9,,1e-1),1:9+rnorm(9,,1e-1))
> Call:
> line(1:9 + rnorm(9, , 0.1), 1:9 + rnorm(9, , 0.1))
>
> Coefficients:
> [1]  -0.9407   1.1948
>
> I.e., it is not likely an issue with exact integers or perfect fit.
>
> -pd
>
>
>
>> On 29 May 2017, at 07:21 , GlenB <glnbrntt at gmail.com> wrote:
>>
>>> Tukey divides the points into three groups, not the x and y values
>> separately.
>>
>>> I'll try to get hold of the book for a direct quote, might take a couple
>> of days.
>>
>> Ah well, I can't get it for a week. But the fact that it's often called
>> Tukey's three group line (try a search on *tukey three group line* and
>> you'll get plenty of hits) is pretty much a giveaway.
>>
>>
>> On Mon, May 29, 2017 at 2:19 PM, GlenB <glnbrntt at gmail.com> wrote:
>>
>>> Tukey divides the points into three groups, not the x and y values
>>> separately.
>>>
>>> I'll try to get hold of the book for a direct quote, might take a couple
>>> of days.
>>>
>>>
>>>
>>> On Mon, May 29, 2017 at 8:40 AM, Duncan Murdoch <murdoch.duncan at gmail.com>
>>> wrote:
>>>
>>>> On 27/05/2017 9:28 PM, GlenB wrote:
>>>>
>>>>> Bug: stats::line() does not produce correct Tukey line when n mod 6 is 2
>>>>> or
>>>>> 3
>>>>>
>>>>> Example: line(1:9,1:9) should have intercept 0 and slope 1 but it gives
>>>>> intercept -1 and slope 1.2
>>>>>
>>>>> Trying line(1:i,1:i) across a range of i makes it clear there's a cycle
>>>>> of
>>>>> length 6, with four of every six correct.
>>>>>
>>>>> Bug has been present across many versions.
>>>>>
>>>>> The machine I just tried it on just now has R3.2.3:
>>>>>
>>>> If you look at the source (in src/library/stats/src/line.c), the
>>>> explanation is clear:  the x value is chosen as the 1/6 quantile (according
>>>> to a particular definition of quantile), and the y value is chosen as the
>>>> median of the y values where x is less than or equal to the 1/3 quantile.
>>>> Those are different definitions (though I think they would be
>>>> asymptotically equivalent under pretty weak assumptions), so it's not
>>>> surprising the x value doesn't correspond perfectly to the y value, and the
>>>> line ends up "wrong".
>>>>
>>>> So is it a bug?  Well, that depends on Tukey's definition.  I don't have
>>>> a copy of his book handy so I can't really say.  Maybe the R function is
>>>> doing exactly what Tukey said it should, and that's not a bug.  Or maybe R
>>>> is wrong.
>>>>
>>>> Duncan Murdoch
>>>>
>>>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel


-- 
Serguei Sokol
Ingenieur de recherche INRA
Metabolisme Integre et Dynamique des Systemes Metaboliques (MetaSys)

LISBP, INSA/INRA UMR 792, INSA/CNRS UMR 5504
135 Avenue de Rangueil
31077 Toulouse Cedex 04

tel: +33 5 6155 9276
fax: +33 5 6704 8825
email: sokol at insa-toulouse.fr
http://metasys.insa-toulouse.fr
http://www.lisbp.fr


From tomas.kalibera at gmail.com  Mon May 29 15:09:19 2017
From: tomas.kalibera at gmail.com (Tomas Kalibera)
Date: Mon, 29 May 2017 15:09:19 +0200
Subject: [Rd] Interpreting R memory profiling statistics from Rprof()
 and gc()
In-Reply-To: <CAPkgxdv+ZkPgyuO73QavJKzEC+Jaen-rmkaSsyk=pAkFLJeaiQ@mail.gmail.com>
References: <CAPkgxdv+ZkPgyuO73QavJKzEC+Jaen-rmkaSsyk=pAkFLJeaiQ@mail.gmail.com>
Message-ID: <9e22e1c5-ab3f-5a84-abc8-c87b70f5c358@gmail.com>

On 05/18/2017 06:54 PM, Joy wrote:
> Sorry, this might be a really basic question, but I'm trying to interpret
> the results from memory profiling, and I have a few questions (marked by
> *Q#*).
>
>  From the summaryRprof() documentation, it seems that the four columns of
> statistics that are reported when setting memory.profiling=TRUE are
> - vector memory in small blocks on the R heap
> - vector memory in large blocks (from malloc)
> - memory in nodes on the R heap
> - number of calls to the internal function duplicate in the time interval
> (*Q1:* Are the units of the first 3 stats in bytes?)
In Rprof.out, vector memory in small and large blocks is given in 8-byte 
units (for historical reasons), but memory in nodes is given in bytes - 
this is not documented/guaranteed in documentation. In 
summaryRprof(memory="both"), memory usage is given in megabytes as 
documented.
For summaryRprof(memory="stats" and memory="tseries") I clarified in 
r72743, now memory usage is in bytes and it is documented.
>
> and from the gc() documentation, the two rows represent
> - ?"Ncells"? (_cons cells_), usually 28 bytes each on 32-bit systems and 56
> bytes on 64-bit systems,
> - ?"Vcells"? (_vector cells_, 8 bytes each)
> (*Q2:* how are Ncells and Vcells related to small heap/large heap/memory in
> nodes?)
Ncells describe memory in nodes (Ncells is the number of nodes).

Vcells describe memory in "small heap" + "large heap". A Vcell today 
does not have much meaning, it is shown for historical reasons, but the 
interesting thing is that Vcells*56 (or 28 on 32-bit systems) gives the 
number of bytes in "small heap"+"large heap" objects.

> And I guess the question that lead to these other questions is - *Q3:* I'd
> like to plot out the total amount of memory used over time, and I don't
> think Rprofmem() give me what I'd like to know because, as I'm
> understanding it, Rprofmem() records the amount of memory allocated with
> each call, but this doesn't tell me the total amount of memory R is using,
> or am I mistaken?
Rprof controls a sampling profiler which regularly asks the GC how much 
memory is currently in use on the R heap (but beware, indeed some of 
that memory is no longer reachable but has not yet been collected - 
running gc more frequently helps, and some of the memory may still be 
reachable but will not be used anymore). You can get this data by 
summaryRprof(memory="tseries") and plot them - add columns 1+2 or 1+2+3 
depending on what you want, in 72743 or more recent, in older version 
you need to multiply columns 1 and 2 by 8. To run the GC more frequently 
you can use gctorture.

Or if you are happy modifying your own R code and you don't insist on 
querying the memory size very frequently, you can also explicitly call 
gc(verbose=T) repeatedly. For this you won't need to use the profiler.

If you were looking instead at how much memory the whole R instance was 
using (that is, including memory allocated by the R gc but not presently 
used for R objects, including memory outside R heap), the easiest way 
would be to use facilities of your OS.

Rprofmem is a different thing and won't help you.

Best
Tomas

>
> Thanks in advance!
>
> Joy
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From sokol at insa-toulouse.fr  Mon May 29 15:13:46 2017
From: sokol at insa-toulouse.fr (Serguei Sokol)
Date: Mon, 29 May 2017 15:13:46 +0200
Subject: [Rd] stats::line() does not produce correct Tukey line when n
 mod 6 is 2 or 3
In-Reply-To: <94b65a5b-da9e-ec5c-e428-7c5dbf175ec1@insa-toulouse.fr>
References: <CAN7_cLjefc605D1ATS__MOV4gN7fKDBo5Nx_CNB_B9cDbAxc6w@mail.gmail.com>
 <ffc3e473-622d-796f-a06a-6e1f92f8ab5f@gmail.com>
 <CAN7_cLjmQFtZf9z-qJKdWRCaRY2uxfP8k5=QJsptz3U+TtP8oA@mail.gmail.com>
 <CAN7_cLjbz6S8u79jtx52XgfD1LcpO8CePYUhrunYqrzXHMMN8A@mail.gmail.com>
 <024A7E7D-45F8-4C95-A9BD-DBEB1D085BFA@gmail.com>
 <94b65a5b-da9e-ec5c-e428-7c5dbf175ec1@insa-toulouse.fr>
Message-ID: <8c167bbb-130d-3798-a9e8-24c4377db313@insa-toulouse.fr>

Here is an attached patch.

Best,
Serguei.

Le 29/05/2017 ? 12:21, Serguei Sokol a ?crit :
> The problem or actual R implementation relies on an assumption
> that median(x[i] | x[i] <= quantile(x, 1/3)) == quantile(x, 1/6)
> which reveals not to be true despite very trustful appearance.
>
> If we continue with the example of x=y=1:9
> then quantile(x, 1/6)=2.5 (here quantile() is taken in C-code sens, not R's one)
> while median(y[i] | x[i] <= quantile(x, 1/3))=2
> On the other sample's side we've got 7.5 and 8 for x and y respectively.
> Hence the slope (8-2)/(7.5-2.5)=1.2
>
> To get a correct version of this, one should calculate x robust points in the same way as the y's,
> i.e. xb=median(x[i] | x[i] <= quantile(x, 1/3)) and xt=median(x[i] | x[i] >= quantile(x, 2/3))
>
> Best,
> Serguei.
>
> Le 29/05/2017 ? 10:02, peter dalgaard a ?crit :
>> A usually trustworthy R correspondent posted a pure R implementation on SO at some point in his lost youth:
>>
>> https://stackoverflow.com/questions/3224731/john-tukey-median-median-or-resistant-line-statistical-test-for-r-and-line
>>
>> This one does indeed generate the line of identity for the (1:9, 1:9) case, so I do suspect that we have a genuine scr*wup in line().
>>
>> Notice, incidentally, that
>>
>>> line(1:9+rnorm(9,,1e-1),1:9+rnorm(9,,1e-1))
>> Call:
>> line(1:9 + rnorm(9, , 0.1), 1:9 + rnorm(9, , 0.1))
>>
>> Coefficients:
>> [1]  -0.9407   1.1948
>>
>> I.e., it is not likely an issue with exact integers or perfect fit.
>>
>> -pd
>>
>>
>>
>>> On 29 May 2017, at 07:21 , GlenB <glnbrntt at gmail.com> wrote:
>>>
>>>> Tukey divides the points into three groups, not the x and y values
>>> separately.
>>>
>>>> I'll try to get hold of the book for a direct quote, might take a couple
>>> of days.
>>>
>>> Ah well, I can't get it for a week. But the fact that it's often called
>>> Tukey's three group line (try a search on *tukey three group line* and
>>> you'll get plenty of hits) is pretty much a giveaway.
>>>
>>>
>>> On Mon, May 29, 2017 at 2:19 PM, GlenB <glnbrntt at gmail.com> wrote:
>>>
>>>> Tukey divides the points into three groups, not the x and y values
>>>> separately.
>>>>
>>>> I'll try to get hold of the book for a direct quote, might take a couple
>>>> of days.
>>>>
>>>>
>>>>
>>>> On Mon, May 29, 2017 at 8:40 AM, Duncan Murdoch <murdoch.duncan at gmail.com>
>>>> wrote:
>>>>
>>>>> On 27/05/2017 9:28 PM, GlenB wrote:
>>>>>
>>>>>> Bug: stats::line() does not produce correct Tukey line when n mod 6 is 2
>>>>>> or
>>>>>> 3
>>>>>>
>>>>>> Example: line(1:9,1:9) should have intercept 0 and slope 1 but it gives
>>>>>> intercept -1 and slope 1.2
>>>>>>
>>>>>> Trying line(1:i,1:i) across a range of i makes it clear there's a cycle
>>>>>> of
>>>>>> length 6, with four of every six correct.
>>>>>>
>>>>>> Bug has been present across many versions.
>>>>>>
>>>>>> The machine I just tried it on just now has R3.2.3:
>>>>>>
>>>>> If you look at the source (in src/library/stats/src/line.c), the
>>>>> explanation is clear:  the x value is chosen as the 1/6 quantile (according
>>>>> to a particular definition of quantile), and the y value is chosen as the
>>>>> median of the y values where x is less than or equal to the 1/3 quantile.
>>>>> Those are different definitions (though I think they would be
>>>>> asymptotically equivalent under pretty weak assumptions), so it's not
>>>>> surprising the x value doesn't correspond perfectly to the y value, and the
>>>>> line ends up "wrong".
>>>>>
>>>>> So is it a bug?  Well, that depends on Tukey's definition.  I don't have
>>>>> a copy of his book handy so I can't really say.  Maybe the R function is
>>>>> doing exactly what Tukey said it should, and that's not a bug.  Or maybe R
>>>>> is wrong.
>>>>>
>>>>> Duncan Murdoch
>>>>>
>>>>>
>>>     [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>
>

-- 
Serguei Sokol
Ingenieur de recherche INRA
Metabolisme Integre et Dynamique des Systemes Metaboliques (MetaSys)

LISBP, INSA/INRA UMR 792, INSA/CNRS UMR 5504
135 Avenue de Rangueil
31077 Toulouse Cedex 04

tel: +33 5 6155 9276
fax: +33 5 6704 8825
email: sokol at insa-toulouse.fr
http://metasys.insa-toulouse.fr
http://www.lisbp.fr

-------------- next part --------------
A non-text attachment was scrubbed...
Name: line.c.patch
Type: text/x-patch
Size: 2651 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20170529/1f1f2e6c/attachment.bin>

From sokol at insa-toulouse.fr  Mon May 29 15:28:12 2017
From: sokol at insa-toulouse.fr (Serguei Sokol)
Date: Mon, 29 May 2017 15:28:12 +0200
Subject: [Rd] stats::line() does not produce correct Tukey line when n
 mod 6 is 2 or 3
In-Reply-To: <8c167bbb-130d-3798-a9e8-24c4377db313@insa-toulouse.fr>
References: <CAN7_cLjefc605D1ATS__MOV4gN7fKDBo5Nx_CNB_B9cDbAxc6w@mail.gmail.com>
 <ffc3e473-622d-796f-a06a-6e1f92f8ab5f@gmail.com>
 <CAN7_cLjmQFtZf9z-qJKdWRCaRY2uxfP8k5=QJsptz3U+TtP8oA@mail.gmail.com>
 <CAN7_cLjbz6S8u79jtx52XgfD1LcpO8CePYUhrunYqrzXHMMN8A@mail.gmail.com>
 <024A7E7D-45F8-4C95-A9BD-DBEB1D085BFA@gmail.com>
 <94b65a5b-da9e-ec5c-e428-7c5dbf175ec1@insa-toulouse.fr>
 <8c167bbb-130d-3798-a9e8-24c4377db313@insa-toulouse.fr>
Message-ID: <d10ebb8c-0c49-5b0e-5b3c-b06073bfe447@insa-toulouse.fr>

Sorry, I have seen it too late that we had different tab width in the original file and my editor.
Here is the patch with all white spaces instead of mixing tabs and white spaces.

Serguei.

Le 29/05/2017 ? 15:13, Serguei Sokol a ?crit :
> Here is an attached patch.
>
> Best,
> Serguei.
>
> Le 29/05/2017 ? 12:21, Serguei Sokol a ?crit :
>> The problem or actual R implementation relies on an assumption
>> that median(x[i] | x[i] <= quantile(x, 1/3)) == quantile(x, 1/6)
>> which reveals not to be true despite very trustful appearance.
>>
>> If we continue with the example of x=y=1:9
>> then quantile(x, 1/6)=2.5 (here quantile() is taken in C-code sens, not R's one)
>> while median(y[i] | x[i] <= quantile(x, 1/3))=2
>> On the other sample's side we've got 7.5 and 8 for x and y respectively.
>> Hence the slope (8-2)/(7.5-2.5)=1.2
>>
>> To get a correct version of this, one should calculate x robust points in the same way as the y's,
>> i.e. xb=median(x[i] | x[i] <= quantile(x, 1/3)) and xt=median(x[i] | x[i] >= quantile(x, 2/3))
>>
>> Best,
>> Serguei.
>>
>> Le 29/05/2017 ? 10:02, peter dalgaard a ?crit :
>>> A usually trustworthy R correspondent posted a pure R implementation on SO at some point in his lost youth:
>>>
>>> https://stackoverflow.com/questions/3224731/john-tukey-median-median-or-resistant-line-statistical-test-for-r-and-line
>>>
>>> This one does indeed generate the line of identity for the (1:9, 1:9) case, so I do suspect that we have a genuine scr*wup in line().
>>>
>>> Notice, incidentally, that
>>>
>>>> line(1:9+rnorm(9,,1e-1),1:9+rnorm(9,,1e-1))
>>> Call:
>>> line(1:9 + rnorm(9, , 0.1), 1:9 + rnorm(9, , 0.1))
>>>
>>> Coefficients:
>>> [1]  -0.9407   1.1948
>>>
>>> I.e., it is not likely an issue with exact integers or perfect fit.
>>>
>>> -pd
>>>
>>>
>>>
>>>> On 29 May 2017, at 07:21 , GlenB <glnbrntt at gmail.com> wrote:
>>>>
>>>>> Tukey divides the points into three groups, not the x and y values
>>>> separately.
>>>>
>>>>> I'll try to get hold of the book for a direct quote, might take a couple
>>>> of days.
>>>>
>>>> Ah well, I can't get it for a week. But the fact that it's often called
>>>> Tukey's three group line (try a search on *tukey three group line* and
>>>> you'll get plenty of hits) is pretty much a giveaway.
>>>>
>>>>
>>>> On Mon, May 29, 2017 at 2:19 PM, GlenB <glnbrntt at gmail.com> wrote:
>>>>
>>>>> Tukey divides the points into three groups, not the x and y values
>>>>> separately.
>>>>>
>>>>> I'll try to get hold of the book for a direct quote, might take a couple
>>>>> of days.
>>>>>
>>>>>
>>>>>
>>>>> On Mon, May 29, 2017 at 8:40 AM, Duncan Murdoch <murdoch.duncan at gmail.com>
>>>>> wrote:
>>>>>
>>>>>> On 27/05/2017 9:28 PM, GlenB wrote:
>>>>>>
>>>>>>> Bug: stats::line() does not produce correct Tukey line when n mod 6 is 2
>>>>>>> or
>>>>>>> 3
>>>>>>>
>>>>>>> Example: line(1:9,1:9) should have intercept 0 and slope 1 but it gives
>>>>>>> intercept -1 and slope 1.2
>>>>>>>
>>>>>>> Trying line(1:i,1:i) across a range of i makes it clear there's a cycle
>>>>>>> of
>>>>>>> length 6, with four of every six correct.
>>>>>>>
>>>>>>> Bug has been present across many versions.
>>>>>>>
>>>>>>> The machine I just tried it on just now has R3.2.3:
>>>>>>>
>>>>>> If you look at the source (in src/library/stats/src/line.c), the
>>>>>> explanation is clear:  the x value is chosen as the 1/6 quantile (according
>>>>>> to a particular definition of quantile), and the y value is chosen as the
>>>>>> median of the y values where x is less than or equal to the 1/3 quantile.
>>>>>> Those are different definitions (though I think they would be
>>>>>> asymptotically equivalent under pretty weak assumptions), so it's not
>>>>>> surprising the x value doesn't correspond perfectly to the y value, and the
>>>>>> line ends up "wrong".
>>>>>>
>>>>>> So is it a bug?  Well, that depends on Tukey's definition.  I don't have
>>>>>> a copy of his book handy so I can't really say.  Maybe the R function is
>>>>>> doing exactly what Tukey said it should, and that's not a bug.  Or maybe R
>>>>>> is wrong.
>>>>>>
>>>>>> Duncan Murdoch
>>>>>>
>>>>>>
>>>>     [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-devel at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>>
>
>
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


-- 
Serguei Sokol
Ingenieur de recherche INRA
Metabolisme Integre et Dynamique des Systemes Metaboliques (MetaSys)

LISBP, INSA/INRA UMR 792, INSA/CNRS UMR 5504
135 Avenue de Rangueil
31077 Toulouse Cedex 04

tel: +33 5 6155 9276
fax: +33 5 6704 8825
email: sokol at insa-toulouse.fr
http://metasys.insa-toulouse.fr
http://www.lisbp.fr

-------------- next part --------------
A non-text attachment was scrubbed...
Name: line.c.patch
Type: text/x-patch
Size: 3680 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20170529/5c29b34e/attachment.bin>

From glnbrntt at gmail.com  Mon May 29 15:27:48 2017
From: glnbrntt at gmail.com (GlenB)
Date: Mon, 29 May 2017 23:27:48 +1000
Subject: [Rd] stats::line() does not produce correct Tukey line when n
 mod 6 is 2 or 3
In-Reply-To: <8c167bbb-130d-3798-a9e8-24c4377db313@insa-toulouse.fr>
References: <CAN7_cLjefc605D1ATS__MOV4gN7fKDBo5Nx_CNB_B9cDbAxc6w@mail.gmail.com>
 <ffc3e473-622d-796f-a06a-6e1f92f8ab5f@gmail.com>
 <CAN7_cLjmQFtZf9z-qJKdWRCaRY2uxfP8k5=QJsptz3U+TtP8oA@mail.gmail.com>
 <CAN7_cLjbz6S8u79jtx52XgfD1LcpO8CePYUhrunYqrzXHMMN8A@mail.gmail.com>
 <024A7E7D-45F8-4C95-A9BD-DBEB1D085BFA@gmail.com>
 <94b65a5b-da9e-ec5c-e428-7c5dbf175ec1@insa-toulouse.fr>
 <8c167bbb-130d-3798-a9e8-24c4377db313@insa-toulouse.fr>
Message-ID: <CAN7_cLha-bJH1kAG4ANhnSG-Jv39gp4FA-j9mZXRBspXjtUSKg@mail.gmail.com>

Incidentally (though I don't expect anyone will want to pursue it)
Johnstone & Velleman give standard errors for the estimates in Johnstone,
Iain M., and Paul F. Velleman. ?The Resistant Line and Related Regression
Methods.? Journal of the American Statistical Association, vol. 80, no.
392, 1985, pp. 1041?1054.

On Mon, May 29, 2017 at 11:13 PM, Serguei Sokol <sokol at insa-toulouse.fr>
wrote:

> Here is an attached patch.
>
> Best,
> Serguei.
>
>
> Le 29/05/2017 ? 12:21, Serguei Sokol a ?crit :
>
>> The problem or actual R implementation relies on an assumption
>> that median(x[i] | x[i] <= quantile(x, 1/3)) == quantile(x, 1/6)
>> which reveals not to be true despite very trustful appearance.
>>
>> If we continue with the example of x=y=1:9
>> then quantile(x, 1/6)=2.5 (here quantile() is taken in C-code sens, not
>> R's one)
>> while median(y[i] | x[i] <= quantile(x, 1/3))=2
>> On the other sample's side we've got 7.5 and 8 for x and y respectively.
>> Hence the slope (8-2)/(7.5-2.5)=1.2
>>
>> To get a correct version of this, one should calculate x robust points in
>> the same way as the y's,
>> i.e. xb=median(x[i] | x[i] <= quantile(x, 1/3)) and xt=median(x[i] | x[i]
>> >= quantile(x, 2/3))
>>
>> Best,
>> Serguei.
>>
>> Le 29/05/2017 ? 10:02, peter dalgaard a ?crit :
>>
>>> A usually trustworthy R correspondent posted a pure R implementation on
>>> SO at some point in his lost youth:
>>>
>>> https://stackoverflow.com/questions/3224731/john-tukey-media
>>> n-median-or-resistant-line-statistical-test-for-r-and-line
>>>
>>> This one does indeed generate the line of identity for the (1:9, 1:9)
>>> case, so I do suspect that we have a genuine scr*wup in line().
>>>
>>> Notice, incidentally, that
>>>
>>> line(1:9+rnorm(9,,1e-1),1:9+rnorm(9,,1e-1))
>>>>
>>> Call:
>>> line(1:9 + rnorm(9, , 0.1), 1:9 + rnorm(9, , 0.1))
>>>
>>> Coefficients:
>>> [1]  -0.9407   1.1948
>>>
>>> I.e., it is not likely an issue with exact integers or perfect fit.
>>>
>>> -pd
>>>
>>>
>>>
>>> On 29 May 2017, at 07:21 , GlenB <glnbrntt at gmail.com> wrote:
>>>>
>>>> Tukey divides the points into three groups, not the x and y values
>>>>>
>>>> separately.
>>>>
>>>> I'll try to get hold of the book for a direct quote, might take a couple
>>>>>
>>>> of days.
>>>>
>>>> Ah well, I can't get it for a week. But the fact that it's often called
>>>> Tukey's three group line (try a search on *tukey three group line* and
>>>> you'll get plenty of hits) is pretty much a giveaway.
>>>>
>>>>
>>>> On Mon, May 29, 2017 at 2:19 PM, GlenB <glnbrntt at gmail.com> wrote:
>>>>
>>>> Tukey divides the points into three groups, not the x and y values
>>>>> separately.
>>>>>
>>>>> I'll try to get hold of the book for a direct quote, might take a
>>>>> couple
>>>>> of days.
>>>>>
>>>>>
>>>>>
>>>>> On Mon, May 29, 2017 at 8:40 AM, Duncan Murdoch <
>>>>> murdoch.duncan at gmail.com>
>>>>> wrote:
>>>>>
>>>>> On 27/05/2017 9:28 PM, GlenB wrote:
>>>>>>
>>>>>> Bug: stats::line() does not produce correct Tukey line when n mod 6
>>>>>>> is 2
>>>>>>> or
>>>>>>> 3
>>>>>>>
>>>>>>> Example: line(1:9,1:9) should have intercept 0 and slope 1 but it
>>>>>>> gives
>>>>>>> intercept -1 and slope 1.2
>>>>>>>
>>>>>>> Trying line(1:i,1:i) across a range of i makes it clear there's a
>>>>>>> cycle
>>>>>>> of
>>>>>>> length 6, with four of every six correct.
>>>>>>>
>>>>>>> Bug has been present across many versions.
>>>>>>>
>>>>>>> The machine I just tried it on just now has R3.2.3:
>>>>>>>
>>>>>>> If you look at the source (in src/library/stats/src/line.c), the
>>>>>> explanation is clear:  the x value is chosen as the 1/6 quantile
>>>>>> (according
>>>>>> to a particular definition of quantile), and the y value is chosen as
>>>>>> the
>>>>>> median of the y values where x is less than or equal to the 1/3
>>>>>> quantile.
>>>>>> Those are different definitions (though I think they would be
>>>>>> asymptotically equivalent under pretty weak assumptions), so it's not
>>>>>> surprising the x value doesn't correspond perfectly to the y value,
>>>>>> and the
>>>>>> line ends up "wrong".
>>>>>>
>>>>>> So is it a bug?  Well, that depends on Tukey's definition.  I don't
>>>>>> have
>>>>>> a copy of his book handy so I can't really say.  Maybe the R function
>>>>>> is
>>>>>> doing exactly what Tukey said it should, and that's not a bug.  Or
>>>>>> maybe R
>>>>>> is wrong.
>>>>>>
>>>>>> Duncan Murdoch
>>>>>>
>>>>>>
>>>>>>     [[alternative HTML version deleted]]
>>>>
>>>> ______________________________________________
>>>> R-devel at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>
>>>
>>
>>
> --
> Serguei Sokol
> Ingenieur de recherche INRA
> Metabolisme Integre et Dynamique des Systemes Metaboliques (MetaSys)
>
> LISBP, INSA/INRA UMR 792, INSA/CNRS UMR 5504
> 135 Avenue de Rangueil
> 31077 Toulouse Cedex 04
>
> tel: +33 5 6155 9276
> fax: +33 5 6704 8825
> email: sokol at insa-toulouse.fr
> http://metasys.insa-toulouse.fr
> http://www.lisbp.fr
>
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From maechler at stat.math.ethz.ch  Tue May 30 09:33:10 2017
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 30 May 2017 09:33:10 +0200
Subject: [Rd] stats::line() does not produce correct Tukey line when n
	mod 6 is 2 or 3
In-Reply-To: <d10ebb8c-0c49-5b0e-5b3c-b06073bfe447@insa-toulouse.fr>
References: <CAN7_cLjefc605D1ATS__MOV4gN7fKDBo5Nx_CNB_B9cDbAxc6w@mail.gmail.com>
 <ffc3e473-622d-796f-a06a-6e1f92f8ab5f@gmail.com>
 <CAN7_cLjmQFtZf9z-qJKdWRCaRY2uxfP8k5=QJsptz3U+TtP8oA@mail.gmail.com>
 <CAN7_cLjbz6S8u79jtx52XgfD1LcpO8CePYUhrunYqrzXHMMN8A@mail.gmail.com>
 <024A7E7D-45F8-4C95-A9BD-DBEB1D085BFA@gmail.com>
 <94b65a5b-da9e-ec5c-e428-7c5dbf175ec1@insa-toulouse.fr>
 <8c167bbb-130d-3798-a9e8-24c4377db313@insa-toulouse.fr>
 <d10ebb8c-0c49-5b0e-5b3c-b06073bfe447@insa-toulouse.fr>
Message-ID: <22829.8246.695545.320438@stat.math.ethz.ch>

>>>>> Serguei Sokol <sokol at insa-toulouse.fr>
>>>>>     on Mon, 29 May 2017 15:28:12 +0200 writes:

    > Sorry, I have seen it too late that we had different tab
    > width in the original file and my editor.  Here is the
    > patch with all white spaces instead of mixing tabs and
    > white spaces.

thank you - it still gives quite a few unnecessary (i.e. "white space")
 diffs with the source code, but that's not the problem :

The patch leads to correct results for the simple
 (1:k, 1:k)  data sets (for all k).

However, even after the patch,
The example from the SO post differs from the result of Richie
Cotton's function...(even though that function had a silly bug
	 in step 1, the bug has not been "kicking" for the example):

Here's a fixed-up version of the pure R function and
the example and some comments :


## From Stackoverflow
## http://stackoverflow.com/questions/3224731/john-tukey-median-median-or-resistant-line-statistical-test-for-r-and-line
## median_median_line by Richie Cotton (July 12 2010, last edited at 13:49)
##
## Shorter variable names, fixed bug in step 1, added 'plot.' option: Martin Maechler, May 2017
MMline <- function(x, y, data, plot. = FALSE)
{
  if(!missing(data))
  {
    x <- eval(substitute(x), data)
    y <- eval(substitute(y), data)
  }
  stopifnot((n <- length(x)) == length(y), n >= 2)

  ## Step 1
  n.3 <- n %/% 3L
  groups <- rep(1:3, times = switch((n %% 3) + 1L,
     n.3,
     c(n.3, n.3 + 1L, n.3),
     c(n.3 + 1L, n.3, n.3 + 1L)
  ))
  groups <- lapply(list(groups), as.factor) # (gain a bit in tapply())

  ## Step 2
  ## sort *both*  x & y "along x":
  x <- sort.int(x, index.return = TRUE)
  y <- y[x$ix]
  x <- x$x
  if(plot.) plot(x,y)

  ## Step 3
  m_x <- tapply(x, groups, median)
  m_y <- tapply(y, groups, median)
  if(plot.) {
      points(m_x, m_y, cex=2, pch=3, col="red")
      segments(m_x[1],m_y[1], m_x[3],m_y[3], col="red")
  }
  ## Step 4
  R <- if(n == 2) 2L else 3L
  slope <- (m_y[[R]] - m_y[[1]]) / (m_x[[R]] - m_x[[1]])
  intercept <- m_y[[1]] - slope * m_x[[1]]

  ## Step 5
  mid_y <- intercept + slope * m_x[[2]]
  intercept <- intercept + (m_y[[2]] - mid_y) / 3
  if(plot.) abline(a = intercept, b = slope, col=adjustcolor("midnight blue", .5), lwd=2)
  c(intercept = intercept, slope = slope)
}

## To test it, here's the second example from that page:

dfr <- data.frame(
      time = c( .16,  .24,  .25,  .30,  .30,    .32,  .36,  .36,  .50,  .50,
                .57,  .61,  .61,  .68,  .72,    .72,  .83,  .88,  .89),
  distance = c(12.1, 29.8, 32.7, 42.8, 44.2,   55.8, 63.5, 65.1,124.6,129.7,
              150.2,182.2,189.4,220.4,250.4,  261.0,334.5,375.5,399.1))

MMline(time, distance, dfr, plot.=TRUE)
## intercept    slope
##   -113.6     520.0
par(new=TRUE)# should plot identically!
MMline(time, distance, dfr[sample.int(nrow(dfr)), ], plot.=TRUE)

## Note the slightly odd way of specifying the groups. The instructions are
## quite picky about how you define group sizes, so the more obvious method
## of cut(x, quantile(x, seq.int(0, 1, 1/3))) doesn't work.

## edited Jul 12 '10 at 13:49 / answered Jul 12 '10 at 13:36
## Richie Cotton

## And someone remarked  that R's  line() did not give the same:

with(dfr, line(time, distance))
## ...
## Coefficients:
## [1]  -108.8   505.2
abline(-108.8, 505.2, col="blue") ##=> this one is wrong

## MM:
(cfs <- t(sapply(setNames(,2:50), function(k) {x <- 1:k; MMline(x,x)})))
##    intercept slope
## 2          0     1 # (special case fixed)
## 3          0     1
## 4          0     1
## 5          0     1
##   ............
## 49         0     1
## 50         0     1

## "Harder": (sort ing!)
cf2 <- t(sapply(setNames(,2:50),
                function(k) {x <- sample.int(k); MMline(x, -10*x)}))
stopifnot(abs(cf2[,"intercept"] -   0) < 1e-12,
          abs(cf2[,    "slope"] - -10) < 1e-12)


From sokol at insa-toulouse.fr  Tue May 30 16:01:17 2017
From: sokol at insa-toulouse.fr (Serguei Sokol)
Date: Tue, 30 May 2017 16:01:17 +0200
Subject: [Rd] stats::line() does not produce correct Tukey line when n
 mod 6 is 2 or 3
In-Reply-To: <22829.8246.695545.320438@stat.math.ethz.ch>
References: <CAN7_cLjefc605D1ATS__MOV4gN7fKDBo5Nx_CNB_B9cDbAxc6w@mail.gmail.com>
 <ffc3e473-622d-796f-a06a-6e1f92f8ab5f@gmail.com>
 <CAN7_cLjmQFtZf9z-qJKdWRCaRY2uxfP8k5=QJsptz3U+TtP8oA@mail.gmail.com>
 <CAN7_cLjbz6S8u79jtx52XgfD1LcpO8CePYUhrunYqrzXHMMN8A@mail.gmail.com>
 <024A7E7D-45F8-4C95-A9BD-DBEB1D085BFA@gmail.com>
 <94b65a5b-da9e-ec5c-e428-7c5dbf175ec1@insa-toulouse.fr>
 <8c167bbb-130d-3798-a9e8-24c4377db313@insa-toulouse.fr>
 <d10ebb8c-0c49-5b0e-5b3c-b06073bfe447@insa-toulouse.fr>
 <22829.8246.695545.320438@stat.math.ethz.ch>
Message-ID: <396781eb-1712-aa54-96f2-a6c4df0284f4@insa-toulouse.fr>

Le 30/05/2017 ? 09:33, Martin Maechler a ?crit :
...
> However, even after the patch,
> The example from the SO post differs from the result of Richie
> Cotton's function...
The explanation is quite simple. In SO function, the first 1/3 quantile
of used example counts 6 points (of 19 in total), while line()'s
definition of quantile leads to 8 points. The same numbers
(6 and 8) are on the other end of sample. In x sample, there are
few repeated values, this is certainly be the reason of different quantiles..

I am not sure that one quantile definition is better or more correct
than the other. So I would leave line()'s definition as is.

Best,
Sergue?.


From maechler at stat.math.ethz.ch  Tue May 30 18:51:08 2017
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 30 May 2017 18:51:08 +0200
Subject: [Rd] stats::line() does not produce correct Tukey line when n
 mod 6 is 2 or 3
In-Reply-To: <396781eb-1712-aa54-96f2-a6c4df0284f4@insa-toulouse.fr>
References: <CAN7_cLjefc605D1ATS__MOV4gN7fKDBo5Nx_CNB_B9cDbAxc6w@mail.gmail.com>
 <ffc3e473-622d-796f-a06a-6e1f92f8ab5f@gmail.com>
 <CAN7_cLjmQFtZf9z-qJKdWRCaRY2uxfP8k5=QJsptz3U+TtP8oA@mail.gmail.com>
 <CAN7_cLjbz6S8u79jtx52XgfD1LcpO8CePYUhrunYqrzXHMMN8A@mail.gmail.com>
 <024A7E7D-45F8-4C95-A9BD-DBEB1D085BFA@gmail.com>
 <94b65a5b-da9e-ec5c-e428-7c5dbf175ec1@insa-toulouse.fr>
 <8c167bbb-130d-3798-a9e8-24c4377db313@insa-toulouse.fr>
 <d10ebb8c-0c49-5b0e-5b3c-b06073bfe447@insa-toulouse.fr>
 <22829.8246.695545.320438@stat.math.ethz.ch>
 <396781eb-1712-aa54-96f2-a6c4df0284f4@insa-toulouse.fr>
Message-ID: <22829.41724.818300.640880@stat.math.ethz.ch>

>>>>> Serguei Sokol <sokol at insa-toulouse.fr>
>>>>>     on Tue, 30 May 2017 16:01:17 +0200 writes:

    > Le 30/05/2017 ? 09:33, Martin Maechler a ?crit : ...
    >> However, even after the patch, The example from the SO
    >> post differs from the result of Richie Cotton's
    >> function...
    > The explanation is quite simple. In SO function, the first
    > 1/3 quantile of used example counts 6 points (of 19 in
    > total), while line()'s definition of quantile leads to 8
    > points. The same numbers (6 and 8) are on the other end of
    > sample. 

so the number of obs. for the three thirds for line() are
   {8, 3, 8}  in line()  [also, after your patch, right?]

whereas in MMline() they are as they should be, namely

   {6, 7, 6}

But the  {8, 3, 8}  split is not at all what all "the literature",
including Tukey himself says that "should" be done.
(Other literature on the topic suggests that the optimal sizes
 of the split in three groups depends on the distribution of x ..)

OTOH, MMline() does exactly what "the literature" and also  the
reference on the  ?line  help pages says.

    > In x sample, there are few repeated values, this
    > is certainly be the reason of different quantiles..

    > I am not sure that one quantile definition is better or
    > more correct than the other. 

    > So I would leave line()'s definition as is.

you mean  _after_ applying your patch, I assume.

I currently tend do disagree. If we change line() we should
rather fix more ..
Note the 'Subject' you've chosen for this thread,
 "... does not produce the correct Tukey line",
so I think we should get better.

Apart from Richie / my  MMline() function, I've also noticed
that   ACSWR :: resistant_line()
exists.

However "the literature" (see references below), notably the two
with Hoaglin, strongly  recommends smarter iterations, and
-- lo and behold! -- when this topic came up last (for me) in
Dec. 2014, I did spend about 2 days work (or more?) to get the
FORTRAN code from the 1981 - book (which is abbreviated the
"ABC of EDA") from a somewhat useful OCR scan into compilable
Fortran code and then f2c'ed, wrote an R interface function
found problems i.e., bugs, including infinite loops, fixed most
AFAICS, but somehow did not finish making the result available.

Yes, and I have too many other things on my desk... this will
have to wait!

References:

     Tukey, J. W. (1977).  _Exploratory Data Analysis_, Reading
     Massachusetts: Addison-Wesley.

     Velleman, P. F. and Hoaglin, D. C. (1981) _Applications, Basics
     and Computing of Exploratory Data Analysis_ Duxbury Press.

     Emerson, J. D. and Hoaglin, D. C. (1983) Resistant Lines for y
     versus x.  Chapter 5 of _Understanding Robust and Exploratory Data
     Analysis_, eds. David C. Hoaglin, Frederick Mosteller and John W.
     Tukey.  Wiley.

     Iain M. Johnstone and Paul F. Velleman (1985) The Resistant Line
     and Related Regression Methods.  _Journal of the American
     Statistical Association_ *80*, 1041-1054.  <URL:
     https://dx.doi.org/10.1080/01621459.1985.10478222>


    > Best, Sergue?.

Martin Maechler, ETH Zurich (and R core team)


From henrik.bengtsson at gmail.com  Tue May 30 21:51:16 2017
From: henrik.bengtsson at gmail.com (Henrik Bengtsson)
Date: Tue, 30 May 2017 12:51:16 -0700
Subject: [Rd] RProfmem output format
In-Reply-To: <CAFDcVCQjTW-aWXmYiPZP023YMx10SJY+uBz7mET+OuBF84AQdQ@mail.gmail.com>
References: <BANLkTinazNHgW44pYFycun576E6C9U1L=Q@mail.gmail.com>
 <BANLkTim=QW62QLuduMsXuPp+5N-rTMjdjg@mail.gmail.com>
 <BANLkTike1kM-wN_GYJEH5WVKQHJMdRijMQ@mail.gmail.com>
 <BANLkTimiLvxHYaV3DT9FVsh+p6nxPoW+5g@mail.gmail.com>
 <CAFDcVCQjTW-aWXmYiPZP023YMx10SJY+uBz7mET+OuBF84AQdQ@mail.gmail.com>
Message-ID: <CAFDcVCRYszZfLMEQiE9VMu2XEkPBenyU28apX1si5XR-f50=PA@mail.gmail.com>

UPDATE: Tomas Kalibera has fixed this bug (on missing newlines in
Rprofmem output) in R-devel r72747
(https://github.com/wch/r-source/commit/ba6665deace4a8dc239aebec977c17d0975fbc27).
  Using the example of this thread, Rprofmem() of R-devel now outputs
with newlines:

$ R Under development (unstable) (2017-05-30 r72747) -- "Unsuffered
Consequences"
Copyright (C) 2017 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)
[...]

> Rprofmem(); x <- raw(2000); Rprofmem("")
> cat(readLines("Rprofmem.out", n=5, warn=FALSE), sep="\n")
232 :
472 :
472 :
1064 :
2040 :"raw"

/Henrik

On Sat, Jun 4, 2016 at 12:40 PM, Henrik Bengtsson
<henrik.bengtsson at gmail.com> wrote:
> I'm picking up this 5-year old thread.
>
> 1. About the four memory allocations without a stacktrace
>
> I think the four memory allocations without a stacktrace reported by Rprofmem():
>
>> Rprofmem(); x <- raw(2000); Rprofmem("")
>> cat(readLines("Rprofmem.out", n=5, warn=FALSE), sep="\n")
> 192 :360 :360 :1064 :2040 :"raw"
>
> are due to some initialization of R that is independent of Rprofmem(),
> because they can be avoided if one allocates some memory before (in a
> fresh R session):
>
>> z <- raw(1000); dummy <- gc()
>> Rprofmem(); x <- raw(2000); Rprofmem("")
>> cat(readLines("Rprofmem.out", n=5, warn=FALSE), sep="\n")
> 2040 :"raw"
>
>
> 2. About missing newlines when stacktrace is empty
>
> As a refresher, the problem is that memory allocations an empty
> stracktrace are reported without newlines, i.e.
>
> 192 :360 :360 :1064 :2040 :"raw"
>
> The question is why this is not reported as:
>
> 192 :
> 360 :
> 360 :
> 1064 :
> 2040 :"raw"
>
> This was/is because C function R_OutputStackTrace() - part of
> src/main/memory.c  - looks like:
>
> static void R_OutputStackTrace(FILE *file)
> {
>     int newline = 0;
>     RCNTXT *cptr;
>
>     for (cptr = R_GlobalContext; cptr; cptr = cptr->nextcontext) {
>         if ((cptr->callflag & (CTXT_FUNCTION | CTXT_BUILTIN))
>            && TYPEOF(cptr->call) == LANGSXP) {
>            SEXP fun = CAR(cptr->call);
>            if (!newline) newline = 1;
>            fprintf(file, "\"%s\" ",
>                TYPEOF(fun) == SYMSXP ? CHAR(PRINTNAME(fun)) :
>                "<Anonymous>");
>         }
>     }
>     if (newline) fprintf(file, "\n");
> }
>
>
> Thomas, your last comment was:
>
>> Yes. It's obviously better to always print a newline, and so clearly
>> deliberate not to, that I suspect there may have been a good reason.
>> If I can't work it out (after my grant deadline this week) I will just
>> assume it's wrong.
>
> When I search the code and the commit history
> (https://github.com/wch/r-source/commit/3d5eb2a09f2d75893efdc8bbf1c72d17603886a0),
> it appears that this was there from the very first commit.  Also,
> searching the code for usages of R_OutputStackTrace(), I only find
> R_ReportAllocation() and R_ReportNewPage(), both part of of
> src/main/memory.c (see below).
>
> static void R_ReportAllocation(R_size_t size)
> {
>     if (R_IsMemReporting) {
> if(size > R_MemReportingThreshold) {
>    fprintf(R_MemReportingOutfile, "%lu :", (unsigned long) size);
>    R_OutputStackTrace(R_MemReportingOutfile);
> }
>     }
>     return;
> }
>
> static void R_ReportNewPage(void)
> {
>     if (R_IsMemReporting) {
> fprintf(R_MemReportingOutfile, "new page:");
> R_OutputStackTrace(R_MemReportingOutfile);
>     }
>     return;
> }
>
>
> Could it be that when you wrote it you had another usage for
> R_OutputStackTrace() in mind as well?  If so, it makes sense that
> R_OutputStackTrace() shouldn't output a newline if the stack trace was
> empty.  But if the above is the only usage, to me it looks pretty safe
> to always add a newline.
>
>> sessionInfo()
> R version 3.3.0 Patched (2016-05-26 r70682)
> Platform: x86_64-w64-mingw32/x64 (64-bit)
> Running under: Windows 7 x64 (build 7601) Service Pack 1
>
> locale:
> [1] LC_COLLATE=English_United States.1252
> [2] LC_CTYPE=English_United States.1252
> [3] LC_MONETARY=English_United States.1252
> [4] LC_NUMERIC=C
> [5] LC_TIME=English_United States.1252
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> /Henrik
>
>
> On Sun, May 15, 2011 at 1:16 PM, Thomas Lumley <tlumley at uw.edu> wrote:
>> On Mon, May 16, 2011 at 1:02 AM, Hadley Wickham <hadley at rice.edu> wrote:
>>> So what causes allocations when the call stack is empty?  Something
>>> internal?  Does the garbage collector trigger allocations (i.e. could
>>> it be caused by moving data to contiguous memory)?
>>
>> The garbage collector doesn't move anything, it just swaps pointers in
>> a linked list.
>>
>> The lexer, parser, and evaluator all have  to do some work before a
>> function context is set up for the top-level function, so I assume
>> that's where it is happening.
>>
>>> Any ideas what the correct thing to do with these memory allocations?
>>> Ignore them because they're not really related to the function they're
>>> attributed to?  Sum them up?
>>>
>>>> I don't see why this is done, and I may well be the person who did it
>>>> (I don't have svn on this computer to check), but it is clearly
>>>> deliberate.
>>>
>>> It seems like it would be more consistent to always print a newline,
>>> and then it would obvious those allocations occurred when the call
>>> stack was empty.  This would make parsing the file a little bit
>>> easier.
>>
>> Yes. It's obviously better to always print a newline, and so clearly
>> deliberate not to, that I suspect there may have been a good reason.
>> If I can't work it out (after my grant deadline this week) I will just
>> assume it's wrong.
>>
>>
>>    -thomas
>>
>> --
>> Thomas Lumley
>> Professor of Biostatistics
>> University of Auckland
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel


From glnbrntt at gmail.com  Wed May 31 06:13:31 2017
From: glnbrntt at gmail.com (GlenB)
Date: Wed, 31 May 2017 14:13:31 +1000
Subject: [Rd] stats::line() does not produce correct Tukey line when n
 mod 6 is 2 or 3
In-Reply-To: <22829.41724.818300.640880@stat.math.ethz.ch>
References: <CAN7_cLjefc605D1ATS__MOV4gN7fKDBo5Nx_CNB_B9cDbAxc6w@mail.gmail.com>
 <ffc3e473-622d-796f-a06a-6e1f92f8ab5f@gmail.com>
 <CAN7_cLjmQFtZf9z-qJKdWRCaRY2uxfP8k5=QJsptz3U+TtP8oA@mail.gmail.com>
 <CAN7_cLjbz6S8u79jtx52XgfD1LcpO8CePYUhrunYqrzXHMMN8A@mail.gmail.com>
 <024A7E7D-45F8-4C95-A9BD-DBEB1D085BFA@gmail.com>
 <94b65a5b-da9e-ec5c-e428-7c5dbf175ec1@insa-toulouse.fr>
 <8c167bbb-130d-3798-a9e8-24c4377db313@insa-toulouse.fr>
 <d10ebb8c-0c49-5b0e-5b3c-b06073bfe447@insa-toulouse.fr>
 <22829.8246.695545.320438@stat.math.ethz.ch>
 <396781eb-1712-aa54-96f2-a6c4df0284f4@insa-toulouse.fr>
 <22829.41724.818300.640880@stat.math.ethz.ch>
Message-ID: <CAN7_cLhPTEZ5=WHVA-zDZ_V2_31yN5+m9DyGzy604kZM37w6zg@mail.gmail.com>

Martin Maechler says in reply to Sergue? Sokol

> Note the 'Subject' you've chosen for this thread,
 "... does not produce the correct Tukey line",

The choice of title was mine not Serguei's; I posted the original message
where the error was pointed out

I agree with Martin's assessment that the correct split  (both by Tukey's
lights and by general practice)
for 19 points would be 6,7,6 and I also agree that it's better to "fix
more" in this instance, where possible.
(e.g. Johnstone&Velleman's standard errors would be a nice thing to add if
feasible) --
but if any blame is attached to the choice of title, it  really should be
aimed at me.

Glen

On Wed, May 31, 2017 at 2:51 AM, Martin Maechler <maechler at stat.math.ethz.ch
> wrote:

> >>>>> Serguei Sokol <sokol at insa-toulouse.fr>
> >>>>>     on Tue, 30 May 2017 16:01:17 +0200 writes:
>
>     > Le 30/05/2017 ? 09:33, Martin Maechler a ?crit : ...
>     >> However, even after the patch, The example from the SO
>     >> post differs from the result of Richie Cotton's
>     >> function...
>     > The explanation is quite simple. In SO function, the first
>     > 1/3 quantile of used example counts 6 points (of 19 in
>     > total), while line()'s definition of quantile leads to 8
>     > points. The same numbers (6 and 8) are on the other end of
>     > sample.
>
> so the number of obs. for the three thirds for line() are
>    {8, 3, 8}  in line()  [also, after your patch, right?]
>
> whereas in MMline() they are as they should be, namely
>
>    {6, 7, 6}
>
> But the  {8, 3, 8}  split is not at all what all "the literature",
> including Tukey himself says that "should" be done.
> (Other literature on the topic suggests that the optimal sizes
>  of the split in three groups depends on the distribution of x ..)
>
> OTOH, MMline() does exactly what "the literature" and also  the
> reference on the  ?line  help pages says.
>
>     > In x sample, there are few repeated values, this
>     > is certainly be the reason of different quantiles..
>
>     > I am not sure that one quantile definition is better or
>     > more correct than the other.
>
>     > So I would leave line()'s definition as is.
>
> you mean  _after_ applying your patch, I assume.
>
> I currently tend do disagree. If we change line() we should
> rather fix more ..
> Note the 'Subject' you've chosen for this thread,
>  "... does not produce the correct Tukey line",
> so I think we should get better.
>
> Apart from Richie / my  MMline() function, I've also noticed
> that   ACSWR :: resistant_line()
> exists.
>
> However "the literature" (see references below), notably the two
> with Hoaglin, strongly  recommends smarter iterations, and
> -- lo and behold! -- when this topic came up last (for me) in
> Dec. 2014, I did spend about 2 days work (or more?) to get the
> FORTRAN code from the 1981 - book (which is abbreviated the
> "ABC of EDA") from a somewhat useful OCR scan into compilable
> Fortran code and then f2c'ed, wrote an R interface function
> found problems i.e., bugs, including infinite loops, fixed most
> AFAICS, but somehow did not finish making the result available.
>
> Yes, and I have too many other things on my desk... this will
> have to wait!
>
> References:
>
>      Tukey, J. W. (1977).  _Exploratory Data Analysis_, Reading
>      Massachusetts: Addison-Wesley.
>
>      Velleman, P. F. and Hoaglin, D. C. (1981) _Applications, Basics
>      and Computing of Exploratory Data Analysis_ Duxbury Press.
>
>      Emerson, J. D. and Hoaglin, D. C. (1983) Resistant Lines for y
>      versus x.  Chapter 5 of _Understanding Robust and Exploratory Data
>      Analysis_, eds. David C. Hoaglin, Frederick Mosteller and John W.
>      Tukey.  Wiley.
>
>      Iain M. Johnstone and Paul F. Velleman (1985) The Resistant Line
>      and Related Regression Methods.  _Journal of the American
>      Statistical Association_ *80*, 1041-1054.  <URL:
>      https://dx.doi.org/10.1080/01621459.1985.10478222>
>
>
>     > Best, Sergue?.
>
> Martin Maechler, ETH Zurich (and R core team)
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From sokol at insa-toulouse.fr  Wed May 31 15:06:43 2017
From: sokol at insa-toulouse.fr (Serguei Sokol)
Date: Wed, 31 May 2017 15:06:43 +0200
Subject: [Rd] stats::line() does not produce correct Tukey line when n
 mod 6 is 2 or 3
In-Reply-To: <22829.41724.818300.640880@stat.math.ethz.ch>
References: <CAN7_cLjefc605D1ATS__MOV4gN7fKDBo5Nx_CNB_B9cDbAxc6w@mail.gmail.com>
 <ffc3e473-622d-796f-a06a-6e1f92f8ab5f@gmail.com>
 <CAN7_cLjmQFtZf9z-qJKdWRCaRY2uxfP8k5=QJsptz3U+TtP8oA@mail.gmail.com>
 <CAN7_cLjbz6S8u79jtx52XgfD1LcpO8CePYUhrunYqrzXHMMN8A@mail.gmail.com>
 <024A7E7D-45F8-4C95-A9BD-DBEB1D085BFA@gmail.com>
 <94b65a5b-da9e-ec5c-e428-7c5dbf175ec1@insa-toulouse.fr>
 <8c167bbb-130d-3798-a9e8-24c4377db313@insa-toulouse.fr>
 <d10ebb8c-0c49-5b0e-5b3c-b06073bfe447@insa-toulouse.fr>
 <22829.8246.695545.320438@stat.math.ethz.ch>
 <396781eb-1712-aa54-96f2-a6c4df0284f4@insa-toulouse.fr>
 <22829.41724.818300.640880@stat.math.ethz.ch>
Message-ID: <c9af2c00-3653-29f8-7c6a-5a08d6de498e@insa-toulouse.fr>

Le 30/05/2017 ? 18:51, Martin Maechler a ?crit :
>>>>>> Serguei Sokol <sokol at insa-toulouse.fr>
>>>>>>      on Tue, 30 May 2017 16:01:17 +0200 writes:
>      > Le 30/05/2017 ? 09:33, Martin Maechler a ?crit : ...
>      >> However, even after the patch, The example from the SO
>      >> post differs from the result of Richie Cotton's
>      >> function...
>      > The explanation is quite simple. In SO function, the first
>      > 1/3 quantile of used example counts 6 points (of 19 in
>      > total), while line()'s definition of quantile leads to 8
>      > points. The same numbers (6 and 8) are on the other end of
>      > sample.
>
> so the number of obs. for the three thirds for line() are
>     {8, 3, 8}  in line()  [also, after your patch, right?]
>
> whereas in MMline() they are as they should be, namely
>
>     {6, 7, 6}
>
> But the  {8, 3, 8}  split is not at all what all "the literature",
> including Tukey himself says that "should" be done.
> (Other literature on the topic suggests that the optimal sizes
>   of the split in three groups depends on the distribution of x ..)
>
> OTOH, MMline() does exactly what "the literature" and also  the
> reference on the  ?line  help pages says.
Well, what I have seen so far in "literature" was mention of 1/3 quantiles
(but, yes I could overlook smth as I did not spend too much time on it)
So the sample distribution in three groups boils down to a particular quantile
definition to use. It turns out that the line()'s version (you are right, _after_ the patch
but my patch left this definition untouched) is consistent with the R's one.
If you do in R sum(dfr$time <= quantile(dfr$time, 1./3.)) you get 8, not 6
(and the same on the 2/3 end).
To my mind, consistency with the rest of R, namely with the quantile definition,
is an argument good enough to let the line()'s definition as is.

Serguei.


From jorismeys at gmail.com  Wed May 31 15:40:17 2017
From: jorismeys at gmail.com (Joris Meys)
Date: Wed, 31 May 2017 15:40:17 +0200
Subject: [Rd] stats::line() does not produce correct Tukey line when n
 mod 6 is 2 or 3
In-Reply-To: <c9af2c00-3653-29f8-7c6a-5a08d6de498e@insa-toulouse.fr>
References: <CAN7_cLjefc605D1ATS__MOV4gN7fKDBo5Nx_CNB_B9cDbAxc6w@mail.gmail.com>
 <ffc3e473-622d-796f-a06a-6e1f92f8ab5f@gmail.com>
 <CAN7_cLjmQFtZf9z-qJKdWRCaRY2uxfP8k5=QJsptz3U+TtP8oA@mail.gmail.com>
 <CAN7_cLjbz6S8u79jtx52XgfD1LcpO8CePYUhrunYqrzXHMMN8A@mail.gmail.com>
 <024A7E7D-45F8-4C95-A9BD-DBEB1D085BFA@gmail.com>
 <94b65a5b-da9e-ec5c-e428-7c5dbf175ec1@insa-toulouse.fr>
 <8c167bbb-130d-3798-a9e8-24c4377db313@insa-toulouse.fr>
 <d10ebb8c-0c49-5b0e-5b3c-b06073bfe447@insa-toulouse.fr>
 <22829.8246.695545.320438@stat.math.ethz.ch>
 <396781eb-1712-aa54-96f2-a6c4df0284f4@insa-toulouse.fr>
 <22829.41724.818300.640880@stat.math.ethz.ch>
 <c9af2c00-3653-29f8-7c6a-5a08d6de498e@insa-toulouse.fr>
Message-ID: <CAO1zAVb8faubH7FrBfsx8c04xsZce2p2Ei9NaBELrfn2Q8JMEA@mail.gmail.com>

OTOH,

> sapply(1:9, function(i){
+   sum(dfr$time <= quantile(dfr$time, 1./3., type = i))
+ })
[1] 8 8 6 6 6 6 8 6 6

Only the default (type = 7) and the first two types give the result lines()
gives now. I think there is plenty of reasons to give why any of the other
6 types might be better suited in Tukey's method.

So to my mind, chaning the definition of line() to give sensible output
that is in accordance with the theory, does not imply any inconsistency
with the quantile definition in R. At least not with 6 out of the 9
different ones ;-)

Cheers
Joris

On Wed, May 31, 2017 at 3:06 PM, Serguei Sokol <sokol at insa-toulouse.fr>
wrote:

> Le 30/05/2017 ? 18:51, Martin Maechler a ?crit :
>
>> Serguei Sokol <sokol at insa-toulouse.fr>
>>>>>>>      on Tue, 30 May 2017 16:01:17 +0200 writes:
>>>>>>>
>>>>>>      > Le 30/05/2017 ? 09:33, Martin Maechler a ?crit : ...
>>      >> However, even after the patch, The example from the SO
>>      >> post differs from the result of Richie Cotton's
>>      >> function...
>>      > The explanation is quite simple. In SO function, the first
>>      > 1/3 quantile of used example counts 6 points (of 19 in
>>      > total), while line()'s definition of quantile leads to 8
>>      > points. The same numbers (6 and 8) are on the other end of
>>      > sample.
>>
>> so the number of obs. for the three thirds for line() are
>>     {8, 3, 8}  in line()  [also, after your patch, right?]
>>
>> whereas in MMline() they are as they should be, namely
>>
>>     {6, 7, 6}
>>
>> But the  {8, 3, 8}  split is not at all what all "the literature",
>> including Tukey himself says that "should" be done.
>> (Other literature on the topic suggests that the optimal sizes
>>   of the split in three groups depends on the distribution of x ..)
>>
>> OTOH, MMline() does exactly what "the literature" and also  the
>> reference on the  ?line  help pages says.
>>
> Well, what I have seen so far in "literature" was mention of 1/3 quantiles
> (but, yes I could overlook smth as I did not spend too much time on it)
> So the sample distribution in three groups boils down to a particular
> quantile
> definition to use. It turns out that the line()'s version (you are right,
> _after_ the patch
> but my patch left this definition untouched) is consistent with the R's
> one.
> If you do in R sum(dfr$time <= quantile(dfr$time, 1./3.)) you get 8, not 6
> (and the same on the 2/3 end).
> To my mind, consistency with the rest of R, namely with the quantile
> definition,
> is an argument good enough to let the line()'s definition as is.
>
> Serguei.
>
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>



-- 
Joris Meys
Statistical consultant

Ghent University
Faculty of Bioscience Engineering
Department of Mathematical Modelling, Statistics and Bio-Informatics

tel :  +32 (0)9 264 61 79
Joris.Meys at Ugent.be
-------------------------------
Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php

	[[alternative HTML version deleted]]


From sokol at insa-toulouse.fr  Wed May 31 16:03:09 2017
From: sokol at insa-toulouse.fr (Serguei Sokol)
Date: Wed, 31 May 2017 16:03:09 +0200
Subject: [Rd] stats::line() does not produce correct Tukey line when n
 mod 6 is 2 or 3
In-Reply-To: <CAO1zAVb8faubH7FrBfsx8c04xsZce2p2Ei9NaBELrfn2Q8JMEA@mail.gmail.com>
References: <CAN7_cLjefc605D1ATS__MOV4gN7fKDBo5Nx_CNB_B9cDbAxc6w@mail.gmail.com>
 <ffc3e473-622d-796f-a06a-6e1f92f8ab5f@gmail.com>
 <CAN7_cLjmQFtZf9z-qJKdWRCaRY2uxfP8k5=QJsptz3U+TtP8oA@mail.gmail.com>
 <CAN7_cLjbz6S8u79jtx52XgfD1LcpO8CePYUhrunYqrzXHMMN8A@mail.gmail.com>
 <024A7E7D-45F8-4C95-A9BD-DBEB1D085BFA@gmail.com>
 <94b65a5b-da9e-ec5c-e428-7c5dbf175ec1@insa-toulouse.fr>
 <8c167bbb-130d-3798-a9e8-24c4377db313@insa-toulouse.fr>
 <d10ebb8c-0c49-5b0e-5b3c-b06073bfe447@insa-toulouse.fr>
 <22829.8246.695545.320438@stat.math.ethz.ch>
 <396781eb-1712-aa54-96f2-a6c4df0284f4@insa-toulouse.fr>
 <22829.41724.818300.640880@stat.math.ethz.ch>
 <c9af2c00-3653-29f8-7c6a-5a08d6de498e@insa-toulouse.fr>
 <CAO1zAVb8faubH7FrBfsx8c04xsZce2p2Ei9NaBELrfn2Q8JMEA@mail.gmail.com>
Message-ID: <055b3d48-5f94-d087-74b8-b511e6c45baf@insa-toulouse.fr>

Le 31/05/2017 ? 15:40, Joris Meys a ?crit :
> OTOH,
>
> > sapply(1:9, function(i){
> +   sum(dfr$time <= quantile(dfr$time, 1./3., type = i))
> + })
> [1] 8 8 6 6 6 6 8 6 6
>
> Only the default (type = 7) and the first two types give the result lines() gives now. I think there is plenty of reasons to give why any of the other 6 types 
> might be better suited in Tukey's method.
>
> So to my mind, chaning the definition of line() to give sensible output that is in accordance with the theory, does not imply any inconsistency with the 
> quantile definition in R. At least not with 6 out of the 9 different ones ;-)
Nice shot.
But OTOE (on the other end ;)
 > sapply(1:9, function(i){
+   sum(dfr$time >= quantile(dfr$time, 2./3., type = i))
+ })
[1] 8 8 8 8 6 6 8 6 6

Here "8" gains 5 votes against 4 for "6". There were two defector methods
that changed the point number and should be discarded. Which leaves us
with the score 3:4, still in favor of "6" but the default method should prevail
in my sens.

Serguei.


From jorismeys at gmail.com  Wed May 31 16:39:48 2017
From: jorismeys at gmail.com (Joris Meys)
Date: Wed, 31 May 2017 16:39:48 +0200
Subject: [Rd] stats::line() does not produce correct Tukey line when n
 mod 6 is 2 or 3
In-Reply-To: <055b3d48-5f94-d087-74b8-b511e6c45baf@insa-toulouse.fr>
References: <CAN7_cLjefc605D1ATS__MOV4gN7fKDBo5Nx_CNB_B9cDbAxc6w@mail.gmail.com>
 <ffc3e473-622d-796f-a06a-6e1f92f8ab5f@gmail.com>
 <CAN7_cLjmQFtZf9z-qJKdWRCaRY2uxfP8k5=QJsptz3U+TtP8oA@mail.gmail.com>
 <CAN7_cLjbz6S8u79jtx52XgfD1LcpO8CePYUhrunYqrzXHMMN8A@mail.gmail.com>
 <024A7E7D-45F8-4C95-A9BD-DBEB1D085BFA@gmail.com>
 <94b65a5b-da9e-ec5c-e428-7c5dbf175ec1@insa-toulouse.fr>
 <8c167bbb-130d-3798-a9e8-24c4377db313@insa-toulouse.fr>
 <d10ebb8c-0c49-5b0e-5b3c-b06073bfe447@insa-toulouse.fr>
 <22829.8246.695545.320438@stat.math.ethz.ch>
 <396781eb-1712-aa54-96f2-a6c4df0284f4@insa-toulouse.fr>
 <22829.41724.818300.640880@stat.math.ethz.ch>
 <c9af2c00-3653-29f8-7c6a-5a08d6de498e@insa-toulouse.fr>
 <CAO1zAVb8faubH7FrBfsx8c04xsZce2p2Ei9NaBELrfn2Q8JMEA@mail.gmail.com>
 <055b3d48-5f94-d087-74b8-b511e6c45baf@insa-toulouse.fr>
Message-ID: <CAO1zAVa3=dA_atBNxj9kAX_Rh=5pCMiLK7BAuwq9hhY6KACQag@mail.gmail.com>

Seriously, if a method gives a wrong result, it's wrong. line() does NOT
implement the algorithm of Tukey, even not after the patch. We're not
discussing Excel here, are we?

The method of Tukey is rather clear, and it is NOT using the default
quantile definition from the quantile function. Actually, it doesn't even
use quantiles to define the groups. It just says that the groups should be
more or less equally spaced. As the method of Tukey relies on the medians
of the subgroups, it would make sense to pick a method that is
approximately unbiased with regard to the median. That would be type 8
imho.

To get the size of the outer groups, Tukey would've been more than happy
enough with a:

> floor(length(dfr$time) / 3)
[1] 6

There you have the size of your left and right group, and now we can
discuss about which median type should be used for the robust fitting.

But I can honestly not understand why anyone in his right mind would defend
a method that is clearly wrong while not working at Microsoft's spreadsheet
department.

Cheers
Joris

On Wed, May 31, 2017 at 4:03 PM, Serguei Sokol <sokol at insa-toulouse.fr>
wrote:

> Le 31/05/2017 ? 15:40, Joris Meys a ?crit :
>
>> OTOH,
>>
>> > sapply(1:9, function(i){
>> +   sum(dfr$time <= quantile(dfr$time, 1./3., type = i))
>> + })
>> [1] 8 8 6 6 6 6 8 6 6
>>
>> Only the default (type = 7) and the first two types give the result
>> lines() gives now. I think there is plenty of reasons to give why any of
>> the other 6 types might be better suited in Tukey's method.
>>
>> So to my mind, chaning the definition of line() to give sensible output
>> that is in accordance with the theory, does not imply any inconsistency
>> with the quantile definition in R. At least not with 6 out of the 9
>> different ones ;-)
>>
> Nice shot.
> But OTOE (on the other end ;)
> > sapply(1:9, function(i){
> +   sum(dfr$time >= quantile(dfr$time, 2./3., type = i))
> + })
> [1] 8 8 8 8 6 6 8 6 6
>
> Here "8" gains 5 votes against 4 for "6". There were two defector methods
> that changed the point number and should be discarded. Which leaves us
> with the score 3:4, still in favor of "6" but the default method should
> prevail
> in my sens.
>
> Serguei.
>



-- 
Joris Meys
Statistical consultant

Ghent University
Faculty of Bioscience Engineering
Department of Mathematical Modelling, Statistics and Bio-Informatics

tel :  +32 (0)9 264 61 79
Joris.Meys at Ugent.be
-------------------------------
Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php

	[[alternative HTML version deleted]]


From jorismeys at gmail.com  Wed May 31 16:40:30 2017
From: jorismeys at gmail.com (Joris Meys)
Date: Wed, 31 May 2017 16:40:30 +0200
Subject: [Rd] stats::line() does not produce correct Tukey line when n
 mod 6 is 2 or 3
In-Reply-To: <CAO1zAVa3=dA_atBNxj9kAX_Rh=5pCMiLK7BAuwq9hhY6KACQag@mail.gmail.com>
References: <CAN7_cLjefc605D1ATS__MOV4gN7fKDBo5Nx_CNB_B9cDbAxc6w@mail.gmail.com>
 <ffc3e473-622d-796f-a06a-6e1f92f8ab5f@gmail.com>
 <CAN7_cLjmQFtZf9z-qJKdWRCaRY2uxfP8k5=QJsptz3U+TtP8oA@mail.gmail.com>
 <CAN7_cLjbz6S8u79jtx52XgfD1LcpO8CePYUhrunYqrzXHMMN8A@mail.gmail.com>
 <024A7E7D-45F8-4C95-A9BD-DBEB1D085BFA@gmail.com>
 <94b65a5b-da9e-ec5c-e428-7c5dbf175ec1@insa-toulouse.fr>
 <8c167bbb-130d-3798-a9e8-24c4377db313@insa-toulouse.fr>
 <d10ebb8c-0c49-5b0e-5b3c-b06073bfe447@insa-toulouse.fr>
 <22829.8246.695545.320438@stat.math.ethz.ch>
 <396781eb-1712-aa54-96f2-a6c4df0284f4@insa-toulouse.fr>
 <22829.41724.818300.640880@stat.math.ethz.ch>
 <c9af2c00-3653-29f8-7c6a-5a08d6de498e@insa-toulouse.fr>
 <CAO1zAVb8faubH7FrBfsx8c04xsZce2p2Ei9NaBELrfn2Q8JMEA@mail.gmail.com>
 <055b3d48-5f94-d087-74b8-b511e6c45baf@insa-toulouse.fr>
 <CAO1zAVa3=dA_atBNxj9kAX_Rh=5pCMiLK7BAuwq9hhY6KACQag@mail.gmail.com>
Message-ID: <CAO1zAVbeDxrQkD3f+ndMEfFyca2eYuUwt0cCV0kh4xb3N2EH2Q@mail.gmail.com>

And with "equally spaced" I obviously meant "of equal size". It's getting
too hot in the office here...

On Wed, May 31, 2017 at 4:39 PM, Joris Meys <jorismeys at gmail.com> wrote:

> Seriously, if a method gives a wrong result, it's wrong. line() does NOT
> implement the algorithm of Tukey, even not after the patch. We're not
> discussing Excel here, are we?
>
> The method of Tukey is rather clear, and it is NOT using the default
> quantile definition from the quantile function. Actually, it doesn't even
> use quantiles to define the groups. It just says that the groups should be
> more or less equally spaced. As the method of Tukey relies on the medians
> of the subgroups, it would make sense to pick a method that is
> approximately unbiased with regard to the median. That would be type 8
> imho.
>
> To get the size of the outer groups, Tukey would've been more than happy
> enough with a:
>
> > floor(length(dfr$time) / 3)
> [1] 6
>
> There you have the size of your left and right group, and now we can
> discuss about which median type should be used for the robust fitting.
>
> But I can honestly not understand why anyone in his right mind would
> defend a method that is clearly wrong while not working at Microsoft's
> spreadsheet department.
>
> Cheers
> Joris
>
> On Wed, May 31, 2017 at 4:03 PM, Serguei Sokol <sokol at insa-toulouse.fr>
> wrote:
>
>> Le 31/05/2017 ? 15:40, Joris Meys a ?crit :
>>
>>> OTOH,
>>>
>>> > sapply(1:9, function(i){
>>> +   sum(dfr$time <= quantile(dfr$time, 1./3., type = i))
>>> + })
>>> [1] 8 8 6 6 6 6 8 6 6
>>>
>>> Only the default (type = 7) and the first two types give the result
>>> lines() gives now. I think there is plenty of reasons to give why any of
>>> the other 6 types might be better suited in Tukey's method.
>>>
>>> So to my mind, chaning the definition of line() to give sensible output
>>> that is in accordance with the theory, does not imply any inconsistency
>>> with the quantile definition in R. At least not with 6 out of the 9
>>> different ones ;-)
>>>
>> Nice shot.
>> But OTOE (on the other end ;)
>> > sapply(1:9, function(i){
>> +   sum(dfr$time >= quantile(dfr$time, 2./3., type = i))
>> + })
>> [1] 8 8 8 8 6 6 8 6 6
>>
>> Here "8" gains 5 votes against 4 for "6". There were two defector methods
>> that changed the point number and should be discarded. Which leaves us
>> with the score 3:4, still in favor of "6" but the default method should
>> prevail
>> in my sens.
>>
>> Serguei.
>>
>
>
>
> --
> Joris Meys
> Statistical consultant
>
> Ghent University
> Faculty of Bioscience Engineering
> Department of Mathematical Modelling, Statistics and Bio-Informatics
>
> tel :  +32 (0)9 264 61 79 <+32%209%20264%2061%2079>
> Joris.Meys at Ugent.be
> -------------------------------
> Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php
>



-- 
Joris Meys
Statistical consultant

Ghent University
Faculty of Bioscience Engineering
Department of Mathematical Modelling, Statistics and Bio-Informatics

tel :  +32 (0)9 264 61 79
Joris.Meys at Ugent.be
-------------------------------
Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php

	[[alternative HTML version deleted]]


From pdalgd at gmail.com  Wed May 31 16:57:59 2017
From: pdalgd at gmail.com (peter dalgaard)
Date: Wed, 31 May 2017 16:57:59 +0200
Subject: [Rd] stats::line() does not produce correct Tukey line when n
 mod 6 is 2 or 3
In-Reply-To: <CAO1zAVbeDxrQkD3f+ndMEfFyca2eYuUwt0cCV0kh4xb3N2EH2Q@mail.gmail.com>
References: <CAN7_cLjefc605D1ATS__MOV4gN7fKDBo5Nx_CNB_B9cDbAxc6w@mail.gmail.com>
 <ffc3e473-622d-796f-a06a-6e1f92f8ab5f@gmail.com>
 <CAN7_cLjmQFtZf9z-qJKdWRCaRY2uxfP8k5=QJsptz3U+TtP8oA@mail.gmail.com>
 <CAN7_cLjbz6S8u79jtx52XgfD1LcpO8CePYUhrunYqrzXHMMN8A@mail.gmail.com>
 <024A7E7D-45F8-4C95-A9BD-DBEB1D085BFA@gmail.com>
 <94b65a5b-da9e-ec5c-e428-7c5dbf175ec1@insa-toulouse.fr>
 <8c167bbb-130d-3798-a9e8-24c4377db313@insa-toulouse.fr>
 <d10ebb8c-0c49-5b0e-5b3c-b06073bfe447@insa-toulouse.fr>
 <22829.8246.695545.320438@stat.math.ethz.ch>
 <396781eb-1712-aa54-96f2-a6c4df0284f4@insa-toulouse.fr>
 <22829.41724.818300.640880@stat.math.ethz.ch>
 <c9af2c00-3653-29f8-7c6a-5a08d6de498e@insa-toulouse.fr>
 <CAO1zAVb8faubH7FrBfsx8c04xsZce2p2Ei9NaBELrfn2Q8JMEA@mail.gmail.com>
 <055b3d48-5f94-d087-74b8-b511e6c45baf@insa-toulouse.fr>
 <CAO1zAVa3=dA_atBNxj9kAX_Rh=5pCMiLK7BAuwq9hhY6KACQag@mail.gmail.com>
 <CAO1zAVbeDxrQkD3f+ndMEfFyca2eYuUwt0cCV0kh4xb3N2EH2Q@mail.gmail.com>
Message-ID: <B8A6AED1-7FB8-43EF-AD58-24A202305C67@gmail.com>


> On 31 May 2017, at 16:40 , Joris Meys <jorismeys at gmail.com> wrote:
> 
> And with "equally spaced" I obviously meant "of equal size". It's getting
> too hot in the office here...

We have a fair amount of cool westerly wind up here that I could transfer to you via  WWTP (Wind and Weather Transport Protocol). If you open up a sufficiently large pipe, that is. 

Anyways, in the past we have tried to follow Tukey's instructions on details like the definition of the "hinges" on boxplots, so presumably we should try and do likewise for this case. 

I suspect that Tukey would say "divide the data into three roughly equal-sized groups" or some such. The obvious thing to do would be to allocate N %/% 3 to each group and then the N %% 3 remaining symmetrically and as evenly as possible, which in my book would rather be (1,0,1) than (0, 2, 0) for the case N %% 3 == 2. If  N %% 3 == 1, there is no alternative to (0, 1, 0) by this logic.

> 
> On Wed, May 31, 2017 at 4:39 PM, Joris Meys <jorismeys at gmail.com> wrote:
> 
>> Seriously, if a method gives a wrong result, it's wrong. line() does NOT
>> implement the algorithm of Tukey, even not after the patch. We're not
>> discussing Excel here, are we?
>> 
>> The method of Tukey is rather clear, and it is NOT using the default
>> quantile definition from the quantile function. Actually, it doesn't even
>> use quantiles to define the groups. It just says that the groups should be
>> more or less equally spaced. As the method of Tukey relies on the medians
>> of the subgroups, it would make sense to pick a method that is
>> approximately unbiased with regard to the median. That would be type 8
>> imho.
>> 
>> To get the size of the outer groups, Tukey would've been more than happy
>> enough with a:
>> 
>>> floor(length(dfr$time) / 3)
>> [1] 6
>> 
>> There you have the size of your left and right group, and now we can
>> discuss about which median type should be used for the robust fitting.
>> 
>> But I can honestly not understand why anyone in his right mind would
>> defend a method that is clearly wrong while not working at Microsoft's
>> spreadsheet department.
>> 
>> Cheers
>> Joris
>> 
>> On Wed, May 31, 2017 at 4:03 PM, Serguei Sokol <sokol at insa-toulouse.fr>
>> wrote:
>> 
>>> Le 31/05/2017 ? 15:40, Joris Meys a ?crit :
>>> 
>>>> OTOH,
>>>> 
>>>>> sapply(1:9, function(i){
>>>> +   sum(dfr$time <= quantile(dfr$time, 1./3., type = i))
>>>> + })
>>>> [1] 8 8 6 6 6 6 8 6 6
>>>> 
>>>> Only the default (type = 7) and the first two types give the result
>>>> lines() gives now. I think there is plenty of reasons to give why any of
>>>> the other 6 types might be better suited in Tukey's method.
>>>> 
>>>> So to my mind, chaning the definition of line() to give sensible output
>>>> that is in accordance with the theory, does not imply any inconsistency
>>>> with the quantile definition in R. At least not with 6 out of the 9
>>>> different ones ;-)
>>>> 
>>> Nice shot.
>>> But OTOE (on the other end ;)
>>>> sapply(1:9, function(i){
>>> +   sum(dfr$time >= quantile(dfr$time, 2./3., type = i))
>>> + })
>>> [1] 8 8 8 8 6 6 8 6 6
>>> 
>>> Here "8" gains 5 votes against 4 for "6". There were two defector methods
>>> that changed the point number and should be discarded. Which leaves us
>>> with the score 3:4, still in favor of "6" but the default method should
>>> prevail
>>> in my sens.
>>> 
>>> Serguei.
>>> 
>> 
>> 
>> 
>> --
>> Joris Meys
>> Statistical consultant
>> 
>> Ghent University
>> Faculty of Bioscience Engineering
>> Department of Mathematical Modelling, Statistics and Bio-Informatics
>> 
>> tel :  +32 (0)9 264 61 79 <+32%209%20264%2061%2079>
>> Joris.Meys at Ugent.be
>> -------------------------------
>> Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php
>> 
> 
> 
> 
> -- 
> Joris Meys
> Statistical consultant
> 
> Ghent University
> Faculty of Bioscience Engineering
> Department of Mathematical Modelling, Statistics and Bio-Informatics
> 
> tel :  +32 (0)9 264 61 79
> Joris.Meys at Ugent.be
> -------------------------------
> Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From sokol at insa-toulouse.fr  Wed May 31 17:30:44 2017
From: sokol at insa-toulouse.fr (Serguei Sokol)
Date: Wed, 31 May 2017 17:30:44 +0200
Subject: [Rd] stats::line() does not produce correct Tukey line when n
 mod 6 is 2 or 3
In-Reply-To: <CAO1zAVa3=dA_atBNxj9kAX_Rh=5pCMiLK7BAuwq9hhY6KACQag@mail.gmail.com>
References: <CAN7_cLjefc605D1ATS__MOV4gN7fKDBo5Nx_CNB_B9cDbAxc6w@mail.gmail.com>
 <ffc3e473-622d-796f-a06a-6e1f92f8ab5f@gmail.com>
 <CAN7_cLjmQFtZf9z-qJKdWRCaRY2uxfP8k5=QJsptz3U+TtP8oA@mail.gmail.com>
 <CAN7_cLjbz6S8u79jtx52XgfD1LcpO8CePYUhrunYqrzXHMMN8A@mail.gmail.com>
 <024A7E7D-45F8-4C95-A9BD-DBEB1D085BFA@gmail.com>
 <94b65a5b-da9e-ec5c-e428-7c5dbf175ec1@insa-toulouse.fr>
 <8c167bbb-130d-3798-a9e8-24c4377db313@insa-toulouse.fr>
 <d10ebb8c-0c49-5b0e-5b3c-b06073bfe447@insa-toulouse.fr>
 <22829.8246.695545.320438@stat.math.ethz.ch>
 <396781eb-1712-aa54-96f2-a6c4df0284f4@insa-toulouse.fr>
 <22829.41724.818300.640880@stat.math.ethz.ch>
 <c9af2c00-3653-29f8-7c6a-5a08d6de498e@insa-toulouse.fr>
 <CAO1zAVb8faubH7FrBfsx8c04xsZce2p2Ei9NaBELrfn2Q8JMEA@mail.gmail.com>
 <055b3d48-5f94-d087-74b8-b511e6c45baf@insa-toulouse.fr>
 <CAO1zAVa3=dA_atBNxj9kAX_Rh=5pCMiLK7BAuwq9hhY6KACQag@mail.gmail.com>
Message-ID: <8fc861d2-1f45-f171-b135-000a668f73b7@insa-toulouse.fr>

Le 31/05/2017 ? 16:39, Joris Meys a ?crit :
> Seriously, if a method gives a wrong result, it's wrong.
I did not understand why you and others were using term "wrong"
based on something that I was considering as just "different" implementation.
More thorough reading revealed that I have overlooked this phrase in the
line's doc: "left and right /thirds/ of the data" (emphasis is mine).

Should I be exiled to Excel department for this sin? That's tough ;)
Serguei.

> line() does NOT implement the algorithm of Tukey, even not after the patch. We're not discussing Excel here, are we?
>
> The method of Tukey is rather clear, and it is NOT using the default quantile definition from the quantile function. Actually, it doesn't even use quantiles 
> to define the groups. It just says that the groups should be more or less equally spaced. As the method of Tukey relies on the medians of the subgroups, it 
> would make sense to pick a method that is approximately unbiased with regard to the median. That would be type 8 imho.
>
> To get the size of the outer groups, Tukey would've been more than happy enough with a:
>
> > floor(length(dfr$time) / 3)
> [1] 6
>
> There you have the size of your left and right group, and now we can discuss about which median type should be used for the robust fitting.
>
> But I can honestly not understand why anyone in his right mind would defend a method that is clearly wrong while not working at Microsoft's spreadsheet 
> department.
>
> Cheers
> Joris
>
> On Wed, May 31, 2017 at 4:03 PM, Serguei Sokol <sokol at insa-toulouse.fr <mailto:sokol at insa-toulouse.fr>> wrote:
>
>     Le 31/05/2017 ? 15:40, Joris Meys a ?crit :
>
>         OTOH,
>
>         > sapply(1:9, function(i){
>         +   sum(dfr$time <= quantile(dfr$time, 1./3., type = i))
>         + })
>         [1] 8 8 6 6 6 6 8 6 6
>
>         Only the default (type = 7) and the first two types give the result lines() gives now. I think there is plenty of reasons to give why any of the other
>         6 types might be better suited in Tukey's method.
>
>         So to my mind, chaning the definition of line() to give sensible output that is in accordance with the theory, does not imply any inconsistency with
>         the quantile definition in R. At least not with 6 out of the 9 different ones ;-)
>
>     Nice shot.
>     But OTOE (on the other end ;)
>     > sapply(1:9, function(i){
>     +   sum(dfr$time >= quantile(dfr$time, 2./3., type = i))
>     + })
>     [1] 8 8 8 8 6 6 8 6 6
>
>     Here "8" gains 5 votes against 4 for "6". There were two defector methods
>     that changed the point number and should be discarded. Which leaves us
>     with the score 3:4, still in favor of "6" but the default method should prevail
>     in my sens.
>
>     Serguei.
>
>
>
>
> -- 
> Joris Meys
> Statistical consultant
>
> Ghent University
> Faculty of Bioscience Engineering
> Department of Mathematical Modelling, Statistics and Bio-Informatics
>
> tel :  +32 (0)9 264 61 79
> Joris.Meys at Ugent.be
> -------------------------------
> Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php


-- 
Serguei Sokol
Ingenieur de recherche INRA
Metabolisme Integre et Dynamique des Systemes Metaboliques (MetaSys)

LISBP, INSA/INRA UMR 792, INSA/CNRS UMR 5504
135 Avenue de Rangueil
31077 Toulouse Cedex 04

tel: +33 5 6155 9276
fax: +33 5 6704 8825
email: sokol at insa-toulouse.fr
http://metasys.insa-toulouse.fr
http://www.lisbp.fr


From jorismeys at gmail.com  Wed May 31 17:57:10 2017
From: jorismeys at gmail.com (Joris Meys)
Date: Wed, 31 May 2017 17:57:10 +0200
Subject: [Rd] stats::line() does not produce correct Tukey line when n
 mod 6 is 2 or 3
In-Reply-To: <B8A6AED1-7FB8-43EF-AD58-24A202305C67@gmail.com>
References: <CAN7_cLjefc605D1ATS__MOV4gN7fKDBo5Nx_CNB_B9cDbAxc6w@mail.gmail.com>
 <ffc3e473-622d-796f-a06a-6e1f92f8ab5f@gmail.com>
 <CAN7_cLjmQFtZf9z-qJKdWRCaRY2uxfP8k5=QJsptz3U+TtP8oA@mail.gmail.com>
 <CAN7_cLjbz6S8u79jtx52XgfD1LcpO8CePYUhrunYqrzXHMMN8A@mail.gmail.com>
 <024A7E7D-45F8-4C95-A9BD-DBEB1D085BFA@gmail.com>
 <94b65a5b-da9e-ec5c-e428-7c5dbf175ec1@insa-toulouse.fr>
 <8c167bbb-130d-3798-a9e8-24c4377db313@insa-toulouse.fr>
 <d10ebb8c-0c49-5b0e-5b3c-b06073bfe447@insa-toulouse.fr>
 <22829.8246.695545.320438@stat.math.ethz.ch>
 <396781eb-1712-aa54-96f2-a6c4df0284f4@insa-toulouse.fr>
 <22829.41724.818300.640880@stat.math.ethz.ch>
 <c9af2c00-3653-29f8-7c6a-5a08d6de498e@insa-toulouse.fr>
 <CAO1zAVb8faubH7FrBfsx8c04xsZce2p2Ei9NaBELrfn2Q8JMEA@mail.gmail.com>
 <055b3d48-5f94-d087-74b8-b511e6c45baf@insa-toulouse.fr>
 <CAO1zAVa3=dA_atBNxj9kAX_Rh=5pCMiLK7BAuwq9hhY6KACQag@mail.gmail.com>
 <CAO1zAVbeDxrQkD3f+ndMEfFyca2eYuUwt0cCV0kh4xb3N2EH2Q@mail.gmail.com>
 <B8A6AED1-7FB8-43EF-AD58-24A202305C67@gmail.com>
Message-ID: <CAO1zAVbMeNh71GMNwZgKXjvP+9uLJTSwEmC03g4yGmsKHKue1A@mail.gmail.com>

On Wed, May 31, 2017 at 4:57 PM, peter dalgaard <pdalgd at gmail.com> wrote:

>
> We have a fair amount of cool westerly wind up here that I could transfer
> to you via  WWTP (Wind and Weather Transport Protocol). If you open up a
> sufficiently large pipe, that is.
>
>
I closed and opened windows again, but it still doesn't want to accept
WWTP...


> I suspect that Tukey would say "divide the data into three roughly
> equal-sized groups" or some such. The obvious thing to do would be to
> allocate N %/% 3 to each group and then the N %% 3 remaining symmetrically
> and as evenly as possible, which in my book would rather be (1,0,1) than
> (0, 2, 0) for the case N %% 3 == 2. If  N %% 3 == 1, there is no
> alternative to (0, 1, 0) by this logic.
>

That's exactly how I would do it on a cold day.



On Wed, May 31, 2017 at 5:30 PM, Serguei Sokol <sokol at insa-toulouse.fr>
wrote:

>
> Should I be exiled to Excel department for this sin? That's tough ;)
> Serguei.
>

You're right, that was really rude of me. My apologies ;)

-- 
Joris Meys
Statistical consultant

Ghent University
Faculty of Bioscience Engineering
Department of Mathematical Modelling, Statistics and Bio-Informatics

tel :  +32 (0)9 264 61 79
Joris.Meys at Ugent.be
-------------------------------
Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php

	[[alternative HTML version deleted]]


From sokol at insa-toulouse.fr  Wed May 31 18:46:34 2017
From: sokol at insa-toulouse.fr (Serguei Sokol)
Date: Wed, 31 May 2017 18:46:34 +0200
Subject: [Rd] stats::line() does not produce correct Tukey line when n
 mod 6 is 2 or 3
In-Reply-To: <8fc861d2-1f45-f171-b135-000a668f73b7@insa-toulouse.fr>
References: <CAN7_cLjefc605D1ATS__MOV4gN7fKDBo5Nx_CNB_B9cDbAxc6w@mail.gmail.com>
 <ffc3e473-622d-796f-a06a-6e1f92f8ab5f@gmail.com>
 <CAN7_cLjmQFtZf9z-qJKdWRCaRY2uxfP8k5=QJsptz3U+TtP8oA@mail.gmail.com>
 <CAN7_cLjbz6S8u79jtx52XgfD1LcpO8CePYUhrunYqrzXHMMN8A@mail.gmail.com>
 <024A7E7D-45F8-4C95-A9BD-DBEB1D085BFA@gmail.com>
 <94b65a5b-da9e-ec5c-e428-7c5dbf175ec1@insa-toulouse.fr>
 <8c167bbb-130d-3798-a9e8-24c4377db313@insa-toulouse.fr>
 <d10ebb8c-0c49-5b0e-5b3c-b06073bfe447@insa-toulouse.fr>
 <22829.8246.695545.320438@stat.math.ethz.ch>
 <396781eb-1712-aa54-96f2-a6c4df0284f4@insa-toulouse.fr>
 <22829.41724.818300.640880@stat.math.ethz.ch>
 <c9af2c00-3653-29f8-7c6a-5a08d6de498e@insa-toulouse.fr>
 <CAO1zAVb8faubH7FrBfsx8c04xsZce2p2Ei9NaBELrfn2Q8JMEA@mail.gmail.com>
 <055b3d48-5f94-d087-74b8-b511e6c45baf@insa-toulouse.fr>
 <CAO1zAVa3=dA_atBNxj9kAX_Rh=5pCMiLK7BAuwq9hhY6KACQag@mail.gmail.com>
 <8fc861d2-1f45-f171-b135-000a668f73b7@insa-toulouse.fr>
Message-ID: <001b1eec-e582-3d29-2b65-107957f57cf3@insa-toulouse.fr>

Le 31/05/2017 ? 17:30, Serguei Sokol a ?crit :
>
> More thorough reading revealed that I have overlooked this phrase in the
> line's doc: "left and right /thirds/ of the data" (emphasis is mine).
Oops. I have read the first ref returned by google and it happened to be
tibco's doc, not the R's one. The layout is very similar hence my mistake.
The latter does not mention "thirds" but ...
Anyway, here is a new line's patch which still gives a result slightly different
form MMline(). The slope is the same but not the intercept.
What are the exact terms for intercept calculation that should be implemented?

Serguei.
-------------- next part --------------
--- line.c.orig	2016-03-17 00:03:03.000000000 +0100
+++ line.c	2017-05-31 18:24:55.330030280 +0200
@@ -17,7 +17,7 @@
  *  https://www.R-project.org/Licenses/
  */
 
-#include <R_ext/Utils.h>	/* R_rsort() */
+#include <R_ext/Utils.h>    /* R_rsort() */
 #include <math.h>
 
 #include <Rinternals.h>
@@ -25,8 +25,8 @@
 
 /* Speed up by `inlining' these (as macros) [since R version 1.2] : */
 #if 1
-#define il(n,x)	(int)floor((n - 1) * x)
-#define iu(n,x)	(int) ceil((n - 1) * x)
+#define il(n,x) (int)floor(((n) - 1) * (x))
+#define iu(n,x) (int) ceil(((n) - 1) * (x))
 
 #else
 static int il(int n, double x)
@@ -41,71 +41,68 @@
 #endif
 
 static void line(double *x, double *y, /* input (x[i],y[i])s */
-		 double *z, double *w, /* work and output: resid. & fitted */
-		 /* all the above of length */ int n,
-		 double coef[2])
+         double *z, double *w, /* work and output: resid. & fitted */
+         int *indx, /* to get thirds of points independently of repeated values */
+         /* all the above of length */ int n,
+         double coef[2])
 {
     int i, j, k;
     double xb, x1, x2, xt, yt, yb, tmp1, tmp2;
     double slope, yint;
 
     for(i = 0 ; i < n ; i++) {
-	z[i] = x[i];
-	w[i] = y[i];
+        z[i] = x[i];
+        w[i] = y[i];
+        indx[i] = i;
     }
-    R_rsort(z, n);/* z = ordered abscissae */
+    rsort_with_index(z, indx, n);/* z = ordered abscissae */
 
-    tmp1 = z[il(n, 1./6.)];
-    tmp2 = z[iu(n, 1./6.)];	xb = 0.5*(tmp1+tmp2);
-
-    tmp1 = z[il(n, 2./6.)];
-    tmp2 = z[iu(n, 2./6.)];	x1 = 0.5*(tmp1+tmp2);
-
-    tmp1 = z[il(n, 4./6.)];
-    tmp2 = z[iu(n, 4./6.)];	x2 = 0.5*(tmp1+tmp2);
-
-    tmp1 = z[il(n, 5./6.)];
-    tmp2 = z[iu(n, 5./6.)];	xt = 0.5*(tmp1+tmp2);
+    k=(n+1)/3;
+    tmp1 = z[il(k, 0.5)];
+    tmp2 = z[iu(k, 0.5)];
+    /* xb := Median(first third of x) */
+    xb = 0.5*(tmp1+tmp2);
+
+    tmp1 = z[n-k+il(k, 0.5)];
+    tmp2 = z[n-k+iu(k, 0.5)];
+    /* xt := Median(last third of x) */
+    xt = 0.5*(tmp1+tmp2);
 
     slope = 0.;
 
     for(j = 1 ; j <= 1 ; j++) {
-	/* yb := Median(y[i]; x[i] <= quantile(x, 1/3) */
-	k = 0;
-	for(i = 0 ; i < n ; i++)
-	    if(x[i] <= x1)
-		z[k++] = w[i];
-	R_rsort(z, k);
-	yb = 0.5 * (z[il(k, 0.5)] + z[iu(k, 0.5)]);
-
-	/* yt := Median(y[i]; x[i] >= quantile(x, 2/3) */
-	k = 0;
-	for(i = 0 ; i < n ; i++)
-	    if(x[i] >= x2)
-		z[k++] = w[i];
-	R_rsort(z,k);
-	yt = 0.5 * (z[il(k, 0.5)] + z[iu(k, 0.5)]);
-
-	slope += (yt - yb)/(xt - xb);
-	for(i = 0 ; i < n ; i++) {
-	    z[i] = y[i] - slope*x[i];
-	    /* never used: w[i] = z[i]; */
-	}
-	R_rsort(z,n);
-	yint = 0.5 * (z[il(n, 0.5)] + z[iu(n, 0.5)]);
+        /* yb := Median(y[i]; x[i] in first third of x) */
+        for(i = 0 ; i < k ; i++)
+            z[i] = w[indx[i]];
+        R_rsort(z, k);
+        yb = 0.5 * (z[il(k, 0.5)] + z[iu(k, 0.5)]);
+
+        /* yt := Median(y[i]; x[i] in last third of x */
+        for(i = 0 ; i < k ; i++)
+            z[i] = w[indx[n-k+i]];
+        R_rsort(z,k);
+        yt = 0.5 * (z[il(k, 0.5)] + z[iu(k, 0.5)]);
+
+        slope += (yt - yb)/(xt - xb);
+        for(i = 0 ; i < n ; i++) {
+            z[i] = y[i] - slope*x[i];
+            /* never used: w[i] = z[i]; */
+        }
+        R_rsort(z,n);
+        yint = 0.5 * (z[il(n, 0.5)] + z[iu(n, 0.5)]);
     }
     for( i = 0 ; i < n ; i++ ) {
-	w[i] = yint + slope*x[i];
-	z[i] = y[i] - w[i];
+        w[i] = yint + slope*x[i];
+        z[i] = y[i] - w[i];
     }
     coef[0] = yint;
     coef[1] = slope;
 }
 
-void tukeyline0(double *x, double *y, double *z, double *w, int *n,
-	       double *coef)
+void tukeyline0(double *x, double *y, double *z, double *w, int *indx, int *n,
+           double *coef)
 {
-    line(x, y, z, w, *n, coef);
+    line(x, y, z, w, indx, *n, coef);
 }
 
 SEXP tukeyline(SEXP x, SEXP y, SEXP call)
@@ -127,7 +124,8 @@
     SET_VECTOR_ELT(ans, 2, res);
     SEXP fit = allocVector(REALSXP, n);
     SET_VECTOR_ELT(ans, 3, fit);
-    line(REAL(x), REAL(y), REAL(res), REAL(fit), n, REAL(coef));
+    SEXP indx = allocVector(INTSXP, n);
+    line(REAL(x), REAL(y), REAL(res), REAL(fit), INTEGER(indx), n, REAL(coef));
     UNPROTECT(1);
     return ans;
 }

From maechler at stat.math.ethz.ch  Wed May 31 22:00:18 2017
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 31 May 2017 22:00:18 +0200
Subject: [Rd] stats::line() does not produce correct Tukey line when n
 mod 6 is 2 or 3
In-Reply-To: <001b1eec-e582-3d29-2b65-107957f57cf3@insa-toulouse.fr>
References: <CAN7_cLjefc605D1ATS__MOV4gN7fKDBo5Nx_CNB_B9cDbAxc6w@mail.gmail.com>
 <ffc3e473-622d-796f-a06a-6e1f92f8ab5f@gmail.com>
 <CAN7_cLjmQFtZf9z-qJKdWRCaRY2uxfP8k5=QJsptz3U+TtP8oA@mail.gmail.com>
 <CAN7_cLjbz6S8u79jtx52XgfD1LcpO8CePYUhrunYqrzXHMMN8A@mail.gmail.com>
 <024A7E7D-45F8-4C95-A9BD-DBEB1D085BFA@gmail.com>
 <94b65a5b-da9e-ec5c-e428-7c5dbf175ec1@insa-toulouse.fr>
 <8c167bbb-130d-3798-a9e8-24c4377db313@insa-toulouse.fr>
 <d10ebb8c-0c49-5b0e-5b3c-b06073bfe447@insa-toulouse.fr>
 <22829.8246.695545.320438@stat.math.ethz.ch>
 <396781eb-1712-aa54-96f2-a6c4df0284f4@insa-toulouse.fr>
 <22829.41724.818300.640880@stat.math.ethz.ch>
 <c9af2c00-3653-29f8-7c6a-5a08d6de498e@insa-toulouse.fr>
 <CAO1zAVb8faubH7FrBfsx8c04xsZce2p2Ei9NaBELrfn2Q8JMEA@mail.gmail.com>
 <055b3d48-5f94-d087-74b8-b511e6c45baf@insa-toulouse.fr>
 <CAO1zAVa3=dA_atBNxj9kAX_Rh=5pCMiLK7BAuwq9hhY6KACQag@mail.gmail.com>
 <8fc861d2-1f45-f171-b135-000a668f73b7@insa-toulouse.fr>
 <001b1eec-e582-3d29-2b65-107957f57cf3@insa-toulouse.fr>
Message-ID: <22831.8402.627874.808222@stat.math.ethz.ch>

>>>>> Serguei Sokol <sokol at insa-toulouse.fr>
>>>>>     on Wed, 31 May 2017 18:46:34 +0200 writes:

    > Le 31/05/2017 ? 17:30, Serguei Sokol a ?crit :
    >> 
    >> More thorough reading revealed that I have overlooked this phrase in the
    >> line's doc: "left and right /thirds/ of the data" (emphasis is mine).
    > Oops. I have read the first ref returned by google and it happened to be
    > tibco's doc, not the R's one. The layout is very similar hence my mistake.
    > The latter does not mention "thirds" but ...
    > Anyway, here is a new line's patch which still gives a result slightly different
    > form MMline(). The slope is the same but not the intercept.
    > What are the exact terms for intercept calculation that should be implemented?

    > Serguei.

Sorry Serguei,   I have new version of line.c  since yesterday,
and will not be disturbed anymore.

Note that I *did* give the litterature, and it seems most
discussants don't have paper books in physical libraries anymore;
In this case, interestingly, you need one of those I think -
almost everything I found online did not have the exact details.

Peter Dalgaard definitely was right that Tukey did not use
quantiles at all, and notably did *not* define the three groups
via   {i;  x_i <= x_L}  and {i; x_i >= X_R}  which (as I think
you noticed) may make the groups quite unbalanced in case of duplicated x's.

But then, for now I had decided to fix the bug (namely computing
the x-medians wrongly as you diagnosed correctly(!) -- but your
first 2 patches only fixed partly) *and* go at least one step in
the direction of Tukey's original, namely by allowing iteration via a new 'iter' argument.

I have also updated the help page to document what  line()  has
been computing all these years {apart from the bug which
typically shows for non-equidistant x[]}.

We could also consider to eventually add a new   'method = <string>'
argument to line()  one version of which would continue to
compute the current solution, another would compute the one
corresponding to Velleman & Hoaglin (1981)'s  FORTRAN
implementation (which had to be corrected for some infinite-loop
cases!)... not in the close future though


Given all this discussions here, I think I should commit what I
currently have  ASAP.

Martin

    > x[DELETED ATTACHMENT line.c.patch2, plain text]


