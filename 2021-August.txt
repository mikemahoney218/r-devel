From dom|n|c@comto|@ @end|ng |rom gm@||@com  Mon Aug  2 08:36:27 2021
From: dom|n|c@comto|@ @end|ng |rom gm@||@com (Dominic Comtois)
Date: Mon, 2 Aug 2021 02:36:27 -0400
Subject: [Rd] I changed my vignette's file name to lowercase,
 then realized the url was case-sensitive
Message-ID: <CAEfsz7oeTPcOBWegHUz+GEu5mb-gR43c9W4rzXtWWqm9YAe74w@mail.gmail.com>

I changed my "Introduction.html" vignette's name to "introduction.html",
realizing only after the fact that CRAN's URLs are case sensitive.

Would the solution of adding to my package's source a new Introduction.html
file pointing to introduction.html using a <meta http-equiv="refresh" ...>
be a viable one? Or is there maybe another, better solution?

Thanks in advance

Dominic Comtois, summarytools author & maintainer

	[[alternative HTML version deleted]]


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Mon Aug  2 11:59:49 2021
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Mon, 2 Aug 2021 11:59:49 +0200
Subject: [Rd] I changed my vignette's file name to lowercase,
 then realized the url was case-sensitive
In-Reply-To: <CAEfsz7oeTPcOBWegHUz+GEu5mb-gR43c9W4rzXtWWqm9YAe74w@mail.gmail.com>
References: <CAEfsz7oeTPcOBWegHUz+GEu5mb-gR43c9W4rzXtWWqm9YAe74w@mail.gmail.com>
Message-ID: <24839.49685.969972.970703@stat.math.ethz.ch>

Dear Dominic,

This is the wrong mailing list for such questions
Do use 'R-help' or 'R-package-devel' instead, please.

(and also please do use  __plain text__
 instead of  "formatted" / "rich text" / ... e-mail  )

Best,
Martin Maechler


>>>>> Dominic Comtois 
>>>>>     on Mon, 2 Aug 2021 02:36:27 -0400 writes:

    > I changed my "Introduction.html" vignette's name to
    > "introduction.html", realizing only after the fact that
    > CRAN's URLs are case sensitive.

    > Would the solution of adding to my package's source a new
    > Introduction.html file pointing to introduction.html using
    > a <meta http-equiv="refresh" ...> be a viable one? Or is
    > there maybe another, better solution?

    > Thanks in advance

    > Dominic Comtois, summarytools author & maintainer

    > 	[[alternative HTML version deleted]]

    > ______________________________________________
    > R-devel at r-project.org mailing list
    > https://stat.ethz.ch/mailman/listinfo/r-devel


From pd@|gd @end|ng |rom gm@||@com  Mon Aug  2 12:08:49 2021
From: pd@|gd @end|ng |rom gm@||@com (peter dalgaard)
Date: Mon, 2 Aug 2021 12:08:49 +0200
Subject: [Rd] I changed my vignette's file name to lowercase,
 then realized the url was case-sensitive
In-Reply-To: <24839.49685.969972.970703@stat.math.ethz.ch>
References: <CAEfsz7oeTPcOBWegHUz+GEu5mb-gR43c9W4rzXtWWqm9YAe74w@mail.gmail.com>
 <24839.49685.969972.970703@stat.math.ethz.ch>
Message-ID: <7612B9A9-6497-4428-BCA4-2012DEBCA1DA@gmail.com>

R-help would surely be wrong, no? (R-package-devel is optimal, I agree)

- Peter

> On 2 Aug 2021, at 11:59 , Martin Maechler <maechler at stat.math.ethz.ch> wrote:
> 
> Dear Dominic,
> 
> This is the wrong mailing list for such questions
> Do use 'R-help' or 'R-package-devel' instead, please.
> 
> (and also please do use  __plain text__
> instead of  "formatted" / "rich text" / ... e-mail  )
> 
> Best,
> Martin Maechler
> 
> 
>>>>>> Dominic Comtois 
>>>>>>    on Mon, 2 Aug 2021 02:36:27 -0400 writes:
> 
>> I changed my "Introduction.html" vignette's name to
>> "introduction.html", realizing only after the fact that
>> CRAN's URLs are case sensitive.
> 
>> Would the solution of adding to my package's source a new
>> Introduction.html file pointing to introduction.html using
>> a <meta http-equiv="refresh" ...> be a viable one? Or is
>> there maybe another, better solution?
> 
>> Thanks in advance
> 
>> Dominic Comtois, summarytools author & maintainer
> 
>> 	[[alternative HTML version deleted]]
> 
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From dom|n|c@comto|@ @end|ng |rom gm@||@com  Mon Aug  2 13:39:42 2021
From: dom|n|c@comto|@ @end|ng |rom gm@||@com (Dominic Comtois)
Date: Mon, 2 Aug 2021 07:39:42 -0400
Subject: [Rd] I changed my vignette's file name to lowercase,
 then realized the url was case-sensitive
In-Reply-To: <7612B9A9-6497-4428-BCA4-2012DEBCA1DA@gmail.com>
References: <CAEfsz7oeTPcOBWegHUz+GEu5mb-gR43c9W4rzXtWWqm9YAe74w@mail.gmail.com>
 <24839.49685.969972.970703@stat.math.ethz.ch>
 <7612B9A9-6497-4428-BCA4-2012DEBCA1DA@gmail.com>
Message-ID: <CAEfsz7pu_x-LpYfbsx=VWsgx_fBN8sTMJEmRPu41mmniZoAs6A@mail.gmail.com>

Oops, indeed I intended to use R-package-devel.

Thx & have a nice day

	[[alternative HTML version deleted]]


From kry|ov@r00t @end|ng |rom gm@||@com  Thu Aug  5 19:42:23 2021
From: kry|ov@r00t @end|ng |rom gm@||@com (Ivan Krylov)
Date: Thu, 5 Aug 2021 19:42:23 +0200
Subject: [Rd] \Sexpr[results=hide] produces \verb{ newlines }
In-Reply-To: <24837.44825.296961.407663@stat.math.ethz.ch>
References: <20210729174838.658c0da7@trisector>
 <24837.44825.296961.407663@stat.math.ethz.ch>
Message-ID: <20210805194223.5e3cbdec@trisector>

Hello Martin,

On Sat, 31 Jul 2021 22:14:17 +0200
Martin Maechler <maechler at stat.math.ethz.ch> wrote:

> I have implemented a version of your patch in my local copy of
> R-devel and tested your example, also with  Rd2latex() ..
> interestingly   Rd2txt()  does not produce the extra new lines
> even without your patch.

That's interesting, indeed. I think that flushBuffer() is responsible
for collapsing multiple blank lines into one in that case, but they do
get preserved in the buffer up to before that.

> I plan to commit your proposal after the weekend unless has 
> reasons against that.

Thanks for the review!

-- 
Best regards,
Ivan


From Andre@G||||bert @end|ng |rom chu-rouen@|r  Sat Aug  7 12:40:24 2021
From: Andre@G||||bert @end|ng |rom chu-rouen@|r (GILLIBERT, Andre)
Date: Sat, 7 Aug 2021 10:40:24 +0000
Subject: [Rd] Redundant source code for random number generation
Message-ID: <73d3aa5257b64e2a8b50c1801edf6ae7@chu-rouen.fr>

Dear R developers,


When trying to fix poor performances of the runif() function (I can easily make it three times faster if you are interested in performance patches, maybe six times faster with a bit of refactoring of R source code), I noticed some redundant code in R source code (R-devel of 2021-08-05).

Indeed, the family of random number generation functions (runif, rnorm, rchisq, rbeta, rbinom, etc.) is implemented via Internal functions described in src/main/names.c and implemented as do_random1, do_random2 and do_random3 in src/main/random.c.


They are also reimplemented in src/library/stats/src/random.c in three main functions (random1, random2, random3) that will eventually be stored in a dynamic library (stats.so or stats.dll).


For instance, the stats::runif R function is implemented as:

function (n, min = 0, max = 1)
.Call(C_runif, n, min, max)


but could equivalently be implemented as:

function(n, min = 0, max = 1)

.Internal(runif(n, min, max))


The former calls the src/library/stats/src/random.c implementation (in stats.so or stats.dll) while the latter would call the src/main/random.c implementation (in the main R binary).


The two implementations (src/main/random.c and src/library/stats/src/random.c) are similar but slightly different on small details. For instance, rbinom always return a vector of doubles (REAL) in src/main/random.c while it tries to return a vector of integers in src/library/stats/src/random.c, unless the integers are too large to fit in an INT.


I see no obvious reason of maintaining both source codes. Actually the src/main/random.c seems to be unused in normal R programs. There could be some weird programs that use the .Internal call, but I do not think that there are many.


There are several strategies to merge both, but I want some feedback of people who know well the R source code before proposing patches.


--

Sincerely

Andr? GILLIBERT

	[[alternative HTML version deleted]]


From b@row||ng@on @end|ng |rom |@nc@@ter@@c@uk  Mon Aug  9 16:06:34 2021
From: b@row||ng@on @end|ng |rom |@nc@@ter@@c@uk (Barry Rowlingson)
Date: Mon, 9 Aug 2021 15:06:34 +0100
Subject: [Rd] attach "warning" is a message
Message-ID: <CANVKczPU53gOHxFxdaZZE92jDrhx4zKXueUWeeDYciQOjZoT4g@mail.gmail.com>

If I mask something via `attach`:

> d = data.frame(x=1:10)
> x=1
> attach(d)
The following object is masked _by_ .GlobalEnv:

    x
>

I get that message. The documentation for `attach` uses the phrase
"warnings", although the message isn't coming from `warning()`:

warn.conflicts: logical.  If ?TRUE?, warnings are printed about
          ?conflicts? from attaching the database, unless that database
          contains an object ?.conflicts.OK?.  A conflict is a function
          masking a function, or a non-function masking a non-function.

and so you can't trap them with options(warn=...) and so on. This sent me
briefly down the wrong track while trying to figure out why R was showing a
masking error in one context but not another - I wondered if I'd supressed
warning()s in the other context.

Personally I'd like these messages to be coming from warning() since that
seems the appropriate way to warn someone they've done something which
might have unwanted effects. But fixing the documentation to say "If
?TRUE?, *messages* are printed" is probably less likely to break existing
code.

Happy to add something to bugzilla if anyone thinks I'm not being overly
pedantic here.

Barry

	[[alternative HTML version deleted]]


From @|mon@urb@nek @end|ng |rom R-project@org  Tue Aug 10 01:28:19 2021
From: @|mon@urb@nek @end|ng |rom R-project@org (Simon Urbanek)
Date: Tue, 10 Aug 2021 11:28:19 +1200
Subject: [Rd] attach "warning" is a message
In-Reply-To: <CANVKczPU53gOHxFxdaZZE92jDrhx4zKXueUWeeDYciQOjZoT4g@mail.gmail.com>
References: <CANVKczPU53gOHxFxdaZZE92jDrhx4zKXueUWeeDYciQOjZoT4g@mail.gmail.com>
Message-ID: <6F805968-4234-4100-91FA-5CC1E6BC8347@R-project.org>


Barry,

it is not a warning nor plain output, it is a message, so you can use 

> d = data.frame(x=1:10)
> x=1
> suppressMessages(attach(d))
>

Looking at the history, this used to be cat() but got changed to a message in R 3.2.0 (r65385, CCIng Martin in case he remembers the rationale for warning vs message). I don't know for sure why it is not a warning, but I can see that it is more in the line with informative messages (like package masking) as opposed to a warning - in fact the change suggest that the intention was to synchonize both. That said, I guess the two options are to clarify the documentation (also in library()) or change to a warning - not sure what the consequences of the latter would be.

Cheers,
Simon



> On 10/08/2021, at 2:06 AM, Barry Rowlingson <b.rowlingson at lancaster.ac.uk> wrote:
> 
> If I mask something via `attach`:
> 
>> d = data.frame(x=1:10)
>> x=1
>> attach(d)
> The following object is masked _by_ .GlobalEnv:
> 
>    x
>> 
> 
> I get that message. The documentation for `attach` uses the phrase
> "warnings", although the message isn't coming from `warning()`:
> 
> warn.conflicts: logical.  If ?TRUE?, warnings are printed about
>          ?conflicts? from attaching the database, unless that database
>          contains an object ?.conflicts.OK?.  A conflict is a function
>          masking a function, or a non-function masking a non-function.
> 
> and so you can't trap them with options(warn=...) and so on. This sent me
> briefly down the wrong track while trying to figure out why R was showing a
> masking error in one context but not another - I wondered if I'd supressed
> warning()s in the other context.
> 
> Personally I'd like these messages to be coming from warning() since that
> seems the appropriate way to warn someone they've done something which
> might have unwanted effects. But fixing the documentation to say "If
> ?TRUE?, *messages* are printed" is probably less likely to break existing
> code.
> 
> Happy to add something to bugzilla if anyone thinks I'm not being overly
> pedantic here.
> 
> Barry
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 


From pd@|gd @end|ng |rom gm@||@com  Tue Aug 10 10:18:08 2021
From: pd@|gd @end|ng |rom gm@||@com (Peter Dalgaard)
Date: Tue, 10 Aug 2021 10:18:08 +0200
Subject: [Rd] R 4.1.1 is released
Message-ID: <8A0CEFBD-74CF-4928-A587-1C58811F799B@gmail.com>

The build system rolled up R-4.1.1.tar.gz (codename "Kick Things") this morning.

The list below details the changes in this release. 

You can get the source code from

https://cran.r-project.org/src/base/R-4/R-4.1.1.tar.gz

or wait for it to be mirrored at a CRAN site nearer to you.

Binaries for various platforms will appear in due course.


For the R Core Team,

Peter Dalgaard

These are the checksums (md5 and SHA-256) for the freshly created files, in case you wish
to check that they are uncorrupted:

MD5 (AUTHORS) = da5e7c699a83608d0f1e39c458d9fc56
MD5 (COPYING) = eb723b61539feef013de476e68b5c50a
MD5 (COPYING.LIB) = a6f89e2100d9b6cdffcea4f398e37343
MD5 (FAQ) = 6094024214a482c0d01d2ab2adca4b3f
MD5 (INSTALL) = 7893f754308ca31f1ccf62055090ad7b
MD5 (NEWS) = fbc4810ff26ebcec514ebaa1c1909ad7
MD5 (NEWS.0) = bfcd7c147251b5474d96848c6f57e5a8
MD5 (NEWS.1) = eb78c4d053ec9c32b815cf0c2ebea801
MD5 (NEWS.2) = a767f7809324c73c49eaff47d14bce81
MD5 (NEWS.3) = e55ed2c8a547b827b46e08eb7137ba23
MD5 (R-latest.tar.gz) = c278cfeb85b1564540ab214e45fe68d9
MD5 (README) = f468f281c919665e276a1b691decbbe6
MD5 (RESOURCES) = a79b9b338cab09bd665f6b62ac6f455b
MD5 (THANKS) = 251d20510bfc3cc93b82c5a99f7efcc6
MD5 (VERSION-INFO.dcf) = 34443dff7fcea700c8ec4740e5804374
MD5 (R-4/R-4.1.1.tar.gz) = c278cfeb85b1564540ab214e45fe68d9

9704a7d96c350a48417ef215888a29f1993ee5dec1b73cb95755e8625b860200  AUTHORS
e6d6a009505e345fe949e1310334fcb0747f28dae2856759de102ab66b722cb4  COPYING
6095e9ffa777dd22839f7801aa845b31c9ed07f3d6bf8a26dc5d2dec8ccc0ef3  COPYING.LIB
2894e7a88634a08c05bfafb8a694a26b635e4042160aab46fa6a0f4eb68ea91e  FAQ
f87461be6cbaecc4dce44ac58e5bd52364b0491ccdadaf846cb9b452e9550f31  INSTALL
e8bdaf546cf65fdc5bf2a81fa5334572886ff2f1317ec6cdc9e61d6de3532dd4  NEWS
4e21b62f515b749f80997063fceab626d7258c7d650e81a662ba8e0640f12f62  NEWS.0
12b30c724117b1b2b11484673906a6dcd48a361f69fc420b36194f9218692d01  NEWS.1
ba74618bc3f4c0e336dca13d472402a1863d12ba6f7f91a1782bc469ee986f6d  NEWS.2
1910a2405300b9bc7c76beeb0753a5249cf799afe175ce28f8d782fab723e012  NEWS.3
515e03265752257d0b7036f380f82e42b46ed8473f54f25c7b67ed25bbbdd364  R-latest.tar.gz
2fdd3e90f23f32692d4b3a0c0452f2c219a10882033d1774f8cadf25886c3ddc  README
8b7d3856100220f4555d4d57140829f2e81c27eccec5b441f5dce616e9ec9061  RESOURCES
c9c7cb32308b4e560a22c858819ade9de524a602abd4e92d1c328c89f8037d73  THANKS
02686ea05e64304a755bf776cdeeadafd2c5017a13f9203f1db9278287c81aa6  VERSION-INFO.dcf
515e03265752257d0b7036f380f82e42b46ed8473f54f25c7b67ed25bbbdd364  R-4/R-4.1.1.tar.gz

This is the relevant part of the NEWS file

CHANGES IN R 4.1.1:

  NEW FEATURES:

    * require(pkg, quietly = TRUE) is quieter and in particular does
      not warn if the package is not found.

  DEPRECATED AND DEFUNCT:

    * Use of ftp:// URIs should be regarded as deprecated, with
      on-going support confined to method = "libcurl" and not routinely
      tested.  (Nowadays no major browser supports them.)

    * The non-default method = "internal" is deprecated for http:// and
      ftp:// URIs for both download.file and url.

    * On Windows, method = "wininet" is deprecated for http://,
      https:// and ftp:// URIs for both download.file and url.  (A
      warning is only given for ftp://.)

      For ftp:// URIs the default method is now "libcurl" if available
      (which it is on CRAN builds).

      method = "wininet" remains the default for http:// and https://
      URIs but if libcurl is available, using method = "libcurl" is
      preferred.

  INSTALLATION:

    * make check now works also without a LaTeX installation.  (Thanks
      to Sebastian Meyer's PR#18103.)

  BUG FIXES:

    * make check-devel works again in an R build configured with
      --without-recommended-packages.

    * qnbinom(p, size, mu) for large size/mu is correct now in a range
      of cases (PR#18095); similarly for the (size, prob)
      parametrization of the negative binomial.  Also qpois() and
      qbinom() are better and or faster for extreme cases.  The
      underlying C code has been modularized and is common to all four
      cases of discrete distributions.

    * gap.axis is now part of the axis() arguments which are passed
      from bxp(), and hence boxplot().  (Thanks to Martin Smith's
      report and suggestions in PR#18109.)

    * .First and .Last can again be set from the site profile.

    * seq.int(from, to, *) and seq.default(..) now work better in large
      range cases where from-to is infinite where the two boundaries
      are finite.

    * all.equal(x,y) now returns TRUE correctly also when several
      entries of abs(x) and abs(y) are close to .Machine$double.xmax,
      the largest finite numeric.

    * model.frame() now clears the object bit when removing the class
      attribute of a value via na.action (PR#18100).

    * charClass() now works with multi-character strings on Windows
      (PR#18104, fixed by Bill Dunlap).

    * encodeString() on Solaris now works again in Latin-1 encoding on
      characters represented differently in UTF-8.  Support for
      surrogate pairs on Solaris has been improved.

    * file.show() on Windows now works with non-ASCII path names
      representable in the current native encoding (PR#18132).

    * Embedded R on Windows can now find R home directory via the
      registry even when installed only for the current user
      (PR#18135).

    * pretty(x) with finite x now returns finite values also in the
      case where the extreme x values are close in size to the maximal
      representable number .Machine$double.xmax.

      Also, it's been tweaked for very small ranges and when a boundary
      is close (or equal) to zero; e.g., pretty(c(0,1e-317)) no longer
      has negative numbers, currently still warning about a very small
      range, and pretty(2^-(1024 - 2^-1/(c(24,10)))) is more accurate.

    * The error message for not finding vignette files when weaving has
      correct file sizes now. (Thanks to Sebastian Meyer's PR#18154.)

    * dnbinom(20, <large>, 1) now correctly gives 0, and similar cases
      are more accurate with underflow precaution.  (Reported by
      Francisco Vera Alcivar in PR#18072.)

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Tue Aug 10 11:50:01 2021
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Tue, 10 Aug 2021 11:50:01 +0200
Subject: [Rd] attach "warning" is a message
In-Reply-To: <6F805968-4234-4100-91FA-5CC1E6BC8347@R-project.org>
References: <CANVKczPU53gOHxFxdaZZE92jDrhx4zKXueUWeeDYciQOjZoT4g@mail.gmail.com>
 <6F805968-4234-4100-91FA-5CC1E6BC8347@R-project.org>
Message-ID: <24850.19401.473687.481057@stat.math.ethz.ch>

>>>>> Simon Urbanek 
>>>>>     on Tue, 10 Aug 2021 11:28:19 +1200 writes:

    > Barry,

    > it is not a warning nor plain output, it is a message, so you can use 

    >> d = data.frame(x=1:10)
    >> x=1
    >> suppressMessages(attach(d))
    >> 

    > Looking at the history, this used to be cat() but got changed to a message in R 3.2.0 (r65385, CCIng Martin in case he remembers the rationale for warning vs message). I don't know for sure why it is not a warning, but I can see that it is more in the line with informative messages (like package masking) as opposed to a warning - in fact the change suggest that the intention was to synchonize both. That said, I guess the two options are to clarify the documentation (also in library()) or change to a warning - not sure what the consequences of the latter would be.

    > Cheers,
    > Simon

Thank you, Simon.
One rationale back then was that  message() *is* in some sense
closer to cat()  than warning() is (and indeed, to synchronize
with the conflict messages from `library(.)` or `require(.)`.
Also, I would say that a message is more appropriate here than a
warning when someone explicitly attach()es something to the
search() path (s)he may get a notice about masking but to be
warned is too strong {warnings being made into errors in some setups}.

So I'd propose to only update the documentation i.e.
help(attach).

Martin


    >> On 10/08/2021, at 2:06 AM, Barry Rowlingson <b.rowlingson at lancaster.ac.uk> wrote:
    >> 
    >> If I mask something via `attach`:
    >> 
    >>> d = data.frame(x=1:10)
    >>> x=1
    >>> attach(d)
    >> The following object is masked _by_ .GlobalEnv:
    >> 
    >> x
    >>> 
    >> 
    >> I get that message. The documentation for `attach` uses the phrase
    >> "warnings", although the message isn't coming from `warning()`:
    >> 
    >> warn.conflicts: logical.  If ?TRUE?, warnings are printed about
    >> ?conflicts? from attaching the database, unless that database
    >> contains an object ?.conflicts.OK?.  A conflict is a function
    >> masking a function, or a non-function masking a non-function.
    >> 
    >> and so you can't trap them with options(warn=...) and so on. This sent me
    >> briefly down the wrong track while trying to figure out why R was showing a
    >> masking error in one context but not another - I wondered if I'd supressed
    >> warning()s in the other context.
    >> 
    >> Personally I'd like these messages to be coming from warning() since that
    >> seems the appropriate way to warn someone they've done something which
    >> might have unwanted effects. But fixing the documentation to say "If
    >> ?TRUE?, *messages* are printed" is probably less likely to break existing
    >> code.
    >> 
    >> Happy to add something to bugzilla if anyone thinks I'm not being overly
    >> pedantic here.
    >> 
    >> Barry
    >> 
    >> [[alternative HTML version deleted]]
    >> 
    >> ______________________________________________
    >> R-devel at r-project.org mailing list
    >> https://stat.ethz.ch/mailman/listinfo/r-devel
    >>


From ggrothend|eck @end|ng |rom gm@||@com  Tue Aug 10 21:33:20 2021
From: ggrothend|eck @end|ng |rom gm@||@com (Gabor Grothendieck)
Date: Tue, 10 Aug 2021 15:33:20 -0400
Subject: [Rd] problem with pipes, textConnection and read.dcf
Message-ID: <CAP01uRn+6zWEgmyGfNTb3aJJoLCy8mbhJFj6SiWWBktmXwsncA@mail.gmail.com>

This gives an error bit if the first gsub line is commented out then there is no
error even though it is equivalent code.

  L <- c("Variable:id", "Length:112630     ")

  L |>
    gsub(pattern = " ", replacement = "") |>
    gsub(pattern = " ", replacement = "") |>
    textConnection() |>
    read.dcf()
  ## Error in textConnection(gsub(gsub(L, pattern = " ", replacement = ""),  :
  ##  argument 'object' must deparse to a single character string

That is this works:

  L |>
    # gsub(pattern = " ", replacement = "") |>
    gsub(pattern = " ", replacement = "") |>
    textConnection() |>
    read.dcf()
  ##      Variable Length
  ## [1,] "id"     "112630"

  R.version.string
  ## [1] "R version 4.1.0 RC (2021-05-16 r80303)"
  win.version()
  ## [1] "Windows 10 x64 (build 19042)"

-- 
Statistics & Software Consulting
GKX Group, GKX Associates Inc.
tel: 1-877-GKX-GROUP
email: ggrothendieck at gmail.com


From pd@|gd @end|ng |rom gm@||@com  Tue Aug 10 22:00:16 2021
From: pd@|gd @end|ng |rom gm@||@com (peter dalgaard)
Date: Tue, 10 Aug 2021 22:00:16 +0200
Subject: [Rd] problem with pipes, textConnection and read.dcf
In-Reply-To: <CAP01uRn+6zWEgmyGfNTb3aJJoLCy8mbhJFj6SiWWBktmXwsncA@mail.gmail.com>
References: <CAP01uRn+6zWEgmyGfNTb3aJJoLCy8mbhJFj6SiWWBktmXwsncA@mail.gmail.com>
Message-ID: <A454DA3A-BD8D-418C-90FC-E5DE1CBB0725@gmail.com>

It's not a pipe issue:

> textConnection(gsub(gsub(L, pattern = " ", replacement = ""), pattern = " ", replacement = ""))
Error in textConnection(gsub(gsub(L, pattern = " ", replacement = ""),  : 
  argument 'object' must deparse to a single character string
> textConnection(gsub(L, pattern = " ", replacement = ""))
A connection with                                                          
description "gsub(L, pattern = \" \", replacement = \"\")"
class       "textConnection"                              
mode        "r"                                           
text        "text"                                        
opened      "opened"                                      
can read    "yes"                                         
can write   "no"                                          

I suppose the culprit is that the deparse(substitute(...)) construct in the definition of textConnection() can generate multiple lines if the object expression gets complicated.

> textConnection
function (object, open = "r", local = FALSE, name = deparse(substitute(object)), 
    encoding = c("", "bytes", "UTF-8")) 

This also suggests that setting name=something might be a cure.

-pd


> On 10 Aug 2021, at 21:33 , Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
> 
> This gives an error bit if the first gsub line is commented out then there is no
> error even though it is equivalent code.
> 
>  L <- c("Variable:id", "Length:112630     ")
> 
>  L |>
>    gsub(pattern = " ", replacement = "") |>
>    gsub(pattern = " ", replacement = "") |>
>    textConnection() |>
>    read.dcf()
>  ## Error in textConnection(gsub(gsub(L, pattern = " ", replacement = ""),  :
>  ##  argument 'object' must deparse to a single character string
> 
> That is this works:
> 
>  L |>
>    # gsub(pattern = " ", replacement = "") |>
>    gsub(pattern = " ", replacement = "") |>
>    textConnection() |>
>    read.dcf()
>  ##      Variable Length
>  ## [1,] "id"     "112630"
> 
>  R.version.string
>  ## [1] "R version 4.1.0 RC (2021-05-16 r80303)"
>  win.version()
>  ## [1] "Windows 10 x64 (build 19042)"
> 
> -- 
> Statistics & Software Consulting
> GKX Group, GKX Associates Inc.
> tel: 1-877-GKX-GROUP
> email: ggrothendieck at gmail.com
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From iuke-tier@ey m@iii@g oii uiow@@edu  Tue Aug 10 22:14:11 2021
From: iuke-tier@ey m@iii@g oii uiow@@edu (iuke-tier@ey m@iii@g oii uiow@@edu)
Date: Tue, 10 Aug 2021 15:14:11 -0500 (CDT)
Subject: [Rd] 
 [External]  problem with pipes, textConnection and read.dcf
In-Reply-To: <CAP01uRn+6zWEgmyGfNTb3aJJoLCy8mbhJFj6SiWWBktmXwsncA@mail.gmail.com>
References: <CAP01uRn+6zWEgmyGfNTb3aJJoLCy8mbhJFj6SiWWBktmXwsncA@mail.gmail.com>
Message-ID: <alpine.DEB.2.22.394.2108101452480.3514@luke-Latitude-7480>

Not an issue with pipes. The pipe just rewrites the expression to a
nested call and that is then evaluated. The call this produces is

> quote(L |>
+    gsub(pattern = " ", replacement = "") |>
+    gsub(pattern = " ", replacement = "") |>
+    textConnection() |>
+    read.dcf())
read.dcf(textConnection(gsub(gsub(L, pattern = " ", replacement = ""),
     pattern = " ", replacement = "")))

If you run that expression, or just the argument to read.dcf, then you
get the error you report. So the issue is somewhere in textConnection().
This produces a similar message:

read.dcf(textConnection(c(L, "aaaaaaaaaaaaaaaaaa", "bbbbbbbbbbbbbbbb", "cccccccccccccccc", "ddddddddddddddddddd")))

File a bug report and someone who understands the textConnection()
internals better than I do can take a look.

Best,

luke

On Tue, 10 Aug 2021, Gabor Grothendieck wrote:

> This gives an error bit if the first gsub line is commented out then there is no
> error even though it is equivalent code.
>
>  L <- c("Variable:id", "Length:112630     ")
>
>  L |>
>    gsub(pattern = " ", replacement = "") |>
>    gsub(pattern = " ", replacement = "") |>
>    textConnection() |>
>    read.dcf()
>  ## Error in textConnection(gsub(gsub(L, pattern = " ", replacement = ""),  :
>  ##  argument 'object' must deparse to a single character string
>
> That is this works:
>
>  L |>
>    # gsub(pattern = " ", replacement = "") |>
>    gsub(pattern = " ", replacement = "") |>
>    textConnection() |>
>    read.dcf()
>  ##      Variable Length
>  ## [1,] "id"     "112630"
>
>  R.version.string
>  ## [1] "R version 4.1.0 RC (2021-05-16 r80303)"
>  win.version()
>  ## [1] "Windows 10 x64 (build 19042)"
>
>

-- 
Luke Tierney
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Wed Aug 11 08:59:30 2021
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Wed, 11 Aug 2021 08:59:30 +0200
Subject: [Rd] problem with pipes, textConnection and read.dcf
In-Reply-To: <A454DA3A-BD8D-418C-90FC-E5DE1CBB0725@gmail.com>
References: <CAP01uRn+6zWEgmyGfNTb3aJJoLCy8mbhJFj6SiWWBktmXwsncA@mail.gmail.com>
 <A454DA3A-BD8D-418C-90FC-E5DE1CBB0725@gmail.com>
Message-ID: <24851.30034.457559.988756@stat.math.ethz.ch>

>>>>> peter dalgaard 
>>>>>     on Tue, 10 Aug 2021 22:00:16 +0200 writes:

    > It's not a pipe issue:

    >> textConnection(gsub(gsub(L, pattern = " ", replacement = ""), pattern = " ", replacement = ""))
    > Error in textConnection(gsub(gsub(L, pattern = " ", replacement = ""),  : 
    > argument 'object' must deparse to a single character string
    >> textConnection(gsub(L, pattern = " ", replacement = ""))
    > A connection with                                                          
    > description "gsub(L, pattern = \" \", replacement = \"\")"
    > class       "textConnection"                              
    > mode        "r"                                           
    > text        "text"                                        
    > opened      "opened"                                      
    > can read    "yes"                                         
    > can write   "no"                                          

    > I suppose the culprit is that the deparse(substitute(...)) construct in the definition of textConnection() can generate multiple lines if the object expression gets complicated.

    >> textConnection
    > function (object, open = "r", local = FALSE, name = deparse(substitute(object)), 
    > encoding = c("", "bytes", "UTF-8")) 

    > This also suggests that setting name=something might be a cure.

    > -pd

Indeed.

In R 4.0.0, I had introduced the deparse1() short cut to be used
in place of  deparse() in such cases:

NEWS has said

    ? New function deparse1() produces one string, wrapping deparse(),
      to be used typically in deparse1(substitute(*)), e.g., to fix
      PR#17671.

and the definition is a simple but useful oneliner

  deparse1 <- function (expr, collapse = " ", width.cutoff = 500L, ...) 
  paste(deparse(expr, width.cutoff, ...), collapse = collapse)


So I'm almost sure we should use  deparse1() in textConnection
(and will make check and potentially commit that unless ...)

Martin


    >> On 10 Aug 2021, at 21:33 , Gabor Grothendieck <ggrothendieck at gmail.com> wrote:
    >> 
    >> This gives an error bit if the first gsub line is commented out then there is no
    >> error even though it is equivalent code.
    >> 
    >> L <- c("Variable:id", "Length:112630     ")
    >> 
    >> L |>
    >> gsub(pattern = " ", replacement = "") |>
    >> gsub(pattern = " ", replacement = "") |>
    >> textConnection() |>
    >> read.dcf()
    >> ## Error in textConnection(gsub(gsub(L, pattern = " ", replacement = ""),  :
    >> ##  argument 'object' must deparse to a single character string
    >> 
    >> That is this works:
    >> 
    >> L |>
    >> # gsub(pattern = " ", replacement = "") |>
    >> gsub(pattern = " ", replacement = "") |>
    >> textConnection() |>
    >> read.dcf()
    >> ##      Variable Length
    >> ## [1,] "id"     "112630"
    >> 
    >> R.version.string
    >> ## [1] "R version 4.1.0 RC (2021-05-16 r80303)"
    >> win.version()
    >> ## [1] "Windows 10 x64 (build 19042)"
    >> 
    >> -- 
    >> Statistics & Software Consulting
    >> GKX Group, GKX Associates Inc.
    >> tel: 1-877-GKX-GROUP
    >> email: ggrothendieck at gmail.com
    >> 
    >> ______________________________________________
    >> R-devel at r-project.org mailing list
    >> https://stat.ethz.ch/mailman/listinfo/r-devel

    > -- 
    > Peter Dalgaard, Professor,
    > Center for Statistics, Copenhagen Business School
    > Solbjerg Plads 3, 2000 Frederiksberg, Denmark
    > Phone: (+45)38153501
    > Office: A 4.23
    > Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com

    > ______________________________________________
    > R-devel at r-project.org mailing list
    > https://stat.ethz.ch/mailman/listinfo/r-devel


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Wed Aug 11 15:15:15 2021
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Wed, 11 Aug 2021 15:15:15 +0200
Subject: [Rd] difference of m1 <- lm(f, data) and update(m1, formula=f)
In-Reply-To: <24851.40349.481350.63082@stat.math.ethz.ch>
References: <LO2P265MB2605932206CC0D6418B8D1E5DDF89@LO2P265MB2605.GBRP265.PROD.OUTLOOK.COM>
 <24851.40349.481350.63082@stat.math.ethz.ch>
Message-ID: <24851.52579.85351.888589@stat.math.ethz.ch>

I'm diverting this from R-help to R-devel,

because I'm asking / musing if and if where we should / could
change R here (see below).

>>>>> Martin Maechler on 11 Aug 2021 11:51:25 +0200

>>>>> Tim Taylor .. on 08:45:48 +0000 writes:

    >> Manipulating formulas within different models I notice the following:

    >> m1 <- lm(formula = hp ~ cyl, data = mtcars)
    >> m2 <- update(m1, formula. = hp ~ cyl)
    >> all.equal(m1, m2)
    >> #> [1] TRUE
    >> identical(m1, m2)
    >> #> [1] FALSE
    >> waldo::compare(m1, m2)
    >> #> `old$call[[2]]` is a call
    >> #> `new$call[[2]]` is an S3 object of class <formula>, a call

    >> I'm aware formulas are a form of call but what I'm unsure
    >> of is whether there is meaningful difference between the
    >> two versions of the models? 

    > A good question.
    > In principle, the promise of an update()  method should be to
    > produce the *same* result as calling the original model-creation
    > (or more generally object-creation) function call.

    > So, already with identical(), you've shown that this is not
    > quite the case for simple lm(),
    > and indeed that is a bit undesirable.

    > To answer your question re "meaningful" difference,
    > given what I say above is:
    > No, there shouldn't be any relevant difference, and if there is,
    > that may considered a bug in the respective update() method,
    > here update.lm.

    > More about this in the following  R code snippet :

Again, a repr.ex.:

---0<-------0<-------0<-------0<-------0<-------0<-------0<----

m1 <- lm(formula = hp ~ cyl, data = mtcars)
m2  <- update(m1, formula. = hp ~ cyl)
m2a <- update(m1)
identical(m1, m2a)#>  TRUE !
## ==> calling update() & explicitly specifying the formula is "the problem"

identical(m1$call, m2$call) #> [1] FALSE
noCall <- function(x) x[setdiff(names(x), "call")]
identical(noCall(m1), noCall(m2))# TRUE!
## look closer:
c1 <- m1$call
c2 <- m2$call
str(as.list(c1))
## List of 3
##  $        : symbol lm
##  $ formula: language hp ~ cyl
##  $ data   : symbol mtcars

str(as.list(c2))
## List of 3
##  $        : symbol lm
##  $ formula:Class 'formula'  language hp ~ cyl
##   .. ..- attr(*, ".Environment")=<environment: R_GlobalEnv>
##  $ data   : symbol mtcars

identical(c1[-2], c2[-2]) # TRUE ==> so, indeed the difference is *only* in the formula ( = [2]) component
f1 <- c1$formula
f2 <- c2$formula
all.equal(f1,f2) # TRUE
identical(f1,f2) # FALSE

## Note that this is typically *not* visible if the user uses
## the accessor functions they should :
identical(formula(m1), formula(m2)) # TRUE !
## and indeed, the formula() method for 'lm'  does set the environment:
stats:::formula.lm

---0<-------0<-------0<-------0<-------0<-------0<-------0<----

We know that it has been important in  R  the formulas have an
environment and that's been the only R-core recommended way to
do non-standard evaluation (!! .. but let's skip that for now !!).

OTOH we have also kept the convention that a formula without
environment implicitly means its environment
is .GlobalEnv aka globalenv().

Currently, I think formula() methods then *should* always return
a formula *with* an environment .. even though that's not
claimed in the reference, i.e., ?formula.

Also, the print() method for formulas by default does *not* show the
environment if it is .GlobalEnv, as you can see on that help
already in the "Usage" section:

     ## S3 method for class 'formula'
     print(x, showEnv = !identical(e, .GlobalEnv), ...)
     
Now, I've looked at the update() here, which is update.default()
and the source code of that currently is

update.formula <- function (old, new, ...)
{
    tmp <- .Call(C_updateform, as.formula(old), as.formula(new))
    ## FIXME?: terms.formula() with "large" unneeded attributes:
    formula(terms.formula(tmp, simplify = TRUE))
}

where the important part is the "FIXME" comment (seen in the R
sources, but no longer in the R function after installation).

My current "idea" is to formalize what we see working here:
namely allow  update.formula() to *not* set the environment of
its result *if* that environment would be .GlobalEnv ..

--> I'm starting to test my proposal
but would still be *very* glad for comments, also contradicting
ones!

Martin


From iuke-tier@ey m@iii@g oii uiow@@edu  Wed Aug 11 17:24:08 2021
From: iuke-tier@ey m@iii@g oii uiow@@edu (iuke-tier@ey m@iii@g oii uiow@@edu)
Date: Wed, 11 Aug 2021 10:24:08 -0500 (CDT)
Subject: [Rd] [External]  difference of m1 <- lm(f, data) and update(m1,
 formula=f)
In-Reply-To: <24851.52579.85351.888589@stat.math.ethz.ch>
References: <LO2P265MB2605932206CC0D6418B8D1E5DDF89@LO2P265MB2605.GBRP265.PROD.OUTLOOK.COM>
 <24851.40349.481350.63082@stat.math.ethz.ch>
 <24851.52579.85351.888589@stat.math.ethz.ch>
Message-ID: <alpine.DEB.2.22.394.2108110951070.3514@luke-Latitude-7480>

On Wed, 11 Aug 2021, Martin Maechler wrote:

> I'm diverting this from R-help to R-devel,
>
> because I'm asking / musing if and if where we should / could
> change R here (see below).
>
>>>>>> Martin Maechler on 11 Aug 2021 11:51:25 +0200
>
>>>>>> Tim Taylor .. on 08:45:48 +0000 writes:
>
>    >> Manipulating formulas within different models I notice the following:
>
>    >> m1 <- lm(formula = hp ~ cyl, data = mtcars)
>    >> m2 <- update(m1, formula. = hp ~ cyl)
>    >> all.equal(m1, m2)
>    >> #> [1] TRUE
>    >> identical(m1, m2)
>    >> #> [1] FALSE
>    >> waldo::compare(m1, m2)
>    >> #> `old$call[[2]]` is a call
>    >> #> `new$call[[2]]` is an S3 object of class <formula>, a call
>
>    >> I'm aware formulas are a form of call but what I'm unsure
>    >> of is whether there is meaningful difference between the
>    >> two versions of the models?
>
>    > A good question.
>    > In principle, the promise of an update()  method should be to
>    > produce the *same* result as calling the original model-creation
>    > (or more generally object-creation) function call.
>
>    > So, already with identical(), you've shown that this is not
>    > quite the case for simple lm(),
>    > and indeed that is a bit undesirable.
>
>    > To answer your question re "meaningful" difference,
>    > given what I say above is:
>    > No, there shouldn't be any relevant difference, and if there is,
>    > that may considered a bug in the respective update() method,
>    > here update.lm.
>
>    > More about this in the following  R code snippet :
>
> Again, a repr.ex.:
>
> ---0<-------0<-------0<-------0<-------0<-------0<-------0<----
>
> m1 <- lm(formula = hp ~ cyl, data = mtcars)
> m2  <- update(m1, formula. = hp ~ cyl)
> m2a <- update(m1)
> identical(m1, m2a)#>  TRUE !
> ## ==> calling update() & explicitly specifying the formula is "the problem"
>
> identical(m1$call, m2$call) #> [1] FALSE
> noCall <- function(x) x[setdiff(names(x), "call")]
> identical(noCall(m1), noCall(m2))# TRUE!
> ## look closer:
> c1 <- m1$call
> c2 <- m2$call
> str(as.list(c1))
> ## List of 3
> ##  $        : symbol lm
> ##  $ formula: language hp ~ cyl
> ##  $ data   : symbol mtcars
>
> str(as.list(c2))
> ## List of 3
> ##  $        : symbol lm
> ##  $ formula:Class 'formula'  language hp ~ cyl
> ##   .. ..- attr(*, ".Environment")=<environment: R_GlobalEnv>
> ##  $ data   : symbol mtcars
>
> identical(c1[-2], c2[-2]) # TRUE ==> so, indeed the difference is *only* in the formula ( = [2]) component
> f1 <- c1$formula
> f2 <- c2$formula
> all.equal(f1,f2) # TRUE
> identical(f1,f2) # FALSE
>
> ## Note that this is typically *not* visible if the user uses
> ## the accessor functions they should :
> identical(formula(m1), formula(m2)) # TRUE !
> ## and indeed, the formula() method for 'lm'  does set the environment:
> stats:::formula.lm
>
> ---0<-------0<-------0<-------0<-------0<-------0<-------0<----
>
> We know that it has been important in  R  the formulas have an
> environment and that's been the only R-core recommended way to
> do non-standard evaluation (!! .. but let's skip that for now !!).
>
> OTOH we have also kept the convention that a formula without
> environment implicitly means its environment
> is .GlobalEnv aka globalenv().
>
> Currently, I think formula() methods then *should* always return
> a formula *with* an environment .. even though that's not
> claimed in the reference, i.e., ?formula.
>
> Also, the print() method for formulas by default does *not* show the
> environment if it is .GlobalEnv, as you can see on that help
> already in the "Usage" section:
>
>     ## S3 method for class 'formula'
>     print(x, showEnv = !identical(e, .GlobalEnv), ...)
>
> Now, I've looked at the update() here, which is update.default()
> and the source code of that currently is
>
> update.formula <- function (old, new, ...)
> {
>    tmp <- .Call(C_updateform, as.formula(old), as.formula(new))
>    ## FIXME?: terms.formula() with "large" unneeded attributes:
>    formula(terms.formula(tmp, simplify = TRUE))
> }
>
> where the important part is the "FIXME" comment (seen in the R
> sources, but no longer in the R function after installation).
>
> My current "idea" is to formalize what we see working here:
> namely allow  update.formula() to *not* set the environment of
> its result *if* that environment would be .GlobalEnv ..
>
> --> I'm starting to test my proposal
> but would still be *very* glad for comments, also contradicting
> ones!

m1$call is the parsed expression for the call to lm(), so
m1$call$formula is the expression that was evaluated to produce the formula.
Typically this will be a call expression with `~` as the function.
It could also be a symbol:

> frm <- hp ~ cyl
> m3 <- lm(formula = frm, data = mtcars)
> m3$call$formula
frm

update.default is creating a new call object by putting in a new
expression for the formula argument. It so happens that putting in a
formula object actually works: The only difference between the AST for
a call of `~` and the formula such a call produces when evaluated is
the class and environment attributes the call adds, and most code that
works with expressions, like eval(), ignores attributes.

It would seem somewhat more consistent if update.default put the
expression that would produce the formula into the call (i.e. stripped
out the two attributes).

But I do not know if there is logic in base R code, never mind package
code, that takes advantage of the attributes on the formula expression
in if they are found. formula() looks in the 'terms' component so would
not be affects, but I don't know if something else might be.

Best,

luke


>
> Martin
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

-- 
Luke Tierney
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu


From d|pter|x@w@ng @end|ng |rom gm@||@com  Thu Aug 12 00:50:17 2021
From: d|pter|x@w@ng @end|ng |rom gm@||@com (Dipterix Wang)
Date: Wed, 11 Aug 2021 18:50:17 -0400
Subject: [Rd] Double to uint64_t on M1
Message-ID: <EE649952-4BB6-432A-B3CA-BD9B11DD9324@gmail.com>

Hi, 

I was trying to convert REALSXP to int64_t in C, then found that converting 2^63 is inconsistent across platforms:


On M1 ARM osx, 2^63 (double) bit converting to `int64_t` becomes 9223372036854775807
On x86_64 ubuntu server, 2^63 (double) bit converting to `int64_t` is -9223372036854775808

I was wondering if this is desired behavior to R?

Here's the code to replicate the results above.

print_bit <- Rcpp::cppFunction(r"(
SEXP print_bit(SEXP obj){

  int64_t tmp1 = *REAL0(obj);
  printf("%lld ", tmp1);

  return(R_NilValue);
}
)")

print_bit(2^63)

Thanks,
- Dipterix
	[[alternative HTML version deleted]]


From @|mon@urb@nek @end|ng |rom R-project@org  Thu Aug 12 05:52:27 2021
From: @|mon@urb@nek @end|ng |rom R-project@org (Simon Urbanek)
Date: Thu, 12 Aug 2021 15:52:27 +1200
Subject: [Rd] Double to uint64_t on M1
In-Reply-To: <EE649952-4BB6-432A-B3CA-BD9B11DD9324@gmail.com>
References: <EE649952-4BB6-432A-B3CA-BD9B11DD9324@gmail.com>
Message-ID: <C582D4F5-6218-42CC-AC63-50CAB13D977E@R-project.org>


Dipterix,

this has nothing to do with R. 2^63 is too large to be represented as singed integer, so the behavior is undefined - to quote from the C99 specs (6.3.1.4):

"If the value of the integral part cannot be represented by the integer type, the behavior is undefined."

Your subject doesn't match your question as the uint64_t conversion is well-defined and the same on both platforms, but the conversion to int64_t in undefined.

Cheers,
Simon


> On 12/08/2021, at 10:50 AM, Dipterix Wang <dipterix.wang at gmail.com> wrote:
> 
> Hi, 
> 
> I was trying to convert REALSXP to int64_t in C, then found that converting 2^63 is inconsistent across platforms:
> 
> 
> On M1 ARM osx, 2^63 (double) bit converting to `int64_t` becomes 9223372036854775807
> On x86_64 ubuntu server, 2^63 (double) bit converting to `int64_t` is -9223372036854775808
> 
> I was wondering if this is desired behavior to R?
> 
> Here's the code to replicate the results above.
> 
> print_bit <- Rcpp::cppFunction(r"(
> SEXP print_bit(SEXP obj){
> 
>  int64_t tmp1 = *REAL0(obj);
>  printf("%lld ", tmp1);
> 
>  return(R_NilValue);
> }
> )")
> 
> print_bit(2^63)
> 
> Thanks,
> - Dipterix
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 


From d|pter|x@w@ng @end|ng |rom gm@||@com  Thu Aug 12 06:50:47 2021
From: d|pter|x@w@ng @end|ng |rom gm@||@com (Dipterix Wang)
Date: Thu, 12 Aug 2021 00:50:47 -0400
Subject: [Rd] Double to uint64_t on M1
In-Reply-To: <C582D4F5-6218-42CC-AC63-50CAB13D977E@R-project.org>
References: <EE649952-4BB6-432A-B3CA-BD9B11DD9324@gmail.com>
 <C582D4F5-6218-42CC-AC63-50CAB13D977E@R-project.org>
Message-ID: <FCD86C45-A75A-47FC-BAA6-ACBB4FD16892@gmail.com>

Thank you,

I guess I should convert double to uint64_t instead of int64_t...

The reason why I asked is because bit64 package `bit64::as.integer64(2^63)` produces different results on my machine vs. another server. This package converts double to int64_t directly. Looks like this is a bug in their package.

Thanks!

Best,

- Dipterix

> On Aug 11, 2021, at 11:52 PM, Simon Urbanek <simon.urbanek at R-project.org> wrote:
> 
> hing to do with R. 2^63 is too large to be represented as singed integer, so the behavior is undefined - to quote from the C99 specs (6.3.1.4):
> 
> "If the value of the integral part cannot be represented by the integer type, the behavior is undefined."
> 
> Your subject doesn't match your question as the uint64_t conversion is well-defined and the same on both platforms, but the conversion to int64_t in undefined


	[[alternative HTML version deleted]]


From tdhock5 @end|ng |rom gm@||@com  Thu Aug 12 06:58:33 2021
From: tdhock5 @end|ng |rom gm@||@com (Toby Hocking)
Date: Wed, 11 Aug 2021 21:58:33 -0700
Subject: [Rd] na.omit inconsistent with is.na on list
Message-ID: <CALK03d0vRmxNLzyU-s8O1Urg6yneYZeSKtdRizx5Z=ZZ57Fdcg@mail.gmail.com>

na.omit is documented as "na.omit returns the object with incomplete cases
removed." and "At present these will handle vectors," so I expected that
when it is used on a list, it should return the same thing as if we subset
via is.na; however I observed the following,

> L <- list(NULL, NA, 0)
> str(L[!is.na(L)])
List of 2
 $ : NULL
 $ : num 0
> str(na.omit(L))
List of 3
 $ : NULL
 $ : logi NA
 $ : num 0

Should na.omit be fixed so that it returns a result that is consistent with
is.na? I assume that is.na is the canonical definition of what should be
considered a missing value in R.

	[[alternative HTML version deleted]]


From tdhock5 @end|ng |rom gm@||@com  Thu Aug 12 07:16:06 2021
From: tdhock5 @end|ng |rom gm@||@com (Toby Hocking)
Date: Wed, 11 Aug 2021 22:16:06 -0700
Subject: [Rd] na.omit inconsistent with is.na on list
In-Reply-To: <CALK03d0vRmxNLzyU-s8O1Urg6yneYZeSKtdRizx5Z=ZZ57Fdcg@mail.gmail.com>
References: <CALK03d0vRmxNLzyU-s8O1Urg6yneYZeSKtdRizx5Z=ZZ57Fdcg@mail.gmail.com>
Message-ID: <CALK03d0pPSy5CL2AHZMtso5_7uXW5NfxZfC+mUsMm8yo8rOViA@mail.gmail.com>

Also, the na.omit method for data.frame with list column seems to be
inconsistent with is.na,

> L <- list(NULL, NA, 0)
> str(f <- data.frame(I(L)))
'data.frame': 3 obs. of  1 variable:
 $ L:List of 3
  ..$ : NULL
  ..$ : logi NA
  ..$ : num 0
  ..- attr(*, "class")= chr "AsIs"
> is.na(f)
         L
[1,] FALSE
[2,]  TRUE
[3,] FALSE
> na.omit(f)
   L
1
2 NA
3  0

On Wed, Aug 11, 2021 at 9:58 PM Toby Hocking <tdhock5 at gmail.com> wrote:

> na.omit is documented as "na.omit returns the object with incomplete cases
> removed." and "At present these will handle vectors," so I expected that
> when it is used on a list, it should return the same thing as if we subset
> via is.na; however I observed the following,
>
> > L <- list(NULL, NA, 0)
> > str(L[!is.na(L)])
> List of 2
>  $ : NULL
>  $ : num 0
> > str(na.omit(L))
> List of 3
>  $ : NULL
>  $ : logi NA
>  $ : num 0
>
> Should na.omit be fixed so that it returns a result that is consistent
> with is.na? I assume that is.na is the canonical definition of what
> should be considered a missing value in R.
>

	[[alternative HTML version deleted]]


From r|p|ey @end|ng |rom @t@t@@ox@@c@uk  Thu Aug 12 09:57:28 2021
From: r|p|ey @end|ng |rom @t@t@@ox@@c@uk (Prof Brian Ripley)
Date: Thu, 12 Aug 2021 08:57:28 +0100
Subject: [Rd] Double to uint64_t on M1
In-Reply-To: <C582D4F5-6218-42CC-AC63-50CAB13D977E@R-project.org>
References: <EE649952-4BB6-432A-B3CA-BD9B11DD9324@gmail.com>
 <C582D4F5-6218-42CC-AC63-50CAB13D977E@R-project.org>
Message-ID: <735e0e1e-4419-6bc2-2a02-6a3f683197b6@stats.ox.ac.uk>

On 12/08/2021 04:52, Simon Urbanek wrote:
> 
> Dipterix,
> 
> this has nothing to do with R. 2^63 is too large to be represented as singed integer, so the behavior is undefined - to quote from the C99 specs (6.3.1.4):
> 
> "If the value of the integral part cannot be represented by the integer type, the behavior is undefined."
> 
> Your subject doesn't match your question as the uint64_t conversion is well-defined and the same on both platforms, but the conversion to int64_t in undefined.

As I was writing a reply to say the same thing, a few more comments.

- the example is actually in C++, but also undefined there.

- R is more careful:
 > as.integer(2^31)
[1] NA
Warning message:
NAs introduced by coercion to integer range

- there is a sanitizer for this, on platforms including Linux and macOS 
(at least with clang, 
https://clang.llvm.org/docs/UndefinedBehaviorSanitizer.html#supported-platforms).

> 
> Cheers,
> Simon
> 
> 
>> On 12/08/2021, at 10:50 AM, Dipterix Wang <dipterix.wang at gmail.com> wrote:
>>
>> Hi,
>>
>> I was trying to convert REALSXP to int64_t in C, then found that converting 2^63 is inconsistent across platforms:
>>
>>
>> On M1 ARM osx, 2^63 (double) bit converting to `int64_t` becomes 9223372036854775807
>> On x86_64 ubuntu server, 2^63 (double) bit converting to `int64_t` is -9223372036854775808
>>
>> I was wondering if this is desired behavior to R?
>>
>> Here's the code to replicate the results above.
>>
>> print_bit <- Rcpp::cppFunction(r"(
>> SEXP print_bit(SEXP obj){
>>
>>   int64_t tmp1 = *REAL0(obj);
>>   printf("%lld ", tmp1);
>>
>>   return(R_NilValue);
>> }
>> )")
>>
>> print_bit(2^63)
>>
>> Thanks,
>> - Dipterix
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Emeritus Professor of Applied Statistics, University of Oxford


From henr|k@bengt@@on @end|ng |rom gm@||@com  Thu Aug 12 10:22:05 2021
From: henr|k@bengt@@on @end|ng |rom gm@||@com (Henrik Bengtsson)
Date: Thu, 12 Aug 2021 10:22:05 +0200
Subject: [Rd] Force quitting a FORK cluster node on macOS and Solaris wreaks
 havoc
Message-ID: <CAFDcVCRbrue58KodpVHOzESkyTFgajRP514_m0q33bAy4WU=nA@mail.gmail.com>

The following smells like a bug in R to me, because it puts the main R
session into an unstable state.  Consider the following R script:

a <- 42
message("a=", a)
cl <- parallel::makeCluster(1L, type="FORK")
try(parallel::clusterEvalQ(cl, quit(save="no")))
message("parallel:::isChild()=", parallel:::isChild())
message("a=", a)
rm(a)

The purpose of this was to emulate what happens when an parallel
workers crashes.

Now, if you source() the above on macOS, you might(*) end up with:

> a <- 42
> message("a=", a)
a=42
> cl <- parallel::makeCluster(1L, type="FORK")
> try(parallel::clusterEvalQ(cl, quit(save="no")))
Error: Error in unserialize(node$con) : error reading from connection
> message("parallel:::isChild()=", parallel:::isChild())
parallel:::isChild()=FALSE
> message("a=", a)
a=42
> rm(a)
> try(parallel::clusterEvalQ(cl, quit(save="no")))
Error: Error in unserialize(node$con) : error reading from connection
> message("parallel:::isChild()=", parallel:::isChild())
parallel:::isChild()=FALSE
> message("a=", a)
Error: Error in message("a=", a) : object 'a' not found
Execution halted

Note how 'rm(a)' is supposed to be the last line of code to be
evaluated.  However, the force quitting of the FORK cluster node
appears to result in the main code being evaluated twice (in
parallel?).

(*) This does not happen on all macOS variants. For example, it works
fine on CRAN's 'r-release-macos-x86_64' but it does give the above
behavior on 'r-release-macos-arm64'.  I can reproduce it on GitHub
Actions (https://github.com/HenrikBengtsson/teeny/runs/3309235106?check_suite_focus=true#step:10:219)
but not on R-hub's 'macos-highsierra-release' and
'macos-highsierra-release-cran'.  I can also reproduce it on R-hub's
'solaris-x86-patched' and solaris-x86-patched-ods' machines.  However,
I still haven't found a Linux machine where this happens.

If one replaces quit(save="no") with tools::pskill(Sys.getpid()) or
parallel:::mcexit(0L), this behavior does not take place (at least not
on GitHub Actions and R-hub).

I don't have access to a macOS or a Solaris machine, so I cannot
investigate further myself. For example, could it be an issue with
quit(), or does is it possible to trigger by other means? And more
importantly, should this be fixed? Also, I'd be curious what happens
if you run the above in an interactive R session.

/Henrik


From Andre@G||||bert @end|ng |rom chu-rouen@|r  Thu Aug 12 11:51:47 2021
From: Andre@G||||bert @end|ng |rom chu-rouen@|r (GILLIBERT, Andre)
Date: Thu, 12 Aug 2021 09:51:47 +0000
Subject: [Rd] Problem in random number generation for Marsaglia-Multicarry +
 Kinderman-Ramage
Message-ID: <54745780b6c1421298e55e1e8c18d6ab@chu-rouen.fr>

Dear R developers,


In my opinion, I discovered a severe flaw that occur with the combination of the Marsaglia-Multicarry pseudo-random number generator associated to the Kinderman-Ramage algorithm to generate normally distributed numbers.


The sample program is very simple (tested on R-4.1.1 x86_64 on Windows 10):

set.seed(1, "Marsaglia-Multicarry", normal.kind="Kinderman-Ramage")
v=rnorm(1e7)
poisson.test(sum(v < (-4)))$conf.int # returns c(34.5, 62.5)
poisson.test(sum(v > (4)))$conf.int # returns c(334.2, 410.7)
pnorm(-4)*1e7 # returns 316.7


There should be approximatively 316 values less than -4 and 316 values greater than +4, bug there are far too few values less than -4.

Results are similar with other random seeds, and things are even more obvious with larger sample sizes.

The Kinderman-Ramage algorithm is fine when combined to Mersenne-Twister, and Marsaglia-Multicarry is fine when combined with the normal.kind="Inversion" algorithm, but the combination of Marsaglia-Multicarry and Kinderman-Ramage seems to have severe flaws.

R should at least warn for that combination !

What do you think? Should I file a bug report?

--
Sincerely
Andr? GILLIBERT

	[[alternative HTML version deleted]]


From Andre@G||||bert @end|ng |rom chu-rouen@|r  Thu Aug 12 12:26:44 2021
From: Andre@G||||bert @end|ng |rom chu-rouen@|r (GILLIBERT, Andre)
Date: Thu, 12 Aug 2021 10:26:44 +0000
Subject: [Rd] Problem in random number generation for Marsaglia-Multicarry +
 Ahrens-Dieter
Message-ID: <edcc1ed20f6a464ca0f49f77e5b8c225@chu-rouen.fr>

Dear R developers,

At the same time I discovered a flaw in Marsaglia-Multicarry + Kinderman-Ramage, I found another in Marsaglia-Multicarry + Ahrens-Dieter.
It is less obvious than for Kinderman-Ramage; so I created a new thread for this bug.

The following code shows the problem (tested on R 4.1.1 x86_64 for Windows 10):

== start of code sample ==
set.seed(1, "Marsaglia-Multicarry", normal.kind="Ahrens-Dieter")
v=rnorm(1e8)

q=qnorm(seq(0.01, 0.99, 0.01))
cv=cut(v, breaks=c(-Inf, q, +Inf))
observed=table(cv)
chisq.test(observed) # p < 2.2e-16
== end of code sample ==

The chisq.test returns a P-value < 2.2e-16 while it was expected to return a non-significant P-value.
The additionnal code below, shows severe irregularities in the distribution of quantiles:

== continuation of code sample ==
expected = chisq.test(observed)$expected
z = (observed - expected)/sqrt(expected)
mean (abs(z) > 6) # 58% of z-scores are greater than 6 while none should be
== end of code sample ==

The bug is specific to the combination Marsaglia-Multicarry + Ahrens-Dieter.
There is no problem with Marsaglia-Multicarry + Inversion or Mersenne-Twister + Ahrens-Dieter

I would expect at least a warning (or an error) from R for such a buggy combination.

--
Sincerely
Andr? GILLIBERT


	[[alternative HTML version deleted]]


From pd@|gd @end|ng |rom gm@||@com  Thu Aug 12 15:07:52 2021
From: pd@|gd @end|ng |rom gm@||@com (peter dalgaard)
Date: Thu, 12 Aug 2021 15:07:52 +0200
Subject: [Rd] 
 Problem in random number generation for Marsaglia-Multicarry +
 Kinderman-Ramage
In-Reply-To: <54745780b6c1421298e55e1e8c18d6ab@chu-rouen.fr>
References: <54745780b6c1421298e55e1e8c18d6ab@chu-rouen.fr>
Message-ID: <0EB03E15-B535-4D78-98D0-6A73E224235A@gmail.com>

With these matters, one has to be careful to distinguish between method error and implementation error. 

The reason for changing the RNG setup in R v. 1.7.0 was pretty much this kind of unfortunate interaction between M-M and K-R. There are even more egregious examples for the distribution of maxima of normal variables. Try e.g.

RNGversion("1.6.0") # Marsaglia-Multicarry, Kinderman-Ramage
 s <- replicate(1e6,max(rnorm(10)))
 plot(density(s))

(A further bug in K-R was fixed in 1.7.1, but that is tangential to this.)

A glimpse of the source of the problem is seen in the "microcorrelations" in this:
 
RNGkind("Mar");m <- matrix(runif(4e7),2)
 plot(m[1,],m[2,],xlim=c(0,1e-3),pch=".")
 m <- matrix(runif(4e7),2)
 points(m[1,],m[2,],pch=".")

These examples are from 2003, so the issue has been known for almost 2 decades. However, to the best of our knowledge, the M-M RNG is a faithful implementation of their method, so we have left the RNG in R's arsenal, in case someone needed it for some specific purpose. 

- pd

> On 12 Aug 2021, at 11:51 , GILLIBERT, Andre <Andre.Gillibert at chu-rouen.fr> wrote:
> 
> Dear R developers,
> 
> 
> In my opinion, I discovered a severe flaw that occur with the combination of the Marsaglia-Multicarry pseudo-random number generator associated to the Kinderman-Ramage algorithm to generate normally distributed numbers.
> 
> 
> The sample program is very simple (tested on R-4.1.1 x86_64 on Windows 10):
> 
> set.seed(1, "Marsaglia-Multicarry", normal.kind="Kinderman-Ramage")
> v=rnorm(1e7)
> poisson.test(sum(v < (-4)))$conf.int # returns c(34.5, 62.5)
> poisson.test(sum(v > (4)))$conf.int # returns c(334.2, 410.7)
> pnorm(-4)*1e7 # returns 316.7
> 
> 
> There should be approximatively 316 values less than -4 and 316 values greater than +4, bug there are far too few values less than -4.
> 
> Results are similar with other random seeds, and things are even more obvious with larger sample sizes.
> 
> The Kinderman-Ramage algorithm is fine when combined to Mersenne-Twister, and Marsaglia-Multicarry is fine when combined with the normal.kind="Inversion" algorithm, but the combination of Marsaglia-Multicarry and Kinderman-Ramage seems to have severe flaws.
> 
> R should at least warn for that combination !
> 
> What do you think? Should I file a bug report?
> 
> --
> Sincerely
> Andr? GILLIBERT
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From @tp @end|ng |rom p|@kor@k|@com  Thu Aug 12 21:19:44 2021
From: @tp @end|ng |rom p|@kor@k|@com (Andrew Piskorski)
Date: Thu, 12 Aug 2021 15:19:44 -0400
Subject: [Rd] Rprofile.site function or variable definitions break with R 4.1
Message-ID: <YRV0UJKvLRRuqrNr@piskorski.com>

With R 4.1, it seems you can no longer do much in your "Rprofile.site"
file.  Attempting to define any functions or set any variables there
gives errors like these:

  Error: cannot add binding of 'my_function_name' to the base environment
  Error: cannot add binding of 'my_variable_name' to the base environment

Presumably that's because of this change in R 4.1.0:

  https://cran.r-project.org/doc/manuals/r-patched/NEWS.html
  CHANGES IN R 4.1.0
  The base environment and its namespace are now locked (so one can no
  longer add bindings to these or remove from these).

Ok, but what's the recommended way to actually USE Rprofile.site now?
Should I move all my local configuration into a special package, and
do nothing in Rprofile.site except require() that package?

Thanks for your help and advice!

-- 
Andrew Piskorski <atp at piskorski.com>


From edd @end|ng |rom deb|@n@org  Thu Aug 12 21:40:11 2021
From: edd @end|ng |rom deb|@n@org (Dirk Eddelbuettel)
Date: Thu, 12 Aug 2021 14:40:11 -0500
Subject: [Rd] 
 Rprofile.site function or variable definitions break with R 4.1
In-Reply-To: <YRV0UJKvLRRuqrNr@piskorski.com>
References: <YRV0UJKvLRRuqrNr@piskorski.com>
Message-ID: <24853.31003.846696.955115@rob.eddelbuettel.com>


On 12 August 2021 at 15:19, Andrew Piskorski wrote:
| Ok, but what's the recommended way to actually USE Rprofile.site now?
| Should I move all my local configuration into a special package, and
| do nothing in Rprofile.site except require() that package?

Exactly as before. I set my mirror as I have before and nothing changes

  ## We set the cloud mirror, which is 'network-close' to everybody, as default
  local({
      r <- getOption("repos")
      r["CRAN"] <- "https://cloud.r-project.org"
      options(repos = r)
  })

I cannot help but think that you are shooting the messenger (here
Rprofile.site) for an actual behaviour change in R itself ?

Dirk

-- 
https://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From g@bembecker @end|ng |rom gm@||@com  Thu Aug 12 22:09:37 2021
From: g@bembecker @end|ng |rom gm@||@com (Gabriel Becker)
Date: Thu, 12 Aug 2021 13:09:37 -0700
Subject: [Rd] 
 Rprofile.site function or variable definitions break with R 4.1
In-Reply-To: <24853.31003.846696.955115@rob.eddelbuettel.com>
References: <YRV0UJKvLRRuqrNr@piskorski.com>
 <24853.31003.846696.955115@rob.eddelbuettel.com>
Message-ID: <CAD4oTHEHY4BXDjHdyKJ=E8FPLwLiL=UnPdmQw4bkVE4p7DKdQg@mail.gmail.com>

Hi Andrew and Dirk,

The other question to think about is what was your Rprofile.site doing
before. We can infer from this error that apparently it was defining things
*in the namespace for the base package*. How often is that actually what
you wanted it to do/a good idea?

I haven't played around with it, as I don't use Rprofile.site to actually
create/assign object only, like Dirk, set options or option-adjacent things
(such as .libPaths), but I imagine you could get it to put things into the
global environment or attach a special "local config" entry to the search
path an put things there, if you so desired.

Best,
~G

On Thu, Aug 12, 2021 at 12:41 PM Dirk Eddelbuettel <edd at debian.org> wrote:

>
> On 12 August 2021 at 15:19, Andrew Piskorski wrote:
> | Ok, but what's the recommended way to actually USE Rprofile.site now?
> | Should I move all my local configuration into a special package, and
> | do nothing in Rprofile.site except require() that package?
>
> Exactly as before. I set my mirror as I have before and nothing changes
>
>   ## We set the cloud mirror, which is 'network-close' to everybody, as
> default
>   local({
>       r <- getOption("repos")
>       r["CRAN"] <- "https://cloud.r-project.org"
>       options(repos = r)
>   })
>
> I cannot help but think that you are shooting the messenger (here
> Rprofile.site) for an actual behaviour change in R itself ?
>
> Dirk
>
> --
> https://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From g@bembecker @end|ng |rom gm@||@com  Thu Aug 12 22:18:58 2021
From: g@bembecker @end|ng |rom gm@||@com (Gabriel Becker)
Date: Thu, 12 Aug 2021 13:18:58 -0700
Subject: [Rd] na.omit inconsistent with is.na on list
In-Reply-To: <CALK03d0pPSy5CL2AHZMtso5_7uXW5NfxZfC+mUsMm8yo8rOViA@mail.gmail.com>
References: <CALK03d0vRmxNLzyU-s8O1Urg6yneYZeSKtdRizx5Z=ZZ57Fdcg@mail.gmail.com>
 <CALK03d0pPSy5CL2AHZMtso5_7uXW5NfxZfC+mUsMm8yo8rOViA@mail.gmail.com>
Message-ID: <CAD4oTHGni5YDKES5yKtzNPqGDV3vrP+7YNFCAJ_Y0s-Cw0xeUQ@mail.gmail.com>

Hi Toby,

This definitely appears intentional, the first  expression of
stats:::na.omit.default is

   if (!is.atomic(object))

        return(object)


So it is explicitly just returning the object in non-atomic cases, which
includes lists. I was not involved in this decision (obviously) but my
guess is that it is due to the fact that what constitutes an observation
"being complete" in unclear in the list case. What should

na.omit(list(5, NA, c(NA, 5)))

return? Just the first element, or the first and the last? It seems, at
least to me, unclear. A small change to the documentation to to add "atomic
(in the sense of is.atomic returning \code{TRUE})" in front of "vectors"
or similar  where what types of objects are supported seems justified,
though, imho, as the current documentation is either ambiguous or
technically incorrect, depending on what we take "vector" to mean.

Best,
~G

On Wed, Aug 11, 2021 at 10:16 PM Toby Hocking <tdhock5 at gmail.com> wrote:

> Also, the na.omit method for data.frame with list column seems to be
> inconsistent with is.na,
>
> > L <- list(NULL, NA, 0)
> > str(f <- data.frame(I(L)))
> 'data.frame': 3 obs. of  1 variable:
>  $ L:List of 3
>   ..$ : NULL
>   ..$ : logi NA
>   ..$ : num 0
>   ..- attr(*, "class")= chr "AsIs"
> > is.na(f)
>          L
> [1,] FALSE
> [2,]  TRUE
> [3,] FALSE
> > na.omit(f)
>    L
> 1
> 2 NA
> 3  0
>
> On Wed, Aug 11, 2021 at 9:58 PM Toby Hocking <tdhock5 at gmail.com> wrote:
>
> > na.omit is documented as "na.omit returns the object with incomplete
> cases
> > removed." and "At present these will handle vectors," so I expected that
> > when it is used on a list, it should return the same thing as if we
> subset
> > via is.na; however I observed the following,
> >
> > > L <- list(NULL, NA, 0)
> > > str(L[!is.na(L)])
> > List of 2
> >  $ : NULL
> >  $ : num 0
> > > str(na.omit(L))
> > List of 3
> >  $ : NULL
> >  $ : logi NA
> >  $ : num 0
> >
> > Should na.omit be fixed so that it returns a result that is consistent
> > with is.na? I assume that is.na is the canonical definition of what
> > should be considered a missing value in R.
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From @|mon@urb@nek @end|ng |rom R-project@org  Fri Aug 13 00:58:01 2021
From: @|mon@urb@nek @end|ng |rom R-project@org (Simon Urbanek)
Date: Fri, 13 Aug 2021 10:58:01 +1200
Subject: [Rd] 
 Force quitting a FORK cluster node on macOS and Solaris wreaks havoc
In-Reply-To: <CAFDcVCRbrue58KodpVHOzESkyTFgajRP514_m0q33bAy4WU=nA@mail.gmail.com>
References: <CAFDcVCRbrue58KodpVHOzESkyTFgajRP514_m0q33bAy4WU=nA@mail.gmail.com>
Message-ID: <4CF34F70-DCAC-4694-A8CF-38E7E0B43272@R-project.org>


Henrik,

I'm not quite sure I understand the report to be honest.

Just a quick comment here - using quit() in a forked child is not allowed, because the R clean-up is only intended for the master as it will be blowing away the master's state, connections, working directory, running master's exit handlers etc. That's why the children have to use either abort or mcexit() to terminate - which is what mcparallel() does. If you use q() a lot of things go wrong no matter the platform - e.g. try using ? in the master session after sourcing your code.

Cheers,
Simon


> On 12/08/2021, at 8:22 PM, Henrik Bengtsson <henrik.bengtsson at gmail.com> wrote:
> 
> The following smells like a bug in R to me, because it puts the main R
> session into an unstable state.  Consider the following R script:
> 
> a <- 42
> message("a=", a)
> cl <- parallel::makeCluster(1L, type="FORK")
> try(parallel::clusterEvalQ(cl, quit(save="no")))
> message("parallel:::isChild()=", parallel:::isChild())
> message("a=", a)
> rm(a)
> 
> The purpose of this was to emulate what happens when an parallel
> workers crashes.
> 
> Now, if you source() the above on macOS, you might(*) end up with:
> 
>> a <- 42
>> message("a=", a)
> a=42
>> cl <- parallel::makeCluster(1L, type="FORK")
>> try(parallel::clusterEvalQ(cl, quit(save="no")))
> Error: Error in unserialize(node$con) : error reading from connection
>> message("parallel:::isChild()=", parallel:::isChild())
> parallel:::isChild()=FALSE
>> message("a=", a)
> a=42
>> rm(a)
>> try(parallel::clusterEvalQ(cl, quit(save="no")))
> Error: Error in unserialize(node$con) : error reading from connection
>> message("parallel:::isChild()=", parallel:::isChild())
> parallel:::isChild()=FALSE
>> message("a=", a)
> Error: Error in message("a=", a) : object 'a' not found
> Execution halted
> 
> Note how 'rm(a)' is supposed to be the last line of code to be
> evaluated.  However, the force quitting of the FORK cluster node
> appears to result in the main code being evaluated twice (in
> parallel?).
> 
> (*) This does not happen on all macOS variants. For example, it works
> fine on CRAN's 'r-release-macos-x86_64' but it does give the above
> behavior on 'r-release-macos-arm64'.  I can reproduce it on GitHub
> Actions (https://github.com/HenrikBengtsson/teeny/runs/3309235106?check_suite_focus=true#step:10:219)
> but not on R-hub's 'macos-highsierra-release' and
> 'macos-highsierra-release-cran'.  I can also reproduce it on R-hub's
> 'solaris-x86-patched' and solaris-x86-patched-ods' machines.  However,
> I still haven't found a Linux machine where this happens.
> 
> If one replaces quit(save="no") with tools::pskill(Sys.getpid()) or
> parallel:::mcexit(0L), this behavior does not take place (at least not
> on GitHub Actions and R-hub).
> 
> I don't have access to a macOS or a Solaris machine, so I cannot
> investigate further myself. For example, could it be an issue with
> quit(), or does is it possible to trigger by other means? And more
> importantly, should this be fixed? Also, I'd be curious what happens
> if you run the above in an interactive R session.
> 
> /Henrik
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 


From tdhock5 @end|ng |rom gm@||@com  Fri Aug 13 01:30:34 2021
From: tdhock5 @end|ng |rom gm@||@com (Toby Hocking)
Date: Thu, 12 Aug 2021 16:30:34 -0700
Subject: [Rd] na.omit inconsistent with is.na on list
In-Reply-To: <CAD4oTHGni5YDKES5yKtzNPqGDV3vrP+7YNFCAJ_Y0s-Cw0xeUQ@mail.gmail.com>
References: <CALK03d0vRmxNLzyU-s8O1Urg6yneYZeSKtdRizx5Z=ZZ57Fdcg@mail.gmail.com>
 <CALK03d0pPSy5CL2AHZMtso5_7uXW5NfxZfC+mUsMm8yo8rOViA@mail.gmail.com>
 <CAD4oTHGni5YDKES5yKtzNPqGDV3vrP+7YNFCAJ_Y0s-Cw0xeUQ@mail.gmail.com>
Message-ID: <CALK03d1swkOx_iYgpMK3Lb-r5MyC9WPzMviJkVDv+EPwEgE5VA@mail.gmail.com>

Hi Gabe thanks for the feedback.

On Thu, Aug 12, 2021 at 1:19 PM Gabriel Becker <gabembecker at gmail.com>
wrote:

> Hi Toby,
>
> This definitely appears intentional, the first  expression of
> stats:::na.omit.default is
>
>    if (!is.atomic(object))
>
>         return(object)
>
> Based on this code it does seem that the documentation could be clarified
to say atomic vectors.

>
> So it is explicitly just returning the object in non-atomic cases, which
> includes lists. I was not involved in this decision (obviously) but my
> guess is that it is due to the fact that what constitutes an observation
> "being complete" in unclear in the list case. What should
>
> na.omit(list(5, NA, c(NA, 5)))
>
> return? Just the first element, or the first and the last? It seems, at
> least to me, unclear.
>
I agree in principle/theory that it is unclear, but in practice is.na has
an un-ambiguous answer (if list element is scalar NA then it is considered
missing, otherwise not).

> A small change to the documentation to to add "atomic (in the sense of
> is.atomic returning \code{TRUE})" in front of "vectors"  or similar  where
> what types of objects are supported seems justified, though, imho, as the
> current documentation is either ambiguous or technically incorrect,
> depending on what we take "vector" to mean.
>
> Best,
> ~G
>
> On Wed, Aug 11, 2021 at 10:16 PM Toby Hocking <tdhock5 at gmail.com> wrote:
>
>> Also, the na.omit method for data.frame with list column seems to be
>> inconsistent with is.na,
>>
>> > L <- list(NULL, NA, 0)
>> > str(f <- data.frame(I(L)))
>> 'data.frame': 3 obs. of  1 variable:
>>  $ L:List of 3
>>   ..$ : NULL
>>   ..$ : logi NA
>>   ..$ : num 0
>>   ..- attr(*, "class")= chr "AsIs"
>> > is.na(f)
>>          L
>> [1,] FALSE
>> [2,]  TRUE
>> [3,] FALSE
>> > na.omit(f)
>>    L
>> 1
>> 2 NA
>> 3  0
>>
>> On Wed, Aug 11, 2021 at 9:58 PM Toby Hocking <tdhock5 at gmail.com> wrote:
>>
>> > na.omit is documented as "na.omit returns the object with incomplete
>> cases
>> > removed." and "At present these will handle vectors," so I expected that
>> > when it is used on a list, it should return the same thing as if we
>> subset
>> > via is.na; however I observed the following,
>> >
>> > > L <- list(NULL, NA, 0)
>> > > str(L[!is.na(L)])
>> > List of 2
>> >  $ : NULL
>> >  $ : num 0
>> > > str(na.omit(L))
>> > List of 3
>> >  $ : NULL
>> >  $ : logi NA
>> >  $ : num 0
>> >
>> > Should na.omit be fixed so that it returns a result that is consistent
>> > with is.na? I assume that is.na is the canonical definition of what
>> > should be considered a missing value in R.
>> >
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>

	[[alternative HTML version deleted]]


From g@bembecker @end|ng |rom gm@||@com  Fri Aug 13 08:46:10 2021
From: g@bembecker @end|ng |rom gm@||@com (Gabriel Becker)
Date: Thu, 12 Aug 2021 23:46:10 -0700
Subject: [Rd] na.omit inconsistent with is.na on list
In-Reply-To: <CALK03d1swkOx_iYgpMK3Lb-r5MyC9WPzMviJkVDv+EPwEgE5VA@mail.gmail.com>
References: <CALK03d0vRmxNLzyU-s8O1Urg6yneYZeSKtdRizx5Z=ZZ57Fdcg@mail.gmail.com>
 <CALK03d0pPSy5CL2AHZMtso5_7uXW5NfxZfC+mUsMm8yo8rOViA@mail.gmail.com>
 <CAD4oTHGni5YDKES5yKtzNPqGDV3vrP+7YNFCAJ_Y0s-Cw0xeUQ@mail.gmail.com>
 <CALK03d1swkOx_iYgpMK3Lb-r5MyC9WPzMviJkVDv+EPwEgE5VA@mail.gmail.com>
Message-ID: <CAD4oTHHrUhrnQJ-423JK1bT55QrF_kbg_3mP1xQVwpJJ73Q5aw@mail.gmail.com>

On Thu, Aug 12, 2021 at 4:30 PM Toby Hocking <tdhock5 at gmail.com> wrote:

> Hi Gabe thanks for the feedback.
>
> On Thu, Aug 12, 2021 at 1:19 PM Gabriel Becker <gabembecker at gmail.com>
> wrote:
>
>> Hi Toby,
>>
>> This definitely appears intentional, the first  expression of
>> stats:::na.omit.default is
>>
>>    if (!is.atomic(object))
>>
>>         return(object)
>>
>> Based on this code it does seem that the documentation could be clarified
> to say atomic vectors.
>
>>
>> So it is explicitly just returning the object in non-atomic cases, which
>> includes lists. I was not involved in this decision (obviously) but my
>> guess is that it is due to the fact that what constitutes an observation
>> "being complete" in unclear in the list case. What should
>>
>> na.omit(list(5, NA, c(NA, 5)))
>>
>> return? Just the first element, or the first and the last? It seems, at
>> least to me, unclear.
>>
> I agree in principle/theory that it is unclear, but in practice is.na has
> an un-ambiguous answer (if list element is scalar NA then it is considered
> missing, otherwise not).
>

Well, yes it's unambiguous, but I would argue less likely than the other
option to be correct. Remember what na.omit is supposed to do: "remove
observations which are not complete".

Now for data.frames, this means it removes any row (i.e. observation,
despite the internal structure) where *any* column contains an NA. The most
analogous interpretation of na.omit on a list, in the well behaved (ie list
of atomic vectors) case, I think, is that we consider it a ragged
collection of "observations", in which case  x[is.na(x)] with x a list
would do the wrong thing because it is not checking these "observations"
for completeness.

Perhaps others disagree with me about that, and anyway, this only works
when you can check the elements of the list for "completeness" at all, the
list can have anything for elements, and then checking for completeness
becomes impossible...

As is, I do also wonder if a warning should be thrown letting the user know
that their call isn't doing ANY of the possible things it could mean...

Best,
~G


> A small change to the documentation to to add "atomic (in the sense of
>> is.atomic returning \code{TRUE})" in front of "vectors"  or similar  where
>> what types of objects are supported seems justified, though, imho, as the
>> current documentation is either ambiguous or technically incorrect,
>> depending on what we take "vector" to mean.
>>
>> Best,
>> ~G
>>
>> On Wed, Aug 11, 2021 at 10:16 PM Toby Hocking <tdhock5 at gmail.com> wrote:
>>
>>> Also, the na.omit method for data.frame with list column seems to be
>>> inconsistent with is.na,
>>>
>>> > L <- list(NULL, NA, 0)
>>> > str(f <- data.frame(I(L)))
>>> 'data.frame': 3 obs. of  1 variable:
>>>  $ L:List of 3
>>>   ..$ : NULL
>>>   ..$ : logi NA
>>>   ..$ : num 0
>>>   ..- attr(*, "class")= chr "AsIs"
>>> > is.na(f)
>>>          L
>>> [1,] FALSE
>>> [2,]  TRUE
>>> [3,] FALSE
>>> > na.omit(f)
>>>    L
>>> 1
>>> 2 NA
>>> 3  0
>>>
>>> On Wed, Aug 11, 2021 at 9:58 PM Toby Hocking <tdhock5 at gmail.com> wrote:
>>>
>>> > na.omit is documented as "na.omit returns the object with incomplete
>>> cases
>>> > removed." and "At present these will handle vectors," so I expected
>>> that
>>> > when it is used on a list, it should return the same thing as if we
>>> subset
>>> > via is.na; however I observed the following,
>>> >
>>> > > L <- list(NULL, NA, 0)
>>> > > str(L[!is.na(L)])
>>> > List of 2
>>> >  $ : NULL
>>> >  $ : num 0
>>> > > str(na.omit(L))
>>> > List of 3
>>> >  $ : NULL
>>> >  $ : logi NA
>>> >  $ : num 0
>>> >
>>> > Should na.omit be fixed so that it returns a result that is consistent
>>> > with is.na? I assume that is.na is the canonical definition of what
>>> > should be considered a missing value in R.
>>> >
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>
>>

	[[alternative HTML version deleted]]


From |uc@r @end|ng |rom |edor@project@org  Fri Aug 13 09:26:48 2021
From: |uc@r @end|ng |rom |edor@project@org (=?UTF-8?Q?I=C3=B1aki_Ucar?=)
Date: Fri, 13 Aug 2021 09:26:48 +0200
Subject: [Rd] na.omit inconsistent with is.na on list
In-Reply-To: <CAD4oTHGni5YDKES5yKtzNPqGDV3vrP+7YNFCAJ_Y0s-Cw0xeUQ@mail.gmail.com>
References: <CALK03d0vRmxNLzyU-s8O1Urg6yneYZeSKtdRizx5Z=ZZ57Fdcg@mail.gmail.com>
 <CALK03d0pPSy5CL2AHZMtso5_7uXW5NfxZfC+mUsMm8yo8rOViA@mail.gmail.com>
 <CAD4oTHGni5YDKES5yKtzNPqGDV3vrP+7YNFCAJ_Y0s-Cw0xeUQ@mail.gmail.com>
Message-ID: <CALEXWq1hQMpf1Cf9NEbEC6eAPsw+4-Uqka7ym5Dxc2PKRqfR9A@mail.gmail.com>

On Thu, 12 Aug 2021 at 22:20, Gabriel Becker <gabembecker at gmail.com> wrote:
>
> Hi Toby,
>
> This definitely appears intentional, the first  expression of
> stats:::na.omit.default is
>
>    if (!is.atomic(object))
>
>         return(object)

I don't follow your point. This only means that the *default* method
is not intended for non-atomic cases, but it doesn't mean it shouldn't
exist a method for lists.

> So it is explicitly just returning the object in non-atomic cases, which
> includes lists. I was not involved in this decision (obviously) but my
> guess is that it is due to the fact that what constitutes an observation
> "being complete" in unclear in the list case. What should
>
> na.omit(list(5, NA, c(NA, 5)))
>
> return? Just the first element, or the first and the last? It seems, at
> least to me, unclear. A small change to the documentation to to add "atomic

> is.na(list(5, NA, c(NA, 5)))
[1] FALSE  TRUE FALSE

Following Toby's argument, it's clear to me: the first and the last.

I?aki

> (in the sense of is.atomic returning \code{TRUE})" in front of "vectors"
> or similar  where what types of objects are supported seems justified,
> though, imho, as the current documentation is either ambiguous or
> technically incorrect, depending on what we take "vector" to mean.
>
> Best,
> ~G
>
> On Wed, Aug 11, 2021 at 10:16 PM Toby Hocking <tdhock5 at gmail.com> wrote:
>
> > Also, the na.omit method for data.frame with list column seems to be
> > inconsistent with is.na,
> >
> > > L <- list(NULL, NA, 0)
> > > str(f <- data.frame(I(L)))
> > 'data.frame': 3 obs. of  1 variable:
> >  $ L:List of 3
> >   ..$ : NULL
> >   ..$ : logi NA
> >   ..$ : num 0
> >   ..- attr(*, "class")= chr "AsIs"
> > > is.na(f)
> >          L
> > [1,] FALSE
> > [2,]  TRUE
> > [3,] FALSE
> > > na.omit(f)
> >    L
> > 1
> > 2 NA
> > 3  0
> >
> > On Wed, Aug 11, 2021 at 9:58 PM Toby Hocking <tdhock5 at gmail.com> wrote:
> >
> > > na.omit is documented as "na.omit returns the object with incomplete
> > cases
> > > removed." and "At present these will handle vectors," so I expected that
> > > when it is used on a list, it should return the same thing as if we
> > subset
> > > via is.na; however I observed the following,
> > >
> > > > L <- list(NULL, NA, 0)
> > > > str(L[!is.na(L)])
> > > List of 2
> > >  $ : NULL
> > >  $ : num 0
> > > > str(na.omit(L))
> > > List of 3
> > >  $ : NULL
> > >  $ : logi NA
> > >  $ : num 0
> > >
> > > Should na.omit be fixed so that it returns a result that is consistent
> > > with is.na? I assume that is.na is the canonical definition of what
> > > should be considered a missing value in R.
> > >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel



-- 
I?aki ?car


From hugh@p@r@on@ge @end|ng |rom gm@||@com  Fri Aug 13 10:09:43 2021
From: hugh@p@r@on@ge @end|ng |rom gm@||@com (Hugh Parsonage)
Date: Fri, 13 Aug 2021 18:09:43 +1000
Subject: [Rd] na.omit inconsistent with is.na on list
In-Reply-To: <CALEXWq1hQMpf1Cf9NEbEC6eAPsw+4-Uqka7ym5Dxc2PKRqfR9A@mail.gmail.com>
References: <CALK03d0vRmxNLzyU-s8O1Urg6yneYZeSKtdRizx5Z=ZZ57Fdcg@mail.gmail.com>
 <CALK03d0pPSy5CL2AHZMtso5_7uXW5NfxZfC+mUsMm8yo8rOViA@mail.gmail.com>
 <CAD4oTHGni5YDKES5yKtzNPqGDV3vrP+7YNFCAJ_Y0s-Cw0xeUQ@mail.gmail.com>
 <CALEXWq1hQMpf1Cf9NEbEC6eAPsw+4-Uqka7ym5Dxc2PKRqfR9A@mail.gmail.com>
Message-ID: <CAJmOi+OEVLufPHwe1y8=1jb_Btmh=0m2TkvTZ6wkkwKGcbGsSw@mail.gmail.com>

The data.frame method deliberately skips non-atomic columns before
invoking is.na(x) so I think it is fair to assume this behaviour is
intentional and assumed.

Not so clear to me that there is a sensible answer for list columns.
(List columns seem to collide with the expectation that in each
variable every observation will be of the same type)

Consider your list L as

L <- list(NULL, NA, c(NA, NA))

Seems like every observation could have a claim to be 'missing' here.
Concretely, if a data.frame had a list column representing the lat-lon
of an observation, we might only be able to represent missing values
like c(NA, NA).

On Fri, 13 Aug 2021 at 17:27, I?aki Ucar <iucar at fedoraproject.org> wrote:
>
> On Thu, 12 Aug 2021 at 22:20, Gabriel Becker <gabembecker at gmail.com> wrote:
> >
> > Hi Toby,
> >
> > This definitely appears intentional, the first  expression of
> > stats:::na.omit.default is
> >
> >    if (!is.atomic(object))
> >
> >         return(object)
>
> I don't follow your point. This only means that the *default* method
> is not intended for non-atomic cases, but it doesn't mean it shouldn't
> exist a method for lists.
>
> > So it is explicitly just returning the object in non-atomic cases, which
> > includes lists. I was not involved in this decision (obviously) but my
> > guess is that it is due to the fact that what constitutes an observation
> > "being complete" in unclear in the list case. What should
> >
> > na.omit(list(5, NA, c(NA, 5)))
> >
> > return? Just the first element, or the first and the last? It seems, at
> > least to me, unclear. A small change to the documentation to to add "atomic
>
> > is.na(list(5, NA, c(NA, 5)))
> [1] FALSE  TRUE FALSE
>
> Following Toby's argument, it's clear to me: the first and the last.
>
> I?aki
>
> > (in the sense of is.atomic returning \code{TRUE})" in front of "vectors"
> > or similar  where what types of objects are supported seems justified,
> > though, imho, as the current documentation is either ambiguous or
> > technically incorrect, depending on what we take "vector" to mean.
> >
> > Best,
> > ~G
> >
> > On Wed, Aug 11, 2021 at 10:16 PM Toby Hocking <tdhock5 at gmail.com> wrote:
> >
> > > Also, the na.omit method for data.frame with list column seems to be
> > > inconsistent with is.na,
> > >
> > > > L <- list(NULL, NA, 0)
> > > > str(f <- data.frame(I(L)))
> > > 'data.frame': 3 obs. of  1 variable:
> > >  $ L:List of 3
> > >   ..$ : NULL
> > >   ..$ : logi NA
> > >   ..$ : num 0
> > >   ..- attr(*, "class")= chr "AsIs"
> > > > is.na(f)
> > >          L
> > > [1,] FALSE
> > > [2,]  TRUE
> > > [3,] FALSE
> > > > na.omit(f)
> > >    L
> > > 1
> > > 2 NA
> > > 3  0
> > >
> > > On Wed, Aug 11, 2021 at 9:58 PM Toby Hocking <tdhock5 at gmail.com> wrote:
> > >
> > > > na.omit is documented as "na.omit returns the object with incomplete
> > > cases
> > > > removed." and "At present these will handle vectors," so I expected that
> > > > when it is used on a list, it should return the same thing as if we
> > > subset
> > > > via is.na; however I observed the following,
> > > >
> > > > > L <- list(NULL, NA, 0)
> > > > > str(L[!is.na(L)])
> > > > List of 2
> > > >  $ : NULL
> > > >  $ : num 0
> > > > > str(na.omit(L))
> > > > List of 3
> > > >  $ : NULL
> > > >  $ : logi NA
> > > >  $ : num 0
> > > >
> > > > Should na.omit be fixed so that it returns a result that is consistent
> > > > with is.na? I assume that is.na is the canonical definition of what
> > > > should be considered a missing value in R.
> > > >
> > >
> > >         [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-devel at r-project.org mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-devel
> > >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
>
>
>
> --
> I?aki ?car
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From d@tr7320 @end|ng |rom un|@@ydney@edu@@u  Fri Aug 13 11:00:05 2021
From: d@tr7320 @end|ng |rom un|@@ydney@edu@@u (Dario Strbenac)
Date: Fri, 13 Aug 2021 09:00:05 +0000
Subject: [Rd] svd For Large Matrix
Message-ID: <SYBPR01MB4761CA7FD01EB93E4CF8511ACDFA9@SYBPR01MB4761.ausprd01.prod.outlook.com>

Good day,

I have a real scenario involving 45 million biological cells (samples) and 60 proteins (variables) which leads to a segmentation fault for svd. I thought this might be a good example of why it might benefit from a long vector upgrade.

test <- matrix(rnorm(45000000*60), ncol = 60)
testSVD <- svd(test)

 *** caught segfault ***
address 0x7fe93514d618, cause 'memory not mapped'

Traceback:
 1: La.svd(x, nu, nv)
 2: svd(test)

--------------------------------------
Dario Strbenac
University of Sydney
Camperdown NSW 2050
Australia


From iuke-tier@ey m@iii@g oii uiow@@edu  Fri Aug 13 16:58:26 2021
From: iuke-tier@ey m@iii@g oii uiow@@edu (iuke-tier@ey m@iii@g oii uiow@@edu)
Date: Fri, 13 Aug 2021 09:58:26 -0500 (CDT)
Subject: [Rd] [External]  svd For Large Matrix
In-Reply-To: <SYBPR01MB4761CA7FD01EB93E4CF8511ACDFA9@SYBPR01MB4761.ausprd01.prod.outlook.com>
References: <SYBPR01MB4761CA7FD01EB93E4CF8511ACDFA9@SYBPR01MB4761.ausprd01.prod.outlook.com>
Message-ID: <alpine.DEB.2.22.394.2108130958100.3514@luke-Latitude-7480>

[copying the list]

svd() does support matrices with long vector data. Your example works
fine for me on a machine with enough memory with either the reference
BLAS/LAPACK or the BLAS/LAPACK used on Fedora 33 (flexiblas backed, I
believe, by a version of openBLAS). Take a look at sessionInfo() to
see what you are using and consider switching to another BLAS/LAPACK
if necessary. Running under gdb may help tracking down where the issue
is and reporting it for the BLAS/LAPACK you are using.

Best,

luke

On Fri, 13 Aug 2021, Dario Strbenac via R-devel wrote:

> Good day,
>
> I have a real scenario involving 45 million biological cells (samples) and 60 proteins (variables) which leads to a segmentation fault for svd. I thought this might be a good example of why it might benefit from a long vector upgrade.
>
> test <- matrix(rnorm(45000000*60), ncol = 60)
> testSVD <- svd(test)
>
> *** caught segfault ***
> address 0x7fe93514d618, cause 'memory not mapped'
>
> Traceback:
> 1: La.svd(x, nu, nv)
> 2: svd(test)
>
> --------------------------------------
> Dario Strbenac
> University of Sydney
> Camperdown NSW 2050
> Australia
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

-- 
Luke Tierney
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu


From r|p|ey @end|ng |rom @t@t@@ox@@c@uk  Fri Aug 13 17:49:14 2021
From: r|p|ey @end|ng |rom @t@t@@ox@@c@uk (Prof Brian Ripley)
Date: Fri, 13 Aug 2021 16:49:14 +0100
Subject: [Rd] [External] svd For Large Matrix
In-Reply-To: <alpine.DEB.2.22.394.2108130958100.3514@luke-Latitude-7480>
References: <SYBPR01MB4761CA7FD01EB93E4CF8511ACDFA9@SYBPR01MB4761.ausprd01.prod.outlook.com>
 <alpine.DEB.2.22.394.2108130958100.3514@luke-Latitude-7480>
Message-ID: <7fdf2173-a187-3ac7-c726-5e2162199e13@stats.ox.ac.uk>

On 13/08/2021 15:58, luke-tierney at uiowa.edu wrote:
> [copying the list]
> 
> svd() does support matrices with long vector data. Your example works
> fine for me on a machine with enough memory with either the reference
> BLAS/LAPACK or the BLAS/LAPACK used on Fedora 33 (flexiblas backed, I
> believe, by a version of openBLAS). Take a look at sessionInfo() to
> see what you are using and consider switching to another BLAS/LAPACK
> if necessary. Running under gdb may help tracking down where the issue
> is and reporting it for the BLAS/LAPACK you are using.

See also 
https://cran.r-project.org/doc/manuals/r-devel/R-ints.html#Large-matrices which 
(to nuance Prof Tierney's comment) mentions that svd on long-vector 
*complex* data has been known to segfault (with the reference BLAS/Lapack).

My guess was that this was an out-of-memory condition not handled 
elegantly by the OS.  (There are many reasons why the posting guide asks 
for the output of sessionInfo().)

We do not have the statistical context but it seems unlikely that anyone 
is interested in each of the 45m samples, and for information on the 
proteins a quite small sample of cells would suffice.  And that not all 
45m left singular values are required (most likely none are, in which 
case the underlying Lapack routine can use a more efficient calculation).

> 
> Best,
> 
> luke
> 
> On Fri, 13 Aug 2021, Dario Strbenac via R-devel wrote:
> 
>> Good day,
>>
>> I have a real scenario involving 45 million biological cells (samples) 
>> and 60 proteins (variables) which leads to a segmentation fault for 
>> svd. I thought this might be a good example of why it might benefit 
>> from a long vector upgrade.
>>
>> test <- matrix(rnorm(45000000*60), ncol = 60)
>> testSVD <- svd(test)
>>
>> *** caught segfault ***
>> address 0x7fe93514d618, cause 'memory not mapped'
>>
>> Traceback:
>> 1: La.svd(x, nu, nv)
>> 2: svd(test)



-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Emeritus Professor of Applied Statistics, University of Oxford


From @ndre @end|ng |rom g||||bert@|r  Fri Aug 13 13:26:43 2021
From: @ndre @end|ng |rom g||||bert@|r (=?UTF-8?Q?Andr=c3=a9_GILLIBERT?=)
Date: Fri, 13 Aug 2021 13:26:43 +0200
Subject: [Rd] ,
 Problem in random number generation for Marsaglia-Multicarry +,
 Kinderman-Ramage
In-Reply-To: <0EB03E15-B535-4D78-98D0-6A73E224235A@gmail.com>
References: <54745780b6c1421298e55e1e8c18d6ab@chu-rouen.fr>
 <0EB03E15-B535-4D78-98D0-6A73E224235A@gmail.com>
Message-ID: <50bb3316-7510-d301-abf6-4aa88c4182e8@gillibert.fr>

Thank you.

It looks like the M-M generator is deeply flawed, although R 
implementation seems to be faithful.

I can understand that R keeps this generator for historical reasons.

However, R should at least warn that this generator is buggy and should 
not be used!

That could be as simple as a Warning message generated by set.seed().


-- 

Sincerely

Andr? GILLIBERT


 > With these matters, one has to be careful to distinguish between 
method error and implementation error.
 > The reason for changing the RNG setup in R v. 1.7.0 was pretty much 
this kind of unfortunate interaction between M-M and K-R. There are even 
more egregious examples for the distribution of maxima of normal 
variables. Try e.g.
 >
 > RNGversion("1.6.0") # Marsaglia-Multicarry, Kinderman-Ramage
 >? s <- replicate(1e6,max(rnorm(10)))
 >? plot(density(s))
 >
 > (A further bug in K-R was fixed in 1.7.1, but that is tangential to 
this.)
 >
 > A glimpse of the source of the problem is seen in the 
"microcorrelations" in this:
 ?>
 > RNGkind("Mar");m <- matrix(runif(4e7),2)
 >? plot(m[1,],m[2,],xlim=c(0,1e-3),pch=".")
 >? m <- matrix(runif(4e7),2)
 >? points(m[1,],m[2,],pch=".")
 >
 > These examples are from 2003, so the issue has been known for almost 
2 decades. However, to the best of our knowledge, the M-M RNG is a 
faithful implementation of their > method, so we have left the RNG in 
R's arsenal, in case someone needed it for some specific purpose.
 >
 > - pd
 >


From d@tr7320 @end|ng |rom un|@@ydney@edu@@u  Sat Aug 14 02:00:04 2021
From: d@tr7320 @end|ng |rom un|@@ydney@edu@@u (Dario Strbenac)
Date: Sat, 14 Aug 2021 00:00:04 +0000
Subject: [Rd] [External]  svd For Large Matrix
In-Reply-To: <alpine.DEB.2.22.394.2108130958100.3514@luke-Latitude-7480>
References: <SYBPR01MB4761CA7FD01EB93E4CF8511ACDFA9@SYBPR01MB4761.ausprd01.prod.outlook.com>
 <alpine.DEB.2.22.394.2108130958100.3514@luke-Latitude-7480>
Message-ID: <SYBPR01MB4761D9277370E4FDF84CA821CDFA9@SYBPR01MB4761.ausprd01.prod.outlook.com>

Good day,

Ah, I was confident it wouldn't be environment-specific but it is. My environment is

R version 4.1.0 (2021-05-18)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Debian GNU/Linux 10 (buster)

Matrix products: default
BLAS:   /usr/lib/x86_64-linux-gnu/atlas/libblas.so.3.10.3
LAPACK: /usr/lib/x86_64-linux-gnu/atlas/liblapack.so.3.10.3

It crashes at about 180 GB RAM usage. The server has 1024 GB physical RAM in it. Modestly downsampling to 30 million cells avoids the segmentation fault. The segmentation fault originates from BLAS

Program received signal SIGSEGV, Segmentation fault.
0x00007ffff7649c10 in ATL_dgecopy () from /usr/lib/x86_64-linux-gnu/libblas.so.3

--------------------------------------
Dario Strbenac
University of Sydney
Camperdown NSW 2050
Australia

From edd @end|ng |rom deb|@n@org  Sat Aug 14 03:39:18 2021
From: edd @end|ng |rom deb|@n@org (Dirk Eddelbuettel)
Date: Fri, 13 Aug 2021 20:39:18 -0500
Subject: [Rd] [External]  svd For Large Matrix
In-Reply-To: <SYBPR01MB4761D9277370E4FDF84CA821CDFA9@SYBPR01MB4761.ausprd01.prod.outlook.com>
References: <SYBPR01MB4761CA7FD01EB93E4CF8511ACDFA9@SYBPR01MB4761.ausprd01.prod.outlook.com>
 <alpine.DEB.2.22.394.2108130958100.3514@luke-Latitude-7480>
 <SYBPR01MB4761D9277370E4FDF84CA821CDFA9@SYBPR01MB4761.ausprd01.prod.outlook.com>
Message-ID: <24855.7878.540352.589621@rob.eddelbuettel.com>


Dario,

On 14 August 2021 at 00:00, Dario Strbenac via R-devel wrote:
| Good day,
| 
| Ah, I was confident it wouldn't be environment-specific but it is. My environment is
| 
| R version 4.1.0 (2021-05-18)
| Platform: x86_64-pc-linux-gnu (64-bit)
| Running under: Debian GNU/Linux 10 (buster)
| 
| Matrix products: default
| BLAS:   /usr/lib/x86_64-linux-gnu/atlas/libblas.so.3.10.3
| LAPACK: /usr/lib/x86_64-linux-gnu/atlas/liblapack.so.3.10.3
| 
| It crashes at about 180 GB RAM usage. The server has 1024 GB physical RAM in it. Modestly downsampling to 30 million cells avoids the segmentation fault. The segmentation fault originates from BLAS
| 
| Program received signal SIGSEGV, Segmentation fault.
| 0x00007ffff7649c10 in ATL_dgecopy () from /usr/lib/x86_64-linux-gnu/libblas.so.3

This would allow to do what was suggested: trying different BLAS. On Debian
and alike you can just install Atlas (as you have), or OpenBLAS, or the
reference BLAS (and there my script and post from a few years ago to use MKL
but depending on which release you use may actually be directly accessible),
or now also BLIS, or ...

In short, we see a bug when using Atlas. I would at least try OpenBLAS.

Here is what I see in a Docker container using testing/unstable -- you will
see a shorter list but you *will* have the three different openblas versions
at a minimum.

root at somedocker:~# apt-cache search libblas | grep -- -dev
libatlas-base-dev - Automatically Tuned Linear Algebra Software, generic static
libblis-openmp-dev - BLAS-like Library Instantiation Software Framework (dev,32bit,openmp)
libblis-pthread-dev - BLAS-like Library Instantiation Software Framework (dev,32bit,pthread)
libblis-serial-dev - BLAS-like Library Instantiation Software Framework (dev,32bit,serial)
libblis64-openmp-dev - BLAS-like Library Instantiation Software Framework (dev,64bit,openmp)
libblis64-pthread-dev - BLAS-like Library Instantiation Software Framework (dev,64bit,pthread)
libblis64-serial-dev - BLAS-like Library Instantiation Software Framework (dev,64bit,serial)
libblas-dev - Basic Linear Algebra Subroutines 3, static library
libblas64-dev - Basic Linear Algebra Subroutines 3, static library (64bit-index)
libopenblas-openmp-dev - Optimized BLAS (linear algebra) library (dev, openmp)
libopenblas-pthread-dev - Optimized BLAS (linear algebra) library (dev, pthread)
libopenblas-serial-dev - Optimized BLAS (linear algebra) library (dev, serial)
libopenblas64-openmp-dev - Optimized BLAS (linear algebra) library (dev, 64bit, openmp)
libopenblas64-pthread-dev - Optimized BLAS (linear algebra) library (dev, 64bit, pthread)
libopenblas64-serial-dev - Optimized BLAS (linear algebra) library (dev, 64bit, serial)
libblasr-dev - tools for aligning PacBio reads to target sequences (development files)
root at somedocker:~# 

Dirk

-- 
https://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From tdhock5 @end|ng |rom gm@||@com  Sat Aug 14 22:48:40 2021
From: tdhock5 @end|ng |rom gm@||@com (Toby Hocking)
Date: Sat, 14 Aug 2021 13:48:40 -0700
Subject: [Rd] na.omit inconsistent with is.na on list
In-Reply-To: <CAJmOi+OEVLufPHwe1y8=1jb_Btmh=0m2TkvTZ6wkkwKGcbGsSw@mail.gmail.com>
References: <CALK03d0vRmxNLzyU-s8O1Urg6yneYZeSKtdRizx5Z=ZZ57Fdcg@mail.gmail.com>
 <CALK03d0pPSy5CL2AHZMtso5_7uXW5NfxZfC+mUsMm8yo8rOViA@mail.gmail.com>
 <CAD4oTHGni5YDKES5yKtzNPqGDV3vrP+7YNFCAJ_Y0s-Cw0xeUQ@mail.gmail.com>
 <CALEXWq1hQMpf1Cf9NEbEC6eAPsw+4-Uqka7ym5Dxc2PKRqfR9A@mail.gmail.com>
 <CAJmOi+OEVLufPHwe1y8=1jb_Btmh=0m2TkvTZ6wkkwKGcbGsSw@mail.gmail.com>
Message-ID: <CALK03d1YchVyVqw_zWZ3N8fHUZ71nr7oeYvhjA=_4XLKGHDqCw@mail.gmail.com>

Some relevant information from ?is.na: the behavior for lists is
documented,

     For is.na, elementwise the result is false unless that element
     is a length-one atomic vector and the single element of that
     vector is regarded as NA or NaN (note that any is.na method
     for the class of the element is ignored).

Also there are other functions anyNA and is.na<- which are consistent with
is.na. That is, anyNA only returns TRUE if the list has an element which is
a scalar NA. And is.na<- sets list elements to logical NA to indicate
missingness.

On Fri, Aug 13, 2021 at 1:10 AM Hugh Parsonage <hugh.parsonage at gmail.com>
wrote:

> The data.frame method deliberately skips non-atomic columns before
> invoking is.na(x) so I think it is fair to assume this behaviour is
> intentional and assumed.
>
> Not so clear to me that there is a sensible answer for list columns.
> (List columns seem to collide with the expectation that in each
> variable every observation will be of the same type)
>
> Consider your list L as
>
> L <- list(NULL, NA, c(NA, NA))
>
> Seems like every observation could have a claim to be 'missing' here.
> Concretely, if a data.frame had a list column representing the lat-lon
> of an observation, we might only be able to represent missing values
> like c(NA, NA).
>
> On Fri, 13 Aug 2021 at 17:27, I?aki Ucar <iucar at fedoraproject.org> wrote:
> >
> > On Thu, 12 Aug 2021 at 22:20, Gabriel Becker <gabembecker at gmail.com>
> wrote:
> > >
> > > Hi Toby,
> > >
> > > This definitely appears intentional, the first  expression of
> > > stats:::na.omit.default is
> > >
> > >    if (!is.atomic(object))
> > >
> > >         return(object)
> >
> > I don't follow your point. This only means that the *default* method
> > is not intended for non-atomic cases, but it doesn't mean it shouldn't
> > exist a method for lists.
> >
> > > So it is explicitly just returning the object in non-atomic cases,
> which
> > > includes lists. I was not involved in this decision (obviously) but my
> > > guess is that it is due to the fact that what constitutes an
> observation
> > > "being complete" in unclear in the list case. What should
> > >
> > > na.omit(list(5, NA, c(NA, 5)))
> > >
> > > return? Just the first element, or the first and the last? It seems, at
> > > least to me, unclear. A small change to the documentation to to add
> "atomic
> >
> > > is.na(list(5, NA, c(NA, 5)))
> > [1] FALSE  TRUE FALSE
> >
> > Following Toby's argument, it's clear to me: the first and the last.
> >
> > I?aki
> >
> > > (in the sense of is.atomic returning \code{TRUE})" in front of
> "vectors"
> > > or similar  where what types of objects are supported seems justified,
> > > though, imho, as the current documentation is either ambiguous or
> > > technically incorrect, depending on what we take "vector" to mean.
> > >
> > > Best,
> > > ~G
> > >
> > > On Wed, Aug 11, 2021 at 10:16 PM Toby Hocking <tdhock5 at gmail.com>
> wrote:
> > >
> > > > Also, the na.omit method for data.frame with list column seems to be
> > > > inconsistent with is.na,
> > > >
> > > > > L <- list(NULL, NA, 0)
> > > > > str(f <- data.frame(I(L)))
> > > > 'data.frame': 3 obs. of  1 variable:
> > > >  $ L:List of 3
> > > >   ..$ : NULL
> > > >   ..$ : logi NA
> > > >   ..$ : num 0
> > > >   ..- attr(*, "class")= chr "AsIs"
> > > > > is.na(f)
> > > >          L
> > > > [1,] FALSE
> > > > [2,]  TRUE
> > > > [3,] FALSE
> > > > > na.omit(f)
> > > >    L
> > > > 1
> > > > 2 NA
> > > > 3  0
> > > >
> > > > On Wed, Aug 11, 2021 at 9:58 PM Toby Hocking <tdhock5 at gmail.com>
> wrote:
> > > >
> > > > > na.omit is documented as "na.omit returns the object with
> incomplete
> > > > cases
> > > > > removed." and "At present these will handle vectors," so I
> expected that
> > > > > when it is used on a list, it should return the same thing as if we
> > > > subset
> > > > > via is.na; however I observed the following,
> > > > >
> > > > > > L <- list(NULL, NA, 0)
> > > > > > str(L[!is.na(L)])
> > > > > List of 2
> > > > >  $ : NULL
> > > > >  $ : num 0
> > > > > > str(na.omit(L))
> > > > > List of 3
> > > > >  $ : NULL
> > > > >  $ : logi NA
> > > > >  $ : num 0
> > > > >
> > > > > Should na.omit be fixed so that it returns a result that is
> consistent
> > > > > with is.na? I assume that is.na is the canonical definition of
> what
> > > > > should be considered a missing value in R.
> > > > >
> > > >
> > > >         [[alternative HTML version deleted]]
> > > >
> > > > ______________________________________________
> > > > R-devel at r-project.org mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-devel
> > > >
> > >
> > >         [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-devel at r-project.org mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-devel
> >
> >
> >
> > --
> > I?aki ?car
> >
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From g@bembecker @end|ng |rom gm@||@com  Sun Aug 15 02:15:13 2021
From: g@bembecker @end|ng |rom gm@||@com (Gabriel Becker)
Date: Sat, 14 Aug 2021 17:15:13 -0700
Subject: [Rd] na.omit inconsistent with is.na on list
In-Reply-To: <CALK03d1YchVyVqw_zWZ3N8fHUZ71nr7oeYvhjA=_4XLKGHDqCw@mail.gmail.com>
References: <CALK03d0vRmxNLzyU-s8O1Urg6yneYZeSKtdRizx5Z=ZZ57Fdcg@mail.gmail.com>
 <CALK03d0pPSy5CL2AHZMtso5_7uXW5NfxZfC+mUsMm8yo8rOViA@mail.gmail.com>
 <CAD4oTHGni5YDKES5yKtzNPqGDV3vrP+7YNFCAJ_Y0s-Cw0xeUQ@mail.gmail.com>
 <CALEXWq1hQMpf1Cf9NEbEC6eAPsw+4-Uqka7ym5Dxc2PKRqfR9A@mail.gmail.com>
 <CAJmOi+OEVLufPHwe1y8=1jb_Btmh=0m2TkvTZ6wkkwKGcbGsSw@mail.gmail.com>
 <CALK03d1YchVyVqw_zWZ3N8fHUZ71nr7oeYvhjA=_4XLKGHDqCw@mail.gmail.com>
Message-ID: <CAD4oTHEnKOa8GHikHzfzN+Vn1D5h+UJ0_EDftF=7qfAhcw4vsA@mail.gmail.com>

I understand what is.na does, the issue I have is that its task is not
equivalent to the conceptual task na.omit is doing, in my opinion, as
illustrated by what the data.frame method does.

Thus what i was getting at above about it not being clear that lst[is.na(lst)]
being the correct thing for na.omit to do

~G

~G

On Sat, Aug 14, 2021, 1:49 PM Toby Hocking <tdhock5 at gmail.com> wrote:

> Some relevant information from ?is.na: the behavior for lists is
> documented,
>
>      For is.na, elementwise the result is false unless that element
>      is a length-one atomic vector and the single element of that
>      vector is regarded as NA or NaN (note that any is.na method
>      for the class of the element is ignored).
>
> Also there are other functions anyNA and is.na<- which are consistent with
> is.na. That is, anyNA only returns TRUE if the list has an element which
> is
> a scalar NA. And is.na<- sets list elements to logical NA to indicate
> missingness.
>
> On Fri, Aug 13, 2021 at 1:10 AM Hugh Parsonage <hugh.parsonage at gmail.com>
> wrote:
>
> > The data.frame method deliberately skips non-atomic columns before
> > invoking is.na(x) so I think it is fair to assume this behaviour is
> > intentional and assumed.
> >
> > Not so clear to me that there is a sensible answer for list columns.
> > (List columns seem to collide with the expectation that in each
> > variable every observation will be of the same type)
> >
> > Consider your list L as
> >
> > L <- list(NULL, NA, c(NA, NA))
> >
> > Seems like every observation could have a claim to be 'missing' here.
> > Concretely, if a data.frame had a list column representing the lat-lon
> > of an observation, we might only be able to represent missing values
> > like c(NA, NA).
> >
> > On Fri, 13 Aug 2021 at 17:27, I?aki Ucar <iucar at fedoraproject.org>
> wrote:
> > >
> > > On Thu, 12 Aug 2021 at 22:20, Gabriel Becker <gabembecker at gmail.com>
> > wrote:
> > > >
> > > > Hi Toby,
> > > >
> > > > This definitely appears intentional, the first  expression of
> > > > stats:::na.omit.default is
> > > >
> > > >    if (!is.atomic(object))
> > > >
> > > >         return(object)
> > >
> > > I don't follow your point. This only means that the *default* method
> > > is not intended for non-atomic cases, but it doesn't mean it shouldn't
> > > exist a method for lists.
> > >
> > > > So it is explicitly just returning the object in non-atomic cases,
> > which
> > > > includes lists. I was not involved in this decision (obviously) but
> my
> > > > guess is that it is due to the fact that what constitutes an
> > observation
> > > > "being complete" in unclear in the list case. What should
> > > >
> > > > na.omit(list(5, NA, c(NA, 5)))
> > > >
> > > > return? Just the first element, or the first and the last? It seems,
> at
> > > > least to me, unclear. A small change to the documentation to to add
> > "atomic
> > >
> > > > is.na(list(5, NA, c(NA, 5)))
> > > [1] FALSE  TRUE FALSE
> > >
> > > Following Toby's argument, it's clear to me: the first and the last.
> > >
> > > I?aki
> > >
> > > > (in the sense of is.atomic returning \code{TRUE})" in front of
> > "vectors"
> > > > or similar  where what types of objects are supported seems
> justified,
> > > > though, imho, as the current documentation is either ambiguous or
> > > > technically incorrect, depending on what we take "vector" to mean.
> > > >
> > > > Best,
> > > > ~G
> > > >
> > > > On Wed, Aug 11, 2021 at 10:16 PM Toby Hocking <tdhock5 at gmail.com>
> > wrote:
> > > >
> > > > > Also, the na.omit method for data.frame with list column seems to
> be
> > > > > inconsistent with is.na,
> > > > >
> > > > > > L <- list(NULL, NA, 0)
> > > > > > str(f <- data.frame(I(L)))
> > > > > 'data.frame': 3 obs. of  1 variable:
> > > > >  $ L:List of 3
> > > > >   ..$ : NULL
> > > > >   ..$ : logi NA
> > > > >   ..$ : num 0
> > > > >   ..- attr(*, "class")= chr "AsIs"
> > > > > > is.na(f)
> > > > >          L
> > > > > [1,] FALSE
> > > > > [2,]  TRUE
> > > > > [3,] FALSE
> > > > > > na.omit(f)
> > > > >    L
> > > > > 1
> > > > > 2 NA
> > > > > 3  0
> > > > >
> > > > > On Wed, Aug 11, 2021 at 9:58 PM Toby Hocking <tdhock5 at gmail.com>
> > wrote:
> > > > >
> > > > > > na.omit is documented as "na.omit returns the object with
> > incomplete
> > > > > cases
> > > > > > removed." and "At present these will handle vectors," so I
> > expected that
> > > > > > when it is used on a list, it should return the same thing as if
> we
> > > > > subset
> > > > > > via is.na; however I observed the following,
> > > > > >
> > > > > > > L <- list(NULL, NA, 0)
> > > > > > > str(L[!is.na(L)])
> > > > > > List of 2
> > > > > >  $ : NULL
> > > > > >  $ : num 0
> > > > > > > str(na.omit(L))
> > > > > > List of 3
> > > > > >  $ : NULL
> > > > > >  $ : logi NA
> > > > > >  $ : num 0
> > > > > >
> > > > > > Should na.omit be fixed so that it returns a result that is
> > consistent
> > > > > > with is.na? I assume that is.na is the canonical definition of
> > what
> > > > > > should be considered a missing value in R.
> > > > > >
> > > > >
> > > > >         [[alternative HTML version deleted]]
> > > > >
> > > > > ______________________________________________
> > > > > R-devel at r-project.org mailing list
> > > > > https://stat.ethz.ch/mailman/listinfo/r-devel
> > > > >
> > > >
> > > >         [[alternative HTML version deleted]]
> > > >
> > > > ______________________________________________
> > > > R-devel at r-project.org mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-devel
> > >
> > >
> > >
> > > --
> > > I?aki ?car
> > >
> > > ______________________________________________
> > > R-devel at r-project.org mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-devel
> >
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From henr|k@bengt@@on @end|ng |rom gm@||@com  Mon Aug 16 13:52:04 2021
From: henr|k@bengt@@on @end|ng |rom gm@||@com (Henrik Bengtsson)
Date: Mon, 16 Aug 2021 13:52:04 +0200
Subject: [Rd] 
 Force quitting a FORK cluster node on macOS and Solaris wreaks havoc
In-Reply-To: <4CF34F70-DCAC-4694-A8CF-38E7E0B43272@R-project.org>
References: <CAFDcVCRbrue58KodpVHOzESkyTFgajRP514_m0q33bAy4WU=nA@mail.gmail.com>
 <4CF34F70-DCAC-4694-A8CF-38E7E0B43272@R-project.org>
Message-ID: <CAFDcVCRjScSTJuYwoyK61w2dqJYXPBs0wHA6S315yzMzx-LqMw@mail.gmail.com>

Thank you Simon, this is helpful.  I take this is specific to quit(),
so it's a poor choice for emulating crashed parallel workers, and
Sys.kill() is much better for that.

I was focusing on that odd extra execution/output, but as you say,
there are lots of other things that is done by quit() here, e.g.
regardless of platform quit() damages the main R process too:

> f <- parallel::mcparallel(quit("no"))
> v <- parallel::mccollect(f)
Warning message:
In parallel::mccollect(f) : 1 parallel job did not deliver a result
> file.exists(tempdir())
[1] FALSE


Would it be sufficient to make quit() fork safe by, conceptually,
doing something like:

quit <- function(save = "default", status = 0, runLast = TRUE) {
  if (parallel:::isChild())
      stop("quit() must not be called in a forked process")
  .Internal(quit(save, status, runLast))
}

This would protect against calling quit() in forked code by mistake,
e.g. when someone parallelize over code/scripts they don't have full
control over and the ones who write those scripts might not be aware
that they may be used in forks.

Thanks,

Henrik


From r@d|ord @end|ng |rom c@@toronto@edu  Mon Aug 16 17:30:32 2021
From: r@d|ord @end|ng |rom c@@toronto@edu (Radford Neal)
Date: Mon, 16 Aug 2021 11:30:32 -0400
Subject: [Rd] svd for Large Matrix
In-Reply-To: <mailman.51877.7.1628848801.65024.r-devel@r-project.org>
References: <mailman.51877.7.1628848801.65024.r-devel@r-project.org>
Message-ID: <20210816153032.GA4468@mail.cs.toronto.edu>

> Dario Strbenac <dstr7320 at uni.sydney.edu.au> writes:
>
> I have a real scenario involving 45 million biological cells
> (samples) and 60 proteins (variables) which leads to a segmentation
> fault for svd. I thought this might be a good example of why it
> might benefit from a long vector upgrade.

Rather than the full SVD of a 45000000x60 X, my guess is that you
may really only be interested in the eigenvalues and eigenvectors of
X^T X, in which case eigen(t(X)%*%X) would probably be much faster.
(And eigen(crossprod(X)) would be even faster.)

Note that if you instead want the eigenvalues and eigenvectors of
X X^T (which is an enormous matrix), the eigenvalues of this are the
same as those of X^T X, and the eigenvectors are Xv, where v is an
eigenvector of X^T X.

For example, with R 4.0.2, and the reference BLAS/LAPACK, I get

  > X<-matrix(rnorm(100000),10000,10)
  > system.time(for(i in 1:1000) rs<-svd(X))
     user  system elapsed
    2.393   0.008   2.403
  > system.time(for(i in 1:1000) re<-eigen(crossprod(X)))
     user  system elapsed
    0.609   0.000   0.609
  > rs$d^2
   [1] 10568.003 10431.864 10318.959 10219.961 10138.025 10068.566  9931.538
   [8]  9813.841  9703.818  9598.532
  > re$values
   [1] 10568.003 10431.864 10318.959 10219.961 10138.025 10068.566  9931.538
   [8]  9813.841  9703.818  9598.532

Possibly some other LAPACK might implement svd better, though I
suspect that R will allocate more big matrices than really necessary
for the svd even aside from whatever LAPACK is doing.

Regards,

   Radford Neal


From @vr@h@m@@d|er @end|ng |rom gm@||@com  Mon Aug 16 17:56:42 2021
From: @vr@h@m@@d|er @end|ng |rom gm@||@com (Avraham Adler)
Date: Mon, 16 Aug 2021 18:56:42 +0300
Subject: [Rd] svd for Large Matrix
In-Reply-To: <20210816153032.GA4468@mail.cs.toronto.edu>
References: <mailman.51877.7.1628848801.65024.r-devel@r-project.org>
 <20210816153032.GA4468@mail.cs.toronto.edu>
Message-ID: <CAL6gwnK5GbPrncSVrAbC4XZVSNjwb2GqhUzmk4vL2mPOg6eYgw@mail.gmail.com>

If you?re crossproding X by itself, I think passing symmetric = TRUE to
eigen will eke out more speed.

Avi

On Mon, Aug 16, 2021 at 6:30 PM Radford Neal <radford at cs.toronto.edu> wrote:

> > Dario Strbenac <dstr7320 at uni.sydney.edu.au> writes:
> >
> > I have a real scenario involving 45 million biological cells
> > (samples) and 60 proteins (variables) which leads to a segmentation
> > fault for svd. I thought this might be a good example of why it
> > might benefit from a long vector upgrade.
>
> Rather than the full SVD of a 45000000x60 X, my guess is that you
> may really only be interested in the eigenvalues and eigenvectors of
> X^T X, in which case eigen(t(X)%*%X) would probably be much faster.
> (And eigen(crossprod(X)) would be even faster.)
>
> Note that if you instead want the eigenvalues and eigenvectors of
> X X^T (which is an enormous matrix), the eigenvalues of this are the
> same as those of X^T X, and the eigenvectors are Xv, where v is an
> eigenvector of X^T X.
>
> For example, with R 4.0.2, and the reference BLAS/LAPACK, I get
>
>   > X<-matrix(rnorm(100000),10000,10)
>   > system.time(for(i in 1:1000) rs<-svd(X))
>      user  system elapsed
>     2.393   0.008   2.403
>   > system.time(for(i in 1:1000) re<-eigen(crossprod(X)))
>      user  system elapsed
>     0.609   0.000   0.609
>   > rs$d^2
>    [1] 10568.003 10431.864 10318.959 10219.961 10138.025 10068.566
> 9931.538
>    [8]  9813.841  9703.818  9598.532
>   > re$values
>    [1] 10568.003 10431.864 10318.959 10219.961 10138.025 10068.566
> 9931.538
>    [8]  9813.841  9703.818  9598.532
>
> Possibly some other LAPACK might implement svd better, though I
> suspect that R will allocate more big matrices than really necessary
> for the svd even aside from whatever LAPACK is doing.
>
> Regards,
>
>    Radford Neal
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>
-- 
Sent from Gmail Mobile

	[[alternative HTML version deleted]]


From tdhock5 @end|ng |rom gm@||@com  Mon Aug 16 19:54:15 2021
From: tdhock5 @end|ng |rom gm@||@com (Toby Hocking)
Date: Mon, 16 Aug 2021 10:54:15 -0700
Subject: [Rd] na.omit inconsistent with is.na on list
In-Reply-To: <CAD4oTHEnKOa8GHikHzfzN+Vn1D5h+UJ0_EDftF=7qfAhcw4vsA@mail.gmail.com>
References: <CALK03d0vRmxNLzyU-s8O1Urg6yneYZeSKtdRizx5Z=ZZ57Fdcg@mail.gmail.com>
 <CALK03d0pPSy5CL2AHZMtso5_7uXW5NfxZfC+mUsMm8yo8rOViA@mail.gmail.com>
 <CAD4oTHGni5YDKES5yKtzNPqGDV3vrP+7YNFCAJ_Y0s-Cw0xeUQ@mail.gmail.com>
 <CALEXWq1hQMpf1Cf9NEbEC6eAPsw+4-Uqka7ym5Dxc2PKRqfR9A@mail.gmail.com>
 <CAJmOi+OEVLufPHwe1y8=1jb_Btmh=0m2TkvTZ6wkkwKGcbGsSw@mail.gmail.com>
 <CALK03d1YchVyVqw_zWZ3N8fHUZ71nr7oeYvhjA=_4XLKGHDqCw@mail.gmail.com>
 <CAD4oTHEnKOa8GHikHzfzN+Vn1D5h+UJ0_EDftF=7qfAhcw4vsA@mail.gmail.com>
Message-ID: <CALK03d0+iwDJ1utWf5P0WAzDq9MEo3Rihm_be-QVtyXOBm07fw@mail.gmail.com>

To clarify, ?is.na docs say that 'na.omit' returns the object with
incomplete cases removed.
If we take is.na to be the definition of "incomplete cases" then a list
element with scalar NA is incomplete.
About the data.frame method, in my opinion it is highly
confusing/inconsistent for na.omit to keep rows with incomplete cases in
list columns, but not in columns which are atomic vectors,

> (f.num <- data.frame(num=c(1,NA,2)))
  num
1   1
2  NA
3   2
> is.na(f.num)
       num
[1,] FALSE
[2,]  TRUE
[3,] FALSE
> na.omit(f.num)
  num
1   1
3   2

> (f.list <- data.frame(list=I(list(1,NA,2))))
  list
1    1
2   NA
3    2
> is.na(f.list)
      list
[1,] FALSE
[2,]  TRUE
[3,] FALSE
> na.omit(f.list)
  list
1    1
2   NA
3    2

On Sat, Aug 14, 2021 at 5:15 PM Gabriel Becker <gabembecker at gmail.com>
wrote:

> I understand what is.na does, the issue I have is that its task is not
> equivalent to the conceptual task na.omit is doing, in my opinion, as
> illustrated by what the data.frame method does.
>
> Thus what i was getting at above about it not being clear that lst[is.na(lst)]
> being the correct thing for na.omit to do
>
> ~G
>
> ~G
>
> On Sat, Aug 14, 2021, 1:49 PM Toby Hocking <tdhock5 at gmail.com> wrote:
>
>> Some relevant information from ?is.na: the behavior for lists is
>> documented,
>>
>>      For is.na, elementwise the result is false unless that element
>>      is a length-one atomic vector and the single element of that
>>      vector is regarded as NA or NaN (note that any is.na method
>>      for the class of the element is ignored).
>>
>> Also there are other functions anyNA and is.na<- which are consistent
>> with
>> is.na. That is, anyNA only returns TRUE if the list has an element which
>> is
>> a scalar NA. And is.na<- sets list elements to logical NA to indicate
>> missingness.
>>
>> On Fri, Aug 13, 2021 at 1:10 AM Hugh Parsonage <hugh.parsonage at gmail.com>
>> wrote:
>>
>> > The data.frame method deliberately skips non-atomic columns before
>> > invoking is.na(x) so I think it is fair to assume this behaviour is
>> > intentional and assumed.
>> >
>> > Not so clear to me that there is a sensible answer for list columns.
>> > (List columns seem to collide with the expectation that in each
>> > variable every observation will be of the same type)
>> >
>> > Consider your list L as
>> >
>> > L <- list(NULL, NA, c(NA, NA))
>> >
>> > Seems like every observation could have a claim to be 'missing' here.
>> > Concretely, if a data.frame had a list column representing the lat-lon
>> > of an observation, we might only be able to represent missing values
>> > like c(NA, NA).
>> >
>> > On Fri, 13 Aug 2021 at 17:27, I?aki Ucar <iucar at fedoraproject.org>
>> wrote:
>> > >
>> > > On Thu, 12 Aug 2021 at 22:20, Gabriel Becker <gabembecker at gmail.com>
>> > wrote:
>> > > >
>> > > > Hi Toby,
>> > > >
>> > > > This definitely appears intentional, the first  expression of
>> > > > stats:::na.omit.default is
>> > > >
>> > > >    if (!is.atomic(object))
>> > > >
>> > > >         return(object)
>> > >
>> > > I don't follow your point. This only means that the *default* method
>> > > is not intended for non-atomic cases, but it doesn't mean it shouldn't
>> > > exist a method for lists.
>> > >
>> > > > So it is explicitly just returning the object in non-atomic cases,
>> > which
>> > > > includes lists. I was not involved in this decision (obviously) but
>> my
>> > > > guess is that it is due to the fact that what constitutes an
>> > observation
>> > > > "being complete" in unclear in the list case. What should
>> > > >
>> > > > na.omit(list(5, NA, c(NA, 5)))
>> > > >
>> > > > return? Just the first element, or the first and the last? It
>> seems, at
>> > > > least to me, unclear. A small change to the documentation to to add
>> > "atomic
>> > >
>> > > > is.na(list(5, NA, c(NA, 5)))
>> > > [1] FALSE  TRUE FALSE
>> > >
>> > > Following Toby's argument, it's clear to me: the first and the last.
>> > >
>> > > I?aki
>> > >
>> > > > (in the sense of is.atomic returning \code{TRUE})" in front of
>> > "vectors"
>> > > > or similar  where what types of objects are supported seems
>> justified,
>> > > > though, imho, as the current documentation is either ambiguous or
>> > > > technically incorrect, depending on what we take "vector" to mean.
>> > > >
>> > > > Best,
>> > > > ~G
>> > > >
>> > > > On Wed, Aug 11, 2021 at 10:16 PM Toby Hocking <tdhock5 at gmail.com>
>> > wrote:
>> > > >
>> > > > > Also, the na.omit method for data.frame with list column seems to
>> be
>> > > > > inconsistent with is.na,
>> > > > >
>> > > > > > L <- list(NULL, NA, 0)
>> > > > > > str(f <- data.frame(I(L)))
>> > > > > 'data.frame': 3 obs. of  1 variable:
>> > > > >  $ L:List of 3
>> > > > >   ..$ : NULL
>> > > > >   ..$ : logi NA
>> > > > >   ..$ : num 0
>> > > > >   ..- attr(*, "class")= chr "AsIs"
>> > > > > > is.na(f)
>> > > > >          L
>> > > > > [1,] FALSE
>> > > > > [2,]  TRUE
>> > > > > [3,] FALSE
>> > > > > > na.omit(f)
>> > > > >    L
>> > > > > 1
>> > > > > 2 NA
>> > > > > 3  0
>> > > > >
>> > > > > On Wed, Aug 11, 2021 at 9:58 PM Toby Hocking <tdhock5 at gmail.com>
>> > wrote:
>> > > > >
>> > > > > > na.omit is documented as "na.omit returns the object with
>> > incomplete
>> > > > > cases
>> > > > > > removed." and "At present these will handle vectors," so I
>> > expected that
>> > > > > > when it is used on a list, it should return the same thing as
>> if we
>> > > > > subset
>> > > > > > via is.na; however I observed the following,
>> > > > > >
>> > > > > > > L <- list(NULL, NA, 0)
>> > > > > > > str(L[!is.na(L)])
>> > > > > > List of 2
>> > > > > >  $ : NULL
>> > > > > >  $ : num 0
>> > > > > > > str(na.omit(L))
>> > > > > > List of 3
>> > > > > >  $ : NULL
>> > > > > >  $ : logi NA
>> > > > > >  $ : num 0
>> > > > > >
>> > > > > > Should na.omit be fixed so that it returns a result that is
>> > consistent
>> > > > > > with is.na? I assume that is.na is the canonical definition of
>> > what
>> > > > > > should be considered a missing value in R.
>> > > > > >
>> > > > >
>> > > > >         [[alternative HTML version deleted]]
>> > > > >
>> > > > > ______________________________________________
>> > > > > R-devel at r-project.org mailing list
>> > > > > https://stat.ethz.ch/mailman/listinfo/r-devel
>> > > > >
>> > > >
>> > > >         [[alternative HTML version deleted]]
>> > > >
>> > > > ______________________________________________
>> > > > R-devel at r-project.org mailing list
>> > > > https://stat.ethz.ch/mailman/listinfo/r-devel
>> > >
>> > >
>> > >
>> > > --
>> > > I?aki ?car
>> > >
>> > > ______________________________________________
>> > > R-devel at r-project.org mailing list
>> > > https://stat.ethz.ch/mailman/listinfo/r-devel
>> >
>> > ______________________________________________
>> > R-devel at r-project.org mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-devel
>> >
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>

	[[alternative HTML version deleted]]


From g@bembecker @end|ng |rom gm@||@com  Mon Aug 16 21:21:45 2021
From: g@bembecker @end|ng |rom gm@||@com (Gabriel Becker)
Date: Mon, 16 Aug 2021 12:21:45 -0700
Subject: [Rd] na.omit inconsistent with is.na on list
In-Reply-To: <CALK03d0+iwDJ1utWf5P0WAzDq9MEo3Rihm_be-QVtyXOBm07fw@mail.gmail.com>
References: <CALK03d0vRmxNLzyU-s8O1Urg6yneYZeSKtdRizx5Z=ZZ57Fdcg@mail.gmail.com>
 <CALK03d0pPSy5CL2AHZMtso5_7uXW5NfxZfC+mUsMm8yo8rOViA@mail.gmail.com>
 <CAD4oTHGni5YDKES5yKtzNPqGDV3vrP+7YNFCAJ_Y0s-Cw0xeUQ@mail.gmail.com>
 <CALEXWq1hQMpf1Cf9NEbEC6eAPsw+4-Uqka7ym5Dxc2PKRqfR9A@mail.gmail.com>
 <CAJmOi+OEVLufPHwe1y8=1jb_Btmh=0m2TkvTZ6wkkwKGcbGsSw@mail.gmail.com>
 <CALK03d1YchVyVqw_zWZ3N8fHUZ71nr7oeYvhjA=_4XLKGHDqCw@mail.gmail.com>
 <CAD4oTHEnKOa8GHikHzfzN+Vn1D5h+UJ0_EDftF=7qfAhcw4vsA@mail.gmail.com>
 <CALK03d0+iwDJ1utWf5P0WAzDq9MEo3Rihm_be-QVtyXOBm07fw@mail.gmail.com>
Message-ID: <CAD4oTHF8gvdKmb=hQ825110AFU+u9_j=Lm+sHEM=s+FQgfo7Hg@mail.gmail.com>

Hi Toby,

Right, my point is that is.na being equivalent to "is an incomplete case"
is really only true for atomic vectors. I don't see it being the case for
lists, given what is.na does for lists. This is all just  my opinion, but
that's my take: vec[!is.na(vec)] happens to be the same as na.omit(vec) for
atomics, but in general the operations are not equivalent and I wouldn't
expect them to be.

Best,
~G

On Mon, Aug 16, 2021 at 10:54 AM Toby Hocking <tdhock5 at gmail.com> wrote:

> To clarify, ?is.na docs say that 'na.omit' returns the object with
> incomplete cases removed.
> If we take is.na to be the definition of "incomplete cases" then a list
> element with scalar NA is incomplete.
> About the data.frame method, in my opinion it is highly
> confusing/inconsistent for na.omit to keep rows with incomplete cases in
> list columns, but not in columns which are atomic vectors,
>
> > (f.num <- data.frame(num=c(1,NA,2)))
>   num
> 1   1
> 2  NA
> 3   2
> > is.na(f.num)
>        num
> [1,] FALSE
> [2,]  TRUE
> [3,] FALSE
> > na.omit(f.num)
>   num
> 1   1
> 3   2
>
> > (f.list <- data.frame(list=I(list(1,NA,2))))
>   list
> 1    1
> 2   NA
> 3    2
> > is.na(f.list)
>       list
> [1,] FALSE
> [2,]  TRUE
> [3,] FALSE
> > na.omit(f.list)
>   list
> 1    1
> 2   NA
> 3    2
>
> On Sat, Aug 14, 2021 at 5:15 PM Gabriel Becker <gabembecker at gmail.com>
> wrote:
>
> > I understand what is.na does, the issue I have is that its task is not
> > equivalent to the conceptual task na.omit is doing, in my opinion, as
> > illustrated by what the data.frame method does.
> >
> > Thus what i was getting at above about it not being clear that lst[is.na
> (lst)]
> > being the correct thing for na.omit to do
> >
> > ~G
> >
> > ~G
> >
> > On Sat, Aug 14, 2021, 1:49 PM Toby Hocking <tdhock5 at gmail.com> wrote:
> >
> >> Some relevant information from ?is.na: the behavior for lists is
> >> documented,
> >>
> >>      For is.na, elementwise the result is false unless that element
> >>      is a length-one atomic vector and the single element of that
> >>      vector is regarded as NA or NaN (note that any is.na method
> >>      for the class of the element is ignored).
> >>
> >> Also there are other functions anyNA and is.na<- which are consistent
> >> with
> >> is.na. That is, anyNA only returns TRUE if the list has an element
> which
> >> is
> >> a scalar NA. And is.na<- sets list elements to logical NA to indicate
> >> missingness.
> >>
> >> On Fri, Aug 13, 2021 at 1:10 AM Hugh Parsonage <
> hugh.parsonage at gmail.com>
> >> wrote:
> >>
> >> > The data.frame method deliberately skips non-atomic columns before
> >> > invoking is.na(x) so I think it is fair to assume this behaviour is
> >> > intentional and assumed.
> >> >
> >> > Not so clear to me that there is a sensible answer for list columns.
> >> > (List columns seem to collide with the expectation that in each
> >> > variable every observation will be of the same type)
> >> >
> >> > Consider your list L as
> >> >
> >> > L <- list(NULL, NA, c(NA, NA))
> >> >
> >> > Seems like every observation could have a claim to be 'missing' here.
> >> > Concretely, if a data.frame had a list column representing the lat-lon
> >> > of an observation, we might only be able to represent missing values
> >> > like c(NA, NA).
> >> >
> >> > On Fri, 13 Aug 2021 at 17:27, I?aki Ucar <iucar at fedoraproject.org>
> >> wrote:
> >> > >
> >> > > On Thu, 12 Aug 2021 at 22:20, Gabriel Becker <gabembecker at gmail.com
> >
> >> > wrote:
> >> > > >
> >> > > > Hi Toby,
> >> > > >
> >> > > > This definitely appears intentional, the first  expression of
> >> > > > stats:::na.omit.default is
> >> > > >
> >> > > >    if (!is.atomic(object))
> >> > > >
> >> > > >         return(object)
> >> > >
> >> > > I don't follow your point. This only means that the *default* method
> >> > > is not intended for non-atomic cases, but it doesn't mean it
> shouldn't
> >> > > exist a method for lists.
> >> > >
> >> > > > So it is explicitly just returning the object in non-atomic cases,
> >> > which
> >> > > > includes lists. I was not involved in this decision (obviously)
> but
> >> my
> >> > > > guess is that it is due to the fact that what constitutes an
> >> > observation
> >> > > > "being complete" in unclear in the list case. What should
> >> > > >
> >> > > > na.omit(list(5, NA, c(NA, 5)))
> >> > > >
> >> > > > return? Just the first element, or the first and the last? It
> >> seems, at
> >> > > > least to me, unclear. A small change to the documentation to to
> add
> >> > "atomic
> >> > >
> >> > > > is.na(list(5, NA, c(NA, 5)))
> >> > > [1] FALSE  TRUE FALSE
> >> > >
> >> > > Following Toby's argument, it's clear to me: the first and the last.
> >> > >
> >> > > I?aki
> >> > >
> >> > > > (in the sense of is.atomic returning \code{TRUE})" in front of
> >> > "vectors"
> >> > > > or similar  where what types of objects are supported seems
> >> justified,
> >> > > > though, imho, as the current documentation is either ambiguous or
> >> > > > technically incorrect, depending on what we take "vector" to mean.
> >> > > >
> >> > > > Best,
> >> > > > ~G
> >> > > >
> >> > > > On Wed, Aug 11, 2021 at 10:16 PM Toby Hocking <tdhock5 at gmail.com>
> >> > wrote:
> >> > > >
> >> > > > > Also, the na.omit method for data.frame with list column seems
> to
> >> be
> >> > > > > inconsistent with is.na,
> >> > > > >
> >> > > > > > L <- list(NULL, NA, 0)
> >> > > > > > str(f <- data.frame(I(L)))
> >> > > > > 'data.frame': 3 obs. of  1 variable:
> >> > > > >  $ L:List of 3
> >> > > > >   ..$ : NULL
> >> > > > >   ..$ : logi NA
> >> > > > >   ..$ : num 0
> >> > > > >   ..- attr(*, "class")= chr "AsIs"
> >> > > > > > is.na(f)
> >> > > > >          L
> >> > > > > [1,] FALSE
> >> > > > > [2,]  TRUE
> >> > > > > [3,] FALSE
> >> > > > > > na.omit(f)
> >> > > > >    L
> >> > > > > 1
> >> > > > > 2 NA
> >> > > > > 3  0
> >> > > > >
> >> > > > > On Wed, Aug 11, 2021 at 9:58 PM Toby Hocking <tdhock5 at gmail.com
> >
> >> > wrote:
> >> > > > >
> >> > > > > > na.omit is documented as "na.omit returns the object with
> >> > incomplete
> >> > > > > cases
> >> > > > > > removed." and "At present these will handle vectors," so I
> >> > expected that
> >> > > > > > when it is used on a list, it should return the same thing as
> >> if we
> >> > > > > subset
> >> > > > > > via is.na; however I observed the following,
> >> > > > > >
> >> > > > > > > L <- list(NULL, NA, 0)
> >> > > > > > > str(L[!is.na(L)])
> >> > > > > > List of 2
> >> > > > > >  $ : NULL
> >> > > > > >  $ : num 0
> >> > > > > > > str(na.omit(L))
> >> > > > > > List of 3
> >> > > > > >  $ : NULL
> >> > > > > >  $ : logi NA
> >> > > > > >  $ : num 0
> >> > > > > >
> >> > > > > > Should na.omit be fixed so that it returns a result that is
> >> > consistent
> >> > > > > > with is.na? I assume that is.na is the canonical definition
> of
> >> > what
> >> > > > > > should be considered a missing value in R.
> >> > > > > >
> >> > > > >
> >> > > > >         [[alternative HTML version deleted]]
> >> > > > >
> >> > > > > ______________________________________________
> >> > > > > R-devel at r-project.org mailing list
> >> > > > > https://stat.ethz.ch/mailman/listinfo/r-devel
> >> > > > >
> >> > > >
> >> > > >         [[alternative HTML version deleted]]
> >> > > >
> >> > > > ______________________________________________
> >> > > > R-devel at r-project.org mailing list
> >> > > > https://stat.ethz.ch/mailman/listinfo/r-devel
> >> > >
> >> > >
> >> > >
> >> > > --
> >> > > I?aki ?car
> >> > >
> >> > > ______________________________________________
> >> > > R-devel at r-project.org mailing list
> >> > > https://stat.ethz.ch/mailman/listinfo/r-devel
> >> >
> >> > ______________________________________________
> >> > R-devel at r-project.org mailing list
> >> > https://stat.ethz.ch/mailman/listinfo/r-devel
> >> >
> >>
> >>         [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-devel at r-project.org mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-devel
> >>
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From t@r@@@z@kh@rko @end|ng |rom uzh@ch  Wed Aug 18 15:00:19 2021
From: t@r@@@z@kh@rko @end|ng |rom uzh@ch (Taras Zakharko)
Date: Wed, 18 Aug 2021 15:00:19 +0200
Subject: [Rd] JIT compiler does not compile closures with custom environments
Message-ID: <894A40DE-C1AA-4586-9995-B9BEC173969F@uzh.ch>

I have encountered a behavior of R?s JIT compiler that I can?t quite figure out. Consider the following code:


   f_global <- function(x) {
     for(i in 1:10000) x <- x + 1
     x
   }

   f_env <- local({
    function(x) {
      for(i in 1:10000) x <- x + 1
      x
    }
   })

   compiler::enableJIT(3)

  bench::mark(f_global(0), f_env(0))
  # 1 f_global(0)    103?s 107.61?s     8770.    11.4KB      0    4384     0
  # 2 f_env(0)       1.1ms   1.42ms      712.        0B     66.3   290    27
  
Inspecting the closures shows that f_global has been byte-compiled while f_env has not been byte-compiled. Furthermore, if I assign a new environment to f_global (e.g. via environment(f_global) <- new.env()), it won?t be byte-compiled either. 

However, if I have a function returning a closure, that closure does get byte-compiled:

  f_closure <- (function() {
    function(x) {
      for(i in 1:10000) x <- x + 1
     x
   }
  })()

  bench::mark(f_closure(0))
  # 1 f_closure(0)    105?s    109?s     8625.        0B     2.01  4284     1      497ms

What is going on here? Both f_closure and f_env have non-global environments. Why is one JIT-compiled, but not the other? Is there a way to ensure that functions defined in environments will be JIT-compiled? 

Thanks, 

Taras

From murdoch@dunc@n @end|ng |rom gm@||@com  Wed Aug 18 16:33:08 2021
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Wed, 18 Aug 2021 10:33:08 -0400
Subject: [Rd] JIT compiler does not compile closures with custom
 environments
In-Reply-To: <894A40DE-C1AA-4586-9995-B9BEC173969F@uzh.ch>
References: <894A40DE-C1AA-4586-9995-B9BEC173969F@uzh.ch>
Message-ID: <69f1c674-a09f-fe16-734f-d63dff4df287@gmail.com>

On 18/08/2021 9:00 a.m., Taras Zakharko wrote:
> I have encountered a behavior of R?s JIT compiler that I can?t quite figure out. Consider the following code:
> 
> 
>     f_global <- function(x) {
>       for(i in 1:10000) x <- x + 1
>       x
>     }
> 
>     f_env <- local({
>      function(x) {
>        for(i in 1:10000) x <- x + 1
>        x
>      }
>     })
> 
>     compiler::enableJIT(3)
> 
>    bench::mark(f_global(0), f_env(0))
>    # 1 f_global(0)    103?s 107.61?s     8770.    11.4KB      0    4384     0
>    # 2 f_env(0)       1.1ms   1.42ms      712.        0B     66.3   290    27
>    
> Inspecting the closures shows that f_global has been byte-compiled while f_env has not been byte-compiled. Furthermore, if I assign a new environment to f_global (e.g. via environment(f_global) <- new.env()), it won?t be byte-compiled either.
> 
> However, if I have a function returning a closure, that closure does get byte-compiled:
> 
>    f_closure <- (function() {
>      function(x) {
>        for(i in 1:10000) x <- x + 1
>       x
>     }
>    })()
> 
>    bench::mark(f_closure(0))
>    # 1 f_closure(0)    105?s    109?s     8625.        0B     2.01  4284     1      497ms
> 
> What is going on here? Both f_closure and f_env have non-global environments. Why is one JIT-compiled, but not the other? Is there a way to ensure that functions defined in environments will be JIT-compiled?

About what is going on in f_closure:  I think the anonymous factory

function() {
       function(x) {
         for(i in 1:10000) x <- x + 1
        x
      }
     }

got byte compiled before first use, and that compiled its result.  That 
seems to be what this code indicates:

   f_closure <- (function() {
   res <- function(x) {
   for(i in 1:10000) x <- x + 1
   x
   }; print(res); res
   })()
   #> function(x) {
   #> for(i in 1:10000) x <- x + 1
   #> x
   #> }
   #> <bytecode: 0x7fb43ec3aa70>
   #> <environment: 0x7fb441117ac0>

But even if that's true, it doesn't address the bigger question of why 
f_global and f_env are treated differently.

Duncan Murdoch


From murdoch@dunc@n @end|ng |rom gm@||@com  Wed Aug 18 16:36:48 2021
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Wed, 18 Aug 2021 10:36:48 -0400
Subject: [Rd] JIT compiler does not compile closures with custom
 environments
In-Reply-To: <69f1c674-a09f-fe16-734f-d63dff4df287@gmail.com>
References: <894A40DE-C1AA-4586-9995-B9BEC173969F@uzh.ch>
 <69f1c674-a09f-fe16-734f-d63dff4df287@gmail.com>
Message-ID: <4a7b7bbc-3ab5-ac4b-ead1-30a0787041c3@gmail.com>

Forgot to add:  you could define f_env like this to get it compiled:

f_env <- local({
   cmpfun(function(x) {
     for(i in 1:10000) x <- x + 1
     x
   })
})

Not as convenient as JIT, but it gets the job done...

Duncan Murdoch

On 18/08/2021 10:33 a.m., Duncan Murdoch wrote:
> On 18/08/2021 9:00 a.m., Taras Zakharko wrote:
>> I have encountered a behavior of R?s JIT compiler that I can?t quite figure out. Consider the following code:
>>
>>
>>      f_global <- function(x) {
>>        for(i in 1:10000) x <- x + 1
>>        x
>>      }
>>
>>      f_env <- local({
>>       function(x) {
>>         for(i in 1:10000) x <- x + 1
>>         x
>>       }
>>      })
>>
>>      compiler::enableJIT(3)
>>
>>     bench::mark(f_global(0), f_env(0))
>>     # 1 f_global(0)    103?s 107.61?s     8770.    11.4KB      0    4384     0
>>     # 2 f_env(0)       1.1ms   1.42ms      712.        0B     66.3   290    27
>>     
>> Inspecting the closures shows that f_global has been byte-compiled while f_env has not been byte-compiled. Furthermore, if I assign a new environment to f_global (e.g. via environment(f_global) <- new.env()), it won?t be byte-compiled either.
>>
>> However, if I have a function returning a closure, that closure does get byte-compiled:
>>
>>     f_closure <- (function() {
>>       function(x) {
>>         for(i in 1:10000) x <- x + 1
>>        x
>>      }
>>     })()
>>
>>     bench::mark(f_closure(0))
>>     # 1 f_closure(0)    105?s    109?s     8625.        0B     2.01  4284     1      497ms
>>
>> What is going on here? Both f_closure and f_env have non-global environments. Why is one JIT-compiled, but not the other? Is there a way to ensure that functions defined in environments will be JIT-compiled?
> 
> About what is going on in f_closure:  I think the anonymous factory
> 
> function() {
>         function(x) {
>           for(i in 1:10000) x <- x + 1
>          x
>        }
>       }
> 
> got byte compiled before first use, and that compiled its result.  That
> seems to be what this code indicates:
> 
>     f_closure <- (function() {
>     res <- function(x) {
>     for(i in 1:10000) x <- x + 1
>     x
>     }; print(res); res
>     })()
>     #> function(x) {
>     #> for(i in 1:10000) x <- x + 1
>     #> x
>     #> }
>     #> <bytecode: 0x7fb43ec3aa70>
>     #> <environment: 0x7fb441117ac0>
> 
> But even if that's true, it doesn't address the bigger question of why
> f_global and f_env are treated differently.
> 
> Duncan Murdoch
>


From t@r@@@z@kh@rko @end|ng |rom uzh@ch  Wed Aug 18 16:40:42 2021
From: t@r@@@z@kh@rko @end|ng |rom uzh@ch (Taras Zakharko)
Date: Wed, 18 Aug 2021 16:40:42 +0200
Subject: [Rd] JIT compiler does not compile closures with custom
 environments
In-Reply-To: <4a7b7bbc-3ab5-ac4b-ead1-30a0787041c3@gmail.com>
References: <894A40DE-C1AA-4586-9995-B9BEC173969F@uzh.ch>
 <69f1c674-a09f-fe16-734f-d63dff4df287@gmail.com>
 <4a7b7bbc-3ab5-ac4b-ead1-30a0787041c3@gmail.com>
Message-ID: <EFB97294-F7BD-410D-B68B-382735B9F1C4@uzh.ch>

That?s the interim solution I am using right now (manually precompiling most functions), but I generate quite a lot of closures dynamically in my code and the potential performance loss is a bit worrying? I don?t think the current behavior is expected based on what the documentation says.

Cheers, 

Taras



> On 18 Aug 2021, at 16:36, Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
> 
> Forgot to add:  you could define f_env like this to get it compiled:
> 
> f_env <- local({
> cmpfun(function(x) {
>   for(i in 1:10000) x <- x + 1
>   x
> })
> })
> 
> Not as convenient as JIT, but it gets the job done...
> 
> Duncan Murdoch
> 
> On 18/08/2021 10:33 a.m., Duncan Murdoch wrote:
>> On 18/08/2021 9:00 a.m., Taras Zakharko wrote:
>>> I have encountered a behavior of R?s JIT compiler that I can?t quite figure out. Consider the following code:
>>> 
>>> 
>>>    f_global <- function(x) {
>>>      for(i in 1:10000) x <- x + 1
>>>      x
>>>    }
>>> 
>>>    f_env <- local({
>>>     function(x) {
>>>       for(i in 1:10000) x <- x + 1
>>>       x
>>>     }
>>>    })
>>> 
>>>    compiler::enableJIT(3)
>>> 
>>>   bench::mark(f_global(0), f_env(0))
>>>   # 1 f_global(0)    103?s 107.61?s     8770.    11.4KB      0    4384     0
>>>   # 2 f_env(0)       1.1ms   1.42ms      712.        0B     66.3   290    27
>>>   Inspecting the closures shows that f_global has been byte-compiled while f_env has not been byte-compiled. Furthermore, if I assign a new environment to f_global (e.g. via environment(f_global) <- new.env()), it won?t be byte-compiled either.
>>> 
>>> However, if I have a function returning a closure, that closure does get byte-compiled:
>>> 
>>>   f_closure <- (function() {
>>>     function(x) {
>>>       for(i in 1:10000) x <- x + 1
>>>      x
>>>    }
>>>   })()
>>> 
>>>   bench::mark(f_closure(0))
>>>   # 1 f_closure(0)    105?s    109?s     8625.        0B     2.01  4284     1      497ms
>>> 
>>> What is going on here? Both f_closure and f_env have non-global environments. Why is one JIT-compiled, but not the other? Is there a way to ensure that functions defined in environments will be JIT-compiled?
>> About what is going on in f_closure:  I think the anonymous factory
>> function() {
>>       function(x) {
>>         for(i in 1:10000) x <- x + 1
>>        x
>>      }
>>     }
>> got byte compiled before first use, and that compiled its result.  That
>> seems to be what this code indicates:
>>   f_closure <- (function() {
>>   res <- function(x) {
>>   for(i in 1:10000) x <- x + 1
>>   x
>>   }; print(res); res
>>   })()
>>   #> function(x) {
>>   #> for(i in 1:10000) x <- x + 1
>>   #> x
>>   #> }
>>   #> <bytecode: 0x7fb43ec3aa70>
>>   #> <environment: 0x7fb441117ac0>
>> But even if that's true, it doesn't address the bigger question of why
>> f_global and f_env are treated differently.
>> Duncan Murdoch
> 


From iuke-tier@ey m@iii@g oii uiow@@edu  Wed Aug 18 16:45:56 2021
From: iuke-tier@ey m@iii@g oii uiow@@edu (iuke-tier@ey m@iii@g oii uiow@@edu)
Date: Wed, 18 Aug 2021 09:45:56 -0500 (CDT)
Subject: [Rd] [External] Re: JIT compiler does not compile closures with
 custom environments
In-Reply-To: <69f1c674-a09f-fe16-734f-d63dff4df287@gmail.com>
References: <894A40DE-C1AA-4586-9995-B9BEC173969F@uzh.ch>
 <69f1c674-a09f-fe16-734f-d63dff4df287@gmail.com>
Message-ID: <alpine.DEB.2.22.394.2108180936030.2943@luke-Latitude-7480>

On Wed, 18 Aug 2021, Duncan Murdoch wrote:

> On 18/08/2021 9:00 a.m., Taras Zakharko wrote:
>> I have encountered a behavior of R?s JIT compiler that I can?t quite figure 
>> out. Consider the following code:
>> 
>>
>>     f_global <- function(x) {
>>       for(i in 1:10000) x <- x + 1
>>       x
>>     }
>>
>>     f_env <- local({
>>      function(x) {
>>        for(i in 1:10000) x <- x + 1
>>        x
>>      }
>>     })
>>
>>     compiler::enableJIT(3)
>>
>>    bench::mark(f_global(0), f_env(0))
>>    # 1 f_global(0)    103?s 107.61?s     8770.    11.4KB      0    4384 
>> 0
>>    # 2 f_env(0)       1.1ms   1.42ms      712.        0B     66.3   290 
>> 27
>>    Inspecting the closures shows that f_global has been byte-compiled while 
>> f_env has not been byte-compiled. Furthermore, if I assign a new 
>> environment to f_global (e.g. via environment(f_global) <- new.env()), it 
>> won?t be byte-compiled either.
>> 
>> However, if I have a function returning a closure, that closure does get 
>> byte-compiled:
>>
>>    f_closure <- (function() {
>>      function(x) {
>>        for(i in 1:10000) x <- x + 1
>>       x
>>     }
>>    })()
>>
>>    bench::mark(f_closure(0))
>>    # 1 f_closure(0)    105?s    109?s     8625.        0B     2.01  4284 
>> 1      497ms
>> 
>> What is going on here? Both f_closure and f_env have non-global 
>> environments. Why is one JIT-compiled, but not the other? Is there a way to 
>> ensure that functions defined in environments will be JIT-compiled?
>
> About what is going on in f_closure:  I think the anonymous factory
>
> function() {
>      function(x) {
>        for(i in 1:10000) x <- x + 1
>       x
>     }
>    }
>
> got byte compiled before first use, and that compiled its result.  That seems 
> to be what this code indicates:
>
>  f_closure <- (function() {
>  res <- function(x) {
>  for(i in 1:10000) x <- x + 1
>  x
>  }; print(res); res
>  })()
>  #> function(x) {
>  #> for(i in 1:10000) x <- x + 1
>  #> x
>  #> }
>  #> <bytecode: 0x7fb43ec3aa70>
>  #> <environment: 0x7fb441117ac0>

That is right.

> But even if that's true, it doesn't address the bigger question of why 
> f_global and f_env are treated differently.

There are various heuristics in the JIT code to avoid spending too
much time in the JIT. The current details are in the source
code. Mostly this is to deal with usually ill-advised coding practices
that programmatically build many small functions.  Hopefully these
heuristics can be reduced or eliminated over time.

For now, putting the code in a package, where the default is to byte
compile on source install, or explicitly calling compiler::cmpfun are
options.

Best,

luke

>
> Duncan Murdoch
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

-- 
Luke Tierney
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu

From murdoch@dunc@n @end|ng |rom gm@||@com  Wed Aug 18 16:56:29 2021
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Wed, 18 Aug 2021 10:56:29 -0400
Subject: [Rd] [External] Re: JIT compiler does not compile closures with
 custom environments
In-Reply-To: <alpine.DEB.2.22.394.2108180936030.2943@luke-Latitude-7480>
References: <894A40DE-C1AA-4586-9995-B9BEC173969F@uzh.ch>
 <69f1c674-a09f-fe16-734f-d63dff4df287@gmail.com>
 <alpine.DEB.2.22.394.2108180936030.2943@luke-Latitude-7480>
Message-ID: <0b8ae0c8-5fd2-9194-c7c7-136556bf7082@gmail.com>

On 18/08/2021 10:45 a.m., luke-tierney at uiowa.edu wrote:
> On Wed, 18 Aug 2021, Duncan Murdoch wrote:
> 
>> On 18/08/2021 9:00 a.m., Taras Zakharko wrote:
>>> I have encountered a behavior of R?s JIT compiler that I can?t quite figure
>>> out. Consider the following code:
>>>
>>>
>>>      f_global <- function(x) {
>>>        for(i in 1:10000) x <- x + 1
>>>        x
>>>      }
>>>
>>>      f_env <- local({
>>>       function(x) {
>>>         for(i in 1:10000) x <- x + 1
>>>         x
>>>       }
>>>      })
>>>
>>>      compiler::enableJIT(3)
>>>
>>>     bench::mark(f_global(0), f_env(0))
>>>     # 1 f_global(0)    103?s 107.61?s     8770.    11.4KB      0    4384
>>> 0
>>>     # 2 f_env(0)       1.1ms   1.42ms      712.        0B     66.3   290
>>> 27
>>>     Inspecting the closures shows that f_global has been byte-compiled while
>>> f_env has not been byte-compiled. Furthermore, if I assign a new
>>> environment to f_global (e.g. via environment(f_global) <- new.env()), it
>>> won?t be byte-compiled either.
>>>
>>> However, if I have a function returning a closure, that closure does get
>>> byte-compiled:
>>>
>>>     f_closure <- (function() {
>>>       function(x) {
>>>         for(i in 1:10000) x <- x + 1
>>>        x
>>>      }
>>>     })()
>>>
>>>     bench::mark(f_closure(0))
>>>     # 1 f_closure(0)    105?s    109?s     8625.        0B     2.01  4284
>>> 1      497ms
>>>
>>> What is going on here? Both f_closure and f_env have non-global
>>> environments. Why is one JIT-compiled, but not the other? Is there a way to
>>> ensure that functions defined in environments will be JIT-compiled?
>>
>> About what is going on in f_closure:  I think the anonymous factory
>>
>> function() {
>>       function(x) {
>>         for(i in 1:10000) x <- x + 1
>>        x
>>      }
>>     }
>>
>> got byte compiled before first use, and that compiled its result.  That seems
>> to be what this code indicates:
>>
>>   f_closure <- (function() {
>>   res <- function(x) {
>>   for(i in 1:10000) x <- x + 1
>>   x
>>   }; print(res); res
>>   })()
>>   #> function(x) {
>>   #> for(i in 1:10000) x <- x + 1
>>   #> x
>>   #> }
>>   #> <bytecode: 0x7fb43ec3aa70>
>>   #> <environment: 0x7fb441117ac0>
> 
> That is right.
> 
>> But even if that's true, it doesn't address the bigger question of why
>> f_global and f_env are treated differently.
> 
> There are various heuristics in the JIT code to avoid spending too
> much time in the JIT. The current details are in the source
> code. Mostly this is to deal with usually ill-advised coding practices
> that programmatically build many small functions.  Hopefully these
> heuristics can be reduced or eliminated over time.
> 
> For now, putting the code in a package, where the default is to byte
> compile on source install, or explicitly calling compiler::cmpfun are
> options.
> 

Thanks!  Putting code in a package seems easiest, and is a good idea for 
lots of other reasons.

Duncan Murdoch


From t@r@@@z@kh@rko @end|ng |rom uzh@ch  Wed Aug 18 19:16:33 2021
From: t@r@@@z@kh@rko @end|ng |rom uzh@ch (Taras Zakharko)
Date: Wed, 18 Aug 2021 19:16:33 +0200
Subject: [Rd] 
 [External] JIT compiler does not compile closures with custom
 environments
In-Reply-To: <alpine.DEB.2.22.394.2108180936030.2943@luke-Latitude-7480>
References: <894A40DE-C1AA-4586-9995-B9BEC173969F@uzh.ch>
 <69f1c674-a09f-fe16-734f-d63dff4df287@gmail.com>
 <alpine.DEB.2.22.394.2108180936030.2943@luke-Latitude-7480>
Message-ID: <BACE293E-9616-42DD-A268-FAEA2ED29FDC@uzh.ch>

Thanks for the explanation, Luke! I?ll keep this behavior in mind. 


> On 18 Aug 2021, at 16:45, luke-tierney at uiowa.edu wrote:
> 
> On Wed, 18 Aug 2021, Duncan Murdoch wrote:
> 
>> On 18/08/2021 9:00 a.m., Taras Zakharko wrote:
>>> I have encountered a behavior of R?s JIT compiler that I can?t quite figure out. Consider the following code:
>>> 
>>>    f_global <- function(x) {
>>>      for(i in 1:10000) x <- x + 1
>>>      x
>>>    }
>>> 
>>>    f_env <- local({
>>>     function(x) {
>>>       for(i in 1:10000) x <- x + 1
>>>       x
>>>     }
>>>    })
>>> 
>>>    compiler::enableJIT(3)
>>> 
>>>   bench::mark(f_global(0), f_env(0))
>>>   # 1 f_global(0)    103?s 107.61?s     8770.    11.4KB      0    4384 0
>>>   # 2 f_env(0)       1.1ms   1.42ms      712.        0B     66.3   290 27
>>>   Inspecting the closures shows that f_global has been byte-compiled while f_env has not been byte-compiled. Furthermore, if I assign a new environment to f_global (e.g. via environment(f_global) <- new.env()), it won?t be byte-compiled either.
>>> However, if I have a function returning a closure, that closure does get byte-compiled:
>>> 
>>>   f_closure <- (function() {
>>>     function(x) {
>>>       for(i in 1:10000) x <- x + 1
>>>      x
>>>    }
>>>   })()
>>> 
>>>   bench::mark(f_closure(0))
>>>   # 1 f_closure(0)    105?s    109?s     8625.        0B     2.01  4284 1      497ms
>>> What is going on here? Both f_closure and f_env have non-global environments. Why is one JIT-compiled, but not the other? Is there a way to ensure that functions defined in environments will be JIT-compiled?
>> 
>> About what is going on in f_closure:  I think the anonymous factory
>> 
>> function() {
>>     function(x) {
>>       for(i in 1:10000) x <- x + 1
>>      x
>>    }
>>   }
>> 
>> got byte compiled before first use, and that compiled its result.  That seems to be what this code indicates:
>> 
>> f_closure <- (function() {
>> res <- function(x) {
>> for(i in 1:10000) x <- x + 1
>> x
>> }; print(res); res
>> })()
>> #> function(x) {
>> #> for(i in 1:10000) x <- x + 1
>> #> x
>> #> }
>> #> <bytecode: 0x7fb43ec3aa70>
>> #> <environment: 0x7fb441117ac0>
> 
> That is right.
> 
>> But even if that's true, it doesn't address the bigger question of why f_global and f_env are treated differently.
> 
> There are various heuristics in the JIT code to avoid spending too
> much time in the JIT. The current details are in the source
> code. Mostly this is to deal with usually ill-advised coding practices
> that programmatically build many small functions.  Hopefully these
> heuristics can be reduced or eliminated over time.
> 
> For now, putting the code in a package, where the default is to byte
> compile on source install, or explicitly calling compiler::cmpfun are
> options.
> 
> Best,
> 
> luke
> 
>> 
>> Duncan Murdoch
>> 
>> ______________________________________________
>> R-devel at r-project.org <mailto:R-devel at r-project.org> mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel <https://stat.ethz.ch/mailman/listinfo/r-devel>
>> 
> 
> -- 
> Luke Tierney
> Ralph E. Wareham Professor of Mathematical Sciences
> University of Iowa                  Phone:             319-335-3386
> Department of Statistics and        Fax:               319-335-3017
>   Actuarial Science
> 241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu <mailto:luke-tierney at uiowa.edu>
> Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu <http://www.stat.uiowa.edu/>

	[[alternative HTML version deleted]]


From jeroen @end|ng |rom berke|ey@edu  Fri Aug 20 12:19:04 2021
From: jeroen @end|ng |rom berke|ey@edu (Jeroen Ooms)
Date: Fri, 20 Aug 2021 12:19:04 +0200
Subject: [Rd] Update on rtools4 and ucrt support
Message-ID: <CABFfbXsA_aRe3kznH3eYMDoOJAzzGb-NHMo4k3tQH4xJHL9sOw@mail.gmail.com>

Hi all,

I received some questions this week about rtools4 (the windows
compiler bundle) in particular regarding support for ucrt, so below a
brief summary of the status quo:

As of May 2021, rtools4 has full support for ucrt. The toolchain
configuration is based on the upstream msys2 configuration, which are
very stable, and widely used by other open source projects as well as
the mingw-w64 developers themselves. The latest builds of rtools4 now
contain 3 toolchains:

 - c:\rtools40\mingw32: the 32-bit gcc-8-3.0 toolchain used as of R 4.0.0
 - c:\rtools40\mingw64: the 64-bit gcc-8-3.0 toolchain used as of R 4.0.0
 - c:\rtools40\ucrt64: a new 64-bit gcc-10.3.0 toolchain targeting ucrt

The total install size is about 1gb. Hence, if R were to switch to
ucrt at some point, users and sysadmins that have installed rtools4
after May 2021 are already equipped with proper toolchains for
building packages for both R 4.0+ as well as a potential ucrt versions
of R.

As before, for each of these toolchains, all extra libraries needed by
CRAN packages can easily be installed in rtools4 through pacman [1].
All system libraries in rtools-packages [2] have ucrt64 binaries [3].
When users contribute an update or a new rtools package, the CI
automatically builds and checks binaries for each of the above
toolchains, e.g [4]. The process is 100% automatic, transparent, and
reproducible. This provides a degree of accountability, and makes it
easy for R package authors to suggest improvements for the C/C++
libraries that they depend on (many have done so in the past 2 years).

Rtools4 is preinstalled on major CI/cloud services such as GitHub
actions. Popular open-source projects such as Apache-Arrow and TileDB
are already using the rtools4 toolchains to automatically build and
test their C++ libraries, as well as R bindings, for each commit, on
all target architectures (including ucrt64). Any R package author can
use the the same free services to check their packages on all compile
targets using rtools4 toolchains [5]. The r-devel CI tool on
https://r-devel.github.io checks every commit to base-R using ucrt64
toolchain from rtools4, which has proven to be very stable.

I am also aware that Tomas Kalibera also provides alternative
"experimental ucrt toolchain": a 6gb tarball with manually built
things on his personal machine. It is unclear to me why it was decided
to take this approach; it is certainly not needed to support ucrt
(ucrt is literally one flag in the toolchain configuration).
Fortunately, the ucrt tooclchains from rtools4 and Tomas Kalibera use
the same version of mingw-w64 and gcc, and are fully compatible, so
package authors could still use the rtools4 ucrt compilers icw the
R-devel-ucrt version that was built using this experimental toolchain
[5].

We spent an enormous amount of effort in the past years standardising
the Windows build tooling, and making the infrastructure automated,
open and accessible, such that everyone can learn how this works and
get involved. Many people have. Today if you install R and Rtools4 on
Windows, things "just work", regardless of whether this is a student
laptop, university server, or online CI system. Anyone can build R
packages, base-R, or any of the system libraries, following the steps,
and using standard tooling that other open source projects use. I
think it would be a big step back of R-core decides to go back to a
black-box system that is so opaque and complex that only one person
knows how it works, and would make it much more difficult for
students, universities, and other organisations to build R packages
and libraries on Windows.

Jeroen


[1] https://github.com/r-windows/docs/blob/master/rtools40.md#readme
[2] https://github.com/r-windows/rtools-packages
[3] https://cran.r-project.org/bin/windows/Rtools/4.0/
[4] https://github.com/r-windows/rtools-packages/pull/221
[5] https://github.com/r-windows/docs/blob/master/ucrt.md


From pro|jcn@@h @end|ng |rom gm@||@com  Fri Aug 20 17:06:25 2021
From: pro|jcn@@h @end|ng |rom gm@||@com (J C Nash)
Date: Fri, 20 Aug 2021 11:06:25 -0400
Subject: [Rd] Seeking opinions on possible change to nls() code
Message-ID: <64a2e494-ed5e-55c4-052e-d7537caa2f8b@gmail.com>

In our work on a Google Summer of Code project "Improvements to nls()",
the code has proved sufficiently entangled that we have found (so far!)
few straightforward changes that would not break legacy behaviour. One
issue that might be fixable is that nls() returns no result if it
encounters some computational blockage AFTER it has already found a
much better "fit" i.e. set of parameters with smaller sum of squares.
Here is a version of the Tetra example:

time=c( 1,  2,  3,  4,  6 , 8, 10, 12, 16)
conc = c( 0.7, 1.2, 1.4, 1.4, 1.1, 0.8, 0.6, 0.5, 0.3)
NLSdata <- data.frame(time,conc)
NLSstart <-c(lrc1=-2,lrc2=0.25,A1=150,A2=50) # a starting vector (named!)
NLSformula <-conc ~ A1*exp(-exp(lrc1)*time)+A2*exp(-exp(lrc2)*time)
tryit <- try(nls(NLSformula, data=NLSdata, start=NLSstart, trace=TRUE))
print(tryit)

If you run this, tryit does not give information that the sum of squares
has been reduced from > 60000 to < 2, as the trace shows.

Should we propose that this be changed so the returned object gives the
best fit so far, albeit with some form of message or return code to indicate
that this is not necessarily a conventional solution? Our concern is that
some examples might need to be adjusted slightly, or we might simply add
the "try-error" class to the output information in such cases.

Comments are welcome, as this is as much an infrastructure matter as a
computational one.

Best,

John Nash


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Fri Aug 20 17:35:18 2021
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Fri, 20 Aug 2021 17:35:18 +0200
Subject: [Rd] Seeking opinions on possible change to nls() code
In-Reply-To: <64a2e494-ed5e-55c4-052e-d7537caa2f8b@gmail.com>
References: <64a2e494-ed5e-55c4-052e-d7537caa2f8b@gmail.com>
Message-ID: <24863.52150.50077.706381@stat.math.ethz.ch>

>>>>> J C Nash 
>>>>>     on Fri, 20 Aug 2021 11:06:25 -0400 writes:

    > In our work on a Google Summer of Code project
    > "Improvements to nls()", the code has proved sufficiently
    > entangled that we have found (so far!)  few
    > straightforward changes that would not break legacy
    > behaviour. One issue that might be fixable is that nls()
    > returns no result if it encounters some computational
    > blockage AFTER it has already found a much better "fit"
    > i.e. set of parameters with smaller sum of squares.  Here
    > is a version of the Tetra example:

    time=c( 1,  2,  3,  4,  6 , 8, 10, 12, 16)
    conc = c( 0.7, 1.2, 1.4, 1.4, 1.1, 0.8, 0.6, 0.5, 0.3)
    NLSdata <- data.frame(time,conc)
    NLSstart <-c(lrc1=-2,lrc2=0.25,A1=150,A2=50) # a starting vector (named!)
    NLSformula <-conc ~ A1*exp(-exp(lrc1)*time)+A2*exp(-exp(lrc2)*time)
    tryit <- try(nls(NLSformula, data=NLSdata, start=NLSstart, trace=TRUE))
    print(tryit)

    > If you run this, tryit does not give information that the
    > sum of squares has been reduced from > 60000 to < 2, as
    > the trace shows.

    > Should we propose that this be changed so the returned
    > object gives the best fit so far, albeit with some form of
    > message or return code to indicate that this is not
    > necessarily a conventional solution? Our concern is that
    > some examples might need to be adjusted slightly, or we
    > might simply add the "try-error" class to the output
    > information in such cases.

    > Comments are welcome, as this is as much an infrastructure
    > matter as a computational one.

Hmm...  many years ago, we had introduced the  'warnOnly=TRUE'
option to nls()  i.e., nls.control()  exactly for such cases,
where people would still like to see the solution:

So,

------------------------------------------------------------------------------
> try2 <- nls(NLSformula, data=NLSdata, start=NLSstart, trace=TRUE, 
              control = nls.control(warnOnly=TRUE))
61215.76    (3.56e+03): par = (-2 0.25 150 50)
2.175672    (2.23e+01): par = (-1.9991 0.3171134 2.618224 -1.366768)
1.621050    (7.14e+00): par = (-1.960475 -2.620293 2.575261 -0.5559918)
Warning message:
In nls(NLSformula, data = NLSdata, start = NLSstart, trace = TRUE,  :
  singular gradient

> try2
Nonlinear regression model
  model: conc ~ A1 * exp(-exp(lrc1) * time) + A2 * exp(-exp(lrc2) * time)
   data: NLSdata
   lrc1    lrc2      A1      A2 
 -22.89   96.43  156.70 -156.68 
 residual sum-of-squares: 218483

Number of iterations till stop: 2 
Achieved convergence tolerance: 7.138
Reason stopped: singular gradient

> coef(try2)
      lrc1       lrc2         A1         A2 
 -22.88540   96.42686  156.69547 -156.68461 


> summary(try2)
Error in chol2inv(object$m$Rmat()) : 
  element (3, 3) is zero, so the inverse cannot be computed
>
------------------------------------------------------------------------------

and similar for  vcov(), of course, where the above error
originates.

{ I think  GSoC (andr other)  students should start by studying and
  exploring relevant help pages before drawing conclusions
  ......
  but yes, I've been born in the last millennium ...
}

;-)

Have a nice weekend!
Martin


From pro|jcn@@h @end|ng |rom gm@||@com  Fri Aug 20 17:41:26 2021
From: pro|jcn@@h @end|ng |rom gm@||@com (J C Nash)
Date: Fri, 20 Aug 2021 11:41:26 -0400
Subject: [Rd] Seeking opinions on possible change to nls() code
In-Reply-To: <24863.52150.50077.706381@stat.math.ethz.ch>
References: <64a2e494-ed5e-55c4-052e-d7537caa2f8b@gmail.com>
 <24863.52150.50077.706381@stat.math.ethz.ch>
Message-ID: <7d11862c-092d-00ba-572a-19cdd9d2d858@gmail.com>

Thanks Martin. I'd missed the intention of that option, but re-reading
it now it is obvious.

FWIW, this problem is quite nasty, and so far I've found no method
that reveals the underlying dangers well. And one of the issues with
nonlinear models is that they reveal how slippery the concept of
inference can be when applied to parameters in such models.

JN


On 2021-08-20 11:35 a.m., Martin Maechler wrote:
>>>>>> J C Nash 
>>>>>>     on Fri, 20 Aug 2021 11:06:25 -0400 writes:
> 
>     > In our work on a Google Summer of Code project
>     > "Improvements to nls()", the code has proved sufficiently
>     > entangled that we have found (so far!)  few
>     > straightforward changes that would not break legacy
>     > behaviour. One issue that might be fixable is that nls()
>     > returns no result if it encounters some computational
>     > blockage AFTER it has already found a much better "fit"
>     > i.e. set of parameters with smaller sum of squares.  Here
>     > is a version of the Tetra example:
> 
>     time=c( 1,  2,  3,  4,  6 , 8, 10, 12, 16)
>     conc = c( 0.7, 1.2, 1.4, 1.4, 1.1, 0.8, 0.6, 0.5, 0.3)
>     NLSdata <- data.frame(time,conc)
>     NLSstart <-c(lrc1=-2,lrc2=0.25,A1=150,A2=50) # a starting vector (named!)
>     NLSformula <-conc ~ A1*exp(-exp(lrc1)*time)+A2*exp(-exp(lrc2)*time)
>     tryit <- try(nls(NLSformula, data=NLSdata, start=NLSstart, trace=TRUE))
>     print(tryit)
> 
>     > If you run this, tryit does not give information that the
>     > sum of squares has been reduced from > 60000 to < 2, as
>     > the trace shows.
> 
>     > Should we propose that this be changed so the returned
>     > object gives the best fit so far, albeit with some form of
>     > message or return code to indicate that this is not
>     > necessarily a conventional solution? Our concern is that
>     > some examples might need to be adjusted slightly, or we
>     > might simply add the "try-error" class to the output
>     > information in such cases.
> 
>     > Comments are welcome, as this is as much an infrastructure
>     > matter as a computational one.
> 
> Hmm...  many years ago, we had introduced the  'warnOnly=TRUE'
> option to nls()  i.e., nls.control()  exactly for such cases,
> where people would still like to see the solution:
> 
> So,
> 
> ------------------------------------------------------------------------------
>> try2 <- nls(NLSformula, data=NLSdata, start=NLSstart, trace=TRUE, 
>               control = nls.control(warnOnly=TRUE))
> 61215.76    (3.56e+03): par = (-2 0.25 150 50)
> 2.175672    (2.23e+01): par = (-1.9991 0.3171134 2.618224 -1.366768)
> 1.621050    (7.14e+00): par = (-1.960475 -2.620293 2.575261 -0.5559918)
> Warning message:
> In nls(NLSformula, data = NLSdata, start = NLSstart, trace = TRUE,  :
>   singular gradient
> 
>> try2
> Nonlinear regression model
>   model: conc ~ A1 * exp(-exp(lrc1) * time) + A2 * exp(-exp(lrc2) * time)
>    data: NLSdata
>    lrc1    lrc2      A1      A2 
>  -22.89   96.43  156.70 -156.68 
>  residual sum-of-squares: 218483
> 
> Number of iterations till stop: 2 
> Achieved convergence tolerance: 7.138
> Reason stopped: singular gradient
> 
>> coef(try2)
>       lrc1       lrc2         A1         A2 
>  -22.88540   96.42686  156.69547 -156.68461 
> 
> 
>> summary(try2)
> Error in chol2inv(object$m$Rmat()) : 
>   element (3, 3) is zero, so the inverse cannot be computed
>>
> ------------------------------------------------------------------------------
> 
> and similar for  vcov(), of course, where the above error
> originates.
> 
> { I think  GSoC (andr other)  students should start by studying and
>   exploring relevant help pages before drawing conclusions
>   ......
>   but yes, I've been born in the last millennium ...
> }
> 
> ;-)
> 
> Have a nice weekend!
> Martin
>


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Fri Aug 20 18:14:51 2021
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Fri, 20 Aug 2021 18:14:51 +0200
Subject: [Rd] Seeking opinions on possible change to nls() code
In-Reply-To: <7d11862c-092d-00ba-572a-19cdd9d2d858@gmail.com>
References: <64a2e494-ed5e-55c4-052e-d7537caa2f8b@gmail.com>
 <24863.52150.50077.706381@stat.math.ethz.ch>
 <7d11862c-092d-00ba-572a-19cdd9d2d858@gmail.com>
Message-ID: <24863.54523.250693.751120@stat.math.ethz.ch>

>>>>> J C Nash 
>>>>>     on Fri, 20 Aug 2021 11:41:26 -0400 writes:

    > Thanks Martin. I'd missed the intention of that option,
    > but re-reading it now it is obvious.

    > FWIW, this problem is quite nasty, and so far I've found
    > no method that reveals the underlying dangers well. And
    > one of the issues with nonlinear models is that they
    > reveal how slippery the concept of inference can be when
    > applied to parameters in such models.

    > JN

Indeed.

Just for the public (and those reading the archives in the future).

When Doug Bates and his phd student Jos? Pinheiro wrote
"the NLME book"  (<==> Recommended R package {nlme}
     	  	       https://cran.R-project.org/package=nlme )

Jos? C. Pinheiro and  Douglas M. Bates
Mixed-Effects Models in S and S-PLUS
Springer-Verlag (January 2000)
DOI: 10.1007/b98882 --> https://link.springer.com/book/10.1007%2Fb98882

They teach quite a bit about non-linear regression much of which
seems not much known nor taught nowadays.

NOTABLY they teach self-starting models, something phantastic,
available in R together with nls()  but unfortunately *also* not
much known nor taught!

I have improved the help pages, notably the examples for these,
in the distant past I vaguely remember.

Your present 9-point example can indeed also be solved beautiful
by R's builtin  SSbiexp()  [Self-starting bi-exponential model]:

NLSdata <- data.frame(
    time = c(  1,   2,   3,   4,   6 ,  8,  10,  12,  16),
    conc = c( 0.7, 1.2, 1.4, 1.4, 1.1, 0.8, 0.6, 0.5, 0.3))

## Once you realize that the above is the "simple"  bi-exponential model,
## you should remember  SSbiexp(),  and then

"everything is easy "

try4 <- nls(conc ~ SSbiexp(time, A1, lrc1, A2, lrc2), data = NLSdata,
            trace=TRUE, control=nls.control(warnOnly=TRUE))
## --> converges nicely and starts much better anyway:
## 0.1369091   (2.52e+00): par = (-0.7623289 -2.116174 -2.339856 2.602446)
## 0.01160784  (4.97e-01): par = (-0.1988961 -1.974059 -3.523139 2.565856)
## 0.01016776  (1.35e-01): par = (-0.3653394 -1.897649 -3.547569 2.862685)
## 0.01005199  (3.22e-02): par = (-0.3253514 -1.909544 -3.55429 2.798951)
## 0.01004574  (8.13e-03): par = (-0.336659 -1.904219 -3.559615 2.821439)
## 0.01004534  (2.08e-03): par = (-0.3338447 -1.905399 -3.558815 2.816159)
## 0.01004532  (5.30e-04): par = (-0.3345701 -1.905083 -3.559067 2.817548)
## 0.01004531  (1.36e-04): par = (-0.3343852 -1.905162 -3.559006 2.817195)
## 0.01004531  (3.46e-05): par = (-0.3344325 -1.905142 -3.559022 2.817286)
## 0.01004531  (8.82e-06): par = (-0.3344204 -1.905147 -3.559018 2.817263)
## 0.01004531  (7.90e-06): par = (-3.559018 -0.3344204 2.817263 -1.905147)

## even adding central differences and  'scaleOffset' .. but that's not making big diff.:
try5 <- nls(conc ~ SSbiexp(time, A1, lrc1, A2, lrc2), data = NLSdata,
            trace=TRUE, control=nls.control(warnOnly=TRUE, nDcentral=TRUE, scaleOffset = 1))
## 0.1369091     (1.43e-01): par = (-0.7623289 -2.116174 -2.339856 2.602446)
## ....
## 0.01004531    (5.43e-06): par = (-3.559006 -0.3343852 2.817195 -1.905162)
fitted(try5)
## [1] 0.6880142 1.2416734 1.3871354 1.3503718 1.1051246 0.8451185 0.6334280 0.4717800
## [9] 0.2604932

all.equal(  coef(try4),   coef(try5)) # "Mean relative difference: 1.502088e-05"
all.equal(fitted(try4), fitted(try5)) # "Mean relative difference: 2.983784e-06"

## and a nice plot:
plot(NLSdata, ylim = c(0, 1.5), pch=21, bg="red")
abline(h=0, lty=3, col="gray")
lines(NLSdata$time, fitted(try5), lty=2, lwd=1/2, col="orange")
tt <- seq(0, 17, by=1/8)
str(pp <- predict(try5, newdata = list(time = tt)))
 ## num [1:137] -0.7418 -0.4891 -0.2615 -0.0569 0.1269 ...
 ## - attr(*, "gradient")= num [1:137, 1:4] 1 0.914 0.836 0.765 0.699 ...
 ##  ..- attr(*, "dimnames")=List of 2
 ##  .. ..$ : NULL
 ##  .. ..$ : chr [1:4] "A1" "lrc1" "A2" "lrc2"
lines(tt, pp, col=4)


From therne@u @end|ng |rom m@yo@edu  Sat Aug 21 15:45:43 2021
From: therne@u @end|ng |rom m@yo@edu (Therneau, Terry M., Ph.D.)
Date: Sat, 21 Aug 2021 08:45:43 -0500
Subject: [Rd] problems with [.terms
Message-ID: <2e47ad$ggnu06@ironport10.mayo.edu>

The survival package uses [.terms a fair bit, in particular it makes use of the index 
returned in the 'specials' attribute, but the base R code has at least two problems.
First, it does not account for offset terms, if present, and also fails for a formula such 
as? y ~ age + (sex=='male').?? Yes, the user should have used I(sex=='male'), but they 
very often don't and the formula works fine without it, but not [.terms.???? Users of 
coxph regularly remind me of these flaws :-)

I first reported this in a bugzilla in 2016.?? I think that I (finally) have a working fix 
for all the issues.?? This is found below in an Sweave document (which is short enough 
that I've pasted it directly in) along with some further discussion.?? Three particular 
questions are 1: my solution for (sex=="male") is a bit of a hack, but it works and I 
don't see anything better that is simple, and 2: we should then add documentation that 
both [.terms and drop.terms also remove any offset() terms.? (This is currently true 
anyway.) and 3. Whether this will break other code the currently works around the flaws.

If this looks good I could try to create a formal patch, though that is not something I 
have done before.

Terry T.

------------------
\documentclass [11pt]{article}
\newcommand{\code}[1]{\texttt{#1}}

\title{Subscripting a terms object}
\author{Terry Therneau}
\begin{document}
\maketitle
<<echo=FALSE>>=
options(continue=' ')
@

\section{Introduction}
The survival package makes use of the the subscripting method for terms.
Consider the following code:
<<test1>>=
library(survival)
fit1 <- coxph(Surv(time, status) ~ age + strata(inst), data=lung)
Terms <- terms(fit1)
@

In a Cox model the strata term acts as a per-institution intercept, but it
is not a part of the $X$ matrix.
There are two strategies:
\begin{itemize}
   \item If there are strata by covariate interactions, the strata columns
     are removed from the $X$ matrix after the call to model.matrix.
     They (and the intercept) are necessary to correctly form interactions.
   \item If there are no strata by covariate interactions, the strata term is
     removed from the terms object before calling model.matrix.
     Some models may have thousands of strata, a nested case-control design for
     instance, so this is important for efficiency.
\end{itemize}

The approach is to add \code{specials='strata'} to our call to \code{terms},
which causes any strata call to be marked: the \code{specials} attribute
of the terms object will then contain 3 for the example above (the specials
index counts the response),
and \code{Terms[-2]} will create a version without the strata term.

\section{Errors in [.terms}
Our problem is that the subscripting operator for a terms object fails in 2
cases.
Consider the following increasingly complex formulas:

<<case1>>=
library(survival)
f1 <- model.frame(terms(Surv(time, status) ~ ph.ecog + strata(inst) + age,
                specials="strata"), data=lung)
f2 <- model.frame(terms(Surv(time, status) ~ offset(sex) + age +
                                strata(ph.ecog), specials="strata"), data= lung)
f3 <- model.frame(terms(~ pspline(wt.loss) + (ph.ecog==1) + strata(inst) +
                             (wt.loss + meal.cal)*sex, specials="strata"), lung)
test1 <- terms(f1)
test2 <- terms(f2)
test3 <- terms(f3)
@

The root problem is the need for multiple subscripts.  Consider \code{test1}.
\begin{itemize}
   \item The object itself is a formula of length 5, with `~' as the first
     element.
   \item Attributes are
     \begin{itemize}
       \item The variables and predvars attributes are call objects, each a list()
         with 4 elments: the response and all 3 predictors.
       \item The factors attribute has 4 rows and 2 columns, with row labels
         corresponding to the \code{variables} list.
       \item The specials, response, and offset (if present) attributes give
         the index of those terms in the variables attribute.
       \item The term.labels and order attributes omit the resonse and the offset,
         so have length 2.
       \item The dataClasses attribute is a character vector of length 4.
     \end{itemize}
\end{itemize}
So the ideal result of  term1[remove the specials] would use subscript of
\begin{itemize}
   \item \code{[-5]} on the formula itself, variables and predvars attributes
   \item \code{[-2]} for term.labels
   \item \code{[-4 , -2, drop=FALSE]} for the factor attribute
   \item \code{[-2]} for order attribute
   \item \code{[-4]} for the dataClasses attribute
\end{itemize}

That will recreate the formula that ``would have been'' had there been no
strata term.  Now look at the first portion of the code in models.R

<<>>=
`[.terms` <- function (termobj, i)
{
     resp <- if (attr(termobj, "response")) termobj[[2L]]
     newformula <- attr(termobj, "term.labels")[i]
     if (length(newformula) == 0L) newformula <- "1"
     newformula <- reformulate(newformula, resp, attr(termobj, "intercept"),
                               environment(termobj))
     result <- terms(newformula, specials = names(attr(termobj, "specials")))

     # Edit the optional attributes
}
@

The use of reformulate() is a nice trick, and correctly creates all the
attributes except predvars and dataClasses.  However, the index reported in
the specials attribute is generated with reference to the variables
attribute, or equivalently the row names of factors, not with respect to the
term.labels attribute; the latter lacks the response and any offset terms.
Thus our code works for test1 but fails for test2: specials points to the wrong
variable in term.labels.

The reformulate trick breaks in another way in test3 due to the
\code{(ph.ecog==1)} term.
In the term.labels attribute the parentheses disappear, and
the result of the reformulate call is not a proper formula.  The + binds
tighter than == leading to an error message that will confuse most users.
We can argue, and I probably would, that the user should have used
\code{I(ph.ecog==1)}.
But they didn't, and without the I() it is a legal
formula, or at least one that currently works.
Fixing this issue is a harder, since only the underlying formula retains the
necessary syntax.
Short of deparsing the formula, a hack that appears to work is to surround every
term with a set of parenthesis.
@

The impact of an offset term was overlooked in second portion of the
subscript routine as well, i.e., the ``edit optional attribute'' section which
attempts to amend the predvars and dataClasses attributes.

Here is an updated subscript routine which works for the examples above.
<<newsub>>=
mytermsub <- function (termobj, i)
{
     # In the terms object for
     #   model.frame(mpg ~ cyl + offset(-.04*disp) + wt*factor(carb), mtcars)
     # The subscript 'i' in the call counts variables on the right hand side,
     #   to drop "wt" use a subscript of -3.
     # Using reformulate() with term.labels is the primary strategy, since the
     #   latter includes all the interactions.  But the offset is missing from
     #   term.labels, so we have to be more clever with indexing.
     rvar <- if (attr(termobj, "response") ==1) termobj[[2L]]
     j <- seq.int(length(attr(termobj, "term.labels")) +
                  length(attr(termobj, "offset")))

     if (!is.null(attr(termobj, "offset"))) {
         # k = where offset() would have appeared in term.labels, before removals
         k <- attr(termobj, "response") - attr(termobj, "offset")
         index1 <- match(j, j[k], nomatch=0)[i]
     } else index1 <- j[i]
     newformula <- attr(termobj, "term.labels")[index1]
     # Adding () around each term is for a formula with  + (sex=='male') in the
     #  formula.
     newformula <- reformulate(paste0("(", newformula, ")"), response= rvar,
                               intercept = attr(termobj, "intercept"),
                               env = environment(termobj))
     if (length(newformula) == 0L) newformula <- "1"
     
     # addition of an extra specials label causes no harm
     result <- terms(newformula, specials = names(attr(termobj, "specials")))
     
     # now add back the predvars and dataClasses attributes; which do contain
     # the response and offset.
     index2 <- j[i]
     if (attr(termobj, "response")==1) index2 <- c(1, index2 +1)
     # if 'i' drops an interaction it won't change predvars or dataClasses
     index2 <- index2[index2 <= length(rownames(attr(termobj, "factors")))]
     if (!is.null(attr(termobj, "offset")))
         index2 <- index2[-attr(termobj, "offset")]
     if (!is.null(attr(termobj, "predvars")))
         attr(result, "predvars") <- attr(termobj, "predvars")[c(1, index2 +1)]
     if (!is.null(attr(termobj, "dataClasses")))
         attr(result, "dataClasses") <- attr(termobj, "dataClasses")[index2]

     result
}
@

Now test this out by dropping the strata and offset terms.
<<testit>>=
f1b <- model.frame(terms(Surv(time, status) ~ ph.ecog + age,
                specials="strata"), data=lung)
f2b <- model.frame(terms(Surv(time, status) ~ age,
                                specials="strata"), data= lung)
f3b <- model.frame(terms(~ pspline(wt.loss) + (ph.ecog==1) +
                             (wt.loss + meal.cal)*sex, specials="strata"), lung)
all.equal(attributes(terms(f1b)), attributes(mytermsub(test1, -2)))
all.equal(attributes(terms(f2b)), attributes(mytermsub(test2, -3)))
all.equal(attributes(terms(f3b)), attributes(mytermsub(test3, -3)))
@
The formula itself changes due to the extra parentheses, hence the check of
only the attributes.

The drop.terms function shares much of the same code.
<<drop>>=
drop.terms <- function(termobj, dropx = NULL, keep.response = FALSE)
{
     if (is.null(dropx)) return(termobj)
     if(!inherits(termobj, "terms"))
         stop(gettextf("'termobj' must be a object of class %s",
                       dQuote("terms")), domain = NA)
     rvar <- if (keep.response & attr(termobj, "response") ==1) termobj[[2L]]
     j <- seq.int(length(attr(termobj, "term.labels")) +
                  length(attr(termobj, "offset")))
         
     if (!is.null(attr(termobj, "offset"))) {
         # k = where offset() would have appeared in term.labels
         k <- attr(termobj, "response") - attr(termobj, "offset")
         index1 <- match(j, j[k], nomatch=0)[-dropx]
     } else index1 <- j[-dropx]
     newformula <- attr(termobj, "term.labels")[index1]
     # Adding () around each term is for a formula with  + (sex=='male')
     newformula <- reformulate(paste0("(", newformula, ")"),
                               response = rvar,
                               intercept = attr(termobj, "intercept"),
                               env = environment(termobj))
     if (length(newformula) == 0L) newformula <- "1"
     
     # addition of an extra specials label causes no harm
     result <- terms(newformula, specials = names(attr(termobj, "specials")))
     
     # now add back the predvars and dataClasses attributes; which do contain
     # the response and offset.
     index2 <- j[-dropx]
     if (attr(termobj, "response")==1) index2 <- c(1, index2 +1)
     # if 'i' drops an interaction it won't change predvars or dataClasses
     index2 <- index2[index2 <= length(rownames(attr(termobj, "factors")))]
     if (!is.null(attr(termobj, "offset")))
         index2 <- index2[-attr(termobj, "offset")]
     if (!is.null(attr(termobj, "predvars")))
         attr(result, "predvars") <-attr(termobj, "predvars")[c(1, index2 +1)]
     if (!is.null(attr(termobj, "dataClasses")))
         attr(result, "dataClasses") <- attr(termobj, "dataClasses")[index2]

     result
}
@

\end{document}


	[[alternative HTML version deleted]]


From beii@vsky m@iii@g oii @oi@com  Sat Aug 21 21:11:45 2021
From: beii@vsky m@iii@g oii @oi@com (beii@vsky m@iii@g oii @oi@com)
Date: Sat, 21 Aug 2021 19:11:45 +0000 (UTC)
Subject: [Rd] Add keywords to descriptions of R packages
References: <734568346.559482.1629573105017.ref@mail.yahoo.com>
Message-ID: <734568346.559482.1629573105017@mail.yahoo.com>

It's great that there are almost 20,000 R packages and that experts have curated the R packages in task views, but the volume of packages is such that task view maintainers may limit additions because of time constraints or because they don't want the task views to become unwieldy. I suggest that keywords, chosen by package authors from an approved list, be allowed in package descriptions. The keywords could be the same as task view names (for example "timeseries") or they could be more specialized (for example "garch"). Users would be able to search packages by keyword. Ideally, all R packages would be indexed and discoverable this way.
I maintain a list of time series packages that were not in the task view at the time I added them:?https://github.com/Beliavsky/R-Time-Series-Task-View-Supplement/blob/main/README.md. There are almost 130 entries. I also have one for finance:?https://github.com/Beliavsky/R-Finance-Task-View-Supplement .
	[[alternative HTML version deleted]]


From murdoch@dunc@n @end|ng |rom gm@||@com  Sun Aug 22 00:42:05 2021
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Sat, 21 Aug 2021 18:42:05 -0400
Subject: [Rd] Add keywords to descriptions of R packages
In-Reply-To: <734568346.559482.1629573105017@mail.yahoo.com>
References: <734568346.559482.1629573105017.ref@mail.yahoo.com>
 <734568346.559482.1629573105017@mail.yahoo.com>
Message-ID: <eb2ac262-6fe1-84a0-2b63-d2ba37134a07@gmail.com>

On 21/08/2021 3:11 p.m., beliavsky--- via R-devel wrote:
> It's great that there are almost 20,000 R packages and that experts have curated the R packages in task views, but the volume of packages is such that task view maintainers may limit additions because of time constraints or because they don't want the task views to become unwieldy. I suggest that keywords, chosen by package authors from an approved list, be allowed in package descriptions. The keywords could be the same as task view names (for example "timeseries") or they could be more specialized (for example "garch"). Users would be able to search packages by keyword. Ideally, all R packages would be indexed and discoverable this way.
> I maintain a list of time series packages that were not in the task view at the time I added them:?https://github.com/Beliavsky/R-Time-Series-Task-View-Supplement/blob/main/README.md. There are almost 130 entries. I also have one for finance:?https://github.com/Beliavsky/R-Finance-Task-View-Supplement 

There are already keywords, and (in my opinion) they are not very 
useful. See Writing R Extensions, section 2.1.1.  Even Roxygen2 supports 
them, using the @keywords token.

Duncan Murdoch


From c@@rd|@g@bor @end|ng |rom gm@||@com  Sun Aug 22 01:31:40 2021
From: c@@rd|@g@bor @end|ng |rom gm@||@com (=?UTF-8?B?R8OhYm9yIENzw6FyZGk=?=)
Date: Sun, 22 Aug 2021 01:31:40 +0200
Subject: [Rd] Add keywords to descriptions of R packages
In-Reply-To: <734568346.559482.1629573105017@mail.yahoo.com>
References: <734568346.559482.1629573105017.ref@mail.yahoo.com>
 <734568346.559482.1629573105017@mail.yahoo.com>
Message-ID: <CABtg=KmU39300PVP+t5aq-_=JSR2UDcXmPhxQH3VR9E7bmDMzA@mail.gmail.com>

FWIW R-hub indexes package metadata, and you can search it online:
https://r-pkg.org/search.html?q=time+series
or from within R:
https://r-hub.github.io/pkgsearch/

Gabor

On Sun, Aug 22, 2021 at 12:13 AM beliavsky--- via R-devel
<r-devel at r-project.org> wrote:
>
> It's great that there are almost 20,000 R packages and that experts have curated the R packages in task views, but the volume of packages is such that task view maintainers may limit additions because of time constraints or because they don't want the task views to become unwieldy. I suggest that keywords, chosen by package authors from an approved list, be allowed in package descriptions. The keywords could be the same as task view names (for example "timeseries") or they could be more specialized (for example "garch"). Users would be able to search packages by keyword. Ideally, all R packages would be indexed and discoverable this way.
> I maintain a list of time series packages that were not in the task view at the time I added them: https://github.com/Beliavsky/R-Time-Series-Task-View-Supplement/blob/main/README.md. There are almost 130 entries. I also have one for finance: https://github.com/Beliavsky/R-Finance-Task-View-Supplement .
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From v|tekj @end|ng |rom |c|oud@com  Mon Aug 23 14:15:53 2021
From: v|tekj @end|ng |rom |c|oud@com (jan Vitek)
Date: Mon, 23 Aug 2021 14:15:53 +0200
Subject: [Rd] Update on rtools4 and ucrt support
In-Reply-To: <CABFfbXsA_aRe3kznH3eYMDoOJAzzGb-NHMo4k3tQH4xJHL9sOw@mail.gmail.com>
References: <CABFfbXsA_aRe3kznH3eYMDoOJAzzGb-NHMo4k3tQH4xJHL9sOw@mail.gmail.com>
Message-ID: <FB660D89-F82D-4F46-ADE1-A906CF364FB6@icloud.com>

Hi Jeroen,

I mostly lurk on this list, but I was struck by your combative tone.

To pick on two random bits:
 
> ? a 6gb tarball with manually built things on his personal machine?

> ? a black-box system that is so opaque and complex that only one person
> knows how it works, and would make it much more difficult for
> students, universities, and other organisations to build R packages
> and libraries on Windows?


Tomas? tool chain isn't a blackbox, it has copious documentation (see [1]) 
and builds on any machine thanks to the provided docker container.

This is not to criticise your work which has its unique strengths, but to
state the obvious: these strengths are best discussed without passion 
based on factually accurate descriptions.


[1] https://developer.r-project.org/Blog/public/2021/03/12/windows/utf-8-toolchain-and-cran-package-checks/index.html


-jan

Jan Vitek, Professor 
Northeastern University


From murdoch@dunc@n @end|ng |rom gm@||@com  Mon Aug 23 14:37:19 2021
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Mon, 23 Aug 2021 08:37:19 -0400
Subject: [Rd] Update on rtools4 and ucrt support
In-Reply-To: <FB660D89-F82D-4F46-ADE1-A906CF364FB6@icloud.com>
References: <CABFfbXsA_aRe3kznH3eYMDoOJAzzGb-NHMo4k3tQH4xJHL9sOw@mail.gmail.com>
 <FB660D89-F82D-4F46-ADE1-A906CF364FB6@icloud.com>
Message-ID: <30bf1602-c33f-14f7-30d9-b54a9605f169@gmail.com>

On 23/08/2021 8:15 a.m., jan Vitek via R-devel wrote:
> Hi Jeroen,
> 
> I mostly lurk on this list, but I was struck by your combative tone.
> 
> To pick on two random bits:
>   
>> ? a 6gb tarball with manually built things on his personal machine?
> 
>> ? a black-box system that is so opaque and complex that only one person
>> knows how it works, and would make it much more difficult for
>> students, universities, and other organisations to build R packages
>> and libraries on Windows?
> 
> 
> Tomas? tool chain isn't a blackbox, it has copious documentation (see [1])
> and builds on any machine thanks to the provided docker container.
> 
> This is not to criticise your work which has its unique strengths, but to
> state the obvious: these strengths are best discussed without passion
> based on factually accurate descriptions.

I agree with Jan.  I'm not sure a discussion in this forum would be 
fruitful, but I really wish Jeroen and Tomas would get together, aiming 
to merge their toolchains, keeping the best aspects of both.

I haven't been involved in the development of either one, but have been 
a "victim" of the two chain rivalry, because the rgl package is not easy 
to build.  I get instructions from each of them on how to do the build, 
and those instructions for one toolchain generally break the build on 
the other one.  While it is probably possible to detect the toolchain 
and have the build adapt to whichever one is in use, it would be a lot 
easier for me (and I imagine every other maintainer of a package using 
external libs) if I just had to follow one set of instructions.

Duncan Murdoch


From |reder|k @end|ng |rom o|b@net  Mon Aug 23 15:04:55 2021
From: |reder|k @end|ng |rom o|b@net (Frederick Eaton)
Date: Mon, 23 Aug 2021 06:04:55 -0700
Subject: [Rd] clarify "by columns"
Message-ID: <20210823130455.elz4zqgcsj5oyduv@localhost>

Dear R Devel,

I realized that I've been reading something without really thinking
about it. In "?matrix" we have:

        byrow: logical. If 'FALSE' (the default) the matrix is filled by
               columns, otherwise the matrix is filled by rows.

I don't understand on the first reading what "by columns" means. An
experiment ("matrix(1:6,ncol=2,nrow=3)") shows that it means "one
column at a time" rather than "rotating through the columns" as I had
first imagined. However, the purpose of my looking up this
documentation was to remind myself of the order of matrix elements in
R; since I had to do the experiment anyway, I think the documentation
added a useless delay. Perhaps this could be fixed if we define the
meaning of "by columns" in the documentation?

Apparently this is the same as "column-major order", but I am usually
just as unfamiliar with the meaning of this other phrase. I would like
the documentation to define both phrases, for example:

        byrow: logical. If 'FALSE' (the default) the matrix is filled
               by columns, i.e. in "column-major order", meaning that
               matrix elements in the same column are assigned
               consecutive data elements. Otherwise the matrix is
               filled by rows.

Is that too wordy?

Thank you,

Frederick


From therne@u @end|ng |rom m@yo@edu  Mon Aug 23 16:36:29 2021
From: therne@u @end|ng |rom m@yo@edu (Therneau, Terry M., Ph.D.)
Date: Mon, 23 Aug 2021 09:36:29 -0500
Subject: [Rd] Issues with drop.terms
Message-ID: <2e47ad$ggtdia@ironport10.mayo.edu>

This is a follow-up to my earlier note on [.terms.?? Based on a couple days' work getting 
the survival package to work around? issues, this will hopefully be more concise and 
better expressed than the prior note.

1.
test1 <- terms( y ~ x1:x2 + x3)
check <- drop.terms(termobj =test1, dropx = 1)
formula(check)
## ~x1:x2

The documentation for the dropx argument is "vector of positions of variables to drop from 
the right hand side of the model", but it is not clear what "positions" is.?? I originally 
assumed "the order in the formula as typed", but was wrong.?? I suggest adding a line? 
"Position refers to the order of terms in the term.labels attribute of the terms object, 
which is also the order they will appear in a coefficient vector (not counting the 
intercept).

2.
library(splines)
test2 <- terms(model.frame(mpg ~? offset(cyl) + ns(hp, df=3) + disp + wt, data=mtcars))
check2 <- drop.terms(test2,? dropx = 2)
formula(check2)
## ~ns(hp, df=3) + wt

One side effect of how drop.terms is implemented, and one that I suspect was not intended, 
is that offsets are completly ignored.??? The above drops both the offset and the disp 
term from the formula ? The dataClasses and predvars attributes of the result are also 
incorrect: they have lost the ns() term rather than the disp term;
the results of predict will be incorrect.

attr(check2, "predvars")
## ?? list(offset(cyl), disp, wt)

Question: should the function be updated to not drop offsets? If not a line needs to be 
added to the help file. ? The handling of predvars needs to be fixed regardless.

3.
test3 <- terms(mpg ~ hp + (cyl==4) + disp + wt )
check3 <- drop.terms(test3, 3)
formula(check3)
lm( check3, data=mtcars)?? # fails

The drop.terms action has lost the () around the logical expression, which leads to an 
invalid formula.? We can argue that the user should have used I(cyc==4), but very many won't.

4. As a footnote, more confusion (for me) is generated by the fact that the "specials" 
attribute of a formula does not use the numbering discussed in 1 above.?? I had solved 
this issue long ago in the untangle.specials function; long enough ago that I forgot I had 
solved it, and just wasted a day rediscovering that fact.

---

I can create a patch for 1 and 2 (once we answer my question), but a fix for 3 is not 
clear to me.? It currently leads to failure in a coxph call that includes a strata so I am 
directly interested in a solution; e.g.,? coxph(Surv(time, status) ~ age + (ph.ecog==2) + 
strata(inst), data=lung)

Terry T

-- 

Terry M Therneau, PhD
Department of Quantitative Health Sciences
Mayo Clinic
therneau at mayo.edu

"TERR-ree THUR-noh"


	[[alternative HTML version deleted]]


From qweytr1 m@iii@g oii m@ii@ustc@edu@c@  Mon Aug 23 18:51:31 2021
From: qweytr1 m@iii@g oii m@ii@ustc@edu@c@ (qweytr1 m@iii@g oii m@ii@ustc@edu@c@)
Date: Tue, 24 Aug 2021 00:51:31 +0800 (GMT+08:00)
Subject: [Rd] Is it a good choice to increase the NCONNECTION value?
Message-ID: <4bf09c46.181e3.17b73ec9a4f.Coremail.qweytr1@mail.ustc.edu.cn>

At least in 2015, a github user, tobigithub, submit an [issue](https://github.com/sneumann/xcms/issues/20) about the error "Error in file(con, "w") : all connections are in use"
Nowadays, since AMD have really cool CPUs which increases the thread numbers to 128 or even 256 on a single server, we found that the NCONNECTIONS variable could prevent us from utilizing all the 128 threads.
It might be a good choice to increase its value.


the variable is defined in `R-4.1.1/src/main/connections.c: 17`
I have tested that, increase it to 1024 generates no error and all the clusters (I tried with 256 clusters on my 16 threads Laptop) works fine.


Is it possible increase the size of NCONNECTION?


	[[alternative HTML version deleted]]


From iuke-tier@ey m@iii@g oii uiow@@edu  Mon Aug 23 22:08:51 2021
From: iuke-tier@ey m@iii@g oii uiow@@edu (iuke-tier@ey m@iii@g oii uiow@@edu)
Date: Mon, 23 Aug 2021 15:08:51 -0500 (CDT)
Subject: [Rd] [External] Re:  Update on rtools4 and ucrt support
In-Reply-To: <30bf1602-c33f-14f7-30d9-b54a9605f169@gmail.com>
References: <CABFfbXsA_aRe3kznH3eYMDoOJAzzGb-NHMo4k3tQH4xJHL9sOw@mail.gmail.com>
 <FB660D89-F82D-4F46-ADE1-A906CF364FB6@icloud.com>
 <30bf1602-c33f-14f7-30d9-b54a9605f169@gmail.com>
Message-ID: <alpine.DEB.2.22.394.2108231505450.2943@luke-Latitude-7480>

On Mon, 23 Aug 2021, Duncan Murdoch wrote:

> On 23/08/2021 8:15 a.m., jan Vitek via R-devel wrote:
>> Hi Jeroen,
>> 
>> I mostly lurk on this list, but I was struck by your combative tone.
>> 
>> To pick on two random bits:
>> 
>>> ? a 6gb tarball with manually built things on his personal machine?
>> 
>>> ? a black-box system that is so opaque and complex that only one person
>>> knows how it works, and would make it much more difficult for
>>> students, universities, and other organisations to build R packages
>>> and libraries on Windows?
>> 
>> 
>> Tomas? tool chain isn't a blackbox, it has copious documentation (see [1])
>> and builds on any machine thanks to the provided docker container.
>> 
>> This is not to criticise your work which has its unique strengths, but to
>> state the obvious: these strengths are best discussed without passion
>> based on factually accurate descriptions.
>
> I agree with Jan.  I'm not sure a discussion in this forum would be fruitful, 
> but I really wish Jeroen and Tomas would get together, aiming to merge their 
> toolchains, keeping the best aspects of both.
>
> I haven't been involved in the development of either one, but have been a 
> "victim" of the two chain rivalry, because the rgl package is not easy to 
> build.  I get instructions from each of them on how to do the build, and 
> those instructions for one toolchain generally break the build on the other 
> one.  While it is probably possible to detect the toolchain and have the 
> build adapt to whichever one is in use, it would be a lot easier for me (and 
> I imagine every other maintainer of a package using external libs) if I just 
> had to follow one set of instructions.
>
> Duncan Murdoch

Here are just a few comments from my perspective (I am an R-core
member, but am not part of the CRAN team and do only very limited work
on Windows). Other R-core members may have different perspectives and
insights.

One bit of background: dealing with encoding issues on Windows has
been taking an unsustainable amount of R-core resources for some time
now. Tomas Kalibera has been taking the lead on trying to address
these issues in the existing framework, but this means he has not had
the time to make any of the many other valuable and important
contributions he could make. The only viable way forward is to move to
a Windows tool chain that supports UTF-8 as the C library current
encoding via the Windows UCRT framework.

Tomas Kalibera has, on behalf of all of R core and in
coordination with CRAN, been looking for a way forward for some
time and has reported on the progress in several blog posts at
https://developer.r-project.org/Blog/public/. This has lead to
the development of the MXE-based UCRT tool chain, which is now
well tested and ready for deployment.  Checks using the UCRT tool
chain have been part of the CRAN check process for a while. I
believe CRAN plans to switch R-devel checks and builds to the
UCRT tool chain during the upcoming CRAN downtime. I expect there
will be some communication from CRAN on this soon, including on
any issues in supporting binaries for both R-devel and R-patched.

In putting together something as large as a tool chain there will
always be many choices, each with advantages and disadvantages.  Some
things may be advantages in some settings and not others. Taking just
one case in point: Cross compilation. This is likely to be a better
approach for CRAN in the future and is supported by the MXE framework
on which the new tool chain is based.

The much more recent changes in rtools4 to support UCRT are at this
point not yet as well tested as the new tool chain. Once these changes
to rtools4 mature, and if binary compatibility can be assured, then
having a second tool chain may be useful in some cases.  But if there
are incompatibilities then it will be up to rtools4 to keep up with
the tool chain used by CRAN. On the other, contributing to improving
the MXE-based tool chain may be a better investment of time.

Best,

luke

>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

-- 
Luke Tierney
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu

From @vr@h@m@@d|er @end|ng |rom gm@||@com  Mon Aug 23 22:34:26 2021
From: @vr@h@m@@d|er @end|ng |rom gm@||@com (Avraham Adler)
Date: Mon, 23 Aug 2021 23:34:26 +0300
Subject: [Rd] [External] Re: Update on rtools4 and ucrt support
In-Reply-To: <alpine.DEB.2.22.394.2108231505450.2943@luke-Latitude-7480>
References: <CABFfbXsA_aRe3kznH3eYMDoOJAzzGb-NHMo4k3tQH4xJHL9sOw@mail.gmail.com>
 <FB660D89-F82D-4F46-ADE1-A906CF364FB6@icloud.com>
 <30bf1602-c33f-14f7-30d9-b54a9605f169@gmail.com>
 <alpine.DEB.2.22.394.2108231505450.2943@luke-Latitude-7480>
Message-ID: <CAL6gwnJfJsy4G6xs4nmWh8RBgHJuotYkukxOzzvpsDHk9-+U3A@mail.gmail.com>

On Mon, Aug 23, 2021 at 11:09 PM <luke-tierney at uiowa.edu> wrote:

> On Mon, 23 Aug 2021, Duncan Murdoch wrote:
>
> > On 23/08/2021 8:15 a.m., jan Vitek via R-devel wrote:
> >> Hi Jeroen,
> >>
> >> I mostly lurk on this list, but I was struck by your combative tone.
> >>
> >> To pick on two random bits:
> >>
> >>> ? a 6gb tarball with manually built things on his personal machine?
> >>
> >>> ? a black-box system that is so opaque and complex that only one person
> >>> knows how it works, and would make it much more difficult for
> >>> students, universities, and other organisations to build R packages
> >>> and libraries on Windows?
> >>
> >>
> >> Tomas? tool chain isn't a blackbox, it has copious documentation (see
> [1])
> >> and builds on any machine thanks to the provided docker container.
> >>
> >> This is not to criticise your work which has its unique strengths, but
> to
> >> state the obvious: these strengths are best discussed without passion
> >> based on factually accurate descriptions.
> >
> > I agree with Jan.  I'm not sure a discussion in this forum would be
> fruitful,
> > but I really wish Jeroen and Tomas would get together, aiming to merge
> their
> > toolchains, keeping the best aspects of both.
> >
> > I haven't been involved in the development of either one, but have been
> a
> > "victim" of the two chain rivalry, because the rgl package is not easy
> to
> > build.  I get instructions from each of them on how to do the build, and
> > those instructions for one toolchain generally break the build on the
> other
> > one.  While it is probably possible to detect the toolchain and have the
> > build adapt to whichever one is in use, it would be a lot easier for me
> (and
> > I imagine every other maintainer of a package using external libs) if I
> just
> > had to follow one set of instructions.
> >
> > Duncan Murdoch
>
> Here are just a few comments from my perspective (I am an R-core
> member, but am not part of the CRAN team and do only very limited work
> on Windows). Other R-core members may have different perspectives and
> insights.
>
> One bit of background: dealing with encoding issues on Windows has
> been taking an unsustainable amount of R-core resources for some time
> now. Tomas Kalibera has been taking the lead on trying to address
> these issues in the existing framework, but this means he has not had
> the time to make any of the many other valuable and important
> contributions he could make. The only viable way forward is to move to
> a Windows tool chain that supports UTF-8 as the C library current
> encoding via the Windows UCRT framework.
>
> Tomas Kalibera has, on behalf of all of R core and in
> coordination with CRAN, been looking for a way forward for some
> time and has reported on the progress in several blog posts at
> https://developer.r-project.org/Blog/public/. This has lead to
> the development of the MXE-based UCRT tool chain, which is now
> well tested and ready for deployment.  Checks using the UCRT tool
> chain have been part of the CRAN check process for a while. I
> believe CRAN plans to switch R-devel checks and builds to the
> UCRT tool chain during the upcoming CRAN downtime. I expect there
> will be some communication from CRAN on this soon, including on
> any issues in supporting binaries for both R-devel and R-patched.
>
> In putting together something as large as a tool chain there will
> always be many choices, each with advantages and disadvantages.  Some
> things may be advantages in some settings and not others. Taking just
> one case in point: Cross compilation. This is likely to be a better
> approach for CRAN in the future and is supported by the MXE framework
> on which the new tool chain is based.
>
> The much more recent changes in rtools4 to support UCRT are at this
> point not yet as well tested as the new tool chain. Once these changes
> to rtools4 mature, and if binary compatibility can be assured, then
> having a second tool chain may be useful in some cases.  But if there
> are incompatibilities then it will be up to rtools4 to keep up with
> the tool chain used by CRAN. On the other, contributing to improving
> the MXE-based tool chain may be a better investment of time.
>
> Best,
>
> luke
>
> >
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
> >
>
> --
> Luke Tierney
> Ralph E. Wareham Professor of Mathematical Sciences
> University of Iowa                  Phone:             319-335-3386
> Department of Statistics and        Fax:               319-335-3017
>     Actuarial Science
> 241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
> Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu
>

Thank you, Dr. Tierney. However, I am concerned about the not-insignificant
number of us who for various reasons can only do our development on
Windows. Rtools has been the official tool chain with which to build
windows for the at least 20 years I have been using R (yes, a babe in the
woods compared to most, but not a complete neophyte). Duncan and Jereoen
have each done yeoman?s jobs in ensuring that R can be built from source on
Windows and that packages can be developed for all OSS on Windows?even
Solaris SPARC.

I am much less aware of Thomas?s work, and I?ll gladly take the blame for
it, but I haven?t seen an accessible tool chain built by him which would
allow me, the Windows developer, to build R and all packages from source on
a native Windows box. Have I just missed it? If not, is that planned? If
R-core switches the official Windows toolchain, where does that leave us?

Thank you,

Avi

-- 
Sent from Gmail Mobile

	[[alternative HTML version deleted]]


From @|mon@urb@nek @end|ng |rom R-project@org  Mon Aug 23 23:41:28 2021
From: @|mon@urb@nek @end|ng |rom R-project@org (Simon Urbanek)
Date: Tue, 24 Aug 2021 09:41:28 +1200
Subject: [Rd] [External] Re: Update on rtools4 and ucrt support
In-Reply-To: <CAL6gwnJfJsy4G6xs4nmWh8RBgHJuotYkukxOzzvpsDHk9-+U3A@mail.gmail.com>
References: <CABFfbXsA_aRe3kznH3eYMDoOJAzzGb-NHMo4k3tQH4xJHL9sOw@mail.gmail.com>
 <FB660D89-F82D-4F46-ADE1-A906CF364FB6@icloud.com>
 <30bf1602-c33f-14f7-30d9-b54a9605f169@gmail.com>
 <alpine.DEB.2.22.394.2108231505450.2943@luke-Latitude-7480>
 <CAL6gwnJfJsy4G6xs4nmWh8RBgHJuotYkukxOzzvpsDHk9-+U3A@mail.gmail.com>
Message-ID: <B2E32AA1-809A-4079-8AFC-72174DD12ACE@R-project.org>

Avi,

please see the announcement:

https://developer.r-project.org/Blog/public/2021/03/12/windows/utf-8-toolchain-and-cran-package-checks/index.html <https://developer.r-project.org/Blog/public/2021/03/12/windows/utf-8-toolchain-and-cran-package-checks/index.html>

the documentation is in

https://svn.r-project.org/R-dev-web/trunk/WindowsBuilds/winutf8/ucrt3/howto.html <https://svn.r-project.org/R-dev-web/trunk/WindowsBuilds/winutf8/ucrt3/howto.html>

Cheers,
Simon




> On Aug 24, 2021, at 8:34 AM, Avraham Adler <avraham.adler at gmail.com> wrote:
> 
> On Mon, Aug 23, 2021 at 11:09 PM <luke-tierney at uiowa.edu> wrote:
> 
>> On Mon, 23 Aug 2021, Duncan Murdoch wrote:
>> 
>>> On 23/08/2021 8:15 a.m., jan Vitek via R-devel wrote:
>>>> Hi Jeroen,
>>>> 
>>>> I mostly lurk on this list, but I was struck by your combative tone.
>>>> 
>>>> To pick on two random bits:
>>>> 
>>>>> ? a 6gb tarball with manually built things on his personal machine?
>>>> 
>>>>> ? a black-box system that is so opaque and complex that only one person
>>>>> knows how it works, and would make it much more difficult for
>>>>> students, universities, and other organisations to build R packages
>>>>> and libraries on Windows?
>>>> 
>>>> 
>>>> Tomas? tool chain isn't a blackbox, it has copious documentation (see
>> [1])
>>>> and builds on any machine thanks to the provided docker container.
>>>> 
>>>> This is not to criticise your work which has its unique strengths, but
>> to
>>>> state the obvious: these strengths are best discussed without passion
>>>> based on factually accurate descriptions.
>>> 
>>> I agree with Jan.  I'm not sure a discussion in this forum would be
>> fruitful,
>>> but I really wish Jeroen and Tomas would get together, aiming to merge
>> their
>>> toolchains, keeping the best aspects of both.
>>> 
>>> I haven't been involved in the development of either one, but have been
>> a
>>> "victim" of the two chain rivalry, because the rgl package is not easy
>> to
>>> build.  I get instructions from each of them on how to do the build, and
>>> those instructions for one toolchain generally break the build on the
>> other
>>> one.  While it is probably possible to detect the toolchain and have the
>>> build adapt to whichever one is in use, it would be a lot easier for me
>> (and
>>> I imagine every other maintainer of a package using external libs) if I
>> just
>>> had to follow one set of instructions.
>>> 
>>> Duncan Murdoch
>> 
>> Here are just a few comments from my perspective (I am an R-core
>> member, but am not part of the CRAN team and do only very limited work
>> on Windows). Other R-core members may have different perspectives and
>> insights.
>> 
>> One bit of background: dealing with encoding issues on Windows has
>> been taking an unsustainable amount of R-core resources for some time
>> now. Tomas Kalibera has been taking the lead on trying to address
>> these issues in the existing framework, but this means he has not had
>> the time to make any of the many other valuable and important
>> contributions he could make. The only viable way forward is to move to
>> a Windows tool chain that supports UTF-8 as the C library current
>> encoding via the Windows UCRT framework.
>> 
>> Tomas Kalibera has, on behalf of all of R core and in
>> coordination with CRAN, been looking for a way forward for some
>> time and has reported on the progress in several blog posts at
>> https://developer.r-project.org/Blog/public/. This has lead to
>> the development of the MXE-based UCRT tool chain, which is now
>> well tested and ready for deployment.  Checks using the UCRT tool
>> chain have been part of the CRAN check process for a while. I
>> believe CRAN plans to switch R-devel checks and builds to the
>> UCRT tool chain during the upcoming CRAN downtime. I expect there
>> will be some communication from CRAN on this soon, including on
>> any issues in supporting binaries for both R-devel and R-patched.
>> 
>> In putting together something as large as a tool chain there will
>> always be many choices, each with advantages and disadvantages.  Some
>> things may be advantages in some settings and not others. Taking just
>> one case in point: Cross compilation. This is likely to be a better
>> approach for CRAN in the future and is supported by the MXE framework
>> on which the new tool chain is based.
>> 
>> The much more recent changes in rtools4 to support UCRT are at this
>> point not yet as well tested as the new tool chain. Once these changes
>> to rtools4 mature, and if binary compatibility can be assured, then
>> having a second tool chain may be useful in some cases.  But if there
>> are incompatibilities then it will be up to rtools4 to keep up with
>> the tool chain used by CRAN. On the other, contributing to improving
>> the MXE-based tool chain may be a better investment of time.
>> 
>> Best,
>> 
>> luke
>> 
>>> 
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>> 
>> 
>> --
>> Luke Tierney
>> Ralph E. Wareham Professor of Mathematical Sciences
>> University of Iowa                  Phone:             319-335-3386
>> Department of Statistics and        Fax:               319-335-3017
>>    Actuarial Science
>> 241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
>> Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu
>> 
> 
> Thank you, Dr. Tierney. However, I am concerned about the not-insignificant
> number of us who for various reasons can only do our development on
> Windows. Rtools has been the official tool chain with which to build
> windows for the at least 20 years I have been using R (yes, a babe in the
> woods compared to most, but not a complete neophyte). Duncan and Jereoen
> have each done yeoman?s jobs in ensuring that R can be built from source on
> Windows and that packages can be developed for all OSS on Windows?even
> Solaris SPARC.
> 
> I am much less aware of Thomas?s work, and I?ll gladly take the blame for
> it, but I haven?t seen an accessible tool chain built by him which would
> allow me, the Windows developer, to build R and all packages from source on
> a native Windows box. Have I just missed it? If not, is that planned? If
> R-core switches the official Windows toolchain, where does that leave us?
> 
> Thank you,
> 
> Avi
> 
> -- 
> Sent from Gmail Mobile
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 


	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Tue Aug 24 00:03:45 2021
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Mon, 23 Aug 2021 18:03:45 -0400
Subject: [Rd] Issues with drop.terms
In-Reply-To: <2e47ad$ggtdia@ironport10.mayo.edu>
References: <2e47ad$ggtdia@ironport10.mayo.edu>
Message-ID: <83923219-eda8-2148-4c76-369afb3e6978@gmail.com>

   Small follow-up: (1) in order for lm() to actually work you need 
keep.response=TRUE in the drop.terms() call (I realize that this is 
*not* the problem in your example)

test4 <- terms(mpg ~ hp + I(cyl==4) + disp + wt )
check4 <- drop.terms(test4, 3, keep.response = TRUE)
formula(check4)
lm( check4, data=mtcars)

(2) I'm ambivalent about your "We can argue that the user should have 
used I(cyl==4), but very many won't." argument. This is the ever-present 
"document precisely and require users to know and follow the 
documentation" vs. "try to protect users from themselves" debate - 
taking either side to an extreme is (IMO) unproductive. I don't know how 
hard it would be to make drop.terms() **not** drop parentheses, but it 
seems like it may be very hard/low-level. My vote would be to see if 
there is a reasonably robust way to detect these constructions and 
**warn** about them.

   I have probably asked about this before, but if anyone knows of 
useful materials that go into more details about the definitions and 
implementation of model matrix/terms/etc. machinery, *beyond* the 
appropriate chapter of "Statistical Models in S" (Becker/Chambers white 
book), *or* the source code itself, I would love some pointers ...

  Ben Bolker


On 8/23/21 10:36 AM, Therneau, Terry M., Ph.D. via R-devel wrote:
> This is a follow-up to my earlier note on [.terms.?? Based on a couple days' work getting
> the survival package to work around? issues, this will hopefully be more concise and
> better expressed than the prior note.
> 
> 1.
> test1 <- terms( y ~ x1:x2 + x3)
> check <- drop.terms(termobj =test1, dropx = 1)
> formula(check)
> ## ~x1:x2
> 
> The documentation for the dropx argument is "vector of positions of variables to drop from
> the right hand side of the model", but it is not clear what "positions" is.?? I originally
> assumed "the order in the formula as typed", but was wrong.?? I suggest adding a line
> "Position refers to the order of terms in the term.labels attribute of the terms object,
> which is also the order they will appear in a coefficient vector (not counting the
> intercept).
> 
> 2.
> library(splines)
> test2 <- terms(model.frame(mpg ~? offset(cyl) + ns(hp, df=3) + disp + wt, data=mtcars))
> check2 <- drop.terms(test2,? dropx = 2)
> formula(check2)
> ## ~ns(hp, df=3) + wt
> 
> One side effect of how drop.terms is implemented, and one that I suspect was not intended,
> is that offsets are completly ignored.??? The above drops both the offset and the disp
> term from the formula ? The dataClasses and predvars attributes of the result are also
> incorrect: they have lost the ns() term rather than the disp term;
> the results of predict will be incorrect.
> 
> attr(check2, "predvars")
> ## ?? list(offset(cyl), disp, wt)
> 
> Question: should the function be updated to not drop offsets? If not a line needs to be
> added to the help file. ? The handling of predvars needs to be fixed regardless.
> 
> 3.
> test3 <- terms(mpg ~ hp + (cyl==4) + disp + wt )
> check3 <- drop.terms(test3, 3)
> formula(check3)
> lm( check3, data=mtcars)?? # fails
> 
> The drop.terms action has lost the () around the logical expression, which leads to an
> invalid formula.? We can argue that the user should have used I(cyc==4), but very many won't.
> 
> 4. As a footnote, more confusion (for me) is generated by the fact that the "specials"
> attribute of a formula does not use the numbering discussed in 1 above.?? I had solved
> this issue long ago in the untangle.specials function; long enough ago that I forgot I had
> solved it, and just wasted a day rediscovering that fact.
> 
> ---
> 
> I can create a patch for 1 and 2 (once we answer my question), but a fix for 3 is not
> clear to me.? It currently leads to failure in a coxph call that includes a strata so I am
> directly interested in a solution; e.g.,? coxph(Surv(time, status) ~ age + (ph.ecog==2) +
> strata(inst), data=lung)
> 
> Terry T
> 

-- 
Dr. Benjamin Bolker
Professor, Mathematics & Statistics and Biology, McMaster University
Director, School of Computational Science and Engineering
Graduate chair, Mathematics & Statistics


From @vr@h@m@@d|er @end|ng |rom gm@||@com  Tue Aug 24 00:03:07 2021
From: @vr@h@m@@d|er @end|ng |rom gm@||@com (Avraham Adler)
Date: Tue, 24 Aug 2021 01:03:07 +0300
Subject: [Rd] [External] Re: Update on rtools4 and ucrt support
In-Reply-To: <B2E32AA1-809A-4079-8AFC-72174DD12ACE@R-project.org>
References: <CABFfbXsA_aRe3kznH3eYMDoOJAzzGb-NHMo4k3tQH4xJHL9sOw@mail.gmail.com>
 <FB660D89-F82D-4F46-ADE1-A906CF364FB6@icloud.com>
 <30bf1602-c33f-14f7-30d9-b54a9605f169@gmail.com>
 <alpine.DEB.2.22.394.2108231505450.2943@luke-Latitude-7480>
 <CAL6gwnJfJsy4G6xs4nmWh8RBgHJuotYkukxOzzvpsDHk9-+U3A@mail.gmail.com>
 <B2E32AA1-809A-4079-8AFC-72174DD12ACE@R-project.org>
Message-ID: <CAL6gwnLozD2RRO7271Tpgbf4yqXckPBQwL_TNv1zCTDTdd3DCw@mail.gmail.com>

On Tue, Aug 24, 2021 at 12:47 AM Simon Urbanek <simon.urbanek at r-project.org>
wrote:

> Avi,
>
> please see the announcement:
>
>
> https://developer.r-project.org/Blog/public/2021/03/12/windows/utf-8-toolchain-and-cran-package-checks/index.html
>
> the documentation is in
>
>
> https://svn.r-project.org/R-dev-web/trunk/WindowsBuilds/winutf8/ucrt3/howto.html
>
> Cheers,
> Simon
>
>
>
>
> On Aug 24, 2021, at 8:34 AM, Avraham Adler <avraham.adler at gmail.com>
> wrote:
>
> On Mon, Aug 23, 2021 at 11:09 PM <luke-tierney at uiowa.edu> wrote:
>
> On Mon, 23 Aug 2021, Duncan Murdoch wrote:
>
> On 23/08/2021 8:15 a.m., jan Vitek via R-devel wrote:
>
> Hi Jeroen,
>
> I mostly lurk on this list, but I was struck by your combative tone.
>
> To pick on two random bits:
>
> ? a 6gb tarball with manually built things on his personal machine?
>
>
> ? a black-box system that is so opaque and complex that only one person
> knows how it works, and would make it much more difficult for
> students, universities, and other organisations to build R packages
> and libraries on Windows?
>
>
>
> Tomas? tool chain isn't a blackbox, it has copious documentation (see
>
> [1])
>
> and builds on any machine thanks to the provided docker container.
>
> This is not to criticise your work which has its unique strengths, but
>
> to
>
> state the obvious: these strengths are best discussed without passion
> based on factually accurate descriptions.
>
>
> I agree with Jan.  I'm not sure a discussion in this forum would be
>
> fruitful,
>
> but I really wish Jeroen and Tomas would get together, aiming to merge
>
> their
>
> toolchains, keeping the best aspects of both.
>
> I haven't been involved in the development of either one, but have been
>
> a
>
> "victim" of the two chain rivalry, because the rgl package is not easy
>
> to
>
> build.  I get instructions from each of them on how to do the build, and
> those instructions for one toolchain generally break the build on the
>
> other
>
> one.  While it is probably possible to detect the toolchain and have the
> build adapt to whichever one is in use, it would be a lot easier for me
>
> (and
>
> I imagine every other maintainer of a package using external libs) if I
>
> just
>
> had to follow one set of instructions.
>
> Duncan Murdoch
>
>
> Here are just a few comments from my perspective (I am an R-core
> member, but am not part of the CRAN team and do only very limited work
> on Windows). Other R-core members may have different perspectives and
> insights.
>
> One bit of background: dealing with encoding issues on Windows has
> been taking an unsustainable amount of R-core resources for some time
> now. Tomas Kalibera has been taking the lead on trying to address
> these issues in the existing framework, but this means he has not had
> the time to make any of the many other valuable and important
> contributions he could make. The only viable way forward is to move to
> a Windows tool chain that supports UTF-8 as the C library current
> encoding via the Windows UCRT framework.
>
> Tomas Kalibera has, on behalf of all of R core and in
> coordination with CRAN, been looking for a way forward for some
> time and has reported on the progress in several blog posts at
> https://developer.r-project.org/Blog/public/. This has lead to
> the development of the MXE-based UCRT tool chain, which is now
> well tested and ready for deployment.  Checks using the UCRT tool
> chain have been part of the CRAN check process for a while. I
> believe CRAN plans to switch R-devel checks and builds to the
> UCRT tool chain during the upcoming CRAN downtime. I expect there
> will be some communication from CRAN on this soon, including on
> any issues in supporting binaries for both R-devel and R-patched.
>
> In putting together something as large as a tool chain there will
> always be many choices, each with advantages and disadvantages.  Some
> things may be advantages in some settings and not others. Taking just
> one case in point: Cross compilation. This is likely to be a better
> approach for CRAN in the future and is supported by the MXE framework
> on which the new tool chain is based.
>
> The much more recent changes in rtools4 to support UCRT are at this
> point not yet as well tested as the new tool chain. Once these changes
> to rtools4 mature, and if binary compatibility can be assured, then
> having a second tool chain may be useful in some cases.  But if there
> are incompatibilities then it will be up to rtools4 to keep up with
> the tool chain used by CRAN. On the other, contributing to improving
> the MXE-based tool chain may be a better investment of time.
>
> Best,
>
> luke
>
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>
>
> --
> Luke Tierney
> Ralph E. Wareham Professor of Mathematical Sciences
> University of Iowa                  Phone:             319-335-3386
> Department of Statistics and        Fax:               319-335-3017
>    Actuarial Science
> 241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
> <luke-tierney at uiowa.edu>
> Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu
>
>
> Thank you, Dr. Tierney. However, I am concerned about the not-insignificant
> number of us who for various reasons can only do our development on
> Windows. Rtools has been the official tool chain with which to build
> windows for the at least 20 years I have been using R (yes, a babe in the
> woods compared to most, but not a complete neophyte). Duncan and Jereoen
> have each done yeoman?s jobs in ensuring that R can be built from source on
> Windows and that packages can be developed for all OSS on Windows?even
> Solaris SPARC.
>
> I am much less aware of Thomas?s work, and I?ll gladly take the blame for
> it, but I haven?t seen an accessible tool chain built by him which would
> allow me, the Windows developer, to build R and all packages from source on
> a native Windows box. Have I just missed it? If not, is that planned? If
> R-core switches the official Windows toolchain, where does that leave us?
>
> Thank you,
>
> Avi
>
> --
> Sent from Gmail Mobile
>
>
Thank you, Simon. That was valuable. Skimming that quickly, I get a bit
concerned. I?ve been building from source and then using OpenBLAS in my R
source for many, many years now, and it looks like its support is tenuous
in the experimental chain. Similarly with packages like nloptr, where I
build NLOPT 2.6+ and have adjusted Jelmer?s code for it to work in R for
Windows. I maintain packages with Fortran/OpenMP and Rcpp(Parallel). I hope
that should the decision be made to switch, it will be done when the build
process is more streamlined, especially for some fundamental packages.

That being said, I must take the opportunity again to thank R-core, Tomas,
Jeroen, Duncan among many others who have built an infrastructure that
allows amateur programmers to contribute to the statistical infrastructure.

Thanks again,

Avi
-- 
Sent from Gmail Mobile

	[[alternative HTML version deleted]]


From tom@@@k@||ber@ @end|ng |rom gm@||@com  Tue Aug 24 00:22:42 2021
From: tom@@@k@||ber@ @end|ng |rom gm@||@com (Tomas Kalibera)
Date: Tue, 24 Aug 2021 00:22:42 +0200
Subject: [Rd] [External] Re: Update on rtools4 and ucrt support
In-Reply-To: <CAL6gwnLozD2RRO7271Tpgbf4yqXckPBQwL_TNv1zCTDTdd3DCw@mail.gmail.com>
References: <CABFfbXsA_aRe3kznH3eYMDoOJAzzGb-NHMo4k3tQH4xJHL9sOw@mail.gmail.com>
 <FB660D89-F82D-4F46-ADE1-A906CF364FB6@icloud.com>
 <30bf1602-c33f-14f7-30d9-b54a9605f169@gmail.com>
 <alpine.DEB.2.22.394.2108231505450.2943@luke-Latitude-7480>
 <CAL6gwnJfJsy4G6xs4nmWh8RBgHJuotYkukxOzzvpsDHk9-+U3A@mail.gmail.com>
 <B2E32AA1-809A-4079-8AFC-72174DD12ACE@R-project.org>
 <CAL6gwnLozD2RRO7271Tpgbf4yqXckPBQwL_TNv1zCTDTdd3DCw@mail.gmail.com>
Message-ID: <23153947-26ed-0e7c-3b79-ec3f5488f4d0@gmail.com>


On 8/24/21 12:03 AM, Avraham Adler wrote:
> On Tue, Aug 24, 2021 at 12:47 AM Simon Urbanek <simon.urbanek at r-project.org>
> wrote:
>
>> Avi,
>>
>> please see the announcement:
>>
>>
>> https://developer.r-project.org/Blog/public/2021/03/12/windows/utf-8-toolchain-and-cran-package-checks/index.html
>>
>> the documentation is in
>>
>>
>> https://svn.r-project.org/R-dev-web/trunk/WindowsBuilds/winutf8/ucrt3/howto.html
>>
>> Cheers,
>> Simon
>>
>>
>>
>>
>> On Aug 24, 2021, at 8:34 AM, Avraham Adler <avraham.adler at gmail.com>
>> wrote:
>>
>> On Mon, Aug 23, 2021 at 11:09 PM <luke-tierney at uiowa.edu> wrote:
>>
>> On Mon, 23 Aug 2021, Duncan Murdoch wrote:
>>
>> On 23/08/2021 8:15 a.m., jan Vitek via R-devel wrote:
>>
>> Hi Jeroen,
>>
>> I mostly lurk on this list, but I was struck by your combative tone.
>>
>> To pick on two random bits:
>>
>> ? a 6gb tarball with manually built things on his personal machine?
>>
>>
>> ? a black-box system that is so opaque and complex that only one person
>> knows how it works, and would make it much more difficult for
>> students, universities, and other organisations to build R packages
>> and libraries on Windows?
>>
>>
>>
>> Tomas? tool chain isn't a blackbox, it has copious documentation (see
>>
>> [1])
>>
>> and builds on any machine thanks to the provided docker container.
>>
>> This is not to criticise your work which has its unique strengths, but
>>
>> to
>>
>> state the obvious: these strengths are best discussed without passion
>> based on factually accurate descriptions.
>>
>>
>> I agree with Jan.  I'm not sure a discussion in this forum would be
>>
>> fruitful,
>>
>> but I really wish Jeroen and Tomas would get together, aiming to merge
>>
>> their
>>
>> toolchains, keeping the best aspects of both.
>>
>> I haven't been involved in the development of either one, but have been
>>
>> a
>>
>> "victim" of the two chain rivalry, because the rgl package is not easy
>>
>> to
>>
>> build.  I get instructions from each of them on how to do the build, and
>> those instructions for one toolchain generally break the build on the
>>
>> other
>>
>> one.  While it is probably possible to detect the toolchain and have the
>> build adapt to whichever one is in use, it would be a lot easier for me
>>
>> (and
>>
>> I imagine every other maintainer of a package using external libs) if I
>>
>> just
>>
>> had to follow one set of instructions.
>>
>> Duncan Murdoch
>>
>>
>> Here are just a few comments from my perspective (I am an R-core
>> member, but am not part of the CRAN team and do only very limited work
>> on Windows). Other R-core members may have different perspectives and
>> insights.
>>
>> One bit of background: dealing with encoding issues on Windows has
>> been taking an unsustainable amount of R-core resources for some time
>> now. Tomas Kalibera has been taking the lead on trying to address
>> these issues in the existing framework, but this means he has not had
>> the time to make any of the many other valuable and important
>> contributions he could make. The only viable way forward is to move to
>> a Windows tool chain that supports UTF-8 as the C library current
>> encoding via the Windows UCRT framework.
>>
>> Tomas Kalibera has, on behalf of all of R core and in
>> coordination with CRAN, been looking for a way forward for some
>> time and has reported on the progress in several blog posts at
>> https://developer.r-project.org/Blog/public/. This has lead to
>> the development of the MXE-based UCRT tool chain, which is now
>> well tested and ready for deployment.  Checks using the UCRT tool
>> chain have been part of the CRAN check process for a while. I
>> believe CRAN plans to switch R-devel checks and builds to the
>> UCRT tool chain during the upcoming CRAN downtime. I expect there
>> will be some communication from CRAN on this soon, including on
>> any issues in supporting binaries for both R-devel and R-patched.
>>
>> In putting together something as large as a tool chain there will
>> always be many choices, each with advantages and disadvantages.  Some
>> things may be advantages in some settings and not others. Taking just
>> one case in point: Cross compilation. This is likely to be a better
>> approach for CRAN in the future and is supported by the MXE framework
>> on which the new tool chain is based.
>>
>> The much more recent changes in rtools4 to support UCRT are at this
>> point not yet as well tested as the new tool chain. Once these changes
>> to rtools4 mature, and if binary compatibility can be assured, then
>> having a second tool chain may be useful in some cases.  But if there
>> are incompatibilities then it will be up to rtools4 to keep up with
>> the tool chain used by CRAN. On the other, contributing to improving
>> the MXE-based tool chain may be a better investment of time.
>>
>> Best,
>>
>> luke
>>
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>>
>> --
>> Luke Tierney
>> Ralph E. Wareham Professor of Mathematical Sciences
>> University of Iowa                  Phone:             319-335-3386
>> Department of Statistics and        Fax:               319-335-3017
>>     Actuarial Science
>> 241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
>> <luke-tierney at uiowa.edu>
>> Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu
>>
>>
>> Thank you, Dr. Tierney. However, I am concerned about the not-insignificant
>> number of us who for various reasons can only do our development on
>> Windows. Rtools has been the official tool chain with which to build
>> windows for the at least 20 years I have been using R (yes, a babe in the
>> woods compared to most, but not a complete neophyte). Duncan and Jereoen
>> have each done yeoman?s jobs in ensuring that R can be built from source on
>> Windows and that packages can be developed for all OSS on Windows?even
>> Solaris SPARC.
>>
>> I am much less aware of Thomas?s work, and I?ll gladly take the blame for
>> it, but I haven?t seen an accessible tool chain built by him which would
>> allow me, the Windows developer, to build R and all packages from source on
>> a native Windows box. Have I just missed it? If not, is that planned? If
>> R-core switches the official Windows toolchain, where does that leave us?
>>
>> Thank you,
>>
>> Avi
>>
>> --
>> Sent from Gmail Mobile
>>
>>
> Thank you, Simon. That was valuable. Skimming that quickly, I get a bit
> concerned. I?ve been building from source and then using OpenBLAS in my R
> source for many, many years now, and it looks like its support is tenuous
> in the experimental chain. Similarly with packages like nloptr, where I
> build NLOPT 2.6+ and have adjusted Jelmer?s code for it to work in R for
> Windows. I maintain packages with Fortran/OpenMP and Rcpp(Parallel). I hope
> that should the decision be made to switch, it will be done when the build
> process is more streamlined, especially for some fundamental packages.

Hi Avi,

if your R package includes source code (C/C++/Fortran), there is no 
fundamental difference, just the version of the compilers and/or 
external libraries may have changed.

nloptr checks are fine, see 
https://cran.r-project.org/web/checks/check_results_nloptr.html

The architecture is r-devel-windows-x86_64-gcc10-UCRT.

Best
Tomas

>
> That being said, I must take the opportunity again to thank R-core, Tomas,
> Jeroen, Duncan among many others who have built an infrastructure that
> allows amateur programmers to contribute to the statistical infrastructure.
>
> Thanks again,
>
> Avi


From @|mon@urb@nek @end|ng |rom R-project@org  Tue Aug 24 00:44:58 2021
From: @|mon@urb@nek @end|ng |rom R-project@org (Simon Urbanek)
Date: Tue, 24 Aug 2021 10:44:58 +1200
Subject: [Rd] [External] Re: Update on rtools4 and ucrt support
In-Reply-To: <CAL6gwnLozD2RRO7271Tpgbf4yqXckPBQwL_TNv1zCTDTdd3DCw@mail.gmail.com>
References: <CABFfbXsA_aRe3kznH3eYMDoOJAzzGb-NHMo4k3tQH4xJHL9sOw@mail.gmail.com>
 <FB660D89-F82D-4F46-ADE1-A906CF364FB6@icloud.com>
 <30bf1602-c33f-14f7-30d9-b54a9605f169@gmail.com>
 <alpine.DEB.2.22.394.2108231505450.2943@luke-Latitude-7480>
 <CAL6gwnJfJsy4G6xs4nmWh8RBgHJuotYkukxOzzvpsDHk9-+U3A@mail.gmail.com>
 <B2E32AA1-809A-4079-8AFC-72174DD12ACE@R-project.org>
 <CAL6gwnLozD2RRO7271Tpgbf4yqXckPBQwL_TNv1zCTDTdd3DCw@mail.gmail.com>
Message-ID: <37D56D1C-87DD-45D4-9ADB-7523AD2B2848@R-project.org>


Avi,

thanks. Yes, the whole point of the developer blog posts by R-core are for uses to provide feedback, so that's great - it's odd that it required a somewhat orthogonal post to start the discussion several months later, but I'm glad we got here.

Note that the point of the switch is to iron out all issues that may be encountered and provide real CRAN testing - that's why both R-core and CRAN is involved in all this. This only affects R-devel since we want to be ready for the R 4.2.0 release, it won't affect the current release builds.

Anyway, now I'll leave it to the Windows experts to address the details and work together.

Cheers,
Simon


> On Aug 24, 2021, at 10:03 AM, Avraham Adler <avraham.adler at gmail.com> wrote:
> 
> 
> Thank you, Simon. That was valuable. Skimming that quickly, I get a bit concerned. I?ve been building from source and then using OpenBLAS in my R source for many, many years now, and it looks like its support is tenuous in the experimental chain. Similarly with packages like nloptr, where I build NLOPT 2.6+ and have adjusted Jelmer?s code for it to work in R for Windows. I maintain packages with Fortran/OpenMP and Rcpp(Parallel). I hope that should the decision be made to switch, it will be done when the build process is more streamlined, especially for some fundamental packages. 
> 
> That being said, I must take the opportunity again to thank R-core, Tomas, Jeroen, Duncan among many others who have built an infrastructure that allows amateur programmers to contribute to the statistical infrastructure. 
> 
> Thanks again,
> 
> Avi


From edd @end|ng |rom deb|@n@org  Tue Aug 24 01:00:11 2021
From: edd @end|ng |rom deb|@n@org (Dirk Eddelbuettel)
Date: Mon, 23 Aug 2021 18:00:11 -0500
Subject: [Rd] [External] Re: Update on rtools4 and ucrt support
In-Reply-To: <23153947-26ed-0e7c-3b79-ec3f5488f4d0@gmail.com>
References: <CABFfbXsA_aRe3kznH3eYMDoOJAzzGb-NHMo4k3tQH4xJHL9sOw@mail.gmail.com>
 <FB660D89-F82D-4F46-ADE1-A906CF364FB6@icloud.com>
 <30bf1602-c33f-14f7-30d9-b54a9605f169@gmail.com>
 <alpine.DEB.2.22.394.2108231505450.2943@luke-Latitude-7480>
 <CAL6gwnJfJsy4G6xs4nmWh8RBgHJuotYkukxOzzvpsDHk9-+U3A@mail.gmail.com>
 <B2E32AA1-809A-4079-8AFC-72174DD12ACE@R-project.org>
 <CAL6gwnLozD2RRO7271Tpgbf4yqXckPBQwL_TNv1zCTDTdd3DCw@mail.gmail.com>
 <23153947-26ed-0e7c-3b79-ec3f5488f4d0@gmail.com>
Message-ID: <24868.10363.899258.369936@rob.eddelbuettel.com>


As I type this, we are eight messages into this thread -- but I am not sure
it has been made clear what the actual contentious issues are.

There appear to be two toolchains, and they appear to be interoperate (though
Duncan stated he had issues with an (arguably demanding) package).  Now, I
have the opposite (hence positive) experience.  For one package I look after,
a colleague took care of the (complicated in that case) 'needed to build the
package' pre-requirements by ensuring we have a UCRT variant.  Jeroen then
(unprompted) supplied a two-line/two-file PR to enable a Windows UCRT build
(piggy-backing on the existing Windows build), and with that the 'ERROR' I
had at CRAN reports under Tomas UCRT entry is gone. Net-net, this looks like
a working setup to me which combines both toolchains without issues.

And I was able to repeat this with a few more packages of mine for which
Jeroen's winlibs factory has libraries---these now build to under Tomas's
builder at CRAN.  So maybe this is not an either-or discussion?  So if there
are issues, could we be told what they are, and could we possibly help Jeroen
and Tomas to iron them out?

Dirk

-- 
https://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From g@bembecker @end|ng |rom gm@||@com  Tue Aug 24 02:46:39 2021
From: g@bembecker @end|ng |rom gm@||@com (Gabriel Becker)
Date: Mon, 23 Aug 2021 17:46:39 -0700
Subject: [Rd] [External] Re: Update on rtools4 and ucrt support
In-Reply-To: <24868.10363.899258.369936@rob.eddelbuettel.com>
References: <CABFfbXsA_aRe3kznH3eYMDoOJAzzGb-NHMo4k3tQH4xJHL9sOw@mail.gmail.com>
 <FB660D89-F82D-4F46-ADE1-A906CF364FB6@icloud.com>
 <30bf1602-c33f-14f7-30d9-b54a9605f169@gmail.com>
 <alpine.DEB.2.22.394.2108231505450.2943@luke-Latitude-7480>
 <CAL6gwnJfJsy4G6xs4nmWh8RBgHJuotYkukxOzzvpsDHk9-+U3A@mail.gmail.com>
 <B2E32AA1-809A-4079-8AFC-72174DD12ACE@R-project.org>
 <CAL6gwnLozD2RRO7271Tpgbf4yqXckPBQwL_TNv1zCTDTdd3DCw@mail.gmail.com>
 <23153947-26ed-0e7c-3b79-ec3f5488f4d0@gmail.com>
 <24868.10363.899258.369936@rob.eddelbuettel.com>
Message-ID: <CAD4oTHG9dX2Qbde4KaOk75xJYmMfGDG7sk_wJj8De+Bsisb+bA@mail.gmail.com>

Hi all,

I will preface this with the fact that I don't do work on windows and the
following is based on remembered conversations/talks/etc from a while ago
so may be either incorrect or out of date, but I recall one of the major
things Jeroen was  targeting was use of/integration with a meaningful
package manager for external library dependencies in windows from-source
package builds, and that *I think* this was a part of his (then explicitly
experimental) Rtools4 setup (?)

Is the above correct, and if so, is there also package manager
integration/usage in Tomas' official R-core UCRT toolchain? If not, could
it be (perhaps, as Duncan suggested, via collaborative effort involving
Jeroen as well)?

I admit, both windows toolchains/builds and non-latin encodings are things
I have so far stayed away from, so I can't really contribute beyond that,
other than to say, as others have, that I do think both are impressive
pieces of work and that both Jeroen and Tomas should have our thanks thanks
for this and a lot of other work they put into R and the R community.

Best,
~G

On Mon, Aug 23, 2021 at 4:02 PM Dirk Eddelbuettel <edd at debian.org> wrote:

>
> As I type this, we are eight messages into this thread -- but I am not sure
> it has been made clear what the actual contentious issues are.
>
> There appear to be two toolchains, and they appear to be interoperate
> (though
> Duncan stated he had issues with an (arguably demanding) package).  Now, I
> have the opposite (hence positive) experience.  For one package I look
> after,
> a colleague took care of the (complicated in that case) 'needed to build
> the
> package' pre-requirements by ensuring we have a UCRT variant.  Jeroen then
> (unprompted) supplied a two-line/two-file PR to enable a Windows UCRT build
> (piggy-backing on the existing Windows build), and with that the 'ERROR' I
> had at CRAN reports under Tomas UCRT entry is gone. Net-net, this looks
> like
> a working setup to me which combines both toolchains without issues.
>
> And I was able to repeat this with a few more packages of mine for which
> Jeroen's winlibs factory has libraries---these now build to under Tomas's
> builder at CRAN.  So maybe this is not an either-or discussion?  So if
> there
> are issues, could we be told what they are, and could we possibly help
> Jeroen
> and Tomas to iron them out?
>
> Dirk
>
> --
> https://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Tue Aug 24 10:44:35 2021
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Tue, 24 Aug 2021 10:44:35 +0200
Subject: [Rd] Is it a good choice to increase the NCONNECTION value?
In-Reply-To: <4bf09c46.181e3.17b73ec9a4f.Coremail.qweytr1@mail.ustc.edu.cn>
References: <4bf09c46.181e3.17b73ec9a4f.Coremail.qweytr1@mail.ustc.edu.cn>
Message-ID: <24868.45427.479782.187351@stat.math.ethz.ch>

>>>>> qweytr1--- via R-devel 
>>>>>     on Tue, 24 Aug 2021 00:51:31 +0800 (GMT+08:00) writes:

    > At least in 2015, a github user, tobigithub, submit an
    > [issue](https://github.com/sneumann/xcms/issues/20) about
    > the error "Error in file(con, "w") : all connections are
    > in use" Nowadays, since AMD have really cool CPUs which
    > increases the thread numbers to 128 or even 256 on a
    > single server, we found that the NCONNECTIONS variable
    > could prevent us from utilizing all the 128 threads.  It
    > might be a good choice to increase its value.


    > the variable is defined in
    > `R-4.1.1/src/main/connections.c: 17` I have tested that,
    > increase it to 1024 generates no error and all the
    > clusters (I tried with 256 clusters on my 16 threads
    > Laptop) works fine.

    > Is it possible increase the size of NCONNECTION?

Yes, of course, it is possible.
The question is how much it costs  and to which number it should
be increased.

A quick look at the source connections.c --> src/R_ext/include/Connections.h
reveals that the Rconnection* <--> Rconn is a struct with about
200 chars and ca 30 int-like plus another 20 pointers .. which
would amount to a rough 400 bytes per connection.
Adding 1024-128 = 896 new ones  would then amount to increase
the R executable by about 360 kB .. all the above being rough.
So personally, I guess that's  "about ok" --
are there other things to consider?

Ideally, of course, the number of possible connections could be
increased dynamically only when needed

Martin


From Andre@G||||bert @end|ng |rom chu-rouen@|r  Tue Aug 24 11:49:52 2021
From: Andre@G||||bert @end|ng |rom chu-rouen@|r (GILLIBERT, Andre)
Date: Tue, 24 Aug 2021 09:49:52 +0000
Subject: [Rd] Is it a good choice to increase the NCONNECTION value?
In-Reply-To: <24868.45427.479782.187351@stat.math.ethz.ch>
References: <4bf09c46.181e3.17b73ec9a4f.Coremail.qweytr1@mail.ustc.edu.cn>,
 <24868.45427.479782.187351@stat.math.ethz.ch>
Message-ID: <637ad8b722e74c0b99e900acf0715efe@chu-rouen.fr>

RConnection is a pointer to a Rconn structure. The Rconn structure must be allocated independently (e.g. by malloc() in R_new_custom_connection).

Therefore, increasing NCONNECTION to 1024 should only use 8 kilobytes on 64-bits platforms and 4 kilobytes on 32 bits platforms.

Ideally, it should be dynamically allocated : either as a linked list or as a dynamic array (malloc/realloc). However, a simple change of NCONNECTION to 1024 should be enough for most uses.


--

Sincerely

Andr? GILLIBERT


________________________________
De : R-devel <r-devel-bounces at r-project.org> de la part de Martin Maechler <maechler at stat.math.ethz.ch>
Envoy? : mardi 24 ao?t 2021 10:44
? : qweytr1 at mail.ustc.edu.cn
Cc : R-devel
Objet : Re: [Rd] Is it a good choice to increase the NCONNECTION value?


>>>>> qweytr1--- via R-devel
>>>>>     on Tue, 24 Aug 2021 00:51:31 +0800 (GMT+08:00) writes:

    > At least in 2015, a github user, tobigithub, submit an
    > [issue](https://github.com/sneumann/xcms/issues/20) about
    > the error "Error in file(con, "w") : all connections are
    > in use" Nowadays, since AMD have really cool CPUs which
    > increases the thread numbers to 128 or even 256 on a
    > single server, we found that the NCONNECTIONS variable
    > could prevent us from utilizing all the 128 threads.  It
    > might be a good choice to increase its value.


    > the variable is defined in
    > `R-4.1.1/src/main/connections.c: 17` I have tested that,
    > increase it to 1024 generates no error and all the
    > clusters (I tried with 256 clusters on my 16 threads
    > Laptop) works fine.

    > Is it possible increase the size of NCONNECTION?

Yes, of course, it is possible.
The question is how much it costs  and to which number it should
be increased.

A quick look at the source connections.c --> src/R_ext/include/Connections.h
reveals that the Rconnection* <--> Rconn is a struct with about
200 chars and ca 30 int-like plus another 20 pointers .. which
would amount to a rough 400 bytes per connection.
Adding 1024-128 = 896 new ones  would then amount to increase
the R executable by about 360 kB .. all the above being rough.
So personally, I guess that's  "about ok" --
are there other things to consider?

Ideally, of course, the number of possible connections could be
increased dynamically only when needed

Martin

______________________________________________
R-devel at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-devel


	[[alternative HTML version deleted]]


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Tue Aug 24 22:53:50 2021
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Tue, 24 Aug 2021 22:53:50 +0200
Subject: [Rd] Is it a good choice to increase the NCONNECTION value?
In-Reply-To: <637ad8b722e74c0b99e900acf0715efe@chu-rouen.fr>
References: <4bf09c46.181e3.17b73ec9a4f.Coremail.qweytr1@mail.ustc.edu.cn>
 <24868.45427.479782.187351@stat.math.ethz.ch>
 <637ad8b722e74c0b99e900acf0715efe@chu-rouen.fr>
Message-ID: <24869.23646.875430.624875@stat.math.ethz.ch>

>>>>> GILLIBERT, Andre 
>>>>>     on Tue, 24 Aug 2021 09:49:52 +0000 writes:

  > RConnection is a pointer to a Rconn structure. The Rconn
  > structure must be allocated independently (e.g. by
  > malloc() in R_new_custom_connection).  Therefore,
  > increasing NCONNECTION to 1024 should only use 8
  > kilobytes on 64-bits platforms and 4 kilobytes on 32
  > bits platforms.

You are right indeed, and I was wrong.

  > Ideally, it should be dynamically allocated : either as
  > a linked list or as a dynamic array
  > (malloc/realloc). However, a simple change of
  > NCONNECTION to 1024 should be enough for most uses.

There is one important other problem I've been made aware
(similarly to the number of open DLL libraries, an issue 1-2
years ago) :

The OS itself has limits on the number of open files
(yes, I know that there are other connections than files) and
these limits may quite differ from platform to platform.

On my Linux laptop, in a shell, I see

  $ ulimit -n
  1024

which is barely conformant with your proposed 1024 NCONNECTION.

Now if NCONNCECTION is larger than the max allowed number of
open files and if R opens more files than the OS allowed, the
user may get quite unpleasant behavior, e.g. R being terminated brutally
(or behaving crazily) without good R-level warning / error messages.

It's also not at all sufficient to check for the open files
limit at compile time, but rather at R process startup time 

So this may need considerably more work than you / we have
hoped, and it's probably hard to find a safe number that is
considerably larger than 128  and less than the smallest of all
non-crazy platforms' {number of open files limit}.

  > Sincerely
  > Andr? GILLIBERT

  [............]


From @|mon@urb@nek @end|ng |rom R-project@org  Wed Aug 25 02:25:47 2021
From: @|mon@urb@nek @end|ng |rom R-project@org (Simon Urbanek)
Date: Wed, 25 Aug 2021 12:25:47 +1200
Subject: [Rd] Is it a good choice to increase the NCONNECTION value?
In-Reply-To: <24869.23646.875430.624875@stat.math.ethz.ch>
References: <4bf09c46.181e3.17b73ec9a4f.Coremail.qweytr1@mail.ustc.edu.cn>
 <24868.45427.479782.187351@stat.math.ethz.ch>
 <637ad8b722e74c0b99e900acf0715efe@chu-rouen.fr>
 <24869.23646.875430.624875@stat.math.ethz.ch>
Message-ID: <BC401C19-8BF7-44DE-A3EB-E830DA5B2AEA@R-project.org>


Martin,

I don't think static connection limit is sensible. Recall that connections can be anything, not just necessarily sockets or file descriptions so they are not linked to the system fd limit. For example, if you use a codec then you will need twice the number of connections than the fds. To be honest the connection limit is one of the main reasons why in our big data applications we have always avoided R connections and used C-level sockets instead (others were lack of control over the socket flags, but that has been addressed in the last release). So I'd vote for at the very least increasing the limit significantly (at least 1k if not more) and, ideally, make it dynamic if memory footprint is an issue.

Cheers,
Simon


> On Aug 25, 2021, at 8:53 AM, Martin Maechler <maechler at stat.math.ethz.ch> wrote:
> 
>>>>>> GILLIBERT, Andre 
>>>>>>    on Tue, 24 Aug 2021 09:49:52 +0000 writes:
> 
>> RConnection is a pointer to a Rconn structure. The Rconn
>> structure must be allocated independently (e.g. by
>> malloc() in R_new_custom_connection).  Therefore,
>> increasing NCONNECTION to 1024 should only use 8
>> kilobytes on 64-bits platforms and 4 kilobytes on 32
>> bits platforms.
> 
> You are right indeed, and I was wrong.
> 
>> Ideally, it should be dynamically allocated : either as
>> a linked list or as a dynamic array
>> (malloc/realloc). However, a simple change of
>> NCONNECTION to 1024 should be enough for most uses.
> 
> There is one important other problem I've been made aware
> (similarly to the number of open DLL libraries, an issue 1-2
> years ago) :
> 
> The OS itself has limits on the number of open files
> (yes, I know that there are other connections than files) and
> these limits may quite differ from platform to platform.
> 
> On my Linux laptop, in a shell, I see
> 
>  $ ulimit -n
>  1024
> 
> which is barely conformant with your proposed 1024 NCONNECTION.
> 
> Now if NCONNCECTION is larger than the max allowed number of
> open files and if R opens more files than the OS allowed, the
> user may get quite unpleasant behavior, e.g. R being terminated brutally
> (or behaving crazily) without good R-level warning / error messages.
> 
> It's also not at all sufficient to check for the open files
> limit at compile time, but rather at R process startup time 
> 
> So this may need considerably more work than you / we have
> hoped, and it's probably hard to find a safe number that is
> considerably larger than 128  and less than the smallest of all
> non-crazy platforms' {number of open files limit}.
> 
>> Sincerely
>> Andr? GILLIBERT
> 
>  [............]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From iuke-tier@ey m@iii@g oii uiow@@edu  Wed Aug 25 03:45:08 2021
From: iuke-tier@ey m@iii@g oii uiow@@edu (iuke-tier@ey m@iii@g oii uiow@@edu)
Date: Tue, 24 Aug 2021 20:45:08 -0500 (CDT)
Subject: [Rd] 
 [External] Re: Is it a good choice to increase the NCONNECTION
 value?
In-Reply-To: <BC401C19-8BF7-44DE-A3EB-E830DA5B2AEA@R-project.org>
References: <4bf09c46.181e3.17b73ec9a4f.Coremail.qweytr1@mail.ustc.edu.cn>
 <24868.45427.479782.187351@stat.math.ethz.ch>
 <637ad8b722e74c0b99e900acf0715efe@chu-rouen.fr>
 <24869.23646.875430.624875@stat.math.ethz.ch>
 <BC401C19-8BF7-44DE-A3EB-E830DA5B2AEA@R-project.org>
Message-ID: <alpine.DEB.2.22.394.2108242023250.2943@luke-Latitude-7480>

We do need to be careful about using too many file descriptors.  The
standard soft limit on Linux is fairly low (1024; the hard limit is
usually quite a bit higher). Hitting that limit, e.g. with runaway
with code allocating lots of connections, can cause other things, like
loading packages, to fail with hard to diagnose error messages. A
static connection limit is a crude way to guard against that. Doing
anything substantially better is probably a lot of work. A simple
option that may be worth pursuing is to allow the limit to be adjusted
at runtime. Users who want to go higher would do so at their own risk
and may need to know how to adjust the soft limit on the process.

Best,

luke

On Wed, 25 Aug 2021, Simon Urbanek wrote:

>
> Martin,
>
> I don't think static connection limit is sensible. Recall that connections can be anything, not just necessarily sockets or file descriptions so they are not linked to the system fd limit. For example, if you use a codec then you will need twice the number of connections than the fds. To be honest the connection limit is one of the main reasons why in our big data applications we have always avoided R connections and used C-level sockets instead (others were lack of control over the socket flags, but that has been addressed in the last release). So I'd vote for at the very least increasing the limit significantly (at least 1k if not more) and, ideally, make it dynamic if memory footprint is an issue.
>
> Cheers,
> Simon
>
>
>> On Aug 25, 2021, at 8:53 AM, Martin Maechler <maechler at stat.math.ethz.ch> wrote:
>>
>>>>>>> GILLIBERT, Andre
>>>>>>>    on Tue, 24 Aug 2021 09:49:52 +0000 writes:
>>
>>> RConnection is a pointer to a Rconn structure. The Rconn
>>> structure must be allocated independently (e.g. by
>>> malloc() in R_new_custom_connection).  Therefore,
>>> increasing NCONNECTION to 1024 should only use 8
>>> kilobytes on 64-bits platforms and 4 kilobytes on 32
>>> bits platforms.
>>
>> You are right indeed, and I was wrong.
>>
>>> Ideally, it should be dynamically allocated : either as
>>> a linked list or as a dynamic array
>>> (malloc/realloc). However, a simple change of
>>> NCONNECTION to 1024 should be enough for most uses.
>>
>> There is one important other problem I've been made aware
>> (similarly to the number of open DLL libraries, an issue 1-2
>> years ago) :
>>
>> The OS itself has limits on the number of open files
>> (yes, I know that there are other connections than files) and
>> these limits may quite differ from platform to platform.
>>
>> On my Linux laptop, in a shell, I see
>>
>>  $ ulimit -n
>>  1024
>>
>> which is barely conformant with your proposed 1024 NCONNECTION.
>>
>> Now if NCONNCECTION is larger than the max allowed number of
>> open files and if R opens more files than the OS allowed, the
>> user may get quite unpleasant behavior, e.g. R being terminated brutally
>> (or behaving crazily) without good R-level warning / error messages.
>>
>> It's also not at all sufficient to check for the open files
>> limit at compile time, but rather at R process startup time
>>
>> So this may need considerably more work than you / we have
>> hoped, and it's probably hard to find a safe number that is
>> considerably larger than 128  and less than the smallest of all
>> non-crazy platforms' {number of open files limit}.
>>
>>> Sincerely
>>> Andr? GILLIBERT
>>
>>  [............]
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

-- 
Luke Tierney
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu

From @|mon@urb@nek @end|ng |rom R-project@org  Wed Aug 25 06:05:03 2021
From: @|mon@urb@nek @end|ng |rom R-project@org (Simon Urbanek)
Date: Wed, 25 Aug 2021 16:05:03 +1200
Subject: [Rd] 
 [External] Re: Is it a good choice to increase the NCONNECTION
 value?
In-Reply-To: <alpine.DEB.2.22.394.2108242023250.2943@luke-Latitude-7480>
References: <4bf09c46.181e3.17b73ec9a4f.Coremail.qweytr1@mail.ustc.edu.cn>
 <24868.45427.479782.187351@stat.math.ethz.ch>
 <637ad8b722e74c0b99e900acf0715efe@chu-rouen.fr>
 <24869.23646.875430.624875@stat.math.ethz.ch>
 <BC401C19-8BF7-44DE-A3EB-E830DA5B2AEA@R-project.org>
 <alpine.DEB.2.22.394.2108242023250.2943@luke-Latitude-7480>
Message-ID: <8634B658-484D-4A9B-B5DC-0C6F709E27AF@R-project.org>


Luke,

sure, adjustment at run-time works just fine, the issue currently is that it is baked-in at compile time so there is no way to adjust it (re-building R is not an option in production environment where this usually happens).

That said, I'm still not sure that connection limit is a good way to guard against the fd limit since there are so many other ways to use up descriptors (DLLs, sockets, pipes, etc. - packages and 3rd party libraries). Apparently we are actually already fiddling with the soft limit - we have R_EnsureFDLimit() and R_GetFDLimit() which is used at startup to raise it to 1024 by default regardless of the ulimit -n setting (comments say this is for DLLs). I guess based on that we know at least what to expect so we could trivially warn if the new setting is larger that the user limit.

Cheers,
Simon


> On Aug 25, 2021, at 1:45 PM, luke-tierney at uiowa.edu wrote:
> 
> We do need to be careful about using too many file descriptors.  The
> standard soft limit on Linux is fairly low (1024; the hard limit is
> usually quite a bit higher). Hitting that limit, e.g. with runaway
> with code allocating lots of connections, can cause other things, like
> loading packages, to fail with hard to diagnose error messages. A
> static connection limit is a crude way to guard against that. Doing
> anything substantially better is probably a lot of work. A simple
> option that may be worth pursuing is to allow the limit to be adjusted
> at runtime. Users who want to go higher would do so at their own risk
> and may need to know how to adjust the soft limit on the process.
> 
> Best,
> 
> luke
> 
> On Wed, 25 Aug 2021, Simon Urbanek wrote:
> 
>> 
>> Martin,
>> 
>> I don't think static connection limit is sensible. Recall that connections can be anything, not just necessarily sockets or file descriptions so they are not linked to the system fd limit. For example, if you use a codec then you will need twice the number of connections than the fds. To be honest the connection limit is one of the main reasons why in our big data applications we have always avoided R connections and used C-level sockets instead (others were lack of control over the socket flags, but that has been addressed in the last release). So I'd vote for at the very least increasing the limit significantly (at least 1k if not more) and, ideally, make it dynamic if memory footprint is an issue.
>> 
>> Cheers,
>> Simon
>> 
>> 
>>> On Aug 25, 2021, at 8:53 AM, Martin Maechler <maechler at stat.math.ethz.ch> wrote:
>>> 
>>>>>>>> GILLIBERT, Andre
>>>>>>>>   on Tue, 24 Aug 2021 09:49:52 +0000 writes:
>>> 
>>>> RConnection is a pointer to a Rconn structure. The Rconn
>>>> structure must be allocated independently (e.g. by
>>>> malloc() in R_new_custom_connection).  Therefore,
>>>> increasing NCONNECTION to 1024 should only use 8
>>>> kilobytes on 64-bits platforms and 4 kilobytes on 32
>>>> bits platforms.
>>> 
>>> You are right indeed, and I was wrong.
>>> 
>>>> Ideally, it should be dynamically allocated : either as
>>>> a linked list or as a dynamic array
>>>> (malloc/realloc). However, a simple change of
>>>> NCONNECTION to 1024 should be enough for most uses.
>>> 
>>> There is one important other problem I've been made aware
>>> (similarly to the number of open DLL libraries, an issue 1-2
>>> years ago) :
>>> 
>>> The OS itself has limits on the number of open files
>>> (yes, I know that there are other connections than files) and
>>> these limits may quite differ from platform to platform.
>>> 
>>> On my Linux laptop, in a shell, I see
>>> 
>>> $ ulimit -n
>>> 1024
>>> 
>>> which is barely conformant with your proposed 1024 NCONNECTION.
>>> 
>>> Now if NCONNCECTION is larger than the max allowed number of
>>> open files and if R opens more files than the OS allowed, the
>>> user may get quite unpleasant behavior, e.g. R being terminated brutally
>>> (or behaving crazily) without good R-level warning / error messages.
>>> 
>>> It's also not at all sufficient to check for the open files
>>> limit at compile time, but rather at R process startup time
>>> 
>>> So this may need considerably more work than you / we have
>>> hoped, and it's probably hard to find a safe number that is
>>> considerably larger than 128  and less than the smallest of all
>>> non-crazy platforms' {number of open files limit}.
>>> 
>>>> Sincerely
>>>> Andr? GILLIBERT
>>> 
>>> [............]
>>> 
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>> 
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>> 
> 
> -- 
> Luke Tierney
> Ralph E. Wareham Professor of Mathematical Sciences
> University of Iowa                  Phone:             319-335-3386
> Department of Statistics and        Fax:               319-335-3017
>   Actuarial Science
> 241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
> Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu


From tom@@@k@||ber@ @end|ng |rom gm@||@com  Wed Aug 25 09:02:49 2021
From: tom@@@k@||ber@ @end|ng |rom gm@||@com (Tomas Kalibera)
Date: Wed, 25 Aug 2021 09:02:49 +0200
Subject: [Rd] 
 [External] Re: Is it a good choice to increase the NCONNECTION
 value?
In-Reply-To: <8634B658-484D-4A9B-B5DC-0C6F709E27AF@R-project.org>
References: <4bf09c46.181e3.17b73ec9a4f.Coremail.qweytr1@mail.ustc.edu.cn>
 <24868.45427.479782.187351@stat.math.ethz.ch>
 <637ad8b722e74c0b99e900acf0715efe@chu-rouen.fr>
 <24869.23646.875430.624875@stat.math.ethz.ch>
 <BC401C19-8BF7-44DE-A3EB-E830DA5B2AEA@R-project.org>
 <alpine.DEB.2.22.394.2108242023250.2943@luke-Latitude-7480>
 <8634B658-484D-4A9B-B5DC-0C6F709E27AF@R-project.org>
Message-ID: <616b7db0-66ad-152b-3239-948165e7b87a@gmail.com>


On 8/25/21 6:05 AM, Simon Urbanek wrote:
> Luke,
>
> sure, adjustment at run-time works just fine, the issue currently is that it is baked-in at compile time so there is no way to adjust it (re-building R is not an option in production environment where this usually happens).
>
> That said, I'm still not sure that connection limit is a good way to guard against the fd limit since there are so many other ways to use up descriptors (DLLs, sockets, pipes, etc. - packages and 3rd party libraries). Apparently we are actually already fiddling with the soft limit - we have R_EnsureFDLimit() and R_GetFDLimit() which is used at startup to raise it to 1024 by default regardless of the ulimit -n setting (comments say this is for DLLs). I guess based on that we know at least what to expect so we could trivially warn if the new setting is larger that the user limit.

Hi Simon,

I think the handling of the OS connections limit (querying, increasing, 
basing the real DLL limit on that and on a user request), which takes 
into account problems described by Martin and Luke, could be extended to 
cover the connections limit in question now. The DLL limit heuristics 
were chosen based on our R hard-limit on the number of connections.

Some background is in 
https://developer.r-project.org/Blog/public/2018/03/23/maximum-number-of-dlls/index.html

If it turns out too much work for the near future/next release (it will 
be a lot of work to get it right, including the heuristics and their 
interactions between the connections limit and the DLL limit), we could 
at least (perhaps temporarily) allow users who explicitly want to 
override and take the risk to do so, perhaps with some warnings when the 
overridden value seems too large given the OS-limit and the DLL-limit.

Cheers,
Tomas
>
> Cheers,
> Simon
>
>
>> On Aug 25, 2021, at 1:45 PM, luke-tierney at uiowa.edu wrote:
>>
>> We do need to be careful about using too many file descriptors.  The
>> standard soft limit on Linux is fairly low (1024; the hard limit is
>> usually quite a bit higher). Hitting that limit, e.g. with runaway
>> with code allocating lots of connections, can cause other things, like
>> loading packages, to fail with hard to diagnose error messages. A
>> static connection limit is a crude way to guard against that. Doing
>> anything substantially better is probably a lot of work. A simple
>> option that may be worth pursuing is to allow the limit to be adjusted
>> at runtime. Users who want to go higher would do so at their own risk
>> and may need to know how to adjust the soft limit on the process.
>>
>> Best,
>>
>> luke
>>
>> On Wed, 25 Aug 2021, Simon Urbanek wrote:
>>
>>> Martin,
>>>
>>> I don't think static connection limit is sensible. Recall that connections can be anything, not just necessarily sockets or file descriptions so they are not linked to the system fd limit. For example, if you use a codec then you will need twice the number of connections than the fds. To be honest the connection limit is one of the main reasons why in our big data applications we have always avoided R connections and used C-level sockets instead (others were lack of control over the socket flags, but that has been addressed in the last release). So I'd vote for at the very least increasing the limit significantly (at least 1k if not more) and, ideally, make it dynamic if memory footprint is an issue.
>>>
>>> Cheers,
>>> Simon
>>>
>>>
>>>> On Aug 25, 2021, at 8:53 AM, Martin Maechler <maechler at stat.math.ethz.ch> wrote:
>>>>
>>>>>>>>> GILLIBERT, Andre
>>>>>>>>>    on Tue, 24 Aug 2021 09:49:52 +0000 writes:
>>>>> RConnection is a pointer to a Rconn structure. The Rconn
>>>>> structure must be allocated independently (e.g. by
>>>>> malloc() in R_new_custom_connection).  Therefore,
>>>>> increasing NCONNECTION to 1024 should only use 8
>>>>> kilobytes on 64-bits platforms and 4 kilobytes on 32
>>>>> bits platforms.
>>>> You are right indeed, and I was wrong.
>>>>
>>>>> Ideally, it should be dynamically allocated : either as
>>>>> a linked list or as a dynamic array
>>>>> (malloc/realloc). However, a simple change of
>>>>> NCONNECTION to 1024 should be enough for most uses.
>>>> There is one important other problem I've been made aware
>>>> (similarly to the number of open DLL libraries, an issue 1-2
>>>> years ago) :
>>>>
>>>> The OS itself has limits on the number of open files
>>>> (yes, I know that there are other connections than files) and
>>>> these limits may quite differ from platform to platform.
>>>>
>>>> On my Linux laptop, in a shell, I see
>>>>
>>>> $ ulimit -n
>>>> 1024
>>>>
>>>> which is barely conformant with your proposed 1024 NCONNECTION.
>>>>
>>>> Now if NCONNCECTION is larger than the max allowed number of
>>>> open files and if R opens more files than the OS allowed, the
>>>> user may get quite unpleasant behavior, e.g. R being terminated brutally
>>>> (or behaving crazily) without good R-level warning / error messages.
>>>>
>>>> It's also not at all sufficient to check for the open files
>>>> limit at compile time, but rather at R process startup time
>>>>
>>>> So this may need considerably more work than you / we have
>>>> hoped, and it's probably hard to find a safe number that is
>>>> considerably larger than 128  and less than the smallest of all
>>>> non-crazy platforms' {number of open files limit}.
>>>>
>>>>> Sincerely
>>>>> Andr? GILLIBERT
>>>> [............]
>>>>
>>>> ______________________________________________
>>>> R-devel at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>
>> -- 
>> Luke Tierney
>> Ralph E. Wareham Professor of Mathematical Sciences
>> University of Iowa                  Phone:             319-335-3386
>> Department of Statistics and        Fax:               319-335-3017
>>    Actuarial Science
>> 241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
>> Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From Andre@G||||bert @end|ng |rom chu-rouen@|r  Wed Aug 25 23:00:56 2021
From: Andre@G||||bert @end|ng |rom chu-rouen@|r (GILLIBERT, Andre)
Date: Wed, 25 Aug 2021 21:00:56 +0000
Subject: [Rd] Is it a good choice to increase the NCONNECTION value?
In-Reply-To: <4bf08fc5.1aaf1.17b7b859ffd.Coremail.qweytr1@mail.ustc.edu.cn>
References: <4bf09c46.181e3.17b73ec9a4f.Coremail.qweytr1@mail.ustc.edu.cn>
 <24868.45427.479782.187351@stat.math.ethz.ch>
 <637ad8b722e74c0b99e900acf0715efe@chu-rouen.fr>
 <24869.23646.875430.624875@stat.math.ethz.ch>
 <BC401C19-8BF7-44DE-A3EB-E830DA5B2AEA@R-project.org>,
 <4bf08fc5.1aaf1.17b7b859ffd.Coremail.qweytr1@mail.ustc.edu.cn>
Message-ID: <aaa6df034a0a497eb4285a2aebdd9f9e@chu-rouen.fr>

Hello,


The soft limit to the number of file descriptors is 1024 on GNU/Linux but the default hard limit is at 1048576 or 524288 on major modern distributions : Ubuntu, Fedora, Debian.

I do not have access to a Macintosh, but it looks like the soft limit is 256 and hard limit is "unlimited", though actually, the real hard limit has been reported as 10240 (https://developer.r-project.org/Blog/public/2018/03/23/maximum-number-of-dlls/index.html).


Therefore, R should easily be able to change the limit without superuser privileges, with a call to setrlimit().

This should make file descriptor exhaustion very unlikely, except for buggy programs leaking file descriptors.


The simplest approach would be to set the soft limit to the value of the hard limit. Maybe to be nicer, R could set it to 10000 (or the hard limit if lower), which should be enough for intensive uses but would not use too much system resources in case of file descriptor leaks.


To get R reliably work in more esoteric operating systems or in poorly configured systems (e.g. systems with a hard limit at 1024), a second security could be added: a request of a new connection would be denied if the actual number of open file descriptors (or connections if that is easier to compute) is too close to the hard limit. A fixed amount (e.g. 128) or a proportion (e.g. 25%) of file descriptors would be reserved for "other uses", such as shared libraries.


This discussion reminds me of the fixed number of file descriptors of MS-DOS, defined at boot time in config.sys (e.g. files=20).

This is incredible that 64 bits computers in 2021 with gigabytes of RAM still have similar limits, and that R, has a hard-coded limit at 128.


--

Sincerely

Andr? GILLIBERT

________________________________
De : qweytr1 at mail.ustc.edu.cn <qweytr1 at mail.ustc.edu.cn>
Envoy? : mercredi 25 ao?t 2021 06:15:59
? : Simon Urbanek
Cc : Martin Maechler; GILLIBERT, Andre; R-devel
Objet : ??: [SPAM] Re: [Rd] Is it a good choice to increase the NCONNECTION value?

ATTENTION: Cet e-mail provient d?une adresse mail ext?rieure au CHU de Rouen. Ne cliquez pas sur les liens ou n'ouvrez pas les pi?ces jointes ? moins de conna?tre l'exp?diteur et de savoir que le contenu est s?r. En cas de doute, transf?rer le mail ? ? DSI, S?curit? ? pour analyse. Merci de votre vigilance


Simon,

What about using a dynamically allocated connections and a modifiable MAX_NCONNECTIONS limit?
ulimit could be modified by root users, at least now NCONNECTION could not.

I tried changing the program using malloc and realloc to allocate memory, due to unfamiliar with `.Internal` calls, I could not provide a function that modify the MAX_NCONNECTIONS (but it is possible.)
test and changes are shown below. I'll be appperciate if you could tell me whether there could be a bug.

(a demo that may change MAX_NCONNECTIONS, not tested.)
static int SetMaxNconnections(int now){ // return current value of MAX_NCONNECTIONS
  if(now<3)error(_("Could not shrink the MAX_NCONNECTIONS less than 3"));
  if(now>65536)warning(_("Setting MAX_NCONNECTIONS=%d, larger than 65536, may be crazy. Use at your own risk."),now);
  // setting MAX_NCONNECTIONS to a really large value is safe, since the allocation is not done immediately. Thus this is a warning.
  if(now>=NCONNECTIONS)return MAX_NCONNECTIONS=now; // if now is larger than NCONNECTIONS<=now,MAX_NCONNECTIONS, thus it is safe.
  R_gc(); /* Try to reclaim unused connections */
  for(int i=NCONNECTIONS;i>=now;--i){// now >= 3 here, thus no underflow occurs.
    // shrink the value of MAX_NCONNECTIONS and NCONNECTIONS
    if(!Connections[i]){now=i+1;break;}
  }
  // here, we could call a realloc, since *Connections only capture several kilobytes, realloc seems meaningless.
  // a true realloc will trigger if NCONNECTIONS<MAX_NCONNECTIONS and call NextConnection with all connections are in use
  return MAX_NCONNECTIONS=NCONNECTIONS=now;
}



test result:

$ LC_ALL=C R-4.1.1/bin/R -q -e 'library(doParallel);cl=makeForkCluster(128);max(sapply(clusterCall(cl,function()runif(10)),"+"))'
WARNING: ignoring environment value of R_HOME
> library(doParallel);cl=makeForkCluster(128);max(sapply(clusterCall(cl,function()runif(10)),"+"))
Loading required package: foreach
Loading required package: iterators
Loading required package: parallel
Warning messages:
1: In socketAccept(socket = socket, blocking = TRUE, open = "a+b",  :
  increase max connections from 16 to 32
2: In socketAccept(socket = socket, blocking = TRUE, open = "a+b",  :
  increase max connections from 32 to 64
3: In socketAccept(socket = socket, blocking = TRUE, open = "a+b",  :
  increase max connections from 64 to 128
4: In socketAccept(socket = socket, blocking = TRUE, open = "a+b",  :
  increase max connections from 128 to 256
[1] 0.9975836
>
>


tested changes:


~line 127

static int NCONNECTIONS=16; /* need one per cluster node, 16 is the
  initial value which grows dynamically */
static int MAX_NCONNECTIONS=8192; /* increase it only affect the speed of
  finding the correct connection, if you have a machine with more than
  4096 threads, you could submit an issue or modify this value manually */
#define NSINKS 21

static Rconnection *Connections=NULL; /* we will allocate it later */
...

~line 146



static int NextConnection(void)
{
    int i;
    for(i = 3; i < NCONNECTIONS; i++)
    if(!Connections[i]) break;
    if(i >= NCONNECTIONS) {
    R_gc(); /* Try to reclaim unused ones */
    for(i = 3; i < NCONNECTIONS; i++)
        if(!Connections[i]) break;
    if(i >= NCONNECTIONS) {
        if(i >= MAX_NCONNECTIONS)
        error(_("all connections are in use"));
        int new_connections=NCONNECTIONS*2;//try dynamic alloc
        if(new_connections > MAX_NCONNECTIONS)
        new_connections = MAX_NCONNECTIONS;
        Rconnection*ptr = realloc(Connections,new_connections*sizeof(Rconnection));
        if (ptr==NULL)
        error(_("alloc extra connections failed"));
        warning(_("increase max connections from %d to %d\n"),NCONNECTIONS,new_connections);
        Connections = ptr;
        NCONNECTIONS = new_connections;
        for(int j = i; j < NCONNECTIONS; j++) Connections[j] = NULL;
    }
    }
    return i;
}
...



~line 5265

void attribute_hidden InitConnections()
{
    int i;
    Connections=malloc(NCONNECTIONS*sizeof(Rconnection));
    if(Connections == NULL) {
    error(_("Cannot alloc connections."));
    abort();
    }
...


> -----????-----
> ???: "Simon Urbanek" <simon.urbanek at R-project.org>
> ????: 2021-08-25 08:25:47 (???)
> ???: "Martin Maechler" <maechler at stat.math.ethz.ch>
> ??: "GILLIBERT, Andre" <Andre.Gillibert at chu-rouen.fr>, "qweytr1 at mail.ustc.edu.cn" <qweytr1 at mail.ustc.edu.cn>, R-devel <R-devel at r-project.org>
> ??: [SPAM] Re: [Rd] Is it a good choice to increase the NCONNECTION value?
>
> Martin,
>
> I don't think static connection limit is sensible. Recall that connections can be anything, not just necessarily sockets or file descriptions so they are not linked to the system fd limit. For example, if you use a codec then you will need twice the number of connections than the fds. To be honest the connection limit is one of the main reasons why in our big data applications we have always avoided R connections and used C-level sockets instead (others were lack of control over the socket flags, but that has been addressed in the last release). So I'd vote for at the very least increasing the limit significantly (at least 1k if not more) and, ideally, make it dynamic if memory footprint is an issue.
>
> Cheers,
> Simon
>
>
> > On Aug 25, 2021, at 8:53 AM, Martin Maechler <maechler at stat.math.ethz.ch> wrote:
> >
> >>>>>> GILLIBERT, Andre
> >>>>>>    on Tue, 24 Aug 2021 09:49:52 +0000 writes:
> >
> >> RConnection is a pointer to a Rconn structure. The Rconn
> >> structure must be allocated independently (e.g. by
> >> malloc() in R_new_custom_connection).  Therefore,
> >> increasing NCONNECTION to 1024 should only use 8
> >> kilobytes on 64-bits platforms and 4 kilobytes on 32
> >> bits platforms.
> >
> > You are right indeed, and I was wrong.
> >
> >> Ideally, it should be dynamically allocated : either as
> >> a linked list or as a dynamic array
> >> (malloc/realloc). However, a simple change of
> >> NCONNECTION to 1024 should be enough for most uses.
> >
> > There is one important other problem I've been made aware
> > (similarly to the number of open DLL libraries, an issue 1-2
> > years ago) :
> >
> > The OS itself has limits on the number of open files
> > (yes, I know that there are other connections than files) and
> > these limits may quite differ from platform to platform.
> >
> > On my Linux laptop, in a shell, I see
> >
> >  $ ulimit -n
> >  1024
> >
> > which is barely conformant with your proposed 1024 NCONNECTION.
> >
> > Now if NCONNCECTION is larger than the max allowed number of
> > open files and if R opens more files than the OS allowed, the
> > user may get quite unpleasant behavior, e.g. R being terminated brutally
> > (or behaving crazily) without good R-level warning / error messages.
> >
> > It's also not at all sufficient to check for the open files
> > limit at compile time, but rather at R process startup time
> >
> > So this may need considerably more work than you / we have
> > hoped, and it's probably hard to find a safe number that is
> > considerably larger than 128  and less than the smallest of all
> > non-crazy platforms' {number of open files limit}.
> >
> >> Sincerely
> >> Andr? GILLIBERT
> >
> >  [............]
> >
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From @|mon@urb@nek @end|ng |rom R-project@org  Thu Aug 26 01:27:51 2021
From: @|mon@urb@nek @end|ng |rom R-project@org (Simon Urbanek)
Date: Thu, 26 Aug 2021 11:27:51 +1200
Subject: [Rd] Is it a good choice to increase the NCONNECTION value?
In-Reply-To: <aaa6df034a0a497eb4285a2aebdd9f9e@chu-rouen.fr>
References: <4bf09c46.181e3.17b73ec9a4f.Coremail.qweytr1@mail.ustc.edu.cn>
 <24868.45427.479782.187351@stat.math.ethz.ch>
 <637ad8b722e74c0b99e900acf0715efe@chu-rouen.fr>
 <24869.23646.875430.624875@stat.math.ethz.ch>
 <BC401C19-8BF7-44DE-A3EB-E830DA5B2AEA@R-project.org>
 <4bf08fc5.1aaf1.17b7b859ffd.Coremail.qweytr1@mail.ustc.edu.cn>
 <aaa6df034a0a497eb4285a2aebdd9f9e@chu-rouen.fr>
Message-ID: <C18B4E5D-55D0-4A10-A536-11B3953BD11A@R-project.org>


Andre,

as stated earlier, R already uses setrlimit() to raise the limit (see my earlier reply).

As for "special" connections, that is not feasible (without some serious re-write), since the connection doesn't know what it is used for and connections are not the only way descriptors may be used.

Anyway, I think the take away was that likely the best way forward is to make it configurable at startup time with possible option to check that value against the feasibility of open connections.

Cheers,
Simon



> On Aug 26, 2021, at 9:00 AM, GILLIBERT, Andre <Andre.Gillibert at chu-rouen.fr> wrote:
> 
> Hello,
> 
> 
> The soft limit to the number of file descriptors is 1024 on GNU/Linux but the default hard limit is at 1048576 or 524288 on major modern distributions : Ubuntu, Fedora, Debian.
> 
> I do not have access to a Macintosh, but it looks like the soft limit is 256 and hard limit is "unlimited", though actually, the real hard limit has been reported as 10240 (https://developer.r-project.org/Blog/public/2018/03/23/maximum-number-of-dlls/index.html).
> 
> 
> Therefore, R should easily be able to change the limit without superuser privileges, with a call to setrlimit().
> 
> This should make file descriptor exhaustion very unlikely, except for buggy programs leaking file descriptors.
> 
> 
> The simplest approach would be to set the soft limit to the value of the hard limit. Maybe to be nicer, R could set it to 10000 (or the hard limit if lower), which should be enough for intensive uses but would not use too much system resources in case of file descriptor leaks.
> 
> 
> To get R reliably work in more esoteric operating systems or in poorly configured systems (e.g. systems with a hard limit at 1024), a second security could be added: a request of a new connection would be denied if the actual number of open file descriptors (or connections if that is easier to compute) is too close to the hard limit. A fixed amount (e.g. 128) or a proportion (e.g. 25%) of file descriptors would be reserved for "other uses", such as shared libraries.
> 
> 
> This discussion reminds me of the fixed number of file descriptors of MS-DOS, defined at boot time in config.sys (e.g. files=20).
> 
> This is incredible that 64 bits computers in 2021 with gigabytes of RAM still have similar limits, and that R, has a hard-coded limit at 128.
> 
> 
> --
> 
> Sincerely
> 
> Andr? GILLIBERT
> 
> ________________________________
> De : qweytr1 at mail.ustc.edu.cn <qweytr1 at mail.ustc.edu.cn>
> Envoy? : mercredi 25 ao?t 2021 06:15:59
> ? : Simon Urbanek
> Cc : Martin Maechler; GILLIBERT, Andre; R-devel
> Objet : ??: [SPAM] Re: [Rd] Is it a good choice to increase the NCONNECTION value?
> 
> ATTENTION: Cet e-mail provient d?une adresse mail ext?rieure au CHU de Rouen. Ne cliquez pas sur les liens ou n'ouvrez pas les pi?ces jointes ? moins de conna?tre l'exp?diteur et de savoir que le contenu est s?r. En cas de doute, transf?rer le mail ? ? DSI, S?curit? ? pour analyse. Merci de votre vigilance
> 
> 
> Simon,
> 
> What about using a dynamically allocated connections and a modifiable MAX_NCONNECTIONS limit?
> ulimit could be modified by root users, at least now NCONNECTION could not.
> 
> I tried changing the program using malloc and realloc to allocate memory, due to unfamiliar with `.Internal` calls, I could not provide a function that modify the MAX_NCONNECTIONS (but it is possible.)
> test and changes are shown below. I'll be appperciate if you could tell me whether there could be a bug.
> 
> (a demo that may change MAX_NCONNECTIONS, not tested.)
> static int SetMaxNconnections(int now){ // return current value of MAX_NCONNECTIONS
>  if(now<3)error(_("Could not shrink the MAX_NCONNECTIONS less than 3"));
>  if(now>65536)warning(_("Setting MAX_NCONNECTIONS=%d, larger than 65536, may be crazy. Use at your own risk."),now);
>  // setting MAX_NCONNECTIONS to a really large value is safe, since the allocation is not done immediately. Thus this is a warning.
>  if(now>=NCONNECTIONS)return MAX_NCONNECTIONS=now; // if now is larger than NCONNECTIONS<=now,MAX_NCONNECTIONS, thus it is safe.
>  R_gc(); /* Try to reclaim unused connections */
>  for(int i=NCONNECTIONS;i>=now;--i){// now >= 3 here, thus no underflow occurs.
>    // shrink the value of MAX_NCONNECTIONS and NCONNECTIONS
>    if(!Connections[i]){now=i+1;break;}
>  }
>  // here, we could call a realloc, since *Connections only capture several kilobytes, realloc seems meaningless.
>  // a true realloc will trigger if NCONNECTIONS<MAX_NCONNECTIONS and call NextConnection with all connections are in use
>  return MAX_NCONNECTIONS=NCONNECTIONS=now;
> }
> 
> 
> 
> test result:
> 
> $ LC_ALL=C R-4.1.1/bin/R -q -e 'library(doParallel);cl=makeForkCluster(128);max(sapply(clusterCall(cl,function()runif(10)),"+"))'
> WARNING: ignoring environment value of R_HOME
>> library(doParallel);cl=makeForkCluster(128);max(sapply(clusterCall(cl,function()runif(10)),"+"))
> Loading required package: foreach
> Loading required package: iterators
> Loading required package: parallel
> Warning messages:
> 1: In socketAccept(socket = socket, blocking = TRUE, open = "a+b",  :
>  increase max connections from 16 to 32
> 2: In socketAccept(socket = socket, blocking = TRUE, open = "a+b",  :
>  increase max connections from 32 to 64
> 3: In socketAccept(socket = socket, blocking = TRUE, open = "a+b",  :
>  increase max connections from 64 to 128
> 4: In socketAccept(socket = socket, blocking = TRUE, open = "a+b",  :
>  increase max connections from 128 to 256
> [1] 0.9975836
>> 
>> 
> 
> 
> tested changes:
> 
> 
> ~line 127
> 
> static int NCONNECTIONS=16; /* need one per cluster node, 16 is the
>  initial value which grows dynamically */
> static int MAX_NCONNECTIONS=8192; /* increase it only affect the speed of
>  finding the correct connection, if you have a machine with more than
>  4096 threads, you could submit an issue or modify this value manually */
> #define NSINKS 21
> 
> static Rconnection *Connections=NULL; /* we will allocate it later */
> ...
> 
> ~line 146
> 
> 
> 
> static int NextConnection(void)
> {
>    int i;
>    for(i = 3; i < NCONNECTIONS; i++)
>    if(!Connections[i]) break;
>    if(i >= NCONNECTIONS) {
>    R_gc(); /* Try to reclaim unused ones */
>    for(i = 3; i < NCONNECTIONS; i++)
>        if(!Connections[i]) break;
>    if(i >= NCONNECTIONS) {
>        if(i >= MAX_NCONNECTIONS)
>        error(_("all connections are in use"));
>        int new_connections=NCONNECTIONS*2;//try dynamic alloc
>        if(new_connections > MAX_NCONNECTIONS)
>        new_connections = MAX_NCONNECTIONS;
>        Rconnection*ptr = realloc(Connections,new_connections*sizeof(Rconnection));
>        if (ptr==NULL)
>        error(_("alloc extra connections failed"));
>        warning(_("increase max connections from %d to %d\n"),NCONNECTIONS,new_connections);
>        Connections = ptr;
>        NCONNECTIONS = new_connections;
>        for(int j = i; j < NCONNECTIONS; j++) Connections[j] = NULL;
>    }
>    }
>    return i;
> }
> ...
> 
> 
> 
> ~line 5265
> 
> void attribute_hidden InitConnections()
> {
>    int i;
>    Connections=malloc(NCONNECTIONS*sizeof(Rconnection));
>    if(Connections == NULL) {
>    error(_("Cannot alloc connections."));
>    abort();
>    }
> ...
> 
> 
>> -----????-----
>> ???: "Simon Urbanek" <simon.urbanek at R-project.org>
>> ????: 2021-08-25 08:25:47 (???)
>> ???: "Martin Maechler" <maechler at stat.math.ethz.ch>
>> ??: "GILLIBERT, Andre" <Andre.Gillibert at chu-rouen.fr>, "qweytr1 at mail.ustc.edu.cn" <qweytr1 at mail.ustc.edu.cn>, R-devel <R-devel at r-project.org>
>> ??: [SPAM] Re: [Rd] Is it a good choice to increase the NCONNECTION value?
>> 
>> Martin,
>> 
>> I don't think static connection limit is sensible. Recall that connections can be anything, not just necessarily sockets or file descriptions so they are not linked to the system fd limit. For example, if you use a codec then you will need twice the number of connections than the fds. To be honest the connection limit is one of the main reasons why in our big data applications we have always avoided R connections and used C-level sockets instead (others were lack of control over the socket flags, but that has been addressed in the last release). So I'd vote for at the very least increasing the limit significantly (at least 1k if not more) and, ideally, make it dynamic if memory footprint is an issue.
>> 
>> Cheers,
>> Simon
>> 
>> 
>>> On Aug 25, 2021, at 8:53 AM, Martin Maechler <maechler at stat.math.ethz.ch> wrote:
>>> 
>>>>>>>> GILLIBERT, Andre
>>>>>>>>   on Tue, 24 Aug 2021 09:49:52 +0000 writes:
>>> 
>>>> RConnection is a pointer to a Rconn structure. The Rconn
>>>> structure must be allocated independently (e.g. by
>>>> malloc() in R_new_custom_connection).  Therefore,
>>>> increasing NCONNECTION to 1024 should only use 8
>>>> kilobytes on 64-bits platforms and 4 kilobytes on 32
>>>> bits platforms.
>>> 
>>> You are right indeed, and I was wrong.
>>> 
>>>> Ideally, it should be dynamically allocated : either as
>>>> a linked list or as a dynamic array
>>>> (malloc/realloc). However, a simple change of
>>>> NCONNECTION to 1024 should be enough for most uses.
>>> 
>>> There is one important other problem I've been made aware
>>> (similarly to the number of open DLL libraries, an issue 1-2
>>> years ago) :
>>> 
>>> The OS itself has limits on the number of open files
>>> (yes, I know that there are other connections than files) and
>>> these limits may quite differ from platform to platform.
>>> 
>>> On my Linux laptop, in a shell, I see
>>> 
>>> $ ulimit -n
>>> 1024
>>> 
>>> which is barely conformant with your proposed 1024 NCONNECTION.
>>> 
>>> Now if NCONNCECTION is larger than the max allowed number of
>>> open files and if R opens more files than the OS allowed, the
>>> user may get quite unpleasant behavior, e.g. R being terminated brutally
>>> (or behaving crazily) without good R-level warning / error messages.
>>> 
>>> It's also not at all sufficient to check for the open files
>>> limit at compile time, but rather at R process startup time
>>> 
>>> So this may need considerably more work than you / we have
>>> hoped, and it's probably hard to find a safe number that is
>>> considerably larger than 128  and less than the smallest of all
>>> non-crazy platforms' {number of open files limit}.
>>> 
>>>> Sincerely
>>>> Andr? GILLIBERT
>>> 
>>> [............]
>>> 
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From Andre@G||||bert @end|ng |rom chu-rouen@|r  Thu Aug 26 19:31:56 2021
From: Andre@G||||bert @end|ng |rom chu-rouen@|r (GILLIBERT, Andre)
Date: Thu, 26 Aug 2021 17:31:56 +0000
Subject: [Rd] Is it a good choice to increase the NCONNECTION value?
In-Reply-To: <C18B4E5D-55D0-4A10-A536-11B3953BD11A@R-project.org>
References: <4bf09c46.181e3.17b73ec9a4f.Coremail.qweytr1@mail.ustc.edu.cn>
 <24868.45427.479782.187351@stat.math.ethz.ch>
 <637ad8b722e74c0b99e900acf0715efe@chu-rouen.fr>
 <24869.23646.875430.624875@stat.math.ethz.ch>
 <BC401C19-8BF7-44DE-A3EB-E830DA5B2AEA@R-project.org>
 <4bf08fc5.1aaf1.17b7b859ffd.Coremail.qweytr1@mail.ustc.edu.cn>
 <aaa6df034a0a497eb4285a2aebdd9f9e@chu-rouen.fr>,
 <C18B4E5D-55D0-4A10-A536-11B3953BD11A@R-project.org>
Message-ID: <410fa631ffee4a869e19f8a5b494d4f1@chu-rouen.fr>


> as stated earlier, R already uses setrlimit() to raise the limit (see my earlier reply).


Currently, setrlimit() is called by R_EnsureFDLimit() and the latter is called by initLoadedDLL() in Rdynload.c depending on the R_MAX_NUM_DLLS environment variable.

R_MAX_NUM_DLLS can be at most 1000 and the ulimit is raised to ceil(R_MAX_NUM_DLLS/0.6), which is at most 1667.

That seems pretty low to me.


I would not mind calling R_EnsureFDLimit(10000) unconditionnally. Ten thousand file descriptors should be "cheap" enough in system resources on any modern system (probably less than hundred megabytes even if kernel structures are large), and should be enough for moderate-to-intensive uses.

> As for "special" connections, that is not feasible (without some serious re-write), since the connection doesn't know what it is used for and connections are not the only

> way descriptors may be used.


I was thinking about something like /proc/$PID/fd on Linux to enumerate file descriptors of the current process, independently of who created them.

However, I did not find a fast and portable way of doing that. If thousands of file descriptors are open, we cannot afford to do thousands of system calls every time a new connection is created.


Anyway, I wrote and tested a patch with the following features:

1) Dynamic allocation of the Connections array with a MAX_NCONNECTIONS limit

2) MAX_NCONNECTIONS defaults to 8192

3) MAX_NCONNECTIONS can be set at startup by an environment variable R_MAX_NCONNECTIONS

4) MAX_NCONNECTIONS can be read and changed at run time by the options("max.n.connections")

5) R_EnsureFDLimit(10000) is called unconditionnally at startup


--

Sincerely

Andr? GILLIBERT

________________________________
De : Simon Urbanek <simon.urbanek at R-project.org>
Envoy? : jeudi 26 ao?t 2021 01:27:51
? : GILLIBERT, Andre
Cc : qweytr1 at mail.ustc.edu.cn; R-devel; Martin Maechler
Objet : Re: [Rd] Is it a good choice to increase the NCONNECTION value?

ATTENTION: Cet e-mail provient d?une adresse mail ext?rieure au CHU de Rouen. Ne cliquez pas sur les liens ou n'ouvrez pas les pi?ces jointes ? moins de conna?tre l'exp?diteur et de savoir que le contenu est s?r. En cas de doute, transf?rer le mail ? ? DSI, S?curit? ? pour analyse. Merci de votre vigilance


Andre,

as stated earlier, R already uses setrlimit() to raise the limit (see my earlier reply).

As for "special" connections, that is not feasible (without some serious re-write), since the connection doesn't know what it is used for and connections are not the only way descriptors may be used.

Anyway, I think the take away was that likely the best way forward is to make it configurable at startup time with possible option to check that value against the feasibility of open connections.

Cheers,
Simon



> On Aug 26, 2021, at 9:00 AM, GILLIBERT, Andre <Andre.Gillibert at chu-rouen.fr> wrote:
>
> Hello,
>
>
> The soft limit to the number of file descriptors is 1024 on GNU/Linux but the default hard limit is at 1048576 or 524288 on major modern distributions : Ubuntu, Fedora, Debian.
>
> I do not have access to a Macintosh, but it looks like the soft limit is 256 and hard limit is "unlimited", though actually, the real hard limit has been reported as 10240 (https://developer.r-project.org/Blog/public/2018/03/23/maximum-number-of-dlls/index.html).
>
>
> Therefore, R should easily be able to change the limit without superuser privileges, with a call to setrlimit().
>
> This should make file descriptor exhaustion very unlikely, except for buggy programs leaking file descriptors.
>
>
> The simplest approach would be to set the soft limit to the value of the hard limit. Maybe to be nicer, R could set it to 10000 (or the hard limit if lower), which should be enough for intensive uses but would not use too much system resources in case of file descriptor leaks.
>
>
> To get R reliably work in more esoteric operating systems or in poorly configured systems (e.g. systems with a hard limit at 1024), a second security could be added: a request of a new connection would be denied if the actual number of open file descriptors (or connections if that is easier to compute) is too close to the hard limit. A fixed amount (e.g. 128) or a proportion (e.g. 25%) of file descriptors would be reserved for "other uses", such as shared libraries.
>
>
> This discussion reminds me of the fixed number of file descriptors of MS-DOS, defined at boot time in config.sys (e.g. files=20).
>
> This is incredible that 64 bits computers in 2021 with gigabytes of RAM still have similar limits, and that R, has a hard-coded limit at 128.
>
>
> --
>
> Sincerely
>
> Andr? GILLIBERT
>
> ________________________________
> De : qweytr1 at mail.ustc.edu.cn <qweytr1 at mail.ustc.edu.cn>
> Envoy? : mercredi 25 ao?t 2021 06:15:59
> ? : Simon Urbanek
> Cc : Martin Maechler; GILLIBERT, Andre; R-devel
> Objet : ??: [SPAM] Re: [Rd] Is it a good choice to increase the NCONNECTION value?
>
> ATTENTION: Cet e-mail provient d?une adresse mail ext?rieure au CHU de Rouen. Ne cliquez pas sur les liens ou n'ouvrez pas les pi?ces jointes ? moins de conna?tre l'exp?diteur et de savoir que le contenu est s?r. En cas de doute, transf?rer le mail ? ? DSI, S?curit? ? pour analyse. Merci de votre vigilance
>
>
> Simon,
>
> What about using a dynamically allocated connections and a modifiable MAX_NCONNECTIONS limit?
> ulimit could be modified by root users, at least now NCONNECTION could not.
>
> I tried changing the program using malloc and realloc to allocate memory, due to unfamiliar with `.Internal` calls, I could not provide a function that modify the MAX_NCONNECTIONS (but it is possible.)
> test and changes are shown below. I'll be appperciate if you could tell me whether there could be a bug.
>
> (a demo that may change MAX_NCONNECTIONS, not tested.)
> static int SetMaxNconnections(int now){ // return current value of MAX_NCONNECTIONS
>  if(now<3)error(_("Could not shrink the MAX_NCONNECTIONS less than 3"));
>  if(now>65536)warning(_("Setting MAX_NCONNECTIONS=%d, larger than 65536, may be crazy. Use at your own risk."),now);
>  // setting MAX_NCONNECTIONS to a really large value is safe, since the allocation is not done immediately. Thus this is a warning.
>  if(now>=NCONNECTIONS)return MAX_NCONNECTIONS=now; // if now is larger than NCONNECTIONS<=now,MAX_NCONNECTIONS, thus it is safe.
>  R_gc(); /* Try to reclaim unused connections */
>  for(int i=NCONNECTIONS;i>=now;--i){// now >= 3 here, thus no underflow occurs.
>    // shrink the value of MAX_NCONNECTIONS and NCONNECTIONS
>    if(!Connections[i]){now=i+1;break;}
>  }
>  // here, we could call a realloc, since *Connections only capture several kilobytes, realloc seems meaningless.
>  // a true realloc will trigger if NCONNECTIONS<MAX_NCONNECTIONS and call NextConnection with all connections are in use
>  return MAX_NCONNECTIONS=NCONNECTIONS=now;
> }
>
>
>
> test result:
>
> $ LC_ALL=C R-4.1.1/bin/R -q -e 'library(doParallel);cl=makeForkCluster(128);max(sapply(clusterCall(cl,function()runif(10)),"+"))'
> WARNING: ignoring environment value of R_HOME
>> library(doParallel);cl=makeForkCluster(128);max(sapply(clusterCall(cl,function()runif(10)),"+"))
> Loading required package: foreach
> Loading required package: iterators
> Loading required package: parallel
> Warning messages:
> 1: In socketAccept(socket = socket, blocking = TRUE, open = "a+b",  :
>  increase max connections from 16 to 32
> 2: In socketAccept(socket = socket, blocking = TRUE, open = "a+b",  :
>  increase max connections from 32 to 64
> 3: In socketAccept(socket = socket, blocking = TRUE, open = "a+b",  :
>  increase max connections from 64 to 128
> 4: In socketAccept(socket = socket, blocking = TRUE, open = "a+b",  :
>  increase max connections from 128 to 256
> [1] 0.9975836
>>
>>
>
>
> tested changes:
>
>
> ~line 127
>
> static int NCONNECTIONS=16; /* need one per cluster node, 16 is the
>  initial value which grows dynamically */
> static int MAX_NCONNECTIONS=8192; /* increase it only affect the speed of
>  finding the correct connection, if you have a machine with more than
>  4096 threads, you could submit an issue or modify this value manually */
> #define NSINKS 21
>
> static Rconnection *Connections=NULL; /* we will allocate it later */
> ...
>
> ~line 146
>
>
>
> static int NextConnection(void)
> {
>    int i;
>    for(i = 3; i < NCONNECTIONS; i++)
>    if(!Connections[i]) break;
>    if(i >= NCONNECTIONS) {
>    R_gc(); /* Try to reclaim unused ones */
>    for(i = 3; i < NCONNECTIONS; i++)
>        if(!Connections[i]) break;
>    if(i >= NCONNECTIONS) {
>        if(i >= MAX_NCONNECTIONS)
>        error(_("all connections are in use"));
>        int new_connections=NCONNECTIONS*2;//try dynamic alloc
>        if(new_connections > MAX_NCONNECTIONS)
>        new_connections = MAX_NCONNECTIONS;
>        Rconnection*ptr = realloc(Connections,new_connections*sizeof(Rconnection));
>        if (ptr==NULL)
>        error(_("alloc extra connections failed"));
>        warning(_("increase max connections from %d to %d\n"),NCONNECTIONS,new_connections);
>        Connections = ptr;
>        NCONNECTIONS = new_connections;
>        for(int j = i; j < NCONNECTIONS; j++) Connections[j] = NULL;
>    }
>    }
>    return i;
> }
> ...
>
>
>
> ~line 5265
>
> void attribute_hidden InitConnections()
> {
>    int i;
>    Connections=malloc(NCONNECTIONS*sizeof(Rconnection));
>    if(Connections == NULL) {
>    error(_("Cannot alloc connections."));
>    abort();
>    }
> ...
>
>
>> -----????-----
>> ???: "Simon Urbanek" <simon.urbanek at R-project.org>
>> ????: 2021-08-25 08:25:47 (???)
>> ???: "Martin Maechler" <maechler at stat.math.ethz.ch>
>> ??: "GILLIBERT, Andre" <Andre.Gillibert at chu-rouen.fr>, "qweytr1 at mail.ustc.edu.cn" <qweytr1 at mail.ustc.edu.cn>, R-devel <R-devel at r-project.org>
>> ??: [SPAM] Re: [Rd] Is it a good choice to increase the NCONNECTION value?
>>
>> Martin,
>>
>> I don't think static connection limit is sensible. Recall that connections can be anything, not just necessarily sockets or file descriptions so they are not linked to the system fd limit. For example, if you use a codec then you will need twice the number of connections than the fds. To be honest the connection limit is one of the main reasons why in our big data applications we have always avoided R connections and used C-level sockets instead (others were lack of control over the socket flags, but that has been addressed in the last release). So I'd vote for at the very least increasing the limit significantly (at least 1k if not more) and, ideally, make it dynamic if memory footprint is an issue.
>>
>> Cheers,
>> Simon
>>
>>
>>> On Aug 25, 2021, at 8:53 AM, Martin Maechler <maechler at stat.math.ethz.ch> wrote:
>>>
>>>>>>>> GILLIBERT, Andre
>>>>>>>>   on Tue, 24 Aug 2021 09:49:52 +0000 writes:
>>>
>>>> RConnection is a pointer to a Rconn structure. The Rconn
>>>> structure must be allocated independently (e.g. by
>>>> malloc() in R_new_custom_connection).  Therefore,
>>>> increasing NCONNECTION to 1024 should only use 8
>>>> kilobytes on 64-bits platforms and 4 kilobytes on 32
>>>> bits platforms.
>>>
>>> You are right indeed, and I was wrong.
>>>
>>>> Ideally, it should be dynamically allocated : either as
>>>> a linked list or as a dynamic array
>>>> (malloc/realloc). However, a simple change of
>>>> NCONNECTION to 1024 should be enough for most uses.
>>>
>>> There is one important other problem I've been made aware
>>> (similarly to the number of open DLL libraries, an issue 1-2
>>> years ago) :
>>>
>>> The OS itself has limits on the number of open files
>>> (yes, I know that there are other connections than files) and
>>> these limits may quite differ from platform to platform.
>>>
>>> On my Linux laptop, in a shell, I see
>>>
>>> $ ulimit -n
>>> 1024
>>>
>>> which is barely conformant with your proposed 1024 NCONNECTION.
>>>
>>> Now if NCONNCECTION is larger than the max allowed number of
>>> open files and if R opens more files than the OS allowed, the
>>> user may get quite unpleasant behavior, e.g. R being terminated brutally
>>> (or behaving crazily) without good R-level warning / error messages.
>>>
>>> It's also not at all sufficient to check for the open files
>>> limit at compile time, but rather at R process startup time
>>>
>>> So this may need considerably more work than you / we have
>>> hoped, and it's probably hard to find a safe number that is
>>> considerably larger than 128  and less than the smallest of all
>>> non-crazy platforms' {number of open files limit}.
>>>
>>>> Sincerely
>>>> Andr? GILLIBERT
>>>
>>> [............]
>>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>
>       [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel



From Andre@G||||bert @end|ng |rom chu-rouen@|r  Fri Aug 27 13:44:56 2021
From: Andre@G||||bert @end|ng |rom chu-rouen@|r (GILLIBERT, Andre)
Date: Fri, 27 Aug 2021 11:44:56 +0000
Subject: [Rd] R_CheckUserInterrupt
Message-ID: <ebce23ec19154286889dd1222c434b72@chu-rouen.fr>

Dear R developers,


R makes some functions interruptible, thanks to a call to R_CheckUserInterrupt. Simple arithmetic operations can be interrupted avoiding freezes when using huge arrays (e.g. length > 1 billion).

But many operations, such as matrix multiplication, are not interruptible. I estimated that a multiplication of two 10000?10000 square matrices would freeze R for at least 7 days on my computer, unless I kill the process.


I found an old commit that deleted the calls to R_CheckUserInterrupt in many basic operations (https://github.com/wch/r-source/commit/b99cd362a65012335a853d954cbeb1c782e6ae37)


Why were they deleted ?


First hypothesis : this slowed down the code too much, because it was done suboptimally, with an integer division (high latency CPU operation) at each iteration.

Second hypothesis : this introduced bugs, such as memory leaks, in code that did not handle interruptions gracefully.


If the first hypothesis is correct, I can write much better code, with almost zero penalty, using R_ITERATE_CHECK (in R_ext/Itermacros.h) and a new macro I wrote: ITERATE_BY_REGION_CHECK.


That would make more operations interruptible and would even provide performances improvements in loops that were not optimized for ALTREPs.


Are you interested in patches?



PS: the slow integer division is actually not very slow with recent GCC versions, because this compiler is smart enough to replace it by a multiplication and a shift because the divisor is known at compile time. Older compilers may not be that smart.


--

Sincerely

Andr? GILLIBERT

	[[alternative HTML version deleted]]


From ggrothend|eck @end|ng |rom gm@||@com  Fri Aug 27 17:17:06 2021
From: ggrothend|eck @end|ng |rom gm@||@com (Gabor Grothendieck)
Date: Fri, 27 Aug 2021 11:17:06 -0400
Subject: [Rd] order of operations
Message-ID: <CAP01uRnu6wfgqD8-Q9atLWzdVcRumT1LkkhXP0T7LkXB1K+DeA@mail.gmail.com>

Are there any guarantees of whether x will equal 1 or 2 after this is run?

(x <- 1) * (x <- 2)
## [1] 2
x
## [1] 2

-- 
Statistics & Software Consulting
GKX Group, GKX Associates Inc.
tel: 1-877-GKX-GROUP
email: ggrothendieck at gmail.com


From th|erry@onke||nx @end|ng |rom |nbo@be  Fri Aug 27 17:28:40 2021
From: th|erry@onke||nx @end|ng |rom |nbo@be (Thierry Onkelinx)
Date: Fri, 27 Aug 2021 17:28:40 +0200
Subject: [Rd] order of operations
In-Reply-To: <CAP01uRnu6wfgqD8-Q9atLWzdVcRumT1LkkhXP0T7LkXB1K+DeA@mail.gmail.com>
References: <CAP01uRnu6wfgqD8-Q9atLWzdVcRumT1LkkhXP0T7LkXB1K+DeA@mail.gmail.com>
Message-ID: <CAJuCY5yBguMgL+ULEMA-q3WgutFODT0V3PKcAtHYQ3aTj2nLiA@mail.gmail.com>

IMHO this is just bad practice. Whether the result is guaranteed or not,
doesn't matter.

ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>


Op vr 27 aug. 2021 om 17:18 schreef Gabor Grothendieck <
ggrothendieck at gmail.com>:

> Are there any guarantees of whether x will equal 1 or 2 after this is run?
>
> (x <- 1) * (x <- 2)
> ## [1] 2
> x
> ## [1] 2
>
> --
> Statistics & Software Consulting
> GKX Group, GKX Associates Inc.
> tel: 1-877-GKX-GROUP
> email: ggrothendieck at gmail.com
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From ggrothend|eck @end|ng |rom gm@||@com  Fri Aug 27 17:32:01 2021
From: ggrothend|eck @end|ng |rom gm@||@com (Gabor Grothendieck)
Date: Fri, 27 Aug 2021 11:32:01 -0400
Subject: [Rd] order of operations
In-Reply-To: <CAJuCY5yBguMgL+ULEMA-q3WgutFODT0V3PKcAtHYQ3aTj2nLiA@mail.gmail.com>
References: <CAP01uRnu6wfgqD8-Q9atLWzdVcRumT1LkkhXP0T7LkXB1K+DeA@mail.gmail.com>
 <CAJuCY5yBguMgL+ULEMA-q3WgutFODT0V3PKcAtHYQ3aTj2nLiA@mail.gmail.com>
Message-ID: <CAP01uRnab0xjnQjcF8U0EXae80VOUdE3WpjtSiPE1UEZeHzBkw@mail.gmail.com>

I agree and personally never do this but I would still like to know if it
is guaranteed behavior or not.

On Fri, Aug 27, 2021 at 11:28 AM Thierry Onkelinx <thierry.onkelinx at inbo.be>
wrote:

> IMHO this is just bad practice. Whether the result is guaranteed or not,
> doesn't matter.
>
> ir. Thierry Onkelinx
> Statisticus / Statistician
>
> Vlaamse Overheid / Government of Flanders
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
> FOREST
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> thierry.onkelinx at inbo.be
> Havenlaan 88 bus 73, 1000 Brussel
> www.inbo.be
>
>
> ///////////////////////////////////////////////////////////////////////////////////////////
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
>
> ///////////////////////////////////////////////////////////////////////////////////////////
>
> <https://www.inbo.be>
>
>
> Op vr 27 aug. 2021 om 17:18 schreef Gabor Grothendieck <
> ggrothendieck at gmail.com>:
>
>> Are there any guarantees of whether x will equal 1 or 2 after this is run?
>>
>> (x <- 1) * (x <- 2)
>> ## [1] 2
>> x
>> ## [1] 2
>>
>> --
>> Statistics & Software Consulting
>> GKX Group, GKX Associates Inc.
>> tel: 1-877-GKX-GROUP
>> email: ggrothendieck at gmail.com
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>

-- 
Statistics & Software Consulting
GKX Group, GKX Associates Inc.
tel: 1-877-GKX-GROUP
email: ggrothendieck at gmail.com

	[[alternative HTML version deleted]]


From @v|gro@@ @end|ng |rom ver|zon@net  Fri Aug 27 18:35:23 2021
From: @v|gro@@ @end|ng |rom ver|zon@net (Avi Gross)
Date: Fri, 27 Aug 2021 12:35:23 -0400
Subject: [Rd] order of operations
In-Reply-To: <CAP01uRnab0xjnQjcF8U0EXae80VOUdE3WpjtSiPE1UEZeHzBkw@mail.gmail.com>
References: <CAP01uRnu6wfgqD8-Q9atLWzdVcRumT1LkkhXP0T7LkXB1K+DeA@mail.gmail.com>
 <CAJuCY5yBguMgL+ULEMA-q3WgutFODT0V3PKcAtHYQ3aTj2nLiA@mail.gmail.com>
 <CAP01uRnab0xjnQjcF8U0EXae80VOUdE3WpjtSiPE1UEZeHzBkw@mail.gmail.com>
Message-ID: <017c01d79b61$890b6f80$9b224e80$@verizon.net>

Does anyone have a case where this construct has a valid use? 

Didn't Python  add a := operator recently that might be intended more for
such uses as compared to using the standard assignment operators? I wonder
if that has explicit guarantees of what happens in such cases, but that is
outside what this forum cares about. Just for the heck of it, I tried the
example there:

	>>> (x := 1) * (x := 2)
	2
	>>> x
	2

Back to R, ...

The constructs can get arbitrarily complex as in:

(x <- (x <- 0) + 1) * (x <- (x <-2) + 1)

My impression is that when evaluation is left to right and also innermost
parentheses before outer ones, then something like the above goes in stages.
The first of two parenthetical expressions is evaluated first.

(x <- (x <- 0) + 1)

The inner parenthesis set x to zero then the outer one increments x to 1.
The full sub-expression evaluates to 1 and that value is set aside for a
later multiplication.

But then the second parenthesis evaluates similarly, from inside out:

(x <- (x <-2) + 1)

It clearly resets x to 2 then increments it by 1 to 3 and returns a value of
3. That is multiplied by the first sub-expression to result in 3.

So for simple addition, even though it is commutative, is there any reason
any compiler or interpreter should not follow rules like the above?
Obviously with something like matrices, some operations are not abelian and
require more strict interpretation in the right order.

And note the expressions like the above can run into more complex quandaries
such as when you have a conditional with OR or AND parts that may be
short-circuited and in some cases, a variable you expected to be set, may
remain unset or ...

This reminds me a bit of languages that allow pre/post increment/decrement
operators like ++ and -- and questions about what order things happen.
Ideally, anything in which a deterministic order is not guaranteed should be
flagged by the language at compile time (or when interpreted) and refuse to
go on. 

All I can say with computer languages and adding ever more features, 
	with greater power comes greater responsibility and often greater
confusion.


-----Original Message-----
From: R-devel <r-devel-bounces at r-project.org> On Behalf Of Gabor
Grothendieck
Sent: Friday, August 27, 2021 11:32 AM
To: Thierry Onkelinx <thierry.onkelinx at inbo.be>
Cc: r-devel at r-project.org
Subject: Re: [Rd] order of operations

I agree and personally never do this but I would still like to know if it is
guaranteed behavior or not.

On Fri, Aug 27, 2021 at 11:28 AM Thierry Onkelinx <thierry.onkelinx at inbo.be>
wrote:

> IMHO this is just bad practice. Whether the result is guaranteed or 
> not, doesn't matter.
>
> ir. Thierry Onkelinx
> Statisticus / Statistician
>
> Vlaamse Overheid / Government of Flanders INSTITUUT VOOR NATUUR- EN 
> BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND FOREST Team Biometrie 
> & Kwaliteitszorg / Team Biometrics & Quality Assurance 
> thierry.onkelinx at inbo.be Havenlaan 88 bus 73, 1000 Brussel www.inbo.be
>
>
> //////////////////////////////////////////////////////////////////////
> ///////////////////// To call in the statistician after the experiment 
> is done may be no more than asking him to perform a post-mortem 
> examination: he may be able to say what the experiment died of. ~ Sir 
> Ronald Aylmer Fisher The plural of anecdote is not data. ~ Roger 
> Brinner The combination of some data and an aching desire for an 
> answer does not ensure that a reasonable answer can be extracted from 
> a given body of data.
> ~ John Tukey
>
> //////////////////////////////////////////////////////////////////////
> /////////////////////
>
> <https://www.inbo.be>
>
>
> Op vr 27 aug. 2021 om 17:18 schreef Gabor Grothendieck <
> ggrothendieck at gmail.com>:
>
>> Are there any guarantees of whether x will equal 1 or 2 after this is
run?
>>
>> (x <- 1) * (x <- 2)
>> ## [1] 2
>> x
>> ## [1] 2
>>
>> --
>> Statistics & Software Consulting
>> GKX Group, GKX Associates Inc.
>> tel: 1-877-GKX-GROUP
>> email: ggrothendieck at gmail.com
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>

--
Statistics & Software Consulting
GKX Group, GKX Associates Inc.
tel: 1-877-GKX-GROUP
email: ggrothendieck at gmail.com

	[[alternative HTML version deleted]]

______________________________________________
R-devel at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-devel


From ggrothend|eck @end|ng |rom gm@||@com  Fri Aug 27 19:57:33 2021
From: ggrothend|eck @end|ng |rom gm@||@com (Gabor Grothendieck)
Date: Fri, 27 Aug 2021 13:57:33 -0400
Subject: [Rd] order of operations
In-Reply-To: <017c01d79b61$890b6f80$9b224e80$@verizon.net>
References: <CAP01uRnu6wfgqD8-Q9atLWzdVcRumT1LkkhXP0T7LkXB1K+DeA@mail.gmail.com>
 <CAJuCY5yBguMgL+ULEMA-q3WgutFODT0V3PKcAtHYQ3aTj2nLiA@mail.gmail.com>
 <CAP01uRnab0xjnQjcF8U0EXae80VOUdE3WpjtSiPE1UEZeHzBkw@mail.gmail.com>
 <017c01d79b61$890b6f80$9b224e80$@verizon.net>
Message-ID: <CAP01uRksX11gsW9BCSW-CTw+TVHf2RGrmP5fCoyUYRFC20pfFA@mail.gmail.com>

It could be that the two sides of * are run in parallel in the future and maybe
not having a guarantee would simplify implementation?


On Fri, Aug 27, 2021 at 12:35 PM Avi Gross via R-devel
<r-devel at r-project.org> wrote:
>
> Does anyone have a case where this construct has a valid use?
>
> Didn't Python  add a := operator recently that might be intended more for
> such uses as compared to using the standard assignment operators? I wonder
> if that has explicit guarantees of what happens in such cases, but that is
> outside what this forum cares about. Just for the heck of it, I tried the
> example there:
>
>         >>> (x := 1) * (x := 2)
>         2
>         >>> x
>         2
>
> Back to R, ...
>
> The constructs can get arbitrarily complex as in:
>
> (x <- (x <- 0) + 1) * (x <- (x <-2) + 1)
>
> My impression is that when evaluation is left to right and also innermost
> parentheses before outer ones, then something like the above goes in stages.
> The first of two parenthetical expressions is evaluated first.
>
> (x <- (x <- 0) + 1)
>
> The inner parenthesis set x to zero then the outer one increments x to 1.
> The full sub-expression evaluates to 1 and that value is set aside for a
> later multiplication.
>
> But then the second parenthesis evaluates similarly, from inside out:
>
> (x <- (x <-2) + 1)
>
> It clearly resets x to 2 then increments it by 1 to 3 and returns a value of
> 3. That is multiplied by the first sub-expression to result in 3.
>
> So for simple addition, even though it is commutative, is there any reason
> any compiler or interpreter should not follow rules like the above?
> Obviously with something like matrices, some operations are not abelian and
> require more strict interpretation in the right order.
>
> And note the expressions like the above can run into more complex quandaries
> such as when you have a conditional with OR or AND parts that may be
> short-circuited and in some cases, a variable you expected to be set, may
> remain unset or ...
>
> This reminds me a bit of languages that allow pre/post increment/decrement
> operators like ++ and -- and questions about what order things happen.
> Ideally, anything in which a deterministic order is not guaranteed should be
> flagged by the language at compile time (or when interpreted) and refuse to
> go on.
>
> All I can say with computer languages and adding ever more features,
>         with greater power comes greater responsibility and often greater
> confusion.
>
>
> -----Original Message-----
> From: R-devel <r-devel-bounces at r-project.org> On Behalf Of Gabor
> Grothendieck
> Sent: Friday, August 27, 2021 11:32 AM
> To: Thierry Onkelinx <thierry.onkelinx at inbo.be>
> Cc: r-devel at r-project.org
> Subject: Re: [Rd] order of operations
>
> I agree and personally never do this but I would still like to know if it is
> guaranteed behavior or not.
>
> On Fri, Aug 27, 2021 at 11:28 AM Thierry Onkelinx <thierry.onkelinx at inbo.be>
> wrote:
>
> > IMHO this is just bad practice. Whether the result is guaranteed or
> > not, doesn't matter.
> >
> > ir. Thierry Onkelinx
> > Statisticus / Statistician
> >
> > Vlaamse Overheid / Government of Flanders INSTITUUT VOOR NATUUR- EN
> > BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND FOREST Team Biometrie
> > & Kwaliteitszorg / Team Biometrics & Quality Assurance
> > thierry.onkelinx at inbo.be Havenlaan 88 bus 73, 1000 Brussel www.inbo.be
> >
> >
> > //////////////////////////////////////////////////////////////////////
> > ///////////////////// To call in the statistician after the experiment
> > is done may be no more than asking him to perform a post-mortem
> > examination: he may be able to say what the experiment died of. ~ Sir
> > Ronald Aylmer Fisher The plural of anecdote is not data. ~ Roger
> > Brinner The combination of some data and an aching desire for an
> > answer does not ensure that a reasonable answer can be extracted from
> > a given body of data.
> > ~ John Tukey
> >
> > //////////////////////////////////////////////////////////////////////
> > /////////////////////
> >
> > <https://www.inbo.be>
> >
> >
> > Op vr 27 aug. 2021 om 17:18 schreef Gabor Grothendieck <
> > ggrothendieck at gmail.com>:
> >
> >> Are there any guarantees of whether x will equal 1 or 2 after this is
> run?
> >>
> >> (x <- 1) * (x <- 2)
> >> ## [1] 2
> >> x
> >> ## [1] 2
> >>
> >> --
> >> Statistics & Software Consulting
> >> GKX Group, GKX Associates Inc.
> >> tel: 1-877-GKX-GROUP
> >> email: ggrothendieck at gmail.com
> >>
> >> ______________________________________________
> >> R-devel at r-project.org mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-devel
> >>
> >
>
> --
> Statistics & Software Consulting
> GKX Group, GKX Associates Inc.
> tel: 1-877-GKX-GROUP
> email: ggrothendieck at gmail.com
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel



-- 
Statistics & Software Consulting
GKX Group, GKX Associates Inc.
tel: 1-877-GKX-GROUP
email: ggrothendieck at gmail.com


From @v|gro@@ @end|ng |rom ver|zon@net  Fri Aug 27 20:29:58 2021
From: @v|gro@@ @end|ng |rom ver|zon@net (Avi Gross)
Date: Fri, 27 Aug 2021 14:29:58 -0400
Subject: [Rd] order of operations
In-Reply-To: <CAP01uRksX11gsW9BCSW-CTw+TVHf2RGrmP5fCoyUYRFC20pfFA@mail.gmail.com>
References: <CAP01uRnu6wfgqD8-Q9atLWzdVcRumT1LkkhXP0T7LkXB1K+DeA@mail.gmail.com>
 <CAJuCY5yBguMgL+ULEMA-q3WgutFODT0V3PKcAtHYQ3aTj2nLiA@mail.gmail.com>
 <CAP01uRnab0xjnQjcF8U0EXae80VOUdE3WpjtSiPE1UEZeHzBkw@mail.gmail.com>
 <017c01d79b61$890b6f80$9b224e80$@verizon.net>
 <CAP01uRksX11gsW9BCSW-CTw+TVHf2RGrmP5fCoyUYRFC20pfFA@mail.gmail.com>
Message-ID: <057601d79b71$8a6abe20$9f403a60$@verizon.net>

Running things in various forms of parallel opens up all kinds of issues. Currently, programs that use forms like "threads" often need to carefully protect any variables that can be changed using things like locks.

So what would they do in the scenario being discussed? Would they need to analyze the entire part of the program before splitting off parts and add code to protect not only from simultaneous access to the variable but set up a guarantee so that one of multiple threads would get to change it first and others freeze until it is their turn?

Strikes me as a bit too complex given the scenario does not look like one that is likely to have serious uses. 

I understand the question is more academic and there are multiple reasonable answers with tradeoffs. And one answer is to make it totally deterministic even if that precludes any ability to speed things up.  Another is to simply declare such use to be either illegal or unsupported.

And, perhaps, there can be support for ways to do this kind of thing more safely. Clearly, the methods of parallelism vary from threads within a program running on the same processor that just interleave, to running on multiple processors and even multiple machines across the world. Darned if I know what issues would come up on  quantum computers which have yet other aspects of the concept of parallelism.


-----Original Message-----
From: Gabor Grothendieck <ggrothendieck at gmail.com> 
Sent: Friday, August 27, 2021 1:58 PM
To: Avi Gross <avigross at verizon.net>
Cc: r-devel at r-project.org
Subject: Re: [Rd] order of operations

It could be that the two sides of * are run in parallel in the future and maybe not having a guarantee would simplify implementation?


On Fri, Aug 27, 2021 at 12:35 PM Avi Gross via R-devel <r-devel at r-project.org> wrote:
>
> Does anyone have a case where this construct has a valid use?
>
> Didn't Python  add a := operator recently that might be intended more 
> for such uses as compared to using the standard assignment operators? 
> I wonder if that has explicit guarantees of what happens in such 
> cases, but that is outside what this forum cares about. Just for the 
> heck of it, I tried the example there:
>
>         >>> (x := 1) * (x := 2)
>         2
>         >>> x
>         2
>
> Back to R, ...
>
> The constructs can get arbitrarily complex as in:
>
> (x <- (x <- 0) + 1) * (x <- (x <-2) + 1)
>
> My impression is that when evaluation is left to right and also 
> innermost parentheses before outer ones, then something like the above goes in stages.
> The first of two parenthetical expressions is evaluated first.
>
> (x <- (x <- 0) + 1)
>
> The inner parenthesis set x to zero then the outer one increments x to 1.
> The full sub-expression evaluates to 1 and that value is set aside for 
> a later multiplication.
>
> But then the second parenthesis evaluates similarly, from inside out:
>
> (x <- (x <-2) + 1)
>
> It clearly resets x to 2 then increments it by 1 to 3 and returns a 
> value of 3. That is multiplied by the first sub-expression to result in 3.
>
> So for simple addition, even though it is commutative, is there any 
> reason any compiler or interpreter should not follow rules like the above?
> Obviously with something like matrices, some operations are not 
> abelian and require more strict interpretation in the right order.
>
> And note the expressions like the above can run into more complex 
> quandaries such as when you have a conditional with OR or AND parts 
> that may be short-circuited and in some cases, a variable you expected 
> to be set, may remain unset or ...
>
> This reminds me a bit of languages that allow pre/post 
> increment/decrement operators like ++ and -- and questions about what order things happen.
> Ideally, anything in which a deterministic order is not guaranteed 
> should be flagged by the language at compile time (or when 
> interpreted) and refuse to go on.
>
> All I can say with computer languages and adding ever more features,
>         with greater power comes greater responsibility and often 
> greater confusion.
>
>
> -----Original Message-----
> From: R-devel <r-devel-bounces at r-project.org> On Behalf Of Gabor 
> Grothendieck
> Sent: Friday, August 27, 2021 11:32 AM
> To: Thierry Onkelinx <thierry.onkelinx at inbo.be>
> Cc: r-devel at r-project.org
> Subject: Re: [Rd] order of operations
>
> I agree and personally never do this but I would still like to know if 
> it is guaranteed behavior or not.
>
> On Fri, Aug 27, 2021 at 11:28 AM Thierry Onkelinx 
> <thierry.onkelinx at inbo.be>
> wrote:
>
> > IMHO this is just bad practice. Whether the result is guaranteed or 
> > not, doesn't matter.
> >
> > ir. Thierry Onkelinx
> > Statisticus / Statistician
> >
> > Vlaamse Overheid / Government of Flanders INSTITUUT VOOR NATUUR- EN 
> > BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND FOREST Team 
> > Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance 
> > thierry.onkelinx at inbo.be Havenlaan 88 bus 73, 1000 Brussel 
> > www.inbo.be
> >
> >
> > ////////////////////////////////////////////////////////////////////
> > // ///////////////////// To call in the statistician after the 
> > experiment is done may be no more than asking him to perform a 
> > post-mortem
> > examination: he may be able to say what the experiment died of. ~ 
> > Sir Ronald Aylmer Fisher The plural of anecdote is not data. ~ Roger 
> > Brinner The combination of some data and an aching desire for an 
> > answer does not ensure that a reasonable answer can be extracted 
> > from a given body of data.
> > ~ John Tukey
> >
> > ////////////////////////////////////////////////////////////////////
> > //
> > /////////////////////
> >
> > <https://www.inbo.be>
> >
> >
> > Op vr 27 aug. 2021 om 17:18 schreef Gabor Grothendieck <
> > ggrothendieck at gmail.com>:
> >
> >> Are there any guarantees of whether x will equal 1 or 2 after this 
> >> is
> run?
> >>
> >> (x <- 1) * (x <- 2)
> >> ## [1] 2
> >> x
> >> ## [1] 2
> >>
> >> --
> >> Statistics & Software Consulting
> >> GKX Group, GKX Associates Inc.
> >> tel: 1-877-GKX-GROUP
> >> email: ggrothendieck at gmail.com
> >>
> >> ______________________________________________
> >> R-devel at r-project.org mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-devel
> >>
> >
>
> --
> Statistics & Software Consulting
> GKX Group, GKX Associates Inc.
> tel: 1-877-GKX-GROUP
> email: ggrothendieck at gmail.com
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel



--
Statistics & Software Consulting
GKX Group, GKX Associates Inc.
tel: 1-877-GKX-GROUP
email: ggrothendieck at gmail.com


From Andre@G||||bert @end|ng |rom chu-rouen@|r  Fri Aug 27 20:36:40 2021
From: Andre@G||||bert @end|ng |rom chu-rouen@|r (GILLIBERT, Andre)
Date: Fri, 27 Aug 2021 18:36:40 +0000
Subject: [Rd] order of operations
In-Reply-To: <CAP01uRksX11gsW9BCSW-CTw+TVHf2RGrmP5fCoyUYRFC20pfFA@mail.gmail.com>
References: <CAP01uRnu6wfgqD8-Q9atLWzdVcRumT1LkkhXP0T7LkXB1K+DeA@mail.gmail.com>
 <CAJuCY5yBguMgL+ULEMA-q3WgutFODT0V3PKcAtHYQ3aTj2nLiA@mail.gmail.com>
 <CAP01uRnab0xjnQjcF8U0EXae80VOUdE3WpjtSiPE1UEZeHzBkw@mail.gmail.com>
 <017c01d79b61$890b6f80$9b224e80$@verizon.net>,
 <CAP01uRksX11gsW9BCSW-CTw+TVHf2RGrmP5fCoyUYRFC20pfFA@mail.gmail.com>
Message-ID: <96c41b43c988420695e9f764ec4c370f@chu-rouen.fr>

Due to lazy evaluation, the order of operations can be pretty random in R. Actually, some operations may not be performed at all, sometimes.


The following program illustrates the issue:

test1=function(x,y) {}
test2=function(x,y) {x;y}
test3=function(x,y) {y;x}
alpha="hello"
test1(alpha <- 1, alpha <- 2)
print(alpha) # prints "hello"

test2(alpha <- 1, alpha <- 2)
print(alpha) # prints 2

test3(alpha <- 1, alpha <- 2)
print(alpha) # prints 1


Many internal functions unconditionally evaluate all their parameters before performing any operation, because they know that they will use all of them anyway. Theoretically, the order of evaluation could be well-specified for these internal functions, but I would recommend against doing that, because the functions could be changed in future, and not evaluate some of their parameters, or change the order of evaluation.

For instance, in R 4.0, the following code displays 6:
alpha<-42
1|(alpha<-6)
print(alpha)

But, I would not be shocked to see it display 42 in a future version of R.

Moreover, internal functions are usually wrapped in R code, that may evaluate parameters in random orders due to lazy evaluation.
See mean.default, for instance...

On R 4.0.3:
mean.default((alpha<-1),(alpha<-2),(alpha<-3))
print(alpha) # prints 2

Things are probably less tricky with a simple addition or multiplication, but I would not rely on that.

--
Sincerely
Andr? GILLIBERT
________________________________
De : R-devel <r-devel-bounces at r-project.org> de la part de Gabor Grothendieck <ggrothendieck at gmail.com>
Envoy? : vendredi 27 ao?t 2021 19:57:33
? : Avi Gross
Cc : r-devel at r-project.org
Objet : Re: [Rd] order of operations

ATTENTION: Cet e-mail provient d?une adresse mail ext?rieure au CHU de Rouen. Ne cliquez pas sur les liens ou n'ouvrez pas les pi?ces jointes ? moins de conna?tre l'exp?diteur et de savoir que le contenu est s?r. En cas de doute, transf?rer le mail ? ? DSI, S?curit? ? pour analyse. Merci de votre vigilance


It could be that the two sides of * are run in parallel in the future and maybe
not having a guarantee would simplify implementation?


On Fri, Aug 27, 2021 at 12:35 PM Avi Gross via R-devel
<r-devel at r-project.org> wrote:
>
> Does anyone have a case where this construct has a valid use?
>
> Didn't Python  add a := operator recently that might be intended more for
> such uses as compared to using the standard assignment operators? I wonder
> if that has explicit guarantees of what happens in such cases, but that is
> outside what this forum cares about. Just for the heck of it, I tried the
> example there:
>
>         >>> (x := 1) * (x := 2)
>         2
>         >>> x
>         2
>
> Back to R, ...
>
> The constructs can get arbitrarily complex as in:
>
> (x <- (x <- 0) + 1) * (x <- (x <-2) + 1)
>
> My impression is that when evaluation is left to right and also innermost
> parentheses before outer ones, then something like the above goes in stages.
> The first of two parenthetical expressions is evaluated first.
>
> (x <- (x <- 0) + 1)
>
> The inner parenthesis set x to zero then the outer one increments x to 1.
> The full sub-expression evaluates to 1 and that value is set aside for a
> later multiplication.
>
> But then the second parenthesis evaluates similarly, from inside out:
>
> (x <- (x <-2) + 1)
>
> It clearly resets x to 2 then increments it by 1 to 3 and returns a value of
> 3. That is multiplied by the first sub-expression to result in 3.
>
> So for simple addition, even though it is commutative, is there any reason
> any compiler or interpreter should not follow rules like the above?
> Obviously with something like matrices, some operations are not abelian and
> require more strict interpretation in the right order.
>
> And note the expressions like the above can run into more complex quandaries
> such as when you have a conditional with OR or AND parts that may be
> short-circuited and in some cases, a variable you expected to be set, may
> remain unset or ...
>
> This reminds me a bit of languages that allow pre/post increment/decrement
> operators like ++ and -- and questions about what order things happen.
> Ideally, anything in which a deterministic order is not guaranteed should be
> flagged by the language at compile time (or when interpreted) and refuse to
> go on.
>
> All I can say with computer languages and adding ever more features,
>         with greater power comes greater responsibility and often greater
> confusion.
>
>
> -----Original Message-----
> From: R-devel <r-devel-bounces at r-project.org> On Behalf Of Gabor
> Grothendieck
> Sent: Friday, August 27, 2021 11:32 AM
> To: Thierry Onkelinx <thierry.onkelinx at inbo.be>
> Cc: r-devel at r-project.org
> Subject: Re: [Rd] order of operations
>
> I agree and personally never do this but I would still like to know if it is
> guaranteed behavior or not.
>
> On Fri, Aug 27, 2021 at 11:28 AM Thierry Onkelinx <thierry.onkelinx at inbo.be>
> wrote:
>
> > IMHO this is just bad practice. Whether the result is guaranteed or
> > not, doesn't matter.
> >
> > ir. Thierry Onkelinx
> > Statisticus / Statistician
> >
> > Vlaamse Overheid / Government of Flanders INSTITUUT VOOR NATUUR- EN
> > BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND FOREST Team Biometrie
> > & Kwaliteitszorg / Team Biometrics & Quality Assurance
> > thierry.onkelinx at inbo.be Havenlaan 88 bus 73, 1000 Brussel www.inbo.be<http://www.inbo.be>
> >
> >
> > //////////////////////////////////////////////////////////////////////
> > ///////////////////// To call in the statistician after the experiment
> > is done may be no more than asking him to perform a post-mortem
> > examination: he may be able to say what the experiment died of. ~ Sir
> > Ronald Aylmer Fisher The plural of anecdote is not data. ~ Roger
> > Brinner The combination of some data and an aching desire for an
> > answer does not ensure that a reasonable answer can be extracted from
> > a given body of data.
> > ~ John Tukey
> >
> > //////////////////////////////////////////////////////////////////////
> > /////////////////////
> >
> > <https://www.inbo.be>
> >
> >
> > Op vr 27 aug. 2021 om 17:18 schreef Gabor Grothendieck <
> > ggrothendieck at gmail.com>:
> >
> >> Are there any guarantees of whether x will equal 1 or 2 after this is
> run?
> >>
> >> (x <- 1) * (x <- 2)
> >> ## [1] 2
> >> x
> >> ## [1] 2
> >>
> >> --
> >> Statistics & Software Consulting
> >> GKX Group, GKX Associates Inc.
> >> tel: 1-877-GKX-GROUP
> >> email: ggrothendieck at gmail.com
> >>
> >> ______________________________________________
> >> R-devel at r-project.org mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-devel
> >>
> >
>
> --
> Statistics & Software Consulting
> GKX Group, GKX Associates Inc.
> tel: 1-877-GKX-GROUP
> email: ggrothendieck at gmail.com
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel



--
Statistics & Software Consulting
GKX Group, GKX Associates Inc.
tel: 1-877-GKX-GROUP
email: ggrothendieck at gmail.com

______________________________________________
R-devel at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-devel


	[[alternative HTML version deleted]]


From e@ @end|ng |rom enr|co@chum@nn@net  Fri Aug 27 21:06:54 2021
From: e@ @end|ng |rom enr|co@chum@nn@net (Enrico Schumann)
Date: Fri, 27 Aug 2021 21:06:54 +0200
Subject: [Rd] order of operations
In-Reply-To: <CAP01uRnu6wfgqD8-Q9atLWzdVcRumT1LkkhXP0T7LkXB1K+DeA@mail.gmail.com>
 (Gabor Grothendieck's message of "Fri, 27 Aug 2021 11:17:06 -0400")
References: <CAP01uRnu6wfgqD8-Q9atLWzdVcRumT1LkkhXP0T7LkXB1K+DeA@mail.gmail.com>
Message-ID: <871r6e3dwx.fsf@enricoschumann.net>

On Fri, 27 Aug 2021, Gabor Grothendieck writes:

> Are there any guarantees of whether x will equal 1 or 2 after this is run?
>
> (x <- 1) * (x <- 2)
> ## [1] 2
> x
> ## [1] 2

At least the "R Language Definition" [1] says

  "The exponentiation operator ?^? and the left
   assignment plus minus operators ?<- - = <<-?
   group right to left, all other operators group
   left to right.  That is  [...]  1 - 1 - 1 is -1"

which would imply 2.

[1] https://cran.r-project.org/doc/manuals/r-release/R-lang.html#Infix-and-prefix-operators

-- 
Enrico Schumann
Lucerne, Switzerland
http://enricoschumann.net


From murdoch@dunc@n @end|ng |rom gm@||@com  Fri Aug 27 21:14:56 2021
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Fri, 27 Aug 2021 15:14:56 -0400
Subject: [Rd] order of operations
In-Reply-To: <871r6e3dwx.fsf@enricoschumann.net>
References: <CAP01uRnu6wfgqD8-Q9atLWzdVcRumT1LkkhXP0T7LkXB1K+DeA@mail.gmail.com>
 <871r6e3dwx.fsf@enricoschumann.net>
Message-ID: <a7db1bf0-3d03-fdb0-03b6-586ac8e48816@gmail.com>

On 27/08/2021 3:06 p.m., Enrico Schumann wrote:
> On Fri, 27 Aug 2021, Gabor Grothendieck writes:
> 
>> Are there any guarantees of whether x will equal 1 or 2 after this is run?
>>
>> (x <- 1) * (x <- 2)
>> ## [1] 2
>> x
>> ## [1] 2
> 
> At least the "R Language Definition" [1] says
> 
>    "The exponentiation operator ?^? and the left
>     assignment plus minus operators ?<- - = <<-?
>     group right to left, all other operators group
>     left to right.  That is  [...]  1 - 1 - 1 is -1"
> 
> which would imply 2.

I think this is a different issue.  There's only one operator in 
question (the "*").  The question is whether x*y evaluates x first or y 
first (and I believe the answer is that there are no guarantees).  I'm 
fairly sure both are guaranteed to be evaluated, under the rules for 
group generics listed in ?groupGeneric, but I'm not certain the 
guarantee is honoured in all cases.

Duncan Murdoch


From pd@|gd @end|ng |rom gm@||@com  Sat Aug 28 09:26:03 2021
From: pd@|gd @end|ng |rom gm@||@com (peter dalgaard)
Date: Sat, 28 Aug 2021 09:26:03 +0200
Subject: [Rd] order of operations
In-Reply-To: <a7db1bf0-3d03-fdb0-03b6-586ac8e48816@gmail.com>
References: <CAP01uRnu6wfgqD8-Q9atLWzdVcRumT1LkkhXP0T7LkXB1K+DeA@mail.gmail.com>
 <871r6e3dwx.fsf@enricoschumann.net>
 <a7db1bf0-3d03-fdb0-03b6-586ac8e48816@gmail.com>
Message-ID: <2912BA92-DE84-4F36-911F-EB654876F0F8@gmail.com>

Yes, and were it not for 0 * NA == NA, you might skip evaluation of y if x evaluates to zero.  In Andre Gillibert's example: 

1 | (alpha<-6)

there really is no reason to evaluate the assignment since (1 | any) is always TRUE. Notwithstanding method dispatch, that is.

With general function calls, all bets are off. Even f(x <- 1) might decide not to evaluate its argument.

- pd

> On 27 Aug 2021, at 21:14 , Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
> 
> On 27/08/2021 3:06 p.m., Enrico Schumann wrote:
>> On Fri, 27 Aug 2021, Gabor Grothendieck writes:
>>> Are there any guarantees of whether x will equal 1 or 2 after this is run?
>>> 
>>> (x <- 1) * (x <- 2)
>>> ## [1] 2
>>> x
>>> ## [1] 2
>> At least the "R Language Definition" [1] says
>>   "The exponentiation operator ?^? and the left
>>    assignment plus minus operators ?<- - = <<-?
>>    group right to left, all other operators group
>>    left to right.  That is  [...]  1 - 1 - 1 is -1"
>> which would imply 2.
> 
> I think this is a different issue.  There's only one operator in question (the "*").  The question is whether x*y evaluates x first or y first (and I believe the answer is that there are no guarantees). I'm fairly sure both are guaranteed to be evaluated, under the rules for group generics listed in ?groupGeneric, but I'm not certain the guarantee is honoured in all cases.
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From Andre@G||||bert @end|ng |rom chu-rouen@|r  Sat Aug 28 09:44:06 2021
From: Andre@G||||bert @end|ng |rom chu-rouen@|r (GILLIBERT, Andre)
Date: Sat, 28 Aug 2021 07:44:06 +0000
Subject: [Rd] order of operations
In-Reply-To: <2912BA92-DE84-4F36-911F-EB654876F0F8@gmail.com>
References: <CAP01uRnu6wfgqD8-Q9atLWzdVcRumT1LkkhXP0T7LkXB1K+DeA@mail.gmail.com>
 <871r6e3dwx.fsf@enricoschumann.net>
 <a7db1bf0-3d03-fdb0-03b6-586ac8e48816@gmail.com>,
 <2912BA92-DE84-4F36-911F-EB654876F0F8@gmail.com>
Message-ID: <7e6819f4f8d4483998e9d5fe2e1cb954@chu-rouen.fr>

For the discussion:

> Yes, and were it not for 0 * NA == NA, you might skip evaluation of y if x evaluates to zero.


With the same idea, NA * (alpha <- 6) could skip the assignment.

I do not think that, on the short term, R would do that thing, but who knows in the future!


As of R 4.1, typeof(`*`) actually returns "builtin" rather than "special", meaning that all its arguments are evaluated.


--

Sincerely

Andre GILLIBERT

________________________________
De : R-devel <r-devel-bounces at r-project.org> de la part de peter dalgaard <pdalgd at gmail.com>
Envoy? : samedi 28 ao?t 2021 09:26:03
? : Duncan Murdoch
Cc : r-devel at r-project.org
Objet : Re: [Rd] order of operations

ATTENTION: Cet e-mail provient d?une adresse mail ext?rieure au CHU de Rouen. Ne cliquez pas sur les liens ou n'ouvrez pas les pi?ces jointes ? moins de conna?tre l'exp?diteur et de savoir que le contenu est s?r. En cas de doute, transf?rer le mail ? ? DSI, S?curit? ? pour analyse. Merci de votre vigilance


Yes, and were it not for 0 * NA == NA, you might skip evaluation of y if x evaluates to zero.  In Andre Gillibert's example:

1 | (alpha<-6)

there really is no reason to evaluate the assignment since (1 | any) is always TRUE. Notwithstanding method dispatch, that is.

With general function calls, all bets are off. Even f(x <- 1) might decide not to evaluate its argument.

- pd

> On 27 Aug 2021, at 21:14 , Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
>
> On 27/08/2021 3:06 p.m., Enrico Schumann wrote:
>> On Fri, 27 Aug 2021, Gabor Grothendieck writes:
>>> Are there any guarantees of whether x will equal 1 or 2 after this is run?
>>>
>>> (x <- 1) * (x <- 2)
>>> ## [1] 2
>>> x
>>> ## [1] 2
>> At least the "R Language Definition" [1] says
>>   "The exponentiation operator ?^? and the left
>>    assignment plus minus operators ?<- - = <<-?
>>    group right to left, all other operators group
>>    left to right.  That is  [...]  1 - 1 - 1 is -1"
>> which would imply 2.
>
> I think this is a different issue.  There's only one operator in question (the "*").  The question is whether x*y evaluates x first or y first (and I believe the answer is that there are no guarantees). I'm fairly sure both are guaranteed to be evaluated, under the rules for group generics listed in ?groupGeneric, but I'm not certain the guarantee is honoured in all cases.
>
> Duncan Murdoch
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

--
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com

______________________________________________
R-devel at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-devel


	[[alternative HTML version deleted]]


From murdoch@dunc@n @end|ng |rom gm@||@com  Sat Aug 28 10:48:51 2021
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Sat, 28 Aug 2021 04:48:51 -0400
Subject: [Rd] order of operations
In-Reply-To: <7e6819f4f8d4483998e9d5fe2e1cb954@chu-rouen.fr>
References: <CAP01uRnu6wfgqD8-Q9atLWzdVcRumT1LkkhXP0T7LkXB1K+DeA@mail.gmail.com>
 <871r6e3dwx.fsf@enricoschumann.net>
 <a7db1bf0-3d03-fdb0-03b6-586ac8e48816@gmail.com>
 <2912BA92-DE84-4F36-911F-EB654876F0F8@gmail.com>
 <7e6819f4f8d4483998e9d5fe2e1cb954@chu-rouen.fr>
Message-ID: <d15b528e-b1a0-0dca-d9ee-bf8e473e1d17@gmail.com>

On 28/08/2021 3:44 a.m., GILLIBERT, Andre wrote:
> For the discussion:
> 
>  > Yes, and were it not for 0 * NA == NA, you might skip evaluation of y 
> if x evaluates to zero.
> 
> 
> With the same idea, NA * (alpha <- 6) could skip the assignment.

No, it can't, because until it evaluates the assignment it doesn't know 
if it has a class that does strange things with *:

 > `*.foo` <- function(x, y) 42
 > NA*(alpha <- structure("bar", class="foo"))
[1] 42

> I do not think that, on the short term, R would do that thing, but who 
> knows in the future!

As I posted yesterday, this is documented behaviour, so one would hope 
for clear notice if it is going to change.

Duncan Murdoch

> 
> 
> As of R 4.1, typeof(`*`) actually returns "builtin" rather than 
> "special", meaning that all its arguments are evaluated.
> 
> 
> -- 
> 
> Sincerely
> 
> Andre GILLIBERT
> 
> 
> ------------------------------------------------------------------------
> *De :* R-devel <r-devel-bounces at r-project.org> de la part de peter 
> dalgaard <pdalgd at gmail.com>
> *Envoy? :* samedi 28 ao?t 2021 09:26:03
> *? :* Duncan Murdoch
> *Cc?:* r-devel at r-project.org
> *Objet :* Re: [Rd] order of operations
> ATTENTION: Cet e-mail provient d?une adresse mail ext?rieure au CHU de 
> Rouen. Ne cliquez pas sur les liens ou n'ouvrez pas les pi?ces jointes ? 
> moins de conna?tre l'exp?diteur et de savoir que le contenu est s?r. En 
> cas de doute, transf?rer le mail ? ? DSI, S?curit? ? pour analyse. Merci 
> de votre vigilance
> 
> 
> Yes, and were it not for 0 * NA == NA, you might skip evaluation of y if 
> x evaluates to zero.? In Andre Gillibert's example:
> 
> 1 | (alpha<-6)
> 
> there really is no reason to evaluate the assignment since (1 | any) is 
> always TRUE. Notwithstanding method dispatch, that is.
> 
> With general function calls, all bets are off. Even f(x <- 1) might 
> decide not to evaluate its argument.
> 
> - pd
> 
>> On 27 Aug 2021, at 21:14 , Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
>>
>> On 27/08/2021 3:06 p.m., Enrico Schumann wrote:
>>> On Fri, 27 Aug 2021, Gabor Grothendieck writes:
>>>> Are there any guarantees of whether x will equal 1 or 2 after this is run?
>>>>
>>>> (x <- 1) * (x <- 2)
>>>> ## [1] 2
>>>> x
>>>> ## [1] 2
>>> At least the "R Language Definition" [1] says
>>>?? "The exponentiation operator ?^? and the left
>>>??? assignment plus minus operators ?<- - = <<-?
>>>??? group right to left, all other operators group
>>>??? left to right.? That is? [...]? 1 - 1 - 1 is -1"
>>> which would imply 2.
>>
>> I think this is a different issue.? There's only one operator in question (the "*").? The question is whether x*y evaluates x first or y first (and I believe the answer is that there are no guarantees). I'm fairly sure both are guaranteed to be evaluated,  under the rules for group generics listed in ?groupGeneric, but I'm 
> not certain the guarantee is honoured in all cases.
>>
>> Duncan Murdoch
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel 
> <https://stat.ethz.ch/mailman/listinfo/r-devel>
> 
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk? Priv: PDalgd at gmail.com
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel 
> <https://stat.ethz.ch/mailman/listinfo/r-devel>
>


From m|n@h@|| @end|ng |rom um|ch@edu  Tue Aug 31 12:13:51 2021
From: m|n@h@|| @end|ng |rom um|ch@edu (Greg Minshall)
Date: Tue, 31 Aug 2021 13:13:51 +0300
Subject: [Rd] order of operations
In-Reply-To: Your message of "Fri, 27 Aug 2021 13:57:33 -0400."
 <CAP01uRksX11gsW9BCSW-CTw+TVHf2RGrmP5fCoyUYRFC20pfFA@mail.gmail.com>
Message-ID: <637409.1630404831@apollo2.minshall.org>

Gabor Grothendieck <ggrothendieck at gmail.com> wrote:

> ... and maybe not having a guarantee would simplify implementation?

+1 for: "The results of such statements are not defined.", or something
to that effect.  (Erasmus had something to say here. :)


