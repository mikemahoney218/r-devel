From dietm@r@schi@dler m@ili@g off m@@rol@@d-web@com  Wed Aug  1 08:48:50 2018
From: dietm@r@schi@dler m@ili@g off m@@rol@@d-web@com (dietm@r@schi@dler m@ili@g off m@@rol@@d-web@com)
Date: Wed, 1 Aug 2018 06:48:50 +0000
Subject: [Rd] RFC: make as.difftime more consistent or convenient
In-Reply-To: <9755E752-FAD8-46D5-8712-077557E583BF@dans.knaw.nl>
References: <9755E752-FAD8-46D5-8712-077557E583BF@dans.knaw.nl>
Message-ID: <17046459260845ffaca8eaf83263c560@AUSMXMBX04.mrws.biz>

Hello!

you, Emil Bode <emil.bode at dans.knaw.nl>, wrote on Tuesday, July 31, 2018 1:55 PM:
> Some of the changes you're proposing could be made (with effort), but note that you're not
> restricted to providing strings with a format.
> What you're trying to do can be accomplished with as.difftime(12, units='weeks'), see also
> ?as.difftime
>
> Or if you're stuck with the strings: as.difftime(as.numeric(substring('12 w', 1, 2)),
> units='weeks')
> That also seems clearer, because in your script, the last part of your string simply gets
> ignored: as.difftime("12 h", "%H") reads your string for something that satisfies %H (which
> 12 does), then stops:
> as.difftime("12 hours and 17 minutes", "%H") gives 12 hours. If you wanted to check, you
> could have used as.difftime("12 h", "%H h")

Thank you for your comments! But, what you wrote is known. What do you want to express with regard to my questions?

I wrote:
> ? there is no appropriate format ...,
> although "weeks" is a legitimate unit of 'difftime':
>
>     > as.difftime("12 w", "%...")
>
>     1. What do you think about making the behavior of 'as.difftime' more consistent by
> accepting also formats for "days" and "weeks"?
>
>     2. Even more convenient it could be if 'as.difftime' accepted strings containing
> magnitude and unit; how about:
>
>     > as.difftime("12 weeks")
>     Time difference of 12 weeks

I'm asking for comments in order to get feedback whether it would be well-received if I proposed or provided the former or the latter change on Bugzilla.
--
Best regards,
Dietmar Schindler
________________________________
manroland web systems NewCo GmbH | Managing Director: Alexander Wassermann
Registered Office: Augsburg | Trade Register: AG Augsburg | HRB-No.: 32609 | VAT: DE815764857

Confidentiality note:
This eMail and any files transmitted with it are confidential and intended solely for the use of the individual or entity to whom they are addressed. If you are not the intended recipient, you are hereby notified that any use or dissemination of this communication is strictly prohibited. If you have received this eMail in error, then please delete this eMail.
________________________________

From m@echler @ending from @t@t@m@th@ethz@ch  Fri Aug  3 12:22:03 2018
From: m@echler @ending from @t@t@m@th@ethz@ch (Martin Maechler)
Date: Fri, 3 Aug 2018 12:22:03 +0200
Subject: [Rd] possible bug in plot.intervals.lmList
In-Reply-To: <406904256.20180728184459@medstat.hu>
References: <406904256.20180728184459@medstat.hu>
Message-ID: <23396.11467.808459.764041@stat.math.ethz.ch>

>>>>> Ferenci Tamas 
>>>>>     on Sat, 28 Jul 2018 18:44:59 +0200 writes:

    > Dear R-devel members,
    > I think I've found a minor bug in plot.intervals.lmList. ( The guide
    > https://www.r-project.org/bugs.html suggests to report it here, as I
    > do not have a bugzilla account.)

 [Just for people reading this:
  alternatively you could've asked here (or in private) to get a
  bugzilla account.] 

    > Here is a minimal reproducible example to demonstrate the problem:

    > fm1 <- lmList(distance ~ age | Subject, Orthodont)
    > plot(intervals(fm1),ylab="a")

    > This results in: "Error in bwplot.formula(x = group ~ intervals |
    > what, data = list(group = c(1L,  :   formal argument "ylab" matched by
    > multiple actual arguments".

    > I am not an experienced R programmer at that level, but I think the
    > problem is due to the fact that ylab is explicitly passed as an
    > argument to dotplot *and* the ... is also passed (which will also
    > include ylab in the above example).

    > Tamas

Exactly. Your diagnosis is correct, and it definitely is a
buglet in package 'nlme' (maintained by R Core, currently).

I've committed a bug fix (and related "cleanup cosmetics") to
the nlme package sources which  are maintained in subversion / svn at
  https://svn.r-project.org/R-packages/trunk/nlme/

Thank you for your precise and reproducible bug report!

Martin Maechler
ETH Zurich and R Core Team


From S@Elli@on @ending from LGCGroup@com  Fri Aug  3 16:23:11 2018
From: S@Elli@on @ending from LGCGroup@com (S Ellison)
Date: Fri, 3 Aug 2018 14:23:11 +0000
Subject: [Rd] RFC: make as.difftime more consistent or convenient
In-Reply-To: <17046459260845ffaca8eaf83263c560@AUSMXMBX04.mrws.biz>
References: <9755E752-FAD8-46D5-8712-077557E583BF@dans.knaw.nl>
 <17046459260845ffaca8eaf83263c560@AUSMXMBX04.mrws.biz>
Message-ID: <e5adbf667132445bb097dbea8676a174@GBDCVPEXC08.corp.lgc-group.com>

> Thank you for your comments! But, what you wrote is known. What do you
> want to express with regard to my questions?
> 
> I wrote:
> > ? there is no appropriate format ...,
> > although "weeks" is a legitimate unit of 'difftime':
> >
> >     > as.difftime("12 w", "%...")
> >
> >     > as.difftime("12 weeks")
> >     Time difference of 12 weeks
> >
> >     1. What do you think about making the behavior of 'as.difftime' more
> >   consistent by accepting also formats for "days" and "weeks"?

as.difftime calls strptime to apply the format argument.

If I  wanted to extend the range of formats as.difftime accepts, I'd leave as.difftime alone and look at how strptime could be extended to cover the formats you envisage.

But... I wouldn?t do that either. strptime is essentially a call to an .Internal function and very likely reliant on established  C code for the already very flexible standard C function strptime, which strptime clearly mirrors intentionally. That usually makes things dangerous to tinker with in the short term and hard to maintain in the long term.

So  if you want to do something that will readily convert all combinations of things like '12 w', '12W', '12wks', '3m 2d', 1wk 2d', '18d' etc, write that as a stand-alone routine that converts those 'simple' formats directly to difftime objects and call it something like 'strpdifftime', which would allow it to be added (if it's wanted a lot) with minimal impact to existing code.

S Ellison




*******************************************************************
This email and any attachments are confidential. Any use, copying or
disclosure other than by the intended recipient is unauthorised. If 
you have received this message in error, please notify the sender 
immediately via +44(0)20 8943 7000 or notify postmaster at lgcgroup.com 
and delete this message and any copies from your computer and network. 
LGC Limited. Registered in England 2991879. 
Registered office: Queens Road, Teddington, Middlesex, TW11 0LY, UK

From Jorgen@H@rm@e @ending from @@m@club@com  Fri Aug  3 18:22:14 2018
From: Jorgen@H@rm@e @ending from @@m@club@com (Jorgen Harmse)
Date: Fri, 3 Aug 2018 16:22:14 +0000
Subject: [Rd] glm Argument-Evaluation Does Not Match Documentation.
Message-ID: <DDBA0F39-9AB6-4484-9ADD-039D7FDD95B6@samsclub.com>

Details in documentation: "All of ?weights?, ?subset?, ?offset?, ?etastart? and ?mustart? are evaluated in the same way as variables in ?formula?, that is first in ?data? and then in the environment of ?formula?."
In fact, `data` is usually not an environment, and I have not seen arguments evaluated in `environment(formula)` when `data` is provided. (Information in `environment(formula)` is used, so presumably they are evaluated in an environment whose parent is `environment(formula)`.)


R --vanilla

R version 3.4.3 (2017-11-30) -- "Kite-Eating Tree"
Copyright (C) 2017 The R Foundation for Statistical Computing
Platform: x86_64-apple-darwin15.6.0 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> environment(glm)
<environment: namespace:stats>
> maintainer('stats')
[1] "R Core Team <R-core at r-project.org>"
> df <- data.frame(x=1:3, y=c(pi,7,sqrt(2)), z=c(1,2.3,3))
> a <- 4:6; b <- 7:9
> lm <- glm(z ~ x+y, data=df, weights = a*b -> v)
> v # Check whether weights was evaluated in .GlobalEnv, the environment of the formula.
Error: object 'v' not found
> env.df <- as.environment(df); lm <- glm(z ~ x+y, data=env.df, weights = a*b -> v)
Error in list(z, x, y) : could not find function "list"
> parent.env(env.df) <- .GlobalEnv; lm <- glm(z ~ x+y, data=env.df, weights = a*b -> v)
> v
Error: object 'v' not found
> ls(env.df) # show that weights was evaluated in env.df
[1] "v" "x" "y" "z"


Suggested rewrite: "All of ?weights?, ?subset?, ?offset?, ?etastart? and ?mustart? are evaluated in the same way as variables in ?formula?. Values needed are found first in `data` and then usually in the environment of ?formula? & its ancestors. (If `data` is provided but is not an environment then expressions are evaluated in an environment with content taken from `data` whose parent is `environment(formula)`.) Side effects of evaluating the arguments will usually NOT occur in the caller's frame."
I think this is correct, but what really happens is inside other functions with unusual argument evaluation, so I'm not certain.


Regards,
Jorgen Harmse
 
Sam?s Club Technology
Phone 512.633.2226 
jorgen.harmse at samsclub.com
 
 
This e-mail and any files transmitted with it are confidential and intended solely
for the individual or entity to whom they are addressed.  If you have received
this e-mail in error, destroy it immediately.  Wal-Mart Confidential.
 


From ruipb@rr@d@@ @ending from @@po@pt  Sat Aug  4 15:32:31 2018
From: ruipb@rr@d@@ @ending from @@po@pt (Rui Barradas)
Date: Sat, 4 Aug 2018 14:32:31 +0100
Subject: [Rd] Is this a bug in `[`?
Message-ID: <4cf085ac-8f7e-38b5-ea02-302df94ad7b5@sapo.pt>

Hello,

Maybe I am not understanding how negative indexing works but

1) This is right.

(1:10)[-1]
#[1]  2  3  4  5  6  7  8  9 10

2) Are these right? They are at least surprising to me.

(1:10)[-0]
#integer(0)

(1:10)[-seq_len(0)]
#integer(0)


It was the last example that made me ask, seq_len(0) whould avoid an 
if/else or something similar.


Thanks in advance,

Rui Barradas


From d@vidhughjone@ @ending from gm@il@com  Sat Aug  4 15:55:34 2018
From: d@vidhughjone@ @ending from gm@il@com (David Hugh-Jones)
Date: Sat, 4 Aug 2018 14:55:34 +0100
Subject: [Rd] Puzzle or bug with matrix indexing
Message-ID: <CAARY7kgosh2XgT+DDg9Y7CQwERdwTrn7AO1MAvV-tS0e=_O0SA@mail.gmail.com>

I'm not sure why this is happening:

tmp <- data.frame(
  a = letters[1:2],
  b=c(TRUE, FALSE),
  stringsAsFactors = FALSE
)
idx <- matrix(c(1, 2, 2, 2), 2, byrow = TRUE)
tmp[idx]

[1] " TRUE" "FALSE"

Notice there is a space before the TRUE: " TRUE".

This space isn't happening purely because of coercion:

c("blah", TRUE, FALSE)
[1] "blah"  "TRUE"  "FALSE"

No space.

Obviously:
as.logical(tmp[idx])
[1]    NA FALSE

which is why I think this might be a bug.

David

	[[alternative HTML version deleted]]


From profjcn@@h @ending from gm@il@com  Sat Aug  4 16:12:04 2018
From: profjcn@@h @ending from gm@il@com (J C Nash)
Date: Sat, 4 Aug 2018 10:12:04 -0400
Subject: [Rd] typo in Ubuntu download page
Message-ID: <926d7eab-89ee-1565-f9cd-e6574a06b702@gmail.com>

In https://cran.r-project.org/bin/linux/ubuntu/

Administration and Maintances of R Packages
                   ^^^^^^^^^^

Minor stuff, but if someone who can edit is on the page,
perhaps it can be changed to "Maintenance"

Best, JN


From i@uc@r86 @ending from gm@il@com  Sat Aug  4 16:51:42 2018
From: i@uc@r86 @ending from gm@il@com (=?UTF-8?B?ScOxYWtpIMOaY2Fy?=)
Date: Sat, 4 Aug 2018 16:51:42 +0200
Subject: [Rd] Is this a bug in `[`?
In-Reply-To: <4cf085ac-8f7e-38b5-ea02-302df94ad7b5@sapo.pt>
References: <4cf085ac-8f7e-38b5-ea02-302df94ad7b5@sapo.pt>
Message-ID: <CALEXWq2LxX7TUS5ia2R3o6GwxdkDZe_PZ=UZ1zfFWLTY1OTS=g@mail.gmail.com>

El s?b., 4 ago. 2018 a las 15:32, Rui Barradas
(<ruipbarradas at sapo.pt>) escribi?:
>
> Hello,
>
> Maybe I am not understanding how negative indexing works but
>
> 1) This is right.
>
> (1:10)[-1]
> #[1]  2  3  4  5  6  7  8  9 10
>
> 2) Are these right? They are at least surprising to me.
>
> (1:10)[-0]
> #integer(0)
>
> (1:10)[-seq_len(0)]
> #integer(0)
>
>
> It was the last example that made me ask, seq_len(0) whould avoid an
> if/else or something similar.

I think it's ok, because there is no negative zero integer, so -0 is 0.

1.0/-0L # Inf
1.0/-0.0 # - Inf

And the same can be said for integer(0), which is the result of
seq_len(0): there is no negative empty integer.

I?aki

>
>
> Thanks in advance,
>
> Rui Barradas
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From ruipb@rr@d@@ @ending from @@po@pt  Sat Aug  4 17:58:13 2018
From: ruipb@rr@d@@ @ending from @@po@pt (Rui Barradas)
Date: Sat, 4 Aug 2018 16:58:13 +0100
Subject: [Rd] Is this a bug in `[`?
In-Reply-To: <CALEXWq2LxX7TUS5ia2R3o6GwxdkDZe_PZ=UZ1zfFWLTY1OTS=g@mail.gmail.com>
References: <4cf085ac-8f7e-38b5-ea02-302df94ad7b5@sapo.pt>
 <CALEXWq2LxX7TUS5ia2R3o6GwxdkDZe_PZ=UZ1zfFWLTY1OTS=g@mail.gmail.com>
Message-ID: <b36d8e0e-431a-e1a5-bbe4-227351fd8c70@sapo.pt>



?s 15:51 de 04/08/2018, I?aki ?car escreveu:
> El s?b., 4 ago. 2018 a las 15:32, Rui Barradas
> (<ruipbarradas at sapo.pt>) escribi?:
>>
>> Hello,
>>
>> Maybe I am not understanding how negative indexing works but
>>
>> 1) This is right.
>>
>> (1:10)[-1]
>> #[1]  2  3  4  5  6  7  8  9 10
>>
>> 2) Are these right? They are at least surprising to me.
>>
>> (1:10)[-0]
>> #integer(0)
>>
>> (1:10)[-seq_len(0)]
>> #integer(0)
>>
>>
>> It was the last example that made me ask, seq_len(0) whould avoid an
>> if/else or something similar.
> 
> I think it's ok, because there is no negative zero integer, so -0 is 0.

Ok, this makes sense, I should have thought about that.

> 
> 1.0/-0L # Inf
> 1.0/-0.0 # - Inf
> 
> And the same can be said for integer(0), which is the result of
> seq_len(0): there is no negative empty integer.

I'm not completely convinced about this one, though.
I would expect -seq_len(n) to remove the first n elements from the 
vector, therefore, when n == 0, it would remove none.

And integer(0) is not the same as 0.

(1:10)[-0] == (1:10)[0] == integer(0) # empty

(1:10)[-seq_len(0)] == (1:10)[-integer(0)]


And I have just reminded myself to run

identical(-integer(0), integer(0))

It returns TRUE so my intuition is wrong, R is right.
End of story.

Thanks for the help,

Rui Barradas

> 
> I?aki
> 
>>
>>
>> Thanks in advance,
>>
>> Rui Barradas
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel


From ccberry @ending from uc@d@edu  Sat Aug  4 18:53:30 2018
From: ccberry @ending from uc@d@edu (Berry, Charles)
Date: Sat, 4 Aug 2018 16:53:30 +0000
Subject: [Rd] Puzzle or bug with matrix indexing
In-Reply-To: <CAARY7kgosh2XgT+DDg9Y7CQwERdwTrn7AO1MAvV-tS0e=_O0SA@mail.gmail.com>
References: <CAARY7kgosh2XgT+DDg9Y7CQwERdwTrn7AO1MAvV-tS0e=_O0SA@mail.gmail.com>
Message-ID: <C1C93F6E-FDD1-4687-BC45-4671C90B548F@ucsd.edu>



> On Aug 4, 2018, at 6:55 AM, David Hugh-Jones <davidhughjones at gmail.com> wrote:
> 
> I'm not sure why this is happening:
> 
> tmp <- data.frame(
>  a = letters[1:2],
>  b=c(TRUE, FALSE),
>  stringsAsFactors = FALSE
> )
> idx <- matrix(c(1, 2, 2, 2), 2, byrow = TRUE)
> tmp[idx]
> 
> [1] " TRUE" "FALSE"
> 

>From ?"[.data.frame"

Extract.data.frame {base}
.
.
.
Matrix indexing (x[i] with a logical or a 2-column integer matrix i) using [ is not recommended. For extraction, x is first coerced to a matrix.

[...]

Thus, something like 

	as.matrix(tmp)

happens converting every column to character. Dig deeper by reading

?matrix

and see the paragraph on `as.matrix' under `Details'.

HTH,

Chuck

From d@vidhughjone@ @ending from gm@il@com  Sat Aug  4 21:47:17 2018
From: d@vidhughjone@ @ending from gm@il@com (David Hugh-Jones)
Date: Sat, 4 Aug 2018 20:47:17 +0100
Subject: [Rd] Puzzle or bug with matrix indexing
In-Reply-To: <C1C93F6E-FDD1-4687-BC45-4671C90B548F@ucsd.edu>
References: <CAARY7kgosh2XgT+DDg9Y7CQwERdwTrn7AO1MAvV-tS0e=_O0SA@mail.gmail.com>
 <C1C93F6E-FDD1-4687-BC45-4671C90B548F@ucsd.edu>
Message-ID: <CAARY7kimfojC=_UCnMczyOTHDMaFXyOoHKFSoPpWr4xWGdMJpg@mail.gmail.com>

Yup, I worked it out in time... for future reference, as.matrix calls
`format` on logicals, converting them to the form seen.

On Sat, 4 Aug 2018 at 17:53, Berry, Charles <ccberry at ucsd.edu> wrote:

>
>
> > On Aug 4, 2018, at 6:55 AM, David Hugh-Jones <davidhughjones at gmail.com>
> wrote:
> >
> > I'm not sure why this is happening:
> >
> > tmp <- data.frame(
> >  a = letters[1:2],
> >  b=c(TRUE, FALSE),
> >  stringsAsFactors = FALSE
> > )
> > idx <- matrix(c(1, 2, 2, 2), 2, byrow = TRUE)
> > tmp[idx]
> >
> > [1] " TRUE" "FALSE"
> >
>
> From ?"[.data.frame"
>
> Extract.data.frame {base}
> .
> .
> .
> Matrix indexing (x[i] with a logical or a 2-column integer matrix i) using
> [ is not recommended. For extraction, x is first coerced to a matrix.
>
> [...]
>
> Thus, something like
>
>         as.matrix(tmp)
>
> happens converting every column to character. Dig deeper by reading
>
> ?matrix
>
> and see the paragraph on `as.matrix' under `Details'.
>
> HTH,
>
> Chuck
>
-- 
Sent from Gmail Mobile

	[[alternative HTML version deleted]]


From kmbell56 @ending from gm@il@com  Sun Aug  5 06:26:54 2018
From: kmbell56 @ending from gm@il@com (Kenny Bell)
Date: Sun, 5 Aug 2018 16:26:54 +1200
Subject: [Rd] Is this a bug in `[`?
In-Reply-To: <b36d8e0e-431a-e1a5-bbe4-227351fd8c70@sapo.pt>
References: <4cf085ac-8f7e-38b5-ea02-302df94ad7b5@sapo.pt>
 <CALEXWq2LxX7TUS5ia2R3o6GwxdkDZe_PZ=UZ1zfFWLTY1OTS=g@mail.gmail.com>
 <b36d8e0e-431a-e1a5-bbe4-227351fd8c70@sapo.pt>
Message-ID: <CAPekMCkyF5f90dXLh6WJZwqXpKPbrg2sLrN-DdjqAoxXGbCtXA@mail.gmail.com>

This should more clearly illustrate the issue:

c(1, 2, 3, 4)[-seq_len(4)]
#> numeric(0)
c(1, 2, 3, 4)[-seq_len(3)]
#> [1] 4
c(1, 2, 3, 4)[-seq_len(2)]
#> [1] 3 4
c(1, 2, 3, 4)[-seq_len(1)]
#> [1] 2 3 4
c(1, 2, 3, 4)[-seq_len(0)]
#> numeric(0)
Created on 2018-08-05 by the reprex package (v0.2.0.9000).

On Sun, Aug 5, 2018 at 3:58 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:

>
>
> ?s 15:51 de 04/08/2018, I?aki ?car escreveu:
> > El s?b., 4 ago. 2018 a las 15:32, Rui Barradas
> > (<ruipbarradas at sapo.pt>) escribi?:
> >>
> >> Hello,
> >>
> >> Maybe I am not understanding how negative indexing works but
> >>
> >> 1) This is right.
> >>
> >> (1:10)[-1]
> >> #[1]  2  3  4  5  6  7  8  9 10
> >>
> >> 2) Are these right? They are at least surprising to me.
> >>
> >> (1:10)[-0]
> >> #integer(0)
> >>
> >> (1:10)[-seq_len(0)]
> >> #integer(0)
> >>
> >>
> >> It was the last example that made me ask, seq_len(0) whould avoid an
> >> if/else or something similar.
> >
> > I think it's ok, because there is no negative zero integer, so -0 is 0.
>
> Ok, this makes sense, I should have thought about that.
>
> >
> > 1.0/-0L # Inf
> > 1.0/-0.0 # - Inf
> >
> > And the same can be said for integer(0), which is the result of
> > seq_len(0): there is no negative empty integer.
>
> I'm not completely convinced about this one, though.
> I would expect -seq_len(n) to remove the first n elements from the
> vector, therefore, when n == 0, it would remove none.
>
> And integer(0) is not the same as 0.
>
> (1:10)[-0] == (1:10)[0] == integer(0) # empty
>
> (1:10)[-seq_len(0)] == (1:10)[-integer(0)]
>
>
> And I have just reminded myself to run
>
> identical(-integer(0), integer(0))
>
> It returns TRUE so my intuition is wrong, R is right.
> End of story.
>
> Thanks for the help,
>
> Rui Barradas
>
> >
> > I?aki
> >
> >>
> >>
> >> Thanks in advance,
> >>
> >> Rui Barradas
> >>
> >> ______________________________________________
> >> R-devel at r-project.org mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-devel
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From ruipb@rr@d@@ @ending from @@po@pt  Sun Aug  5 07:57:38 2018
From: ruipb@rr@d@@ @ending from @@po@pt (Rui Barradas)
Date: Sun, 5 Aug 2018 06:57:38 +0100
Subject: [Rd] Is this a bug in `[`?
In-Reply-To: <CAPekMCkyF5f90dXLh6WJZwqXpKPbrg2sLrN-DdjqAoxXGbCtXA@mail.gmail.com>
References: <4cf085ac-8f7e-38b5-ea02-302df94ad7b5@sapo.pt>
 <CALEXWq2LxX7TUS5ia2R3o6GwxdkDZe_PZ=UZ1zfFWLTY1OTS=g@mail.gmail.com>
 <b36d8e0e-431a-e1a5-bbe4-227351fd8c70@sapo.pt>
 <CAPekMCkyF5f90dXLh6WJZwqXpKPbrg2sLrN-DdjqAoxXGbCtXA@mail.gmail.com>
Message-ID: <bd6150f0-c23d-741f-13c0-cf0f31228b2b@sapo.pt>

Thanks.
This is exactly the doubt I had.

Rui Barradas

?s 05:26 de 05/08/2018, Kenny Bell escreveu:
> This should more clearly illustrate the issue:
> 
> c(1, 2, 3, 4)[-seq_len(4)]
> #> numeric(0)
> c(1, 2, 3, 4)[-seq_len(3)]
> #> [1] 4
> c(1, 2, 3, 4)[-seq_len(2)]
> #> [1] 3 4
> c(1, 2, 3, 4)[-seq_len(1)]
> #> [1] 2 3 4
> c(1, 2, 3, 4)[-seq_len(0)]
> #> numeric(0)
> Created on 2018-08-05 by the reprex package (v0.2.0.9000).
> 
> On Sun, Aug 5, 2018 at 3:58 AM Rui Barradas <ruipbarradas at sapo.pt 
> <mailto:ruipbarradas at sapo.pt>> wrote:
> 
> 
> 
>     ?s 15:51 de 04/08/2018, I?aki ?car escreveu:
>      > El s?b., 4 ago. 2018 a las 15:32, Rui Barradas
>      > (<ruipbarradas at sapo.pt <mailto:ruipbarradas at sapo.pt>>) escribi?:
>      >>
>      >> Hello,
>      >>
>      >> Maybe I am not understanding how negative indexing works but
>      >>
>      >> 1) This is right.
>      >>
>      >> (1:10)[-1]
>      >> #[1]? 2? 3? 4? 5? 6? 7? 8? 9 10
>      >>
>      >> 2) Are these right? They are at least surprising to me.
>      >>
>      >> (1:10)[-0]
>      >> #integer(0)
>      >>
>      >> (1:10)[-seq_len(0)]
>      >> #integer(0)
>      >>
>      >>
>      >> It was the last example that made me ask, seq_len(0) whould avoid an
>      >> if/else or something similar.
>      >
>      > I think it's ok, because there is no negative zero integer, so -0
>     is 0.
> 
>     Ok, this makes sense, I should have thought about that.
> 
>      >
>      > 1.0/-0L # Inf
>      > 1.0/-0.0 # - Inf
>      >
>      > And the same can be said for integer(0), which is the result of
>      > seq_len(0): there is no negative empty integer.
> 
>     I'm not completely convinced about this one, though.
>     I would expect -seq_len(n) to remove the first n elements from the
>     vector, therefore, when n == 0, it would remove none.
> 
>     And integer(0) is not the same as 0.
> 
>     (1:10)[-0] == (1:10)[0] == integer(0) # empty
> 
>     (1:10)[-seq_len(0)] == (1:10)[-integer(0)]
> 
> 
>     And I have just reminded myself to run
> 
>     identical(-integer(0), integer(0))
> 
>     It returns TRUE so my intuition is wrong, R is right.
>     End of story.
> 
>     Thanks for the help,
> 
>     Rui Barradas
> 
>      >
>      > I?aki
>      >
>      >>
>      >>
>      >> Thanks in advance,
>      >>
>      >> Rui Barradas
>      >>
>      >> ______________________________________________
>      >> R-devel at r-project.org <mailto:R-devel at r-project.org> mailing list
>      >> https://stat.ethz.ch/mailman/listinfo/r-devel
> 
>     ______________________________________________
>     R-devel at r-project.org <mailto:R-devel at r-project.org> mailing list
>     https://stat.ethz.ch/mailman/listinfo/r-devel
>


From pburn@ @ending from pburn@@@e@net@com  Sun Aug  5 09:46:06 2018
From: pburn@ @ending from pburn@@@e@net@com (Patrick Burns)
Date: Sun, 5 Aug 2018 08:46:06 +0100
Subject: [Rd] Is this a bug in `[`?
In-Reply-To: <bd6150f0-c23d-741f-13c0-cf0f31228b2b@sapo.pt>
References: <4cf085ac-8f7e-38b5-ea02-302df94ad7b5@sapo.pt>
 <CALEXWq2LxX7TUS5ia2R3o6GwxdkDZe_PZ=UZ1zfFWLTY1OTS=g@mail.gmail.com>
 <b36d8e0e-431a-e1a5-bbe4-227351fd8c70@sapo.pt>
 <CAPekMCkyF5f90dXLh6WJZwqXpKPbrg2sLrN-DdjqAoxXGbCtXA@mail.gmail.com>
 <bd6150f0-c23d-741f-13c0-cf0f31228b2b@sapo.pt>
Message-ID: <90106d5c-4af6-b752-89f6-ca67030dfa46@pburns.seanet.com>

This is Circle 8..1.13 of the R Inferno.


On 05/08/2018 06:57, Rui Barradas wrote:
> Thanks.
> This is exactly the doubt I had.
> 
> Rui Barradas
> 
> ?s 05:26 de 05/08/2018, Kenny Bell escreveu:
>> This should more clearly illustrate the issue:
>>
>> c(1, 2, 3, 4)[-seq_len(4)]
>> #> numeric(0)
>> c(1, 2, 3, 4)[-seq_len(3)]
>> #> [1] 4
>> c(1, 2, 3, 4)[-seq_len(2)]
>> #> [1] 3 4
>> c(1, 2, 3, 4)[-seq_len(1)]
>> #> [1] 2 3 4
>> c(1, 2, 3, 4)[-seq_len(0)]
>> #> numeric(0)
>> Created on 2018-08-05 by the reprex package (v0.2.0.9000).
>>
>> On Sun, Aug 5, 2018 at 3:58 AM Rui Barradas <ruipbarradas at sapo.pt 
>> <mailto:ruipbarradas at sapo.pt>> wrote:
>>
>>
>>
>> ??? ?s 15:51 de 04/08/2018, I?aki ?car escreveu:
>> ???? > El s?b., 4 ago. 2018 a las 15:32, Rui Barradas
>> ???? > (<ruipbarradas at sapo.pt <mailto:ruipbarradas at sapo.pt>>) escribi?:
>> ???? >>
>> ???? >> Hello,
>> ???? >>
>> ???? >> Maybe I am not understanding how negative indexing works but
>> ???? >>
>> ???? >> 1) This is right.
>> ???? >>
>> ???? >> (1:10)[-1]
>> ???? >> #[1]? 2? 3? 4? 5? 6? 7? 8? 9 10
>> ???? >>
>> ???? >> 2) Are these right? They are at least surprising to me.
>> ???? >>
>> ???? >> (1:10)[-0]
>> ???? >> #integer(0)
>> ???? >>
>> ???? >> (1:10)[-seq_len(0)]
>> ???? >> #integer(0)
>> ???? >>
>> ???? >>
>> ???? >> It was the last example that made me ask, seq_len(0) whould 
>> avoid an
>> ???? >> if/else or something similar.
>> ???? >
>> ???? > I think it's ok, because there is no negative zero integer, so -0
>> ??? is 0.
>>
>> ??? Ok, this makes sense, I should have thought about that.
>>
>> ???? >
>> ???? > 1.0/-0L # Inf
>> ???? > 1.0/-0.0 # - Inf
>> ???? >
>> ???? > And the same can be said for integer(0), which is the result of
>> ???? > seq_len(0): there is no negative empty integer.
>>
>> ??? I'm not completely convinced about this one, though.
>> ??? I would expect -seq_len(n) to remove the first n elements from the
>> ??? vector, therefore, when n == 0, it would remove none.
>>
>> ??? And integer(0) is not the same as 0.
>>
>> ??? (1:10)[-0] == (1:10)[0] == integer(0) # empty
>>
>> ??? (1:10)[-seq_len(0)] == (1:10)[-integer(0)]
>>
>>
>> ??? And I have just reminded myself to run
>>
>> ??? identical(-integer(0), integer(0))
>>
>> ??? It returns TRUE so my intuition is wrong, R is right.
>> ??? End of story.
>>
>> ??? Thanks for the help,
>>
>> ??? Rui Barradas
>>
>> ???? >
>> ???? > I?aki
>> ???? >
>> ???? >>
>> ???? >>
>> ???? >> Thanks in advance,
>> ???? >>
>> ???? >> Rui Barradas
>> ???? >>
>> ???? >> ______________________________________________
>> ???? >> R-devel at r-project.org <mailto:R-devel at r-project.org> mailing list
>> ???? >> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>> ??? ______________________________________________
>> ??? R-devel at r-project.org <mailto:R-devel at r-project.org> mailing list
>> ??? https://stat.ethz.ch/mailman/listinfo/r-devel
>>
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 

-- 
Patrick Burns
pburns at pburns.seanet.com
twitter: @burnsstat @portfolioprobe
http://www.portfolioprobe.com/blog
http://www.burns-stat.com
(home of:
  'Impatient R'
  'The R Inferno'
  'Tao Te Programming')


From ruipb@rr@d@@ @ending from @@po@pt  Sun Aug  5 10:45:39 2018
From: ruipb@rr@d@@ @ending from @@po@pt (Rui Barradas)
Date: Sun, 5 Aug 2018 09:45:39 +0100
Subject: [Rd] Is this a bug in `[`?
In-Reply-To: <90106d5c-4af6-b752-89f6-ca67030dfa46@pburns.seanet.com>
References: <4cf085ac-8f7e-38b5-ea02-302df94ad7b5@sapo.pt>
 <CALEXWq2LxX7TUS5ia2R3o6GwxdkDZe_PZ=UZ1zfFWLTY1OTS=g@mail.gmail.com>
 <b36d8e0e-431a-e1a5-bbe4-227351fd8c70@sapo.pt>
 <CAPekMCkyF5f90dXLh6WJZwqXpKPbrg2sLrN-DdjqAoxXGbCtXA@mail.gmail.com>
 <bd6150f0-c23d-741f-13c0-cf0f31228b2b@sapo.pt>
 <90106d5c-4af6-b752-89f6-ca67030dfa46@pburns.seanet.com>
Message-ID: <1419079d-390e-fe88-365d-9f91232c6508@sapo.pt>

Thanks,

This is what I needed.
I had read the R Inferno a long time ago and apparently forgot this one.

Rui Barradas

?s 08:46 de 05/08/2018, Patrick Burns escreveu:
> This is Circle 8..1.13 of the R Inferno.
> 
> 
> On 05/08/2018 06:57, Rui Barradas wrote:
>> Thanks.
>> This is exactly the doubt I had.
>>
>> Rui Barradas
>>
>> ?s 05:26 de 05/08/2018, Kenny Bell escreveu:
>>> This should more clearly illustrate the issue:
>>>
>>> c(1, 2, 3, 4)[-seq_len(4)]
>>> #> numeric(0)
>>> c(1, 2, 3, 4)[-seq_len(3)]
>>> #> [1] 4
>>> c(1, 2, 3, 4)[-seq_len(2)]
>>> #> [1] 3 4
>>> c(1, 2, 3, 4)[-seq_len(1)]
>>> #> [1] 2 3 4
>>> c(1, 2, 3, 4)[-seq_len(0)]
>>> #> numeric(0)
>>> Created on 2018-08-05 by the reprex package (v0.2.0.9000).
>>>
>>> On Sun, Aug 5, 2018 at 3:58 AM Rui Barradas <ruipbarradas at sapo.pt 
>>> <mailto:ruipbarradas at sapo.pt>> wrote:
>>>
>>>
>>>
>>> ??? ?s 15:51 de 04/08/2018, I?aki ?car escreveu:
>>> ???? > El s?b., 4 ago. 2018 a las 15:32, Rui Barradas
>>> ???? > (<ruipbarradas at sapo.pt <mailto:ruipbarradas at sapo.pt>>) escribi?:
>>> ???? >>
>>> ???? >> Hello,
>>> ???? >>
>>> ???? >> Maybe I am not understanding how negative indexing works but
>>> ???? >>
>>> ???? >> 1) This is right.
>>> ???? >>
>>> ???? >> (1:10)[-1]
>>> ???? >> #[1]? 2? 3? 4? 5? 6? 7? 8? 9 10
>>> ???? >>
>>> ???? >> 2) Are these right? They are at least surprising to me.
>>> ???? >>
>>> ???? >> (1:10)[-0]
>>> ???? >> #integer(0)
>>> ???? >>
>>> ???? >> (1:10)[-seq_len(0)]
>>> ???? >> #integer(0)
>>> ???? >>
>>> ???? >>
>>> ???? >> It was the last example that made me ask, seq_len(0) whould 
>>> avoid an
>>> ???? >> if/else or something similar.
>>> ???? >
>>> ???? > I think it's ok, because there is no negative zero integer, so -0
>>> ??? is 0.
>>>
>>> ??? Ok, this makes sense, I should have thought about that.
>>>
>>> ???? >
>>> ???? > 1.0/-0L # Inf
>>> ???? > 1.0/-0.0 # - Inf
>>> ???? >
>>> ???? > And the same can be said for integer(0), which is the result of
>>> ???? > seq_len(0): there is no negative empty integer.
>>>
>>> ??? I'm not completely convinced about this one, though.
>>> ??? I would expect -seq_len(n) to remove the first n elements from the
>>> ??? vector, therefore, when n == 0, it would remove none.
>>>
>>> ??? And integer(0) is not the same as 0.
>>>
>>> ??? (1:10)[-0] == (1:10)[0] == integer(0) # empty
>>>
>>> ??? (1:10)[-seq_len(0)] == (1:10)[-integer(0)]
>>>
>>>
>>> ??? And I have just reminded myself to run
>>>
>>> ??? identical(-integer(0), integer(0))
>>>
>>> ??? It returns TRUE so my intuition is wrong, R is right.
>>> ??? End of story.
>>>
>>> ??? Thanks for the help,
>>>
>>> ??? Rui Barradas
>>>
>>> ???? >
>>> ???? > I?aki
>>> ???? >
>>> ???? >>
>>> ???? >>
>>> ???? >> Thanks in advance,
>>> ???? >>
>>> ???? >> Rui Barradas
>>> ???? >>
>>> ???? >> ______________________________________________
>>> ???? >> R-devel at r-project.org <mailto:R-devel at r-project.org> mailing 
>>> list
>>> ???? >> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>
>>> ??? ______________________________________________
>>> ??? R-devel at r-project.org <mailto:R-devel at r-project.org> mailing list
>>> ??? https://stat.ethz.ch/mailman/listinfo/r-devel
>>>
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>


From i@uc@r86 @ending from gm@il@com  Sun Aug  5 12:42:30 2018
From: i@uc@r86 @ending from gm@il@com (=?UTF-8?B?ScOxYWtpIMOaY2Fy?=)
Date: Sun, 5 Aug 2018 12:42:30 +0200
Subject: [Rd] Is this a bug in `[`?
In-Reply-To: <CAPekMCkyF5f90dXLh6WJZwqXpKPbrg2sLrN-DdjqAoxXGbCtXA@mail.gmail.com>
References: <4cf085ac-8f7e-38b5-ea02-302df94ad7b5@sapo.pt>
 <CALEXWq2LxX7TUS5ia2R3o6GwxdkDZe_PZ=UZ1zfFWLTY1OTS=g@mail.gmail.com>
 <b36d8e0e-431a-e1a5-bbe4-227351fd8c70@sapo.pt>
 <CAPekMCkyF5f90dXLh6WJZwqXpKPbrg2sLrN-DdjqAoxXGbCtXA@mail.gmail.com>
Message-ID: <CALEXWq2gSrtWt3hZ4igMntTE62+jq4Vh0FDgkHuKp0hhK=ic7w@mail.gmail.com>

El dom., 5 ago. 2018 a las 6:27, Kenny Bell (<kmbell56 at gmail.com>) escribi?:
>
> This should more clearly illustrate the issue:
>
> c(1, 2, 3, 4)[-seq_len(4)]
> #> numeric(0)
> c(1, 2, 3, 4)[-seq_len(3)]
> #> [1] 4
> c(1, 2, 3, 4)[-seq_len(2)]
> #> [1] 3 4
> c(1, 2, 3, 4)[-seq_len(1)]
> #> [1] 2 3 4
> c(1, 2, 3, 4)[-seq_len(0)]
> #> numeric(0)
> Created on 2018-08-05 by the reprex package (v0.2.0.9000).

IMO, the problem is that you are reading it sequentially: "-" remove
"seq_" a sequence "len(0)" of length zero. But that's not how R works
(how programming languages work in general). Instead, the sequence is
evaluated in the first place, and then the sign may apply as long as
you provided something that can hold a sign. And an empty element has
no sign, so the sign is lost.

I?aki

>
> On Sun, Aug 5, 2018 at 3:58 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:
>>
>>
>>
>> ?s 15:51 de 04/08/2018, I?aki ?car escreveu:
>> > El s?b., 4 ago. 2018 a las 15:32, Rui Barradas
>> > (<ruipbarradas at sapo.pt>) escribi?:
>> >>
>> >> Hello,
>> >>
>> >> Maybe I am not understanding how negative indexing works but
>> >>
>> >> 1) This is right.
>> >>
>> >> (1:10)[-1]
>> >> #[1]  2  3  4  5  6  7  8  9 10
>> >>
>> >> 2) Are these right? They are at least surprising to me.
>> >>
>> >> (1:10)[-0]
>> >> #integer(0)
>> >>
>> >> (1:10)[-seq_len(0)]
>> >> #integer(0)
>> >>
>> >>
>> >> It was the last example that made me ask, seq_len(0) whould avoid an
>> >> if/else or something similar.
>> >
>> > I think it's ok, because there is no negative zero integer, so -0 is 0.
>>
>> Ok, this makes sense, I should have thought about that.
>>
>> >
>> > 1.0/-0L # Inf
>> > 1.0/-0.0 # - Inf
>> >
>> > And the same can be said for integer(0), which is the result of
>> > seq_len(0): there is no negative empty integer.
>>
>> I'm not completely convinced about this one, though.
>> I would expect -seq_len(n) to remove the first n elements from the
>> vector, therefore, when n == 0, it would remove none.
>>
>> And integer(0) is not the same as 0.
>>
>> (1:10)[-0] == (1:10)[0] == integer(0) # empty
>>
>> (1:10)[-seq_len(0)] == (1:10)[-integer(0)]
>>
>>
>> And I have just reminded myself to run
>>
>> identical(-integer(0), integer(0))
>>
>> It returns TRUE so my intuition is wrong, R is right.
>> End of story.
>>
>> Thanks for the help,
>>
>> Rui Barradas
>>
>> >
>> > I?aki
>> >
>> >>
>> >>
>> >> Thanks in advance,
>> >>
>> >> Rui Barradas
>> >>
>> >> ______________________________________________
>> >> R-devel at r-project.org mailing list
>> >> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel


From pd@lgd @ending from gm@il@com  Mon Aug  6 10:34:21 2018
From: pd@lgd @ending from gm@il@com (peter dalgaard)
Date: Mon, 6 Aug 2018 10:34:21 +0200
Subject: [Rd] [R] MASS::boxcox "object not found"
In-Reply-To: <6ed54e57.1067.1650a16a89d.Coremail.jszhao@yeah.net>
References: <6ed54e57.1067.1650a16a89d.Coremail.jszhao@yeah.net>
Message-ID: <22AD76FF-FBA8-4202-8193-9410171CD14B@gmail.com>

Hmm, this looks like a buglet/infelicity in update.lm rather than MASS::boxcox per se. Moving to R-devel.

I think the story is that update.lm eventually does

        eval(call, parent.frame())

where the call is extracted from the lm object, but call$formula is unevaluated, and does not contain environment information like formula(obj) would do. Then when the call is evaluated and parent.frame() differs from that of the original call, trouble ensues.

A workaround not involving <<- seems to be 

  ...
  obj <- lm(vec ~ 1)
  obj$call$formula <- formula(obj)
  lam <- boxcox(obj)
  ...

I'm not sure whether this is actually fixable. It is the kind of thing where you tend to discover that someone, somewhere has actually been relying on current behaviour...

-pd

> On 5 Aug 2018, at 14:36 , Jinsong Zhao <jszhao at yeah.net> wrote:
> 
> Hi there,
> 
> I wrote a function that wraps MASS::boxcox as:
> 
> bc <- function(vec) {
>   lam <- boxcox(lm(vec ~ 1))
>   lam <- lam$x[which.max(lam$y)]
>   (vec^lam - 1)/lam
> }
> 
> When I invoke it as:
> 
>> x <- runif(20)
>> bc(x)
> Error in eval(predvars, data, env) : object 'vec' not found
> 
> I have googled, and rewrote the above function as:
> 
> bc <- function(vec) {
>   dat <<- data.frame(vec = vec)
>   lam <- boxcox(lm(vec ~ 1, dat))
>   lam <- lam$x[which.max(lam$y)]
>   rm(dat, envir = .GlobalEnv)
>   (vec^lam - 1)/lam
> }
> 
> It works. But, I am wondering why MASS::boxcox have to wrap in such way that have to use the data in .GlobalEnv.
> 
> Best,
> Jinsong
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at r-project.org mailing list -- To UNSUBSCRIBE and more, see
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide http://www.R-project.org/posting-guide.html
> and provide commented, minimal, self-contained, reproducible code.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From i@uc@r86 @ending from gm@il@com  Mon Aug  6 17:35:48 2018
From: i@uc@r86 @ending from gm@il@com (=?UTF-8?B?ScOxYWtpIMOaY2Fy?=)
Date: Mon, 6 Aug 2018 17:35:48 +0200
Subject: [Rd] SIGSEGV in R_RunWeakRefFinalizer, object allocated with Rcpp
Message-ID: <CALEXWq2E+-k=Oka1YZAzGzYU1bLmcC3ptUUrv8RUMUrpcviK-g@mail.gmail.com>

Hi all,

I'm not sure if I'm not supposed to do the following (the dyn.unload
part, I mean) or this could be a bug (in R or Rcpp):

```
Rcpp::sourceCpp(code='
  #include <Rcpp.h>
  class Object {};
  //[[Rcpp::export]]
  SEXP new_object() {
    return Rcpp::XPtr<Object>(new Object());
  }'
)

new_object()
dyn.unload(list.files(tempdir(), ".(so|dll)$", recursive=TRUE, full.names=TRUE))
gc() # segfault in R_RunWeakRefFinalizer

message("This is not printed")
```

This is the backtrace I get with R 3.5.1:

#0  0x61ec4fd0 in ?? ()
#1  0x6ca1cafc in R_RunWeakRefFinalizer (w=0xc786a98) at memory.c:1393
#2  0x6ca1cdba in RunFinalizers () at memory.c:1459
#3  0x6ca1d024 in R_RunPendingFinalizers () at memory.c:1495
#4  R_gc () at memory.c:2893
#5  do_gc (call=0x1d45b88, op=0x15241d0, args=0x1d454b8,
rho=0x1d45318) at memory.c:2013
#6  0x6c9db60f in bcEval (body=body at entry=0x1d45a88,
rho=rho at entry=0x1d45318, useCache=useCache at entry=TRUE)
    at eval.c:6781
#7  0x6c9ecfb2 in Rf_eval (e=0x1d45a88, rho=0x1d45318) at eval.c:624
#8  0x6c9ee6f1 in R_execClosure (call=call at entry=0x0,
newrho=<optimized out>, sysparent=<optimized out>,
    rho=0x15a3370, arglist=0x18c2498, op=0x1d45968) at eval.c:1773
#9  0x6c9ef605 in Rf_applyClosure (call=0x15a3370,
call at entry=0x1d45828, op=0x18c2498, op at entry=0x1d45968,
    arglist=0x1d45968, rho=rho at entry=0x15a3370,
suppliedvars=0x18c2498) at eval.c:1701
#10 0x6c9ecf78 in Rf_eval (e=e at entry=0x1d45828,
rho=rho at entry=0x15a3370) at eval.c:747
#11 0x6ca11170 in Rf_ReplIteration (rho=0x15a3370, savestack=0,
browselevel=0, state=0x142edec) at main.c:258
#12 0x6ca11567 in R_ReplConsole (rho=<optimized out>, savestack=0,
browselevel=0) at main.c:308
#13 0x6ca11604 in run_Rmainloop () at main.c:1082
#14 0x6ca11700 in Rf_mainloop () at main.c:1089
#15 0x00401836 in AppMain (argc=1, argv=0x15c16f8) at rterm.c:86
#16 0x00401649 in WinMain at 16 (Instance=0x400000, PrevInstance=0x0,
CmdLine=0x1904797 "", CmdShow=10)
    at graphappmain.c:23
#17 0x00402a8d in main ()

Any ideas?

I?aki


From h@wickh@m @ending from gm@il@com  Mon Aug  6 18:21:09 2018
From: h@wickh@m @ending from gm@il@com (Hadley Wickham)
Date: Mon, 6 Aug 2018 09:21:09 -0700
Subject: [Rd] vctrs: a type system for the tidyverse
Message-ID: <CABdHhvFARmiExcEk=NUjfPcRtYnoF4pAA5WTws6r1C+X76zHEw@mail.gmail.com>

Hi all,

I wanted to share with you an experimental package that I?m currently
working on: vctrs, <https://github.com/r-lib/vctrs>. The motivation for
vctrs is to think deeply about the output ?type? of functions like
`c()`, `ifelse()`, and `rbind()`, with an eye to implementing one
strategy throughout the tidyverse (i.e. all the functions listed at
<https://github.com/r-lib/vctrs#tidyverse-functions>). Because this is
going to be a big change, I thought it would be very useful to get
comments from a wide audience, so I?m reaching out to R-devel to get
your thoughts.

There is quite a lot already in the readme
(<https://github.com/r-lib/vctrs#vctrs>), so here I?ll try to motivate
vctrs as succinctly as possible by comparing `base::c()` to its
equivalent `vctrs::vec_c()`. I think the drawbacks of `c()` are well
known, but to refresh your memory, I?ve highlighted a few at
<https://github.com/r-lib/vctrs#compared-to-base-r>. I think they arise
because of two main challenges: `c()` has to both combine vectors *and*
strip attributes, and it only dispatches on the first argument.

The design of vctrs is largely driven by a pair of principles:

-   The type of `vec_c(x, y)` should be the same as `vec_c(y, x)`

-   The type of `vec_c(x, vec_c(y, z))` should be the same as
    `vec_c(vec_c(x, y), z)`

i.e. the type should be associative and commutative. I think these are
good principles because they makes types simpler to understand and to
implement.

Method dispatch for `vec_c()` is quite simple because associativity and
commutativity mean that we can determine the output type only by
considering a pair of inputs at a time. To this end, vctrs provides
`vec_type2()` which takes two inputs and returns their common type
(represented as zero length vector):

    str(vec_type2(integer(), double()))
    #>  num(0)

    str(vec_type2(factor("a"), factor("b")))
    #>  Factor w/ 2 levels "a","b":

    # NB: not all types have a common/unifying type
    str(vec_type2(Sys.Date(), factor("a")))
    #> Error: No common type for date and factor

(`vec_type()` currently implements double dispatch through a combination
of S3 dispatch and if-else blocks, but this will change to a pure S3
approach in the near future.)

To find the common type of multiple vectors, we can use `Reduce()`:

    vecs <- list(TRUE, 1:10, 1.5)

    type <- Reduce(vec_type2, vecs)
    str(type)
    #>  num(0)

There?s one other piece of the puzzle: casting one vector to another
type. That?s implemented by `vec_cast()` (which also uses double
dispatch):

    str(lapply(vecs, vec_cast, to = type))
    #> List of 3
    #>  $ : num 1
    #>  $ : num [1:10] 1 2 3 4 5 6 7 8 9 10
    #>  $ : num 1.5

All up, this means that we can implement the essence of `vec_c()` in
only a few lines:

    vec_c2 <- function(...) {
      args <- list(...)
      type <- Reduce(vec_type, args)

      cast <- lapply(type, vec_cast, to = type)
      unlist(cast, recurse = FALSE)
    }

    vec_c(factor("a"), factor("b"))
    #> [1] a b
    #> Levels: a b

    vec_c(Sys.Date(), Sys.time())
    #> [1] "2018-08-06 00:00:00 CDT" "2018-08-06 11:20:32 CDT"

(The real implementation is little more complex:
<https://github.com/r-lib/vctrs/blob/master/R/c.R>)

On top of this foundation, vctrs expands in a few different ways:

-   To consider the ?type? of a data frame, and what the common type of
    two data frames should be. This leads to a natural implementation of
    `vec_rbind()` which includes all columns that appear in any input.

-   To create a new ?list\_of? type, a list where every element is of
    fixed type (enforced by `[<-`, `[[<-`, and `$<-`)

-   To think a little about the ?shape? of a vector, and to consider
    recycling as part of the type system. (This thinking is not yet
    fully fleshed out)

Thanks for making it to the bottom of this long email :) I would love to
hear your thoughts on vctrs. It?s something that I?ve been having a lot
of fun exploring, and I?d like to make sure it is as robust as possible
(and the motivations are as clear as possible) before we start using it
in other packages.

Hadley


-- 
http://hadley.nz


From becker@g@be @ending from gene@com  Mon Aug  6 19:46:10 2018
From: becker@g@be @ending from gene@com (Gabe Becker)
Date: Mon, 6 Aug 2018 10:46:10 -0700
Subject: [Rd] vctrs: a type system for the tidyverse
In-Reply-To: <CABdHhvFARmiExcEk=NUjfPcRtYnoF4pAA5WTws6r1C+X76zHEw@mail.gmail.com>
References: <CABdHhvFARmiExcEk=NUjfPcRtYnoF4pAA5WTws6r1C+X76zHEw@mail.gmail.com>
Message-ID: <CAMFmJsn_wNxuV2SQqUiZroJYV+rHRKdADi8Fjg8fRjJr0RGuwQ@mail.gmail.com>

Hadley,

Looks interesting and like a fun project from what you said in the email (I
don't have time right now to dig deep into the readme) A few thoughts.

First off, you are using the word "type" throughout this email; You seem to
mean class (judging by your Date and factor examples, and the fact you
mention S3 dispatch) as opposed to type in the sense of what is returned by
R's  typeof() function. I think it would be clearer if you called it class
throughout unless that isn't actually what you mean (in which case I would
have other questions...)

More thoughts inline.

On Mon, Aug 6, 2018 at 9:21 AM, Hadley Wickham <h.wickham at gmail.com> wrote:

> Hi all,
>
> I wanted to share with you an experimental package that I?m currently
> working on: vctrs, <https://github.com/r-lib/vctrs>. The motivation for
> vctrs is to think deeply about the output ?type? of functions like
> `c()`, `ifelse()`, and `rbind()`, with an eye to implementing one
> strategy throughout the tidyverse (i.e. all the functions listed at
> <https://github.com/r-lib/vctrs#tidyverse-functions>). Because this is
> going to be a big change, I thought it would be very useful to get
> comments from a wide audience, so I?m reaching out to R-devel to get
> your thoughts.
>
> There is quite a lot already in the readme
> (<https://github.com/r-lib/vctrs#vctrs>), so here I?ll try to motivate
> vctrs as succinctly as possible by comparing `base::c()` to its
> equivalent `vctrs::vec_c()`. I think the drawbacks of `c()` are well
> known, but to refresh your memory, I?ve highlighted a few at
> <https://github.com/r-lib/vctrs#compared-to-base-r>. I think they arise
> because of two main challenges: `c()` has to both combine vectors *and*
> strip attributes, and it only dispatches on the first argument.
>
> The design of vctrs is largely driven by a pair of principles:
>
> -   The type of `vec_c(x, y)` should be the same as `vec_c(y, x)`
>
> -   The type of `vec_c(x, vec_c(y, z))` should be the same as
>     `vec_c(vec_c(x, y), z)`
>
> i.e. the type should be associative and commutative. I think these are
> good principles because they makes types simpler to understand and to
> implement.
>
> Method dispatch for `vec_c()` is quite simple because associativity and
> commutativity mean that we can determine the output type only by
> considering a pair of inputs at a time. To this end, vctrs provides
> `vec_type2()` which takes two inputs and returns their common type
> (represented as zero length vector):
>
>     str(vec_type2(integer(), double()))
>     #>  num(0)
>
>     str(vec_type2(factor("a"), factor("b")))
>     #>  Factor w/ 2 levels "a","b":
>

What is the reasoning behind taking the union of the levels here? I'm not
sure that is actually the behavior I would want if I have a vector of
factors and I try to append some new data to it. I might want/ expect to
retain the existing levels and get either NAs or an error if the new data
has (present) levels not in the first data. The behavior as above doesn't
seem in-line with what I understand the purpose of factors to be (explicit
restriction of possible values).

I guess what I'm saying is that while I agree associativity is good for
most things, it doesn't seem like the right behavior to me in the case of
factors.

Also, while we're on factors, what does

vec_type2(factor("a"), "a")

return, character or factor with levels "a"?



>
>     # NB: not all types have a common/unifying type
>     str(vec_type2(Sys.Date(), factor("a")))
>     #> Error: No common type for date and factor
>

Why is this not a list? Do you have the additional restraint that vec_type2
must return the class of one of its operands? If so, what is the
justification of that? Are you not counting list as a "type of vector"?


>
> (`vec_type()` currently implements double dispatch through a combination
> of S3 dispatch and if-else blocks, but this will change to a pure S3
> approach in the near future.)
>
> To find the common type of multiple vectors, we can use `Reduce()`:
>
>     vecs <- list(TRUE, 1:10, 1.5)
>
>     type <- Reduce(vec_type2, vecs)
>     str(type)
>     #>  num(0)
>
> There?s one other piece of the puzzle: casting one vector to another
> type. That?s implemented by `vec_cast()` (which also uses double
> dispatch):
>
>     str(lapply(vecs, vec_cast, to = type))
>     #> List of 3
>     #>  $ : num 1
>     #>  $ : num [1:10] 1 2 3 4 5 6 7 8 9 10
>     #>  $ : num 1.5
>
> All up, this means that we can implement the essence of `vec_c()` in
> only a few lines:
>
>     vec_c2 <- function(...) {
>       args <- list(...)
>       type <- Reduce(vec_type, args)
>
>       cast <- lapply(type, vec_cast, to = type)
>       unlist(cast, recurse = FALSE)
>     }
>
>     vec_c(factor("a"), factor("b"))
>     #> [1] a b
>     #> Levels: a b
>
>     vec_c(Sys.Date(), Sys.time())
>     #> [1] "2018-08-06 00:00:00 CDT" "2018-08-06 11:20:32 CDT"
>
> (The real implementation is little more complex:
> <https://github.com/r-lib/vctrs/blob/master/R/c.R>)
>
> On top of this foundation, vctrs expands in a few different ways:
>
> -   To consider the ?type? of a data frame, and what the common type of
>     two data frames should be. This leads to a natural implementation of
>     `vec_rbind()` which includes all columns that appear in any input.
>

I must admit I'm a bit surprised here. rbind is one of the few places that
immediately come to mind where R takes a fail early and loud approach to
likely errors (as opposed to the more permissive do soemthing  that could
be what they meant appraoch of, e.g., out-of-bounds indexing). Are we sure
we want rbind to get less strict with respect to compatibility of the
data.frames being combined? Another "permissive" option would be to return
a data.frame which has only the intersection of the columns. There are
certainly times when that is what I want (rather than columns with tons of
NAs in them) and it would be convenient not to need to do the column
subsetting myself. This behavior would also meet your design goals of
associativity and commutivity.

I want to be clear, I think what you describe is a useful operation, if it
is what is intended, but perhaps a different name rather than calling it
rbind? maybe vec_rcbind to indicate that both rows and columns are being
potentially added to any given individual input.

Best,
~G


> -   To create a new ?list\_of? type, a list where every element is of
>     fixed type (enforced by `[<-`, `[[<-`, and `$<-`)
>
> -   To think a little about the ?shape? of a vector, and to consider
>     recycling as part of the type system. (This thinking is not yet
>     fully fleshed out)
>
> Thanks for making it to the bottom of this long email :) I would love to
> hear your thoughts on vctrs. It?s something that I?ve been having a lot
> of fun exploring, and I?d like to make sure it is as robust as possible
> (and the motivations are as clear as possible) before we start using it
> in other packages.
>
> Hadley
>
>
> --
> http://hadley.nz
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>



-- 
Gabriel Becker, Ph.D
Scientist
Bioinformatics and Computational Biology
Genentech Research

	[[alternative HTML version deleted]]


From h@wickh@m @ending from gm@il@com  Mon Aug  6 22:22:59 2018
From: h@wickh@m @ending from gm@il@com (Hadley Wickham)
Date: Mon, 6 Aug 2018 15:22:59 -0500
Subject: [Rd] vctrs: a type system for the tidyverse
In-Reply-To: <CAMFmJsn_wNxuV2SQqUiZroJYV+rHRKdADi8Fjg8fRjJr0RGuwQ@mail.gmail.com>
References: <CABdHhvFARmiExcEk=NUjfPcRtYnoF4pAA5WTws6r1C+X76zHEw@mail.gmail.com>
 <CAMFmJsn_wNxuV2SQqUiZroJYV+rHRKdADi8Fjg8fRjJr0RGuwQ@mail.gmail.com>
Message-ID: <CABdHhvEAV-bhprxVMHF_R8vKdN3VEriV86kGNNU02RiK=+m4dQ@mail.gmail.com>

> First off, you are using the word "type" throughout this email; You seem to
> mean class (judging by your Date and factor examples, and the fact you
> mention S3 dispatch) as opposed to type in the sense of what is returned by
> R's  typeof() function. I think it would be clearer if you called it class
> throughout unless that isn't actually what you mean (in which case I would
> have other questions...)

I used "type" to hand wave away the precise definition - it's not S3
class or base type (i.e. typeof()) but some hybrid of the two. I do
want to emphasise that it's a type system, not a oo system, in that
coercions are not defined by superclass/subclass relationships.

> More thoughts inline.
>
> On Mon, Aug 6, 2018 at 9:21 AM, Hadley Wickham <h.wickham at gmail.com> wrote:
>>
>> Hi all,
>>
>> I wanted to share with you an experimental package that I?m currently
>> working on: vctrs, <https://github.com/r-lib/vctrs>. The motivation for
>> vctrs is to think deeply about the output ?type? of functions like
>> `c()`, `ifelse()`, and `rbind()`, with an eye to implementing one
>> strategy throughout the tidyverse (i.e. all the functions listed at
>> <https://github.com/r-lib/vctrs#tidyverse-functions>). Because this is
>> going to be a big change, I thought it would be very useful to get
>> comments from a wide audience, so I?m reaching out to R-devel to get
>> your thoughts.
>>
>> There is quite a lot already in the readme
>> (<https://github.com/r-lib/vctrs#vctrs>), so here I?ll try to motivate
>> vctrs as succinctly as possible by comparing `base::c()` to its
>> equivalent `vctrs::vec_c()`. I think the drawbacks of `c()` are well
>> known, but to refresh your memory, I?ve highlighted a few at
>> <https://github.com/r-lib/vctrs#compared-to-base-r>. I think they arise
>> because of two main challenges: `c()` has to both combine vectors *and*
>> strip attributes, and it only dispatches on the first argument.
>>
>> The design of vctrs is largely driven by a pair of principles:
>>
>> -   The type of `vec_c(x, y)` should be the same as `vec_c(y, x)`
>>
>> -   The type of `vec_c(x, vec_c(y, z))` should be the same as
>>     `vec_c(vec_c(x, y), z)`
>>
>> i.e. the type should be associative and commutative. I think these are
>> good principles because they makes types simpler to understand and to
>> implement.
>>
>> Method dispatch for `vec_c()` is quite simple because associativity and
>> commutativity mean that we can determine the output type only by
>> considering a pair of inputs at a time. To this end, vctrs provides
>> `vec_type2()` which takes two inputs and returns their common type
>> (represented as zero length vector):
>>
>>     str(vec_type2(integer(), double()))
>>     #>  num(0)
>>
>>     str(vec_type2(factor("a"), factor("b")))
>>     #>  Factor w/ 2 levels "a","b":
>
>
> What is the reasoning behind taking the union of the levels here? I'm not
> sure that is actually the behavior I would want if I have a vector of
> factors and I try to append some new data to it. I might want/ expect to
> retain the existing levels and get either NAs or an error if the new data
> has (present) levels not in the first data. The behavior as above doesn't
> seem in-line with what I understand the purpose of factors to be (explicit
> restriction of possible values).

Originally (like a week ago ?), we threw an error if the factors
didn't have the same level, and provided an optional coercion to
character. I decided that while correct (the factor levels are a
parameter of the type, and hence factors with different levels aren't
comparable), that this fights too much against how people actually use
factors in practice. It also seems like base R is moving more in this
direction, i.e. in 3.4 factor("a") == factor("b") is an error, whereas
in R 3.5 it returns FALSE.

I'm not wedded to the current approach, but it feels like the same
principle should apply in comparisons like x == y (even though == is
outside the scope of vctrs, ideally the underlying principles would be
robust enough to suggest what should happen).

> I guess what I'm saying is that while I agree associativity is good for most
> things, it doesn't seem like the right behavior to me in the case of
> factors.

I think associativity is such a strong and useful principle that it
may be worth making some sacrifices for factors. That said, my claim
of associativity is only on the type, not the values of the type:
vec_c(fa, fb) and vec_c(fb, fa) both return factors, but the levels
are in different orders.

> Also, while we're on factors, what does
>
> vec_type2(factor("a"), "a")
>
> return, character or factor with levels "a"?

Character. Coercing to character would potentially lose too much
information. I think you could argue that this could be an error, but
again I feel like this would make the type system a little too strict
and cause extra friction for most uses.

>>     # NB: not all types have a common/unifying type
>>     str(vec_type2(Sys.Date(), factor("a")))
>>     #> Error: No common type for date and factor
>
>
> Why is this not a list? Do you have the additional restraint that vec_type2
> must return the class of one of its operands? If so, what is the
> justification of that? Are you not counting list as a "type of vector"?

You can always request a list, with `vec_type2(Sys.Date(),
factor("a"), .type = list())` - generally the philosophy is too not
make major changes to the type without explicit user input.

I can't currently fully articulate my reasoning for why some coercions
happen automatically, and why some don't. I think these decisions have
to be made somewhat on the basis of pragmatics, and what R users are
currently familiar with. You can see a visual summary of implicit
casts (arrows) + explicit casts (circles) at
https://github.com/r-lib/vctrs/blob/master/man/figures/combined.png.
This matrix must be symmetric, and I think it should be block
diagonal, but I don't otherwise know what the constraints are.

>> On top of this foundation, vctrs expands in a few different ways:
>>
>> -   To consider the ?type? of a data frame, and what the common type of
>>     two data frames should be. This leads to a natural implementation of
>>     `vec_rbind()` which includes all columns that appear in any input.
>
>
> I must admit I'm a bit surprised here. rbind is one of the few places that
> immediately come to mind where R takes a fail early and loud approach to
> likely errors (as opposed to the more permissive do soemthing  that could be
> what they meant appraoch of, e.g., out-of-bounds indexing). Are we sure we
> want rbind to get less strict with respect to compatibility of the
> data.frames being combined?

Pragmatically, it's clearly needed for data analysis.

Also note that there are some inputs to rbind that lead to silent data loss:

rbind(data.frame(x = 1:3), c(1, 1000000))
#>   x
#> 1 1
#> 2 2
#> 3 3
#> 4 1

So while it's pretty good in general, there are still a few
infelicities (In particular, I suspect R-core might be interested in
fixing this one)

> Another "permissive" option would be to return a
> data.frame which has only the intersection of the columns. There are
> certainly times when that is what I want (rather than columns with tons of
> NAs in them) and it would be convenient not to need to do the column
> subsetting myself. This behavior would also meet your design goals of
> associativity and commutivity.

Yes, I think that would make sense as an option and would be trivial
to implemet (issue at https://github.com/r-lib/vctrs/issues/46).

Another thing I need to implement is the ability to specify the types
of some columns. Currently it's all or nothing:

vec_rbind(
  data.frame(x = F, y = 1),
  data.frame(x = 1L, y = 2),
  .type = data.frame(x = logical())
)

#>       x
#> 1 FALSE
#> 2  TRUE
#> Warning messages:
#> 1: Lossy conversion from data.frame to data.frame
#> Dropped variables: y
#> 2: Lossy conversion from data.frame to data.frame
#> Dropped variables: y

> I want to be clear, I think what you describe is a useful operation, if it
> is what is intended, but perhaps a different name rather than calling it
> rbind? maybe vec_rcbind to indicate that both rows and columns are being
> potentially added to any given individual input.

Sorry, I should have mentioned that this is unlikely to be the final
name. As well as the problem you mention, I think calling them
vec_cbind() and vec_rbind() over-emphasises the symmetry between the
two operations. cbind() and rbind() are symmetric for matrices, but
for data frames, rbind() is more about common types, and cbind() is
more about common shapes.

Thanks for your feedback, it's very useful!

Hadley

-- 
http://hadley.nz


From rhelp @ending from eoo@@dd@@nl  Tue Aug  7 15:19:55 2018
From: rhelp @ending from eoo@@dd@@nl (Jan van der Laan)
Date: Tue, 7 Aug 2018 15:19:55 +0200
Subject: [Rd] Run garbage collector when too many open files
Message-ID: <7484b808-0993-6760-b5ac-ec8dd52e982e@eoos.dds.nl>



In my package [1] I open handles to temporary files from c++, pointer to 
the objects containing those handles are returned to R as external 
pointer objects. The files are deleted when the corresponding R-object 
is deleted and the garbage collector runs:

a <- lvec(10, "integer")
rm(a)

Then when the garbage collector runs the file is deleted. However, on 
some platforms (probably with lower limits on the maximum number of file 
handles a process can have open), I run into the problem that the 
garbage collector doesn't run often enough. In this case that means, for 
example, that another package of mine using this package generates an 
error when its tests are run [2].

The simplest solution is to add some calls to gc() in my tests. But a 
more general/automatic solution would be nice.

I thought about something in the lines of

robust_lvec <- function(...) {
   tryCatch({
     lvec(...)
   }, error = function(e) {
     gc()
     lvec(...) # duplicated code
   })
}

e.g. try to open a file, when that fails call the garbage collector and 
try again. However, this introduces duplicated code (in this case only 
one line, but that can be more), and doesn't help if it is another 
function that tries to open a file.

Is there a better solution?

Thanks!

Jan


[1] https://cran.r-project.org/web/packages/lvec/index.html
[2] https://cran.r-project.org/web/checks/check_results_reclin.html


From rhelp @ending from eoo@@dd@@nl  Tue Aug  7 15:53:12 2018
From: rhelp @ending from eoo@@dd@@nl (Jan van der Laan)
Date: Tue, 7 Aug 2018 15:53:12 +0200
Subject: [Rd] 
 [R-pkg-devel] Run garbage collector when too many open files
In-Reply-To: <ccb2f0f9-ce61-510f-6f6f-6e41d5aae6b4@statistik.tu-dortmund.de>
References: <627f7758-c5ae-0ab8-b87b-10c6f5a1edc1@eoos.dds.nl>
 <ccb2f0f9-ce61-510f-6f6f-6e41d5aae6b4@statistik.tu-dortmund.de>
Message-ID: <da92aa42-00eb-dbbe-5267-1bed89d75a4b@eoos.dds.nl>

Dear Uwe,

(When replying to your message, I sent the reply to r-devel and not 
r-package-devel, as Martin Meachler suggested that this thread would be 
a better fit for r-devel.)

Thanks. In the example below I used rm() explicitly, but in general 
users wouldn't do that.

One of the reasons for the large number of file handles is that 
sometimes unnamed temporary objects are created. For example:

 > library(ldat)
 > libraty(lvec)
 >
 > a <- lvec(10, "integer")
OPENFILE '/tmp/RtmpVqkDsw/file32145169fb06/lvec3214753f2af0'
 > b <- as_rvec(a[1:3])
OPENFILE '/tmp/RtmpVqkDsw/file32145169fb06/lvec32146a50f383'
OPENFILE '/tmp/RtmpVqkDsw/file32145169fb06/lvec3214484b652c'
 > print(b)
[1] 0 0 0
 >
 >
 > gc()
CLOSEFILE '/tmp/RtmpVqkDsw/file32145169fb06/lvec3214484b652c'
CLOSEFILE '/tmp/RtmpVqkDsw/file32145169fb06/lvec32146a50f383'
           used (Mb) gc trigger (Mb) max used (Mb)
Ncells  796936 42.6    1442291 77.1  1168576 62.5
Vcells 1519523 11.6    4356532 33.3  4740854 36.2


For debugging, I log when files are opened and closed. The call a[1:3] 
(which creates a slice of a) creates two temporary objects [1]. These 
are only deleted when I explicitly call gc() or on some other random 
moment in time.

I hope this illustrates the problem better.


Best,
Jan


[1] One improvement would be to create less temporary files; often these 
contain only very little information that is better kept in memory. But 
that is only a partial solution.




On 07-08-18 15:24, Uwe Ligges wrote:
> Why not add functionality that allows to delete object + runs cleanup code?
> 
> Best,
> Uwe Ligges
> 
> 
> 
> On 07.08.2018 14:26, Jan van der Laan wrote:
>>
>>
>> In my package I open handles to temporary files from c++, handles to 
>> them are returned to R through vptr objects. The files are deleted 
>> then the corresponding R-object is deleted and the garbage collector 
>> runs:
>>
>> a <- lvec(10, "integer")
>> rm(a)
>>
>> Then when the garbage collector runs the file is deleted. However, on 
>> some platforms (probably with lower limits on the maximum number of 
>> file handles a process can have open), I run into the problem that the 
>> garbage collector doesn't run often enough. In this case that means 
>> that another package of mine using this package generates an error 
>> when its tests are run.
>>
>> The simplest solution is to add some calls to gc() in my tests. But a 
>> more general/automatic solution would be nice.
>>
>> I thought about something in the lines of
>>
>> robust_lvec <- function(...) {
>> ?? tryCatch({
>> ???? lvec(...)
>> ?? }, error = function(e) {
>> ???? gc()
>> ???? lvec(...) # duplicated code
>> ?? })
>> }
>>
>> e.g. try to open a file, when that fails call the garbage collector 
>> and try again. However, this introduces duplicated code (in this case 
>> only one line, but that can be more), and doesn't help if it is 
>> another function that tries to open a file.
>>
>> Is there a better solution?
>>
>> Thanks!
>>
>> Jan
>>
>> ______________________________________________
>> R-package-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-package-devel


From luke-tier@ey m@ili@g off uiow@@edu  Tue Aug  7 17:07:28 2018
From: luke-tier@ey m@ili@g off uiow@@edu (luke-tier@ey m@ili@g off uiow@@edu)
Date: Tue, 7 Aug 2018 10:07:28 -0500 (CDT)
Subject: [Rd] 
 [R-pkg-devel] Run garbage collector when too many open files
In-Reply-To: <da92aa42-00eb-dbbe-5267-1bed89d75a4b@eoos.dds.nl>
References: <627f7758-c5ae-0ab8-b87b-10c6f5a1edc1@eoos.dds.nl>
 <ccb2f0f9-ce61-510f-6f6f-6e41d5aae6b4@statistik.tu-dortmund.de>
 <da92aa42-00eb-dbbe-5267-1bed89d75a4b@eoos.dds.nl>
Message-ID: <alpine.DEB.2.20.1808071004180.3237@luke-Latitude-7480>

In R 3.5 and later you should not need to gc() -- that should happen
automatically within the connections code.

Nevertheless, I would recommend redesigning your approach to avoid
hanging onto open file connections as these are a scarce resource.
You can keep around your temporary files without having them open and
only open/close them on access, with the close run in an on.exit or a
tryCatch/finally clause.

Best,

luke

On Tue, 7 Aug 2018, Jan van der Laan wrote:

> Dear Uwe,
>
> (When replying to your message, I sent the reply to r-devel and not 
> r-package-devel, as Martin Meachler suggested that this thread would be a 
> better fit for r-devel.)
>
> Thanks. In the example below I used rm() explicitly, but in general users 
> wouldn't do that.
>
> One of the reasons for the large number of file handles is that sometimes 
> unnamed temporary objects are created. For example:
>
>> library(ldat)
>> libraty(lvec)
>>
>> a <- lvec(10, "integer")
> OPENFILE '/tmp/RtmpVqkDsw/file32145169fb06/lvec3214753f2af0'
>> b <- as_rvec(a[1:3])
> OPENFILE '/tmp/RtmpVqkDsw/file32145169fb06/lvec32146a50f383'
> OPENFILE '/tmp/RtmpVqkDsw/file32145169fb06/lvec3214484b652c'
>> print(b)
> [1] 0 0 0
>>
>>
>> gc()
> CLOSEFILE '/tmp/RtmpVqkDsw/file32145169fb06/lvec3214484b652c'
> CLOSEFILE '/tmp/RtmpVqkDsw/file32145169fb06/lvec32146a50f383'
>          used (Mb) gc trigger (Mb) max used (Mb)
> Ncells  796936 42.6    1442291 77.1  1168576 62.5
> Vcells 1519523 11.6    4356532 33.3  4740854 36.2
>
>
> For debugging, I log when files are opened and closed. The call a[1:3] (which 
> creates a slice of a) creates two temporary objects [1]. These are only 
> deleted when I explicitly call gc() or on some other random moment in time.
>
> I hope this illustrates the problem better.
>
>
> Best,
> Jan
>
>
> [1] One improvement would be to create less temporary files; often these 
> contain only very little information that is better kept in memory. But that 
> is only a partial solution.
>
>
>
>
> On 07-08-18 15:24, Uwe Ligges wrote:
>> Why not add functionality that allows to delete object + runs cleanup code?
>> 
>> Best,
>> Uwe Ligges
>> 
>> 
>> 
>> On 07.08.2018 14:26, Jan van der Laan wrote:
>>> 
>>> 
>>> In my package I open handles to temporary files from c++, handles to them 
>>> are returned to R through vptr objects. The files are deleted then the 
>>> corresponding R-object is deleted and the garbage collector runs:
>>> 
>>> a <- lvec(10, "integer")
>>> rm(a)
>>> 
>>> Then when the garbage collector runs the file is deleted. However, on some 
>>> platforms (probably with lower limits on the maximum number of file 
>>> handles a process can have open), I run into the problem that the garbage 
>>> collector doesn't run often enough. In this case that means that another 
>>> package of mine using this package generates an error when its tests are 
>>> run.
>>> 
>>> The simplest solution is to add some calls to gc() in my tests. But a more 
>>> general/automatic solution would be nice.
>>> 
>>> I thought about something in the lines of
>>> 
>>> robust_lvec <- function(...) {
>>> ?? tryCatch({
>>> ???? lvec(...)
>>> ?? }, error = function(e) {
>>> ???? gc()
>>> ???? lvec(...) # duplicated code
>>> ?? })
>>> }
>>> 
>>> e.g. try to open a file, when that fails call the garbage collector and 
>>> try again. However, this introduces duplicated code (in this case only one 
>>> line, but that can be more), and doesn't help if it is another function 
>>> that tries to open a file.
>>> 
>>> Is there a better solution?
>>> 
>>> Thanks!
>>> 
>>> Jan
>>> 
>>> ______________________________________________
>>> R-package-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-package-devel
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

-- 
Luke Tierney
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu

From rhelp @ending from eoo@@dd@@nl  Tue Aug  7 20:37:17 2018
From: rhelp @ending from eoo@@dd@@nl (Jan van der Laan)
Date: Tue, 7 Aug 2018 20:37:17 +0200
Subject: [Rd] Run garbage collector when too many open files
In-Reply-To: <alpine.DEB.2.20.1808071004180.3237@luke-Latitude-7480>
References: <627f7758-c5ae-0ab8-b87b-10c6f5a1edc1@eoos.dds.nl>
 <ccb2f0f9-ce61-510f-6f6f-6e41d5aae6b4@statistik.tu-dortmund.de>
 <da92aa42-00eb-dbbe-5267-1bed89d75a4b@eoos.dds.nl>
 <alpine.DEB.2.20.1808071004180.3237@luke-Latitude-7480>
Message-ID: <67ee9978-262a-9ea3-8c7a-fd5672e662ae@eoos.dds.nl>

Dear Luke,


Thanks. See below


On 07-08-18 17:07, luke-tierney at uiowa.edu wrote:
> In R 3.5 and later you should not need to gc() -- that should happen
> automatically within the connections code.

Could you elaborate on what has changed in R 3.5? As far as I can tell 
my problem also occurs in R 3.5 (my computer is still on 3.4.4; but I 
assume the solaris CRAN machine isn't). And what do you mean with 'the 
connections code'? Is there something I can so/should do to have the 
garbage collector be a bit more aggressive in cleaning up my mess?

>
> Nevertheless, I would recommend redesigning your approach to avoid
> hanging onto open file connections as these are a scarce resource.
> You can keep around your temporary files without having them open and
> only open/close them on access, with the close run in an on.exit or a
> tryCatch/finally clause.

I am afraid that this will have a large performance penalty. The files 
in question are memory mapped files from which code will reading and 
writing continuously in most cases. Of course, there will probably 
objects that are not used for large amounts of time that could be 
temporarily closed , but it will be a bit difficult for the package to 
detect which objects that will be. I would have to write my own 'garbage 
collector'.


Best,
Uwe

>
> Best,
>
> luke
>
> On Tue, 7 Aug 2018, Jan van der Laan wrote:
>
>> Dear Uwe,
>>
>> (When replying to your message, I sent the reply to r-devel and not 
>> r-package-devel, as Martin Meachler suggested that this thread would 
>> be a better fit for r-devel.)
>>
>> Thanks. In the example below I used rm() explicitly, but in general 
>> users wouldn't do that.
>>
>> One of the reasons for the large number of file handles is that 
>> sometimes unnamed temporary objects are created. For example:
>>
>>> library(ldat)
>>> libraty(lvec)
>>>
>>> a <- lvec(10, "integer")
>> OPENFILE '/tmp/RtmpVqkDsw/file32145169fb06/lvec3214753f2af0'
>>> b <- as_rvec(a[1:3])
>> OPENFILE '/tmp/RtmpVqkDsw/file32145169fb06/lvec32146a50f383'
>> OPENFILE '/tmp/RtmpVqkDsw/file32145169fb06/lvec3214484b652c'
>>> print(b)
>> [1] 0 0 0
>>>
>>>
>>> gc()
>> CLOSEFILE '/tmp/RtmpVqkDsw/file32145169fb06/lvec3214484b652c'
>> CLOSEFILE '/tmp/RtmpVqkDsw/file32145169fb06/lvec32146a50f383'
>> ???????? used (Mb) gc trigger (Mb) max used (Mb)
>> Ncells? 796936 42.6??? 1442291 77.1? 1168576 62.5
>> Vcells 1519523 11.6??? 4356532 33.3? 4740854 36.2
>>
>>
>> For debugging, I log when files are opened and closed. The call 
>> a[1:3] (which creates a slice of a) creates two temporary objects 
>> [1]. These are only deleted when I explicitly call gc() or on some 
>> other random moment in time.
>>
>> I hope this illustrates the problem better.
>>
>>
>> Best,
>> Jan
>>
>>
>> [1] One improvement would be to create less temporary files; often 
>> these contain only very little information that is better kept in 
>> memory. But that is only a partial solution.
>>
>>
>>
>>
>> On 07-08-18 15:24, Uwe Ligges wrote:
>>> Why not add functionality that allows to delete object + runs 
>>> cleanup code?
>>>
>>> Best,
>>> Uwe Ligges
>>>
>>>
>>>
>>> On 07.08.2018 14:26, Jan van der Laan wrote:
>>>>
>>>>
>>>> In my package I open handles to temporary files from c++, handles 
>>>> to them are returned to R through vptr objects. The files are 
>>>> deleted then the corresponding R-object is deleted and the garbage 
>>>> collector runs:
>>>>
>>>> a <- lvec(10, "integer")
>>>> rm(a)
>>>>
>>>> Then when the garbage collector runs the file is deleted. However, 
>>>> on some platforms (probably with lower limits on the maximum number 
>>>> of file handles a process can have open), I run into the problem 
>>>> that the garbage collector doesn't run often enough. In this case 
>>>> that means that another package of mine using this package 
>>>> generates an error when its tests are run.
>>>>
>>>> The simplest solution is to add some calls to gc() in my tests. But 
>>>> a more general/automatic solution would be nice.
>>>>
>>>> I thought about something in the lines of
>>>>
>>>> robust_lvec <- function(...) {
>>>> ?? tryCatch({
>>>> ???? lvec(...)
>>>> ?? }, error = function(e) {
>>>> ???? gc()
>>>> ???? lvec(...) # duplicated code
>>>> ?? })
>>>> }
>>>>
>>>> e.g. try to open a file, when that fails call the garbage collector 
>>>> and try again. However, this introduces duplicated code (in this 
>>>> case only one line, but that can be more), and doesn't help if it 
>>>> is another function that tries to open a file.
>>>>
>>>> Is there a better solution?
>>>>
>>>> Thanks!
>>>>
>>>> Jan
>>>>
>>>> ______________________________________________
>>>> R-package-devel at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-package-devel
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>


From luke-tier@ey m@ili@g off uiow@@edu  Tue Aug  7 22:47:42 2018
From: luke-tier@ey m@ili@g off uiow@@edu (luke-tier@ey m@ili@g off uiow@@edu)
Date: Tue, 7 Aug 2018 15:47:42 -0500 (CDT)
Subject: [Rd] Run garbage collector when too many open files
In-Reply-To: <67ee9978-262a-9ea3-8c7a-fd5672e662ae@eoos.dds.nl>
References: <627f7758-c5ae-0ab8-b87b-10c6f5a1edc1@eoos.dds.nl>
 <ccb2f0f9-ce61-510f-6f6f-6e41d5aae6b4@statistik.tu-dortmund.de>
 <da92aa42-00eb-dbbe-5267-1bed89d75a4b@eoos.dds.nl>
 <alpine.DEB.2.20.1808071004180.3237@luke-Latitude-7480>
 <67ee9978-262a-9ea3-8c7a-fd5672e662ae@eoos.dds.nl>
Message-ID: <alpine.DEB.2.20.1808071533530.3237@luke-Latitude-7480>

On Tue, 7 Aug 2018, Jan van der Laan wrote:

> Dear Luke,
>
>
> Thanks. See below
>
>
> On 07-08-18 17:07, luke-tierney at uiowa.edu wrote:
>> In R 3.5 and later you should not need to gc() -- that should happen
>> automatically within the connections code.
>
> Could you elaborate on what has changed in R 3.5? As far as I can tell my 
> problem also occurs in R 3.5 (my computer is still on 3.4.4; but I assume the 
> solaris CRAN machine isn't). And what do you mean with 'the connections 
> code'? Is there something I can so/should do to have the garbage collector be 
> a bit more aggressive in cleaning up my mess?

If you are not opening files through R connections then this is not relevant.

If you are opening files on your own via C or C++ level calls then it
is a good idea to run gc if there is a failure -- that is what the
connections code does. I would put this logic at a low level, inside
lvec and such, before signaling an error. But this should be a
fall-back. You might be starving other libraries of file handles

>> Nevertheless, I would recommend redesigning your approach to avoid
>> hanging onto open file connections as these are a scarce resource.
>> You can keep around your temporary files without having them open and
>> only open/close them on access, with the close run in an on.exit or a
>> tryCatch/finally clause.
>
> I am afraid that this will have a large performance penalty.

Have you done enough profiling to be sure this is true, in particular
for realistic usage, not small toy examples? This would be the
cleanest design. You could also maintain a small cache of open files,
but that is more work to implement.

Best,

luke

> The files in 
> question are memory mapped files from which code will reading and writing 
> continuously in most cases. Of course, there will probably objects that are 
> not used for large amounts of time that could be temporarily closed , but it 
> will be a bit difficult for the package to detect which objects that will be. 
> I would have to write my own 'garbage collector'.
>
>
> Best,
> Uwe
>
>> 
>> Best,
>> 
>> luke
>> 
>> On Tue, 7 Aug 2018, Jan van der Laan wrote:
>> 
>>> Dear Uwe,
>>> 
>>> (When replying to your message, I sent the reply to r-devel and not 
>>> r-package-devel, as Martin Meachler suggested that this thread would be a 
>>> better fit for r-devel.)
>>> 
>>> Thanks. In the example below I used rm() explicitly, but in general users 
>>> wouldn't do that.
>>> 
>>> One of the reasons for the large number of file handles is that sometimes 
>>> unnamed temporary objects are created. For example:
>>> 
>>>> library(ldat)
>>>> libraty(lvec)
>>>> 
>>>> a <- lvec(10, "integer")
>>> OPENFILE '/tmp/RtmpVqkDsw/file32145169fb06/lvec3214753f2af0'
>>>> b <- as_rvec(a[1:3])
>>> OPENFILE '/tmp/RtmpVqkDsw/file32145169fb06/lvec32146a50f383'
>>> OPENFILE '/tmp/RtmpVqkDsw/file32145169fb06/lvec3214484b652c'
>>>> print(b)
>>> [1] 0 0 0
>>>> 
>>>> 
>>>> gc()
>>> CLOSEFILE '/tmp/RtmpVqkDsw/file32145169fb06/lvec3214484b652c'
>>> CLOSEFILE '/tmp/RtmpVqkDsw/file32145169fb06/lvec32146a50f383'
>>> ???????? used (Mb) gc trigger (Mb) max used (Mb)
>>> Ncells? 796936 42.6??? 1442291 77.1? 1168576 62.5
>>> Vcells 1519523 11.6??? 4356532 33.3? 4740854 36.2
>>> 
>>> 
>>> For debugging, I log when files are opened and closed. The call a[1:3] 
>>> (which creates a slice of a) creates two temporary objects [1]. These are 
>>> only deleted when I explicitly call gc() or on some other random moment in 
>>> time.
>>> 
>>> I hope this illustrates the problem better.
>>> 
>>> 
>>> Best,
>>> Jan
>>> 
>>> 
>>> [1] One improvement would be to create less temporary files; often these 
>>> contain only very little information that is better kept in memory. But 
>>> that is only a partial solution.
>>> 
>>> 
>>> 
>>> 
>>> On 07-08-18 15:24, Uwe Ligges wrote:
>>>> Why not add functionality that allows to delete object + runs cleanup 
>>>> code?
>>>> 
>>>> Best,
>>>> Uwe Ligges
>>>> 
>>>> 
>>>> 
>>>> On 07.08.2018 14:26, Jan van der Laan wrote:
>>>>> 
>>>>> 
>>>>> In my package I open handles to temporary files from c++, handles to 
>>>>> them are returned to R through vptr objects. The files are deleted then 
>>>>> the corresponding R-object is deleted and the garbage collector runs:
>>>>> 
>>>>> a <- lvec(10, "integer")
>>>>> rm(a)
>>>>> 
>>>>> Then when the garbage collector runs the file is deleted. However, on 
>>>>> some platforms (probably with lower limits on the maximum number of file 
>>>>> handles a process can have open), I run into the problem that the 
>>>>> garbage collector doesn't run often enough. In this case that means that 
>>>>> another package of mine using this package generates an error when its 
>>>>> tests are run.
>>>>> 
>>>>> The simplest solution is to add some calls to gc() in my tests. But a 
>>>>> more general/automatic solution would be nice.
>>>>> 
>>>>> I thought about something in the lines of
>>>>> 
>>>>> robust_lvec <- function(...) {
>>>>> ?? tryCatch({
>>>>> ???? lvec(...)
>>>>> ?? }, error = function(e) {
>>>>> ???? gc()
>>>>> ???? lvec(...) # duplicated code
>>>>> ?? })
>>>>> }
>>>>> 
>>>>> e.g. try to open a file, when that fails call the garbage collector and 
>>>>> try again. However, this introduces duplicated code (in this case only 
>>>>> one line, but that can be more), and doesn't help if it is another 
>>>>> function that tries to open a file.
>>>>> 
>>>>> Is there a better solution?
>>>>> 
>>>>> Thanks!
>>>>> 
>>>>> Jan
>>>>> 
>>>>> ______________________________________________
>>>>> R-package-devel at r-project.org mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-package-devel
>>> 
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>> 
>> 
>
>

-- 
Luke Tierney
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu

From m@echler @ending from @t@t@m@th@ethz@ch  Wed Aug  8 12:09:24 2018
From: m@echler @ending from @t@t@m@th@ethz@ch (Martin Maechler)
Date: Wed, 8 Aug 2018 12:09:24 +0200
Subject: [Rd] 
	withTimeout bug,	it does not work properly with nlme anymore
In-Reply-To: <23152.32398.680101.584166@stat.math.ethz.ch>
References: <C7338A7EFF31BB4D831BB06C00887789B9C4AA4D@MBX023-W1-CA-2.exch023.domain.local>
 <23152.32398.680101.584166@stat.math.ethz.ch>
Message-ID: <23402.49492.50571.700532@stat.math.ethz.ch>

>>>>> Martin Maechler 
>>>>>     on Tue, 30 Jan 2018 15:17:50 +0100 writes:

(a bit more than 6 months ago)

>>>>> Ramiro Barrantes <ramiro at precisionbioassay.com>
>>>>>     on Mon, 27 Nov 2017 21:02:52 +0000 writes:

    >> Hello, I was relying on withTimeout (from R.utils) to
    >> help me stop nlme when it ?hangs?.  However, recently
    >> this stopped working.  I am pasting a reproducible
    >> example below: withTimeout should stop nlme after 10
    >> seconds but the code will generate data for which nlme
    >> does not converge (or takes too long) and withTimeout
    >> does not stop it.  I tried this both on a linux (64 bit,
    >> CentOS 7, R 3.4.1, nlme 3.1-131 R.util 2.6, and also with
    >> R 3.2.5) and mac (Sierra 10.13.1, R 3.4.2, same versions
    >> or nlme and R.utils).  It takes over R and I need to use
    >> brute-force to stop it.  As mentioned, this used to work
    >> and it is very helpful for the purposes of having a loop
    >> where nlme goes through many models.

    >> Thank you in advance for any help, Ramiro

    > Dear Ramiro,

    > as I thought you are reporting a bug about R.utils
    > withTimeout(), I and maybe others have not reacted.

    > You've addressed this again in a non-public e-mail, and
    > indeed the underlying bug is really in nlme which you do
    > mention implicitly.

    > I'm appending a version of your example that is not using
    > R.utils at all and reproducible hangs for me with R 3.4.3,
    > R 3.4.3 patched and R-devel (and almost surely earlier
    > versions of R which I did not check.

    > Indeed, the call to nlme() "stalls" // "hangs" / "freezes"
    > / ... R indeed, and cannot be terminated in a regular way,
    > and, as you, I do need "brute force" to stop it, killing
    > the R process too.

    > As the maintainer of the 'nlme' *is* R-core, we are asked
    > to fix this, at least making it interruptable.

    > Still I should not take time for that for the next couple
    > of weeks as I should fulfill several other day jobs
    > duties, instead, and so will not promise anything here.

    > Tested (minimal) patches are welcome!

I had forgotten to follow up on this, here.
We did fix this bug in the nlme source code (in the end by simply
replacing old Fortran code in PYTHAG() which looped infinitely when
passed an NAN by a call to C99's hypot()).
This was released with nlme 3.1-137, which is also part of R
3.5.0 an 3.5.1 (and current R development versions).

These examples should now all give an error (about a singular matrix),
instead of hang.

I also had added a regression test for this problem, but
interestingly that test did fail in rare circumstances and hence
it was decided not to be run by default, activated only by an
environment variable setting.
The test *is* part of nlme's sources, and hence also available
directly from
  https://svn.r-project.org/R-packages/trunk/nlme/tests/nlme-stall.R

Best,
Martin Maechler


From h@wickh@m @ending from gm@il@com  Wed Aug  8 16:34:42 2018
From: h@wickh@m @ending from gm@il@com (Hadley Wickham)
Date: Wed, 8 Aug 2018 09:34:42 -0500
Subject: [Rd] vctrs: a type system for the tidyverse
In-Reply-To: <CABdHhvEAV-bhprxVMHF_R8vKdN3VEriV86kGNNU02RiK=+m4dQ@mail.gmail.com>
References: <CABdHhvFARmiExcEk=NUjfPcRtYnoF4pAA5WTws6r1C+X76zHEw@mail.gmail.com>
 <CAMFmJsn_wNxuV2SQqUiZroJYV+rHRKdADi8Fjg8fRjJr0RGuwQ@mail.gmail.com>
 <CABdHhvEAV-bhprxVMHF_R8vKdN3VEriV86kGNNU02RiK=+m4dQ@mail.gmail.com>
Message-ID: <CABdHhvHw+_sFcD3H92KQhvurxC8eAyR2A0frZUk7pBcOaVB2Hw@mail.gmail.com>

>>> Method dispatch for `vec_c()` is quite simple because associativity and
>>> commutativity mean that we can determine the output type only by
>>> considering a pair of inputs at a time. To this end, vctrs provides
>>> `vec_type2()` which takes two inputs and returns their common type
>>> (represented as zero length vector):
>>>
>>>     str(vec_type2(integer(), double()))
>>>     #>  num(0)
>>>
>>>     str(vec_type2(factor("a"), factor("b")))
>>>     #>  Factor w/ 2 levels "a","b":
>>
>>
>> What is the reasoning behind taking the union of the levels here? I'm not
>> sure that is actually the behavior I would want if I have a vector of
>> factors and I try to append some new data to it. I might want/ expect to
>> retain the existing levels and get either NAs or an error if the new data
>> has (present) levels not in the first data. The behavior as above doesn't
>> seem in-line with what I understand the purpose of factors to be (explicit
>> restriction of possible values).
>
> Originally (like a week ago ?), we threw an error if the factors
> didn't have the same level, and provided an optional coercion to
> character. I decided that while correct (the factor levels are a
> parameter of the type, and hence factors with different levels aren't
> comparable), that this fights too much against how people actually use
> factors in practice. It also seems like base R is moving more in this
> direction, i.e. in 3.4 factor("a") == factor("b") is an error, whereas
> in R 3.5 it returns FALSE.

I now have a better argument, I think:

If you squint your brain a little, I think you can see that each set
of automatic coercions is about increasing resolution. Integers are
low resolution versions of doubles, and dates are low resolution
versions of date-times. Logicals are low resolution version of
integers because there's a strong convention that `TRUE` and `FALSE`
can be used interchangeably with `1` and `0`.

But what is the resolution of a factor? We must take a somewhat
pragmatic approach because base R often converts character vectors to
factors, and we don't want to be burdensome to users. So we say that a
factor `x` has finer resolution than factor `y` if the levels of `y`
are contained in `x`. So to find the common type of two factors, we
take the union of the levels of each factor, given a factor that has
finer resolution than both. Finally, you can think of a character
vector as a factor with every possible level, so factors and character
vectors are coercible.

(extracted from the in-progress vignette explaining how to extend
vctrs to work with your own vctrs, now that vctrs has been rewritten
to use double dispatch)

Hadley

-- 
http://hadley.nz


From m@echler @ending from @t@t@m@th@ethz@ch  Wed Aug  8 17:54:16 2018
From: m@echler @ending from @t@t@m@th@ethz@ch (Martin Maechler)
Date: Wed, 8 Aug 2018 17:54:16 +0200
Subject: [Rd] vctrs: a type system for the tidyverse
In-Reply-To: <CABdHhvHw+_sFcD3H92KQhvurxC8eAyR2A0frZUk7pBcOaVB2Hw@mail.gmail.com>
References: <CABdHhvFARmiExcEk=NUjfPcRtYnoF4pAA5WTws6r1C+X76zHEw@mail.gmail.com>
 <CAMFmJsn_wNxuV2SQqUiZroJYV+rHRKdADi8Fjg8fRjJr0RGuwQ@mail.gmail.com>
 <CABdHhvEAV-bhprxVMHF_R8vKdN3VEriV86kGNNU02RiK=+m4dQ@mail.gmail.com>
 <CABdHhvHw+_sFcD3H92KQhvurxC8eAyR2A0frZUk7pBcOaVB2Hw@mail.gmail.com>
Message-ID: <23403.4648.841262.201573@stat.math.ethz.ch>

>>>>> Hadley Wickham 
>>>>>     on Wed, 8 Aug 2018 09:34:42 -0500 writes:

    >>>> Method dispatch for `vec_c()` is quite simple because
    >>>> associativity and commutativity mean that we can
    >>>> determine the output type only by considering a pair of
    >>>> inputs at a time. To this end, vctrs provides
    >>>> `vec_type2()` which takes two inputs and returns their
    >>>> common type (represented as zero length vector):
    >>>> 
    >>>> str(vec_type2(integer(), double())) #> num(0)
    >>>> 
    >>>> str(vec_type2(factor("a"), factor("b"))) #> Factor w/ 2
    >>>> levels "a","b":
    >>> 
    >>> 
    >>> What is the reasoning behind taking the union of the
    >>> levels here? I'm not sure that is actually the behavior
    >>> I would want if I have a vector of factors and I try to
    >>> append some new data to it. I might want/ expect to
    >>> retain the existing levels and get either NAs or an
    >>> error if the new data has (present) levels not in the
    >>> first data. The behavior as above doesn't seem in-line
    >>> with what I understand the purpose of factors to be
    >>> (explicit restriction of possible values).
    >> 
    >> Originally (like a week ago ?), we threw an error if the
    >> factors didn't have the same level, and provided an
    >> optional coercion to character. I decided that while
    >> correct (the factor levels are a parameter of the type,
    >> and hence factors with different levels aren't
    >> comparable), that this fights too much against how people
    >> actually use factors in practice. It also seems like base
    >> R is moving more in this direction, i.e. in 3.4
    >> factor("a") == factor("b") is an error, whereas in R 3.5
    >> it returns FALSE.

    > I now have a better argument, I think:

    > If you squint your brain a little, I think you can see
    > that each set of automatic coercions is about increasing
    > resolution. Integers are low resolution versions of
    > doubles, and dates are low resolution versions of
    > date-times. Logicals are low resolution version of
    > integers because there's a strong convention that `TRUE`
    > and `FALSE` can be used interchangeably with `1` and `0`.

    > But what is the resolution of a factor? We must take a
    > somewhat pragmatic approach because base R often converts
    > character vectors to factors, and we don't want to be
    > burdensome to users. So we say that a factor `x` has finer
    > resolution than factor `y` if the levels of `y` are
    > contained in `x`. So to find the common type of two
    > factors, we take the union of the levels of each factor,
    > given a factor that has finer resolution than
    > both. Finally, you can think of a character vector as a
    > factor with every possible level, so factors and character
    > vectors are coercible.

    > (extracted from the in-progress vignette explaining how to
    > extend vctrs to work with your own vctrs, now that vctrs
    > has been rewritten to use double dispatch)

I like this argumentation, and find it very nice indeed!
It confirms my own gut feeling which had lead me to agreeing
with you, Hadley, that taking the union of all factor levels
should be done here.

As Gabe mentioned (and you've explained about) the term "type"
is really confusing here.  As you know, the R internals are all
about SEXPs, TYPEOF(), etc, and that's what the R level
typeof(.) also returns.  As you want to use something slightly
different, it should be different naming, ideally something not
existing yet in the R / S world, maybe 'kind' ?

Martin


    > Hadley

    > -- 
    > http://hadley.nz

    > ______________________________________________
    > R-devel at r-project.org mailing list
    > https://stat.ethz.ch/mailman/listinfo/r-devel


From becker@g@be @ending from gene@com  Wed Aug  8 17:56:00 2018
From: becker@g@be @ending from gene@com (Gabe Becker)
Date: Wed, 8 Aug 2018 08:56:00 -0700
Subject: [Rd] vctrs: a type system for the tidyverse
In-Reply-To: <CABdHhvHw+_sFcD3H92KQhvurxC8eAyR2A0frZUk7pBcOaVB2Hw@mail.gmail.com>
References: <CABdHhvFARmiExcEk=NUjfPcRtYnoF4pAA5WTws6r1C+X76zHEw@mail.gmail.com>
 <CAMFmJsn_wNxuV2SQqUiZroJYV+rHRKdADi8Fjg8fRjJr0RGuwQ@mail.gmail.com>
 <CABdHhvEAV-bhprxVMHF_R8vKdN3VEriV86kGNNU02RiK=+m4dQ@mail.gmail.com>
 <CABdHhvHw+_sFcD3H92KQhvurxC8eAyR2A0frZUk7pBcOaVB2Hw@mail.gmail.com>
Message-ID: <CAMFmJskmkCpVGOYC7=1oPm=xRUbBQJTvyzg=G+Of7+R6UDVUJw@mail.gmail.com>

Hadley,

Responses inline.

On Wed, Aug 8, 2018 at 7:34 AM, Hadley Wickham <h.wickham at gmail.com> wrote:

> >>> Method dispatch for `vec_c()` is quite simple because associativity and
> >>> commutativity mean that we can determine the output type only by
> >>> considering a pair of inputs at a time. To this end, vctrs provides
> >>> `vec_type2()` which takes two inputs and returns their common type
> >>> (represented as zero length vector):
> >>>
> >>>     str(vec_type2(integer(), double()))
> >>>     #>  num(0)
> >>>
> >>>     str(vec_type2(factor("a"), factor("b")))
> >>>     #>  Factor w/ 2 levels "a","b":
> >>
> >>
> >> What is the reasoning behind taking the union of the levels here? I'm
> not
> >> sure that is actually the behavior I would want if I have a vector of
> >> factors and I try to append some new data to it. I might want/ expect to
> >> retain the existing levels and get either NAs or an error if the new
> data
> >> has (present) levels not in the first data. The behavior as above
> doesn't
> >> seem in-line with what I understand the purpose of factors to be
> (explicit
> >> restriction of possible values).
> >
> > Originally (like a week ago ?), we threw an error if the factors
> > didn't have the same level, and provided an optional coercion to
> > character. I decided that while correct (the factor levels are a
> > parameter of the type, and hence factors with different levels aren't
> > comparable), that this fights too much against how people actually use
> > factors in practice. It also seems like base R is moving more in this
> > direction, i.e. in 3.4 factor("a") == factor("b") is an error, whereas
> > in R 3.5 it returns FALSE.
>
> I now have a better argument, I think:
>
> If you squint your brain a little, I think you can see that each set
> of automatic coercions is about increasing resolution. Integers are
> low resolution versions of doubles, and dates are low resolution
> versions of date-times. Logicals are low resolution version of
> integers because there's a strong convention that `TRUE` and `FALSE`
> can be used interchangeably with `1` and `0`.
>
> But what is the resolution of a factor? We must take a somewhat
> pragmatic approach because base R often converts character vectors to
> factors, and we don't want to be burdensome to users.


I don't know, I personally just don't buy this line of reasoning. Yes, you
can convert between characters and factors, but that doesn't make factors
"a special kind of character", which you seem to be implicitly arguing they
are. Fundamentally they are different objects with different purposes. As I
said in my previous email, the primary semantic purpose of factors is value
restriction. You don't WANT to increase the set of levels when your set of
values has already been carefully curated. Certainly not automagically.


> So we say that a
> factor `x` has finer resolution than factor `y` if the levels of `y`
> are contained in `x`. So to find the common type of two factors, we
> take the union of the levels of each factor, given a factor that has
> finer resolution than both.


I'm not so sure. I think a more useful definition of resolution may be that
it is about increasing the precision of information. In that case, a factor
with 4 levels each of which is present has a *higher* resolution than the
same data with additional-but-absent levels on the factor object.  Now that
may be different when the the new levels are not absent, but my point is
that its not clear to me that resolution is a useful way of talking about
factors.


> Finally, you can think of a character
> vector as a factor with every possible level, so factors and character
> vectors are coercible.
>



If users want unrestricted character type behavior, then IMHO they should
just be using characters, and it's quite easy for them to do so in any case
I can easily think of where they have somehow gotten their hands on a
factor. If, however, they want a factor, it must be - I imagine - because
they actually want the the semantics and behavior *specific* to factors.

Best,
~G


>
> (extracted from the in-progress vignette explaining how to extend
> vctrs to work with your own vctrs, now that vctrs has been rewritten
> to use double dispatch)
>
> Hadley
>
> --
> http://hadley.nz
>



-- 
Gabriel Becker, Ph.D
Scientist
Bioinformatics and Computational Biology
Genentech Research

	[[alternative HTML version deleted]]


From becker@g@be @ending from gene@com  Wed Aug  8 18:03:37 2018
From: becker@g@be @ending from gene@com (Gabe Becker)
Date: Wed, 8 Aug 2018 09:03:37 -0700
Subject: [Rd] vctrs: a type system for the tidyverse
In-Reply-To: <23403.4648.841262.201573@stat.math.ethz.ch>
References: <CABdHhvFARmiExcEk=NUjfPcRtYnoF4pAA5WTws6r1C+X76zHEw@mail.gmail.com>
 <CAMFmJsn_wNxuV2SQqUiZroJYV+rHRKdADi8Fjg8fRjJr0RGuwQ@mail.gmail.com>
 <CABdHhvEAV-bhprxVMHF_R8vKdN3VEriV86kGNNU02RiK=+m4dQ@mail.gmail.com>
 <CABdHhvHw+_sFcD3H92KQhvurxC8eAyR2A0frZUk7pBcOaVB2Hw@mail.gmail.com>
 <23403.4648.841262.201573@stat.math.ethz.ch>
Message-ID: <CAMFmJs=hHXYdkd-2kRH48sxB5VJAO4g5Rs35einXs8GwQJ7C8g@mail.gmail.com>

Actually, I sent that too quickly, I should have let it stew a bit more.
I've changed my mind about the resolution argument I Was trying to make.
There is more information, technically speaking, in the factor with empty
levels. I'm still not convinced that its the right behavior, personally. It
may just be me though, since Martin seems on board. Mostly I'm just very
wary of taking away the thing about factors that makes them fundamentally
not characters, and removing the effectiveness of the level restriction, in
practice, does that.

Best,
~G

On Wed, Aug 8, 2018 at 8:54 AM, Martin Maechler <maechler at stat.math.ethz.ch>
wrote:

> >>>>> Hadley Wickham
> >>>>>     on Wed, 8 Aug 2018 09:34:42 -0500 writes:
>
>     >>>> Method dispatch for `vec_c()` is quite simple because
>     >>>> associativity and commutativity mean that we can
>     >>>> determine the output type only by considering a pair of
>     >>>> inputs at a time. To this end, vctrs provides
>     >>>> `vec_type2()` which takes two inputs and returns their
>     >>>> common type (represented as zero length vector):
>     >>>>
>     >>>> str(vec_type2(integer(), double())) #> num(0)
>     >>>>
>     >>>> str(vec_type2(factor("a"), factor("b"))) #> Factor w/ 2
>     >>>> levels "a","b":
>     >>>
>     >>>
>     >>> What is the reasoning behind taking the union of the
>     >>> levels here? I'm not sure that is actually the behavior
>     >>> I would want if I have a vector of factors and I try to
>     >>> append some new data to it. I might want/ expect to
>     >>> retain the existing levels and get either NAs or an
>     >>> error if the new data has (present) levels not in the
>     >>> first data. The behavior as above doesn't seem in-line
>     >>> with what I understand the purpose of factors to be
>     >>> (explicit restriction of possible values).
>     >>
>     >> Originally (like a week ago ?), we threw an error if the
>     >> factors didn't have the same level, and provided an
>     >> optional coercion to character. I decided that while
>     >> correct (the factor levels are a parameter of the type,
>     >> and hence factors with different levels aren't
>     >> comparable), that this fights too much against how people
>     >> actually use factors in practice. It also seems like base
>     >> R is moving more in this direction, i.e. in 3.4
>     >> factor("a") == factor("b") is an error, whereas in R 3.5
>     >> it returns FALSE.
>
>     > I now have a better argument, I think:
>
>     > If you squint your brain a little, I think you can see
>     > that each set of automatic coercions is about increasing
>     > resolution. Integers are low resolution versions of
>     > doubles, and dates are low resolution versions of
>     > date-times. Logicals are low resolution version of
>     > integers because there's a strong convention that `TRUE`
>     > and `FALSE` can be used interchangeably with `1` and `0`.
>
>     > But what is the resolution of a factor? We must take a
>     > somewhat pragmatic approach because base R often converts
>     > character vectors to factors, and we don't want to be
>     > burdensome to users. So we say that a factor `x` has finer
>     > resolution than factor `y` if the levels of `y` are
>     > contained in `x`. So to find the common type of two
>     > factors, we take the union of the levels of each factor,
>     > given a factor that has finer resolution than
>     > both. Finally, you can think of a character vector as a
>     > factor with every possible level, so factors and character
>     > vectors are coercible.
>
>     > (extracted from the in-progress vignette explaining how to
>     > extend vctrs to work with your own vctrs, now that vctrs
>     > has been rewritten to use double dispatch)
>
> I like this argumentation, and find it very nice indeed!
> It confirms my own gut feeling which had lead me to agreeing
> with you, Hadley, that taking the union of all factor levels
> should be done here.
>
> As Gabe mentioned (and you've explained about) the term "type"
> is really confusing here.  As you know, the R internals are all
> about SEXPs, TYPEOF(), etc, and that's what the R level
> typeof(.) also returns.  As you want to use something slightly
> different, it should be different naming, ideally something not
> existing yet in the R / S world, maybe 'kind' ?
>
> Martin
>
>
>     > Hadley
>
>     > --
>     > http://hadley.nz
>
>     > ______________________________________________
>     > R-devel at r-project.org mailing list
>     > https://stat.ethz.ch/mailman/listinfo/r-devel
>
>


-- 
Gabriel Becker, Ph.D
Scientist
Bioinformatics and Computational Biology
Genentech Research

	[[alternative HTML version deleted]]


From h@wickh@m @ending from gm@il@com  Wed Aug  8 19:37:05 2018
From: h@wickh@m @ending from gm@il@com (Hadley Wickham)
Date: Wed, 8 Aug 2018 12:37:05 -0500
Subject: [Rd] vctrs: a type system for the tidyverse
In-Reply-To: <23403.4648.841262.201573@stat.math.ethz.ch>
References: <CABdHhvFARmiExcEk=NUjfPcRtYnoF4pAA5WTws6r1C+X76zHEw@mail.gmail.com>
 <CAMFmJsn_wNxuV2SQqUiZroJYV+rHRKdADi8Fjg8fRjJr0RGuwQ@mail.gmail.com>
 <CABdHhvEAV-bhprxVMHF_R8vKdN3VEriV86kGNNU02RiK=+m4dQ@mail.gmail.com>
 <CABdHhvHw+_sFcD3H92KQhvurxC8eAyR2A0frZUk7pBcOaVB2Hw@mail.gmail.com>
 <23403.4648.841262.201573@stat.math.ethz.ch>
Message-ID: <CABdHhvH53=0b7Hg=9dAPx9zUTn4YSYR0=JoDzWMRMjkMpmXbBQ@mail.gmail.com>

>     > I now have a better argument, I think:
>
>     > If you squint your brain a little, I think you can see
>     > that each set of automatic coercions is about increasing
>     > resolution. Integers are low resolution versions of
>     > doubles, and dates are low resolution versions of
>     > date-times. Logicals are low resolution version of
>     > integers because there's a strong convention that `TRUE`
>     > and `FALSE` can be used interchangeably with `1` and `0`.
>
>     > But what is the resolution of a factor? We must take a
>     > somewhat pragmatic approach because base R often converts
>     > character vectors to factors, and we don't want to be
>     > burdensome to users. So we say that a factor `x` has finer
>     > resolution than factor `y` if the levels of `y` are
>     > contained in `x`. So to find the common type of two
>     > factors, we take the union of the levels of each factor,
>     > given a factor that has finer resolution than
>     > both. Finally, you can think of a character vector as a
>     > factor with every possible level, so factors and character
>     > vectors are coercible.
>
>     > (extracted from the in-progress vignette explaining how to
>     > extend vctrs to work with your own vctrs, now that vctrs
>     > has been rewritten to use double dispatch)
>
> I like this argumentation, and find it very nice indeed!
> It confirms my own gut feeling which had lead me to agreeing
> with you, Hadley, that taking the union of all factor levels
> should be done here.

That's great to hear :)

> As Gabe mentioned (and you've explained about) the term "type"
> is really confusing here.  As you know, the R internals are all
> about SEXPs, TYPEOF(), etc, and that's what the R level
> typeof(.) also returns.  As you want to use something slightly
> different, it should be different naming, ideally something not
> existing yet in the R / S world, maybe 'kind' ?

Agreed - I've been using type in the sense of "type system"
(particularly as it related to algebraic data types), but that's not
obvious from the current presentation, and as you note, is confusing
with existing notions of type in R. I like your suggestion of kind,
but I think it might be possible to just talk about classes, and
instead emphasise that while the components of the system are classes
(and indeed it's implemented using S3), the coercion/casting
relationship do not strictly follow the subclass/superclass
relationships.

A good motivating example is now ordered vs factor - I don't think you
can say that ordered or factor have greater resolution than the other
so:

vec_c(factor("a"), ordered("a"))
#> Error: No common type for factor and ordered

This is not what you'd expect from an _object_ system since ordered is
a subclass of factor.

Hadley

-- 
http://hadley.nz


From h@wickh@m @ending from gm@il@com  Wed Aug  8 19:40:48 2018
From: h@wickh@m @ending from gm@il@com (Hadley Wickham)
Date: Wed, 8 Aug 2018 12:40:48 -0500
Subject: [Rd] vctrs: a type system for the tidyverse
In-Reply-To: <CAMFmJskmkCpVGOYC7=1oPm=xRUbBQJTvyzg=G+Of7+R6UDVUJw@mail.gmail.com>
References: <CABdHhvFARmiExcEk=NUjfPcRtYnoF4pAA5WTws6r1C+X76zHEw@mail.gmail.com>
 <CAMFmJsn_wNxuV2SQqUiZroJYV+rHRKdADi8Fjg8fRjJr0RGuwQ@mail.gmail.com>
 <CABdHhvEAV-bhprxVMHF_R8vKdN3VEriV86kGNNU02RiK=+m4dQ@mail.gmail.com>
 <CABdHhvHw+_sFcD3H92KQhvurxC8eAyR2A0frZUk7pBcOaVB2Hw@mail.gmail.com>
 <CAMFmJskmkCpVGOYC7=1oPm=xRUbBQJTvyzg=G+Of7+R6UDVUJw@mail.gmail.com>
Message-ID: <CABdHhvH=9iGykkmo77b3ZbreWh5_g=bfLcJm_KrdOGAP859cfQ@mail.gmail.com>

>> So we say that a
>> factor `x` has finer resolution than factor `y` if the levels of `y`
>> are contained in `x`. So to find the common type of two factors, we
>> take the union of the levels of each factor, given a factor that has
>> finer resolution than both.
>
> I'm not so sure. I think a more useful definition of resolution may be
> that it is about increasing the precision of information. In that case,
> a factor with 4 levels each of which is present has a higher resolution
> than the same data with additional-but-absent levels on the factor object.
> Now that may be different when the the new levels are not absent, but
> my point is that its not clear to me that resolution is a useful way of
> talking about factors.

An alternative way of framing factors is that they're about tracking
possible values, particular possible values that don't exist in the
data that you have. Thinking about factors in that way, makes unioning
the levels more natural.

> If users want unrestricted character type behavior, then IMHO they should
> just be using characters, and it's quite easy for them to do so in any case
> I can easily think of where they have somehow gotten their hands on a factor.
> If, however, they want a factor, it must be - I imagine - because they actually
> want the the semantics and behavior specific to factors.

I think this is true in the tidyverse, which will never give you a
factor unless you explicitly ask for one, but the default in base R
(at least as soon as a data frame is involved) is to turn character
vectors into factors.

Hadley

-- 
http://hadley.nz


From i@uc@r86 @ending from gm@il@com  Wed Aug  8 19:47:38 2018
From: i@uc@r86 @ending from gm@il@com (=?UTF-8?B?ScOxYWtpIMOaY2Fy?=)
Date: Wed, 8 Aug 2018 19:47:38 +0200
Subject: [Rd] vctrs: a type system for the tidyverse
In-Reply-To: <CAMFmJs=hHXYdkd-2kRH48sxB5VJAO4g5Rs35einXs8GwQJ7C8g@mail.gmail.com>
References: <CABdHhvFARmiExcEk=NUjfPcRtYnoF4pAA5WTws6r1C+X76zHEw@mail.gmail.com>
 <CAMFmJsn_wNxuV2SQqUiZroJYV+rHRKdADi8Fjg8fRjJr0RGuwQ@mail.gmail.com>
 <CABdHhvEAV-bhprxVMHF_R8vKdN3VEriV86kGNNU02RiK=+m4dQ@mail.gmail.com>
 <CABdHhvHw+_sFcD3H92KQhvurxC8eAyR2A0frZUk7pBcOaVB2Hw@mail.gmail.com>
 <23403.4648.841262.201573@stat.math.ethz.ch>
 <CAMFmJs=hHXYdkd-2kRH48sxB5VJAO4g5Rs35einXs8GwQJ7C8g@mail.gmail.com>
Message-ID: <CALEXWq2J-J5obavQ=dpkHJmqpt9WQ5a3uZS7L6gY-Fs4X_AuNg@mail.gmail.com>

El mi?., 8 ago. 2018 a las 19:23, Gabe Becker (<becker.gabe at gene.com>) escribi?:
>
> Actually, I sent that too quickly, I should have let it stew a bit more.
> I've changed my mind about the resolution argument I Was trying to make.
> There is more information, technically speaking, in the factor with empty
> levels. I'm still not convinced that its the right behavior, personally. It
> may just be me though, since Martin seems on board. Mostly I'm just very
> wary of taking away the thing about factors that makes them fundamentally
> not characters, and removing the effectiveness of the level restriction, in
> practice, does that.

For what it's worth, I always thought about factors as fundamentally
characters, but with restrictions: a subspace of all possible strings.
And I'd say that a non-negligible number of R users may think about
them in a similar way.

In fact, if you search "concatenation factors", you'll see that back
in 2008 somebody asked on R-help [1] because he wanted to do exactly
what Hadley is describing (i.e., concatenation as character with
levels as a union of the levels), and he was surprised because...
well, the behaviour of c.factor is quite surprising if you don't read
the manual.

BTW, the solution proposed was unlist(list(fct1, fct2)).

[1] https://www.mail-archive.com/r-help at r-project.org/msg38360.html

I?aki

>
> Best,
> ~G
>
> On Wed, Aug 8, 2018 at 8:54 AM, Martin Maechler <maechler at stat.math.ethz.ch>
> wrote:
>
> > >>>>> Hadley Wickham
> > >>>>>     on Wed, 8 Aug 2018 09:34:42 -0500 writes:
> >
> >     >>>> Method dispatch for `vec_c()` is quite simple because
> >     >>>> associativity and commutativity mean that we can
> >     >>>> determine the output type only by considering a pair of
> >     >>>> inputs at a time. To this end, vctrs provides
> >     >>>> `vec_type2()` which takes two inputs and returns their
> >     >>>> common type (represented as zero length vector):
> >     >>>>
> >     >>>> str(vec_type2(integer(), double())) #> num(0)
> >     >>>>
> >     >>>> str(vec_type2(factor("a"), factor("b"))) #> Factor w/ 2
> >     >>>> levels "a","b":
> >     >>>
> >     >>>
> >     >>> What is the reasoning behind taking the union of the
> >     >>> levels here? I'm not sure that is actually the behavior
> >     >>> I would want if I have a vector of factors and I try to
> >     >>> append some new data to it. I might want/ expect to
> >     >>> retain the existing levels and get either NAs or an
> >     >>> error if the new data has (present) levels not in the
> >     >>> first data. The behavior as above doesn't seem in-line
> >     >>> with what I understand the purpose of factors to be
> >     >>> (explicit restriction of possible values).
> >     >>
> >     >> Originally (like a week ago ), we threw an error if the
> >     >> factors didn't have the same level, and provided an
> >     >> optional coercion to character. I decided that while
> >     >> correct (the factor levels are a parameter of the type,
> >     >> and hence factors with different levels aren't
> >     >> comparable), that this fights too much against how people
> >     >> actually use factors in practice. It also seems like base
> >     >> R is moving more in this direction, i.e. in 3.4
> >     >> factor("a") == factor("b") is an error, whereas in R 3.5
> >     >> it returns FALSE.
> >
> >     > I now have a better argument, I think:
> >
> >     > If you squint your brain a little, I think you can see
> >     > that each set of automatic coercions is about increasing
> >     > resolution. Integers are low resolution versions of
> >     > doubles, and dates are low resolution versions of
> >     > date-times. Logicals are low resolution version of
> >     > integers because there's a strong convention that `TRUE`
> >     > and `FALSE` can be used interchangeably with `1` and `0`.
> >
> >     > But what is the resolution of a factor? We must take a
> >     > somewhat pragmatic approach because base R often converts
> >     > character vectors to factors, and we don't want to be
> >     > burdensome to users. So we say that a factor `x` has finer
> >     > resolution than factor `y` if the levels of `y` are
> >     > contained in `x`. So to find the common type of two
> >     > factors, we take the union of the levels of each factor,
> >     > given a factor that has finer resolution than
> >     > both. Finally, you can think of a character vector as a
> >     > factor with every possible level, so factors and character
> >     > vectors are coercible.
> >
> >     > (extracted from the in-progress vignette explaining how to
> >     > extend vctrs to work with your own vctrs, now that vctrs
> >     > has been rewritten to use double dispatch)
> >
> > I like this argumentation, and find it very nice indeed!
> > It confirms my own gut feeling which had lead me to agreeing
> > with you, Hadley, that taking the union of all factor levels
> > should be done here.
> >
> > As Gabe mentioned (and you've explained about) the term "type"
> > is really confusing here.  As you know, the R internals are all
> > about SEXPs, TYPEOF(), etc, and that's what the R level
> > typeof(.) also returns.  As you want to use something slightly
> > different, it should be different naming, ideally something not
> > existing yet in the R / S world, maybe 'kind' ?
> >
> > Martin
> >
> >
> >     > Hadley
> >
> >     > --
> >     > http://hadley.nz
> >
> >     > ______________________________________________
> >     > R-devel at r-project.org mailing list
> >     > https://stat.ethz.ch/mailman/listinfo/r-devel
> >
> >
>
>
> --
> Gabriel Becker, Ph.D
> Scientist
> Bioinformatics and Computational Biology
> Genentech Research
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From jori@mey@ @ending from gm@il@com  Thu Aug  9 10:58:52 2018
From: jori@mey@ @ending from gm@il@com (Joris Meys)
Date: Thu, 9 Aug 2018 10:58:52 +0200
Subject: [Rd] vctrs: a type system for the tidyverse
In-Reply-To: <CALEXWq2J-J5obavQ=dpkHJmqpt9WQ5a3uZS7L6gY-Fs4X_AuNg@mail.gmail.com>
References: <CABdHhvFARmiExcEk=NUjfPcRtYnoF4pAA5WTws6r1C+X76zHEw@mail.gmail.com>
 <CAMFmJsn_wNxuV2SQqUiZroJYV+rHRKdADi8Fjg8fRjJr0RGuwQ@mail.gmail.com>
 <CABdHhvEAV-bhprxVMHF_R8vKdN3VEriV86kGNNU02RiK=+m4dQ@mail.gmail.com>
 <CABdHhvHw+_sFcD3H92KQhvurxC8eAyR2A0frZUk7pBcOaVB2Hw@mail.gmail.com>
 <23403.4648.841262.201573@stat.math.ethz.ch>
 <CAMFmJs=hHXYdkd-2kRH48sxB5VJAO4g5Rs35einXs8GwQJ7C8g@mail.gmail.com>
 <CALEXWq2J-J5obavQ=dpkHJmqpt9WQ5a3uZS7L6gY-Fs4X_AuNg@mail.gmail.com>
Message-ID: <CAO1zAVbaB9fmtrFFYLuusm2jOyh1SHQnDm3U8vjFPNiitk=+hw@mail.gmail.com>

 I sent this to  I?aki personally by mistake. Thank you for notifying me.

On Wed, Aug 8, 2018 at 7:53 PM I?aki ?car <i.ucar86 at gmail.com> wrote:

>
> For what it's worth, I always thought about factors as fundamentally
> characters, but with restrictions: a subspace of all possible strings.
> And I'd say that a non-negligible number of R users may think about
> them in a similar way.
>

That idea has been a common source of bugs and the most important reason
why I always explain my students that factors are a special kind of
numeric(integer), not character. Especially people coming from SPSS see
immediately the link with categorical variables in that way, and understand
that a factor is a modeling aid rather than an alternative for characters.
It is a categorical variable and a more readable way of representing a set
of dummy variables.

I do agree that some of the factor behaviour is confusing at best, but that
doesn't change the appropriate use and meaning of factors as categorical
variables.

Even more, I oppose the ideas that :

1) factors with different levels should be concatenated.

2) when combining factors, the union of the levels would somehow be a good
choice.

Factors with different levels are variables with different information, not
more or less information. If one factor codes low and high and another
codes low, mid and high, you can't say whether mid in one factor would be
low or high in the first one. The second has a higher resolution, and
that's exactly the reason why they should NOT be combined. Different levels
indicate a different grouping, and hence that data should never be used as
one set of dummy variables in any model.

Even when combining factors, the union of levels only makes sense to me if
there's no overlap between levels of both factors. In all other cases, a
researcher will need to determine whether levels with the same label do
mean the same thing in both factors, and that's not guaranteed. And when
we're talking a factor with a higher resolution and a lower resolution, the
correct thing to do modelwise is to recode one of the factors so they have
the same resolution and every level the same definition before you merge
that data.

So imho the combination of two factors with different levels (or even
levels in a different order) should give an error. Which R currently
doesn't throw, so I get there's room for improvement.

Cheers
Joris
-- 
Joris Meys
Statistical consultant

Department of Data Analysis and Mathematical Modelling
Ghent University
Coupure Links 653, B-9000 Gent (Belgium)
<https://maps.google.com/?q=Coupure+links+653,%C2%A0B-9000+Gent,%C2%A0Belgium&entry=gmail&source=g>

	[[alternative HTML version deleted]]


From Andre@@@Fe@tl @ending from v2c2@@t  Thu Aug  9 09:26:31 2018
From: Andre@@@Fe@tl @ending from v2c2@@t (Festl, Andreas)
Date: Thu, 9 Aug 2018 07:26:31 +0000
Subject: [Rd] Bug in POSIXct string representation?
Message-ID: <C4657A34F48F9540B0852AC259A2128526D9DF4D@EX2010MX01.v2c2.at>

Dear all,

I just have identified the following issue which I believe could be a bug in R:

Let me illustrate:

First, enable the display of fractional seconds and check that it works:
> options(digits.secs = 6, digits = 6)
> as.character(as.POSIXct("2018-08-31 14:15:16.123456"))
[1] "2018-08-31 14:15:16.123456"

Now create a sequence of POSIXct with stepwidth 0.1sec:
> test <- as.POSIXct("2018-08-31 14:15:16.000000")
> test_seq <- seq(test, test + 1, by = 1/10)

Calling format with the millisecond conversion specification gives the intended result (even though there is a small representation error):
> format(test_seq, "%F %T.%OS")
 [1] "2018-08-31 14:15:16.16.000000" "2018-08-31 14:15:16.16.099999" "2018-08-31 14:15:16.16.200000" "2018-08-31 14:15:16.16.299999"
 [5] "2018-08-31 14:15:16.16.400000" "2018-08-31 14:15:16.16.500000" "2018-08-31 14:15:16.16.599999" "2018-08-31 14:15:16.16.700000"
 [9] "2018-08-31 14:15:16.16.799999" "2018-08-31 14:15:16.16.900000" "2018-08-31 14:15:17.17.000000"

However, if I use as.character, the milliseconds seemingly just get cut-off after one digit, resulting in incorrect representations:
> as.character(test_seq)
 [1] "2018-08-31 14:15:16.0" "2018-08-31 14:15:16.0" "2018-08-31 14:15:16.2" "2018-08-31 14:15:16.2" "2018-08-31 14:15:16.4" "2018-08-31 14:15:16.5"
 [7] "2018-08-31 14:15:16.5" "2018-08-31 14:15:16.7" "2018-08-31 14:15:16.7" "2018-08-31 14:15:16.9" "2018-08-31 14:15:17.0"

It seems to me, that R correctly decides that there is only one significant digit after the decimal point, but then incorrectly (due to representation error) just cuts off after the first digit.

BR,
  Andreas


From h@wickh@m @ending from gm@il@com  Thu Aug  9 14:36:28 2018
From: h@wickh@m @ending from gm@il@com (Hadley Wickham)
Date: Thu, 9 Aug 2018 07:36:28 -0500
Subject: [Rd] vctrs: a type system for the tidyverse
In-Reply-To: <CAO1zAVbaB9fmtrFFYLuusm2jOyh1SHQnDm3U8vjFPNiitk=+hw@mail.gmail.com>
References: <CABdHhvFARmiExcEk=NUjfPcRtYnoF4pAA5WTws6r1C+X76zHEw@mail.gmail.com>
 <CAMFmJsn_wNxuV2SQqUiZroJYV+rHRKdADi8Fjg8fRjJr0RGuwQ@mail.gmail.com>
 <CABdHhvEAV-bhprxVMHF_R8vKdN3VEriV86kGNNU02RiK=+m4dQ@mail.gmail.com>
 <CABdHhvHw+_sFcD3H92KQhvurxC8eAyR2A0frZUk7pBcOaVB2Hw@mail.gmail.com>
 <23403.4648.841262.201573@stat.math.ethz.ch>
 <CAMFmJs=hHXYdkd-2kRH48sxB5VJAO4g5Rs35einXs8GwQJ7C8g@mail.gmail.com>
 <CALEXWq2J-J5obavQ=dpkHJmqpt9WQ5a3uZS7L6gY-Fs4X_AuNg@mail.gmail.com>
 <CAO1zAVbaB9fmtrFFYLuusm2jOyh1SHQnDm3U8vjFPNiitk=+hw@mail.gmail.com>
Message-ID: <CABdHhvH9Xo-W3gj7Sk+htfa9uzVLhWZSmk_mP+o6BoaFub=1hQ@mail.gmail.com>

On Thu, Aug 9, 2018 at 3:57 AM Joris Meys <jorismeys at gmail.com> wrote:
>
>  I sent this to  I?aki personally by mistake. Thank you for notifying me.
>
> On Wed, Aug 8, 2018 at 7:53 PM I?aki ?car <i.ucar86 at gmail.com> wrote:
>
> >
> > For what it's worth, I always thought about factors as fundamentally
> > characters, but with restrictions: a subspace of all possible strings.
> > And I'd say that a non-negligible number of R users may think about
> > them in a similar way.
> >
>
> That idea has been a common source of bugs and the most important reason
> why I always explain my students that factors are a special kind of
> numeric(integer), not character. Especially people coming from SPSS see
> immediately the link with categorical variables in that way, and understand
> that a factor is a modeling aid rather than an alternative for characters.
> It is a categorical variable and a more readable way of representing a set
> of dummy variables.
>
> I do agree that some of the factor behaviour is confusing at best, but that
> doesn't change the appropriate use and meaning of factors as categorical
> variables.
>
> Even more, I oppose the ideas that :
>
> 1) factors with different levels should be concatenated.
>
> 2) when combining factors, the union of the levels would somehow be a good
> choice.
>
> Factors with different levels are variables with different information, not
> more or less information. If one factor codes low and high and another
> codes low, mid and high, you can't say whether mid in one factor would be
> low or high in the first one. The second has a higher resolution, and
> that's exactly the reason why they should NOT be combined. Different levels
> indicate a different grouping, and hence that data should never be used as
> one set of dummy variables in any model.
>
> Even when combining factors, the union of levels only makes sense to me if
> there's no overlap between levels of both factors. In all other cases, a
> researcher will need to determine whether levels with the same label do
> mean the same thing in both factors, and that's not guaranteed. And when
> we're talking a factor with a higher resolution and a lower resolution, the
> correct thing to do modelwise is to recode one of the factors so they have
> the same resolution and every level the same definition before you merge
> that data.
>
> So imho the combination of two factors with different levels (or even
> levels in a different order) should give an error. Which R currently
> doesn't throw, so I get there's room for improvement.

I 100% agree with you, and is this the behaviour that vctrs used to
have and dplyr currently has (at least in bind_rows()). But
pragmatically, my experience with dplyr is that people find this
behaviour confusing and unhelpful. And when I played the full
expression of this behaviour in vctrs, I found that it forced me to
think about the levels of factors more than I'd otherwise like to: it
made me think like a programmer, not like a data analyst. So in an
ideal world, yes, I think factors would have stricter behaviour, but
my sense is that imposing this strictness now will be onerous to most
analysts.

Hadley

-- 
http://hadley.nz


From edd @ending from debi@n@org  Thu Aug  9 14:42:58 2018
From: edd @ending from debi@n@org (Dirk Eddelbuettel)
Date: Thu, 9 Aug 2018 07:42:58 -0500
Subject: [Rd] vctrs: a type system for the tidyverse
In-Reply-To: <CABdHhvH=9iGykkmo77b3ZbreWh5_g=bfLcJm_KrdOGAP859cfQ@mail.gmail.com>
References: <CABdHhvFARmiExcEk=NUjfPcRtYnoF4pAA5WTws6r1C+X76zHEw@mail.gmail.com>
 <CAMFmJsn_wNxuV2SQqUiZroJYV+rHRKdADi8Fjg8fRjJr0RGuwQ@mail.gmail.com>
 <CABdHhvEAV-bhprxVMHF_R8vKdN3VEriV86kGNNU02RiK=+m4dQ@mail.gmail.com>
 <CABdHhvHw+_sFcD3H92KQhvurxC8eAyR2A0frZUk7pBcOaVB2Hw@mail.gmail.com>
 <CAMFmJskmkCpVGOYC7=1oPm=xRUbBQJTvyzg=G+Of7+R6UDVUJw@mail.gmail.com>
 <CABdHhvH=9iGykkmo77b3ZbreWh5_g=bfLcJm_KrdOGAP859cfQ@mail.gmail.com>
Message-ID: <23404.14034.740537.291529@rob.eddelbuettel.com>


On 8 August 2018 at 12:40, Hadley Wickham wrote:
| I think this is true in the tidyverse, which will never give you a
| factor unless you explicitly ask for one, but the default in base R
| (at least as soon as a data frame is involved) is to turn character
| vectors into factors.

False. Base R does what the option stringsAsFactors tells it to. Whereas your
incorrect statement implies unconditional behaviour.  

Dirk

-- 
http://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From jori@mey@ @ending from gm@il@com  Thu Aug  9 14:55:30 2018
From: jori@mey@ @ending from gm@il@com (Joris Meys)
Date: Thu, 9 Aug 2018 14:55:30 +0200
Subject: [Rd] vctrs: a type system for the tidyverse
In-Reply-To: <CABdHhvH9Xo-W3gj7Sk+htfa9uzVLhWZSmk_mP+o6BoaFub=1hQ@mail.gmail.com>
References: <CABdHhvFARmiExcEk=NUjfPcRtYnoF4pAA5WTws6r1C+X76zHEw@mail.gmail.com>
 <CAMFmJsn_wNxuV2SQqUiZroJYV+rHRKdADi8Fjg8fRjJr0RGuwQ@mail.gmail.com>
 <CABdHhvEAV-bhprxVMHF_R8vKdN3VEriV86kGNNU02RiK=+m4dQ@mail.gmail.com>
 <CABdHhvHw+_sFcD3H92KQhvurxC8eAyR2A0frZUk7pBcOaVB2Hw@mail.gmail.com>
 <23403.4648.841262.201573@stat.math.ethz.ch>
 <CAMFmJs=hHXYdkd-2kRH48sxB5VJAO4g5Rs35einXs8GwQJ7C8g@mail.gmail.com>
 <CALEXWq2J-J5obavQ=dpkHJmqpt9WQ5a3uZS7L6gY-Fs4X_AuNg@mail.gmail.com>
 <CAO1zAVbaB9fmtrFFYLuusm2jOyh1SHQnDm3U8vjFPNiitk=+hw@mail.gmail.com>
 <CABdHhvH9Xo-W3gj7Sk+htfa9uzVLhWZSmk_mP+o6BoaFub=1hQ@mail.gmail.com>
Message-ID: <CAO1zAVbutLaTFbL5BM8kgv2Lo7VVYVkYoCxC-YQwDR+ptGkjRA@mail.gmail.com>

Hi Hadley,

my point actually came from a data analyst point of view. A character
variable is something used for extra information, eg the "any other ideas?"
field of a questionnaire. A categorical variable is a variable describing
categories defined by the researcher. If it is made clear that a factor is
the object type needed for a categorical variable, there is no confusion.
All my students get it. But I agree that in many cases people are taught
that a factor is somehow related to character variables. And that does not
make sense from a data analyst point of view if you think about variables
as continuous, ordinal and nominal in a model context.

So I don't think adding more confusing behaviour and pitfalls is a solution
to something that's essentially a misunderstanding. It's something that's
only solved by explaining it correctly imho.

Cheers
Joris

On Thu, Aug 9, 2018 at 2:36 PM Hadley Wickham <h.wickham at gmail.com> wrote:

>
> I 100% agree with you, and is this the behaviour that vctrs used to
> have and dplyr currently has (at least in bind_rows()). But
> pragmatically, my experience with dplyr is that people find this
> behaviour confusing and unhelpful. And when I played the full
> expression of this behaviour in vctrs, I found that it forced me to
> think about the levels of factors more than I'd otherwise like to: it
> made me think like a programmer, not like a data analyst. So in an
> ideal world, yes, I think factors would have stricter behaviour, but
> my sense is that imposing this strictness now will be onerous to most
> analysts.
>
> Hadley
>
> --
> http://hadley.nz
>


-- 
Joris Meys
Statistical consultant

Department of Data Analysis and Mathematical Modelling
Ghent University
Coupure Links 653, B-9000 Gent (Belgium)
<https://maps.google.com/?q=Coupure+links+653,%C2%A0B-9000+Gent,%C2%A0Belgium&entry=gmail&source=g>

-----------
Biowiskundedagen 2017-2018
http://www.biowiskundedagen.ugent.be/

-------------------------------
Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php

	[[alternative HTML version deleted]]


From h@wickh@m @ending from gm@il@com  Thu Aug  9 16:30:38 2018
From: h@wickh@m @ending from gm@il@com (Hadley Wickham)
Date: Thu, 9 Aug 2018 09:30:38 -0500
Subject: [Rd] vctrs: a type system for the tidyverse
In-Reply-To: <CAO1zAVbutLaTFbL5BM8kgv2Lo7VVYVkYoCxC-YQwDR+ptGkjRA@mail.gmail.com>
References: <CABdHhvFARmiExcEk=NUjfPcRtYnoF4pAA5WTws6r1C+X76zHEw@mail.gmail.com>
 <CAMFmJsn_wNxuV2SQqUiZroJYV+rHRKdADi8Fjg8fRjJr0RGuwQ@mail.gmail.com>
 <CABdHhvEAV-bhprxVMHF_R8vKdN3VEriV86kGNNU02RiK=+m4dQ@mail.gmail.com>
 <CABdHhvHw+_sFcD3H92KQhvurxC8eAyR2A0frZUk7pBcOaVB2Hw@mail.gmail.com>
 <23403.4648.841262.201573@stat.math.ethz.ch>
 <CAMFmJs=hHXYdkd-2kRH48sxB5VJAO4g5Rs35einXs8GwQJ7C8g@mail.gmail.com>
 <CALEXWq2J-J5obavQ=dpkHJmqpt9WQ5a3uZS7L6gY-Fs4X_AuNg@mail.gmail.com>
 <CAO1zAVbaB9fmtrFFYLuusm2jOyh1SHQnDm3U8vjFPNiitk=+hw@mail.gmail.com>
 <CABdHhvH9Xo-W3gj7Sk+htfa9uzVLhWZSmk_mP+o6BoaFub=1hQ@mail.gmail.com>
 <CAO1zAVbutLaTFbL5BM8kgv2Lo7VVYVkYoCxC-YQwDR+ptGkjRA@mail.gmail.com>
Message-ID: <CABdHhvG=Gogpa8ZPQAKcrVeQtZoAEiWJezB=k46LL90K65-VHQ@mail.gmail.com>

On Thu, Aug 9, 2018 at 7:54 AM Joris Meys <jorismeys at gmail.com> wrote:
>
> Hi Hadley,
>
> my point actually came from a data analyst point of view. A character variable is something used for extra information, eg the "any other ideas?" field of a questionnaire. A categorical variable is a variable describing categories defined by the researcher. If it is made clear that a factor is the object type needed for a categorical variable, there is no confusion. All my students get it. But I agree that in many cases people are taught that a factor is somehow related to character variables. And that does not make sense from a data analyst point of view if you think about variables as continuous, ordinal and nominal in a model context.
>
> So I don't think adding more confusing behaviour and pitfalls is a solution to something that's essentially a misunderstanding. It's something that's only solved by explaining it correctly imho.

I agree with your definition of character and factor variables. It's
an important distinction, and I agree that the blurring of factors and
characters is generally undesirable. However, the merits of respecting
R's existing behaviour, and Martin M?chler's support, means that I'm
not going to change vctr's approach at this point in time. However, I
hear from you and Gabe that this is an important issue, and I'll
definitely keep it in mind as I solicit further feedback from users.

Hadley

-- 
http://hadley.nz


From h@wickh@m @ending from gm@il@com  Thu Aug  9 16:36:32 2018
From: h@wickh@m @ending from gm@il@com (Hadley Wickham)
Date: Thu, 9 Aug 2018 09:36:32 -0500
Subject: [Rd] vctrs: a type system for the tidyverse
In-Reply-To: <CABdHhvH53=0b7Hg=9dAPx9zUTn4YSYR0=JoDzWMRMjkMpmXbBQ@mail.gmail.com>
References: <CABdHhvFARmiExcEk=NUjfPcRtYnoF4pAA5WTws6r1C+X76zHEw@mail.gmail.com>
 <CAMFmJsn_wNxuV2SQqUiZroJYV+rHRKdADi8Fjg8fRjJr0RGuwQ@mail.gmail.com>
 <CABdHhvEAV-bhprxVMHF_R8vKdN3VEriV86kGNNU02RiK=+m4dQ@mail.gmail.com>
 <CABdHhvHw+_sFcD3H92KQhvurxC8eAyR2A0frZUk7pBcOaVB2Hw@mail.gmail.com>
 <23403.4648.841262.201573@stat.math.ethz.ch>
 <CABdHhvH53=0b7Hg=9dAPx9zUTn4YSYR0=JoDzWMRMjkMpmXbBQ@mail.gmail.com>
Message-ID: <CABdHhvGucJKbaFmB1gk3i0vgen9OPQ7Wb=nQKm7aOoBFbC0sqA@mail.gmail.com>

> > As Gabe mentioned (and you've explained about) the term "type"
> > is really confusing here.  As you know, the R internals are all
> > about SEXPs, TYPEOF(), etc, and that's what the R level
> > typeof(.) also returns.  As you want to use something slightly
> > different, it should be different naming, ideally something not
> > existing yet in the R / S world, maybe 'kind' ?
>
> Agreed - I've been using type in the sense of "type system"
> (particularly as it related to algebraic data types), but that's not
> obvious from the current presentation, and as you note, is confusing
> with existing notions of type in R. I like your suggestion of kind,
> but I think it might be possible to just talk about classes, and
> instead emphasise that while the components of the system are classes
> (and indeed it's implemented using S3), the coercion/casting
> relationship do not strictly follow the subclass/superclass
> relationships.

I've taken another pass through (the first part of) the readme
(https://github.com/r-lib/vctrs#vctrs), and I'm now confident that I
can avoid using "type" by itself, and instead always use it in a
compound phrase (like type system) to avoid confusion. That leaves the
`.type` argument to many vctrs functions. I'm considering change it to
.prototype, because what you actually give it is a zero-length vector
of the class you want, i.e. a prototype of the desired output. What do
you think of prototype as a name?

Do you have any thoughts on good names for distinction vectors without
a class (i.e. logical, integer, double, ...) from vectors with a class
(e.g. factors, dates, etc). I've been thinking bare vector and S3
vector (leaving room to later think about S4 vectors). Do those sound
reasonable to you?

Hadley

-- 
http://hadley.nz


From tom@@@k@liber@ @ending from gm@il@com  Thu Aug  9 20:37:50 2018
From: tom@@@k@liber@ @ending from gm@il@com (Tomas Kalibera)
Date: Thu, 9 Aug 2018 20:37:50 +0200
Subject: [Rd] 
 SIGSEGV in R_RunWeakRefFinalizer, object allocated with Rcpp
In-Reply-To: <CALEXWq2E+-k=Oka1YZAzGzYU1bLmcC3ptUUrv8RUMUrpcviK-g@mail.gmail.com>
References: <CALEXWq2E+-k=Oka1YZAzGzYU1bLmcC3ptUUrv8RUMUrpcviK-g@mail.gmail.com>
Message-ID: <a03f65c7-3e0e-7857-0e1e-a809e1051b56@gmail.com>


R's dyn.unload() will unconditionally unload the given shared object; it 
does not check whether there is any object (external pointer or weak 
reference) with a C finalizer pointing into the space of the shared 
object being unloaded. So it is expected that R will segfault later when 
such finalizer is run.

Currently there is no other way than to handle this on the side of the 
shared library/package, e.g. as Luke does in his simplemmap package. A 
library can declare R_unload_XXX function (see Writing R Extensions) 
that will be called before it is unloaded. So, one can keep say a list 
of objects with finalizers and clear/run the finalizers in R_unload_XXX.

I don't think we could do this automatically on the R's side with the 
current API. I don't think there is a portable way to check if a given C 
pointer is from a given loaded shared object (non-portable way on Linux 
might be scanning the maps in the /proc filesystem). Also, doing this 
for all objects with finalizers when unloading any library would 
probably be slow. Adding some API to aid libraries in doing the 
housekeeping is possible in principle.

So to answer your original question, this could probably be handled in 
Rcpp, but in either case I would not use dyn.unload() in the first 
place. This problem may be just one of many.

Best
Tomas



On 6.8.2018 17:35, I?aki ?car wrote:
> Hi all,
>
> I'm not sure if I'm not supposed to do the following (the dyn.unload
> part, I mean) or this could be a bug (in R or Rcpp):
>
> ```
> Rcpp::sourceCpp(code='
>    #include <Rcpp.h>
>    class Object {};
>    //[[Rcpp::export]]
>    SEXP new_object() {
>      return Rcpp::XPtr<Object>(new Object());
>    }'
> )
>
> new_object()
> dyn.unload(list.files(tempdir(), ".(so|dll)$", recursive=TRUE, full.names=TRUE))
> gc() # segfault in R_RunWeakRefFinalizer
>
> message("This is not printed")
> ```
>
> This is the backtrace I get with R 3.5.1:
>
> #0  0x61ec4fd0 in ?? ()
> #1  0x6ca1cafc in R_RunWeakRefFinalizer (w=0xc786a98) at memory.c:1393
> #2  0x6ca1cdba in RunFinalizers () at memory.c:1459
> #3  0x6ca1d024 in R_RunPendingFinalizers () at memory.c:1495
> #4  R_gc () at memory.c:2893
> #5  do_gc (call=0x1d45b88, op=0x15241d0, args=0x1d454b8,
> rho=0x1d45318) at memory.c:2013
> #6  0x6c9db60f in bcEval (body=body at entry=0x1d45a88,
> rho=rho at entry=0x1d45318, useCache=useCache at entry=TRUE)
>      at eval.c:6781
> #7  0x6c9ecfb2 in Rf_eval (e=0x1d45a88, rho=0x1d45318) at eval.c:624
> #8  0x6c9ee6f1 in R_execClosure (call=call at entry=0x0,
> newrho=<optimized out>, sysparent=<optimized out>,
>      rho=0x15a3370, arglist=0x18c2498, op=0x1d45968) at eval.c:1773
> #9  0x6c9ef605 in Rf_applyClosure (call=0x15a3370,
> call at entry=0x1d45828, op=0x18c2498, op at entry=0x1d45968,
>      arglist=0x1d45968, rho=rho at entry=0x15a3370,
> suppliedvars=0x18c2498) at eval.c:1701
> #10 0x6c9ecf78 in Rf_eval (e=e at entry=0x1d45828,
> rho=rho at entry=0x15a3370) at eval.c:747
> #11 0x6ca11170 in Rf_ReplIteration (rho=0x15a3370, savestack=0,
> browselevel=0, state=0x142edec) at main.c:258
> #12 0x6ca11567 in R_ReplConsole (rho=<optimized out>, savestack=0,
> browselevel=0) at main.c:308
> #13 0x6ca11604 in run_Rmainloop () at main.c:1082
> #14 0x6ca11700 in Rf_mainloop () at main.c:1089
> #15 0x00401836 in AppMain (argc=1, argv=0x15c16f8) at rterm.c:86
> #16 0x00401649 in WinMain at 16 (Instance=0x400000, PrevInstance=0x0,
> CmdLine=0x1904797 "", CmdShow=10)
>      at graphappmain.c:23
> #17 0x00402a8d in main ()
>
> Any ideas?
>
> I?aki
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From edd @ending from debi@n@org  Thu Aug  9 20:58:44 2018
From: edd @ending from debi@n@org (Dirk Eddelbuettel)
Date: Thu, 9 Aug 2018 13:58:44 -0500
Subject: [Rd] 
 SIGSEGV in R_RunWeakRefFinalizer, object allocated with Rcpp
In-Reply-To: <a03f65c7-3e0e-7857-0e1e-a809e1051b56@gmail.com>
References: <CALEXWq2E+-k=Oka1YZAzGzYU1bLmcC3ptUUrv8RUMUrpcviK-g@mail.gmail.com>
 <a03f65c7-3e0e-7857-0e1e-a809e1051b56@gmail.com>
Message-ID: <23404.36580.740017.811392@rob.eddelbuettel.com>


On 9 August 2018 at 20:37, Tomas Kalibera wrote:
| So to answer your original question, this could probably be handled in 
| Rcpp,

Hm. Why do you say that / what did you have in mind?

Recall that we do not alter SEXPs or introduce additional additional
reference counters -- because we do not think that altering the basic R API
for such calls would be a wise strategy.  So we do more or less what is done
in C for R, with some additional hand-holding which circumvents a number of
common errors.

| but in either case I would not use dyn.unload() in the first 
| place. This problem may be just one of many.

I think I'd second that. I never had much unloading packages or dynamic
libraries and tend to "just say no". Both short-lived processes (ie via 'r')
as well as long sessions (ie R via ESS, running for weeks) work for my
workflows.

Dirk

-- 
http://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From tom@@@k@liber@ @ending from gm@il@com  Thu Aug  9 21:11:59 2018
From: tom@@@k@liber@ @ending from gm@il@com (Tomas Kalibera)
Date: Thu, 9 Aug 2018 21:11:59 +0200
Subject: [Rd] 
 SIGSEGV in R_RunWeakRefFinalizer, object allocated with Rcpp
In-Reply-To: <23404.36580.740017.811392@rob.eddelbuettel.com>
References: <CALEXWq2E+-k=Oka1YZAzGzYU1bLmcC3ptUUrv8RUMUrpcviK-g@mail.gmail.com>
 <a03f65c7-3e0e-7857-0e1e-a809e1051b56@gmail.com>
 <23404.36580.740017.811392@rob.eddelbuettel.com>
Message-ID: <7227ca03-4239-670c-486c-29a6bfcfd18e@gmail.com>



On 9.8.2018 20:58, Dirk Eddelbuettel wrote:
> On 9 August 2018 at 20:37, Tomas Kalibera wrote:
> | So to answer your original question, this could probably be handled in
> | Rcpp,
>
> Hm. Why do you say that / what did you have in mind?
>
> Recall that we do not alter SEXPs or introduce additional additional
> reference counters -- because we do not think that altering the basic R API
> for such calls would be a wise strategy.  So we do more or less what is done
> in C for R, with some additional hand-holding which circumvents a number of
> common errors.
Well if you wanted to support unloading - and I am not at all asking for 
that - you could keep R objects (weak references, external pointers) 
with C finalizers in some list and then unconditionally run or clear the 
finalizers in your R_unload, something like Luke does in his example 
package.

https://github.com/ltierney/Rpkg-simplemmap/blob/master/src/simplemmap.c

Best
Tomas
>
> | but in either case I would not use dyn.unload() in the first
> | place. This problem may be just one of many.
>
> I think I'd second that. I never had much unloading packages or dynamic
> libraries and tend to "just say no". Both short-lived processes (ie via 'r')
> as well as long sessions (ie R via ESS, running for weeks) work for my
> workflows.
>
> Dirk
>


From luke-tier@ey m@ili@g off uiow@@edu  Thu Aug  9 21:13:21 2018
From: luke-tier@ey m@ili@g off uiow@@edu (luke-tier@ey m@ili@g off uiow@@edu)
Date: Thu, 9 Aug 2018 14:13:21 -0500 (CDT)
Subject: [Rd] 
 SIGSEGV in R_RunWeakRefFinalizer, object allocated with Rcpp
In-Reply-To: <23404.36580.740017.811392@rob.eddelbuettel.com>
References: <CALEXWq2E+-k=Oka1YZAzGzYU1bLmcC3ptUUrv8RUMUrpcviK-g@mail.gmail.com>
 <a03f65c7-3e0e-7857-0e1e-a809e1051b56@gmail.com>
 <23404.36580.740017.811392@rob.eddelbuettel.com>
Message-ID: <alpine.DEB.2.20.1808091406240.3237@luke-Latitude-7480>

On Thu, 9 Aug 2018, Dirk Eddelbuettel wrote:

>
> On 9 August 2018 at 20:37, Tomas Kalibera wrote:
> | So to answer your original question, this could probably be handled in
> | Rcpp,
>
> Hm. Why do you say that / what did you have in mind?

We say it because it is true. Rcpp registers C finalizers and running
them after unloading will segfault. For now it would be better for Rcpp
(and everyone else) to explicitly discourage unloading as it is
unreliable on many levels.

What Rcpp could do to avoid segfaulting is to keep a weak list of all
objects to which it attaches C finalizers and arrange for those to be
cleaned up in an R_unload_<dllname> routine. Not clear it is worth the
trouble. At the R level we could provide more support for this since
we already have a weak list of objects with finalizers, but again not
clear it is worth the trouble.

Best,

luke

> Recall that we do not alter SEXPs or introduce additional additional
> reference counters -- because we do not think that altering the basic R API
> for such calls would be a wise strategy.  So we do more or less what is done
> in C for R, with some additional hand-holding which circumvents a number of
> common errors.
>
> | but in either case I would not use dyn.unload() in the first
> | place. This problem may be just one of many.
>
> I think I'd second that. I never had much unloading packages or dynamic
> libraries and tend to "just say no". Both short-lived processes (ie via 'r')
> as well as long sessions (ie R via ESS, running for weeks) work for my
> workflows.
>
> Dirk
>
>

-- 
Luke Tierney
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu


From edd @ending from debi@n@org  Thu Aug  9 21:38:20 2018
From: edd @ending from debi@n@org (Dirk Eddelbuettel)
Date: Thu, 9 Aug 2018 14:38:20 -0500
Subject: [Rd] 
 SIGSEGV in R_RunWeakRefFinalizer, object allocated with Rcpp
In-Reply-To: <alpine.DEB.2.20.1808091406240.3237@luke-Latitude-7480>
References: <CALEXWq2E+-k=Oka1YZAzGzYU1bLmcC3ptUUrv8RUMUrpcviK-g@mail.gmail.com>
 <a03f65c7-3e0e-7857-0e1e-a809e1051b56@gmail.com>
 <23404.36580.740017.811392@rob.eddelbuettel.com>
 <alpine.DEB.2.20.1808091406240.3237@luke-Latitude-7480>
Message-ID: <23404.38956.878475.983796@rob.eddelbuettel.com>


On 9 August 2018 at 14:13, luke-tierney at uiowa.edu wrote:
| On Thu, 9 Aug 2018, Dirk Eddelbuettel wrote:
| 
| >
| > On 9 August 2018 at 20:37, Tomas Kalibera wrote:
| > | So to answer your original question, this could probably be handled in
| > | Rcpp,
| >
| > Hm. Why do you say that / what did you have in mind?
| 
| We say it because it is true. Rcpp registers C finalizers and running
| them after unloading will segfault. For now it would be better for Rcpp
| (and everyone else) to explicitly discourage unloading as it is
| unreliable on many levels.
| 
| What Rcpp could do to avoid segfaulting is to keep a weak list of all
| objects to which it attaches C finalizers and arrange for those to be
| cleaned up in an R_unload_<dllname> routine. Not clear it is worth the
| trouble. At the R level we could provide more support for this since
| we already have a weak list of objects with finalizers, but again not
| clear it is worth the trouble.

Ah, noted, thanks for the explainer.

As I remain in camp "don't try unloading" I won't make this a priority
myself, but as always open to credible PRs. Could start with an issue ticket
and discussion if anybody is so inclined.

Dirk

-- 
http://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From vitekj @ending from icloud@com  Thu Aug  9 23:25:01 2018
From: vitekj @ending from icloud@com (jan Vitek)
Date: Thu, 09 Aug 2018 22:25:01 +0100
Subject: [Rd] vctrs: a type system for the tidyverse
In-Reply-To: <CABdHhvGucJKbaFmB1gk3i0vgen9OPQ7Wb=nQKm7aOoBFbC0sqA@mail.gmail.com>
References: <CABdHhvFARmiExcEk=NUjfPcRtYnoF4pAA5WTws6r1C+X76zHEw@mail.gmail.com>
 <CAMFmJsn_wNxuV2SQqUiZroJYV+rHRKdADi8Fjg8fRjJr0RGuwQ@mail.gmail.com>
 <CABdHhvEAV-bhprxVMHF_R8vKdN3VEriV86kGNNU02RiK=+m4dQ@mail.gmail.com>
 <CABdHhvHw+_sFcD3H92KQhvurxC8eAyR2A0frZUk7pBcOaVB2Hw@mail.gmail.com>
 <23403.4648.841262.201573@stat.math.ethz.ch>
 <CABdHhvH53=0b7Hg=9dAPx9zUTn4YSYR0=JoDzWMRMjkMpmXbBQ@mail.gmail.com>
 <CABdHhvGucJKbaFmB1gk3i0vgen9OPQ7Wb=nQKm7aOoBFbC0sqA@mail.gmail.com>
Message-ID: <A260BC05-0755-4911-98B3-74071569A9F4@icloud.com>

> I'm now confident that I
> can avoid using "type" by itself, and instead always use it in a
> compound phrase (like type system) to avoid confusion. That leaves the
> `.type` argument to many vctrs functions. I'm considering change it to
> .prototype, because what you actually give it is a zero-length vector
> of the class you want, i.e. a prototype of the desired output. What do
> you think of prototype as a name?


The term ?type system? in computer science is used in very different ways.
What the note describes is not a type system, but rather a set of 
coercions used by a small number of functions in one package.

Typically it refers to a set of rules (either statically enforced
by the compiler or dynamically enforced by the runtime) that ensure
that some particular category of errors can be caught by the 
language.

There is none of that here.

My suggestion would be to avoid ?type system?.


"The short-term goal of vctrs is to develop a type system for vectors which will help reason about functions that combine different types of input (e.g. c(), ifelse(), rbind()). The vctrs type system encompasses base vectors (e.g. logical, numeric, character, list), S3 vectors (e.g. factor, ordered, Date, POSIXct), and data frames; and can be extended to deal with S3 vectors defined in other packages, as described in vignette("extending-vctrs?).?

==>

The short-term goal of vctrs is to specify the behavior of functions that combine different types of vectors (e.g. c(), ifelse(), rbind()). The specification encompasses base vectors (e.g. logical, numeric, character, list), S3 vectors (e.g. factor, ordered, Date, POSIXct), and data frames; and can be extended to deal with S3 vectors defined in other packages, as described in vignette("extending-vctrs?).

and so on.

-j

From @purdle@@ @ending from gm@il@com  Thu Aug  9 23:45:29 2018
From: @purdle@@ @ending from gm@il@com (Abs Spurdle)
Date: Fri, 10 Aug 2018 09:45:29 +1200
Subject: [Rd] WishList: Remove Generic Arguments
Message-ID: <CAB8pepwoUPZRQ8V_OCaZq=QdM3fRbT4zYa8OjOgZvKucmSVFQA@mail.gmail.com>

 I apologize if this issue has been raised before.

I really like object oriented S3 programming.
However, there's one feature of object oriented S3 programming that I don't
like.
Generic functions can have arguments other than dots.

Lets say you have an R package with something like:

print.myfunction (f, ...)
{   dosomething (f, ...)
}

Noting that I use function objects a lot.

R CMD check will generate a warning because you've named your object f
rather than x.

I don't want to name my object x.
I want to name my object f.
Naming the object x makes the program unreadable.
Especially if f contains an attribute or an argument named x.

There's a work around.
You can redefine the print function, using something like:

print = function (...) base::print (...)

However, you have to export and document the function.

I think that it would be better if generic functions didn't have any
arguments except for dots.

	[[alternative HTML version deleted]]


From l@wrence@mich@el @ending from gene@com  Thu Aug  9 23:59:26 2018
From: l@wrence@mich@el @ending from gene@com (Michael Lawrence)
Date: Thu, 9 Aug 2018 14:59:26 -0700
Subject: [Rd] WishList: Remove Generic Arguments
In-Reply-To: <CAB8pepwoUPZRQ8V_OCaZq=QdM3fRbT4zYa8OjOgZvKucmSVFQA@mail.gmail.com>
References: <CAB8pepwoUPZRQ8V_OCaZq=QdM3fRbT4zYa8OjOgZvKucmSVFQA@mail.gmail.com>
Message-ID: <CAOQ5NycQZBoJvDbJcPoSje9f-n+Vvp7g85ddBn5MwYYpfuiLsg@mail.gmail.com>

A generic function is not simply a way to name two functions (methods)
the same. It has a particular purpose, and the argument names are
aligned with and convey that purpose. The methods only implement
polymorphism; they don't change the purpose. Changing the purpose
would make code unreadable.

Michael

On Thu, Aug 9, 2018 at 2:45 PM, Abs Spurdle <spurdle.a at gmail.com> wrote:
>  I apologize if this issue has been raised before.
>
> I really like object oriented S3 programming.
> However, there's one feature of object oriented S3 programming that I don't
> like.
> Generic functions can have arguments other than dots.
>
> Lets say you have an R package with something like:
>
> print.myfunction (f, ...)
> {   dosomething (f, ...)
> }
>
> Noting that I use function objects a lot.
>
> R CMD check will generate a warning because you've named your object f
> rather than x.
>
> I don't want to name my object x.
> I want to name my object f.
> Naming the object x makes the program unreadable.
> Especially if f contains an attribute or an argument named x.
>
> There's a work around.
> You can redefine the print function, using something like:
>
> print = function (...) base::print (...)
>
> However, you have to export and document the function.
>
> I think that it would be better if generic functions didn't have any
> arguments except for dots.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From murdoch@dunc@n @ending from gm@il@com  Fri Aug 10 00:20:54 2018
From: murdoch@dunc@n @ending from gm@il@com (Duncan Murdoch)
Date: Thu, 9 Aug 2018 18:20:54 -0400
Subject: [Rd] WishList: Remove Generic Arguments
In-Reply-To: <CAB8pepwoUPZRQ8V_OCaZq=QdM3fRbT4zYa8OjOgZvKucmSVFQA@mail.gmail.com>
References: <CAB8pepwoUPZRQ8V_OCaZq=QdM3fRbT4zYa8OjOgZvKucmSVFQA@mail.gmail.com>
Message-ID: <bcc81123-3c54-2a64-8396-6492eaf7165b@gmail.com>

On 09/08/2018 5:45 PM, Abs Spurdle wrote:
>   I apologize if this issue has been raised before.
> 
> I really like object oriented S3 programming.
> However, there's one feature of object oriented S3 programming that I don't
> like.
> Generic functions can have arguments other than dots.
> 
> Lets say you have an R package with something like:
> 
> print.myfunction (f, ...)
> {   dosomething (f, ...)
> }
> 
> Noting that I use function objects a lot.
> 
> R CMD check will generate a warning because you've named your object f
> rather than x.
> 
> I don't want to name my object x.
> I want to name my object f.

There are reasons for this requirement.  Suppose I have an object of 
class myfunction named myobj.  However, being a 3rd party, I have no 
idea what class myfunction is about.  Then I might say

print(x = myobj)

That would not work with your method, because the x would be absorbed 
into ..., it would not bind to f.


> Naming the object x makes the program unreadable.
> Especially if f contains an attribute or an argument named x.

That's nonsense.  You don't need to name your object the same as an 
argument.  You name your object something readable (e.g. myobj), then 
pass it to print(), which binds it to x.

If x is not a sensible name within the print.myfunction() method, then 
there's a one line fix:

print.myfunction <- function(x, ...) {
   f <- x
   dosomething(f)
}
> 
> There's a work around.
> You can redefine the print function, using something like:
> 
> print = function (...) base::print (...)
> 
> However, you have to export and document the function.

That's a really, really bad idea.  If there are two generics named the 
same, how are your users going to know which one they are getting when 
they just say print(myobj)?

> 
> I think that it would be better if generic functions didn't have any
> arguments except for dots.

That makes argument checking much harder.  If a generic always needs two 
arguments, why not name them, so R can complain when a user calls it 
with just one?

Duncan Murdoch


From i@uc@r86 @ending from gm@il@com  Fri Aug 10 00:46:08 2018
From: i@uc@r86 @ending from gm@il@com (=?UTF-8?B?ScOxYWtpIMOaY2Fy?=)
Date: Fri, 10 Aug 2018 00:46:08 +0200
Subject: [Rd] 
 SIGSEGV in R_RunWeakRefFinalizer, object allocated with Rcpp
In-Reply-To: <alpine.DEB.2.20.1808091406240.3237@luke-Latitude-7480>
References: <CALEXWq2E+-k=Oka1YZAzGzYU1bLmcC3ptUUrv8RUMUrpcviK-g@mail.gmail.com>
 <a03f65c7-3e0e-7857-0e1e-a809e1051b56@gmail.com>
 <23404.36580.740017.811392@rob.eddelbuettel.com>
 <alpine.DEB.2.20.1808091406240.3237@luke-Latitude-7480>
Message-ID: <CALEXWq0H8Qdtc18Enr+TTm5DbDSJx3YtaLHaGZxr7V9Okz8q+Q@mail.gmail.com>

Thanks, Tomas, Luke, for the clarifications. Then, I have another question.

But first, let me introduce how I ended up here, because obviously I
just don't go around dyn.unloading things that I've just compiled. I
was testing a package with valgrind. Everything ok, no leaks. Great.
But I'm always suspicious (probably unjustifiably) of all the memory
that is reported as "still reachable", so I wanted to check whether
there was any difference if I detach(unload=TRUE) the package after
all the tests.

In a nutshell, I ended up discovering that the following code:

```
library(simmer)
simmer() # allocates a C++ object, as in my initial example
detach("package:simmer", unload=TRUE)
```

segfaults on Windows, but not on Linux (then I built the example in my
initial email to confirm it wasn't simmer's fault). So given that,
from your explanation, I should expect a segfault here, the question
is: what on Earth does (or does not) R on Linux do to avoid
segfaulting compared to Windows? :) And a corolary would be, can't R
on Windows do the same?

Regards,
I?aki

El jue., 9 ago. 2018 a las 21:13, <luke-tierney at uiowa.edu> escribi?:
>
> On Thu, 9 Aug 2018, Dirk Eddelbuettel wrote:
>
> >
> > On 9 August 2018 at 20:37, Tomas Kalibera wrote:
> > | So to answer your original question, this could probably be handled in
> > | Rcpp,
> >
> > Hm. Why do you say that / what did you have in mind?
>
> We say it because it is true. Rcpp registers C finalizers and running
> them after unloading will segfault. For now it would be better for Rcpp
> (and everyone else) to explicitly discourage unloading as it is
> unreliable on many levels.
>
> What Rcpp could do to avoid segfaulting is to keep a weak list of all
> objects to which it attaches C finalizers and arrange for those to be
> cleaned up in an R_unload_<dllname> routine. Not clear it is worth the
> trouble. At the R level we could provide more support for this since
> we already have a weak list of objects with finalizers, but again not
> clear it is worth the trouble.
>
> Best,
>
> luke
>
> > Recall that we do not alter SEXPs or introduce additional additional
> > reference counters -- because we do not think that altering the basic R API
> > for such calls would be a wise strategy.  So we do more or less what is done
> > in C for R, with some additional hand-holding which circumvents a number of
> > common errors.
> >
> > | but in either case I would not use dyn.unload() in the first
> > | place. This problem may be just one of many.
> >
> > I think I'd second that. I never had much unloading packages or dynamic
> > libraries and tend to "just say no". Both short-lived processes (ie via 'r')
> > as well as long sessions (ie R via ESS, running for weeks) work for my
> > workflows.
> >
> > Dirk
> >
> >
>
> --
> Luke Tierney
> Ralph E. Wareham Professor of Mathematical Sciences
> University of Iowa                  Phone:             319-335-3386
> Department of Statistics and        Fax:               319-335-3017
>     Actuarial Science
> 241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
> Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu



--
I?aki ?car
http://www.enchufa2.es
@Enchufa2


From h@wickh@m @ending from gm@il@com  Fri Aug 10 00:56:32 2018
From: h@wickh@m @ending from gm@il@com (Hadley Wickham)
Date: Thu, 9 Aug 2018 17:56:32 -0500
Subject: [Rd] vctrs: a type system for the tidyverse
In-Reply-To: <A260BC05-0755-4911-98B3-74071569A9F4@icloud.com>
References: <CABdHhvFARmiExcEk=NUjfPcRtYnoF4pAA5WTws6r1C+X76zHEw@mail.gmail.com>
 <CAMFmJsn_wNxuV2SQqUiZroJYV+rHRKdADi8Fjg8fRjJr0RGuwQ@mail.gmail.com>
 <CABdHhvEAV-bhprxVMHF_R8vKdN3VEriV86kGNNU02RiK=+m4dQ@mail.gmail.com>
 <CABdHhvHw+_sFcD3H92KQhvurxC8eAyR2A0frZUk7pBcOaVB2Hw@mail.gmail.com>
 <23403.4648.841262.201573@stat.math.ethz.ch>
 <CABdHhvH53=0b7Hg=9dAPx9zUTn4YSYR0=JoDzWMRMjkMpmXbBQ@mail.gmail.com>
 <CABdHhvGucJKbaFmB1gk3i0vgen9OPQ7Wb=nQKm7aOoBFbC0sqA@mail.gmail.com>
 <A260BC05-0755-4911-98B3-74071569A9F4@icloud.com>
Message-ID: <CABdHhvHNqWFDamNQU1ZC_Z022LdE0qBzz4swYGkMV1eFi-2hHA@mail.gmail.com>

On Thu, Aug 9, 2018 at 4:26 PM jan Vitek <vitekj at icloud.com> wrote:
>
> > I'm now confident that I
> > can avoid using "type" by itself, and instead always use it in a
> > compound phrase (like type system) to avoid confusion. That leaves the
> > `.type` argument to many vctrs functions. I'm considering change it to
> > .prototype, because what you actually give it is a zero-length vector
> > of the class you want, i.e. a prototype of the desired output. What do
> > you think of prototype as a name?
>
>
> The term ?type system? in computer science is used in very different ways.
> What the note describes is not a type system, but rather a set of
> coercions used by a small number of functions in one package.
>
> Typically it refers to a set of rules (either statically enforced
> by the compiler or dynamically enforced by the runtime) that ensure
> that some particular category of errors can be caught by the
> language.
>
> There is none of that here.

I think there's a bit of that flavour here:

vec_c(factor("a"), Sys.Date())
#> Error: No common type for factor and date

This isn't a type system imposed by the language, but I don't think
that's a reason not to call it a type system.

That said, I agree that calling it a type system is currently
overselling it, and I have made your proposed change to the README
(and added a very-long term goal of making a type system that could be
applied using (e.g.) annotations).

> "The short-term goal of vctrs is to develop a type system for vectors which will help reason about functions that combine different types of input (e.g. c(), ifelse(), rbind()). The vctrs type system encompasses base vectors (e.g. logical, numeric, character, list), S3 vectors (e.g. factor, ordered, Date, POSIXct), and data frames; and can be extended to deal with S3 vectors defined in other packages, as described in vignette("extending-vctrs?).?
>
> ==>
>
> The short-term goal of vctrs is to specify the behavior of functions that combine different types of vectors (e.g. c(), ifelse(), rbind()). The specification encompasses base vectors (e.g. logical, numeric, character, list), S3 vectors (e.g. factor, ordered, Date, POSIXct), and data frames; and can be extended to deal with S3 vectors defined in other packages, as described in vignette("extending-vctrs?).

Thanks for the nice wording!

Hadley


-- 
http://hadley.nz


From luke-tier@ey m@ili@g off uiow@@edu  Fri Aug 10 01:13:30 2018
From: luke-tier@ey m@ili@g off uiow@@edu (luke-tier@ey m@ili@g off uiow@@edu)
Date: Thu, 9 Aug 2018 18:13:30 -0500 (CDT)
Subject: [Rd] vctrs: a type system for the tidyverse
In-Reply-To: <CABdHhvHNqWFDamNQU1ZC_Z022LdE0qBzz4swYGkMV1eFi-2hHA@mail.gmail.com>
References: <CABdHhvFARmiExcEk=NUjfPcRtYnoF4pAA5WTws6r1C+X76zHEw@mail.gmail.com>
 <CAMFmJsn_wNxuV2SQqUiZroJYV+rHRKdADi8Fjg8fRjJr0RGuwQ@mail.gmail.com>
 <CABdHhvEAV-bhprxVMHF_R8vKdN3VEriV86kGNNU02RiK=+m4dQ@mail.gmail.com>
 <CABdHhvHw+_sFcD3H92KQhvurxC8eAyR2A0frZUk7pBcOaVB2Hw@mail.gmail.com>
 <23403.4648.841262.201573@stat.math.ethz.ch>
 <CABdHhvH53=0b7Hg=9dAPx9zUTn4YSYR0=JoDzWMRMjkMpmXbBQ@mail.gmail.com>
 <CABdHhvGucJKbaFmB1gk3i0vgen9OPQ7Wb=nQKm7aOoBFbC0sqA@mail.gmail.com>
 <A260BC05-0755-4911-98B3-74071569A9F4@icloud.com>
 <CABdHhvHNqWFDamNQU1ZC_Z022LdE0qBzz4swYGkMV1eFi-2hHA@mail.gmail.com>
Message-ID: <alpine.OSX.2.21.1808091812440.1263@lukes-air.hsd1.mn.comcast.net>

Some ideas from the 'numeric tower' notion in scheme/lisp might also
be useful.

Best,

luke

On Thu, 9 Aug 2018, Hadley Wickham wrote:

> On Thu, Aug 9, 2018 at 4:26 PM jan Vitek <vitekj at icloud.com> wrote:
>>
>>> I'm now confident that I
>>> can avoid using "type" by itself, and instead always use it in a
>>> compound phrase (like type system) to avoid confusion. That leaves the
>>> `.type` argument to many vctrs functions. I'm considering change it to
>>> .prototype, because what you actually give it is a zero-length vector
>>> of the class you want, i.e. a prototype of the desired output. What do
>>> you think of prototype as a name?
>>
>>
>> The term ?type system? in computer science is used in very different ways.
>> What the note describes is not a type system, but rather a set of
>> coercions used by a small number of functions in one package.
>>
>> Typically it refers to a set of rules (either statically enforced
>> by the compiler or dynamically enforced by the runtime) that ensure
>> that some particular category of errors can be caught by the
>> language.
>>
>> There is none of that here.
>
> I think there's a bit of that flavour here:
>
> vec_c(factor("a"), Sys.Date())
> #> Error: No common type for factor and date
>
> This isn't a type system imposed by the language, but I don't think
> that's a reason not to call it a type system.
>
> That said, I agree that calling it a type system is currently
> overselling it, and I have made your proposed change to the README
> (and added a very-long term goal of making a type system that could be
> applied using (e.g.) annotations).
>
>> "The short-term goal of vctrs is to develop a type system for vectors which will help reason about functions that combine different types of input (e.g. c(), ifelse(), rbind()). The vctrs type system encompasses base vectors (e.g. logical, numeric, character, list), S3 vectors (e.g. factor, ordered, Date, POSIXct), and data frames; and can be extended to deal with S3 vectors defined in other packages, as described in vignette("extending-vctrs?).?
>>
>> ==>
>>
>> The short-term goal of vctrs is to specify the behavior of functions that combine different types of vectors (e.g. c(), ifelse(), rbind()). The specification encompasses base vectors (e.g. logical, numeric, character, list), S3 vectors (e.g. factor, ordered, Date, POSIXct), and data frames; and can be extended to deal with S3 vectors defined in other packages, as described in vignette("extending-vctrs?).
>
> Thanks for the nice wording!
>
> Hadley
>
>
>

-- 
Luke Tierney
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu

From vitekj @ending from icloud@com  Fri Aug 10 01:54:24 2018
From: vitekj @ending from icloud@com (jan Vitek)
Date: Fri, 10 Aug 2018 00:54:24 +0100
Subject: [Rd] vctrs: a type system for the tidyverse
In-Reply-To: <CABdHhvHNqWFDamNQU1ZC_Z022LdE0qBzz4swYGkMV1eFi-2hHA@mail.gmail.com>
References: <CABdHhvFARmiExcEk=NUjfPcRtYnoF4pAA5WTws6r1C+X76zHEw@mail.gmail.com>
 <CAMFmJsn_wNxuV2SQqUiZroJYV+rHRKdADi8Fjg8fRjJr0RGuwQ@mail.gmail.com>
 <CABdHhvEAV-bhprxVMHF_R8vKdN3VEriV86kGNNU02RiK=+m4dQ@mail.gmail.com>
 <CABdHhvHw+_sFcD3H92KQhvurxC8eAyR2A0frZUk7pBcOaVB2Hw@mail.gmail.com>
 <23403.4648.841262.201573@stat.math.ethz.ch>
 <CABdHhvH53=0b7Hg=9dAPx9zUTn4YSYR0=JoDzWMRMjkMpmXbBQ@mail.gmail.com>
 <CABdHhvGucJKbaFmB1gk3i0vgen9OPQ7Wb=nQKm7aOoBFbC0sqA@mail.gmail.com>
 <A260BC05-0755-4911-98B3-74071569A9F4@icloud.com>
 <CABdHhvHNqWFDamNQU1ZC_Z022LdE0qBzz4swYGkMV1eFi-2hHA@mail.gmail.com>
Message-ID: <DEF40A8A-7A92-4E7F-9456-9185B6992CFD@icloud.com>


> 
> I think there's a bit of that flavour here:
> 
> vec_c(factor("a"), Sys.Date())
> #> Error: No common type for factor and date
> 
> This isn't a type system imposed by the language, but I don't think
> that's a reason not to call it a type system.

All I am saying is that without a clear definition of what is the class 
of errors being prevented CS folks would not think of it as a type system. 

In Java, the type system guarantees that you will not have a Method Not 
Understood Error.  In ML, the type system ensure that all operations are 
applied to the data types that they are defined for.

Also, a type system gives a guarantee over all programs. Here the guarantee
only applies for certain functions.


This said it would be interesting to describe precisely what are things
that are errors that ought to be prevent for R.  

The discussion on what happens when you merge two vectors with different 
factor levels is really interesting in that respect as it suggest that
there are non-trivial issues that need to be worked out.


-j


From jo@h@m@ulrich @ending from gm@il@com  Fri Aug 10 12:51:48 2018
From: jo@h@m@ulrich @ending from gm@il@com (Joshua Ulrich)
Date: Fri, 10 Aug 2018 05:51:48 -0500
Subject: [Rd] Bug in POSIXct string representation?
In-Reply-To: <C4657A34F48F9540B0852AC259A2128526D9DF4D@EX2010MX01.v2c2.at>
References: <C4657A34F48F9540B0852AC259A2128526D9DF4D@EX2010MX01.v2c2.at>
Message-ID: <CAPPM_gSA4mfj9HMzY7okG470ac8tjtu8g9p6iD5w5DTijFPAPA@mail.gmail.com>

Hi Andreas,

On Thu, Aug 9, 2018 at 2:26 AM, Festl, Andreas <Andreas.Festl at v2c2.at> wrote:
> Dear all,
>
> I just have identified the following issue which I believe could be a bug in R:
>
> Let me illustrate:
>
> First, enable the display of fractional seconds and check that it works:
>> options(digits.secs = 6, digits = 6)
>> as.character(as.POSIXct("2018-08-31 14:15:16.123456"))
> [1] "2018-08-31 14:15:16.123456"
>
> Now create a sequence of POSIXct with stepwidth 0.1sec:
>> test <- as.POSIXct("2018-08-31 14:15:16.000000")
>> test_seq <- seq(test, test + 1, by = 1/10)
>
> Calling format with the millisecond conversion specification gives the intended result (even though there is a small representation error):
>> format(test_seq, "%F %T.%OS")
>  [1] "2018-08-31 14:15:16.16.000000" "2018-08-31 14:15:16.16.099999" "2018-08-31 14:15:16.16.200000" "2018-08-31 14:15:16.16.299999"
>  [5] "2018-08-31 14:15:16.16.400000" "2018-08-31 14:15:16.16.500000" "2018-08-31 14:15:16.16.599999" "2018-08-31 14:15:16.16.700000"
>  [9] "2018-08-31 14:15:16.16.799999" "2018-08-31 14:15:16.16.900000" "2018-08-31 14:15:17.17.000000"
>
> However, if I use as.character, the milliseconds seemingly just get cut-off after one digit, resulting in incorrect representations:
>> as.character(test_seq)
>  [1] "2018-08-31 14:15:16.0" "2018-08-31 14:15:16.0" "2018-08-31 14:15:16.2" "2018-08-31 14:15:16.2" "2018-08-31 14:15:16.4" "2018-08-31 14:15:16.5"
>  [7] "2018-08-31 14:15:16.5" "2018-08-31 14:15:16.7" "2018-08-31 14:15:16.7" "2018-08-31 14:15:16.9" "2018-08-31 14:15:17.0"
>
> It seems to me, that R correctly decides that there is only one significant digit after the decimal point, but then incorrectly (due to representation error) just cuts off after the first digit.
>
This is known behavior with how POSIXt objects are printed, and has
been discussed before on R-help:
https://stat.ethz.ch/pipermail/r-help/2015-June/429600.html

Basically, the behavior is a combination of truncating fractional
seconds rather than rounding combined with the floating point
representation error you noticed.  And truncation is the behavior for
printing whole seconds:
format(as.POSIXct("2018-08-31 14:15:16.9"))  # 16s, not 17s
[1] "2018-08-31 14:15:16"

So it would not be consistent to round fractional seconds, unless you
kept track of the rounding error relative to the desired resolution.

There are more details in the R-help thread and the StackOverflow Q&A
it references.

Best,
Josh

> BR,
>   Andreas
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel



-- 
Joshua Ulrich  |  about.me/joshuaulrich
FOSS Trading  |  www.fosstrading.com
R/Finance 2018 | www.rinfinance.com


From tom@@@k@liber@ @ending from gm@il@com  Fri Aug 10 14:17:29 2018
From: tom@@@k@liber@ @ending from gm@il@com (Tomas Kalibera)
Date: Fri, 10 Aug 2018 14:17:29 +0200
Subject: [Rd] 
 SIGSEGV in R_RunWeakRefFinalizer, object allocated with Rcpp
In-Reply-To: <CALEXWq0H8Qdtc18Enr+TTm5DbDSJx3YtaLHaGZxr7V9Okz8q+Q@mail.gmail.com>
References: <CALEXWq2E+-k=Oka1YZAzGzYU1bLmcC3ptUUrv8RUMUrpcviK-g@mail.gmail.com>
 <a03f65c7-3e0e-7857-0e1e-a809e1051b56@gmail.com>
 <23404.36580.740017.811392@rob.eddelbuettel.com>
 <alpine.DEB.2.20.1808091406240.3237@luke-Latitude-7480>
 <CALEXWq0H8Qdtc18Enr+TTm5DbDSJx3YtaLHaGZxr7V9Okz8q+Q@mail.gmail.com>
Message-ID: <e190406d-80f6-954d-572c-1141b837be0c@gmail.com>

Hi I?aki,

I think that "still reachable" memory is potentially a problem only if 
you cared about (frequent) package unloading, and if package unloading 
did not have correctness problems in the first place. I would only worry 
about "memory leaks" reported by valgrind.

That your example (unloading a library while there is still an object 
with a finalizer implemented in that library) segfaults on one system 
but not another is not too surprising. Depending on the OS, the dynamic 
linker, the current state of the system, etc - when the GC tries to run 
the finalizer, it may point to inaccessible memory (segfault), but also 
still to the code of the unloaded finalizer (possibly no segfault) or 
something else. There is no way to distinguish between all these cases 
before running the finalizer - neither the OS nor R can do it - we could 
only in principle, with a lot of other problems and platform-dependent 
hacks, detect when the memory is inaccessible, but that may not be worth it.

Best
Tomas


On 08/10/2018 12:46 AM, I?aki ?car wrote:
> Thanks, Tomas, Luke, for the clarifications. Then, I have another question.
>
> But first, let me introduce how I ended up here, because obviously I
> just don't go around dyn.unloading things that I've just compiled. I
> was testing a package with valgrind. Everything ok, no leaks. Great.
> But I'm always suspicious (probably unjustifiably) of all the memory
> that is reported as "still reachable", so I wanted to check whether
> there was any difference if I detach(unload=TRUE) the package after
> all the tests.
>
> In a nutshell, I ended up discovering that the following code:
>
> ```
> library(simmer)
> simmer() # allocates a C++ object, as in my initial example
> detach("package:simmer", unload=TRUE)
> ```
>
> segfaults on Windows, but not on Linux (then I built the example in my
> initial email to confirm it wasn't simmer's fault). So given that,
> from your explanation, I should expect a segfault here, the question
> is: what on Earth does (or does not) R on Linux do to avoid
> segfaulting compared to Windows? :) And a corolary would be, can't R
> on Windows do the same?
>
> Regards,
> I?aki
>
> El jue., 9 ago. 2018 a las 21:13, <luke-tierney at uiowa.edu> escribi?:
>> On Thu, 9 Aug 2018, Dirk Eddelbuettel wrote:
>>
>>> On 9 August 2018 at 20:37, Tomas Kalibera wrote:
>>> | So to answer your original question, this could probably be handled in
>>> | Rcpp,
>>>
>>> Hm. Why do you say that / what did you have in mind?
>> We say it because it is true. Rcpp registers C finalizers and running
>> them after unloading will segfault. For now it would be better for Rcpp
>> (and everyone else) to explicitly discourage unloading as it is
>> unreliable on many levels.
>>
>> What Rcpp could do to avoid segfaulting is to keep a weak list of all
>> objects to which it attaches C finalizers and arrange for those to be
>> cleaned up in an R_unload_<dllname> routine. Not clear it is worth the
>> trouble. At the R level we could provide more support for this since
>> we already have a weak list of objects with finalizers, but again not
>> clear it is worth the trouble.
>>
>> Best,
>>
>> luke
>>
>>> Recall that we do not alter SEXPs or introduce additional additional
>>> reference counters -- because we do not think that altering the basic R API
>>> for such calls would be a wise strategy.  So we do more or less what is done
>>> in C for R, with some additional hand-holding which circumvents a number of
>>> common errors.
>>>
>>> | but in either case I would not use dyn.unload() in the first
>>> | place. This problem may be just one of many.
>>>
>>> I think I'd second that. I never had much unloading packages or dynamic
>>> libraries and tend to "just say no". Both short-lived processes (ie via 'r')
>>> as well as long sessions (ie R via ESS, running for weeks) work for my
>>> workflows.
>>>
>>> Dirk
>>>
>>>
>> --
>> Luke Tierney
>> Ralph E. Wareham Professor of Mathematical Sciences
>> University of Iowa                  Phone:             319-335-3386
>> Department of Statistics and        Fax:               319-335-3017
>>      Actuarial Science
>> 241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
>> Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu
>
>
> --
> I?aki ?car
> http://www.enchufa2.es
> @Enchufa2


From @purdle@@ @ending from gm@il@com  Sat Aug 11 01:03:13 2018
From: @purdle@@ @ending from gm@il@com (Abs Spurdle)
Date: Sat, 11 Aug 2018 11:03:13 +1200
Subject: [Rd] WishList: Remove Generic Arguments
In-Reply-To: <bcc81123-3c54-2a64-8396-6492eaf7165b@gmail.com>
References: <CAB8pepwoUPZRQ8V_OCaZq=QdM3fRbT4zYa8OjOgZvKucmSVFQA@mail.gmail.com>
 <bcc81123-3c54-2a64-8396-6492eaf7165b@gmail.com>
Message-ID: <CAB8pepyEBVFQYPtvZmjMLekEgaM48N+T5b+BXx7yTEqhPoaonw@mail.gmail.com>

 > If x is not a sensible name within the print.myfunction() method, then
there's a one line fix:
>
> print.myfunction <- function(x, ...) {
>   f <- x
>   dosomething(f)
> }

Naming the argument x is not an option.

>>  print = function (...) base::print (...)

>  That's a really, really bad idea.  If there are two generics named the
same, how are your users going to know which one they are getting when they
just say print(myobj)?

If redefining the generics is "a really, really bad idea" then an
alternative would be to define a new generic and two methods, one of which
acts as a kind of bridging function, using something like:

myprint = function (...) UseMethod ("myprint")
myprint.myfunction = function (f, ...) {dosomething (f, ...)}

print.myfunction = function (x, ...) myprint (x, ...)

>  If a generic always needs two arguments, why not name them, so R can
complain when a user calls it with just one?

We can apply that principle to the methods rather than the generic.



On Fri, Aug 10, 2018 at 10:20 AM, Duncan Murdoch <murdoch.duncan at gmail.com>
wrote:

> On 09/08/2018 5:45 PM, Abs Spurdle wrote:
>
>>   I apologize if this issue has been raised before.
>>
>> I really like object oriented S3 programming.
>> However, there's one feature of object oriented S3 programming that I
>> don't
>> like.
>> Generic functions can have arguments other than dots.
>>
>> Lets say you have an R package with something like:
>>
>> print.myfunction (f, ...)
>> {   dosomething (f, ...)
>> }
>>
>> Noting that I use function objects a lot.
>>
>> R CMD check will generate a warning because you've named your object f
>> rather than x.
>>
>> I don't want to name my object x.
>> I want to name my object f.
>>
>
> There are reasons for this requirement.  Suppose I have an object of class
> myfunction named myobj.  However, being a 3rd party, I have no idea what
> class myfunction is about.  Then I might say
>
> print(x = myobj)
>
> That would not work with your method, because the x would be absorbed into
> ..., it would not bind to f.
>
>
> Naming the object x makes the program unreadable.
>> Especially if f contains an attribute or an argument named x.
>>
>
> That's nonsense.  You don't need to name your object the same as an
> argument.  You name your object something readable (e.g. myobj), then pass
> it to print(), which binds it to x.
>
> If x is not a sensible name within the print.myfunction() method, then
> there's a one line fix:
>
> print.myfunction <- function(x, ...) {
>   f <- x
>   dosomething(f)
> }
>
>>
>> There's a work around.
>> You can redefine the print function, using something like:
>>
>> print = function (...) base::print (...)
>>
>> However, you have to export and document the function.
>>
>
> That's a really, really bad idea.  If there are two generics named the
> same, how are your users going to know which one they are getting when they
> just say print(myobj)?
>
>
>> I think that it would be better if generic functions didn't have any
>> arguments except for dots.
>>
>
> That makes argument checking much harder.  If a generic always needs two
> arguments, why not name them, so R can complain when a user calls it with
> just one?
>
> Duncan Murdoch
>

	[[alternative HTML version deleted]]


From i@uc@r86 @ending from gm@il@com  Sat Aug 11 01:38:21 2018
From: i@uc@r86 @ending from gm@il@com (=?UTF-8?B?ScOxYWtpIMOaY2Fy?=)
Date: Sat, 11 Aug 2018 01:38:21 +0200
Subject: [Rd] 
 SIGSEGV in R_RunWeakRefFinalizer, object allocated with Rcpp
In-Reply-To: <e190406d-80f6-954d-572c-1141b837be0c@gmail.com>
References: <CALEXWq2E+-k=Oka1YZAzGzYU1bLmcC3ptUUrv8RUMUrpcviK-g@mail.gmail.com>
 <a03f65c7-3e0e-7857-0e1e-a809e1051b56@gmail.com>
 <23404.36580.740017.811392@rob.eddelbuettel.com>
 <alpine.DEB.2.20.1808091406240.3237@luke-Latitude-7480>
 <CALEXWq0H8Qdtc18Enr+TTm5DbDSJx3YtaLHaGZxr7V9Okz8q+Q@mail.gmail.com>
 <e190406d-80f6-954d-572c-1141b837be0c@gmail.com>
Message-ID: <CALEXWq0iTbun=yRNBOMao_GappHc2KKT1an-WyREy-Z3KoSRGw@mail.gmail.com>

El vie., 10 ago. 2018 a las 14:17, Tomas Kalibera
(<tomas.kalibera at gmail.com>) escribi?:
>
> Hi I?aki,
>
> I think that "still reachable" memory is potentially a problem only if
> you cared about (frequent) package unloading, and if package unloading
> did not have correctness problems in the first place. I would only worry
> about "memory leaks" reported by valgrind.
>
> That your example (unloading a library while there is still an object
> with a finalizer implemented in that library) segfaults on one system
> but not another is not too surprising. Depending on the OS, the dynamic
> linker, the current state of the system, etc - when the GC tries to run
> the finalizer, it may point to inaccessible memory (segfault), but also
> still to the code of the unloaded finalizer (possibly no segfault) or
> something else. There is no way to distinguish between all these cases
> before running the finalizer - neither the OS nor R can do it - we could
> only in principle, with a lot of other problems and platform-dependent
> hacks, detect when the memory is inaccessible, but that may not be worth it.

Ok, I get it. And I agree: it's not worth it. Thanks again.

I?aki

>
> Best
> Tomas
>
>
> On 08/10/2018 12:46 AM, I?aki ?car wrote:
> > Thanks, Tomas, Luke, for the clarifications. Then, I have another question.
> >
> > But first, let me introduce how I ended up here, because obviously I
> > just don't go around dyn.unloading things that I've just compiled. I
> > was testing a package with valgrind. Everything ok, no leaks. Great.
> > But I'm always suspicious (probably unjustifiably) of all the memory
> > that is reported as "still reachable", so I wanted to check whether
> > there was any difference if I detach(unload=TRUE) the package after
> > all the tests.
> >
> > In a nutshell, I ended up discovering that the following code:
> >
> > ```
> > library(simmer)
> > simmer() # allocates a C++ object, as in my initial example
> > detach("package:simmer", unload=TRUE)
> > ```
> >
> > segfaults on Windows, but not on Linux (then I built the example in my
> > initial email to confirm it wasn't simmer's fault). So given that,
> > from your explanation, I should expect a segfault here, the question
> > is: what on Earth does (or does not) R on Linux do to avoid
> > segfaulting compared to Windows? :) And a corolary would be, can't R
> > on Windows do the same?
> >
> > Regards,
> > I?aki
> >
> > El jue., 9 ago. 2018 a las 21:13, <luke-tierney at uiowa.edu> escribi?:
> >> On Thu, 9 Aug 2018, Dirk Eddelbuettel wrote:
> >>
> >>> On 9 August 2018 at 20:37, Tomas Kalibera wrote:
> >>> | So to answer your original question, this could probably be handled in
> >>> | Rcpp,
> >>>
> >>> Hm. Why do you say that / what did you have in mind?
> >> We say it because it is true. Rcpp registers C finalizers and running
> >> them after unloading will segfault. For now it would be better for Rcpp
> >> (and everyone else) to explicitly discourage unloading as it is
> >> unreliable on many levels.
> >>
> >> What Rcpp could do to avoid segfaulting is to keep a weak list of all
> >> objects to which it attaches C finalizers and arrange for those to be
> >> cleaned up in an R_unload_<dllname> routine. Not clear it is worth the
> >> trouble. At the R level we could provide more support for this since
> >> we already have a weak list of objects with finalizers, but again not
> >> clear it is worth the trouble.
> >>
> >> Best,
> >>
> >> luke
> >>
> >>> Recall that we do not alter SEXPs or introduce additional additional
> >>> reference counters -- because we do not think that altering the basic R API
> >>> for such calls would be a wise strategy.  So we do more or less what is done
> >>> in C for R, with some additional hand-holding which circumvents a number of
> >>> common errors.
> >>>
> >>> | but in either case I would not use dyn.unload() in the first
> >>> | place. This problem may be just one of many.
> >>>
> >>> I think I'd second that. I never had much unloading packages or dynamic
> >>> libraries and tend to "just say no". Both short-lived processes (ie via 'r')
> >>> as well as long sessions (ie R via ESS, running for weeks) work for my
> >>> workflows.
> >>>
> >>> Dirk
> >>>
> >>>
> >> --
> >> Luke Tierney
> >> Ralph E. Wareham Professor of Mathematical Sciences
> >> University of Iowa                  Phone:             319-335-3386
> >> Department of Statistics and        Fax:               319-335-3017
> >>      Actuarial Science
> >> 241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
> >> Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu


From henrik@bengt@@on @ending from gm@il@com  Sun Aug 12 22:00:57 2018
From: henrik@bengt@@on @ending from gm@il@com (Henrik Bengtsson)
Date: Sun, 12 Aug 2018 22:00:57 +0200
Subject: [Rd] substitute() on arguments in ellipsis ("dot dot dot")?
Message-ID: <CAFDcVCQppvvnXSgnO0CeG8Hpr7JWxFUMTnO7ORjK2JkUFKKWAw@mail.gmail.com>

Hi. For any number of *known* arguments, we can do:

one <- function(a) list(a = substitute(a))
two <- function(a, b) list(a = substitute(a), b = substitute(b))

and so on. But how do I achieve the same when I have:

dots <- function(...) list(???)

I want to implement this such that I can do:

> exprs <- dots(1+2)
> str(exprs)
List of 1
 $ : language 1 + 2

as well as:

> exprs <- dots(1+2, "a", rnorm(3))
> str(exprs)
List of 3
 $ : language 1 + 2
 $ : chr "a"
 $ : language rnorm(3)

Is this possible to achieve using plain R code?

Thanks,

Henrik


From murdoch@dunc@n @ending from gm@il@com  Sun Aug 12 22:16:07 2018
From: murdoch@dunc@n @ending from gm@il@com (Duncan Murdoch)
Date: Sun, 12 Aug 2018 16:16:07 -0400
Subject: [Rd] substitute() on arguments in ellipsis ("dot dot dot")?
In-Reply-To: <CAFDcVCQppvvnXSgnO0CeG8Hpr7JWxFUMTnO7ORjK2JkUFKKWAw@mail.gmail.com>
References: <CAFDcVCQppvvnXSgnO0CeG8Hpr7JWxFUMTnO7ORjK2JkUFKKWAw@mail.gmail.com>
Message-ID: <601ad637-43cd-63e7-81b7-7b350bff5624@gmail.com>

On 12/08/2018 4:00 PM, Henrik Bengtsson wrote:
> Hi. For any number of *known* arguments, we can do:
> 
> one <- function(a) list(a = substitute(a))
> two <- function(a, b) list(a = substitute(a), b = substitute(b))
> 
> and so on. But how do I achieve the same when I have:
> 
> dots <- function(...) list(???)
> 
> I want to implement this such that I can do:
> 
>> exprs <- dots(1+2)
>> str(exprs)
> List of 1
>   $ : language 1 + 2
> 
> as well as:
> 
>> exprs <- dots(1+2, "a", rnorm(3))
>> str(exprs)
> List of 3
>   $ : language 1 + 2
>   $ : chr "a"
>   $ : language rnorm(3)
> 
> Is this possible to achieve using plain R code?

I think so.  substitute(list(...)) gives you a single expression 
containing a call to list() with the unevaluated arguments; you can 
convert that to what you want using something like

dots <- function (...) {
   exprs <- substitute(list(...))
   as.list(exprs[-1])
}

Duncan Murdoch


From jeroenoom@ @ending from gm@il@com  Sun Aug 12 22:46:09 2018
From: jeroenoom@ @ending from gm@il@com (Jeroen Ooms)
Date: Sun, 12 Aug 2018 22:46:09 +0200
Subject: [Rd] substitute() on arguments in ellipsis ("dot dot dot")?
In-Reply-To: <CAFDcVCQppvvnXSgnO0CeG8Hpr7JWxFUMTnO7ORjK2JkUFKKWAw@mail.gmail.com>
References: <CAFDcVCQppvvnXSgnO0CeG8Hpr7JWxFUMTnO7ORjK2JkUFKKWAw@mail.gmail.com>
Message-ID: <CABFfbXt+XtK82Pz9NN6_2R8W4A72LgQ1NkF27rZxdBUOYEwYSw@mail.gmail.com>

On Sun, Aug 12, 2018 at 10:00 PM, Henrik Bengtsson
<henrik.bengtsson at gmail.com> wrote:
> Hi. For any number of *known* arguments, we can do:
>
> one <- function(a) list(a = substitute(a))
> two <- function(a, b) list(a = substitute(a), b = substitute(b))
>
> and so on. But how do I achieve the same when I have:
>
> dots <- function(...) list(???)
>
> I want to implement this such that I can do:
>
>> exprs <- dots(1+2)
>> str(exprs)
> List of 1
>  $ : language 1 + 2
>
> as well as:
>
>> exprs <- dots(1+2, "a", rnorm(3))
>> str(exprs)
> List of 3
>  $ : language 1 + 2
>  $ : chr "a"
>  $ : language rnorm(3)
>
> Is this possible to achieve using plain R code?

You could use match.call, for example:

  dots <- function(...) match.call(expand.dots = FALSE)[['...']]

Note that this returns a pairlist, so if you want an ordinary list you
should wrap it in as.list()


From gecon@m@inten@nce @ending from gm@il@com  Sun Aug 12 21:31:43 2018
From: gecon@m@inten@nce @ending from gm@il@com (Karol Podemski)
Date: Sun, 12 Aug 2018 21:31:43 +0200
Subject: [Rd] Package compiler - efficiency problem
Message-ID: <CAC_=cNYEie=yjdDvJ1eKhRYSXSYAFmrgU5aO_baJud-6-xBsOw@mail.gmail.com>

 Dear R team,

I am a co-author and maintainer of one of R packages distributed by R-forge
(gEcon). One of gEcon package users found a strange behaviour of package (R
froze for couple of minutes) and reported it to me. I traced the strange
behaviour to compiler package. I attach short demonstration of the problem
to this mail (demonstration makes use of compiler and tictoc packages only).

In short, the compiler package has problems in compiling large functions -
their compilation and execution may take much longer than direct execution
of an uncompiled function. Such functions are generated by gEcon package as
they describe steady state for economy.

I am curious if you are aware of such problems and plan to handle the
efficiency issues. On one of the boards I saw that there were efficiency
issues in rpart package but they have been resolved. Or would you advise to
turn off JIT on package load (package heavily uses such long functions
generated whenever a new model is created)?

Best regards,
Karol Podemski

-------------- next part --------------
A non-text attachment was scrubbed...
Name: possible_bug_report.tar.gz
Type: application/x-gzip
Size: 88725 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20180812/6ce1f0a0/attachment.bin>

From peter@meil@trup @ending from gm@il@com  Mon Aug 13 07:09:51 2018
From: peter@meil@trup @ending from gm@il@com (Peter Meilstrup)
Date: Sun, 12 Aug 2018 22:09:51 -0700
Subject: [Rd] substitute() on arguments in ellipsis ("dot dot dot")?
In-Reply-To: <601ad637-43cd-63e7-81b7-7b350bff5624@gmail.com>
References: <CAFDcVCQppvvnXSgnO0CeG8Hpr7JWxFUMTnO7ORjK2JkUFKKWAw@mail.gmail.com>
 <601ad637-43cd-63e7-81b7-7b350bff5624@gmail.com>
Message-ID: <CAJoaRhZDpA4PaaBRdoTCmpNSD4kjaN7+OVKwycW5Fue72R0-Pg@mail.gmail.com>

Interestingly,

   as.list(substitute(...()))

also works.

On Sun, Aug 12, 2018 at 1:16 PM, Duncan Murdoch
<murdoch.duncan at gmail.com> wrote:
> On 12/08/2018 4:00 PM, Henrik Bengtsson wrote:
>>
>> Hi. For any number of *known* arguments, we can do:
>>
>> one <- function(a) list(a = substitute(a))
>> two <- function(a, b) list(a = substitute(a), b = substitute(b))
>>
>> and so on. But how do I achieve the same when I have:
>>
>> dots <- function(...) list(???)
>>
>> I want to implement this such that I can do:
>>
>>> exprs <- dots(1+2)
>>> str(exprs)
>>
>> List of 1
>>   $ : language 1 + 2
>>
>> as well as:
>>
>>> exprs <- dots(1+2, "a", rnorm(3))
>>> str(exprs)
>>
>> List of 3
>>   $ : language 1 + 2
>>   $ : chr "a"
>>   $ : language rnorm(3)
>>
>> Is this possible to achieve using plain R code?
>
>
> I think so.  substitute(list(...)) gives you a single expression containing
> a call to list() with the unevaluated arguments; you can convert that to
> what you want using something like
>
> dots <- function (...) {
>   exprs <- substitute(list(...))
>   as.list(exprs[-1])
> }
>
> Duncan Murdoch
>
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From henrik@bengt@@on @ending from gm@il@com  Mon Aug 13 11:18:59 2018
From: henrik@bengt@@on @ending from gm@il@com (Henrik Bengtsson)
Date: Mon, 13 Aug 2018 11:18:59 +0200
Subject: [Rd] substitute() on arguments in ellipsis ("dot dot dot")?
In-Reply-To: <CAJoaRhZDpA4PaaBRdoTCmpNSD4kjaN7+OVKwycW5Fue72R0-Pg@mail.gmail.com>
References: <CAFDcVCQppvvnXSgnO0CeG8Hpr7JWxFUMTnO7ORjK2JkUFKKWAw@mail.gmail.com>
 <601ad637-43cd-63e7-81b7-7b350bff5624@gmail.com>
 <CAJoaRhZDpA4PaaBRdoTCmpNSD4kjaN7+OVKwycW5Fue72R0-Pg@mail.gmail.com>
Message-ID: <CAFDcVCT2undj5oQHW4-wiuU2-F5jx9c6-z817rth08Bz3ycvGQ@mail.gmail.com>

Thanks all, this was very helpful.  Peter's finding - dots2() below -
is indeed interesting - I'd be curious to learn what goes on there.

The different alternatives perform approximately the same;

dots1 <- function(...) as.list(substitute(list(...)))[-1L]
dots2 <- function(...) as.list(substitute(...()))
dots3 <- function(...) match.call(expand.dots = FALSE)[["..."]]

stats <- microbenchmark::microbenchmark(
  dots1(1+2, "a", rnorm(3), stop("bang!")),
  dots2(1+2, "a", rnorm(3), stop("bang!")),
  dots3(1+2, "a", rnorm(3), stop("bang!")),
  times = 10e3
)
print(stats)
# Unit: microseconds
#                                        expr  min   lq mean median
uq  max neval
#  dots1(1 + 2, "a", rnorm(3), stop("bang!")) 2.14 2.45 3.04   2.58
2.73 1110 10000
#  dots2(1 + 2, "a", rnorm(3), stop("bang!")) 1.81 2.10 2.47   2.21
2.34 1626 10000
#  dots3(1 + 2, "a", rnorm(3), stop("bang!")) 2.59 2.98 3.36   3.15
3.31 1037 10000

/Henrik

On Mon, Aug 13, 2018 at 7:10 AM Peter Meilstrup
<peter.meilstrup at gmail.com> wrote:
>
> Interestingly,
>
>    as.list(substitute(...()))
>
> also works.
>
> On Sun, Aug 12, 2018 at 1:16 PM, Duncan Murdoch
> <murdoch.duncan at gmail.com> wrote:
> > On 12/08/2018 4:00 PM, Henrik Bengtsson wrote:
> >>
> >> Hi. For any number of *known* arguments, we can do:
> >>
> >> one <- function(a) list(a = substitute(a))
> >> two <- function(a, b) list(a = substitute(a), b = substitute(b))
> >>
> >> and so on. But how do I achieve the same when I have:
> >>
> >> dots <- function(...) list(???)
> >>
> >> I want to implement this such that I can do:
> >>
> >>> exprs <- dots(1+2)
> >>> str(exprs)
> >>
> >> List of 1
> >>   $ : language 1 + 2
> >>
> >> as well as:
> >>
> >>> exprs <- dots(1+2, "a", rnorm(3))
> >>> str(exprs)
> >>
> >> List of 3
> >>   $ : language 1 + 2
> >>   $ : chr "a"
> >>   $ : language rnorm(3)
> >>
> >> Is this possible to achieve using plain R code?
> >
> >
> > I think so.  substitute(list(...)) gives you a single expression containing
> > a call to list() with the unevaluated arguments; you can convert that to
> > what you want using something like
> >
> > dots <- function (...) {
> >   exprs <- substitute(list(...))
> >   as.list(exprs[-1])
> > }
> >
> > Duncan Murdoch
> >
> >
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel


From tom@@@k@liber@ @ending from gm@il@com  Mon Aug 13 18:02:23 2018
From: tom@@@k@liber@ @ending from gm@il@com (Tomas Kalibera)
Date: Mon, 13 Aug 2018 18:02:23 +0200
Subject: [Rd] Package compiler - efficiency problem
In-Reply-To: <CAC_=cNYEie=yjdDvJ1eKhRYSXSYAFmrgU5aO_baJud-6-xBsOw@mail.gmail.com>
References: <CAC_=cNYEie=yjdDvJ1eKhRYSXSYAFmrgU5aO_baJud-6-xBsOw@mail.gmail.com>
Message-ID: <b0c9736d-8d9c-c3cc-decd-eaf1a2b82b60@gmail.com>

Dear Karol,

thank you for the report. I can reproduce that the function from you 
example takes very long to compile and I can see where most time is 
spent. The compiler is itself written in R and requires a lot of 
resources for large functions (foo() has over 16,000 lines of code, 
nearly 1 million of instructions/operands, 45,000 constants). In 
particular a lot of time is spent in garbage collection and in finding a 
unique set of constants. Some optimizations of the compiler may be 
possible, but it is unlikely that functions this large will compile fast 
any soon. For non-generated code, we now have the byte-compilation on 
installation by default which at least removes the compile overhead from 
runtime. Even though the compiler is slow, it is important to keep in 
mind that in principle, with any compiler there will be functions where 
compilation would not be improve performance (when the compile time is 
included or not).

I think it is not a good idea to generate code for functions like foo() 
in R (or any interpreted language). You say that R's byte-code compiler 
produces code that runs 5-10x faster than when the function is 
interpreted by the AST interpreter (uncompiled), which sounds like a 
good result, but I believe that avoiding code generation would be much 
faster than that, apart from drastically reducing code size and 
therefore compile time. The generator of these functions has much more 
information than the compiler - it could be turned into an interpreter 
of these functions and compute their values on the fly.

A significant source of inefficiency of the generated code are 
element-wise operations, such as

r[12] <- -vv[88] + vv[16] * (1 + ppff[1307])
...

r[139] <- -vv[215] + vv[47] * (1 + ppff[1434])

(these could be vectorized, which would reduce code size and improve 
interpretation speed; and make it somewhat readable). Most of the code 
lines in the generated functions seem to be easily vectorizable.

Compilers and interpreters necessarily use some heuristics or optimize 
at some code patterns. Optimizing for generated code may be tricky as it 
could even harm performance of usual code. And, I would much rather 
optimize the compiler for the usual code.

Indeed, a pragmatic solution requiring the least amount of work would be 
to disable compilation of these generated functions. There is not a 
documented way to do that and maybe we could add it (and technically it 
is trivial), but I have been reluctant so far - in some cases, 
compilation even of these functions may be beneficial - if the speedup 
is 5-10x and we run very many times. But once the generated code 
included some pragma preventing compilation, it won't be ever compiled. 
Also, the trade-offs may change as the compiler evolves, perhaps not in 
this case, but in other where such pragma may be used.

Well so the short answer would be that these functions should not be 
generated in the first place. If it were too much work rewriting, 
perhaps the generator could just be improved to produce vectorized 
operations.

Best
Tomas

On 12.8.2018 21:31, Karol Podemski wrote:
>   Dear R team,
>
> I am a co-author and maintainer of one of R packages distributed by R-forge
> (gEcon). One of gEcon package users found a strange behaviour of package (R
> froze for couple of minutes) and reported it to me. I traced the strange
> behaviour to compiler package. I attach short demonstration of the problem
> to this mail (demonstration makes use of compiler and tictoc packages only).
>
> In short, the compiler package has problems in compiling large functions -
> their compilation and execution may take much longer than direct execution
> of an uncompiled function. Such functions are generated by gEcon package as
> they describe steady state for economy.
>
> I am curious if you are aware of such problems and plan to handle the
> efficiency issues. On one of the boards I saw that there were efficiency
> issues in rpart package but they have been resolved. Or would you advise to
> turn off JIT on package load (package heavily uses such long functions
> generated whenever a new model is created)?
>
> Best regards,
> Karol Podemski
>
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


	[[alternative HTML version deleted]]


From i@omorphi@m@ @ending from SDF@ORG  Mon Aug 13 19:55:05 2018
From: i@omorphi@m@ @ending from SDF@ORG (isomorphismes)
Date: Mon, 13 Aug 2018 17:55:05 +0000
Subject: [Rd] vector arithmetic
Message-ID: <20180813175505.GE23272@SDF.ORG>

I'm looking for where in the source recycling and vector multiplication+addition are defined. I see some stuff in ~/src/main/arithmetic.c.

Is there anywhere else I should be looking as well?

Cheers

-- 
isomorphisms at sdf.org
SDF Public Access UNIX System - http://sdf.org


From wdunl@p @ending from tibco@com  Mon Aug 13 23:41:36 2018
From: wdunl@p @ending from tibco@com (William Dunlap)
Date: Mon, 13 Aug 2018 14:41:36 -0700
Subject: [Rd] apply with zero-row matrix
In-Reply-To: <23391.1740.722692.223110@stat.math.ethz.ch>
References: <CAARY7khVXzxutThhKcq+ZftTE8-66eH-B-82xs-8TFyTTmGrkw@mail.gmail.com>
 <CAARY7kjL=buR9GE9rP8Y64D_WTdrFunfTMUXD2mw8P4gMsNrTQ@mail.gmail.com>
 <23390.49459.112386.395212@stat.math.ethz.ch>
 <CAARY7kgmMCbMtigw4i6NZMVMfv-VFdg0VQY1qKECPyAaJ=kbYA@mail.gmail.com>
 <23391.1740.722692.223110@stat.math.ethz.ch>
Message-ID: <CAF8bMcaLDg_2LT0eU3paEAmdvmkRV+RZtVycOUqhFOh_9OkMLg@mail.gmail.com>

vapply has a mandatory FUN.VALUE argument which specifies the type and size
of FUN's return value.  This helps when you want to cover the 0-length case
without 'if' statements.  You can change your apply calls to vapply calls,
but they will be a bit more complicated.  E.g.,  change
   apply(X=myMatrix, MARGIN=2, FUN=quantile)
to
   vapply(seq_len(ncol(myMatrix)), FUN=function(i)quantile(myMatrix[,i]),
FUN.VALUE=numeric(5))

The latter will always return a 5-row by ncol(myMatrix) matrix.

Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Mon, Jul 30, 2018 at 5:38 AM, Martin Maechler <maechler at stat.math.ethz.ch
> wrote:

> >>>>> David Hugh-Jones
> >>>>>     on Mon, 30 Jul 2018 10:12:24 +0100 writes:
>
>     > Hi Martin, Fair enough for R functions in general. But the
>     > behaviour of apply violates the expectation that apply(m,
>     > 1, fun) calls fun n times when m has n rows.  That seems
>     > pretty basic.
>
> Well, that expectation is obviously wrong ;-)  see below
>
>     > Also, I understand from your argument why it makes sense
>     > to call apply and return a special result (presumably
>     > NULL) for an empty argument; but why should apply call fun?
>
>     > Cheers David
>
> The reason is seen e.g. in
>
>     > apply(matrix(,0,3), 2, quantile)
>          [,1] [,2] [,3]
>     0%     NA   NA   NA
>     25%    NA   NA   NA
>     50%    NA   NA   NA
>     75%    NA   NA   NA
>     100%   NA   NA   NA
>     >
>
> and that is documented (+/-) in the first paragraph of the
> 'Value:' section of help(apply) :
>
>  > Value:
>  >
>  >      If each call to ?FUN? returns a vector of length ?n?, then ?apply?
>  >      returns an array of dimension ?c(n, dim(X)[MARGIN])? if ?n > 1?.
>  >      If ?n? equals ?1?, ?apply? returns a vector if ?MARGIN? has length
>  >      1 and an array of dimension ?dim(X)[MARGIN]? otherwise.  If ?n? is
>  >      ?0?, the result has length 0 but not necessarily the ?correct?
>  >      dimension.
>
>
> To determine 'n', the function *is* called once even when
> length(X) ==  0
>
> It may indeed be would helpful to add this explicitly to the
> help page  ( <R>/src/library/base/man/apply.Rd ).
> Can you propose a wording (in *.Rd if possible) ?
>
> With regards,
> Martin
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From wdunl@p @ending from tibco@com  Tue Aug 14 00:00:45 2018
From: wdunl@p @ending from tibco@com (William Dunlap)
Date: Mon, 13 Aug 2018 15:00:45 -0700
Subject: [Rd] trace in uniroot() ?
In-Reply-To: <941b7615-6075-6c74-537e-70ad0bc16652@gmail.com>
References: <941b7615-6075-6c74-537e-70ad0bc16652@gmail.com>
Message-ID: <CAF8bMcZBbNF74F-x-7G0gRW9Q6FA5GbGnouMMw9FD_FRefdXHQ@mail.gmail.com>

I tend to avoid the the trace/verbose arguments for the various root
finders and optimizers and instead use the trace function or otherwise
modify the function handed to the operator.  You can print or plot the
arguments or save them.  E.g.,

> trace(ff, print=FALSE, quote(cat("x=", deparse(x), "\n", sep="")))
[1] "ff"
> ff0 <- uniroot(ff, c(0, 10))
x=0
x=10
x=0.0678365490630423
x=5.03391827453152
x=0.490045026724842
x=2.76198165062818
x=1.09760394309444
x=1.92979279686131
x=1.34802524899502
x=1.38677998493585
x=1.3862897003949
x=1.38635073555115
x=1.3862897003949

or

> X <- numeric()
> trace(ff, print=FALSE, quote(X[[length(X)+1]] <<- x))
[1] "ff"
> ff0 <- uniroot(ff, c(0, 10))
> X
 [1]  0.00000000 10.00000000  0.06783655
 [4]  5.03391827  0.49004503  2.76198165
 [7]  1.09760394  1.92979280  1.34802525
[10]  1.38677998  1.38628970  1.38635074
[13]  1.38628970

This will not tell you why the objective function is being called (e.g. in
a line search
or in derivative estimation), but some plotting or other postprocessing can
ususally figure that out.


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Mon, Jul 30, 2018 at 11:35 AM, J C Nash <profjcnash at gmail.com> wrote:

> In looking at rootfinding for the histoRicalg project (see
> gitlab.com/nashjc/histoRicalg),
> I thought I would check how uniroot() solves some problems. The following
> short example
>
> ff <- function(x){ exp(0.5*x) - 2 }
> ff(2)
> ff(1)
> uniroot(ff, 0, 10)
> uniroot(ff, c(0, 10), trace=1)
> uniroot(ff, c(0, 10), trace=TRUE)
>
>
> shows that the trace parameter, as described in the Rd file, does not seem
> to
> be functional except in limited situations (and it suggests an
> integer, then uses a logical for the example, e.g.,
>  ## numerically, f(-|M|) becomes zero :
>      u3 <- uniroot(exp, c(0,2), extendInt="yes", trace=TRUE)
> )
>
> When extendInt is set, then there is some information output, but trace
> alone
> produces nothing.
>
> I looked at the source code -- it is in R-3.5.1/src/library/stats/R/nlm.R
> and
> calls zeroin2 code from R-3.5.1/src/library/stats/src/optimize.c as far
> as I
> can determing. My code inspection suggests trace does not show the
> iterations
> of the rootfinding, and only has effect when the search interval is allowed
> to be extended. It does not appear that there is any mechanism to ask
> the zeroin2 C code to display intermediate work.
>
> This isn't desperately important for me as I wrote an R version of the
> code in
> package rootoned on R-forge (which Martin Maechler adapted as unirootR.R in
> Rmpfr so multi-precision roots can be found). My zeroin.R has 'trace' to
> get
> the pattern of different steps. In fact it is a bit excessive. Note
> unirootR.R uses 'verbose' rather than 'trace'. However, it would be nice
> to be
> able to see what is going on with uniroot() to verify equivalent operation
> at
> the same precision level. It is very easy for codes to be very slightly
> different and give quite widely different output.
>
> Indeed, even without the trace, we see (zeroin from rootoned here)
>
> > zeroin(ff, c(0, 10), trace=FALSE)
> $root
> [1] 1.386294
>
> $froot
> [1] -5.658169e-10
>
> $rtol
> [1] 7.450581e-09
>
> $maxit
> [1] 9
>
> > uniroot(ff, c(0, 10), trace=FALSE)
> $root
> [1] 1.38629
>
> $f.root
> [1] -4.66072e-06
>
> $iter
> [1] 10
>
> $init.it
> [1] NA
>
> $estim.prec
> [1] 6.103516e-05
>
> >
>
> Is the lack of trace a bug, or at least an oversight? Being able to follow
> iterations is a
> classic approach to checking that computations are proceeding as they
> should.
>
> Best, JN
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From profjcn@@h @ending from gm@il@com  Tue Aug 14 00:44:15 2018
From: profjcn@@h @ending from gm@il@com (J C Nash)
Date: Mon, 13 Aug 2018 18:44:15 -0400
Subject: [Rd] trace in uniroot() ?
In-Reply-To: <CAF8bMcZBbNF74F-x-7G0gRW9Q6FA5GbGnouMMw9FD_FRefdXHQ@mail.gmail.com>
References: <941b7615-6075-6c74-537e-70ad0bc16652@gmail.com>
 <CAF8bMcZBbNF74F-x-7G0gRW9Q6FA5GbGnouMMw9FD_FRefdXHQ@mail.gmail.com>
Message-ID: <3babc515-ed8c-e990-3ede-0e80e845ebba@gmail.com>

Despite my years with R, I didn't know about trace(). Thanks.

However, my decades in the minimization and root finding game make me like having
a trace that gives some info on the operation, the argument and the current function value.
I've usually found glitches are a result of things like >= rather than > in tests etc., and
knowing what was done is the quickest way to get there.

This is, of course, the numerical software developer view. I know "users" (a far too vague
term) don't like such output. I've sometimes been tempted with my svd or optimization codes to
have a return message in bold-caps "YOUR ANSWER IS WRONG AND THERE'S A LAWYER WAITING TO
MAKE YOU PAY", but I usually just satisfy myself with "Not at a minimum/root".

Best, JN

On 2018-08-13 06:00 PM, William Dunlap wrote:
> I tend to avoid the the trace/verbose arguments for the various root finders and optimizers and instead use the trace
> function or otherwise modify the function handed to the operator.? You can print or plot the arguments or save them.? E.g.,
> 
>> trace(ff, print=FALSE, quote(cat("x=", deparse(x), "\n", sep="")))
> [1] "ff"
>> ff0 <- uniroot(ff, c(0, 10))
> x=0
> x=10
> x=0.0678365490630423
> x=5.03391827453152
> x=0.490045026724842
> x=2.76198165062818
> x=1.09760394309444
> x=1.92979279686131
> x=1.34802524899502
> x=1.38677998493585
> x=1.3862897003949
> x=1.38635073555115
> x=1.3862897003949
> 
> or
> 
>> X <- numeric()
>> trace(ff, print=FALSE, quote(X[[length(X)+1]] <<- x))
> [1] "ff"
>> ff0 <- uniroot(ff, c(0, 10))
>> X
> ?[1]? 0.00000000 10.00000000? 0.06783655
> ?[4]? 5.03391827? 0.49004503? 2.76198165
> ?[7]? 1.09760394? 1.92979280? 1.34802525
> [10]? 1.38677998? 1.38628970? 1.38635074
> [13]? 1.38628970
> 
> This will not tell you why the objective function is being called (e.g. in a line search
> or in derivative estimation), but some plotting or other postprocessing can ususally figure that out.
> 
> 
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com <http://tibco.com>
> 
> On Mon, Jul 30, 2018 at 11:35 AM, J C Nash <profjcnash at gmail.com <mailto:profjcnash at gmail.com>> wrote:
> 
>     In looking at rootfinding for the histoRicalg project (see gitlab.com/nashjc/histoRicalg
>     <http://gitlab.com/nashjc/histoRicalg>),
>     I thought I would check how uniroot() solves some problems. The following short example
> 
>     ff <- function(x){ exp(0.5*x) - 2 }
>     ff(2)
>     ff(1)
>     uniroot(ff, 0, 10)
>     uniroot(ff, c(0, 10), trace=1)
>     uniroot(ff, c(0, 10), trace=TRUE)
> 
> 
>     shows that the trace parameter, as described in the Rd file, does not seem to
>     be functional except in limited situations (and it suggests an
>     integer, then uses a logical for the example, e.g.,
>     ?## numerically, f(-|M|) becomes zero :
>     ? ? ?u3 <- uniroot(exp, c(0,2), extendInt="yes", trace=TRUE)
>     )
> 
>     When extendInt is set, then there is some information output, but trace alone
>     produces nothing.
> 
>     I looked at the source code -- it is in R-3.5.1/src/library/stats/R/nlm.R and
>     calls zeroin2 code from R-3.5.1/src/library/stats/src/optimize.c as far as I
>     can determing. My code inspection suggests trace does not show the iterations
>     of the rootfinding, and only has effect when the search interval is allowed
>     to be extended. It does not appear that there is any mechanism to ask
>     the zeroin2 C code to display intermediate work.
> 
>     This isn't desperately important for me as I wrote an R version of the code in
>     package rootoned on R-forge (which Martin Maechler adapted as unirootR.R in
>     Rmpfr so multi-precision roots can be found). My zeroin.R has 'trace' to get
>     the pattern of different steps. In fact it is a bit excessive. Note
>     unirootR.R uses 'verbose' rather than 'trace'. However, it would be nice to be
>     able to see what is going on with uniroot() to verify equivalent operation at
>     the same precision level. It is very easy for codes to be very slightly
>     different and give quite widely different output.
> 
>     Indeed, even without the trace, we see (zeroin from rootoned here)
> 
>     > zeroin(ff, c(0, 10), trace=FALSE)
>     $root
>     [1] 1.386294
> 
>     $froot
>     [1] -5.658169e-10
> 
>     $rtol
>     [1] 7.450581e-09
> 
>     $maxit
>     [1] 9
> 
>     > uniroot(ff, c(0, 10), trace=FALSE)
>     $root
>     [1] 1.38629
> 
>     $f.root
>     [1] -4.66072e-06
> 
>     $iter
>     [1] 10
> 
>     $init.it <http://init.it>
>     [1] NA
> 
>     $estim.prec
>     [1] 6.103516e-05
> 
>     >
> 
>     Is the lack of trace a bug, or at least an oversight? Being able to follow iterations is a
>     classic approach to checking that computations are proceeding as they should.
> 
>     Best, JN
> 
>     ______________________________________________
>     R-devel at r-project.org <mailto:R-devel at r-project.org> mailing list
>     https://stat.ethz.ch/mailman/listinfo/r-devel <https://stat.ethz.ch/mailman/listinfo/r-devel>
> 
>


From h@wickh@m @ending from gm@il@com  Tue Aug 14 01:58:41 2018
From: h@wickh@m @ending from gm@il@com (Hadley Wickham)
Date: Mon, 13 Aug 2018 18:58:41 -0500
Subject: [Rd] substitute() on arguments in ellipsis ("dot dot dot")?
In-Reply-To: <CAFDcVCT2undj5oQHW4-wiuU2-F5jx9c6-z817rth08Bz3ycvGQ@mail.gmail.com>
References: <CAFDcVCQppvvnXSgnO0CeG8Hpr7JWxFUMTnO7ORjK2JkUFKKWAw@mail.gmail.com>
 <601ad637-43cd-63e7-81b7-7b350bff5624@gmail.com>
 <CAJoaRhZDpA4PaaBRdoTCmpNSD4kjaN7+OVKwycW5Fue72R0-Pg@mail.gmail.com>
 <CAFDcVCT2undj5oQHW4-wiuU2-F5jx9c6-z817rth08Bz3ycvGQ@mail.gmail.com>
Message-ID: <CABdHhvFeeAds1kYqw+8MZXUjfArKzFAOpFVOAeuDt1pFik0sdA@mail.gmail.com>

Since you're already using bang-bang ;)

library(rlang)

dots1 <- function(...) as.list(substitute(list(...)))[-1L]
dots2 <- function(...) as.list(substitute(...()))
dots3 <- function(...) match.call(expand.dots = FALSE)[["..."]]
dots4 <- function(...) exprs(...)

bench::mark(
  dots1(1+2, "a", rnorm(3), stop("bang!")),
  dots2(1+2, "a", rnorm(3), stop("bang!")),
  dots3(1+2, "a", rnorm(3), stop("bang!")),
  dots4(1+2, "a", rnorm(3), stop("bang!")),
  check = FALSE
)[1:4]
#> # A tibble: 4 x 4
#>   expression                                          min     mean  median
#>   <chr>                                          <bch:tm> <bch:tm> <bch:t>
#> 1 "dots1(1 + 2, \"a\", rnorm(3), stop(\"bang!\"?   3.23?s   4.15?s  3.81?s
#> 2 "dots2(1 + 2, \"a\", rnorm(3), stop(\"bang!\"?   2.72?s   4.48?s  3.37?s
#> 3 "dots3(1 + 2, \"a\", rnorm(3), stop(\"bang!\"?   4.06?s   4.94?s  4.69?s
#> 4 "dots4(1 + 2, \"a\", rnorm(3), stop(\"bang!\"?   3.92?s    4.9?s  4.46?s


On Mon, Aug 13, 2018 at 4:19 AM Henrik Bengtsson
<henrik.bengtsson at gmail.com> wrote:
>
> Thanks all, this was very helpful.  Peter's finding - dots2() below -
> is indeed interesting - I'd be curious to learn what goes on there.
>
> The different alternatives perform approximately the same;
>
> dots1 <- function(...) as.list(substitute(list(...)))[-1L]
> dots2 <- function(...) as.list(substitute(...()))
> dots3 <- function(...) match.call(expand.dots = FALSE)[["..."]]
>
> stats <- microbenchmark::microbenchmark(
>   dots1(1+2, "a", rnorm(3), stop("bang!")),
>   dots2(1+2, "a", rnorm(3), stop("bang!")),
>   dots3(1+2, "a", rnorm(3), stop("bang!")),
>   times = 10e3
> )
> print(stats)
> # Unit: microseconds
> #                                        expr  min   lq mean median
> uq  max neval
> #  dots1(1 + 2, "a", rnorm(3), stop("bang!")) 2.14 2.45 3.04   2.58
> 2.73 1110 10000
> #  dots2(1 + 2, "a", rnorm(3), stop("bang!")) 1.81 2.10 2.47   2.21
> 2.34 1626 10000
> #  dots3(1 + 2, "a", rnorm(3), stop("bang!")) 2.59 2.98 3.36   3.15
> 3.31 1037 10000
>
> /Henrik
>
> On Mon, Aug 13, 2018 at 7:10 AM Peter Meilstrup
> <peter.meilstrup at gmail.com> wrote:
> >
> > Interestingly,
> >
> >    as.list(substitute(...()))
> >
> > also works.
> >
> > On Sun, Aug 12, 2018 at 1:16 PM, Duncan Murdoch
> > <murdoch.duncan at gmail.com> wrote:
> > > On 12/08/2018 4:00 PM, Henrik Bengtsson wrote:
> > >>
> > >> Hi. For any number of *known* arguments, we can do:
> > >>
> > >> one <- function(a) list(a = substitute(a))
> > >> two <- function(a, b) list(a = substitute(a), b = substitute(b))
> > >>
> > >> and so on. But how do I achieve the same when I have:
> > >>
> > >> dots <- function(...) list(???)
> > >>
> > >> I want to implement this such that I can do:
> > >>
> > >>> exprs <- dots(1+2)
> > >>> str(exprs)
> > >>
> > >> List of 1
> > >>   $ : language 1 + 2
> > >>
> > >> as well as:
> > >>
> > >>> exprs <- dots(1+2, "a", rnorm(3))
> > >>> str(exprs)
> > >>
> > >> List of 3
> > >>   $ : language 1 + 2
> > >>   $ : chr "a"
> > >>   $ : language rnorm(3)
> > >>
> > >> Is this possible to achieve using plain R code?
> > >
> > >
> > > I think so.  substitute(list(...)) gives you a single expression containing
> > > a call to list() with the unevaluated arguments; you can convert that to
> > > what you want using something like
> > >
> > > dots <- function (...) {
> > >   exprs <- substitute(list(...))
> > >   as.list(exprs[-1])
> > > }
> > >
> > > Duncan Murdoch
> > >
> > >
> > > ______________________________________________
> > > R-devel at r-project.org mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-devel
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel



-- 
http://hadley.nz


From wdunl@p @ending from tibco@com  Tue Aug 14 02:14:44 2018
From: wdunl@p @ending from tibco@com (William Dunlap)
Date: Mon, 13 Aug 2018 17:14:44 -0700
Subject: [Rd] trace in uniroot() ?
In-Reply-To: <3babc515-ed8c-e990-3ede-0e80e845ebba@gmail.com>
References: <941b7615-6075-6c74-537e-70ad0bc16652@gmail.com>
 <CAF8bMcZBbNF74F-x-7G0gRW9Q6FA5GbGnouMMw9FD_FRefdXHQ@mail.gmail.com>
 <3babc515-ed8c-e990-3ede-0e80e845ebba@gmail.com>
Message-ID: <CAF8bMcYMTqrhHU=X26HC_E2J2KkwHzFvDQ-kjuwk7U23O358Zw@mail.gmail.com>

To record the value of the function as well as the arguments, you can use
the following

instrumentObjectiveFunction <- function(FUN) {
    newFUN <- local({
        INFO <- list()
        function(...) {
            value <- FUN(...)
            INFO[[length(INFO)+1]] <<- list(args=list(...), value=value)
            value
        }
    })
    newFUN
}

E.g.,
> untrace(ff)
> ff0 <- uniroot(instrumentedFF <- instrumentObjectiveFunction(ff), c(0,
10))
> str(environment(instrumentedFF)$INFO)
List of 13
 $ :List of 2
  ..$ args :List of 1
  .. ..$ : num 0
  ..$ value: num -1
 $ :List of 2
  ..$ args :List of 1
  .. ..$ : num 10
  ..$ value: num 146
 $ :List of 2
  ..$ args :List of 1
  .. ..$ : num 0.0678
  ..$ value: num -0.965
 $ :List of 2
  ..$ args :List of 1
  .. ..$ : num 5.03
  ..$ value: num 10.4
 $ :List of 2
  ..$ args :List of 1
  .. ..$ : num 0.49
  ..$ value: num -0.722
...


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Mon, Aug 13, 2018 at 3:44 PM, J C Nash <profjcnash at gmail.com> wrote:

> Despite my years with R, I didn't know about trace(). Thanks.
>
> However, my decades in the minimization and root finding game make me like
> having
> a trace that gives some info on the operation, the argument and the
> current function value.
> I've usually found glitches are a result of things like >= rather than >
> in tests etc., and
> knowing what was done is the quickest way to get there.
>
> This is, of course, the numerical software developer view. I know "users"
> (a far too vague
> term) don't like such output. I've sometimes been tempted with my svd or
> optimization codes to
> have a return message in bold-caps "YOUR ANSWER IS WRONG AND THERE'S A
> LAWYER WAITING TO
> MAKE YOU PAY", but I usually just satisfy myself with "Not at a
> minimum/root".
>
> Best, JN
>
> On 2018-08-13 06:00 PM, William Dunlap wrote:
> > I tend to avoid the the trace/verbose arguments for the various root
> finders and optimizers and instead use the trace
> > function or otherwise modify the function handed to the operator.  You
> can print or plot the arguments or save them.  E.g.,
> >
> >> trace(ff, print=FALSE, quote(cat("x=", deparse(x), "\n", sep="")))
> > [1] "ff"
> >> ff0 <- uniroot(ff, c(0, 10))
> > x=0
> > x=10
> > x=0.0678365490630423
> > x=5.03391827453152
> > x=0.490045026724842
> > x=2.76198165062818
> > x=1.09760394309444
> > x=1.92979279686131
> > x=1.34802524899502
> > x=1.38677998493585
> > x=1.3862897003949
> > x=1.38635073555115
> > x=1.3862897003949
> >
> > or
> >
> >> X <- numeric()
> >> trace(ff, print=FALSE, quote(X[[length(X)+1]] <<- x))
> > [1] "ff"
> >> ff0 <- uniroot(ff, c(0, 10))
> >> X
> >  [1]  0.00000000 10.00000000  0.06783655
> >  [4]  5.03391827  0.49004503  2.76198165
> >  [7]  1.09760394  1.92979280  1.34802525
> > [10]  1.38677998  1.38628970  1.38635074
> > [13]  1.38628970
> >
> > This will not tell you why the objective function is being called (e.g.
> in a line search
> > or in derivative estimation), but some plotting or other postprocessing
> can ususally figure that out.
> >
> >
> > Bill Dunlap
> > TIBCO Software
> > wdunlap tibco.com <http://tibco.com>
> >
> > On Mon, Jul 30, 2018 at 11:35 AM, J C Nash <profjcnash at gmail.com
> <mailto:profjcnash at gmail.com>> wrote:
> >
> >     In looking at rootfinding for the histoRicalg project (see
> gitlab.com/nashjc/histoRicalg
> >     <http://gitlab.com/nashjc/histoRicalg>),
> >     I thought I would check how uniroot() solves some problems. The
> following short example
> >
> >     ff <- function(x){ exp(0.5*x) - 2 }
> >     ff(2)
> >     ff(1)
> >     uniroot(ff, 0, 10)
> >     uniroot(ff, c(0, 10), trace=1)
> >     uniroot(ff, c(0, 10), trace=TRUE)
> >
> >
> >     shows that the trace parameter, as described in the Rd file, does
> not seem to
> >     be functional except in limited situations (and it suggests an
> >     integer, then uses a logical for the example, e.g.,
> >      ## numerically, f(-|M|) becomes zero :
> >          u3 <- uniroot(exp, c(0,2), extendInt="yes", trace=TRUE)
> >     )
> >
> >     When extendInt is set, then there is some information output, but
> trace alone
> >     produces nothing.
> >
> >     I looked at the source code -- it is in R-3.5.1/src/library/stats/R/nlm.R
> and
> >     calls zeroin2 code from R-3.5.1/src/library/stats/src/optimize.c as
> far as I
> >     can determing. My code inspection suggests trace does not show the
> iterations
> >     of the rootfinding, and only has effect when the search interval is
> allowed
> >     to be extended. It does not appear that there is any mechanism to ask
> >     the zeroin2 C code to display intermediate work.
> >
> >     This isn't desperately important for me as I wrote an R version of
> the code in
> >     package rootoned on R-forge (which Martin Maechler adapted as
> unirootR.R in
> >     Rmpfr so multi-precision roots can be found). My zeroin.R has
> 'trace' to get
> >     the pattern of different steps. In fact it is a bit excessive. Note
> >     unirootR.R uses 'verbose' rather than 'trace'. However, it would be
> nice to be
> >     able to see what is going on with uniroot() to verify equivalent
> operation at
> >     the same precision level. It is very easy for codes to be very
> slightly
> >     different and give quite widely different output.
> >
> >     Indeed, even without the trace, we see (zeroin from rootoned here)
> >
> >     > zeroin(ff, c(0, 10), trace=FALSE)
> >     $root
> >     [1] 1.386294
> >
> >     $froot
> >     [1] -5.658169e-10
> >
> >     $rtol
> >     [1] 7.450581e-09
> >
> >     $maxit
> >     [1] 9
> >
> >     > uniroot(ff, c(0, 10), trace=FALSE)
> >     $root
> >     [1] 1.38629
> >
> >     $f.root
> >     [1] -4.66072e-06
> >
> >     $iter
> >     [1] 10
> >
> >     $init.it <http://init.it>
> >     [1] NA
> >
> >     $estim.prec
> >     [1] 6.103516e-05
> >
> >     >
> >
> >     Is the lack of trace a bug, or at least an oversight? Being able to
> follow iterations is a
> >     classic approach to checking that computations are proceeding as
> they should.
> >
> >     Best, JN
> >
> >     ______________________________________________
> >     R-devel at r-project.org <mailto:R-devel at r-project.org> mailing list
> >     https://stat.ethz.ch/mailman/listinfo/r-devel <
> https://stat.ethz.ch/mailman/listinfo/r-devel>
> >
> >
>

	[[alternative HTML version deleted]]


From hugh@p@r@on@ge @ending from gm@il@com  Tue Aug 14 04:52:03 2018
From: hugh@p@r@on@ge @ending from gm@il@com (Hugh Parsonage)
Date: Tue, 14 Aug 2018 12:52:03 +1000
Subject: [Rd] premature use of startsWith in r75110
Message-ID: <CAJmOi+MOG1PdzF+9jhv-yESG0zjSY7zTrQD=6T=wUh-AcmayQQ@mail.gmail.com>

In r75110 at line 1846 in src/library/tools/R/check.R the following
line was changed

- if(length(grep("^Found the defunct/removed function", out8)))
+ if(any(startsWith(out8, "Found the defunct/removed function")))

However, if `out8` is NULL at this point (which is plausible), the
original returns FALSE, but the second returns an error. Recommend

+ if(!is.null(out8) && any(startsWith(out8, "Found the defunct/removed
function")))




Best,

HP


From Kurt@Hornik @ending from wu@@c@@t  Tue Aug 14 06:55:53 2018
From: Kurt@Hornik @ending from wu@@c@@t (Kurt Hornik)
Date: Tue, 14 Aug 2018 06:55:53 +0200
Subject: [Rd] premature use of startsWith in r75110
In-Reply-To: <CAJmOi+MOG1PdzF+9jhv-yESG0zjSY7zTrQD=6T=wUh-AcmayQQ@mail.gmail.com>
References: <CAJmOi+MOG1PdzF+9jhv-yESG0zjSY7zTrQD=6T=wUh-AcmayQQ@mail.gmail.com>
Message-ID: <23410.24793.112685.591158@hornik.net>

>>>>> Hugh Parsonage writes:

Thanks, will fix.

Best
-k

> In r75110 at line 1846 in src/library/tools/R/check.R the following
> line was changed

> - if(length(grep("^Found the defunct/removed function", out8)))
> + if(any(startsWith(out8, "Found the defunct/removed function")))

> However, if `out8` is NULL at this point (which is plausible), the
> original returns FALSE, but the second returns an error. Recommend

> + if(!is.null(out8) && any(startsWith(out8, "Found the defunct/removed
> function")))




> Best,

> HP

> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From yut@ni@ini @ending from gm@il@com  Tue Aug 14 03:23:18 2018
From: yut@ni@ini @ending from gm@il@com (Hiroaki Yutani)
Date: Tue, 14 Aug 2018 10:23:18 +0900
Subject: [Rd] vector arithmetic
In-Reply-To: <20180813175505.GE23272@SDF.ORG>
References: <20180813175505.GE23272@SDF.ORG>
Message-ID: <CALyqOb8E48Gzo2J9nNU5aRQ+yFAN6YqM3KLyXWmCYWnY2GtzcQ@mail.gmail.com>

I've been wondering this, too! Following the codes in arithmetic.c, I've
finally reached MOD_ITERATE2_CORE macro in src/include/R_ext/Itermacros.h.
Is this the place?

Best

2018?8?14?(?) 2:59 isomorphismes <isomorphisms at sdf.org>:

> I'm looking for where in the source recycling and vector
> multiplication+addition are defined. I see some stuff in
> ~/src/main/arithmetic.c.
>
> Is there anywhere else I should be looking as well?
>
> Cheers
>
> --
> isomorphisms at sdf.org
> SDF Public Access UNIX System - http://sdf.org
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From ligge@ @ending from @t@ti@tik@tu-dortmund@de  Tue Aug 14 15:06:23 2018
From: ligge@ @ending from @t@ti@tik@tu-dortmund@de (Uwe Ligges)
Date: Tue, 14 Aug 2018 15:06:23 +0200
Subject: [Rd] CRAN incoming queue closed from Sep 1 to Sep 9
Message-ID: <39ca09db-2a5c-01cd-f418-1f4116e73460@statistik.tu-dortmund.de>

Dear developers,

the CRAN incoming queue will be closed from Sep 1 to Sep 9. Hence 
package submissions are only possible before and after that period.

Best,
Uwe Ligges
(for the CRAN team)


From Zw@ng @ending from connecticutchildren@@org  Tue Aug 14 17:00:57 2018
From: Zw@ng @ending from connecticutchildren@@org (Wang, Zhu)
Date: Tue, 14 Aug 2018 15:00:57 +0000
Subject: [Rd] R CMD check warnings on Windows
Message-ID: <DABBA49CC8C7AA4B959D2D0FEE496D7273137177@EXINFRCHMB1P.ccmc.local>

Hi all,

For the R package bujar, the warnings below were generated on CRAN's Windows systems. The package uses some Fortran subroutines. I would appreciate any advice to eliminate the warnings. By the way, similar warnings were generated to some unrelated R packages as well: https://www.r-project.org/nosvn/R.check/r-oldrel-windows-ix86+x86_64/imputeTS-00check.html.

Thanks in advance.

Zhu Wang

https://www.r-project.org/nosvn/R.check/r-devel-windows-ix86+x86_64/bujar-00check.html


* installing *source* package 'bujar' ...
** package 'bujar' successfully unpacked and MD5 sums checked
** R
** data
** inst
** byte-compile and prepare package for lazy loading
Warning: S3 methods '[.fun_list', '[.grouped_df', 'all.equal.tbl_df', 'anti_join.data.frame', 'anti_join.tbl_df', 'arrange.data.frame', 'arrange.default', 'arrange.grouped_df', 'arrange.tbl_df', 'arrange_.data.frame', 'arrange_.tbl_df', 'as.data.frame.grouped_df', 'as.data.frame.rowwise_df', 'as.data.frame.tbl_cube', 'as.data.frame.tbl_df', 'as.table.tbl_cube', 'as.tbl.data.frame', 'as.tbl.tbl', 'as.tbl_cube.array', 'as.tbl_cube.data.frame', 'as.tbl_cube.matrix', 'as.tbl_cube.table', 'as_data_frame.grouped_df', 'as_data_frame.tbl_cube', 'auto_copy.tbl_cube', 'auto_copy.tbl_df', 'cbind.grouped_df', 'collapse.data.frame', 'collect.data.frame', 'common_by.NULL', 'common_by.character', 'common_by.default', 'common_by.list', 'compute.data.frame', 'copy_to.DBIConnection', 'copy_to.src_local', 'default_missing.data.frame', 'default_missing.default', 'dim.tbl_cube', 'distinct.data.frame', 'distinct.default', 'distinct.grouped_df', 'distinct.tbl_df', 'distinct_.data.frame', 'distinct_.grouped_df', 'dist [... truncated]
** help
*** installing help indices
** building package indices
** installing vignettes
** testing if installed package can be loaded
*** arch - i386
Warning: S3 methods '[.fun_list', '[.grouped_df', 'all.equal.tbl_df', 'anti_join.data.frame', 'anti_join.tbl_df', 'arrange.data.frame', 'arrange.default', 'arrange.grouped_df', 'arrange.tbl_df', 'arrange_.data.frame', 'arrange_.tbl_df', 'as.data.frame.grouped_df', 'as.data.frame.rowwise_df', 'as.data.frame.tbl_cube', 'as.data.frame.tbl_df', 'as.table.tbl_cube', 'as.tbl.data.frame', 'as.tbl.tbl', 'as.tbl_cube.array', 'as.tbl_cube.data.frame', 'as.tbl_cube.matrix', 'as.tbl_cube.table', 'as_data_frame.grouped_df', 'as_data_frame.tbl_cube', 'auto_copy.tbl_cube', 'auto_copy.tbl_df', 'cbind.grouped_df', 'collapse.data.frame', 'collect.data.frame', 'common_by.NULL', 'common_by.character', 'common_by.default', 'common_by.list', 'compute.data.frame', 'copy_to.DBIConnection', 'copy_to.src_local', 'default_missing.data.frame', 'default_missing.default', 'dim.tbl_cube', 'distinct.data.frame', 'distinct.default', 'distinct.grouped_df', 'distinct.tbl_df', 'distinct_.data.frame', 'distinct_.grouped_df', 'dist [... truncated]
*** arch - x64
Warning: S3 methods '[.fun_list', '[.grouped_df', 'all.equal.tbl_df', 'anti_join.data.frame', 'anti_join.tbl_df', 'arrange.data.frame', 'arrange.default', 'arrange.grouped_df', 'arrange.tbl_df', 'arrange_.data.frame', 'arrange_.tbl_df', 'as.data.frame.grouped_df', 'as.data.frame.rowwise_df', 'as.data.frame.tbl_cube', 'as.data.frame.tbl_df', 'as.table.tbl_cube', 'as.tbl.data.frame', 'as.tbl.tbl', 'as.tbl_cube.array', 'as.tbl_cube.data.frame', 'as.tbl_cube.matrix', 'as.tbl_cube.table', 'as_data_frame.grouped_df', 'as_data_frame.tbl_cube', 'auto_copy.tbl_cube', 'auto_copy.tbl_df', 'cbind.grouped_df', 'collapse.data.frame', 'collect.data.frame', 'common_by.NULL', 'common_by.character', 'common_by.default', 'common_by.list', 'compute.data.frame', 'copy_to.DBIConnection', 'copy_to.src_local', 'default_missing.data.frame', 'default_missing.default', 'dim.tbl_cube', 'distinct.data.frame', 'distinct.default', 'distinct.grouped_df', 'distinct.tbl_df', 'distinct_.data.frame', 'distinct_.grouped_df', 'dist [... truncated]
* MD5 sums
packaged installation of 'bujar' as bujar_0.2-3.zip
* DONE (bujar)
In R CMD INSTALL


**Connecticut Children's Confidentiality Notice**
This e-mail message, including any attachments, is for t...{{dropped:11}}


From pd@lgd @ending from gm@il@com  Tue Aug 14 17:31:26 2018
From: pd@lgd @ending from gm@il@com (peter dalgaard)
Date: Tue, 14 Aug 2018 17:31:26 +0200
Subject: [Rd] R CMD check warnings on Windows
In-Reply-To: <DABBA49CC8C7AA4B959D2D0FEE496D7273137177@EXINFRCHMB1P.ccmc.local>
References: <DABBA49CC8C7AA4B959D2D0FEE496D7273137177@EXINFRCHMB1P.ccmc.local>
Message-ID: <47A3D63B-6B09-4C51-8F20-6364AD3DEA24@gmail.com>

I don't think this has anything to do with Fortran.

The full warning message is presumably like

Warning: S3 methods . . . were declared in NAMESPACE but not found

which according to google-fu happens if you are declaring methods from a package that isn't loaded, in this case possibly dplyr. 

On previous occasions, this has happened due to race conditions so that one package is not completely  updated before another is installed, and that could well be the case if you see trouble on just the one platform, so maybe wait and see if the warning goes away. Otherwise, check for missing dependencies.



> On 14 Aug 2018, at 17:00 , Wang, Zhu <Zwang at connecticutchildrens.org> wrote:
> 
> Hi all,
> 
> For the R package bujar, the warnings below were generated on CRAN's Windows systems. The package uses some Fortran subroutines. I would appreciate any advice to eliminate the warnings. By the way, similar warnings were generated to some unrelated R packages as well: https://www.r-project.org/nosvn/R.check/r-oldrel-windows-ix86+x86_64/imputeTS-00check.html.
> 
> Thanks in advance.
> 
> Zhu Wang
> 
> https://www.r-project.org/nosvn/R.check/r-devel-windows-ix86+x86_64/bujar-00check.html
> 
> 
> * installing *source* package 'bujar' ...
> ** package 'bujar' successfully unpacked and MD5 sums checked
> ** R
> ** data
> ** inst
> ** byte-compile and prepare package for lazy loading
> Warning: S3 methods '[.fun_list', '[.grouped_df', 'all.equal.tbl_df', 'anti_join.data.frame', 'anti_join.tbl_df', 'arrange.data.frame', 'arrange.default', 'arrange.grouped_df', 'arrange.tbl_df', 'arrange_.data.frame', 'arrange_.tbl_df', 'as.data.frame.grouped_df', 'as.data.frame.rowwise_df', 'as.data.frame.tbl_cube', 'as.data.frame.tbl_df', 'as.table.tbl_cube', 'as.tbl.data.frame', 'as.tbl.tbl', 'as.tbl_cube.array', 'as.tbl_cube.data.frame', 'as.tbl_cube.matrix', 'as.tbl_cube.table', 'as_data_frame.grouped_df', 'as_data_frame.tbl_cube', 'auto_copy.tbl_cube', 'auto_copy.tbl_df', 'cbind.grouped_df', 'collapse.data.frame', 'collect.data.frame', 'common_by.NULL', 'common_by.character', 'common_by.default', 'common_by.list', 'compute.data.frame', 'copy_to.DBIConnection', 'copy_to.src_local', 'default_missing.data.frame', 'default_missing.default', 'dim.tbl_cube', 'distinct.data.frame', 'distinct.default', 'distinct.grouped_df', 'distinct.tbl_df', 'distinct_.data.frame', 'distinct_.grouped
> _df', 'dist [... truncated]
> ** help
> *** installing help indices
> ** building package indices
> ** installing vignettes
> ** testing if installed package can be loaded
> *** arch - i386
> Warning: S3 methods '[.fun_list', '[.grouped_df', 'all.equal.tbl_df', 'anti_join.data.frame', 'anti_join.tbl_df', 'arrange.data.frame', 'arrange.default', 'arrange.grouped_df', 'arrange.tbl_df', 'arrange_.data.frame', 'arrange_.tbl_df', 'as.data.frame.grouped_df', 'as.data.frame.rowwise_df', 'as.data.frame.tbl_cube', 'as.data.frame.tbl_df', 'as.table.tbl_cube', 'as.tbl.data.frame', 'as.tbl.tbl', 'as.tbl_cube.array', 'as.tbl_cube.data.frame', 'as.tbl_cube.matrix', 'as.tbl_cube.table', 'as_data_frame.grouped_df', 'as_data_frame.tbl_cube', 'auto_copy.tbl_cube', 'auto_copy.tbl_df', 'cbind.grouped_df', 'collapse.data.frame', 'collect.data.frame', 'common_by.NULL', 'common_by.character', 'common_by.default', 'common_by.list', 'compute.data.frame', 'copy_to.DBIConnection', 'copy_to.src_local', 'default_missing.data.frame', 'default_missing.default', 'dim.tbl_cube', 'distinct.data.frame', 'distinct.default', 'distinct.grouped_df', 'distinct.tbl_df', 'distinct_.data.frame', 'distinct_.grouped
> _df', 'dist [... truncated]
> *** arch - x64
> Warning: S3 methods '[.fun_list', '[.grouped_df', 'all.equal.tbl_df', 'anti_join.data.frame', 'anti_join.tbl_df', 'arrange.data.frame', 'arrange.default', 'arrange.grouped_df', 'arrange.tbl_df', 'arrange_.data.frame', 'arrange_.tbl_df', 'as.data.frame.grouped_df', 'as.data.frame.rowwise_df', 'as.data.frame.tbl_cube', 'as.data.frame.tbl_df', 'as.table.tbl_cube', 'as.tbl.data.frame', 'as.tbl.tbl', 'as.tbl_cube.array', 'as.tbl_cube.data.frame', 'as.tbl_cube.matrix', 'as.tbl_cube.table', 'as_data_frame.grouped_df', 'as_data_frame.tbl_cube', 'auto_copy.tbl_cube', 'auto_copy.tbl_df', 'cbind.grouped_df', 'collapse.data.frame', 'collect.data.frame', 'common_by.NULL', 'common_by.character', 'common_by.default', 'common_by.list', 'compute.data.frame', 'copy_to.DBIConnection', 'copy_to.src_local', 'default_missing.data.frame', 'default_missing.default', 'dim.tbl_cube', 'distinct.data.frame', 'distinct.default', 'distinct.grouped_df', 'distinct.tbl_df', 'distinct_.data.frame', 'distinct_.grouped
> _df', 'dist [... truncated]
> * MD5 sums
> packaged installation of 'bujar' as bujar_0.2-3.zip
> * DONE (bujar)
> In R CMD INSTALL
> 
> 
> **Connecticut Children's Confidentiality Notice**
> This e-mail message, including any attachments, is for...{{dropped:23}}


From rpb@rry @ending from @l@@k@@edu  Wed Aug 15 02:56:49 2018
From: rpb@rry @ending from @l@@k@@edu (Ronald Barry)
Date: Tue, 14 Aug 2018 16:56:49 -0800
Subject: [Rd] validspamobject?
Message-ID: <CAOVYLHdqrAKUtZ23DH1d6z5PZqLmbWTLjr=9H0+-TFcCPyzbww@mail.gmail.com>

Greetings,
  My R package has been showing warnings of the form:

`validspamobject()` is deprecated. Use `validate_spam()` directly

None of my code uses the function validspamobject, so it must be a problem
in another package I'm calling, possibly spam or spdep.  Has this problem
occurred with other people?  It doesn't have any deleterious effect, but
it's annoying.  In particular, how do I determine which package is causing
this warning?  Thanks.

Ron B.

	[[alternative HTML version deleted]]


From roy@mendel@@ohn @ending from no@@@gov  Wed Aug 15 02:59:38 2018
From: roy@mendel@@ohn @ending from no@@@gov (Roy Mendelssohn - NOAA Federal)
Date: Tue, 14 Aug 2018 17:59:38 -0700
Subject: [Rd] validspamobject?
In-Reply-To: <CAOVYLHdqrAKUtZ23DH1d6z5PZqLmbWTLjr=9H0+-TFcCPyzbww@mail.gmail.com>
References: <CAOVYLHdqrAKUtZ23DH1d6z5PZqLmbWTLjr=9H0+-TFcCPyzbww@mail.gmail.com>
Message-ID: <1AF5ABCF-DE4E-462F-80AD-D9DB8B751E65@noaa.gov>

A quick google search has a function called `validate_spam()` in the package:

SPAM . SPArse Matrix Package

spam is a collection of functions for sparse matrix algebra.

Are you doing anything with sparse matrices?

HTH,

-Roy

> On Aug 14, 2018, at 5:56 PM, Ronald Barry <rpbarry at alaska.edu> wrote:
> 
> validspamobject()` is deprecated. Use `validate_spam()` directly

**********************
"The contents of this message do not reflect any position of the U.S. Government or NOAA."
**********************
Roy Mendelssohn
Supervisory Operations Research Analyst
NOAA/NMFS
Environmental Research Division
Southwest Fisheries Science Center
***Note new street address***
110 McAllister Way
Santa Cruz, CA 95060
Phone: (831)-420-3666
Fax: (831) 420-3980
e-mail: Roy.Mendelssohn at noaa.gov www: http://www.pfeg.noaa.gov/

"Old age and treachery will overcome youth and skill."
"From those who have been given much, much will be expected" 
"the arc of the moral universe is long, but it bends toward justice" -MLK Jr.


From lukemol@on @ending from y@hoo@com  Wed Aug 15 03:39:01 2018
From: lukemol@on @ending from y@hoo@com (lmo)
Date: Wed, 15 Aug 2018 01:39:01 +0000 (UTC)
Subject: [Rd] substitute() on arguments in ellipsis ("dot dot dot")?
References: <347697200.7639319.1534297141896.ref@mail.yahoo.com>
Message-ID: <347697200.7639319.1534297141896@mail.yahoo.com>

A potential solution, at least in terms of producing the desired output, is the base R function alist:
> alist(1+2, "a", rnorm(3))
[[1]]
1 + 2

[[2]]
[1] "a"

[[3]]
rnorm(3)

> str(alist(1+2, "a", rnorm(3)))
List of 3
?$ : language 1 + 2
?$ : chr "a"
?$ : language rnorm(3)


luke

	[[alternative HTML version deleted]]


From emil@bode @ending from d@n@@kn@w@nl  Wed Aug 15 10:26:48 2018
From: emil@bode @ending from d@n@@kn@w@nl (Emil Bode)
Date: Wed, 15 Aug 2018 08:26:48 +0000
Subject: [Rd] validspamobject?
In-Reply-To: <CAOVYLHdqrAKUtZ23DH1d6z5PZqLmbWTLjr=9H0+-TFcCPyzbww@mail.gmail.com>
References: <CAOVYLHdqrAKUtZ23DH1d6z5PZqLmbWTLjr=9H0+-TFcCPyzbww@mail.gmail.com>
Message-ID: <9FDDFFF4-C6F1-4A4A-8762-9E40C8785D16@dans.knaw.nl>

Hello,

If you want to determine where the warning is generated, I think it's easiest to run R with options(warn=2).
In that case all warnings are converted to errors, and you have more debugging tools, e.g. you can run traceback() to see the calling stack, or use options(error=recover).
Hope you can catch it.


Best regards, 
Emil Bode

is an institute of the Dutch Academy KNAW <http://knaw.nl/nl> and funding organisation NWO <http://www.nwo.nl/>. 

?On 15/08/2018, 02:57, "R-devel on behalf of Ronald Barry" <r-devel-bounces at r-project.org on behalf of rpbarry at alaska.edu> wrote:

    Greetings,
      My R package has been showing warnings of the form:
    
    `validspamobject()` is deprecated. Use `validate_spam()` directly
    
    None of my code uses the function validspamobject, so it must be a problem
    in another package I'm calling, possibly spam or spdep.  Has this problem
    occurred with other people?  It doesn't have any deleterious effect, but
    it's annoying.  In particular, how do I determine which package is causing
    this warning?  Thanks.
    
    Ron B.
    
    	[[alternative HTML version deleted]]
    
    ______________________________________________
    R-devel at r-project.org mailing list
    https://stat.ethz.ch/mailman/listinfo/r-devel
    


From btyner @ending from gm@il@com  Wed Aug 15 13:08:42 2018
From: btyner @ending from gm@il@com (Benjamin Tyner)
Date: Wed, 15 Aug 2018 07:08:42 -0400
Subject: [Rd] longint
Message-ID: <b8cc39c0-2e5b-2157-173b-7341ab687188@gmail.com>

Hi

In my R package, imagine I have a C function defined:

 ?? void myfunc(int *x) {
 ????? // some code
 ?? }

but when I call it, I pass it a pointer to a longint instead of a 
pointer to an int. Could this practice potentially result in a segfault?

Regards
Ben


From murdoch@dunc@n @ending from gm@il@com  Wed Aug 15 13:48:23 2018
From: murdoch@dunc@n @ending from gm@il@com (Duncan Murdoch)
Date: Wed, 15 Aug 2018 07:48:23 -0400
Subject: [Rd] longint
In-Reply-To: <b8cc39c0-2e5b-2157-173b-7341ab687188@gmail.com>
References: <b8cc39c0-2e5b-2157-173b-7341ab687188@gmail.com>
Message-ID: <2f3f2125-2cb1-51b8-a5ec-9986a18e698b@gmail.com>

On 15/08/2018 7:08 AM, Benjamin Tyner wrote:
> Hi
> 
> In my R package, imagine I have a C function defined:
> 
>   ?? void myfunc(int *x) {
>   ????? // some code
>   ?? }
> 
> but when I call it, I pass it a pointer to a longint instead of a
> pointer to an int. Could this practice potentially result in a segfault?

I don't think the passing would cause a segfault, but "some code" might 
be expecting a positive number, and due to the type error you could pass 
in a positive longint and have it interpreted as a negative int.

Duncan Murdoch


From j@me@@f@he@ter @ending from gm@il@com  Wed Aug 15 15:04:59 2018
From: j@me@@f@he@ter @ending from gm@il@com (Jim Hester)
Date: Wed, 15 Aug 2018 09:04:59 -0400
Subject: [Rd] substitute() on arguments in ellipsis ("dot dot dot")?
In-Reply-To: <CABdHhvFeeAds1kYqw+8MZXUjfArKzFAOpFVOAeuDt1pFik0sdA@mail.gmail.com>
References: <CAFDcVCQppvvnXSgnO0CeG8Hpr7JWxFUMTnO7ORjK2JkUFKKWAw@mail.gmail.com>
 <601ad637-43cd-63e7-81b7-7b350bff5624@gmail.com>
 <CAJoaRhZDpA4PaaBRdoTCmpNSD4kjaN7+OVKwycW5Fue72R0-Pg@mail.gmail.com>
 <CAFDcVCT2undj5oQHW4-wiuU2-F5jx9c6-z817rth08Bz3ycvGQ@mail.gmail.com>
 <CABdHhvFeeAds1kYqw+8MZXUjfArKzFAOpFVOAeuDt1pFik0sdA@mail.gmail.com>
Message-ID: <CAD6tx95SP4bXeZeFLcxk9grn8FZpuMU5Vft0b77g8Dd7g3E-dA@mail.gmail.com>

Assuming you are fine with a pairlist instead of a list avoiding the
`as.list()` call for dots2 saves a reasonable amount of time and makes it
clearly the fastest.

library(rlang)

dots1 <- function(...) as.list(substitute(list(...)))[-1L]
dots2 <- function(...) as.list(substitute(...()))
dots2.5 <- function(...) substitute(...())
dots3 <- function(...) match.call(expand.dots = FALSE)[["..."]]
dots4 <- function(...) exprs(...)

bench::mark(
  dots1(1+2, "a", rnorm(3), stop("bang!")),
  dots2(1+2, "a", rnorm(3), stop("bang!")),
  dots2.5(1+2, "a", rnorm(3), stop("bang!")),
  dots3(1+2, "a", rnorm(3), stop("bang!")),
  dots4(1+2, "a", rnorm(3), stop("bang!")),
  check = FALSE
)[1:4]
#> # A tibble: 5 x 4
#>   expression                                         min     mean
 median
#>   <chr>                                         <bch:tm> <bch:tm>
<bch:tm>
#> 1 "dots1(1 + 2, \"a\", rnorm(3), stop(\"bang!\?   2.38?s   5.63?s
 2.89?s
#> 2 "dots2(1 + 2, \"a\", rnorm(3), stop(\"bang!\?   2.07?s    3.1?s
2.6?s
#> 3 "dots2.5(1 + 2, \"a\", rnorm(3), stop(\"bang?    471ns  789.5ns
638ns
#> 4 "dots3(1 + 2, \"a\", rnorm(3), stop(\"bang!\?   3.17?s   4.83?s
 4.22?s
#> 5 "dots4(1 + 2, \"a\", rnorm(3), stop(\"bang!\?   3.16?s   4.43?s
 3.87?s


On Mon, Aug 13, 2018 at 7:59 PM Hadley Wickham <h.wickham at gmail.com> wrote:

> Since you're already using bang-bang ;)
>
> library(rlang)
>
> dots1 <- function(...) as.list(substitute(list(...)))[-1L]
> dots2 <- function(...) as.list(substitute(...()))
> dots3 <- function(...) match.call(expand.dots = FALSE)[["..."]]
> dots4 <- function(...) exprs(...)
>
> bench::mark(
>   dots1(1+2, "a", rnorm(3), stop("bang!")),
>   dots2(1+2, "a", rnorm(3), stop("bang!")),
>   dots3(1+2, "a", rnorm(3), stop("bang!")),
>   dots4(1+2, "a", rnorm(3), stop("bang!")),
>   check = FALSE
> )[1:4]
> #> # A tibble: 4 x 4
> #>   expression                                          min     mean
> median
> #>   <chr>                                          <bch:tm> <bch:tm>
> <bch:t>
> #> 1 "dots1(1 + 2, \"a\", rnorm(3), stop(\"bang!\"?   3.23?s   4.15?s
> 3.81?s
> #> 2 "dots2(1 + 2, \"a\", rnorm(3), stop(\"bang!\"?   2.72?s   4.48?s
> 3.37?s
> #> 3 "dots3(1 + 2, \"a\", rnorm(3), stop(\"bang!\"?   4.06?s   4.94?s
> 4.69?s
> #> 4 "dots4(1 + 2, \"a\", rnorm(3), stop(\"bang!\"?   3.92?s    4.9?s
> 4.46?s
>
>
> On Mon, Aug 13, 2018 at 4:19 AM Henrik Bengtsson
> <henrik.bengtsson at gmail.com> wrote:
> >
> > Thanks all, this was very helpful.  Peter's finding - dots2() below -
> > is indeed interesting - I'd be curious to learn what goes on there.
> >
> > The different alternatives perform approximately the same;
> >
> > dots1 <- function(...) as.list(substitute(list(...)))[-1L]
> > dots2 <- function(...) as.list(substitute(...()))
> > dots3 <- function(...) match.call(expand.dots = FALSE)[["..."]]
> >
> > stats <- microbenchmark::microbenchmark(
> >   dots1(1+2, "a", rnorm(3), stop("bang!")),
> >   dots2(1+2, "a", rnorm(3), stop("bang!")),
> >   dots3(1+2, "a", rnorm(3), stop("bang!")),
> >   times = 10e3
> > )
> > print(stats)
> > # Unit: microseconds
> > #                                        expr  min   lq mean median
> > uq  max neval
> > #  dots1(1 + 2, "a", rnorm(3), stop("bang!")) 2.14 2.45 3.04   2.58
> > 2.73 1110 10000
> > #  dots2(1 + 2, "a", rnorm(3), stop("bang!")) 1.81 2.10 2.47   2.21
> > 2.34 1626 10000
> > #  dots3(1 + 2, "a", rnorm(3), stop("bang!")) 2.59 2.98 3.36   3.15
> > 3.31 1037 10000
> >
> > /Henrik
> >
> > On Mon, Aug 13, 2018 at 7:10 AM Peter Meilstrup
> > <peter.meilstrup at gmail.com> wrote:
> > >
> > > Interestingly,
> > >
> > >    as.list(substitute(...()))
> > >
> > > also works.
> > >
> > > On Sun, Aug 12, 2018 at 1:16 PM, Duncan Murdoch
> > > <murdoch.duncan at gmail.com> wrote:
> > > > On 12/08/2018 4:00 PM, Henrik Bengtsson wrote:
> > > >>
> > > >> Hi. For any number of *known* arguments, we can do:
> > > >>
> > > >> one <- function(a) list(a = substitute(a))
> > > >> two <- function(a, b) list(a = substitute(a), b = substitute(b))
> > > >>
> > > >> and so on. But how do I achieve the same when I have:
> > > >>
> > > >> dots <- function(...) list(???)
> > > >>
> > > >> I want to implement this such that I can do:
> > > >>
> > > >>> exprs <- dots(1+2)
> > > >>> str(exprs)
> > > >>
> > > >> List of 1
> > > >>   $ : language 1 + 2
> > > >>
> > > >> as well as:
> > > >>
> > > >>> exprs <- dots(1+2, "a", rnorm(3))
> > > >>> str(exprs)
> > > >>
> > > >> List of 3
> > > >>   $ : language 1 + 2
> > > >>   $ : chr "a"
> > > >>   $ : language rnorm(3)
> > > >>
> > > >> Is this possible to achieve using plain R code?
> > > >
> > > >
> > > > I think so.  substitute(list(...)) gives you a single expression
> containing
> > > > a call to list() with the unevaluated arguments; you can convert
> that to
> > > > what you want using something like
> > > >
> > > > dots <- function (...) {
> > > >   exprs <- substitute(list(...))
> > > >   as.list(exprs[-1])
> > > > }
> > > >
> > > > Duncan Murdoch
> > > >
> > > >
> > > > ______________________________________________
> > > > R-devel at r-project.org mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-devel
> >
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
>
>
>
> --
> http://hadley.nz
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From ripley @ending from @t@t@@ox@@c@uk  Wed Aug 15 16:05:29 2018
From: ripley @ending from @t@t@@ox@@c@uk (Brian Ripley)
Date: Wed, 15 Aug 2018 15:05:29 +0100
Subject: [Rd] longint
In-Reply-To: <2f3f2125-2cb1-51b8-a5ec-9986a18e698b@gmail.com>
References: <b8cc39c0-2e5b-2157-173b-7341ab687188@gmail.com>
 <2f3f2125-2cb1-51b8-a5ec-9986a18e698b@gmail.com>
Message-ID: <B6423F6B-A085-444B-9119-6B09B9349F78@stats.ox.ac.uk>



> On 15 Aug 2018, at 12:48, Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
> 
>> On 15/08/2018 7:08 AM, Benjamin Tyner wrote:
>> Hi
>> In my R package, imagine I have a C function defined:
>>     void myfunc(int *x) {
>>        // some code
>>     }
>> but when I call it, I pass it a pointer to a longint instead of a
>> pointer to an int. Could this practice potentially result in a segfault?
> 
> I don't think the passing would cause a segfault, but "some code" might be expecting a positive number, and due to the type error you could pass in a positive longint and have it interpreted as a negative int.

Are you thinking only of a little-endian system?  A 32-bit lookup of a pointer to a 64-bit area could read the wrong half and get a completely different value.

> 
> Duncan Murdoch
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From wdunl@p @ending from tibco@com  Wed Aug 15 16:32:36 2018
From: wdunl@p @ending from tibco@com (William Dunlap)
Date: Wed, 15 Aug 2018 07:32:36 -0700
Subject: [Rd] validspamobject?
In-Reply-To: <9FDDFFF4-C6F1-4A4A-8762-9E40C8785D16@dans.knaw.nl>
References: <CAOVYLHdqrAKUtZ23DH1d6z5PZqLmbWTLjr=9H0+-TFcCPyzbww@mail.gmail.com>
 <9FDDFFF4-C6F1-4A4A-8762-9E40C8785D16@dans.knaw.nl>
Message-ID: <CAF8bMcZuBNUdMJCwLFHmYFeckpf5T=yn5hpMKakdsRMrELgQTQ@mail.gmail.com>

That was my first thought (my second was trace(.Deprecated,...)).  However,
the spam authors don't use .Deprecated() or warning() to tell about
deprecated functions.  See spam/R/deprecated.R:

validspamobject <- function( ...) {
#    .Deprecated('validate_spam()')
    message("`validspamobject()` is deprecated. Use `validate_spam()`
directly")
    validate_spam( ...)
    }

spam.getOption <- function(...) {
#    .Deprecated(msg="`spam.getOption( arg)` is deprecated.\n Use
`getOption( spam.arg)` directly")
    message("`spam.getOption( arg)` is deprecated. Use `getOption(
spam.arg)` directly")
    getOption(...)

}



Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Wed, Aug 15, 2018 at 1:26 AM, Emil Bode <emil.bode at dans.knaw.nl> wrote:

> Hello,
>
> If you want to determine where the warning is generated, I think it's
> easiest to run R with options(warn=2).
> In that case all warnings are converted to errors, and you have more
> debugging tools, e.g. you can run traceback() to see the calling stack, or
> use options(error=recover).
> Hope you can catch it.
>
>
> Best regards,
> Emil Bode
>
> is an institute of the Dutch Academy KNAW <http://knaw.nl/nl> and funding
> organisation NWO <http://www.nwo.nl/>.
>
> ?On 15/08/2018, 02:57, "R-devel on behalf of Ronald Barry" <
> r-devel-bounces at r-project.org on behalf of rpbarry at alaska.edu> wrote:
>
>     Greetings,
>       My R package has been showing warnings of the form:
>
>     `validspamobject()` is deprecated. Use `validate_spam()` directly
>
>     None of my code uses the function validspamobject, so it must be a
> problem
>     in another package I'm calling, possibly spam or spdep.  Has this
> problem
>     occurred with other people?  It doesn't have any deleterious effect,
> but
>     it's annoying.  In particular, how do I determine which package is
> causing
>     this warning?  Thanks.
>
>     Ron B.
>
>         [[alternative HTML version deleted]]
>
>     ______________________________________________
>     R-devel at r-project.org mailing list
>     https://stat.ethz.ch/mailman/listinfo/r-devel
>
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From hp@ge@ @ending from fredhutch@org  Wed Aug 15 20:47:20 2018
From: hp@ge@ @ending from fredhutch@org (=?UTF-8?B?SGVydsOpIFBhZ8Oocw==?=)
Date: Wed, 15 Aug 2018 11:47:20 -0700
Subject: [Rd] longint
In-Reply-To: <B6423F6B-A085-444B-9119-6B09B9349F78@stats.ox.ac.uk>
References: <b8cc39c0-2e5b-2157-173b-7341ab687188@gmail.com>
 <2f3f2125-2cb1-51b8-a5ec-9986a18e698b@gmail.com>
 <B6423F6B-A085-444B-9119-6B09B9349F78@stats.ox.ac.uk>
Message-ID: <1608bafb-374d-1953-7694-de44e3ce9007@fredhutch.org>

No segfault but a BIG warning from the compiler. That's because 
dereferencing the pointer inside your myfunc() function will
produce an int that is not predictable i.e. it is system-dependent.
Its value will depend on sizeof(long int) (which is not
guaranteed to be 8) and on the endianness of the system.

Also if the pointer you pass in the call to the function is
an array of long ints, then pointer arithmetic inside your myfunc()
won't necessarily take you to the array element that you'd expect.

Note that there are very specific situations where you can actually
do this kind of things e.g. in the context of writing a callback
function to pass to qsort(). See 'man 3 qsort' if you are on a Unix
system. In that case pointers to void and explicit casts should
be used. If done properly, this is portable code and the compiler won't
issue warnings.

H.


On 08/15/2018 07:05 AM, Brian Ripley wrote:
> 
> 
>> On 15 Aug 2018, at 12:48, Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
>>
>>> On 15/08/2018 7:08 AM, Benjamin Tyner wrote:
>>> Hi
>>> In my R package, imagine I have a C function defined:
>>>      void myfunc(int *x) {
>>>         // some code
>>>      }
>>> but when I call it, I pass it a pointer to a longint instead of a
>>> pointer to an int. Could this practice potentially result in a segfault?
>>
>> I don't think the passing would cause a segfault, but "some code" might be expecting a positive number, and due to the type error you could pass in a positive longint and have it interpreted as a negative int.
> 
> Are you thinking only of a little-endian system?  A 32-bit lookup of a pointer to a 64-bit area could read the wrong half and get a completely different value.
> 
>>
>> Duncan Murdoch
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Ddevel&d=DwIFAg&c=eRAMFD45gAfqt84VtBcfhQ&r=BK7q3XeAvimeWdGbWY_wJYbW0WYiZvSXAJJKaaPhzWA&m=ERck0y30d00Np6hqTNYfjusx1beZim0OrKe9O4vkUxU&s=x1gI9ACZol7WbaWQ7Ocv60csJFJClZotWkJIMwUdjIc&e=
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Ddevel&d=DwIFAg&c=eRAMFD45gAfqt84VtBcfhQ&r=BK7q3XeAvimeWdGbWY_wJYbW0WYiZvSXAJJKaaPhzWA&m=ERck0y30d00Np6hqTNYfjusx1beZim0OrKe9O4vkUxU&s=x1gI9ACZol7WbaWQ7Ocv60csJFJClZotWkJIMwUdjIc&e=
> 

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fredhutch.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From btyner @ending from gm@il@com  Thu Aug 16 02:32:14 2018
From: btyner @ending from gm@il@com (Benjamin Tyner)
Date: Wed, 15 Aug 2018 20:32:14 -0400
Subject: [Rd] longint
In-Reply-To: <1608bafb-374d-1953-7694-de44e3ce9007@fredhutch.org>
References: <b8cc39c0-2e5b-2157-173b-7341ab687188@gmail.com>
 <2f3f2125-2cb1-51b8-a5ec-9986a18e698b@gmail.com>
 <B6423F6B-A085-444B-9119-6B09B9349F78@stats.ox.ac.uk>
 <1608bafb-374d-1953-7694-de44e3ce9007@fredhutch.org>
Message-ID: <7238fd29-6008-c32a-2446-8b6f534b77fd@gmail.com>

Thanks for the replies and for confirming my suspicion.

Interestingly, src/include/S.h uses a trick:

 ?? #define longint int

and so does the nlme package (within src/init.c).

On 08/15/2018 02:47 PM, Herv? Pag?s wrote:
> No segfault but a BIG warning from the compiler. That's because 
> dereferencing the pointer inside your myfunc() function will
> produce an int that is not predictable i.e. it is system-dependent.
> Its value will depend on sizeof(long int) (which is not
> guaranteed to be 8) and on the endianness of the system.
>
> Also if the pointer you pass in the call to the function is
> an array of long ints, then pointer arithmetic inside your myfunc()
> won't necessarily take you to the array element that you'd expect.
>
> Note that there are very specific situations where you can actually
> do this kind of things e.g. in the context of writing a callback
> function to pass to qsort(). See 'man 3 qsort' if you are on a Unix
> system. In that case pointers to void and explicit casts should
> be used. If done properly, this is portable code and the compiler won't
> issue warnings.
>
> H.
>
>
> On 08/15/2018 07:05 AM, Brian Ripley wrote:
>>
>>
>>> On 15 Aug 2018, at 12:48, Duncan Murdoch <murdoch.duncan at gmail.com> 
>>> wrote:
>>>
>>>> On 15/08/2018 7:08 AM, Benjamin Tyner wrote:
>>>> Hi
>>>> In my R package, imagine I have a C function defined:
>>>> ???? void myfunc(int *x) {
>>>> ??????? // some code
>>>> ???? }
>>>> but when I call it, I pass it a pointer to a longint instead of a
>>>> pointer to an int. Could this practice potentially result in a 
>>>> segfault?
>>>
>>> I don't think the passing would cause a segfault, but "some code" 
>>> might be expecting a positive number, and due to the type error you 
>>> could pass in a positive longint and have it interpreted as a 
>>> negative int.
>>
>> Are you thinking only of a little-endian system?? A 32-bit lookup of 
>> a pointer to a 64-bit area could read the wrong half and get a 
>> completely different value.
>>
>>>
>>> Duncan Murdoch
>>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Ddevel&d=DwIFAg&c=eRAMFD45gAfqt84VtBcfhQ&r=BK7q3XeAvimeWdGbWY_wJYbW0WYiZvSXAJJKaaPhzWA&m=ERck0y30d00Np6hqTNYfjusx1beZim0OrKe9O4vkUxU&s=x1gI9ACZol7WbaWQ7Ocv60csJFJClZotWkJIMwUdjIc&e= 
>>>
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Ddevel&d=DwIFAg&c=eRAMFD45gAfqt84VtBcfhQ&r=BK7q3XeAvimeWdGbWY_wJYbW0WYiZvSXAJJKaaPhzWA&m=ERck0y30d00Np6hqTNYfjusx1beZim0OrKe9O4vkUxU&s=x1gI9ACZol7WbaWQ7Ocv60csJFJClZotWkJIMwUdjIc&e= 
>>
>>
>


From wdunl@p @ending from tibco@com  Thu Aug 16 04:06:21 2018
From: wdunl@p @ending from tibco@com (William Dunlap)
Date: Wed, 15 Aug 2018 19:06:21 -0700
Subject: [Rd] longint
In-Reply-To: <7238fd29-6008-c32a-2446-8b6f534b77fd@gmail.com>
References: <b8cc39c0-2e5b-2157-173b-7341ab687188@gmail.com>
 <2f3f2125-2cb1-51b8-a5ec-9986a18e698b@gmail.com>
 <B6423F6B-A085-444B-9119-6B09B9349F78@stats.ox.ac.uk>
 <1608bafb-374d-1953-7694-de44e3ce9007@fredhutch.org>
 <7238fd29-6008-c32a-2446-8b6f534b77fd@gmail.com>
Message-ID: <CAF8bMcaS36b8jm-GAu0uLRnFXfgXCZo3Sc-fC+iwDt13OfcNcA@mail.gmail.com>

Note that include/S.h contains
  /*
     This is a legacy header and no longer documented.
     Code using it should be converted to use R.h
  */
  ...
  /* is this a good idea? - conflicts with many versions of f2c.h */
  # define longint int

S.h was meant to be used while converting to R C code written for S or S+.
S/S+ "integers" are represented as C "long ints", whose size depends on
the architecture, while R "integers" are represented as 32-bit C "ints".
"longint" was invented to hide this difference.


Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Wed, Aug 15, 2018 at 5:32 PM, Benjamin Tyner <btyner at gmail.com> wrote:

> Thanks for the replies and for confirming my suspicion.
>
> Interestingly, src/include/S.h uses a trick:
>
>    #define longint int
>
> and so does the nlme package (within src/init.c).
>
> On 08/15/2018 02:47 PM, Herv? Pag?s wrote:
>
>> No segfault but a BIG warning from the compiler. That's because
>> dereferencing the pointer inside your myfunc() function will
>> produce an int that is not predictable i.e. it is system-dependent.
>> Its value will depend on sizeof(long int) (which is not
>> guaranteed to be 8) and on the endianness of the system.
>>
>> Also if the pointer you pass in the call to the function is
>> an array of long ints, then pointer arithmetic inside your myfunc()
>> won't necessarily take you to the array element that you'd expect.
>>
>> Note that there are very specific situations where you can actually
>> do this kind of things e.g. in the context of writing a callback
>> function to pass to qsort(). See 'man 3 qsort' if you are on a Unix
>> system. In that case pointers to void and explicit casts should
>> be used. If done properly, this is portable code and the compiler won't
>> issue warnings.
>>
>> H.
>>
>>
>> On 08/15/2018 07:05 AM, Brian Ripley wrote:
>>
>>>
>>>
>>> On 15 Aug 2018, at 12:48, Duncan Murdoch <murdoch.duncan at gmail.com>
>>>> wrote:
>>>>
>>>> On 15/08/2018 7:08 AM, Benjamin Tyner wrote:
>>>>> Hi
>>>>> In my R package, imagine I have a C function defined:
>>>>>      void myfunc(int *x) {
>>>>>         // some code
>>>>>      }
>>>>> but when I call it, I pass it a pointer to a longint instead of a
>>>>> pointer to an int. Could this practice potentially result in a
>>>>> segfault?
>>>>>
>>>>
>>>> I don't think the passing would cause a segfault, but "some code" might
>>>> be expecting a positive number, and due to the type error you could pass in
>>>> a positive longint and have it interpreted as a negative int.
>>>>
>>>
>>> Are you thinking only of a little-endian system?  A 32-bit lookup of a
>>> pointer to a 64-bit area could read the wrong half and get a completely
>>> different value.
>>>
>>>
>>>> Duncan Murdoch
>>>>
>>>> ______________________________________________
>>>> R-devel at r-project.org mailing list
>>>> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.et
>>>> hz.ch_mailman_listinfo_r-2Ddevel&d=DwIFAg&c=eRAMFD45gAfqt84V
>>>> tBcfhQ&r=BK7q3XeAvimeWdGbWY_wJYbW0WYiZvSXAJJKaaPhzWA&m=ERck0
>>>> y30d00Np6hqTNYfjusx1beZim0OrKe9O4vkUxU&s=x1gI9ACZol7WbaWQ7Oc
>>>> v60csJFJClZotWkJIMwUdjIc&e=
>>>>
>>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.et
>>> hz.ch_mailman_listinfo_r-2Ddevel&d=DwIFAg&c=eRAMFD45gAfqt84V
>>> tBcfhQ&r=BK7q3XeAvimeWdGbWY_wJYbW0WYiZvSXAJJKaaPhzWA&m=ERck0
>>> y30d00Np6hqTNYfjusx1beZim0OrKe9O4vkUxU&s=x1gI9ACZol7WbaWQ7Oc
>>> v60csJFJClZotWkJIMwUdjIc&e=
>>>
>>>
>>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From tom@@@k@liber@ @ending from gm@il@com  Thu Aug 16 10:06:55 2018
From: tom@@@k@liber@ @ending from gm@il@com (Tomas Kalibera)
Date: Thu, 16 Aug 2018 10:06:55 +0200
Subject: [Rd] Problem with parseData
In-Reply-To: <fda485ef-3ca1-6507-f227-51a5cb8af4a3@mtholyoke.edu>
References: <fda485ef-3ca1-6507-f227-51a5cb8af4a3@mtholyoke.edu>
Message-ID: <d4cdfe7e-10c1-eceb-b0c4-88c2ebfb000f@gmail.com>

Dear Barbara,

thank you for the report. This is something to be fixed in R - I am now 
testing a patch that adds the extra node for the equality assignment 
expression.

Best,
Tomas

On 07/30/2018 05:35 PM, Barbara Lerner wrote:
> Hi,
>
> I have run into a problem with parseData from the utils package.? When
> an assignment is done with = instead of <-, the information provided by
> parseData does not include an entry for the assignment.
>
> For this input, stored in file "BadPosition.R":
>
> y <- 5
> foo = 7
>
> And running this code:
>
> parsed <- parse("BadPosition.R", keep.source=TRUE)
> parsedData <- utils::getParseData (parsed, includeText=TRUE)
> print(paste("parseData =", parsedData))
>
> I get the following output:
>
> [1] "parseData = c(1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2)"
> [2] "parseData = c(1, 1, 1, 3, 6, 6, 1, 1, 5, 7, 7)"
> [3] "parseData = c(1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2)"
> [4] "parseData = c(6, 1, 1, 4, 6, 6, 3, 3, 5, 7, 7)"
> [5] "parseData = c(7, 1, 3, 2, 4, 5, 10, 12, 11, 13, 14)"
> [6] "parseData = c(0, 3, 7, 7, 5, 7, 12, 0, 0, 14, 0)"
> [7] "parseData = c(\"expr\", \"SYMBOL\", \"expr\", \"LEFT_ASSIGN\",
> \"NUM_CONST\", \"expr\", \"SYMBOL\", \"expr\", \"EQ_ASSIGN\",
> \"NUM_CONST\", \"expr\")"
> [8] "parseData = c(FALSE, TRUE, FALSE, TRUE, TRUE, FALSE, TRUE, FALSE,
> TRUE, TRUE, FALSE)"
> [9] "parseData = c(\"y <- 5\", \"y\", \"y\", \"<-\", \"5\", \"5\",
> \"foo\", \"foo\", \"=\", \"7\", \"7\")"
>
> Notice how there is an entry for "y <- 5" beginning on line 1, column 1,
> ending at line 1, column 6, but there is no analogous entry for "foo = 7".
>
> I am running R 3.5.0 on a Mac running 10.12.6.
>
> Thanks for your help and please let me know if you need any further
> information.
>
> Barbara
>


From edd @ending from debi@n@org  Thu Aug 16 14:12:22 2018
From: edd @ending from debi@n@org (Dirk Eddelbuettel)
Date: Thu, 16 Aug 2018 07:12:22 -0500
Subject: [Rd] longint
In-Reply-To: <7238fd29-6008-c32a-2446-8b6f534b77fd@gmail.com>
References: <b8cc39c0-2e5b-2157-173b-7341ab687188@gmail.com>
 <2f3f2125-2cb1-51b8-a5ec-9986a18e698b@gmail.com>
 <B6423F6B-A085-444B-9119-6B09B9349F78@stats.ox.ac.uk>
 <1608bafb-374d-1953-7694-de44e3ce9007@fredhutch.org>
 <7238fd29-6008-c32a-2446-8b6f534b77fd@gmail.com>
Message-ID: <23413.27174.647572.685196@rob.eddelbuettel.com>


On 15 August 2018 at 20:32, Benjamin Tyner wrote:
| Thanks for the replies and for confirming my suspicion.
| 
| Interestingly, src/include/S.h uses a trick:
| 
|  ?? #define longint int
| 
| and so does the nlme package (within src/init.c).

As Bill Dunlap already told you, this is a) ancient and b) was concerned with
the int as 16 bit to 32 bit transition period. Ie a long time ago. Old C
programmers remember.

You should preferably not even use 'long int' on the other side but rely on
the fact that all compiler nowadays allow you to specify exactly what size is
used via int64_t (long), int32_t (int), ... and the unsigned cousins (which R
does not have).  So please receive the value as a int64_t and then cast it to
an int32_t -- which corresponds to R's notion of an integer on every platform.

And please note that that conversion is lossy.  If you must keep 64 bits then
the bit64 package by Jens Oehlschlaegel is good and eg fully supported inside
data.table. We use it for 64-bit integers as nanosecond timestamps in our
nanotime package (which has some converters).

Dirk

-- 
http://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From hp@ge@ @ending from fredhutch@org  Thu Aug 16 19:33:15 2018
From: hp@ge@ @ending from fredhutch@org (=?UTF-8?B?SGVydsOpIFBhZ8Oocw==?=)
Date: Thu, 16 Aug 2018 10:33:15 -0700
Subject: [Rd] longint
In-Reply-To: <23413.27174.647572.685196@rob.eddelbuettel.com>
References: <b8cc39c0-2e5b-2157-173b-7341ab687188@gmail.com>
 <2f3f2125-2cb1-51b8-a5ec-9986a18e698b@gmail.com>
 <B6423F6B-A085-444B-9119-6B09B9349F78@stats.ox.ac.uk>
 <1608bafb-374d-1953-7694-de44e3ce9007@fredhutch.org>
 <7238fd29-6008-c32a-2446-8b6f534b77fd@gmail.com>
 <23413.27174.647572.685196@rob.eddelbuettel.com>
Message-ID: <17f75ffc-d13e-087a-0108-7aeb04d0958e@fredhutch.org>

On 08/16/2018 05:12 AM, Dirk Eddelbuettel wrote:
> 
> On 15 August 2018 at 20:32, Benjamin Tyner wrote:
> | Thanks for the replies and for confirming my suspicion.
> |
> | Interestingly, src/include/S.h uses a trick:
> |
> |  ?? #define longint int
> |
> | and so does the nlme package (within src/init.c).
> 
> As Bill Dunlap already told you, this is a) ancient and b) was concerned with
> the int as 16 bit to 32 bit transition period. Ie a long time ago. Old C
> programmers remember.
> 
> You should preferably not even use 'long int' on the other side but rely on
> the fact that all compiler nowadays allow you to specify exactly what size is
> used via int64_t (long), int32_t (int), ... and the unsigned cousins (which R
> does not have).  So please receive the value as a int64_t and then cast it to
> an int32_t -- which corresponds to R's notion of an integer on every platform.

Only on Intel platforms int is 32 bits. Strictly speaking int is only
required to be >= 16 bits. Who knows what the size of an int is on
the Sunway TaihuLight for example ;-)

H.

> 
> And please note that that conversion is lossy.  If you must keep 64 bits then
> the bit64 package by Jens Oehlschlaegel is good and eg fully supported inside
> data.table. We use it for 64-bit integers as nanosecond timestamps in our
> nanotime package (which has some converters).
> 
> Dirk
> 

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fredhutch.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From ripley @ending from @t@t@@ox@@c@uk  Thu Aug 16 20:30:55 2018
From: ripley @ending from @t@t@@ox@@c@uk (Prof Brian Ripley)
Date: Thu, 16 Aug 2018 19:30:55 +0100
Subject: [Rd] longint
In-Reply-To: <17f75ffc-d13e-087a-0108-7aeb04d0958e@fredhutch.org>
References: <b8cc39c0-2e5b-2157-173b-7341ab687188@gmail.com>
 <2f3f2125-2cb1-51b8-a5ec-9986a18e698b@gmail.com>
 <B6423F6B-A085-444B-9119-6B09B9349F78@stats.ox.ac.uk>
 <1608bafb-374d-1953-7694-de44e3ce9007@fredhutch.org>
 <7238fd29-6008-c32a-2446-8b6f534b77fd@gmail.com>
 <23413.27174.647572.685196@rob.eddelbuettel.com>
 <17f75ffc-d13e-087a-0108-7aeb04d0958e@fredhutch.org>
Message-ID: <3df99277-61b3-58f2-7275-0f502fdf30af@stats.ox.ac.uk>

On 16/08/2018 18:33, Herv? Pag?s wrote:
> On 08/16/2018 05:12 AM, Dirk Eddelbuettel wrote:
>>
>> On 15 August 2018 at 20:32, Benjamin Tyner wrote:
>> | Thanks for the replies and for confirming my suspicion.
>> |
>> | Interestingly, src/include/S.h uses a trick:
>> |
>> |? ?? #define longint int
>> |
>> | and so does the nlme package (within src/init.c).
>>
>> As Bill Dunlap already told you, this is a) ancient and b) was 
>> concerned with
>> the int as 16 bit to 32 bit transition period. Ie a long time ago. Old C
>> programmers remember.
>>
>> You should preferably not even use 'long int' on the other side but 
>> rely on
>> the fact that all compiler nowadays allow you to specify exactly what 
>> size is
>> used via int64_t (long), int32_t (int), ... and the unsigned cousins 

Well, not all compilers.  Those types were introduced in C99, but are 
optional in that standard and in C11 and C++11.  I have not checked 
C++1[47], but expect they are also optional there.  int_fast64_t is not 
optional in C99, so R uses that if int64_t is not supported.

[It is easy to overlook that they are optional in C99 and at one time R 
assumed them.]

>> (which R
>> does not have).? So please receive the value as a int64_t and then 
>> cast it to
>> an int32_t -- which corresponds to R's notion of an integer on every 
>> platform.
> 
> Only on Intel platforms int is 32 bits. Strictly speaking int is only
> required to be >= 16 bits. Who knows what the size of an int is on
> the Sunway TaihuLight for example ;-)

R's configure checks that int is 32 bit and will not compile without it 
(src/main/arithmetic.c) ... so int and int32_t are the same on all 
platforms where the latter is defined.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Emeritus Professor of Applied Statistics, University of Oxford


From hp@ge@ @ending from fredhutch@org  Thu Aug 16 21:03:24 2018
From: hp@ge@ @ending from fredhutch@org (=?UTF-8?B?SGVydsOpIFBhZ8Oocw==?=)
Date: Thu, 16 Aug 2018 12:03:24 -0700
Subject: [Rd] longint
In-Reply-To: <3df99277-61b3-58f2-7275-0f502fdf30af@stats.ox.ac.uk>
References: <b8cc39c0-2e5b-2157-173b-7341ab687188@gmail.com>
 <2f3f2125-2cb1-51b8-a5ec-9986a18e698b@gmail.com>
 <B6423F6B-A085-444B-9119-6B09B9349F78@stats.ox.ac.uk>
 <1608bafb-374d-1953-7694-de44e3ce9007@fredhutch.org>
 <7238fd29-6008-c32a-2446-8b6f534b77fd@gmail.com>
 <23413.27174.647572.685196@rob.eddelbuettel.com>
 <17f75ffc-d13e-087a-0108-7aeb04d0958e@fredhutch.org>
 <3df99277-61b3-58f2-7275-0f502fdf30af@stats.ox.ac.uk>
Message-ID: <30ccd0c7-221c-dc3e-f7a0-62cf8898c6ea@fredhutch.org>

On 08/16/2018 11:30 AM, Prof Brian Ripley wrote:
> On 16/08/2018 18:33, Herv? Pag?s wrote:
...
>>
>> Only on Intel platforms int is 32 bits. Strictly speaking int is only
>> required to be >= 16 bits. Who knows what the size of an int is on
>> the Sunway TaihuLight for example ;-)
> 
> R's configure checks that int is 32 bit and will not compile without it 
> (src/main/arithmetic.c) ... so int and int32_t are the same on all 
> platforms where the latter is defined.

Good to know. Thanks for the clarification!

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fredhutch.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From rpb@rry @ending from @l@@k@@edu  Thu Aug 16 22:00:04 2018
From: rpb@rry @ending from @l@@k@@edu (Ronald Barry)
Date: Thu, 16 Aug 2018 12:00:04 -0800
Subject: [Rd] Thanks for help with validspamobject
Message-ID: <CAOVYLHeicFCPZTtpLrfPN4CTEuKY2fwTKvjgEv-3rpxR_r7TWA@mail.gmail.com>

Hi,
  Thanks for all your help.  The problem with an error involving
validspamobject() has been resolved, as a new version of spdep (0.7-7) was
just released and it seems to have stopped using the deprecated function.

Ron B.

	[[alternative HTML version deleted]]


From gecon@m@inten@nce @ending from gm@il@com  Fri Aug 17 00:43:37 2018
From: gecon@m@inten@nce @ending from gm@il@com (Karol Podemski)
Date: Fri, 17 Aug 2018 00:43:37 +0200
Subject: [Rd] Package compiler - efficiency problem
In-Reply-To: <b0c9736d-8d9c-c3cc-decd-eaf1a2b82b60@gmail.com>
References: <CAC_=cNYEie=yjdDvJ1eKhRYSXSYAFmrgU5aO_baJud-6-xBsOw@mail.gmail.com>
 <b0c9736d-8d9c-c3cc-decd-eaf1a2b82b60@gmail.com>
Message-ID: <CAC_=cNaG59M4q0_j66vFOUctqgJW2T7nmBq3LTRrsuEURkyOGw@mail.gmail.com>

Dear Thomas,

thank you for prompt response and taking interest in this issue. I really
appreciate your compiler project and efficiency gains in usual case. I am
aware of limitations of interpreted languages too and because of that even
when writing my first mail I had a hunch that it is not that easy to
address this problem.  As you mentioned optimisation of compiler for
handling non-standard code may be tricky and harmful for usual code. The
question is if gEcon is the only package that may face the same issue
because of compilation.

The functions generated by gEcon are systems of non-linear equations
defining the equilibrium of an economy (see
http://gecon.r-forge.r-project.org/files/gEcon-users-guide.pdf  if you want
to learn a bit how we obtain it). The rows, you suggested to vectorise, are
indeed vectorisable because they define equilibrium for similiar markets
(e.g. production and sale of beverages and food) but do not have to be
vectorisable in general case. So that not to delve into too much details I
will stop here in description of how the equations originate. However, I
would like to point that similiar large systems of linear equations may
arise in other fields ( https://en.wikipedia.org/wiki/Steady_state ) and
there may be other packages that generate similar large systems (e.g.
network problems like hydraulic networks). In that case, reports such as
mine may help you to assess the scale of the problems.

Thank you for suggestions for improvement in our approach, i am going to
discuss them with other package developers.

Regards,
Karol Podemski

pon., 13 sie 2018 o 18:02 Tomas Kalibera <tomas.kalibera at gmail.com>
napisa?(a):

> Dear Karol,
>
> thank you for the report. I can reproduce that the function from you
> example takes very long to compile and I can see where most time is spent.
> The compiler is itself written in R and requires a lot of resources for
> large functions (foo() has over 16,000 lines of code, nearly 1 million of
> instructions/operands, 45,000 constants). In particular a lot of time is
> spent in garbage collection and in finding a unique set of constants. Some
> optimizations of the compiler may be possible, but it is unlikely that
> functions this large will compile fast any soon. For non-generated code, we
> now have the byte-compilation on installation by default which at least
> removes the compile overhead from runtime. Even though the compiler is
> slow, it is important to keep in mind that in principle, with any compiler
> there will be functions where compilation would not be improve performance
> (when the compile time is included or not).
>
> I think it is not a good idea to generate code for functions like foo() in
> R (or any interpreted language). You say that R's byte-code compiler
> produces code that runs 5-10x faster than when the function is interpreted
> by the AST interpreter (uncompiled), which sounds like a good result, but I
> believe that avoiding code generation would be much faster than that, apart
> from drastically reducing code size and therefore compile time. The
> generator of these functions has much more information than the compiler -
> it could be turned into an interpreter of these functions and compute their
> values on the fly.
>
> A significant source of inefficiency of the generated code are
> element-wise operations, such as
>
> r[12] <- -vv[88] + vv[16] * (1 + ppff[1307])
> ...
>
> r[139] <- -vv[215] + vv[47] * (1 + ppff[1434])
>
> (these could be vectorized, which would reduce code size and improve
> interpretation speed; and make it somewhat readable). Most of the code
> lines in the generated functions seem to be easily vectorizable.
>
> Compilers and interpreters necessarily use some heuristics or optimize at
> some code patterns. Optimizing for generated code may be tricky as it could
> even harm performance of usual code. And, I would much rather optimize the
> compiler for the usual code.
>
> Indeed, a pragmatic solution requiring the least amount of work would be
> to disable compilation of these generated functions. There is not a
> documented way to do that and maybe we could add it (and technically it is
> trivial), but I have been reluctant so far - in some cases, compilation
> even of these functions may be beneficial - if the speedup is 5-10x and we
> run very many times. But once the generated code included some pragma
> preventing compilation, it won't be ever compiled. Also, the trade-offs may
> change as the compiler evolves, perhaps not in this case, but in other
> where such pragma may be used.
>
> Well so the short answer would be that these functions should not be
> generated in the first place. If it were too much work rewriting, perhaps
> the generator could just be improved to produce vectorized operations.
>
> Best
> Tomas
> On 12.8.2018 21:31, Karol Podemski wrote:
>
>  Dear R team,
>
> I am a co-author and maintainer of one of R packages distributed by R-forge
> (gEcon). One of gEcon package users found a strange behaviour of package (R
> froze for couple of minutes) and reported it to me. I traced the strange
> behaviour to compiler package. I attach short demonstration of the problem
> to this mail (demonstration makes use of compiler and tictoc packages only).
>
> In short, the compiler package has problems in compiling large functions -
> their compilation and execution may take much longer than direct execution
> of an uncompiled function. Such functions are generated by gEcon package as
> they describe steady state for economy.
>
> I am curious if you are aware of such problems and plan to handle the
> efficiency issues. On one of the boards I saw that there were efficiency
> issues in rpart package but they have been resolved. Or would you advise to
> turn off JIT on package load (package heavily uses such long functions
> generated whenever a new model is created)?
>
> Best regards,
> Karol Podemski
>
>
>
> ______________________________________________R-devel at r-project.org mailing listhttps://stat.ethz.ch/mailman/listinfo/r-devel
>
>
>

	[[alternative HTML version deleted]]


From iuc@r @ending from fedor@project@org  Fri Aug 17 02:20:50 2018
From: iuc@r @ending from fedor@project@org (=?UTF-8?Q?I=C3=B1aki_Ucar?=)
Date: Fri, 17 Aug 2018 02:20:50 +0200
Subject: [Rd] Package compiler - efficiency problem
In-Reply-To: <CAC_=cNaG59M4q0_j66vFOUctqgJW2T7nmBq3LTRrsuEURkyOGw@mail.gmail.com>
References: <CAC_=cNYEie=yjdDvJ1eKhRYSXSYAFmrgU5aO_baJud-6-xBsOw@mail.gmail.com>
 <b0c9736d-8d9c-c3cc-decd-eaf1a2b82b60@gmail.com>
 <CAC_=cNaG59M4q0_j66vFOUctqgJW2T7nmBq3LTRrsuEURkyOGw@mail.gmail.com>
Message-ID: <CALEXWq1T+YowWc5pT5tRJ6guH-+eXUn+rN6mFApPX2moOBwECA@mail.gmail.com>

Karol,

If I understood correctly, functions like "foo" are automatically
generated by gEcon's model parser. For such a long function, and
depending on how many times you need to call it, it may make more
sense to generate C++ code instead (including the 'for' loop). Then
you can use Rcpp::sourceCpp, or Rcpp::cppFunction, to compile it and
run it from R.

I?aki

El vie., 17 ago. 2018 a las 0:47, Karol Podemski
(<gecon.maintenance at gmail.com>) escribi?:
>
> Dear Thomas,
>
> thank you for prompt response and taking interest in this issue. I really
> appreciate your compiler project and efficiency gains in usual case. I am
> aware of limitations of interpreted languages too and because of that even
> when writing my first mail I had a hunch that it is not that easy to
> address this problem.  As you mentioned optimisation of compiler for
> handling non-standard code may be tricky and harmful for usual code. The
> question is if gEcon is the only package that may face the same issue
> because of compilation.
>
> The functions generated by gEcon are systems of non-linear equations
> defining the equilibrium of an economy (see
> http://gecon.r-forge.r-project.org/files/gEcon-users-guide.pdf  if you want
> to learn a bit how we obtain it). The rows, you suggested to vectorise, are
> indeed vectorisable because they define equilibrium for similiar markets
> (e.g. production and sale of beverages and food) but do not have to be
> vectorisable in general case. So that not to delve into too much details I
> will stop here in description of how the equations originate. However, I
> would like to point that similiar large systems of linear equations may
> arise in other fields ( https://en.wikipedia.org/wiki/Steady_state ) and
> there may be other packages that generate similar large systems (e.g.
> network problems like hydraulic networks). In that case, reports such as
> mine may help you to assess the scale of the problems.
>
> Thank you for suggestions for improvement in our approach, i am going to
> discuss them with other package developers.
>
> Regards,
> Karol Podemski
>
> pon., 13 sie 2018 o 18:02 Tomas Kalibera <tomas.kalibera at gmail.com>
> napisa?(a):
>
> > Dear Karol,
> >
> > thank you for the report. I can reproduce that the function from you
> > example takes very long to compile and I can see where most time is spent.
> > The compiler is itself written in R and requires a lot of resources for
> > large functions (foo() has over 16,000 lines of code, nearly 1 million of
> > instructions/operands, 45,000 constants). In particular a lot of time is
> > spent in garbage collection and in finding a unique set of constants. Some
> > optimizations of the compiler may be possible, but it is unlikely that
> > functions this large will compile fast any soon. For non-generated code, we
> > now have the byte-compilation on installation by default which at least
> > removes the compile overhead from runtime. Even though the compiler is
> > slow, it is important to keep in mind that in principle, with any compiler
> > there will be functions where compilation would not be improve performance
> > (when the compile time is included or not).
> >
> > I think it is not a good idea to generate code for functions like foo() in
> > R (or any interpreted language). You say that R's byte-code compiler
> > produces code that runs 5-10x faster than when the function is interpreted
> > by the AST interpreter (uncompiled), which sounds like a good result, but I
> > believe that avoiding code generation would be much faster than that, apart
> > from drastically reducing code size and therefore compile time. The
> > generator of these functions has much more information than the compiler -
> > it could be turned into an interpreter of these functions and compute their
> > values on the fly.
> >
> > A significant source of inefficiency of the generated code are
> > element-wise operations, such as
> >
> > r[12] <- -vv[88] + vv[16] * (1 + ppff[1307])
> > ...
> >
> > r[139] <- -vv[215] + vv[47] * (1 + ppff[1434])
> >
> > (these could be vectorized, which would reduce code size and improve
> > interpretation speed; and make it somewhat readable). Most of the code
> > lines in the generated functions seem to be easily vectorizable.
> >
> > Compilers and interpreters necessarily use some heuristics or optimize at
> > some code patterns. Optimizing for generated code may be tricky as it could
> > even harm performance of usual code. And, I would much rather optimize the
> > compiler for the usual code.
> >
> > Indeed, a pragmatic solution requiring the least amount of work would be
> > to disable compilation of these generated functions. There is not a
> > documented way to do that and maybe we could add it (and technically it is
> > trivial), but I have been reluctant so far - in some cases, compilation
> > even of these functions may be beneficial - if the speedup is 5-10x and we
> > run very many times. But once the generated code included some pragma
> > preventing compilation, it won't be ever compiled. Also, the trade-offs may
> > change as the compiler evolves, perhaps not in this case, but in other
> > where such pragma may be used.
> >
> > Well so the short answer would be that these functions should not be
> > generated in the first place. If it were too much work rewriting, perhaps
> > the generator could just be improved to produce vectorized operations.
> >
> > Best
> > Tomas
> > On 12.8.2018 21:31, Karol Podemski wrote:
> >
> >  Dear R team,
> >
> > I am a co-author and maintainer of one of R packages distributed by R-forge
> > (gEcon). One of gEcon package users found a strange behaviour of package (R
> > froze for couple of minutes) and reported it to me. I traced the strange
> > behaviour to compiler package. I attach short demonstration of the problem
> > to this mail (demonstration makes use of compiler and tictoc packages only).
> >
> > In short, the compiler package has problems in compiling large functions -
> > their compilation and execution may take much longer than direct execution
> > of an uncompiled function. Such functions are generated by gEcon package as
> > they describe steady state for economy.
> >
> > I am curious if you are aware of such problems and plan to handle the
> > efficiency issues. On one of the boards I saw that there were efficiency
> > issues in rpart package but they have been resolved. Or would you advise to
> > turn off JIT on package load (package heavily uses such long functions
> > generated whenever a new model is created)?
> >
> > Best regards,
> > Karol Podemski
> >
> >
> >
> > ______________________________________________R-devel at r-project.org mailing listhttps://stat.ethz.ch/mailman/listinfo/r-devel
> >
> >
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel



-- 
I?aki Ucar


From tom@@@k@liber@ @ending from gm@il@com  Fri Aug 17 13:38:52 2018
From: tom@@@k@liber@ @ending from gm@il@com (Tomas Kalibera)
Date: Fri, 17 Aug 2018 13:38:52 +0200
Subject: [Rd] Package compiler - efficiency problem
In-Reply-To: <CAC_=cNaG59M4q0_j66vFOUctqgJW2T7nmBq3LTRrsuEURkyOGw@mail.gmail.com>
References: <CAC_=cNYEie=yjdDvJ1eKhRYSXSYAFmrgU5aO_baJud-6-xBsOw@mail.gmail.com>
 <b0c9736d-8d9c-c3cc-decd-eaf1a2b82b60@gmail.com>
 <CAC_=cNaG59M4q0_j66vFOUctqgJW2T7nmBq3LTRrsuEURkyOGw@mail.gmail.com>
Message-ID: <8020085b-0e76-9ac6-9223-7350c4d7ab91@gmail.com>

Dear Karol,

I don't understand the models behind these function, but I can tell that 
the code generated is very inefficient. The AST interpreter will be very 
inefficient performing each scalar computation with all boxing, 
allocations, function calls. The byte-code compiler removes some of the 
boxing and allocation. While it could certainly compile faster, it will 
always be taking long compiling such functions with so many commands: so 
many expressions to track, so many source references to map, for so 
little computation. The same code could be much more efficient if it 
used vectorized operations and loops. The compiler cannot infer the 
loops and vector operations from the code - it is not that smart and I 
doubt it could easily be for R, but such optimizations could certainly 
be done easily at a higher level, when optimizing computation within the 
model, not within R with all its complicated semantics. I think you 
could hope for ~100x speedups compared to current generated code running 
with R AST interpreter.

So I think it might be worth thinking about writing an interpreter for 
the model (the generator would compute function values on the fly, 
without actually generating code). If that was too slow, it might pay 
off to generate some intermediate representation for the model that 
would be faster to interpret. If that was too hard, then perhaps 
generating the code from the model in a smarter way (use vector 
operations, loops). It is ok to do that opportunistically - only when 
possible. With compilers, this is normal, optimizations often take 
advantage of certain patterns in the code if they are present. If you 
had more specific questions how to optimize the code feel free to ask 
(also offline).

Certainly I don't want the existence of the byte-code compiler to 
require you to switch from R to C/C++, that would be exactly the 
opposite of what the compiler is aiming for. If it turns out you really 
need a way to disable compilation of these generated functions (so they 
run much slower, but you don't have to wait for them to compile), we 
will provide it and using a hack/workaround it is already possible in 
existing versions of R, with all the drawbacks I mentioned previously.

Best
Tomas


On 08/17/2018 12:43 AM, Karol Podemski wrote:
> Dear Thomas,
>
> thank you for prompt response and taking interest in this issue. I 
> really appreciate your compiler project and efficiency gains in usual 
> case. I am aware of limitations of interpreted languages too and 
> because of that even when writing my first mail I had a hunch that it 
> is not that easy to address this problem.? As you mentioned 
> optimisation of compiler for handling non-standard code may be tricky 
> and harmful for usual code. The question is if gEcon is the only 
> package that may face the same issue because of compilation.
>
> The functions generated by gEcon are systems of non-linear equations 
> defining the equilibrium of an economy (see 
> http://gecon.r-forge.r-project.org/files/gEcon-users-guide.pdf if you 
> want to learn a bit how we obtain it). The rows, you suggested to 
> vectorise, are indeed vectorisable because they define equilibrium for 
> similiar markets (e.g. production and sale of beverages and food) but 
> do not have to be vectorisable in general case. So that not to delve 
> into too much details I will stop here in description of how the 
> equations originate. However, I would like to point that similiar 
> large systems of linear equations may arise in other fields ( 
> https://en.wikipedia.org/wiki/Steady_state ) and there may be other 
> packages that generate similar large systems (e.g. network problems 
> like hydraulic networks). In that case, reports such as mine may help 
> you to assess the scale of the problems.
>
> Thank you for suggestions for improvement in our approach, i am going 
> to discuss them with other package developers.
>
> Regards,
> Karol Podemski
>
> pon., 13 sie 2018 o 18:02?Tomas Kalibera <tomas.kalibera at gmail.com 
> <mailto:tomas.kalibera at gmail.com>> napisa?(a):
>
>     Dear Karol,
>
>     thank you for the report. I can reproduce that the function from
>     you example takes very long to compile and I can see where most
>     time is spent. The compiler is itself written in R and requires a
>     lot of resources for large functions (foo() has over 16,000 lines
>     of code, nearly 1 million of instructions/operands, 45,000
>     constants). In particular a lot of time is spent in garbage
>     collection and in finding a unique set of constants. Some
>     optimizations of the compiler may be possible, but it is unlikely
>     that functions this large will compile fast any soon. For
>     non-generated code, we now have the byte-compilation on
>     installation by default which at least removes the compile
>     overhead from runtime. Even though the compiler is slow, it is
>     important to keep in mind that in principle, with any compiler
>     there will be functions where compilation would not be improve
>     performance (when the compile time is included or not).
>
>     I think it is not a good idea to generate code for functions like
>     foo() in R (or any interpreted language). You say that R's
>     byte-code compiler produces code that runs 5-10x faster than when
>     the function is interpreted by the AST interpreter (uncompiled),
>     which sounds like a good result, but I believe that avoiding code
>     generation would be much faster than that, apart from drastically
>     reducing code size and therefore compile time. The generator of
>     these functions has much more information than the compiler - it
>     could be turned into an interpreter of these functions and compute
>     their values on the fly.
>
>     A significant source of inefficiency of the generated code are
>     element-wise operations, such as
>
>     r[12] <- -vv[88] + vv[16] * (1 + ppff[1307])
>     ...
>
>     r[139] <- -vv[215] + vv[47] * (1 + ppff[1434])
>
>     (these could be vectorized, which would reduce code size and
>     improve interpretation speed; and make it somewhat readable). Most
>     of the code lines in the generated functions seem to be easily
>     vectorizable.
>
>     Compilers and interpreters necessarily use some heuristics or
>     optimize at some code patterns. Optimizing for generated code may
>     be tricky as it could even harm performance of usual code. And, I
>     would much rather optimize the compiler for the usual code.
>
>     Indeed, a pragmatic solution requiring the least amount of work
>     would be to disable compilation of these generated functions.
>     There is not a documented way to do that and maybe we could add it
>     (and technically it is trivial), but I have been reluctant so far
>     - in some cases, compilation even of these functions may be
>     beneficial - if the speedup is 5-10x and we run very many times.
>     But once the generated code included some pragma preventing
>     compilation, it won't be ever compiled. Also, the trade-offs may
>     change as the compiler evolves, perhaps not in this case, but in
>     other where such pragma may be used.
>
>     Well so the short answer would be that these functions should not
>     be generated in the first place. If it were too much work
>     rewriting, perhaps the generator could just be improved to produce
>     vectorized operations.
>
>     Best
>     Tomas
>
>     On 12.8.2018 21:31, Karol Podemski wrote:
>>       Dear R team,
>>
>>     I am a co-author and maintainer of one of R packages distributed by R-forge
>>     (gEcon). One of gEcon package users found a strange behaviour of package (R
>>     froze for couple of minutes) and reported it to me. I traced the strange
>>     behaviour to compiler package. I attach short demonstration of the problem
>>     to this mail (demonstration makes use of compiler and tictoc packages only).
>>
>>     In short, the compiler package has problems in compiling large functions -
>>     their compilation and execution may take much longer than direct execution
>>     of an uncompiled function. Such functions are generated by gEcon package as
>>     they describe steady state for economy.
>>
>>     I am curious if you are aware of such problems and plan to handle the
>>     efficiency issues. On one of the boards I saw that there were efficiency
>>     issues in rpart package but they have been resolved. Or would you advise to
>>     turn off JIT on package load (package heavily uses such long functions
>>     generated whenever a new model is created)?
>>
>>     Best regards,
>>     Karol Podemski
>>
>>
>>     ______________________________________________
>>     R-devel at r-project.org <mailto:R-devel at r-project.org>  mailing list
>>     https://stat.ethz.ch/mailman/listinfo/r-devel
>


	[[alternative HTML version deleted]]


From tom@@@k@liber@ @ending from gm@il@com  Fri Aug 17 17:16:12 2018
From: tom@@@k@liber@ @ending from gm@il@com (Tomas Kalibera)
Date: Fri, 17 Aug 2018 17:16:12 +0200
Subject: [Rd] odd behavior of names
In-Reply-To: <23390.48896.646829.975074@stat.math.ethz.ch>
References: <CAP01uR=K40ZD9M=wMdtAh77pyOd-4B39dcDzhE4Jdeh4XPDCrg@mail.gmail.com>
 <CCAD7D06-AC31-483A-90C9-756C0F402247@comcast.net>
 <CAF8bMcZYoaN02TpYh2O=1zYWyjynHti2gga1O0pfdkrgKAojFg@mail.gmail.com>
 <23390.48896.646829.975074@stat.math.ethz.ch>
Message-ID: <31b37ff2-261d-488b-1a54-776960a866c8@gmail.com>


Both bugs should now be fixed in R-devel. The first one was causing the 
difference between the first and the following element. The second one 
was causing the printing of unnecessary backticks, inconsistently with 
Unix systems.

Best
Tomas

On 07/30/2018 09:32 AM, Martin Maechler wrote:
>>>>>> William Dunlap via R-devel
>>>>>>      on Sun, 29 Jul 2018 10:06:40 -0700 writes:
>      > Bugzilla issue 16101 describes another
>      > first-list-name-printed-differently oddity with
>      > the Windows GUI version of R:
>        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
>
> Indeed:
> 	1) "first-list-name-printed" [i.e "names" only in that
> 	   			     context of printing]
> 	2) Windows GUI version of R only
> 	   (not 'Rterm', i.e., not in ESS (emacs speaks
> 	   statistics), nor in Rstudio on Windows
>
> Kevin Ushey has reported this last week in an official (and
> perfect) bug report to R's bugzilla bug reporting site:
>
>   https://bugs.r-project.org/bugzilla/show_bug.cgi?id=17447
>
>              Bug ID: 17447
>             Summary: list() prints first variable name in backticks
> 	   ....
>           Component: Windows GUI / Window specific
>            Reporter: kevinushey ..  gmail ..
>
> His minimal REPREX was even much simpler:
>
>     > list(a = 1, b = 2)
>      $`a`
>      [1] 1
>
>      $b
>      [1] 2
>
>
> Thank you, Bill, for the nice extra example.
>
> Martin Maechler
> ETH Zurich and R Core Team

>
>
>>> a <- "One is \u043E\u0434\u0438\u043D\nTwo is \u0434\u0432\u0430\n"
>>> Encoding(a) # expect "UTF-8"
>> [1] "UTF-8"
>>> sapply(strsplit(a, "\n")[[1]], charToRaw)[c(1,1,2)]
>> $`One is ????`
>>   [1] 4f 6e 65 20 69 73 20 d0 be d0 b4 d0
>> [13] b8 d0 bd
>>
>> $`One is <U+043E><U+0434><U+0438><U+043D>`
>>   [1] 4f 6e 65 20 69 73 20 d0 be d0 b4 d0
>> [13] b8 d0 bd
>>
>> $`Two is <U+0434><U+0432><U+0430>`
>>   [1] 54 77 6f 20 69 73 20 d0 b4 d0 b2 d0
>> [13] b0
>>
>>> names(.Last.value)
>> [1] "One is ????" "One is ????"
>> [3] "Two is ???"
>>
>>
>>
>> Bill Dunlap
>> TIBCO Software
>> wdunlap tibco.com
> [..............................]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From Arunkum@r@Sriniv@@@n @ending from uk@mlp@com  Fri Aug 17 17:11:50 2018
From: Arunkum@r@Sriniv@@@n @ending from uk@mlp@com (Srinivasan, Arunkumar)
Date: Fri, 17 Aug 2018 15:11:50 +0000
Subject: [Rd] Get Logical processor count correctly whether NUMA is enabled
 or disabled
Message-ID: <928CB09B31774648949A4D61FE4622AB023343B7@PWSTLCEXMBX001.AD.MLP.com>

Dear R-devel list,

R's detectCores() function internally calls "ncpus" function to get the total number of logical processors. However, this doesnot seem to take NUMA into account on Windows machines.

On a machine having 48 processors (24 cores) in total and windows server 2012 installed, if NUMA is enabled and has 2 nodes (node 0 and node 1 each having 24 CPUs), then R's detectCores() only detects 24 instead of the total 48. If NUMA is disabled, detectCores() returns 48.

Similarly, on a machine with 88 cores (176 processors) and windows server 2012, detectCores() with NUMA disabled only returns the maximum value of 64. If NUMA is enabled with 4 nodes (44 processors each), then detectCores() will only return 44. This is particularly limiting since we cannot get to use all processors by enabling/disabling NUMA in this case.

We think this is because R's ncpus.c file uses "PSYSTEM_LOGICAL_PROCESSOR_INFORMATION" (https://msdn.microsoft.com/en-us/library/windows/desktop/ms683194(v=vs.85).aspx) instead of "PSYSTEM_LOGICAL_PROCESSOR_INFORMATION_EX" (https://msdn.microsoft.com/en-us/library/windows/desktop/dd405488(v=vs.85).aspx). Specifically, quoting from the first link:

"On systems with more than 64 logical processors, the?GetLogicalProcessorInformation?function retrieves logical processor information about processors in the?https://msdn.microsoft.com/en-us/library/windows/desktop/dd405503(v=vs.85).aspx?to which the calling thread is currently assigned. Use the?https://msdn.microsoft.com/en-us/library/windows/desktop/dd405488(v=vs.85).aspx?function to retrieve information about processors in all processor groups on the system."

Therefore, it might be possible to get the right count of total processors even with NUMA enabled by using "GetLogicalProcessorInformationEX".  It'd be nice to know what you think.

Thank you very much,
Arun.

--
Arun Srinivasan
Analyst, Millennium Management LLC 
50 Berkeley Street | London, W1J 8HD


######################################################################

The information contained in this communication is confi...{{dropped:30}}


From rpb@rry @ending from @l@@k@@edu  Sat Aug 18 21:47:02 2018
From: rpb@rry @ending from @l@@k@@edu (Ronald Barry)
Date: Sat, 18 Aug 2018 11:47:02 -0800
Subject: [Rd] validspamobject?
In-Reply-To: <CAOVYLHdqrAKUtZ23DH1d6z5PZqLmbWTLjr=9H0+-TFcCPyzbww@mail.gmail.com>
References: <CAOVYLHdqrAKUtZ23DH1d6z5PZqLmbWTLjr=9H0+-TFcCPyzbww@mail.gmail.com>
Message-ID: <CAOVYLHdFq3YRnbQ5V4zutD496rePFCi=pqMJsHyeqFjj-3+X2A@mail.gmail.com>

Hi,
  I have submitted an updated R package to CRAN, the only problem seems to
be the examples take too long to run on Windows and Debian.  Several
examples take over 10 seconds and the entire set takes around 110 seconds.
However, I don't see what the 'target' amount of time is.  How much will
this need to be sped up to pass the test?  Also, are there some tricks to
speeding up examples that you have seen?  For instance, is it possible to
have the identical example in two help files but only have them run once
(other than the obvious, which is putting a 'do not run' on one of them,
which would seem to invite possible bad code)?  Thanks for any information.

Ron Barry

On Tue, Aug 14, 2018 at 4:56 PM, Ronald Barry <rpbarry at alaska.edu> wrote:

> Greetings,
>   My R package has been showing warnings of the form:
>
> `validspamobject()` is deprecated. Use `validate_spam()` directly
>
> None of my code uses the function validspamobject, so it must be a problem
> in another package I'm calling, possibly spam or spdep.  Has this problem
> occurred with other people?  It doesn't have any deleterious effect, but
> it's annoying.  In particular, how do I determine which package is causing
> this warning?  Thanks.
>
> Ron B.
>

	[[alternative HTML version deleted]]


From bbolker @ending from gm@il@com  Sat Aug 18 23:51:38 2018
From: bbolker @ending from gm@il@com (Ben Bolker)
Date: Sat, 18 Aug 2018 17:51:38 -0400
Subject: [Rd] image() method for Matrix fails on empty matrices (?)
Message-ID: <b708dbad-9062-0742-8297-8c6b85f4e0c5@gmail.com>


 Reasonably easy to avoid, but maybe an edge case that should be
handled?  Haven't looked yet to see how easy it would be to fix ... Am I
missing something?

> library(Matrix)
> m <- Matrix(0,nrow=3,ncol=3)
> m
3 x 3 sparse Matrix of class "dsCMatrix"

[1,] . . .
[2,] . . .
[3,] . . .
> image(m)
Error in seq.default(zrng[1], zrng[2], length.out = cuts + 2) :
  'from' must be a finite number
In addition: Warning messages:
1: In min(xx, na.rm = TRUE) :
  no non-missing arguments to min; returning Inf
2: In min(x) : no non-missing arguments to min; returning Inf
3: In max(x) : no non-missing arguments to max; returning -Inf
4: In min(x) : no non-missing arguments to min; returning Inf
5: In max(x) : no non-missing arguments to max; returning -Inf

## now modify matrix
> m[3,3] <- 1
> image(m) ## works fine
## reset:
> m[3,3] <- 0
> image(m) ## fails again


From murdoch@dunc@n @ending from gm@il@com  Sun Aug 19 00:22:16 2018
From: murdoch@dunc@n @ending from gm@il@com (Duncan Murdoch)
Date: Sat, 18 Aug 2018 18:22:16 -0400
Subject: [Rd] validspamobject?
In-Reply-To: <CAOVYLHdFq3YRnbQ5V4zutD496rePFCi=pqMJsHyeqFjj-3+X2A@mail.gmail.com>
References: <CAOVYLHdqrAKUtZ23DH1d6z5PZqLmbWTLjr=9H0+-TFcCPyzbww@mail.gmail.com>
 <CAOVYLHdFq3YRnbQ5V4zutD496rePFCi=pqMJsHyeqFjj-3+X2A@mail.gmail.com>
Message-ID: <1f45e243-b6cf-ce24-0506-b612c762a3c0@gmail.com>

On 18/08/2018 3:47 PM, Ronald Barry wrote:
> Hi,
>    I have submitted an updated R package to CRAN, the only problem seems to
> be the examples take too long to run on Windows and Debian.  Several
> examples take over 10 seconds and the entire set takes around 110 seconds.
> However, I don't see what the 'target' amount of time is.  How much will
> this need to be sped up to pass the test?  Also, are there some tricks to
> speeding up examples that you have seen?  For instance, is it possible to
> have the identical example in two help files but only have them run once
> (other than the obvious, which is putting a 'do not run' on one of them,
> which would seem to invite possible bad code)?  Thanks for any information.

Doesn't the error message say something like "Examples with CPU or 
elapsed time > 5s"?  So to avoid that message, reduce the time to less 
than the stated limit.

You can't share examples between different help pages, but you can 
combine topics in a single page, so there's only one example to run.

The usual way to speed up examples is to make up trivial datasets for 
them, rather than running them on real data.

Duncan Murdoch

> 
> Ron Barry
> 
> On Tue, Aug 14, 2018 at 4:56 PM, Ronald Barry <rpbarry at alaska.edu> wrote:
> 
>> Greetings,
>>    My R package has been showing warnings of the form:
>>
>> `validspamobject()` is deprecated. Use `validate_spam()` directly
>>
>> None of my code uses the function validspamobject, so it must be a problem
>> in another package I'm calling, possibly spam or spdep.  Has this problem
>> occurred with other people?  It doesn't have any deleterious effect, but
>> it's annoying.  In particular, how do I determine which package is causing
>> this warning?  Thanks.
>>
>> Ron B.
>>
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From m@echler @ending from @t@t@m@th@ethz@ch  Mon Aug 20 10:30:28 2018
From: m@echler @ending from @t@t@m@th@ethz@ch (Martin Maechler)
Date: Mon, 20 Aug 2018 10:30:28 +0200
Subject: [Rd] image() method for Matrix fails on empty matrices (?)
In-Reply-To: <b708dbad-9062-0742-8297-8c6b85f4e0c5@gmail.com>
References: <b708dbad-9062-0742-8297-8c6b85f4e0c5@gmail.com>
Message-ID: <23418.31780.822283.881580@stat.math.ethz.ch>

>>>>> Ben Bolker 
>>>>>     on Sat, 18 Aug 2018 17:51:38 -0400 writes:

    >  Reasonably easy to avoid, but maybe an edge case that
    > should be handled?  Haven't looked yet to see how easy it
    > would be to fix ... Am I missing something?

No, I don't think so.

"Of course", we would typically not encounter this case, as we
typically would not visualize triviality ...

but nevertheless an edge case bug that I will fix immediately
and hence it will soon be available in the development version of
Matrix from R-forge e.g., by

   install.packages("Matrix", repos="http://R-Forge.R-project.org")

Thank you for the report, Ben!
Martin


> > library(Matrix)
> > m <- Matrix(0,nrow=3,ncol=3)
> > m
> 3 x 3 sparse Matrix of class "dsCMatrix"

> [1,] . . .
> [2,] . . .
> [3,] . . .
> > image(m)
> Error in seq.default(zrng[1], zrng[2], length.out = cuts + 2) :
>   'from' must be a finite number
> In addition: Warning messages:
> 1: In min(xx, na.rm = TRUE) :
>   no non-missing arguments to min; returning Inf
> 2: In min(x) : no non-missing arguments to min; returning Inf
> 3: In max(x) : no non-missing arguments to max; returning -Inf
> 4: In min(x) : no non-missing arguments to min; returning Inf
> 5: In max(x) : no non-missing arguments to max; returning -Inf

> ## now modify matrix
> > m[3,3] <- 1
> > image(m) ## works fine
> ## reset:
> > m[3,3] <- 0
> > image(m) ## fails again

> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From iuc@r @ending from fedor@project@org  Mon Aug 20 18:36:07 2018
From: iuc@r @ending from fedor@project@org (=?UTF-8?Q?I=C3=B1aki_Ucar?=)
Date: Mon, 20 Aug 2018 18:36:07 +0200
Subject: [Rd] Consider setting RTLD_GLOBAL when loading packages in LinkingTo
Message-ID: <CALEXWq0DppMvVvM9ULWYgJfVbfB4JGRU2yeLU+hmkwKNG1RPNQ@mail.gmail.com>

Hi everyone,

Some of you probably received the following thread from the Rcpp-devel
mailing list:

http://lists.r-forge.r-project.org/pipermail/rcpp-devel/2018-August/010072.html

Summing up, the issue described is the following: pkg1 provides type1
in pkg1.so building on some headers. pkg2 links to pkg1 (BTW,
LinkingTo is actually misleading, because it doesn't really link to
it), i.e., provides type1 in pkg2.so building on the same headers.
Now, pkg2 creates an external pointer to type1 using pkg1.so, and
dynamically casts it and manipulates it using functions in pkg2.so.

This works perfectly, because type1 is exactly the same in pkg1.so and
pkg2.so. *But* UBSAN sanitizers give a runtime error, which arguably
is a false positive. Real example on CRAN:

https://www.stats.ox.ac.uk/pub/bdr/memtests/gcc-UBSAN/ldat/ldat-Ex.Rout

A solution to this would be to dlopen pkg1.so with RTLD_GLOBAL,
instead of RTLD_LOCAL, i.e., dyn.load(local=FALSE). So my proposal is
to automatically set RTLD_GLOBAL for those packages that are listed at
the same time in Depends/Imports/Suggests and LinkingTo, at least for
those machines on CRAN running UBSAN tests.

Regards,
-- 
I?aki Ucar


From tom@@@k@liber@ @ending from gm@il@com  Tue Aug 21 12:50:22 2018
From: tom@@@k@liber@ @ending from gm@il@com (Tomas Kalibera)
Date: Tue, 21 Aug 2018 12:50:22 +0200
Subject: [Rd] 
 Get Logical processor count correctly whether NUMA is enabled
 or disabled
In-Reply-To: <928CB09B31774648949A4D61FE4622AB023343B7@PWSTLCEXMBX001.AD.MLP.com>
References: <928CB09B31774648949A4D61FE4622AB023343B7@PWSTLCEXMBX001.AD.MLP.com>
Message-ID: <1ce7682a-4656-0366-ba0f-8c751697db7e@gmail.com>

Dear Arun,

thank you for the report. I agree with the analysis, detectCores() will 
only report logical processors in the NUMA group in which R is running. 
I don't have a system to test on, could you please check these 
workarounds for me on your systems?

# number of logical processors - what detectCores() should return

out <- system("wmic cpu get numberoflogicalprocessors", intern=TRUE)
sum(as.numeric(gsub("([0-9]+).*", "\\1", grep("[0-9]+[ \t]*", out, 
value=TRUE))))

# number of cores - what detectCores(FALSE) should return

out <- system("wmic cpu get numberofcores", intern=TRUE)
sum(as.numeric(gsub("([0-9]+).*", "\\1", grep("[0-9]+[ \t]*", out, 
value=TRUE))))

# number of physical processors - as a sanity check

system("wmic computersystem get numberofprocessors")

Thanks,
Tomas

On 08/17/2018 05:11 PM, Srinivasan, Arunkumar wrote:
> Dear R-devel list,
>
> R's detectCores() function internally calls "ncpus" function to get the total number of logical processors. However, this doesnot seem to take NUMA into account on Windows machines.
>
> On a machine having 48 processors (24 cores) in total and windows server 2012 installed, if NUMA is enabled and has 2 nodes (node 0 and node 1 each having 24 CPUs), then R's detectCores() only detects 24 instead of the total 48. If NUMA is disabled, detectCores() returns 48.
>
> Similarly, on a machine with 88 cores (176 processors) and windows server 2012, detectCores() with NUMA disabled only returns the maximum value of 64. If NUMA is enabled with 4 nodes (44 processors each), then detectCores() will only return 44. This is particularly limiting since we cannot get to use all processors by enabling/disabling NUMA in this case.
>
> We think this is because R's ncpus.c file uses "PSYSTEM_LOGICAL_PROCESSOR_INFORMATION" (https://msdn.microsoft.com/en-us/library/windows/desktop/ms683194(v=vs.85).aspx) instead of "PSYSTEM_LOGICAL_PROCESSOR_INFORMATION_EX" (https://msdn.microsoft.com/en-us/library/windows/desktop/dd405488(v=vs.85).aspx). Specifically, quoting from the first link:
>
> "On systems with more than 64 logical processors, the?GetLogicalProcessorInformation?function retrieves logical processor information about processors in the?https://msdn.microsoft.com/en-us/library/windows/desktop/dd405503(v=vs.85).aspx?to which the calling thread is currently assigned. Use the?https://msdn.microsoft.com/en-us/library/windows/desktop/dd405488(v=vs.85).aspx?function to retrieve information about processors in all processor groups on the system."
>
> Therefore, it might be possible to get the right count of total processors even with NUMA enabled by using "GetLogicalProcessorInformationEX".  It'd be nice to know what you think.
>
> Thank you very much,
> Arun.
>
> --
> Arun Srinivasan
> Analyst, Millennium Management LLC
> 50 Berkeley Street | London, W1J 8HD
>
>
> ######################################################################
>
> The information contained in this communication is con...{{dropped:11}}


From Arunkum@r@Sriniv@@@n @ending from uk@mlp@com  Tue Aug 21 14:53:46 2018
From: Arunkum@r@Sriniv@@@n @ending from uk@mlp@com (Srinivasan, Arunkumar)
Date: Tue, 21 Aug 2018 12:53:46 +0000
Subject: [Rd] 
 Get Logical processor count correctly whether NUMA is enabled
 or disabled
In-Reply-To: <1ce7682a-4656-0366-ba0f-8c751697db7e@gmail.com>
References: <928CB09B31774648949A4D61FE4622AB023343B7@PWSTLCEXMBX001.AD.MLP.com>
 <1ce7682a-4656-0366-ba0f-8c751697db7e@gmail.com>
Message-ID: <928CB09B31774648949A4D61FE4622AB02335A3A@PWSTLCEXMBX001.AD.MLP.com>

Dear Tomas, thank you for looking into this. Here's the output:

# number of logical processors - what detectCores() should return
out <- system("wmic cpu get numberoflogicalprocessors", intern=TRUE) 
[1] "NumberOfLogicalProcessors  \r" "22                         \r" "22                         \r"
[4] "20                         \r" "22                         \r" "\r"                           
sum(as.numeric(gsub("([0-9]+).*", "\\1", grep("[0-9]+[ \t]*", out, value=TRUE)))) 
# [1] 86

[I've asked the IT team to understand why one of the values is 20 instead of 22].

# number of cores - what detectCores(FALSE) should return
out <- system("wmic cpu get numberofcores", intern=TRUE)
[1] "NumberOfCores  \r" "22             \r" "22             \r" "20             \r" "22             \r"
[6] "\r"
sum(as.numeric(gsub("([0-9]+).*", "\\1", grep("[0-9]+[ \t]*", out, value=TRUE))))
# [1] 86

[Currently hyperthreading is disabled. So this output being identical to the previous output makes sense].

system("wmic computersystem get numberofprocessors")
NumberOfProcessors  
4

In addition, I'd also bring to your attention this documentation: https://docs.microsoft.com/en-us/windows/desktop/ProcThread/processor-groups on processor groups which explain how one should go about running a process ro run on multiple groups (which seems to be different to NUMA). All this seems overly complicated to allow a process to use all cores by default TBH.

Here's a project on Github 'fio' where the issue of running a process on more than 1 processor group has come up -  https://github.com/axboe/fio/issues/527 and is addressed - https://github.com/axboe/fio/blob/c479640d6208236744f0562b1e79535eec290e2b/os/os-windows-7.h . I am not sure though if this is entirely relevant since we would be forking new processes in R instead of allowing a single process to use all cores. Apologies if this is utterly irrelevant.

Thank you,
Arun.

From: Tomas Kalibera <tomas.kalibera at gmail.com> 
Sent: 21 August 2018 11:50
To: Srinivasan, Arunkumar <Arunkumar.Srinivasan at uk.mlp.com>; r-devel at r-project.org
Subject: Re: [Rd] Get Logical processor count correctly whether NUMA is enabled or disabled

Dear Arun,

thank you for the report. I agree with the analysis, detectCores() will only report logical processors in the NUMA group in which R is running. I don't have a system to test on, could you please check these workarounds for me on your systems?

# number of logical processors - what detectCores() should return
out <- system("wmic cpu get numberoflogicalprocessors", intern=TRUE)
sum(as.numeric(gsub("([0-9]+).*", "\\1", grep("[0-9]+[ \t]*", out, value=TRUE))))

# number of cores - what detectCores(FALSE) should return
out <- system("wmic cpu get numberofcores", intern=TRUE)
sum(as.numeric(gsub("([0-9]+).*", "\\1", grep("[0-9]+[ \t]*", out, value=TRUE))))

# number of physical processors - as a sanity check

system("wmic computersystem get numberofprocessors")

Thanks,
Tomas

On 08/17/2018 05:11 PM, Srinivasan, Arunkumar wrote:
Dear R-devel list,

R's detectCores() function internally calls "ncpus" function to get the total number of logical processors. However, this doesnot seem to take NUMA into account on Windows machines.

On a machine having 48 processors (24 cores) in total and windows server 2012 installed, if NUMA is enabled and has 2 nodes (node 0 and node 1 each having 24 CPUs), then R's detectCores() only detects 24 instead of the total 48. If NUMA is disabled, detectCores() returns 48.

Similarly, on a machine with 88 cores (176 processors) and windows server 2012, detectCores() with NUMA disabled only returns the maximum value of 64. If NUMA is enabled with 4 nodes (44 processors each), then detectCores() will only return 44. This is particularly limiting since we cannot get to use all processors by enabling/disabling NUMA in this case.

We think this is because R's ncpus.c file uses "PSYSTEM_LOGICAL_PROCESSOR_INFORMATION" (https://msdn.microsoft.com/en-us/library/windows/desktop/ms683194(v=vs.85).aspx) instead of "PSYSTEM_LOGICAL_PROCESSOR_INFORMATION_EX" (https://msdn.microsoft.com/en-us/library/windows/desktop/dd405488(v=vs.85).aspx). Specifically, quoting from the first link:

"On systems with more than 64 logical processors, the?GetLogicalProcessorInformation?function retrieves logical processor information about processors in the?https://msdn.microsoft.com/en-us/library/windows/desktop/dd405503(v=vs.85).aspx?to which the calling thread is currently assigned. Use the?https://msdn.microsoft.com/en-us/library/windows/desktop/dd405488(v=vs.85).aspx?function to retrieve information about processors in all processor groups on the system."

Therefore, it might be possible to get the right count of total processors even with NUMA enabled by using "GetLogicalProcessorInformationEX".  It'd be nice to know what you think.

Thank you very much,
Arun.

--
Arun Srinivasan
Analyst, Millennium Management LLC 
50 Berkeley Street | London, W1J 8HD


######################################################################

The information contained in this communication is confi...{{dropped:30}}

______________________________________________
mailto:R-devel at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-devel

######################################################################

The information contained in this communication is confidential and

intended only for the individual(s) named above. If you are not a named

addressee, please notify the sender immediately and delete this email

from your system and do not disclose the email or any part of it to any

person. The views expressed in this email are the views of the author

and do not necessarily represent the views of Millennium Capital Partners

LLP (MCP LLP) or any of its affiliates. Outgoing and incoming electronic

communications of MCP LLP and its affiliates, including telephone

communications, may be electronically archived and subject to review

and/or disclosure to someone other than the recipient. MCP LLP is

authorized and regulated by the Financial Conduct Authority. Millennium

Capital Partners LLP is a limited liability partnership registered in

England & Wales with number OC312897 and with its registered office at

50 Berkeley Street, London, W1J 8HD.

######################################################################

From c@@rdi@g@bor @ending from gm@il@com  Tue Aug 21 23:53:28 2018
From: c@@rdi@g@bor @ending from gm@il@com (=?UTF-8?B?R8OhYm9yIENzw6FyZGk=?=)
Date: Tue, 21 Aug 2018 23:53:28 +0200
Subject: [Rd] Translating Rd files
Message-ID: <CABtg=K=AZzo9UTyR6fKM=kLChOZfbPdqbT_wzgAh2Am0xpkWDQ@mail.gmail.com>

Dear All,

are there any resources (code, guidelines, anything) for translating Rd files?
As far as I can tell, the help system does not have support for this.
Have I missed something? Is such support desired?

I am thinking about a way to register manual pages in different
languages, and then`help()` would bring up the page with the preferred
language, defaulting to the current locale.

Thanks, Best,
Gabor


From murdoch@dunc@n @ending from gm@il@com  Wed Aug 22 00:30:27 2018
From: murdoch@dunc@n @ending from gm@il@com (Duncan Murdoch)
Date: Tue, 21 Aug 2018 18:30:27 -0400
Subject: [Rd] Translating Rd files
In-Reply-To: <CABtg=K=AZzo9UTyR6fKM=kLChOZfbPdqbT_wzgAh2Am0xpkWDQ@mail.gmail.com>
References: <CABtg=K=AZzo9UTyR6fKM=kLChOZfbPdqbT_wzgAh2Am0xpkWDQ@mail.gmail.com>
Message-ID: <d603cc07-d0cb-36b7-10e8-4f02ebf28a5a@gmail.com>

On 21/08/2018 5:53 PM, G?bor Cs?rdi wrote:
> Dear All,
> 
> are there any resources (code, guidelines, anything) for translating Rd files?
> As far as I can tell, the help system does not have support for this.
> Have I missed something? Is such support desired?

As of last year, support for this was seen as desirable but no action 
had been taken to put it in place.  The thinking was that it was too 
much work for the translators to have to follow every update to each 
help page, so the translated pages would likely soon be out of date.

Duncan Murdoch

> 
> I am thinking about a way to register manual pages in different
> languages, and then`help()` would bring up the page with the preferred
> language, defaulting to the current locale.


From @ntoine@f@bri @ending from gm@il@com  Tue Aug 21 16:31:13 2018
From: @ntoine@f@bri @ending from gm@il@com (Ant F)
Date: Tue, 21 Aug 2018 16:31:13 +0200
Subject: [Rd] bug report: inaccurate error message for stats::chisq.test
Message-ID: <CAEKh8ugry7UaJ2a3AmjCbkz8iB4cqE7++r523esTuKVvPfRP2w@mail.gmail.com>

Hi,

`stats::chisq.test` checks that x and y each have at least 2 levels AFTER
filtering on complete cases.

It makes sense but the error message is misleading : ?'x' and 'y' must have
at least 2 levels?

Here?s how to reproduce the issue :

    x <- structure(c(1L, 1L, 1L, 2L, 1L, 2L), .Label = c("0001", "0003"),
class = "factor")

    y <- structure(c(1L, 2L, 2L, NA, 2L, NA), .Label = c("0001", "0002"),
class = "factor")

    chisq.test(x,y)

    # Error in chisq.test(...) : 'x' and 'y' must have at least 2 levels

In this case they do have 2 levels.

Best regards,

Antoine

>

	[[alternative HTML version deleted]]


From ruipb@rr@d@@ @ending from @@po@pt  Wed Aug 22 22:43:23 2018
From: ruipb@rr@d@@ @ending from @@po@pt (Rui Barradas)
Date: Wed, 22 Aug 2018 21:43:23 +0100
Subject: [Rd] bug report: inaccurate error message for stats::chisq.test
In-Reply-To: <CAEKh8ugry7UaJ2a3AmjCbkz8iB4cqE7++r523esTuKVvPfRP2w@mail.gmail.com>
References: <CAEKh8ugry7UaJ2a3AmjCbkz8iB4cqE7++r523esTuKVvPfRP2w@mail.gmail.com>
Message-ID: <1d204df8-652c-2aff-df8c-733ca37fb610@sapo.pt>

Hello,

A workaround could be

y <- addNA(y)
chisq.test(x,y)


But this implies that the user was aware of the  reason why the error.

Hope this helps,

Rui Barradas

On 21/08/2018 15:31, Ant F wrote:
> Hi,
> 
> `stats::chisq.test` checks that x and y each have at least 2 levels AFTER
> filtering on complete cases.
> 
> It makes sense but the error message is misleading : ?'x' and 'y' must have
> at least 2 levels?
> 
> Here?s how to reproduce the issue :
> 
>      x <- structure(c(1L, 1L, 1L, 2L, 1L, 2L), .Label = c("0001", "0003"),
> class = "factor")
> 
>      y <- structure(c(1L, 2L, 2L, NA, 2L, NA), .Label = c("0001", "0002"),
> class = "factor")
> 
>      chisq.test(x,y)
> 
>      # Error in chisq.test(...) : 'x' and 'y' must have at least 2 levels
> 
> In this case they do have 2 levels.
> 
> Best regards,
> 
> Antoine
> 
>>
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 

---
This email has been checked for viruses by AVG.
https://www.avg.com


From c@gille@pie @ending from gm@il@com  Thu Aug 23 17:06:35 2018
From: c@gille@pie @ending from gm@il@com (Colin Gillespie)
Date: Thu, 23 Aug 2018 16:06:35 +0100
Subject: [Rd] Slightly misleading help page: tools::add_datalist
Message-ID: <CADbDLZkC3OGFi1o1PtkcitWzTmyuQpjG1J1gJ=hZFBQxnwwBEg@mail.gmail.com>

Hi,

I was looking at the help page for add_datalist() and it states under
Details:

"R CMD build will call this function to add a data list to packages with
1MB or more of data."

This is correct, however, what is omitted from the help page is that
add_datalist() will *only* create the datalist file on packages with 1MB or
more of data, as in the function we have

if (size <= 1024^2) return()

As an aside what's the reason for the 1MB limit?

Thanks

Colin

	[[alternative HTML version deleted]]


From h@wickh@m @ending from gm@il@com  Thu Aug 23 20:31:58 2018
From: h@wickh@m @ending from gm@il@com (Hadley Wickham)
Date: Thu, 23 Aug 2018 13:31:58 -0500
Subject: [Rd] conflicted: an alternative conflict resolution strategy
Message-ID: <CABdHhvHyV7YXUp9fChvpH-Q7cpW81XhbfNDP8_KwzMFPmAJyHQ@mail.gmail.com>

Hi all,

I?d love to get your feedback on the conflicted package, which provides an
alternative strategy for resolving ambiugous function names (i.e. when
multiple packages provide identically named functions). conflicted 0.1.0
is already on CRAN, but I?m currently preparing a revision
(<https://github.com/r-lib/conflicted>), and looking for feedback.

As you are no doubt aware, R?s default approach means that the most
recently loaded package ?wins? any conflicts. You do get a message about
conflicts on load, but I see a lot newer R users experiencing problems
caused by function conflicts. I think there are three primary reasons:

-   People don?t read messages about conflicts. Even if you are
    conscientious and do read the messages, it?s hard to notice a single
    new conflict caused by a package upgrade.

-   The warning and the problem may be quite far apart. If you load all
    your packages at the top of the script, it may potentially be 100s
    of lines before you encounter a conflict.

-   The error messages caused by conflicts are cryptic because you end
    up calling a function with utterly unexpected arguments.

For these reasons, conflicted takes an alternative approach, forcing the
user to explicitly disambiguate any conflicts:

    library(conflicted)
    library(dplyr)
    library(MASS)

    select
    #> Error: [conflicted] `select` found in 2 packages.
    #> Either pick the one you want with `::`
    #> * MASS::select
    #> * dplyr::select
    #> Or declare a preference with `conflicted_prefer()`
    #> * conflict_prefer("select", "MASS")
    #> * conflict_prefer("select", "dplyr")

conflicted works by attaching a new ?conflicted? environment just after
the global environment. This environment contains an active binding for
any ambiguous bindings. The conflicted environment also contains
bindings for `library()` and `require()` that rebuild the conflicted
environemnt suppress default reporting (but are otherwise thin wrapeprs
around the base equivalents).

conflicted also provides a `conflict_scout()` helper which you can use
to see what?s going on:

    conflict_scout(c("dplyr", "MASS"))
    #> 1 conflict:
    #> * `select`: dplyr, MASS

conflicted applies a few heuristics to minimise false positives (at the
cost of introducing a few false negatives). The overarching goal is to
ensure that code behaves identically regardless of the order in which
packages are attached.

-   A number of packages provide a function that appears to conflict
    with a function in a base package, but they follow the superset
    principle (i.e. they only extend the API, as explained to me by
    Herv? Pages).

    conflicted assumes that packages adhere to the superset principle,
    which appears to be true in most of the cases that I?ve seen. For
    example, the lubridate package provides `as.difftime()` and `date()`
    which extend the behaviour of base functions, and provides S4
    generics for the set operators.

        conflict_scout(c("lubridate", "base"))
        #> 5 conflicts:
        #> * `as.difftime`: [lubridate]
        #> * `date`       : [lubridate]
        #> * `intersect`  : [lubridate]
        #> * `setdiff`    : [lubridate]
        #> * `union`      : [lubridate]

    There are two popular functions that don?t adhere to this principle:
    `dplyr::filter()` and `dplyr::lag()` :(. conflicted handles these
    special cases so they correctly generate conflicts. (I sure wish I?d
    know about the subset principle when creating dplyr!)

        conflict_scout(c("dplyr", "stats"))
        #> 2 conflicts:
        #> * `filter`: dplyr, stats
        #> * `lag`   : dplyr, stats

-   Deprecated functions should never win a conflict, so conflicted
    checks for use of `.Deprecated()`. This rule is very useful when
    moving functions from one package to another. For example, many
    devtools functions were moved to usethis, and conflicted ensures
    that you always get the non-deprecated version, regardess of package
    attach order:

        head(conflict_scout(c("devtools", "usethis")))
        #> 26 conflicts:
        #> * `use_appveyor`       : [usethis]
        #> * `use_build_ignore`   : [usethis]
        #> * `use_code_of_conduct`: [usethis]
        #> * `use_coverage`       : [usethis]
        #> * `use_cran_badge`     : [usethis]
        #> * `use_cran_comments`  : [usethis]
        #> ...

Finally, as mentioned above, the user can declare preferences:

    conflict_prefer("select", "MASS")
    #> [conflicted] Will prefer MASS::select over any other package
    conflict_scout(c("dplyr", "MASS"))
    #> 1 conflict:
    #> * `select`: [MASS]

I?d love to hear what people think about the general idea, and if there
are any obviously missing pieces.

Thanks!

Hadley


-- 
http://hadley.nz


From murdoch@dunc@n @ending from gm@il@com  Thu Aug 23 22:46:29 2018
From: murdoch@dunc@n @ending from gm@il@com (Duncan Murdoch)
Date: Thu, 23 Aug 2018 16:46:29 -0400
Subject: [Rd] conflicted: an alternative conflict resolution strategy
In-Reply-To: <CABdHhvHyV7YXUp9fChvpH-Q7cpW81XhbfNDP8_KwzMFPmAJyHQ@mail.gmail.com>
References: <CABdHhvHyV7YXUp9fChvpH-Q7cpW81XhbfNDP8_KwzMFPmAJyHQ@mail.gmail.com>
Message-ID: <adf26ba5-6b0a-e7ad-f8e3-50abdf7d0493@gmail.com>

First, some general comments:

This sounds like a useful package.

I would guess it has very little impact on runtime efficiency except 
when attaching a new package; have you checked that?

I am not so sure about your heuristics.  Can they be disabled, so the 
user is always forced to make the choice?  Even when a function is 
intended to adhere to the superset principle, they don't always get it 
right, so a really careful user should always do explicit disambiguation.

And of course, if users wrote most of their long scripts as packages 
instead of as long scripts, the ambiguity issue would arise far less 
often, because namespaces in packages are intended to solve the same 
problem as your package does.

One more comment inline about a typo, possibly in an error message.

Duncan Murdoch

On 23/08/2018 2:31 PM, Hadley Wickham wrote:
> Hi all,
> 
> I?d love to get your feedback on the conflicted package, which provides an
> alternative strategy for resolving ambiugous function names (i.e. when
> multiple packages provide identically named functions). conflicted 0.1.0
> is already on CRAN, but I?m currently preparing a revision
> (<https://github.com/r-lib/conflicted>), and looking for feedback.
> 
> As you are no doubt aware, R?s default approach means that the most
> recently loaded package ?wins? any conflicts. You do get a message about
> conflicts on load, but I see a lot newer R users experiencing problems
> caused by function conflicts. I think there are three primary reasons:
> 
> -   People don?t read messages about conflicts. Even if you are
>      conscientious and do read the messages, it?s hard to notice a single
>      new conflict caused by a package upgrade.
> 
> -   The warning and the problem may be quite far apart. If you load all
>      your packages at the top of the script, it may potentially be 100s
>      of lines before you encounter a conflict.
> 
> -   The error messages caused by conflicts are cryptic because you end
>      up calling a function with utterly unexpected arguments.
> 
> For these reasons, conflicted takes an alternative approach, forcing the
> user to explicitly disambiguate any conflicts:
> 
>      library(conflicted)
>      library(dplyr)
>      library(MASS)
> 
>      select
>      #> Error: [conflicted] `select` found in 2 packages.
>      #> Either pick the one you want with `::`
>      #> * MASS::select
>      #> * dplyr::select
>      #> Or declare a preference with `conflicted_prefer()`
>      #> * conflict_prefer("select", "MASS")
>      #> * conflict_prefer("select", "dplyr")

I don't know if this is a typo in your r-devel message or a typo in the 
error message, but you say `conflicted_prefer()` in one place and 
conflict_prefer() in the other.

> 
> conflicted works by attaching a new ?conflicted? environment just after
> the global environment. This environment contains an active binding for
> any ambiguous bindings. The conflicted environment also contains
> bindings for `library()` and `require()` that rebuild the conflicted
> environemnt suppress default reporting (but are otherwise thin wrapeprs
> around the base equivalents).
> 
> conflicted also provides a `conflict_scout()` helper which you can use
> to see what?s going on:
> 
>      conflict_scout(c("dplyr", "MASS"))
>      #> 1 conflict:
>      #> * `select`: dplyr, MASS
> 
> conflicted applies a few heuristics to minimise false positives (at the
> cost of introducing a few false negatives). The overarching goal is to
> ensure that code behaves identically regardless of the order in which
> packages are attached.
> 
> -   A number of packages provide a function that appears to conflict
>      with a function in a base package, but they follow the superset
>      principle (i.e. they only extend the API, as explained to me by
>      Herv? Pages).
> 
>      conflicted assumes that packages adhere to the superset principle,
>      which appears to be true in most of the cases that I?ve seen. For
>      example, the lubridate package provides `as.difftime()` and `date()`
>      which extend the behaviour of base functions, and provides S4
>      generics for the set operators.
> 
>          conflict_scout(c("lubridate", "base"))
>          #> 5 conflicts:
>          #> * `as.difftime`: [lubridate]
>          #> * `date`       : [lubridate]
>          #> * `intersect`  : [lubridate]
>          #> * `setdiff`    : [lubridate]
>          #> * `union`      : [lubridate]
> 
>      There are two popular functions that don?t adhere to this principle:
>      `dplyr::filter()` and `dplyr::lag()` :(. conflicted handles these
>      special cases so they correctly generate conflicts. (I sure wish I?d
>      know about the subset principle when creating dplyr!)
> 
>          conflict_scout(c("dplyr", "stats"))
>          #> 2 conflicts:
>          #> * `filter`: dplyr, stats
>          #> * `lag`   : dplyr, stats
> 
> -   Deprecated functions should never win a conflict, so conflicted
>      checks for use of `.Deprecated()`. This rule is very useful when
>      moving functions from one package to another. For example, many
>      devtools functions were moved to usethis, and conflicted ensures
>      that you always get the non-deprecated version, regardess of package
>      attach order:
> 
>          head(conflict_scout(c("devtools", "usethis")))
>          #> 26 conflicts:
>          #> * `use_appveyor`       : [usethis]
>          #> * `use_build_ignore`   : [usethis]
>          #> * `use_code_of_conduct`: [usethis]
>          #> * `use_coverage`       : [usethis]
>          #> * `use_cran_badge`     : [usethis]
>          #> * `use_cran_comments`  : [usethis]
>          #> ...
> 
> Finally, as mentioned above, the user can declare preferences:
> 
>      conflict_prefer("select", "MASS")
>      #> [conflicted] Will prefer MASS::select over any other package
>      conflict_scout(c("dplyr", "MASS"))
>      #> 1 conflict:
>      #> * `select`: [MASS]
> 
> I?d love to hear what people think about the general idea, and if there
> are any obviously missing pieces.
> 
> Thanks!
> 
> Hadley
> 
>


From j@ri@ok@@nen @ending from oulu@fi  Fri Aug 24 09:12:13 2018
From: j@ri@ok@@nen @ending from oulu@fi (Jari Oksanen)
Date: Fri, 24 Aug 2018 07:12:13 +0000
Subject: [Rd] conflicted: an alternative conflict resolution strategy
In-Reply-To: <adf26ba5-6b0a-e7ad-f8e3-50abdf7d0493@gmail.com>
References: <CABdHhvHyV7YXUp9fChvpH-Q7cpW81XhbfNDP8_KwzMFPmAJyHQ@mail.gmail.com>
 <adf26ba5-6b0a-e7ad-f8e3-50abdf7d0493@gmail.com>
Message-ID: <EF40A9BC-8A83-44A9-9736-9C752BF2C41D@oulu.fi>

If you have to load two packages which both export the same name in their namespaces, namespace does not help in resolving which synonymous function to use. Neither does it help to have a package instead of a script as long as you end up loading two namespaces with name conflicts. The order of importing namespaces can also be difficult to control, because you may end up loading a namespace already when you start your R with a saved workspace. Moving a function to another package may be a transitional issue which disappears when both packages are at their final stages, but if you use the recommend deprecation stage, the same names can live together for a long time. So this package is a good idea, and preferably base R should be able to handle the issue of choosing between exported synonymous functions.

This has bitten me several times in package development, and with growing CRAN it is a growing problem. Package authors often have poor control of the issue, as they do not know what packages users use. Now we can only have a FAQ that tells that a certain error message does not come from a function in our package, but from some other package having a synonymous function that was used instead.

cheers, Jari Oksanen

On 23 Aug 2018, at 23:46 pm, Duncan Murdoch <murdoch.duncan at gmail.com<mailto:murdoch.duncan at gmail.com>> wrote:

First, some general comments:

This sounds like a useful package.

I would guess it has very little impact on runtime efficiency except when attaching a new package; have you checked that?

I am not so sure about your heuristics.  Can they be disabled, so the user is always forced to make the choice?  Even when a function is intended to adhere to the superset principle, they don't always get it right, so a really careful user should always do explicit disambiguation.

And of course, if users wrote most of their long scripts as packages instead of as long scripts, the ambiguity issue would arise far less often, because namespaces in packages are intended to solve the same problem as your package does.

One more comment inline about a typo, possibly in an error message.

Duncan Murdoch

On 23/08/2018 2:31 PM, Hadley Wickham wrote:
Hi all,
I?d love to get your feedback on the conflicted package, which provides an
alternative strategy for resolving ambiugous function names (i.e. when
multiple packages provide identically named functions). conflicted 0.1.0
is already on CRAN, but I?m currently preparing a revision
(<https://github.com/r-lib/conflicted>), and looking for feedback.
As you are no doubt aware, R?s default approach means that the most
recently loaded package ?wins? any conflicts. You do get a message about
conflicts on load, but I see a lot newer R users experiencing problems
caused by function conflicts. I think there are three primary reasons:
-   People don?t read messages about conflicts. Even if you are
    conscientious and do read the messages, it?s hard to notice a single
    new conflict caused by a package upgrade.
-   The warning and the problem may be quite far apart. If you load all
    your packages at the top of the script, it may potentially be 100s
    of lines before you encounter a conflict.
-   The error messages caused by conflicts are cryptic because you end
    up calling a function with utterly unexpected arguments.
For these reasons, conflicted takes an alternative approach, forcing the
user to explicitly disambiguate any conflicts:
    library(conflicted)
    library(dplyr)
    library(MASS)
    select
    #> Error: [conflicted] `select` found in 2 packages.
    #> Either pick the one you want with `::`
    #> * MASS::select
    #> * dplyr::select
    #> Or declare a preference with `conflicted_prefer()`
    #> * conflict_prefer("select", "MASS")
    #> * conflict_prefer("select", "dplyr")

I don't know if this is a typo in your r-devel message or a typo in the error message, but you say `conflicted_prefer()` in one place and conflict_prefer() in the other.

conflicted works by attaching a new ?conflicted? environment just after
the global environment. This environment contains an active binding for
any ambiguous bindings. The conflicted environment also contains
bindings for `library()` and `require()` that rebuild the conflicted
environemnt suppress default reporting (but are otherwise thin wrapeprs
around the base equivalents).
conflicted also provides a `conflict_scout()` helper which you can use
to see what?s going on:
    conflict_scout(c("dplyr", "MASS"))
    #> 1 conflict:
    #> * `select`: dplyr, MASS
conflicted applies a few heuristics to minimise false positives (at the
cost of introducing a few false negatives). The overarching goal is to
ensure that code behaves identically regardless of the order in which
packages are attached.
-   A number of packages provide a function that appears to conflict
    with a function in a base package, but they follow the superset
    principle (i.e. they only extend the API, as explained to me by
    Herv? Pages).
    conflicted assumes that packages adhere to the superset principle,
    which appears to be true in most of the cases that I?ve seen. For
    example, the lubridate package provides `as.difftime()` and `date()`
    which extend the behaviour of base functions, and provides S4
    generics for the set operators.
        conflict_scout(c("lubridate", "base"))
        #> 5 conflicts:
        #> * `as.difftime`: [lubridate]
        #> * `date`       : [lubridate]
        #> * `intersect`  : [lubridate]
        #> * `setdiff`    : [lubridate]
        #> * `union`      : [lubridate]
    There are two popular functions that don?t adhere to this principle:
    `dplyr::filter()` and `dplyr::lag()` :(. conflicted handles these
    special cases so they correctly generate conflicts. (I sure wish I?d
    know about the subset principle when creating dplyr!)
        conflict_scout(c("dplyr", "stats"))
        #> 2 conflicts:
        #> * `filter`: dplyr, stats
        #> * `lag`   : dplyr, stats
-   Deprecated functions should never win a conflict, so conflicted
    checks for use of `.Deprecated()`. This rule is very useful when
    moving functions from one package to another. For example, many
    devtools functions were moved to usethis, and conflicted ensures
    that you always get the non-deprecated version, regardess of package
    attach order:
        head(conflict_scout(c("devtools", "usethis")))
        #> 26 conflicts:
        #> * `use_appveyor`       : [usethis]
        #> * `use_build_ignore`   : [usethis]
        #> * `use_code_of_conduct`: [usethis]
        #> * `use_coverage`       : [usethis]
        #> * `use_cran_badge`     : [usethis]
        #> * `use_cran_comments`  : [usethis]
        #> ...
Finally, as mentioned above, the user can declare preferences:
    conflict_prefer("select", "MASS")
    #> [conflicted] Will prefer MASS::select over any other package
    conflict_scout(c("dplyr", "MASS"))
    #> 1 conflict:
    #> * `select`: [MASS]
I?d love to hear what people think about the general idea, and if there
are any obviously missing pieces.
Thanks!
Hadley


______________________________________________
R-devel at r-project.org<mailto:R-devel at r-project.org> mailing list
https://stat.ethz.ch/mailman/listinfo/r-devel


	[[alternative HTML version deleted]]


From c@@rdi@g@bor @ending from gm@il@com  Fri Aug 24 10:55:37 2018
From: c@@rdi@g@bor @ending from gm@il@com (=?UTF-8?B?R8OhYm9yIENzw6FyZGk=?=)
Date: Fri, 24 Aug 2018 10:55:37 +0200
Subject: [Rd] Translating Rd files
In-Reply-To: <d603cc07-d0cb-36b7-10e8-4f02ebf28a5a@gmail.com>
References: <CABtg=K=AZzo9UTyR6fKM=kLChOZfbPdqbT_wzgAh2Am0xpkWDQ@mail.gmail.com>
 <d603cc07-d0cb-36b7-10e8-4f02ebf28a5a@gmail.com>
Message-ID: <CABtg=Kn=cema5zXhE0v73ipQm7bnarwRmeDhvS_XrsuQkoSVSg@mail.gmail.com>

On Wed, Aug 22, 2018 at 12:30 AM Duncan Murdoch
<murdoch.duncan at gmail.com> wrote:
>
> On 21/08/2018 5:53 PM, G?bor Cs?rdi wrote:
> > Dear All,
> >
> > are there any resources (code, guidelines, anything) for translating Rd files?
> > As far as I can tell, the help system does not have support for this.
> > Have I missed something? Is such support desired?
>
> As of last year, support for this was seen as desirable but no action
> had been taken to put it in place.  The thinking was that it was too
> much work for the translators to have to follow every update to each
> help page, so the translated pages would likely soon be out of date.

I agree that this the biggest concern. In some cases maybe one can ensure
that the translated docs are still in sync. E.g. if the translated man page is
within the translated package. Then it is the package authors' responsibility
to keep it updated (or to remove it), and the regular package check tools can
be used as well.

This does make collaboration with "external" translators and package updates
somewhat more cumbersome, and it does not scale very well. But it still could
be a start. Most packages will not have a hundred translations any time soon.

If we allow translations in separate packages, that does make things more
complicated.

Gabor

> Duncan Murdoch
>
> >
> > I am thinking about a way to register manual pages in different
> > languages, and then`help()` would bring up the page with the preferred
> > language, defaulting to the current locale.
>
>


From jori@mey@ @ending from gm@il@com  Fri Aug 24 11:28:28 2018
From: jori@mey@ @ending from gm@il@com (Joris Meys)
Date: Fri, 24 Aug 2018 11:28:28 +0200
Subject: [Rd] conflicted: an alternative conflict resolution strategy
In-Reply-To: <CABdHhvHyV7YXUp9fChvpH-Q7cpW81XhbfNDP8_KwzMFPmAJyHQ@mail.gmail.com>
References: <CABdHhvHyV7YXUp9fChvpH-Q7cpW81XhbfNDP8_KwzMFPmAJyHQ@mail.gmail.com>
Message-ID: <CAO1zAVa-B909i_+SxLuvWJ4q3MhHVFUNuDqpU-aiON+yLGWALQ@mail.gmail.com>

Dear Hadley,

There's been some mails from you lately about packages on R-devel. I would
argue that the appropriate list for that is R-pkg-devel, as I've been told
myself not too long ago. People might get confused and think this is about
a change to R itself, which it obviously is not.

Kind regards
Joris

On Thu, Aug 23, 2018 at 8:32 PM Hadley Wickham <h.wickham at gmail.com> wrote:

> Hi all,
>
> I?d love to get your feedback on the conflicted package, which provides an
> alternative strategy for resolving ambiugous function names (i.e. when
> multiple packages provide identically named functions). conflicted 0.1.0
> is already on CRAN, but I?m currently preparing a revision
> (<https://github.com/r-lib/conflicted>), and looking for feedback.
>
> As you are no doubt aware, R?s default approach means that the most
> recently loaded package ?wins? any conflicts. You do get a message about
> conflicts on load, but I see a lot newer R users experiencing problems
> caused by function conflicts. I think there are three primary reasons:
>
> -   People don?t read messages about conflicts. Even if you are
>     conscientious and do read the messages, it?s hard to notice a single
>     new conflict caused by a package upgrade.
>
> -   The warning and the problem may be quite far apart. If you load all
>     your packages at the top of the script, it may potentially be 100s
>     of lines before you encounter a conflict.
>
> -   The error messages caused by conflicts are cryptic because you end
>     up calling a function with utterly unexpected arguments.
>
> For these reasons, conflicted takes an alternative approach, forcing the
> user to explicitly disambiguate any conflicts:
>
>     library(conflicted)
>     library(dplyr)
>     library(MASS)
>
>     select
>     #> Error: [conflicted] `select` found in 2 packages.
>     #> Either pick the one you want with `::`
>     #> * MASS::select
>     #> * dplyr::select
>     #> Or declare a preference with `conflicted_prefer()`
>     #> * conflict_prefer("select", "MASS")
>     #> * conflict_prefer("select", "dplyr")
>
> conflicted works by attaching a new ?conflicted? environment just after
> the global environment. This environment contains an active binding for
> any ambiguous bindings. The conflicted environment also contains
> bindings for `library()` and `require()` that rebuild the conflicted
> environemnt suppress default reporting (but are otherwise thin wrapeprs
> around the base equivalents).
>
> conflicted also provides a `conflict_scout()` helper which you can use
> to see what?s going on:
>
>     conflict_scout(c("dplyr", "MASS"))
>     #> 1 conflict:
>     #> * `select`: dplyr, MASS
>
> conflicted applies a few heuristics to minimise false positives (at the
> cost of introducing a few false negatives). The overarching goal is to
> ensure that code behaves identically regardless of the order in which
> packages are attached.
>
> -   A number of packages provide a function that appears to conflict
>     with a function in a base package, but they follow the superset
>     principle (i.e. they only extend the API, as explained to me by
>     Herv? Pages).
>
>     conflicted assumes that packages adhere to the superset principle,
>     which appears to be true in most of the cases that I?ve seen. For
>     example, the lubridate package provides `as.difftime()` and `date()`
>     which extend the behaviour of base functions, and provides S4
>     generics for the set operators.
>
>         conflict_scout(c("lubridate", "base"))
>         #> 5 conflicts:
>         #> * `as.difftime`: [lubridate]
>         #> * `date`       : [lubridate]
>         #> * `intersect`  : [lubridate]
>         #> * `setdiff`    : [lubridate]
>         #> * `union`      : [lubridate]
>
>     There are two popular functions that don?t adhere to this principle:
>     `dplyr::filter()` and `dplyr::lag()` :(. conflicted handles these
>     special cases so they correctly generate conflicts. (I sure wish I?d
>     know about the subset principle when creating dplyr!)
>
>         conflict_scout(c("dplyr", "stats"))
>         #> 2 conflicts:
>         #> * `filter`: dplyr, stats
>         #> * `lag`   : dplyr, stats
>
> -   Deprecated functions should never win a conflict, so conflicted
>     checks for use of `.Deprecated()`. This rule is very useful when
>     moving functions from one package to another. For example, many
>     devtools functions were moved to usethis, and conflicted ensures
>     that you always get the non-deprecated version, regardess of package
>     attach order:
>
>         head(conflict_scout(c("devtools", "usethis")))
>         #> 26 conflicts:
>         #> * `use_appveyor`       : [usethis]
>         #> * `use_build_ignore`   : [usethis]
>         #> * `use_code_of_conduct`: [usethis]
>         #> * `use_coverage`       : [usethis]
>         #> * `use_cran_badge`     : [usethis]
>         #> * `use_cran_comments`  : [usethis]
>         #> ...
>
> Finally, as mentioned above, the user can declare preferences:
>
>     conflict_prefer("select", "MASS")
>     #> [conflicted] Will prefer MASS::select over any other package
>     conflict_scout(c("dplyr", "MASS"))
>     #> 1 conflict:
>     #> * `select`: [MASS]
>
> I?d love to hear what people think about the general idea, and if there
> are any obviously missing pieces.
>
> Thanks!
>
> Hadley
>
>
> --
> http://hadley.nz
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


-- 
Joris Meys
Statistical consultant

Department of Data Analysis and Mathematical Modelling
Ghent University
Coupure Links 653, B-9000 Gent (Belgium)
<https://maps.google.com/?q=Coupure+links+653,%C2%A0B-9000+Gent,%C2%A0Belgium&entry=gmail&source=g>

-----------
Biowiskundedagen 2017-2018
http://www.biowiskundedagen.ugent.be/

-------------------------------
Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php

	[[alternative HTML version deleted]]


From h@wickh@m @ending from gm@il@com  Fri Aug 24 14:18:39 2018
From: h@wickh@m @ending from gm@il@com (Hadley Wickham)
Date: Fri, 24 Aug 2018 07:18:39 -0500
Subject: [Rd] conflicted: an alternative conflict resolution strategy
In-Reply-To: <adf26ba5-6b0a-e7ad-f8e3-50abdf7d0493@gmail.com>
References: <CABdHhvHyV7YXUp9fChvpH-Q7cpW81XhbfNDP8_KwzMFPmAJyHQ@mail.gmail.com>
 <adf26ba5-6b0a-e7ad-f8e3-50abdf7d0493@gmail.com>
Message-ID: <CABdHhvEg8KPqo2+sZg50UcDxpg8O9bb5CQvSMxO_bRZqw1g0EQ@mail.gmail.com>

On Thu, Aug 23, 2018 at 3:46 PM Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
>
> First, some general comments:
>
> This sounds like a useful package.
>
> I would guess it has very little impact on runtime efficiency except
> when attaching a new package; have you checked that?

It adds one extra element to the search path, so the impact on speed
should be equivalent to loading one additional package (i.e.
negligible)

I've also done some benchmarking to see the impact on calls to
library(). These are now a little outdated (because I've added more
heuristics so I should re-do), but previously conflicted added about
100 ms overhead to a library() call when I had ~170 packages loaded
(the most I could load without running out of dlls).

> I am not so sure about your heuristics.  Can they be disabled, so the
> user is always forced to make the choice?  Even when a function is
> intended to adhere to the superset principle, they don't always get it
> right, so a really careful user should always do explicit disambiguation.

That is a good question - my intuition is always to start with less
user control as it makes it easier to get the core ideas right, and
it's easy to add more control later (whereas if you later take it
away, people get unhappy). Maybe it's natural to have a function that
does the opposite of conflict_prefer(), and declare that something
that doesn't appear to be a conflict actually is?

I don't think that an option to suppress the superset principle
altogether will work - my sense is that it will generate too many
false positives, to the point where you'll get frustrated and stop
using conflicted.

> And of course, if users wrote most of their long scripts as packages
> instead of as long scripts, the ambiguity issue would arise far less
> often, because namespaces in packages are intended to solve the same
> problem as your package does.

Agreed.

> One more comment inline about a typo, possibly in an error message.

Thanks for spotting; fixed in devel now.

Hadley


-- 
http://hadley.nz


From h@wickh@m @ending from gm@il@com  Fri Aug 24 14:27:15 2018
From: h@wickh@m @ending from gm@il@com (Hadley Wickham)
Date: Fri, 24 Aug 2018 07:27:15 -0500
Subject: [Rd] conflicted: an alternative conflict resolution strategy
In-Reply-To: <CAO1zAVa-B909i_+SxLuvWJ4q3MhHVFUNuDqpU-aiON+yLGWALQ@mail.gmail.com>
References: <CABdHhvHyV7YXUp9fChvpH-Q7cpW81XhbfNDP8_KwzMFPmAJyHQ@mail.gmail.com>
 <CAO1zAVa-B909i_+SxLuvWJ4q3MhHVFUNuDqpU-aiON+yLGWALQ@mail.gmail.com>
Message-ID: <CABdHhvHHkz1gpdN7akNTDhKT7Kw3zx3h+PYTQcAOUb5r_wAq=g@mail.gmail.com>

On Fri, Aug 24, 2018 at 4:28 AM Joris Meys <jorismeys at gmail.com> wrote:
>
> Dear Hadley,
>
> There's been some mails from you lately about packages on R-devel. I would argue that the appropriate list for that is R-pkg-devel, as I've been told myself not too long ago. People might get confused and think this is about a change to R itself, which it obviously is not.

The description for R-pkg-devel states:

> This list is to get help about package development in R. The goal of the list is to provide a forum for learning about the package development process. We hope to build a community of R package developers who can help each other solve problems, and reduce some of the burden on the CRAN maintainers. If you are having problems developing a package or passing R CMD check, this is the place to ask!

The description for R-devel states:

> This list is intended for questions and discussion about code development in R. Questions likely to prompt discussion unintelligible to non-programmers or topics that are too technical for R-help's audience should go to R-devel, unless they are specifically about problems in R package development where the R-package-devel list is rather appropriate, see the posting guide section. The main R mailing list is R-help.

My questions are not about how to develop a package, R CMD check, or
how to get it on CRAN, but instead about the semantics of the packages
I am working on. My opinion is supported by the fact that a number of
members of the R core team have responded (both on list and off) and
have not expressed concern about my choice of venue.

That said, I am happy to change venues (or simply not email at all) if
there is widespread concern that my emails are inappropriate.

Hadley

-- 
http://hadley.nz


From jori@mey@ @ending from gm@il@com  Fri Aug 24 14:38:05 2018
From: jori@mey@ @ending from gm@il@com (Joris Meys)
Date: Fri, 24 Aug 2018 14:38:05 +0200
Subject: [Rd] conflicted: an alternative conflict resolution strategy
In-Reply-To: <CABdHhvHHkz1gpdN7akNTDhKT7Kw3zx3h+PYTQcAOUb5r_wAq=g@mail.gmail.com>
References: <CABdHhvHyV7YXUp9fChvpH-Q7cpW81XhbfNDP8_KwzMFPmAJyHQ@mail.gmail.com>
 <CAO1zAVa-B909i_+SxLuvWJ4q3MhHVFUNuDqpU-aiON+yLGWALQ@mail.gmail.com>
 <CABdHhvHHkz1gpdN7akNTDhKT7Kw3zx3h+PYTQcAOUb5r_wAq=g@mail.gmail.com>
Message-ID: <CAO1zAVaM6zjjYTs9iQUpp0NDXzLRSQ_xCnNWkUbcfY5K0dFS2w@mail.gmail.com>

On Fri, Aug 24, 2018 at 2:27 PM Hadley Wickham <h.wickham at gmail.com> wrote:

>
> My questions are not about how to develop a package, R CMD check, or
> how to get it on CRAN, but instead about the semantics of the packages
> I am working on. My opinion is supported by the fact that a number of
> members of the R core team have responded (both on list and off) and
> have not expressed concern about my choice of venue.
>

If those moderating the lists are fine with it, all good.

Cheers
Joris


> That said, I am happy to change venues (or simply not email at all) if
> there is widespread concern that my emails are inappropriate.
>
> Hadley
>
> --
> http://hadley.nz
>


-- 
Joris Meys
Statistical consultant

Department of Data Analysis and Mathematical Modelling
Ghent University
Coupure Links 653, B-9000 Gent (Belgium)
<https://maps.google.com/?q=Coupure+links+653,%C2%A0B-9000+Gent,%C2%A0Belgium&entry=gmail&source=g>

-----------
Biowiskundedagen 2017-2018
http://www.biowiskundedagen.ugent.be/

-------------------------------
Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php

	[[alternative HTML version deleted]]


From edzer@pebe@m@ @ending from uni-muen@ter@de  Fri Aug 24 19:53:15 2018
From: edzer@pebe@m@ @ending from uni-muen@ter@de (Edzer Pebesma)
Date: Fri, 24 Aug 2018 19:53:15 +0200
Subject: [Rd] plotmath degree symbol
Message-ID: <d83487b0-3b9b-43ab-bfd7-6526ee63012a@uni-muenster.de>

In plotmath expressions, R's degree symbol, e.g. shown by

plot(1, main = parse(text = "1*degree*C"))

has sunk to halfway the text line, instead of touching its top. In older
R versions this looked much better.
-- 
Edzer Pebesma
Institute for Geoinformatics
Heisenbergstrasse 2, 48151 Muenster, Germany
Phone: +49 251 8333081


From henrik@bengt@@on @ending from gm@il@com  Fri Aug 24 19:55:54 2018
From: henrik@bengt@@on @ending from gm@il@com (Henrik Bengtsson)
Date: Fri, 24 Aug 2018 10:55:54 -0700
Subject: [Rd] True length - length(unclass(x)) - without having to call
 unclass()?
Message-ID: <CAFDcVCQdmLYM4gPj_zoz4KeoHGGUzJux9hLK_3vMVaeWUsrHQQ@mail.gmail.com>

Is there a low-level function that returns the length of an object 'x'
- the length that for instance .subset(x) and .subset2(x) see? An
obvious candidate would be to use:

.length <- function(x) length(unclass(x))

However, I'm concerned that calling unclass(x) may trigger an
expensive copy internally in some cases.  Is that concern unfounded?

Thxs,

Henrik


From murdoch@dunc@n @ending from gm@il@com  Fri Aug 24 20:33:41 2018
From: murdoch@dunc@n @ending from gm@il@com (Duncan Murdoch)
Date: Fri, 24 Aug 2018 14:33:41 -0400
Subject: [Rd] conflicted: an alternative conflict resolution strategy
In-Reply-To: <EF40A9BC-8A83-44A9-9736-9C752BF2C41D@oulu.fi>
References: <CABdHhvHyV7YXUp9fChvpH-Q7cpW81XhbfNDP8_KwzMFPmAJyHQ@mail.gmail.com>
 <adf26ba5-6b0a-e7ad-f8e3-50abdf7d0493@gmail.com>
 <EF40A9BC-8A83-44A9-9736-9C752BF2C41D@oulu.fi>
Message-ID: <c4e90d2c-6256-d684-b427-2f97048630a8@gmail.com>

On 24/08/2018 3:12 AM, Jari Oksanen wrote:
> If you have to load two packages which both export the same name in 
> their namespaces, namespace does not help in resolving which synonymous 
> function to use. Neither does it help to have a package instead of a 
> script as long as you end up loading two namespaces with name conflicts. 

You can't import the same name from two packages without getting an 
error message (at least when checking --as-cran, I'm not sure about 
vanilla checks), so this is already handled.

If you really only want one of the imports, then importing individual 
functions is the solution.  Don't import everything from the package. 
This is a good idea in any case.

If you want both of the imports, then there's the undocumented (?) 
ability to rename a function on import, as well as the documented 
possibility of using :: for one of them instead of importing it.

> The order of importing namespaces can also be difficult to control, 
> because you may end up loading a namespace already when you start your R 
> with a saved workspace.

That doesn't make sense in the context of a package.  Packages import 
what they ask to import. The user's workspace is irrelevant to code 
within the package if it does its imports properly.  You can reference 
functions that are not imported, but you get a message when you run 
checks to tell you not to do that.

Duncan Murdoch

  Moving a function to another package may be a
> transitional issue which disappears when both packages are at their 
> final stages, but if you use the recommend deprecation stage, the same 
> names can live together for a long time. So this package is a good idea, 
> and preferably base R should be able to handle the issue of choosing 
> between exported synonymous functions.
> 
> This has bitten me several times in package development, and with 
> growing CRAN it is a growing problem. Package authors often have poor 
> control of the issue, as they do not know what packages users use. Now 
> we can only have a FAQ that tells that a certain error message does not 
> come from a function in our package, but from some other package having 
> a synonymous function that was used instead.
> 
> cheers, Jari Oksanen
> 
>> On 23 Aug 2018, at 23:46 pm, Duncan Murdoch <murdoch.duncan at gmail.com 
>> <mailto:murdoch.duncan at gmail.com>> wrote:
>>
>> First, some general comments:
>>
>> This sounds like a useful package.
>>
>> I would guess it has very little impact on runtime efficiency except 
>> when attaching a new package; have you checked that?
>>
>> I am not so sure about your heuristics. ?Can they be disabled, so the 
>> user is always forced to make the choice? ?Even when a function is 
>> intended to adhere to the superset principle, they don't always get it 
>> right, so a really careful user should always do explicit disambiguation.
>>
>> And of course, if users wrote most of their long scripts as packages 
>> instead of as long scripts, the ambiguity issue would arise far less 
>> often, because namespaces in packages are intended to solve the same 
>> problem as your package does.
>>
>> One more comment inline about a typo, possibly in an error message.
>>
>> Duncan Murdoch
>>
>> On 23/08/2018 2:31 PM, Hadley Wickham wrote:
>>> Hi all,
>>> I?d love to get your feedback on the conflicted package, which 
>>> provides an
>>> alternative strategy for resolving ambiugous function names (i.e. when
>>> multiple packages provide identically named functions). conflicted 0.1.0
>>> is already on CRAN, but I?m currently preparing a revision
>>> (<https://github.com/r-lib/conflicted>), and looking for feedback.
>>> As you are no doubt aware, R?s default approach means that the most
>>> recently loaded package ?wins? any conflicts. You do get a message about
>>> conflicts on load, but I see a lot newer R users experiencing problems
>>> caused by function conflicts. I think there are three primary reasons:
>>> - ??People don?t read messages about conflicts. Even if you are
>>> ????conscientious and do read the messages, it?s hard to notice a single
>>> ????new conflict caused by a package upgrade.
>>> - ??The warning and the problem may be quite far apart. If you load all
>>> ????your packages at the top of the script, it may potentially be 100s
>>> ????of lines before you encounter a conflict.
>>> - ??The error messages caused by conflicts are cryptic because you end
>>> ????up calling a function with utterly unexpected arguments.
>>> For these reasons, conflicted takes an alternative approach, forcing the
>>> user to explicitly disambiguate any conflicts:
>>> ????library(conflicted)
>>> ????library(dplyr)
>>> ????library(MASS)
>>> ????select
>>> ????#> Error: [conflicted] `select` found in 2 packages.
>>> ????#> Either pick the one you want with `::`
>>> ????#> * MASS::select
>>> ????#> * dplyr::select
>>> ????#> Or declare a preference with `conflicted_prefer()`
>>> ????#> * conflict_prefer("select", "MASS")
>>> ????#> * conflict_prefer("select", "dplyr")
>>
>> I don't know if this is a typo in your r-devel message or a typo in 
>> the error message, but you say `conflicted_prefer()` in one place and 
>> conflict_prefer() in the other.
>>
>>> conflicted works by attaching a new ?conflicted? environment just after
>>> the global environment. This environment contains an active binding for
>>> any ambiguous bindings. The conflicted environment also contains
>>> bindings for `library()` and `require()` that rebuild the conflicted
>>> environemnt suppress default reporting (but are otherwise thin wrapeprs
>>> around the base equivalents).
>>> conflicted also provides a `conflict_scout()` helper which you can use
>>> to see what?s going on:
>>> ????conflict_scout(c("dplyr", "MASS"))
>>> ????#> 1 conflict:
>>> ????#> * `select`: dplyr, MASS
>>> conflicted applies a few heuristics to minimise false positives (at the
>>> cost of introducing a few false negatives). The overarching goal is to
>>> ensure that code behaves identically regardless of the order in which
>>> packages are attached.
>>> - ??A number of packages provide a function that appears to conflict
>>> ????with a function in a base package, but they follow the superset
>>> ????principle (i.e. they only extend the API, as explained to me by
>>> ????Herv? Pages).
>>> ????conflicted assumes that packages adhere to the superset principle,
>>> ????which appears to be true in most of the cases that I?ve seen. For
>>> ????example, the lubridate package provides `as.difftime()` and `date()`
>>> ????which extend the behaviour of base functions, and provides S4
>>> ????generics for the set operators.
>>> ????????conflict_scout(c("lubridate", "base"))
>>> ????????#> 5 conflicts:
>>> ????????#> * `as.difftime`: [lubridate]
>>> ????????#> * `date` ??????: [lubridate]
>>> ????????#> * `intersect` ?: [lubridate]
>>> ????????#> * `setdiff` ???: [lubridate]
>>> ????????#> * `union` ?????: [lubridate]
>>> ????There are two popular functions that don?t adhere to this principle:
>>> ????`dplyr::filter()` and `dplyr::lag()` :(. conflicted handles these
>>> ????special cases so they correctly generate conflicts. (I sure wish I?d
>>> ????know about the subset principle when creating dplyr!)
>>> ????????conflict_scout(c("dplyr", "stats"))
>>> ????????#> 2 conflicts:
>>> ????????#> * `filter`: dplyr, stats
>>> ????????#> * `lag` ??: dplyr, stats
>>> - ??Deprecated functions should never win a conflict, so conflicted
>>> ????checks for use of `.Deprecated()`. This rule is very useful when
>>> ????moving functions from one package to another. For example, many
>>> ????devtools functions were moved to usethis, and conflicted ensures
>>> ????that you always get the non-deprecated version, regardess of package
>>> ????attach order:
>>> ????????head(conflict_scout(c("devtools", "usethis")))
>>> ????????#> 26 conflicts:
>>> ????????#> * `use_appveyor` ??????: [usethis]
>>> ????????#> * `use_build_ignore` ??: [usethis]
>>> ????????#> * `use_code_of_conduct`: [usethis]
>>> ????????#> * `use_coverage` ??????: [usethis]
>>> ????????#> * `use_cran_badge` ????: [usethis]
>>> ????????#> * `use_cran_comments` ?: [usethis]
>>> ????????#> ...
>>> Finally, as mentioned above, the user can declare preferences:
>>> ????conflict_prefer("select", "MASS")
>>> ????#> [conflicted] Will prefer MASS::select over any other package
>>> ????conflict_scout(c("dplyr", "MASS"))
>>> ????#> 1 conflict:
>>> ????#> * `select`: [MASS]
>>> I?d love to hear what people think about the general idea, and if there
>>> are any obviously missing pieces.
>>> Thanks!
>>> Hadley
>>>
>>
>> ______________________________________________
>> R-devel at r-project.org <mailto:R-devel at r-project.org> mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From becker@g@be @ending from gene@com  Fri Aug 24 21:37:40 2018
From: becker@g@be @ending from gene@com (Gabe Becker)
Date: Fri, 24 Aug 2018 12:37:40 -0700
Subject: [Rd] conflicted: an alternative conflict resolution strategy
In-Reply-To: <CABdHhvHyV7YXUp9fChvpH-Q7cpW81XhbfNDP8_KwzMFPmAJyHQ@mail.gmail.com>
References: <CABdHhvHyV7YXUp9fChvpH-Q7cpW81XhbfNDP8_KwzMFPmAJyHQ@mail.gmail.com>
Message-ID: <CAMFmJs=5wNzMkDjy1K6hTX1Vs6Hgk+r2BKZBnw_Z8nZG2CQC7Q@mail.gmail.com>

Hadley,

Overall seems like a cool and potentially really idea. I do have some
thoughts/feedback, which I've put in-line below

On Thu, Aug 23, 2018 at 11:31 AM, Hadley Wickham <h.wickham at gmail.com>
wrote:

>
> <snip>
>

> conflicted applies a few heuristics to minimise false positives (at the
> cost of introducing a few false negatives). The overarching goal is to
> ensure that code behaves identically regardless of the order in which
> packages are attached.
>
> -   A number of packages provide a function that appears to conflict
>     with a function in a base package, but they follow the superset
>     principle (i.e. they only extend the API, as explained to me by
>     Herv? Pages).
>
>     conflicted assumes that packages adhere to the superset principle,
>     which appears to be true in most of the cases that I?ve seen.


It seems that you may be able to strengthen this heuristic from a blanket
assumption to something more narrowly targeted by looking for one or more
of the following to confirm likely-superset adherence

   1. matching or purely extending formals (ie all the named arguments of
   base::fun match including order, and there are new arguments in pkg::fun
   only if base::fun takes ...)
   2. explicit call to  base::fun in the body of pkg::fun
   3. UseMethod(funname) and at least one provided S3 method calls base::fun
   4. S4 generic creation using fun or base::fun as the seeding/default
   method body or called from at least one method



> For
>     example, the lubridate package provides `as.difftime()` and `date()`
>     which extend the behaviour of base functions, and provides S4
>     generics for the set operators.
>
>         conflict_scout(c("lubridate", "base"))
>         #> 5 conflicts:
>         #> * `as.difftime`: [lubridate]
>         #> * `date`       : [lubridate]
>         #> * `intersect`  : [lubridate]
>         #> * `setdiff`    : [lubridate]
>         #> * `union`      : [lubridate]
>
>     There are two popular functions that don?t adhere to this principle:
>     `dplyr::filter()` and `dplyr::lag()` :(. conflicted handles these
>     special cases so they correctly generate conflicts. (I sure wish I?d
>     know about the subset principle when creating dplyr!)
>
>         conflict_scout(c("dplyr", "stats"))
>         #> 2 conflicts:
>         #> * `filter`: dplyr, stats
>         #> * `lag`   : dplyr, stats
>
> -   Deprecated functions should never win a conflict, so conflicted
>     checks for use of `.Deprecated()`. This rule is very useful when
>     moving functions from one package to another. For example, many
>     devtools functions were moved to usethis, and conflicted ensures
>     that you always get the non-deprecated version, regardess of package
>     attach order:
>

I would completely believe this rule is useful for refactoring as you
describe, but that is the "same function" case. For an end-user in the
"different function same symbol" case it's not at all clear to me that the
deprecated function should always win.

People sometimes use deprecated functions. It's not great, and eventually
they'll need to fix that for any given case, but imagine if you deprecated
the filter verb in dplyr (I know this will never happen, but I think it's
illustrative none the less).

Consider a piece of code someone wrote before this hypothetical deprecation
of filter. The fact that it's now deprecated certainly doesn't mean that
they secretly wanted stats::filter all along, right? Conflicted acting as
if it does will lead to them getting the exact kind of error you're looking
to protect them from, and with even less ability to understand why because
they are already doing "The right thing" to protect themselves by using
conflicted in the first place...


> Finally, as mentioned above, the user can declare preferences:
>
>     conflict_prefer("select", "MASS")
>     #> [conflicted] Will prefer MASS::select over any other package
>     conflict_scout(c("dplyr", "MASS"))
>     #> 1 conflict:
>     #> * `select`: [MASS]
>
>
I deeply worry about people putting this kind of thing, or even just
library(conflicted), in their .Rprofile and thus making their scripts
*substantially* less reproducible. Is that a consequence you have thought
about to this kind of functionality?

Best,
~G


> I?d love to hear what people think about the general idea, and if there
> are any obviously missing pieces.
>
> Thanks!
>
> Hadley
>
>
> --
> http://hadley.nz
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>
>
Best,
~G

-- 
Gabriel Becker, Ph.D
Scientist
Bioinformatics and Computational Biology
Genentech Research

	[[alternative HTML version deleted]]


From h@wickh@m @ending from gm@il@com  Sat Aug 25 15:26:51 2018
From: h@wickh@m @ending from gm@il@com (Hadley Wickham)
Date: Sat, 25 Aug 2018 08:26:51 -0500
Subject: [Rd] Where does L come from?
Message-ID: <CABdHhvGa67SEpGkCffGpTu6P+u+0sKj-XOMP0ZNTGp0S6RmQqA@mail.gmail.com>

Hi all,

Would someone mind pointing to me to the inspiration for the use of
the L suffix to mean "integer"?  This is obviously hard to google for,
and the R language definition
(https://cran.r-project.org/doc/manuals/r-release/R-lang.html#Constants)
is silent.

Hadley

-- 
http://hadley.nz


From m@rc_@chw@rtz @ending from me@com  Sat Aug 25 15:40:07 2018
From: m@rc_@chw@rtz @ending from me@com (Marc Schwartz)
Date: Sat, 25 Aug 2018 09:40:07 -0400
Subject: [Rd] Where does L come from?
In-Reply-To: <CABdHhvGa67SEpGkCffGpTu6P+u+0sKj-XOMP0ZNTGp0S6RmQqA@mail.gmail.com>
References: <CABdHhvGa67SEpGkCffGpTu6P+u+0sKj-XOMP0ZNTGp0S6RmQqA@mail.gmail.com>
Message-ID: <96A60DDD-DD37-48FF-BF43-94B1821A3E7F@me.com>

On Aug 25, 2018, at 9:26 AM, Hadley Wickham <h.wickham at gmail.com> wrote:
> 
> Hi all,
> 
> Would someone mind pointing to me to the inspiration for the use of
> the L suffix to mean "integer"?  This is obviously hard to google for,
> and the R language definition
> (https://cran.r-project.org/doc/manuals/r-release/R-lang.html#Constants)
> is silent.
> 
> Hadley


The link you have above, does reference the use of 'L', but not the derivation.

There is a thread on R-Help from 2012 ("Difference between 10 and 10L"), where Prof. Ripley addresses the issue in response to Bill Dunlap and the OP:

  https://stat.ethz.ch/pipermail/r-help/2012-May/311771.html

In searching, I also found the following thread on SO:

  https://stackoverflow.com/questions/22191324/clarification-of-l-in-r/22192378

which had a link to the R-Help thread above and others.

Regards,

Marc Schwartz


From edd @ending from debi@n@org  Sat Aug 25 15:46:17 2018
From: edd @ending from debi@n@org (Dirk Eddelbuettel)
Date: Sat, 25 Aug 2018 08:46:17 -0500
Subject: [Rd] Where does L come from?
In-Reply-To: <CABdHhvGa67SEpGkCffGpTu6P+u+0sKj-XOMP0ZNTGp0S6RmQqA@mail.gmail.com>
References: <CABdHhvGa67SEpGkCffGpTu6P+u+0sKj-XOMP0ZNTGp0S6RmQqA@mail.gmail.com>
Message-ID: <23425.23977.965712.960744@rob.eddelbuettel.com>


On 25 August 2018 at 08:26, Hadley Wickham wrote:
| Would someone mind pointing to me to the inspiration for the use of
| the L suffix to mean "integer"?  This is obviously hard to google for,
| and the R language definition
| (https://cran.r-project.org/doc/manuals/r-release/R-lang.html#Constants)
| is silent.

https://www.geeksforgeeks.org/integer-literal-in-c-cpp-prefixes-suffixes/

https://en.cppreference.com/w/cpp/language/integer_literal

And many similar references if you Google for 'c language integer suffix'

Dirk

-- 
http://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From henrik@bengt@@on @ending from gm@il@com  Sat Aug 25 15:48:35 2018
From: henrik@bengt@@on @ending from gm@il@com (Henrik Bengtsson)
Date: Sat, 25 Aug 2018 06:48:35 -0700
Subject: [Rd] Where does L come from?
In-Reply-To: <96A60DDD-DD37-48FF-BF43-94B1821A3E7F@me.com>
References: <CABdHhvGa67SEpGkCffGpTu6P+u+0sKj-XOMP0ZNTGp0S6RmQqA@mail.gmail.com>
 <96A60DDD-DD37-48FF-BF43-94B1821A3E7F@me.com>
Message-ID: <CAFDcVCQ91xLdHDRoFE1iLCxEc=cOMRg4YMfHhF++pjadbvUT-A@mail.gmail.com>

Not that it brings closure, but there's also
https://stat.ethz.ch/pipermail/r-devel/2017-June/074462.html

Henrik

On Sat, Aug 25, 2018, 06:40 Marc Schwartz via R-devel <r-devel at r-project.org>
wrote:

> On Aug 25, 2018, at 9:26 AM, Hadley Wickham <h.wickham at gmail.com> wrote:
> >
> > Hi all,
> >
> > Would someone mind pointing to me to the inspiration for the use of
> > the L suffix to mean "integer"?  This is obviously hard to google for,
> > and the R language definition
> > (https://cran.r-project.org/doc/manuals/r-release/R-lang.html#Constants)
> > is silent.
> >
> > Hadley
>
>
> The link you have above, does reference the use of 'L', but not the
> derivation.
>
> There is a thread on R-Help from 2012 ("Difference between 10 and 10L"),
> where Prof. Ripley addresses the issue in response to Bill Dunlap and the
> OP:
>
>   https://stat.ethz.ch/pipermail/r-help/2012-May/311771.html
>
> In searching, I also found the following thread on SO:
>
>
> https://stackoverflow.com/questions/22191324/clarification-of-l-in-r/22192378
>
> which had a link to the R-Help thread above and others.
>
> Regards,
>
> Marc Schwartz
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From cboettig @ending from gm@il@com  Sat Aug 25 18:28:18 2018
From: cboettig @ending from gm@il@com (Carl Boettiger)
Date: Sat, 25 Aug 2018 09:28:18 -0700
Subject: [Rd] Where does L come from?
In-Reply-To: <CAFDcVCQ91xLdHDRoFE1iLCxEc=cOMRg4YMfHhF++pjadbvUT-A@mail.gmail.com>
References: <CABdHhvGa67SEpGkCffGpTu6P+u+0sKj-XOMP0ZNTGp0S6RmQqA@mail.gmail.com>
 <96A60DDD-DD37-48FF-BF43-94B1821A3E7F@me.com>
 <CAFDcVCQ91xLdHDRoFE1iLCxEc=cOMRg4YMfHhF++pjadbvUT-A@mail.gmail.com>
Message-ID: <CAN_1p9z=CKznjkZ-_Gwu5pGV6LX1oUKaKKnQBw-hqt6h0C1O6Q@mail.gmail.com>

I always thought it meant "Long" (I'm assuming R's integers are long
integers in C sense (iirrc one can declare 'long x', and it being common to
refer to integers as "longs"  in the same way we use "doubles" to mean
double precision floating point).  But pure speculation on my part, so I'm
curious!

On Sat, Aug 25, 2018 at 6:50 AM Henrik Bengtsson <henrik.bengtsson at gmail.com>
wrote:

> Not that it brings closure, but there's also
> https://stat.ethz.ch/pipermail/r-devel/2017-June/074462.html
>
> Henrik
>
> On Sat, Aug 25, 2018, 06:40 Marc Schwartz via R-devel <
> r-devel at r-project.org>
> wrote:
>
> > On Aug 25, 2018, at 9:26 AM, Hadley Wickham <h.wickham at gmail.com> wrote:
> > >
> > > Hi all,
> > >
> > > Would someone mind pointing to me to the inspiration for the use of
> > > the L suffix to mean "integer"?  This is obviously hard to google for,
> > > and the R language definition
> > > (
> https://cran.r-project.org/doc/manuals/r-release/R-lang.html#Constants)
> > > is silent.
> > >
> > > Hadley
> >
> >
> > The link you have above, does reference the use of 'L', but not the
> > derivation.
> >
> > There is a thread on R-Help from 2012 ("Difference between 10 and 10L"),
> > where Prof. Ripley addresses the issue in response to Bill Dunlap and the
> > OP:
> >
> >   https://stat.ethz.ch/pipermail/r-help/2012-May/311771.html
> >
> > In searching, I also found the following thread on SO:
> >
> >
> >
> https://stackoverflow.com/questions/22191324/clarification-of-l-in-r/22192378
> >
> > which had a link to the R-Help thread above and others.
> >
> > Regards,
> >
> > Marc Schwartz
> >
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>
-- 

http://carlboettiger.info

	[[alternative HTML version deleted]]


From edd @ending from debi@n@org  Sat Aug 25 19:01:28 2018
From: edd @ending from debi@n@org (Dirk Eddelbuettel)
Date: Sat, 25 Aug 2018 12:01:28 -0500
Subject: [Rd] Where does L come from?
In-Reply-To: <CAN_1p9z=CKznjkZ-_Gwu5pGV6LX1oUKaKKnQBw-hqt6h0C1O6Q@mail.gmail.com>
References: <CABdHhvGa67SEpGkCffGpTu6P+u+0sKj-XOMP0ZNTGp0S6RmQqA@mail.gmail.com>
 <96A60DDD-DD37-48FF-BF43-94B1821A3E7F@me.com>
 <CAFDcVCQ91xLdHDRoFE1iLCxEc=cOMRg4YMfHhF++pjadbvUT-A@mail.gmail.com>
 <CAN_1p9z=CKznjkZ-_Gwu5pGV6LX1oUKaKKnQBw-hqt6h0C1O6Q@mail.gmail.com>
Message-ID: <23425.35688.465437.842641@rob.eddelbuettel.com>


On 25 August 2018 at 09:28, Carl Boettiger wrote:
| I always thought it meant "Long" (I'm assuming R's integers are long
| integers in C sense (iirrc one can declare 'long x', and it being common to
| refer to integers as "longs"  in the same way we use "doubles" to mean
| double precision floating point).  But pure speculation on my part, so I'm
| curious!

It does per my copy (dated 1990 !!) of the 2nd ed of Kernighan & Ritchie.  It
explicitly mentions (sec 2.2) that 'int' may be 16 or 32 bits, and 'long' is
32 bit; and (in sec 2.3) introduces the I, U, and L labels for constants.  So
"back then when" 32 bit was indeed long.  And as R uses 32 bit integers ...

(It is all murky because the size is an implementation detail and later
"essentially everybody" moved to 32 bit integers and 64 bit longs as the 64
bit architectures became prevalent.  Which is why when it matters one should
really use more explicit types like int32_t or int64_t.)

Dirk

-- 
http://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From hp@ge@ @ending from fredhutch@org  Sat Aug 25 22:49:30 2018
From: hp@ge@ @ending from fredhutch@org (=?UTF-8?B?SGVydsOpIFBhZ8Oocw==?=)
Date: Sat, 25 Aug 2018 13:49:30 -0700
Subject: [Rd] Where does L come from?
In-Reply-To: <23425.35688.465437.842641@rob.eddelbuettel.com>
References: <CABdHhvGa67SEpGkCffGpTu6P+u+0sKj-XOMP0ZNTGp0S6RmQqA@mail.gmail.com>
 <96A60DDD-DD37-48FF-BF43-94B1821A3E7F@me.com>
 <CAFDcVCQ91xLdHDRoFE1iLCxEc=cOMRg4YMfHhF++pjadbvUT-A@mail.gmail.com>
 <CAN_1p9z=CKznjkZ-_Gwu5pGV6LX1oUKaKKnQBw-hqt6h0C1O6Q@mail.gmail.com>
 <23425.35688.465437.842641@rob.eddelbuettel.com>
Message-ID: <6ae89d21-0a0d-805a-e8db-9de20c42a019@fredhutch.org>

The choice of the L suffix in R to mean "R integer type", which
is mapped to the "int" type at the C level, and NOT to the "long int"
type, is really unfortunate as it seems to be misleading and confusing
a lot of people.

The fact that nowadays "int" and "long int" have the same size on most
platforms is only anecdotal here.

Just my 2 cents.

H.

On 08/25/2018 10:01 AM, Dirk Eddelbuettel wrote:
> 
> On 25 August 2018 at 09:28, Carl Boettiger wrote:
> | I always thought it meant "Long" (I'm assuming R's integers are long
> | integers in C sense (iirrc one can declare 'long x', and it being common to
> | refer to integers as "longs"  in the same way we use "doubles" to mean
> | double precision floating point).  But pure speculation on my part, so I'm
> | curious!
> 
> It does per my copy (dated 1990 !!) of the 2nd ed of Kernighan & Ritchie.  It
> explicitly mentions (sec 2.2) that 'int' may be 16 or 32 bits, and 'long' is
> 32 bit; and (in sec 2.3) introduces the I, U, and L labels for constants.  So
> "back then when" 32 bit was indeed long.  And as R uses 32 bit integers ...
> 
> (It is all murky because the size is an implementation detail and later
> "essentially everybody" moved to 32 bit integers and 64 bit longs as the 64
> bit architectures became prevalent.  Which is why when it matters one should
> really use more explicit types like int32_t or int64_t.)
> 
> Dirk
> 

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fredhutch.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From edd @ending from debi@n@org  Sat Aug 25 23:23:36 2018
From: edd @ending from debi@n@org (Dirk Eddelbuettel)
Date: Sat, 25 Aug 2018 16:23:36 -0500
Subject: [Rd] Where does L come from?
In-Reply-To: <6ae89d21-0a0d-805a-e8db-9de20c42a019@fredhutch.org>
References: <CABdHhvGa67SEpGkCffGpTu6P+u+0sKj-XOMP0ZNTGp0S6RmQqA@mail.gmail.com>
 <96A60DDD-DD37-48FF-BF43-94B1821A3E7F@me.com>
 <CAFDcVCQ91xLdHDRoFE1iLCxEc=cOMRg4YMfHhF++pjadbvUT-A@mail.gmail.com>
 <CAN_1p9z=CKznjkZ-_Gwu5pGV6LX1oUKaKKnQBw-hqt6h0C1O6Q@mail.gmail.com>
 <23425.35688.465437.842641@rob.eddelbuettel.com>
 <6ae89d21-0a0d-805a-e8db-9de20c42a019@fredhutch.org>
Message-ID: <23425.51416.778722.954108@rob.eddelbuettel.com>


On 25 August 2018 at 13:49, Herv? Pag?s wrote:
| The choice of the L suffix in R to mean "R integer type", which
| is mapped to the "int" type at the C level, and NOT to the "long int"
| type, is really unfortunate as it seems to be misleading and confusing
| a lot of people.

The point I was trying to make in what you quote below is that the L may come
from a time when int and long int were in fact the same on most relevant
architectures. And it is hardly R's fault that C was allowed to change.

Also, it hardly matters given that R has precisely one integer type so I am
unsure where you see the confusion between long int and int.
 
| The fact that nowadays "int" and "long int" have the same size on most
| platforms is only anecdotal here.
|
| Just my 2 cents.

Are you sure?

  R> Rcpp::evalCpp("sizeof(long int)")
  [1] 8
  R> Rcpp::evalCpp("sizeof(int)")
  [1] 4
  R> 

Dirk

| H.
| 
| On 08/25/2018 10:01 AM, Dirk Eddelbuettel wrote:
| > 
| > On 25 August 2018 at 09:28, Carl Boettiger wrote:
| > | I always thought it meant "Long" (I'm assuming R's integers are long
| > | integers in C sense (iirrc one can declare 'long x', and it being common to
| > | refer to integers as "longs"  in the same way we use "doubles" to mean
| > | double precision floating point).  But pure speculation on my part, so I'm
| > | curious!
| > 
| > It does per my copy (dated 1990 !!) of the 2nd ed of Kernighan & Ritchie.  It
| > explicitly mentions (sec 2.2) that 'int' may be 16 or 32 bits, and 'long' is
| > 32 bit; and (in sec 2.3) introduces the I, U, and L labels for constants.  So
| > "back then when" 32 bit was indeed long.  And as R uses 32 bit integers ...
| > 
| > (It is all murky because the size is an implementation detail and later
| > "essentially everybody" moved to 32 bit integers and 64 bit longs as the 64
| > bit architectures became prevalent.  Which is why when it matters one should
| > really use more explicit types like int32_t or int64_t.)
| > 
| > Dirk
| > 
| 
| -- 
| Herv? Pag?s
| 
| Program in Computational Biology
| Division of Public Health Sciences
| Fred Hutchinson Cancer Research Center
| 1100 Fairview Ave. N, M1-B514
| P.O. Box 19024
| Seattle, WA 98109-1024
| 
| E-mail: hpages at fredhutch.org
| Phone:  (206) 667-5791
| Fax:    (206) 667-1319

-- 
http://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From murdoch@dunc@n @ending from gm@il@com  Sun Aug 26 01:33:23 2018
From: murdoch@dunc@n @ending from gm@il@com (Duncan Murdoch)
Date: Sat, 25 Aug 2018 19:33:23 -0400
Subject: [Rd] Where does L come from?
In-Reply-To: <6ae89d21-0a0d-805a-e8db-9de20c42a019@fredhutch.org>
References: <CABdHhvGa67SEpGkCffGpTu6P+u+0sKj-XOMP0ZNTGp0S6RmQqA@mail.gmail.com>
 <96A60DDD-DD37-48FF-BF43-94B1821A3E7F@me.com>
 <CAFDcVCQ91xLdHDRoFE1iLCxEc=cOMRg4YMfHhF++pjadbvUT-A@mail.gmail.com>
 <CAN_1p9z=CKznjkZ-_Gwu5pGV6LX1oUKaKKnQBw-hqt6h0C1O6Q@mail.gmail.com>
 <23425.35688.465437.842641@rob.eddelbuettel.com>
 <6ae89d21-0a0d-805a-e8db-9de20c42a019@fredhutch.org>
Message-ID: <74ce0f99-29b2-6cd4-8144-e0b191f49a85@gmail.com>

On 25/08/2018 4:49 PM, Herv? Pag?s wrote:
> The choice of the L suffix in R to mean "R integer type", which
> is mapped to the "int" type at the C level, and NOT to the "long int"
> type, is really unfortunate as it seems to be misleading and confusing
> a lot of people.

Can you provide any evidence of that (e.g. a link to a message from one 
of these people)?  I think a lot of people don't really know about the L 
suffix, but that's different from being confused or misleaded by it.

And if you make a criticism like that, it would really be fair to 
suggest what R should have done instead.  I can't think of anything 
better, given that "i" was already taken, and that the lack of a decimal 
place had historically not been significant.  Using "I" *would* have 
been confusing (3i versus 3I being very different).  Deciding that 3 
suddenly became an integer value different from 3. would have led to 
lots of inefficient conversions (since stats mainly deals with floating 
point values).

Duncan Murdoch


> 
> The fact that nowadays "int" and "long int" have the same size on most
> platforms is only anecdotal here.
> 
> Just my 2 cents.
> 
> H.
> 
> On 08/25/2018 10:01 AM, Dirk Eddelbuettel wrote:
>>
>> On 25 August 2018 at 09:28, Carl Boettiger wrote:
>> | I always thought it meant "Long" (I'm assuming R's integers are long
>> | integers in C sense (iirrc one can declare 'long x', and it being common to
>> | refer to integers as "longs"  in the same way we use "doubles" to mean
>> | double precision floating point).  But pure speculation on my part, so I'm
>> | curious!
>>
>> It does per my copy (dated 1990 !!) of the 2nd ed of Kernighan & Ritchie.  It
>> explicitly mentions (sec 2.2) that 'int' may be 16 or 32 bits, and 'long' is
>> 32 bit; and (in sec 2.3) introduces the I, U, and L labels for constants.  So
>> "back then when" 32 bit was indeed long.  And as R uses 32 bit integers ...
>>
>> (It is all murky because the size is an implementation detail and later
>> "essentially everybody" moved to 32 bit integers and 64 bit longs as the 64
>> bit architectures became prevalent.  Which is why when it matters one should
>> really use more explicit types like int32_t or int64_t.)
>>
>> Dirk
>>
>


From hp@ge@ @ending from fredhutch@org  Sun Aug 26 05:07:04 2018
From: hp@ge@ @ending from fredhutch@org (=?UTF-8?B?SGVydsOpIFBhZ8Oocw==?=)
Date: Sat, 25 Aug 2018 20:07:04 -0700
Subject: [Rd] Where does L come from?
In-Reply-To: <23425.51416.778722.954108@rob.eddelbuettel.com>
References: <CABdHhvGa67SEpGkCffGpTu6P+u+0sKj-XOMP0ZNTGp0S6RmQqA@mail.gmail.com>
 <96A60DDD-DD37-48FF-BF43-94B1821A3E7F@me.com>
 <CAFDcVCQ91xLdHDRoFE1iLCxEc=cOMRg4YMfHhF++pjadbvUT-A@mail.gmail.com>
 <CAN_1p9z=CKznjkZ-_Gwu5pGV6LX1oUKaKKnQBw-hqt6h0C1O6Q@mail.gmail.com>
 <23425.35688.465437.842641@rob.eddelbuettel.com>
 <6ae89d21-0a0d-805a-e8db-9de20c42a019@fredhutch.org>
 <23425.51416.778722.954108@rob.eddelbuettel.com>
Message-ID: <90f3fe0f-1192-2b24-b356-90fd072b6bf6@fredhutch.org>



On 08/25/2018 02:23 PM, Dirk Eddelbuettel wrote:
> 
> On 25 August 2018 at 13:49, Herv? Pag?s wrote:
> | The choice of the L suffix in R to mean "R integer type", which
> | is mapped to the "int" type at the C level, and NOT to the "long int"
> | type, is really unfortunate as it seems to be misleading and confusing
> | a lot of people.
> 
> The point I was trying to make in what you quote below is that the L may come
> from a time when int and long int were in fact the same on most relevant
> architectures. And it is hardly R's fault that C was allowed to change.
> 
> Also, it hardly matters given that R has precisely one integer type so I am
> unsure where you see the confusion between long int and int.
>   
> | The fact that nowadays "int" and "long int" have the same size on most
> | platforms is only anecdotal here.
> |
> | Just my 2 cents.
> 
> Are you sure?
> 
>    R> Rcpp::evalCpp("sizeof(long int)")
>    [1] 8
>    R> Rcpp::evalCpp("sizeof(int)")
>    [1] 4
>    R>

My bad, it's only the same on Windows. My point is that the discussion
about the size of int vs long int is only a distraction here. The 
important bit is that 10L in R is represented by 10 in C, which is an
int, not by 10L, which is a long int. Could hardly be more confusing.

H.


> 
> Dirk
> 
> | H.
> |
> | On 08/25/2018 10:01 AM, Dirk Eddelbuettel wrote:
> | >
> | > On 25 August 2018 at 09:28, Carl Boettiger wrote:
> | > | I always thought it meant "Long" (I'm assuming R's integers are long
> | > | integers in C sense (iirrc one can declare 'long x', and it being common to
> | > | refer to integers as "longs"  in the same way we use "doubles" to mean
> | > | double precision floating point).  But pure speculation on my part, so I'm
> | > | curious!
> | >
> | > It does per my copy (dated 1990 !!) of the 2nd ed of Kernighan & Ritchie.  It
> | > explicitly mentions (sec 2.2) that 'int' may be 16 or 32 bits, and 'long' is
> | > 32 bit; and (in sec 2.3) introduces the I, U, and L labels for constants.  So
> | > "back then when" 32 bit was indeed long.  And as R uses 32 bit integers ...
> | >
> | > (It is all murky because the size is an implementation detail and later
> | > "essentially everybody" moved to 32 bit integers and 64 bit longs as the 64
> | > bit architectures became prevalent.  Which is why when it matters one should
> | > really use more explicit types like int32_t or int64_t.)
> | >
> | > Dirk
> | >
> |
> | --
> | Herv? Pag?s
> |
> | Program in Computational Biology
> | Division of Public Health Sciences
> | Fred Hutchinson Cancer Research Center
> | 1100 Fairview Ave. N, M1-B514
> | P.O. Box 19024
> | Seattle, WA 98109-1024
> |
> | E-mail: hpages at fredhutch.org
> | Phone:  (206) 667-5791
> | Fax:    (206) 667-1319
> 

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fredhutch.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From hp@ge@ @ending from fredhutch@org  Sun Aug 26 07:57:20 2018
From: hp@ge@ @ending from fredhutch@org (=?UTF-8?B?SGVydsOpIFBhZ8Oocw==?=)
Date: Sat, 25 Aug 2018 22:57:20 -0700
Subject: [Rd] Where does L come from?
In-Reply-To: <74ce0f99-29b2-6cd4-8144-e0b191f49a85@gmail.com>
References: <CABdHhvGa67SEpGkCffGpTu6P+u+0sKj-XOMP0ZNTGp0S6RmQqA@mail.gmail.com>
 <96A60DDD-DD37-48FF-BF43-94B1821A3E7F@me.com>
 <CAFDcVCQ91xLdHDRoFE1iLCxEc=cOMRg4YMfHhF++pjadbvUT-A@mail.gmail.com>
 <CAN_1p9z=CKznjkZ-_Gwu5pGV6LX1oUKaKKnQBw-hqt6h0C1O6Q@mail.gmail.com>
 <23425.35688.465437.842641@rob.eddelbuettel.com>
 <6ae89d21-0a0d-805a-e8db-9de20c42a019@fredhutch.org>
 <74ce0f99-29b2-6cd4-8144-e0b191f49a85@gmail.com>
Message-ID: <11abc75b-dfab-0670-7664-fd5da2be6cdd@fredhutch.org>

On 08/25/2018 04:33 PM, Duncan Murdoch wrote:
> On 25/08/2018 4:49 PM, Herv? Pag?s wrote:
>> The choice of the L suffix in R to mean "R integer type", which
>> is mapped to the "int" type at the C level, and NOT to the "long int"
>> type, is really unfortunate as it seems to be misleading and confusing
>> a lot of people.

I don't have stats about this so I take back the "lot".

> Can you provide any evidence of that (e.g. a link to a message from one 
> of these people)?? I think a lot of people don't really know about the L 
> suffix, but that's different from being confused or misleaded by it.
> 
> And if you make a criticism like that, it would really be fair to 
> suggest what R should have done instead.? I can't think of anything 
> better, given that "i" was already taken, and that the lack of a decimal 
> place had historically not been significant.? Using "I" *would* have 
> been confusing (3i versus 3I being very different).? Deciding that 3 
> suddenly became an integer value different from 3. would have led to 
> lots of inefficient conversions (since stats mainly deals with floating 
> point values).

Maybe 10N, or 10n? I'm not convinced that 10I would have been
confusing but the I can easily be mistaken for a 1.

H.

> 
> Duncan Murdoch
> 
> 
>>
>> The fact that nowadays "int" and "long int" have the same size on most
>> platforms is only anecdotal here.
>>
>> Just my 2 cents.
>>
>> H.
>>
>> On 08/25/2018 10:01 AM, Dirk Eddelbuettel wrote:
>>>
>>> On 25 August 2018 at 09:28, Carl Boettiger wrote:
>>> | I always thought it meant "Long" (I'm assuming R's integers are long
>>> | integers in C sense (iirrc one can declare 'long x', and it being 
>>> common to
>>> | refer to integers as "longs"? in the same way we use "doubles" to mean
>>> | double precision floating point).? But pure speculation on my part, 
>>> so I'm
>>> | curious!
>>>
>>> It does per my copy (dated 1990 !!) of the 2nd ed of Kernighan & 
>>> Ritchie.? It
>>> explicitly mentions (sec 2.2) that 'int' may be 16 or 32 bits, and 
>>> 'long' is
>>> 32 bit; and (in sec 2.3) introduces the I, U, and L labels for 
>>> constants.? So
>>> "back then when" 32 bit was indeed long.? And as R uses 32 bit 
>>> integers ...
>>>
>>> (It is all murky because the size is an implementation detail and later
>>> "essentially everybody" moved to 32 bit integers and 64 bit longs as 
>>> the 64
>>> bit architectures became prevalent.? Which is why when it matters one 
>>> should
>>> really use more explicit types like int32_t or int64_t.)
>>>
>>> Dirk
>>>
>>
> 

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fredhutch.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From tho@jleeper @ending from gm@il@com  Sun Aug 26 16:09:36 2018
From: tho@jleeper @ending from gm@il@com (Thomas J. Leeper)
Date: Sun, 26 Aug 2018 09:09:36 -0500
Subject: [Rd] Where does L come from?
In-Reply-To: <mailman.47543.5.1535277601.41090.r-devel@r-project.org>
References: <mailman.47543.5.1535277601.41090.r-devel@r-project.org>
Message-ID: <CAOC91MRvW3CA5K6TBHDkEV6pyhc_Bb855ayBN=B-L91zcANyUQ@mail.gmail.com>

As long as we're on this point about not many users knowing about "L"
notation, I'm going bump my earlier suggestion that it be at least
mentioned in the `? integer` documentation page:
https://stat.ethz.ch/pipermail/r-devel/2018-May/076203.html

Cheers,
-Thomas

> From: Duncan Murdoch <murdoch.duncan at gmail.com>
> To: =?UTF-8?B?SGVydsOpIFBhZ8Oocw==?= <hpages at fredhutch.org>, Dirk
>         Eddelbuettel <edd at debian.org>, Carl Boettiger <cboettig at gmail.com>
> Subject: Re: [Rd] Where does L come from?
>
> On 25/08/2018 4:49 PM, Herv? Pag?s wrote:
> > The choice of the L suffix in R to mean "R integer type", which
> > is mapped to the "int" type at the C level, and NOT to the "long int"
> > type, is really unfortunate as it seems to be misleading and confusing
> > a lot of people.
>
> Can you provide any evidence of that (e.g. a link to a message from one
> of these people)?  I think a lot of people don't really know about the L
> suffix, but that's different from being confused or misleaded by it.
>
> And if you make a criticism like that, it would really be fair to
> suggest what R should have done instead.  I can't think of anything
> better, given that "i" was already taken, and that the lack of a decimal
> place had historically not been significant.  Using "I" *would* have
> been confusing (3i versus 3I being very different).  Deciding that 3
> suddenly became an integer value different from 3. would have led to
> lots of inefficient conversions (since stats mainly deals with floating
> point values).
>
> Duncan Murdoch


From p@ul @ending from @t@t@@uckl@nd@@c@nz  Sun Aug 26 23:36:57 2018
From: p@ul @ending from @t@t@@uckl@nd@@c@nz (Paul Murrell)
Date: Mon, 27 Aug 2018 09:36:57 +1200
Subject: [Rd] plotmath degree symbol
In-Reply-To: <d83487b0-3b9b-43ab-bfd7-6526ee63012a@uni-muenster.de>
References: <d83487b0-3b9b-43ab-bfd7-6526ee63012a@uni-muenster.de>
Message-ID: <b7654ca8-9a9e-edda-805c-4384e1a95e1a@stat.auckland.ac.nz>

Hi

Sorry, but this seems to be working ok for me ...

 > sessionInfo()
R version 3.4.2 (2017-09-28)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Ubuntu 16.04.5 LTS

Matrix products: default
BLAS: /usr/lib/libblas/libblas.so.3.6.0
LAPACK: /usr/lib/lapack/liblapack.so.3.6.0

locale:
  [1] LC_CTYPE=en_NZ.UTF-8       LC_NUMERIC=C
  [3] LC_TIME=en_NZ.UTF-8        LC_COLLATE=en_NZ.UTF-8
  [5] LC_MONETARY=en_NZ.UTF-8    LC_MESSAGES=en_NZ.UTF-8
  [7] LC_PAPER=en_NZ.UTF-8       LC_NAME=C
  [9] LC_ADDRESS=C               LC_TELEPHONE=C
[11] LC_MEASUREMENT=en_NZ.UTF-8 LC_IDENTIFICATION=C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

loaded via a namespace (and not attached):
[1] compiler_3.4.2

... and ...

 > sessionInfo()
R Under development (unstable) (2018-08-22 r75177)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Ubuntu 16.04.3 LTS

Matrix products: default
BLAS: /home/pmur002/R/r-devel/BUILD/lib/libRblas.so
LAPACK: /home/pmur002/R/r-devel/BUILD/lib/libRlapack.so

locale:
  [1] LC_CTYPE=en_NZ.UTF-8       LC_NUMERIC=C
  [3] LC_TIME=en_NZ.UTF-8        LC_COLLATE=en_NZ.UTF-8
  [5] LC_MONETARY=en_NZ.UTF-8    LC_MESSAGES=en_NZ.UTF-8
  [7] LC_PAPER=en_NZ.UTF-8       LC_NAME=C
  [9] LC_ADDRESS=C               LC_TELEPHONE=C
[11] LC_MEASUREMENT=en_NZ.UTF-8 LC_IDENTIFICATION=C

attached base packages:
[1] stats     graphics  utils     datasets  grDevices methods   base

loaded via a namespace (and not attached):
[1] compiler_3.6.0

... what is your setup ?

Paul

On 25/08/18 05:53, Edzer Pebesma wrote:
> In plotmath expressions, R's degree symbol, e.g. shown by
> 
> plot(1, main = parse(text = "1*degree*C"))
> 
> has sunk to halfway the text line, instead of touching its top. In older
> R versions this looked much better.
> 

-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/


From ruipb@rr@d@@ @ending from @@po@pt  Sun Aug 26 23:57:03 2018
From: ruipb@rr@d@@ @ending from @@po@pt (Rui Barradas)
Date: Sun, 26 Aug 2018 22:57:03 +0100
Subject: [Rd] plotmath degree symbol
In-Reply-To: <b7654ca8-9a9e-edda-805c-4384e1a95e1a@stat.auckland.ac.nz>
References: <d83487b0-3b9b-43ab-bfd7-6526ee63012a@uni-muenster.de>
 <b7654ca8-9a9e-edda-805c-4384e1a95e1a@stat.auckland.ac.nz>
Message-ID: <533b4c59-6111-b340-0cfb-8889ddd0ece1@sapo.pt>

Hello,

Same here. Tested on R 3.1.1 and R 3.5.1 and they look *exactly* the same.

Hope this helps,

Rui Barradas

On 26/08/2018 22:36, Paul Murrell wrote:
> Hi
> 
> Sorry, but this seems to be working ok for me ...
> 
>  > sessionInfo()
> R version 3.4.2 (2017-09-28)
> Platform: x86_64-pc-linux-gnu (64-bit)
> Running under: Ubuntu 16.04.5 LTS
> 
> Matrix products: default
> BLAS: /usr/lib/libblas/libblas.so.3.6.0
> LAPACK: /usr/lib/lapack/liblapack.so.3.6.0
> 
> locale:
>  ?[1] LC_CTYPE=en_NZ.UTF-8?????? LC_NUMERIC=C
>  ?[3] LC_TIME=en_NZ.UTF-8??????? LC_COLLATE=en_NZ.UTF-8
>  ?[5] LC_MONETARY=en_NZ.UTF-8??? LC_MESSAGES=en_NZ.UTF-8
>  ?[7] LC_PAPER=en_NZ.UTF-8?????? LC_NAME=C
>  ?[9] LC_ADDRESS=C?????????????? LC_TELEPHONE=C
> [11] LC_MEASUREMENT=en_NZ.UTF-8 LC_IDENTIFICATION=C
> 
> attached base packages:
> [1] stats???? graphics? grDevices utils???? datasets? methods?? base
> 
> loaded via a namespace (and not attached):
> [1] compiler_3.4.2
> 
> ... and ...
> 
>  > sessionInfo()
> R Under development (unstable) (2018-08-22 r75177)
> Platform: x86_64-pc-linux-gnu (64-bit)
> Running under: Ubuntu 16.04.3 LTS
> 
> Matrix products: default
> BLAS: /home/pmur002/R/r-devel/BUILD/lib/libRblas.so
> LAPACK: /home/pmur002/R/r-devel/BUILD/lib/libRlapack.so
> 
> locale:
>  ?[1] LC_CTYPE=en_NZ.UTF-8?????? LC_NUMERIC=C
>  ?[3] LC_TIME=en_NZ.UTF-8??????? LC_COLLATE=en_NZ.UTF-8
>  ?[5] LC_MONETARY=en_NZ.UTF-8??? LC_MESSAGES=en_NZ.UTF-8
>  ?[7] LC_PAPER=en_NZ.UTF-8?????? LC_NAME=C
>  ?[9] LC_ADDRESS=C?????????????? LC_TELEPHONE=C
> [11] LC_MEASUREMENT=en_NZ.UTF-8 LC_IDENTIFICATION=C
> 
> attached base packages:
> [1] stats???? graphics? utils???? datasets? grDevices methods?? base
> 
> loaded via a namespace (and not attached):
> [1] compiler_3.6.0
> 
> ... what is your setup ?
> 
> Paul
> 
> On 25/08/18 05:53, Edzer Pebesma wrote:
>> In plotmath expressions, R's degree symbol, e.g. shown by
>>
>> plot(1, main = parse(text = "1*degree*C"))
>>
>> has sunk to halfway the text line, instead of touching its top. In older
>> R versions this looked much better.
>>
> 

---
This email has been checked for viruses by AVG.
https://www.avg.com


From bbolker @ending from gm@il@com  Mon Aug 27 01:10:44 2018
From: bbolker @ending from gm@il@com (Ben Bolker)
Date: Sun, 26 Aug 2018 19:10:44 -0400
Subject: [Rd] proposed patch to /src/library/datasets/man/mtcars.Rd
Message-ID: <3f87b6c4-b5b8-72da-1b0b-c8b143442a39@gmail.com>

Mara Averick noticed some oddities in the mtcars data set:

https://twitter.com/dataandme/status/1033341784959709184


I propose the following patch. While anyone with access to JSTOR could
dig in and find this information themselves, it would seem a friendly
gesture to include it ...

  cheers
    Ben Bolker

===================================================================
--- mtcars.Rd	(revision 75186)
+++ mtcars.Rd	(working copy)
@@ -35,6 +35,14 @@
   Building multiple regression models interactively.
   \emph{Biometrics}, \bold{37}, 391--411.
 }
+\details{
+Henderson and Velleman (1981) comment in a footnote to Table 1:
+\sQuote{Hocking [original transcriber]'s  noncrucial coding of the
+Mazda's rotary engine as a straight six-cylinder engine and the
+Porsche's flat engine as a V engine, as well as the inclusion of the diesel
+Mercedes 240D, have been retained to enable direct comparisons to be made
+with previous analyses.}
+}
 \examples{
 require(graphics)
 pairs(mtcars, main = "mtcars data", gap = 1/4)


From rpb@rry @ending from @l@@k@@edu  Mon Aug 27 01:59:58 2018
From: rpb@rry @ending from @l@@k@@edu (Ronald Barry)
Date: Sun, 26 Aug 2018 15:59:58 -0800
Subject: [Rd] Thanks for help with validspamobject
In-Reply-To: <CAOVYLHeicFCPZTtpLrfPN4CTEuKY2fwTKvjgEv-3rpxR_r7TWA@mail.gmail.com>
References: <CAOVYLHeicFCPZTtpLrfPN4CTEuKY2fwTKvjgEv-3rpxR_r7TWA@mail.gmail.com>
Message-ID: <CAOVYLHejQx-Bpf3ibuMH8=cpYJreY7XG1bW-ziPD28-hgfzGXg@mail.gmail.com>

I'm curious about a warning coming from the use of package sqldf.  I opened
a file and read it using a SQLite select statement:

f = "/Users/ronaldbarry/Desktop/Three-Dee/LakeMichDepth.txt"
> sql_statement = "select long,lat,depth,(lat - 41.62) as templat, (long +
> 88) as templong
> from file
> where (abs(templat - 0.1*round(templat/0.1,0)) < 0.01)and(abs(templong -
> 0.15*round(templong/0.15,0)) < 0.01)"
> Mich_depth = sqldf::read.csv.sql(file=f,sql =
> sql_statement,header=TRUE,sep=' ')


Which ran perfectly well, but it threw a warning:

Warning messages:
1: In stopifnot(is.list(hooks)) :
  closing unused connection 6
(/Users/ronaldbarry/Desktop/Three-Dee/LakeMichDepth.txt)
2: In stopifnot(is.list(hooks)) :
  closing unused connection 5
(/Users/ronaldbarry/Desktop/Three-Dee/LakeMichDepth.txt)

It looks like it would be good practice to close the connection in code,
but I don't see any function in sqldf

to do so.  Is there a way to nicely close the connection, or should I just
ignore the warning?  Operating

system is Mac OSX 13.16 High Sierra, R Studio version 1.1.146, R version
3.5.0.  Thanks for any info.

On Thu, Aug 16, 2018 at 12:00 PM, Ronald Barry <rpbarry at alaska.edu> wrote:

> Hi,
>   Thanks for all your help.  The problem with an error involving
> validspamobject() has been resolved, as a new version of spdep (0.7-7) was
> just released and it seems to have stopped using the deprecated function.
>
> Ron B.
>

	[[alternative HTML version deleted]]


From wdunl@p @ending from tibco@com  Mon Aug 27 05:30:17 2018
From: wdunl@p @ending from tibco@com (William Dunlap)
Date: Sun, 26 Aug 2018 20:30:17 -0700
Subject: [Rd] Where does L come from?
In-Reply-To: <74ce0f99-29b2-6cd4-8144-e0b191f49a85@gmail.com>
References: <CABdHhvGa67SEpGkCffGpTu6P+u+0sKj-XOMP0ZNTGp0S6RmQqA@mail.gmail.com>
 <96A60DDD-DD37-48FF-BF43-94B1821A3E7F@me.com>
 <CAFDcVCQ91xLdHDRoFE1iLCxEc=cOMRg4YMfHhF++pjadbvUT-A@mail.gmail.com>
 <CAN_1p9z=CKznjkZ-_Gwu5pGV6LX1oUKaKKnQBw-hqt6h0C1O6Q@mail.gmail.com>
 <23425.35688.465437.842641@rob.eddelbuettel.com>
 <6ae89d21-0a0d-805a-e8db-9de20c42a019@fredhutch.org>
 <74ce0f99-29b2-6cd4-8144-e0b191f49a85@gmail.com>
Message-ID: <CAF8bMca4-bO_3GK-=9O8jWU2nR0uyqw5SqLA+UjDSmjH7zvK4A@mail.gmail.com>

>  the lack of a decimal place had historically not been significant

Version 4 of S (c. 1991) and versions of S+ based on it treated a sequence
of digits without a decimal  point as integer.

Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Sat, Aug 25, 2018 at 4:33 PM, Duncan Murdoch <murdoch.duncan at gmail.com>
wrote:

> On 25/08/2018 4:49 PM, Herv? Pag?s wrote:
>
>> The choice of the L suffix in R to mean "R integer type", which
>> is mapped to the "int" type at the C level, and NOT to the "long int"
>> type, is really unfortunate as it seems to be misleading and confusing
>> a lot of people.
>>
>
> Can you provide any evidence of that (e.g. a link to a message from one of
> these people)?  I think a lot of people don't really know about the L
> suffix, but that's different from being confused or misleaded by it.
>
> And if you make a criticism like that, it would really be fair to suggest
> what R should have done instead.  I can't think of anything better, given
> that "i" was already taken, and that the lack of a decimal place had
> historically not been significant.  Using "I" *would* have been confusing
> (3i versus 3I being very different).  Deciding that 3 suddenly became an
> integer value different from 3. would have led to lots of inefficient
> conversions (since stats mainly deals with floating point values).
>
> Duncan Murdoch
>
>
>
>> The fact that nowadays "int" and "long int" have the same size on most
>> platforms is only anecdotal here.
>>
>> Just my 2 cents.
>>
>> H.
>>
>> On 08/25/2018 10:01 AM, Dirk Eddelbuettel wrote:
>>
>>>
>>> On 25 August 2018 at 09:28, Carl Boettiger wrote:
>>> | I always thought it meant "Long" (I'm assuming R's integers are long
>>> | integers in C sense (iirrc one can declare 'long x', and it being
>>> common to
>>> | refer to integers as "longs"  in the same way we use "doubles" to mean
>>> | double precision floating point).  But pure speculation on my part, so
>>> I'm
>>> | curious!
>>>
>>> It does per my copy (dated 1990 !!) of the 2nd ed of Kernighan &
>>> Ritchie.  It
>>> explicitly mentions (sec 2.2) that 'int' may be 16 or 32 bits, and
>>> 'long' is
>>> 32 bit; and (in sec 2.3) introduces the I, U, and L labels for
>>> constants.  So
>>> "back then when" 32 bit was indeed long.  And as R uses 32 bit integers
>>> ...
>>>
>>> (It is all murky because the size is an implementation detail and later
>>> "essentially everybody" moved to 32 bit integers and 64 bit longs as the
>>> 64
>>> bit architectures became prevalent.  Which is why when it matters one
>>> should
>>> really use more explicit types like int32_t or int64_t.)
>>>
>>> Dirk
>>>
>>>
>>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From @d@m @ending from @rkho@u@  Mon Aug 27 06:15:19 2018
From: @d@m @ending from @rkho@u@ (Adam M. Dobrin)
Date: Mon, 27 Aug 2018 00:15:19 -0400
Subject: [Rd] Where does L come from?
In-Reply-To: <CAF8bMca4-bO_3GK-=9O8jWU2nR0uyqw5SqLA+UjDSmjH7zvK4A@mail.gmail.com>
References: <CABdHhvGa67SEpGkCffGpTu6P+u+0sKj-XOMP0ZNTGp0S6RmQqA@mail.gmail.com>
 <96A60DDD-DD37-48FF-BF43-94B1821A3E7F@me.com>
 <CAFDcVCQ91xLdHDRoFE1iLCxEc=cOMRg4YMfHhF++pjadbvUT-A@mail.gmail.com>
 <CAN_1p9z=CKznjkZ-_Gwu5pGV6LX1oUKaKKnQBw-hqt6h0C1O6Q@mail.gmail.com>
 <23425.35688.465437.842641@rob.eddelbuettel.com>
 <6ae89d21-0a0d-805a-e8db-9de20c42a019@fredhutch.org>
 <74ce0f99-29b2-6cd4-8144-e0b191f49a85@gmail.com>
 <CAF8bMca4-bO_3GK-=9O8jWU2nR0uyqw5SqLA+UjDSmjH7zvK4A@mail.gmail.com>
Message-ID: <CAKaCjJn3hEfu5XLrsn_cQWPRCad=0J5BfWKROTXLmiGmeaerkw@mail.gmail.com>

most likely L comes from Michel or Obelisk.

http://img.izing.ml/MARSHALL.html = why you are making Mars colonization
(and space) "just a game"
http://img.izing.ml/IT.html = why i could care less.
?

On Sun, Aug 26, 2018 at 11:30 PM, William Dunlap via R-devel <
r-devel at r-project.org> wrote:

> >  the lack of a decimal place had historically not been significant
>
> Version 4 of S (c. 1991) and versions of S+ based on it treated a sequence
> of digits without a decimal  point as integer.
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
> On Sat, Aug 25, 2018 at 4:33 PM, Duncan Murdoch <murdoch.duncan at gmail.com>
> wrote:
>
> > On 25/08/2018 4:49 PM, Herv? Pag?s wrote:
> >
> >> The choice of the L suffix in R to mean "R integer type", which
> >> is mapped to the "int" type at the C level, and NOT to the "long int"
> >> type, is really unfortunate as it seems to be misleading and confusing
> >> a lot of people.
> >>
> >
> > Can you provide any evidence of that (e.g. a link to a message from one
> of
> > these people)?  I think a lot of people don't really know about the L
> > suffix, but that's different from being confused or misleaded by it.
> >
> > And if you make a criticism like that, it would really be fair to suggest
> > what R should have done instead.  I can't think of anything better, given
> > that "i" was already taken, and that the lack of a decimal place had
> > historically not been significant.  Using "I" *would* have been confusing
> > (3i versus 3I being very different).  Deciding that 3 suddenly became an
> > integer value different from 3. would have led to lots of inefficient
> > conversions (since stats mainly deals with floating point values).
> >
> > Duncan Murdoch
> >
> >
> >
> >> The fact that nowadays "int" and "long int" have the same size on most
> >> platforms is only anecdotal here.
> >>
> >> Just my 2 cents.
> >>
> >> H.
> >>
> >> On 08/25/2018 10:01 AM, Dirk Eddelbuettel wrote:
> >>
> >>>
> >>> On 25 August 2018 at 09:28, Carl Boettiger wrote:
> >>> | I always thought it meant "Long" (I'm assuming R's integers are long
> >>> | integers in C sense (iirrc one can declare 'long x', and it being
> >>> common to
> >>> | refer to integers as "longs"  in the same way we use "doubles" to
> mean
> >>> | double precision floating point).  But pure speculation on my part,
> so
> >>> I'm
> >>> | curious!
> >>>
> >>> It does per my copy (dated 1990 !!) of the 2nd ed of Kernighan &
> >>> Ritchie.  It
> >>> explicitly mentions (sec 2.2) that 'int' may be 16 or 32 bits, and
> >>> 'long' is
> >>> 32 bit; and (in sec 2.3) introduces the I, U, and L labels for
> >>> constants.  So
> >>> "back then when" 32 bit was indeed long.  And as R uses 32 bit integers
> >>> ...
> >>>
> >>> (It is all murky because the size is an implementation detail and later
> >>> "essentially everybody" moved to 32 bit integers and 64 bit longs as
> the
> >>> 64
> >>> bit architectures became prevalent.  Which is why when it matters one
> >>> should
> >>> really use more explicit types like int32_t or int64_t.)
> >>>
> >>> Dirk
> >>>
> >>>
> >>
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From gecon@m@inten@nce @ending from gm@il@com  Sun Aug 26 21:26:10 2018
From: gecon@m@inten@nce @ending from gm@il@com (Karol Podemski)
Date: Sun, 26 Aug 2018 21:26:10 +0200
Subject: [Rd] Package compiler - efficiency problem
In-Reply-To: <8020085b-0e76-9ac6-9223-7350c4d7ab91@gmail.com>
References: <CAC_=cNYEie=yjdDvJ1eKhRYSXSYAFmrgU5aO_baJud-6-xBsOw@mail.gmail.com>
 <b0c9736d-8d9c-c3cc-decd-eaf1a2b82b60@gmail.com>
 <CAC_=cNaG59M4q0_j66vFOUctqgJW2T7nmBq3LTRrsuEURkyOGw@mail.gmail.com>
 <8020085b-0e76-9ac6-9223-7350c4d7ab91@gmail.com>
Message-ID: <CAC_=cNbf74g2c7gbz-tbQLZKk6kNdsZEy_tP5AMyLr6ADfuNUQ@mail.gmail.com>

Dear Tomas, Inaki and the rest of R-devel team,

thank you for your explainations and suggestions. I talked with gEcon
development team and we decided to change our implementation along the
lines you suggested.

Best regards,
Karol Podemski


pt., 17 sie 2018 o 13:38 Tomas Kalibera <tomas.kalibera at gmail.com>
napisa?(a):

> Dear Karol,
>
> I don't understand the models behind these function, but I can tell that
> the code generated is very inefficient. The AST interpreter will be very
> inefficient performing each scalar computation with all boxing,
> allocations, function calls. The byte-code compiler removes some of the
> boxing and allocation. While it could certainly compile faster, it will
> always be taking long compiling such functions with so many commands: so
> many expressions to track, so many source references to map, for so little
> computation. The same code could be much more efficient if it used
> vectorized operations and loops. The compiler cannot infer the loops and
> vector operations from the code - it is not that smart and I doubt it could
> easily be for R, but such optimizations could certainly be done easily at a
> higher level, when optimizing computation within the model, not within R
> with all its complicated semantics. I think you could hope for ~100x
> speedups compared to current generated code running with R AST interpreter.
>
> So I think it might be worth thinking about writing an interpreter for the
> model (the generator would compute function values on the fly, without
> actually generating code). If that was too slow, it might pay off to
> generate some intermediate representation for the model that would be
> faster to interpret. If that was too hard, then perhaps generating the code
> from the model in a smarter way (use vector operations, loops). It is ok to
> do that opportunistically - only when possible. With compilers, this is
> normal, optimizations often take advantage of certain patterns in the code
> if they are present. If you had more specific questions how to optimize the
> code feel free to ask (also offline).
>
> Certainly I don't want the existence of the byte-code compiler to require
> you to switch from R to C/C++, that would be exactly the opposite of what
> the compiler is aiming for. If it turns out you really need a way to
> disable compilation of these generated functions (so they run much slower,
> but you don't have to wait for them to compile), we will provide it and
> using a hack/workaround it is already possible in existing versions of R,
> with all the drawbacks I mentioned previously.
>
> Best
> Tomas
>
>
> On 08/17/2018 12:43 AM, Karol Podemski wrote:
>
> Dear Thomas,
>
> thank you for prompt response and taking interest in this issue. I really
> appreciate your compiler project and efficiency gains in usual case. I am
> aware of limitations of interpreted languages too and because of that even
> when writing my first mail I had a hunch that it is not that easy to
> address this problem.  As you mentioned optimisation of compiler for
> handling non-standard code may be tricky and harmful for usual code. The
> question is if gEcon is the only package that may face the same issue
> because of compilation.
>
> The functions generated by gEcon are systems of non-linear equations
> defining the equilibrium of an economy (see
> http://gecon.r-forge.r-project.org/files/gEcon-users-guide.pdf  if you
> want to learn a bit how we obtain it). The rows, you suggested to
> vectorise, are indeed vectorisable because they define equilibrium for
> similiar markets (e.g. production and sale of beverages and food) but do
> not have to be vectorisable in general case. So that not to delve into too
> much details I will stop here in description of how the equations
> originate. However, I would like to point that similiar large systems of
> linear equations may arise in other fields (
> https://en.wikipedia.org/wiki/Steady_state ) and there may be other
> packages that generate similar large systems (e.g. network problems like
> hydraulic networks). In that case, reports such as mine may help you to
> assess the scale of the problems.
>
> Thank you for suggestions for improvement in our approach, i am going to
> discuss them with other package developers.
>
> Regards,
> Karol Podemski
>
> pon., 13 sie 2018 o 18:02 Tomas Kalibera <tomas.kalibera at gmail.com>
> napisa?(a):
>
>> Dear Karol,
>>
>> thank you for the report. I can reproduce that the function from you
>> example takes very long to compile and I can see where most time is spent.
>> The compiler is itself written in R and requires a lot of resources for
>> large functions (foo() has over 16,000 lines of code, nearly 1 million of
>> instructions/operands, 45,000 constants). In particular a lot of time is
>> spent in garbage collection and in finding a unique set of constants. Some
>> optimizations of the compiler may be possible, but it is unlikely that
>> functions this large will compile fast any soon. For non-generated code, we
>> now have the byte-compilation on installation by default which at least
>> removes the compile overhead from runtime. Even though the compiler is
>> slow, it is important to keep in mind that in principle, with any compiler
>> there will be functions where compilation would not be improve performance
>> (when the compile time is included or not).
>>
>> I think it is not a good idea to generate code for functions like foo()
>> in R (or any interpreted language). You say that R's byte-code compiler
>> produces code that runs 5-10x faster than when the function is interpreted
>> by the AST interpreter (uncompiled), which sounds like a good result, but I
>> believe that avoiding code generation would be much faster than that, apart
>> from drastically reducing code size and therefore compile time. The
>> generator of these functions has much more information than the compiler -
>> it could be turned into an interpreter of these functions and compute their
>> values on the fly.
>>
>> A significant source of inefficiency of the generated code are
>> element-wise operations, such as
>>
>> r[12] <- -vv[88] + vv[16] * (1 + ppff[1307])
>> ...
>>
>> r[139] <- -vv[215] + vv[47] * (1 + ppff[1434])
>>
>> (these could be vectorized, which would reduce code size and improve
>> interpretation speed; and make it somewhat readable). Most of the code
>> lines in the generated functions seem to be easily vectorizable.
>>
>> Compilers and interpreters necessarily use some heuristics or optimize at
>> some code patterns. Optimizing for generated code may be tricky as it could
>> even harm performance of usual code. And, I would much rather optimize the
>> compiler for the usual code.
>>
>> Indeed, a pragmatic solution requiring the least amount of work would be
>> to disable compilation of these generated functions. There is not a
>> documented way to do that and maybe we could add it (and technically it is
>> trivial), but I have been reluctant so far - in some cases, compilation
>> even of these functions may be beneficial - if the speedup is 5-10x and we
>> run very many times. But once the generated code included some pragma
>> preventing compilation, it won't be ever compiled. Also, the trade-offs may
>> change as the compiler evolves, perhaps not in this case, but in other
>> where such pragma may be used.
>>
>> Well so the short answer would be that these functions should not be
>> generated in the first place. If it were too much work rewriting, perhaps
>> the generator could just be improved to produce vectorized operations.
>>
>> Best
>> Tomas
>> On 12.8.2018 21:31, Karol Podemski wrote:
>>
>>  Dear R team,
>>
>> I am a co-author and maintainer of one of R packages distributed by R-forge
>> (gEcon). One of gEcon package users found a strange behaviour of package (R
>> froze for couple of minutes) and reported it to me. I traced the strange
>> behaviour to compiler package. I attach short demonstration of the problem
>> to this mail (demonstration makes use of compiler and tictoc packages only).
>>
>> In short, the compiler package has problems in compiling large functions -
>> their compilation and execution may take much longer than direct execution
>> of an uncompiled function. Such functions are generated by gEcon package as
>> they describe steady state for economy.
>>
>> I am curious if you are aware of such problems and plan to handle the
>> efficiency issues. On one of the boards I saw that there were efficiency
>> issues in rpart package but they have been resolved. Or would you advise to
>> turn off JIT on package load (package heavily uses such long functions
>> generated whenever a new model is created)?
>>
>> Best regards,
>> Karol Podemski
>>
>>
>>
>> ______________________________________________R-devel at r-project.org mailing listhttps://stat.ethz.ch/mailman/listinfo/r-devel
>>
>>
>>
>

	[[alternative HTML version deleted]]


From wdunl@p @ending from tibco@com  Mon Aug 27 18:57:30 2018
From: wdunl@p @ending from tibco@com (William Dunlap)
Date: Mon, 27 Aug 2018 09:57:30 -0700
Subject: [Rd] Where does L come from?
In-Reply-To: <CAF8bMca4-bO_3GK-=9O8jWU2nR0uyqw5SqLA+UjDSmjH7zvK4A@mail.gmail.com>
References: <CABdHhvGa67SEpGkCffGpTu6P+u+0sKj-XOMP0ZNTGp0S6RmQqA@mail.gmail.com>
 <96A60DDD-DD37-48FF-BF43-94B1821A3E7F@me.com>
 <CAFDcVCQ91xLdHDRoFE1iLCxEc=cOMRg4YMfHhF++pjadbvUT-A@mail.gmail.com>
 <CAN_1p9z=CKznjkZ-_Gwu5pGV6LX1oUKaKKnQBw-hqt6h0C1O6Q@mail.gmail.com>
 <23425.35688.465437.842641@rob.eddelbuettel.com>
 <6ae89d21-0a0d-805a-e8db-9de20c42a019@fredhutch.org>
 <74ce0f99-29b2-6cd4-8144-e0b191f49a85@gmail.com>
 <CAF8bMca4-bO_3GK-=9O8jWU2nR0uyqw5SqLA+UjDSmjH7zvK4A@mail.gmail.com>
Message-ID: <CAF8bMcbG2XckXhaxgaw6e+s62t2WwiAw+pb8FS4Kyw-cO+6xrg@mail.gmail.com>

Rich Calaway pointed out that S4 came out c. 1996-97, not 1991.

Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Sun, Aug 26, 2018 at 8:30 PM, William Dunlap <wdunlap at tibco.com> wrote:

> >  the lack of a decimal place had historically not been significant
>
> Version 4 of S (c. 1991) and versions of S+ based on it treated a sequence
> of digits without a decimal  point as integer.
>
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
>
> On Sat, Aug 25, 2018 at 4:33 PM, Duncan Murdoch <murdoch.duncan at gmail.com>
> wrote:
>
>> On 25/08/2018 4:49 PM, Herv? Pag?s wrote:
>>
>>> The choice of the L suffix in R to mean "R integer type", which
>>> is mapped to the "int" type at the C level, and NOT to the "long int"
>>> type, is really unfortunate as it seems to be misleading and confusing
>>> a lot of people.
>>>
>>
>> Can you provide any evidence of that (e.g. a link to a message from one
>> of these people)?  I think a lot of people don't really know about the L
>> suffix, but that's different from being confused or misleaded by it.
>>
>> And if you make a criticism like that, it would really be fair to suggest
>> what R should have done instead.  I can't think of anything better, given
>> that "i" was already taken, and that the lack of a decimal place had
>> historically not been significant.  Using "I" *would* have been confusing
>> (3i versus 3I being very different).  Deciding that 3 suddenly became an
>> integer value different from 3. would have led to lots of inefficient
>> conversions (since stats mainly deals with floating point values).
>>
>> Duncan Murdoch
>>
>>
>>
>>> The fact that nowadays "int" and "long int" have the same size on most
>>> platforms is only anecdotal here.
>>>
>>> Just my 2 cents.
>>>
>>> H.
>>>
>>> On 08/25/2018 10:01 AM, Dirk Eddelbuettel wrote:
>>>
>>>>
>>>> On 25 August 2018 at 09:28, Carl Boettiger wrote:
>>>> | I always thought it meant "Long" (I'm assuming R's integers are long
>>>> | integers in C sense (iirrc one can declare 'long x', and it being
>>>> common to
>>>> | refer to integers as "longs"  in the same way we use "doubles" to mean
>>>> | double precision floating point).  But pure speculation on my part,
>>>> so I'm
>>>> | curious!
>>>>
>>>> It does per my copy (dated 1990 !!) of the 2nd ed of Kernighan &
>>>> Ritchie.  It
>>>> explicitly mentions (sec 2.2) that 'int' may be 16 or 32 bits, and
>>>> 'long' is
>>>> 32 bit; and (in sec 2.3) introduces the I, U, and L labels for
>>>> constants.  So
>>>> "back then when" 32 bit was indeed long.  And as R uses 32 bit integers
>>>> ...
>>>>
>>>> (It is all murky because the size is an implementation detail and later
>>>> "essentially everybody" moved to 32 bit integers and 64 bit longs as
>>>> the 64
>>>> bit architectures became prevalent.  Which is why when it matters one
>>>> should
>>>> really use more explicit types like int32_t or int64_t.)
>>>>
>>>> Dirk
>>>>
>>>>
>>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>
>

	[[alternative HTML version deleted]]


From tom@@@k@liber@ @ending from gm@il@com  Mon Aug 27 20:42:55 2018
From: tom@@@k@liber@ @ending from gm@il@com (Tomas Kalibera)
Date: Mon, 27 Aug 2018 20:42:55 +0200
Subject: [Rd] 
 Get Logical processor count correctly whether NUMA is enabled
 or disabled
In-Reply-To: <928CB09B31774648949A4D61FE4622AB02335A3A@PWSTLCEXMBX001.AD.MLP.com>
References: <928CB09B31774648949A4D61FE4622AB023343B7@PWSTLCEXMBX001.AD.MLP.com>
 <1ce7682a-4656-0366-ba0f-8c751697db7e@gmail.com>
 <928CB09B31774648949A4D61FE4622AB02335A3A@PWSTLCEXMBX001.AD.MLP.com>
Message-ID: <7ea84564-bc20-ba10-fb78-74c53f65f3b4@gmail.com>

Dear Arun,

thank you for checking the workaround scripts.

I've modified detectCores() to use GetLogicalProcessorInformationEx. It 
is in revision 75198 of R-devel, could you please test it on your 
machines? For a binary, you can wait until the R-devel snapshot build 
gets to at least this svn revision.

Thanks for the link to the processor groups documentation. I don't have 
a machine to test this on, but I would hope that snow clusters (e.g. 
PSOCK) should work fine on systems with >64 logical processors as they 
spawn new processes (not just threads). Note that FORK clusters are not 
supported on Windows.

Thanks
Tomas

On 08/21/2018 02:53 PM, Srinivasan, Arunkumar wrote:
> Dear Tomas, thank you for looking into this. Here's the output:
>
> # number of logical processors - what detectCores() should return
> out <- system("wmic cpu get numberoflogicalprocessors", intern=TRUE)
> [1] "NumberOfLogicalProcessors  \r" "22                         \r" "22                         \r"
> [4] "20                         \r" "22                         \r" "\r"
> sum(as.numeric(gsub("([0-9]+).*", "\\1", grep("[0-9]+[ \t]*", out, value=TRUE))))
> # [1] 86
>
> [I've asked the IT team to understand why one of the values is 20 instead of 22].
>
> # number of cores - what detectCores(FALSE) should return
> out <- system("wmic cpu get numberofcores", intern=TRUE)
> [1] "NumberOfCores  \r" "22             \r" "22             \r" "20             \r" "22             \r"
> [6] "\r"
> sum(as.numeric(gsub("([0-9]+).*", "\\1", grep("[0-9]+[ \t]*", out, value=TRUE))))
> # [1] 86
>
> [Currently hyperthreading is disabled. So this output being identical to the previous output makes sense].
>
> system("wmic computersystem get numberofprocessors")
> NumberOfProcessors
> 4
>
> In addition, I'd also bring to your attention this documentation: https://docs.microsoft.com/en-us/windows/desktop/ProcThread/processor-groups on processor groups which explain how one should go about running a process ro run on multiple groups (which seems to be different to NUMA). All this seems overly complicated to allow a process to use all cores by default TBH.
>
> Here's a project on Github 'fio' where the issue of running a process on more than 1 processor group has come up -  https://github.com/axboe/fio/issues/527 and is addressed - https://github.com/axboe/fio/blob/c479640d6208236744f0562b1e79535eec290e2b/os/os-windows-7.h . I am not sure though if this is entirely relevant since we would be forking new processes in R instead of allowing a single process to use all cores. Apologies if this is utterly irrelevant.
>
> Thank you,
> Arun.
>
> From: Tomas Kalibera <tomas.kalibera at gmail.com>
> Sent: 21 August 2018 11:50
> To: Srinivasan, Arunkumar <Arunkumar.Srinivasan at uk.mlp.com>; r-devel at r-project.org
> Subject: Re: [Rd] Get Logical processor count correctly whether NUMA is enabled or disabled
>
> Dear Arun,
>
> thank you for the report. I agree with the analysis, detectCores() will only report logical processors in the NUMA group in which R is running. I don't have a system to test on, could you please check these workarounds for me on your systems?
>
> # number of logical processors - what detectCores() should return
> out <- system("wmic cpu get numberoflogicalprocessors", intern=TRUE)
> sum(as.numeric(gsub("([0-9]+).*", "\\1", grep("[0-9]+[ \t]*", out, value=TRUE))))
>
> # number of cores - what detectCores(FALSE) should return
> out <- system("wmic cpu get numberofcores", intern=TRUE)
> sum(as.numeric(gsub("([0-9]+).*", "\\1", grep("[0-9]+[ \t]*", out, value=TRUE))))
>
> # number of physical processors - as a sanity check
>
> system("wmic computersystem get numberofprocessors")
>
> Thanks,
> Tomas
>
> On 08/17/2018 05:11 PM, Srinivasan, Arunkumar wrote:
> Dear R-devel list,
>
> R's detectCores() function internally calls "ncpus" function to get the total number of logical processors. However, this doesnot seem to take NUMA into account on Windows machines.
>
> On a machine having 48 processors (24 cores) in total and windows server 2012 installed, if NUMA is enabled and has 2 nodes (node 0 and node 1 each having 24 CPUs), then R's detectCores() only detects 24 instead of the total 48. If NUMA is disabled, detectCores() returns 48.
>
> Similarly, on a machine with 88 cores (176 processors) and windows server 2012, detectCores() with NUMA disabled only returns the maximum value of 64. If NUMA is enabled with 4 nodes (44 processors each), then detectCores() will only return 44. This is particularly limiting since we cannot get to use all processors by enabling/disabling NUMA in this case.
>
> We think this is because R's ncpus.c file uses "PSYSTEM_LOGICAL_PROCESSOR_INFORMATION" (https://msdn.microsoft.com/en-us/library/windows/desktop/ms683194(v=vs.85).aspx) instead of "PSYSTEM_LOGICAL_PROCESSOR_INFORMATION_EX" (https://msdn.microsoft.com/en-us/library/windows/desktop/dd405488(v=vs.85).aspx). Specifically, quoting from the first link:
>
> "On systems with more than 64 logical processors, the?GetLogicalProcessorInformation?function retrieves logical processor information about processors in the?https://msdn.microsoft.com/en-us/library/windows/desktop/dd405503(v=vs.85).aspx?to which the calling thread is currently assigned. Use the?https://msdn.microsoft.com/en-us/library/windows/desktop/dd405488(v=vs.85).aspx?function to retrieve information about processors in all processor groups on the system."
>
> Therefore, it might be possible to get the right count of total processors even with NUMA enabled by using "GetLogicalProcessorInformationEX".  It'd be nice to know what you think.
>
> Thank you very much,
> Arun.
>
> --
> Arun Srinivasan
> Analyst, Millennium Management LLC
> 50 Berkeley Street | London, W1J 8HD
>


From r@ndy@c@@l@i @ending from gm@il@com  Tue Aug 28 21:07:36 2018
From: r@ndy@c@@l@i @ending from gm@il@com (Randy Lai)
Date: Tue, 28 Aug 2018 15:07:36 -0400
Subject: [Rd] "utils::file.edit" does not understand "editor" with
 additional arguments
Message-ID: <6EF551C0-AC7B-412F-965C-36EECBDFBFEA@gmail.com>

I am using Sublime Text as my editor. If I run `subl -n .Rprofile` in bash, a file would be opened in a new window.

Back in R, if I run this

> file.edit(".Rprofile", editor="'subl -n'")
sh: 'subl -n': command not found
Warning message:
error in running command

However, the interesting bit happens when I run

edit(1:10, editor="'subl -n?")

It does open Sublime Text. It seems that `file.edit` and `edit` are behaving differently when ?editor? has additional arguments.

Randy

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 488 bytes
Desc: Message signed with OpenPGP
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20180828/28f66613/attachment.sig>

From henrik@bengt@@on @ending from gm@il@com  Wed Aug 29 05:03:21 2018
From: henrik@bengt@@on @ending from gm@il@com (Henrik Bengtsson)
Date: Tue, 28 Aug 2018 20:03:21 -0700
Subject: [Rd] ROBUSTNESS: x || y and x && y to give warning/error if
 length(x) != 1 or length(y) != 1
Message-ID: <CAFDcVCSEqHvfhD358GD3Fgz6FjuOiMiYHswz+pyTnAEoV0CyZw@mail.gmail.com>

# Issue

'x || y' performs 'x[1] || y' for length(x) > 1.  For instance (here
using R 3.5.1),

> c(TRUE, TRUE) || FALSE
[1] TRUE
> c(TRUE, FALSE) || FALSE
[1] TRUE
> c(TRUE, NA) || FALSE
[1] TRUE
> c(FALSE, TRUE) || FALSE
[1] FALSE

This property is symmetric in LHS and RHS (i.e. 'y || x' behaves the
same) and it also applies to 'x && y'.

Note also how the above truncation of 'x' is completely silent -
there's neither an error nor a warning being produced.


# Discussion/Suggestion

Using 'x || y' and 'x && y' with a non-scalar 'x' or 'y' is likely a
mistake.  Either the code is written assuming 'x' and 'y' are scalars,
or there is a coding error and vectorized versions 'x | y' and 'x & y'
were intended.  Should 'x || y' always be considered an mistake if
'length(x) != 1' or 'length(y) != 1'?  If so, should it be a warning
or an error?  For instance,
'''r
> x <- c(TRUE, TRUE)
> y <- FALSE
> x || y

Error in x || y : applying scalar operator || to non-scalar elements
Execution halted

What about the case where 'length(x) == 0' or 'length(y) == 0'?  Today
'x || y' returns 'NA' in such cases, e.g.

> logical(0) || c(FALSE, NA)
[1] NA
> logical(0) || logical(0)
[1] NA
> logical(0) && logical(0)
[1] NA

I don't know the background for this behavior, but I'm sure there is
an argument behind that one.  Maybe it's simply that '||' and '&&'
should always return a scalar logical and neither TRUE nor FALSE can
be returned.

/Henrik

PS. This is in the same vein as
https://mailman.stat.ethz.ch/pipermail/r-devel/2017-March/073817.html
- in R (>=3.4.0) we now get that if (1:2 == 1) ... is an error if
_R_CHECK_LENGTH_1_CONDITION_=true


From henrik@bengt@@on @ending from gm@il@com  Wed Aug 29 05:17:46 2018
From: henrik@bengt@@on @ending from gm@il@com (Henrik Bengtsson)
Date: Tue, 28 Aug 2018 20:17:46 -0700
Subject: [Rd] Is this a bug in `[`?
In-Reply-To: <CALEXWq2gSrtWt3hZ4igMntTE62+jq4Vh0FDgkHuKp0hhK=ic7w@mail.gmail.com>
References: <4cf085ac-8f7e-38b5-ea02-302df94ad7b5@sapo.pt>
 <CALEXWq2LxX7TUS5ia2R3o6GwxdkDZe_PZ=UZ1zfFWLTY1OTS=g@mail.gmail.com>
 <b36d8e0e-431a-e1a5-bbe4-227351fd8c70@sapo.pt>
 <CAPekMCkyF5f90dXLh6WJZwqXpKPbrg2sLrN-DdjqAoxXGbCtXA@mail.gmail.com>
 <CALEXWq2gSrtWt3hZ4igMntTE62+jq4Vh0FDgkHuKp0hhK=ic7w@mail.gmail.com>
Message-ID: <CAFDcVCQruOy6z4bS1Xqe+acD=jJ5Cnrgpovbam5RkZRPDeij+w@mail.gmail.com>

FYI, this behavior is documented in Section 3.4.1 'Indexing by
vectors' of 'R Language Definition' (accessible for instance via
help.start()):

"*Integer* [...] A special case is the zero index, which has null
effects: x[0] is an empty vector and otherwise including zeros among
positive or negative indices has the same effect as if they were
omitted."

The rest of that section is very useful and well written. I used it as
the go-to reference to implement support for all those indexing
alternatives in matrixStats.

/Henrik
On Sun, Aug 5, 2018 at 3:42 AM I?aki ?car <i.ucar86 at gmail.com> wrote:
>
> El dom., 5 ago. 2018 a las 6:27, Kenny Bell (<kmbell56 at gmail.com>) escribi?:
> >
> > This should more clearly illustrate the issue:
> >
> > c(1, 2, 3, 4)[-seq_len(4)]
> > #> numeric(0)
> > c(1, 2, 3, 4)[-seq_len(3)]
> > #> [1] 4
> > c(1, 2, 3, 4)[-seq_len(2)]
> > #> [1] 3 4
> > c(1, 2, 3, 4)[-seq_len(1)]
> > #> [1] 2 3 4
> > c(1, 2, 3, 4)[-seq_len(0)]
> > #> numeric(0)
> > Created on 2018-08-05 by the reprex package (v0.2.0.9000).
>
> IMO, the problem is that you are reading it sequentially: "-" remove
> "seq_" a sequence "len(0)" of length zero. But that's not how R works
> (how programming languages work in general). Instead, the sequence is
> evaluated in the first place, and then the sign may apply as long as
> you provided something that can hold a sign. And an empty element has
> no sign, so the sign is lost.
>
> I?aki
>
> >
> > On Sun, Aug 5, 2018 at 3:58 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:
> >>
> >>
> >>
> >> ?s 15:51 de 04/08/2018, I?aki ?car escreveu:
> >> > El s?b., 4 ago. 2018 a las 15:32, Rui Barradas
> >> > (<ruipbarradas at sapo.pt>) escribi?:
> >> >>
> >> >> Hello,
> >> >>
> >> >> Maybe I am not understanding how negative indexing works but
> >> >>
> >> >> 1) This is right.
> >> >>
> >> >> (1:10)[-1]
> >> >> #[1]  2  3  4  5  6  7  8  9 10
> >> >>
> >> >> 2) Are these right? They are at least surprising to me.
> >> >>
> >> >> (1:10)[-0]
> >> >> #integer(0)
> >> >>
> >> >> (1:10)[-seq_len(0)]
> >> >> #integer(0)
> >> >>
> >> >>
> >> >> It was the last example that made me ask, seq_len(0) whould avoid an
> >> >> if/else or something similar.
> >> >
> >> > I think it's ok, because there is no negative zero integer, so -0 is 0.
> >>
> >> Ok, this makes sense, I should have thought about that.
> >>
> >> >
> >> > 1.0/-0L # Inf
> >> > 1.0/-0.0 # - Inf
> >> >
> >> > And the same can be said for integer(0), which is the result of
> >> > seq_len(0): there is no negative empty integer.
> >>
> >> I'm not completely convinced about this one, though.
> >> I would expect -seq_len(n) to remove the first n elements from the
> >> vector, therefore, when n == 0, it would remove none.
> >>
> >> And integer(0) is not the same as 0.
> >>
> >> (1:10)[-0] == (1:10)[0] == integer(0) # empty
> >>
> >> (1:10)[-seq_len(0)] == (1:10)[-integer(0)]
> >>
> >>
> >> And I have just reminded myself to run
> >>
> >> identical(-integer(0), integer(0))
> >>
> >> It returns TRUE so my intuition is wrong, R is right.
> >> End of story.
> >>
> >> Thanks for the help,
> >>
> >> Rui Barradas
> >>
> >> >
> >> > I?aki
> >> >
> >> >>
> >> >>
> >> >> Thanks in advance,
> >> >>
> >> >> Rui Barradas
> >> >>
> >> >> ______________________________________________
> >> >> R-devel at r-project.org mailing list
> >> >> https://stat.ethz.ch/mailman/listinfo/r-devel
> >>
> >> ______________________________________________
> >> R-devel at r-project.org mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-devel
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From ruipb@rr@d@@ @ending from @@po@pt  Wed Aug 29 06:44:44 2018
From: ruipb@rr@d@@ @ending from @@po@pt (Rui Barradas)
Date: Wed, 29 Aug 2018 05:44:44 +0100
Subject: [Rd] Is this a bug in `[`?
In-Reply-To: <CAFDcVCQruOy6z4bS1Xqe+acD=jJ5Cnrgpovbam5RkZRPDeij+w@mail.gmail.com>
References: <4cf085ac-8f7e-38b5-ea02-302df94ad7b5@sapo.pt>
 <CALEXWq2LxX7TUS5ia2R3o6GwxdkDZe_PZ=UZ1zfFWLTY1OTS=g@mail.gmail.com>
 <b36d8e0e-431a-e1a5-bbe4-227351fd8c70@sapo.pt>
 <CAPekMCkyF5f90dXLh6WJZwqXpKPbrg2sLrN-DdjqAoxXGbCtXA@mail.gmail.com>
 <CALEXWq2gSrtWt3hZ4igMntTE62+jq4Vh0FDgkHuKp0hhK=ic7w@mail.gmail.com>
 <CAFDcVCQruOy6z4bS1Xqe+acD=jJ5Cnrgpovbam5RkZRPDeij+w@mail.gmail.com>
Message-ID: <f0bbd43f-45d2-36a5-d82d-0112bfe24208@sapo.pt>

Hello,

Thanks for the pointer.
Inline.

On 29/08/2018 04:17, Henrik Bengtsson wrote:
> FYI, this behavior is documented in Section 3.4.1 'Indexing by
> vectors' of 'R Language Definition' (accessible for instance via
> help.start()):
> 
> "*Integer* [...] A special case is the zero index, which has null
> effects: x[0] is an empty vector and otherwise including zeros among
> positive or negative indices has the same effect as if they were
> omitted."
> 

So I was in part right, the zero index is handled as a special case.
My use case was an operation in a function. I wasn't testing whether the 
result was of length zero, I was just using seq_len(result) to avoid the 
test. And found the error surprising.

Thanks again,

Rui Barradas


> The rest of that section is very useful and well written. I used it as
> the go-to reference to implement support for all those indexing
> alternatives in matrixStats.
> 
> /Henrik
> On Sun, Aug 5, 2018 at 3:42 AM I?aki ?car <i.ucar86 at gmail.com> wrote:
>>
>> El dom., 5 ago. 2018 a las 6:27, Kenny Bell (<kmbell56 at gmail.com>) escribi?:
>>>
>>> This should more clearly illustrate the issue:
>>>
>>> c(1, 2, 3, 4)[-seq_len(4)]
>>> #> numeric(0)
>>> c(1, 2, 3, 4)[-seq_len(3)]
>>> #> [1] 4
>>> c(1, 2, 3, 4)[-seq_len(2)]
>>> #> [1] 3 4
>>> c(1, 2, 3, 4)[-seq_len(1)]
>>> #> [1] 2 3 4
>>> c(1, 2, 3, 4)[-seq_len(0)]
>>> #> numeric(0)
>>> Created on 2018-08-05 by the reprex package (v0.2.0.9000).
>>
>> IMO, the problem is that you are reading it sequentially: "-" remove
>> "seq_" a sequence "len(0)" of length zero. But that's not how R works
>> (how programming languages work in general). Instead, the sequence is
>> evaluated in the first place, and then the sign may apply as long as
>> you provided something that can hold a sign. And an empty element has
>> no sign, so the sign is lost.
>>
>> I?aki
>>
>>>
>>> On Sun, Aug 5, 2018 at 3:58 AM Rui Barradas <ruipbarradas at sapo.pt> wrote:
>>>>
>>>>
>>>>
>>>> ?s 15:51 de 04/08/2018, I?aki ?car escreveu:
>>>>> El s?b., 4 ago. 2018 a las 15:32, Rui Barradas
>>>>> (<ruipbarradas at sapo.pt>) escribi?:
>>>>>>
>>>>>> Hello,
>>>>>>
>>>>>> Maybe I am not understanding how negative indexing works but
>>>>>>
>>>>>> 1) This is right.
>>>>>>
>>>>>> (1:10)[-1]
>>>>>> #[1]  2  3  4  5  6  7  8  9 10
>>>>>>
>>>>>> 2) Are these right? They are at least surprising to me.
>>>>>>
>>>>>> (1:10)[-0]
>>>>>> #integer(0)
>>>>>>
>>>>>> (1:10)[-seq_len(0)]
>>>>>> #integer(0)
>>>>>>
>>>>>>
>>>>>> It was the last example that made me ask, seq_len(0) whould avoid an
>>>>>> if/else or something similar.
>>>>>
>>>>> I think it's ok, because there is no negative zero integer, so -0 is 0.
>>>>
>>>> Ok, this makes sense, I should have thought about that.
>>>>
>>>>>
>>>>> 1.0/-0L # Inf
>>>>> 1.0/-0.0 # - Inf
>>>>>
>>>>> And the same can be said for integer(0), which is the result of
>>>>> seq_len(0): there is no negative empty integer.
>>>>
>>>> I'm not completely convinced about this one, though.
>>>> I would expect -seq_len(n) to remove the first n elements from the
>>>> vector, therefore, when n == 0, it would remove none.
>>>>
>>>> And integer(0) is not the same as 0.
>>>>
>>>> (1:10)[-0] == (1:10)[0] == integer(0) # empty
>>>>
>>>> (1:10)[-seq_len(0)] == (1:10)[-integer(0)]
>>>>
>>>>
>>>> And I have just reminded myself to run
>>>>
>>>> identical(-integer(0), integer(0))
>>>>
>>>> It returns TRUE so my intuition is wrong, R is right.
>>>> End of story.
>>>>
>>>> Thanks for the help,
>>>>
>>>> Rui Barradas
>>>>
>>>>>
>>>>> I?aki
>>>>>
>>>>>>
>>>>>>
>>>>>> Thanks in advance,
>>>>>>
>>>>>> Rui Barradas
>>>>>>
>>>>>> ______________________________________________
>>>>>> R-devel at r-project.org mailing list
>>>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>
>>>> ______________________________________________
>>>> R-devel at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 

---
This email has been checked for viruses by AVG.
https://www.avg.com


From Arunkum@r@Sriniv@@@n @ending from uk@mlp@com  Wed Aug 29 12:42:07 2018
From: Arunkum@r@Sriniv@@@n @ending from uk@mlp@com (Srinivasan, Arunkumar)
Date: Wed, 29 Aug 2018 10:42:07 +0000
Subject: [Rd] 
 Get Logical processor count correctly whether NUMA is enabled
 or disabled
In-Reply-To: <7ea84564-bc20-ba10-fb78-74c53f65f3b4@gmail.com>
References: <928CB09B31774648949A4D61FE4622AB023343B7@PWSTLCEXMBX001.AD.MLP.com>
 <1ce7682a-4656-0366-ba0f-8c751697db7e@gmail.com>
 <928CB09B31774648949A4D61FE4622AB02335A3A@PWSTLCEXMBX001.AD.MLP.com>
 <7ea84564-bc20-ba10-fb78-74c53f65f3b4@gmail.com>
Message-ID: <928CB09B31774648949A4D61FE4622AB02338369@PWSTLCEXMBX001.AD.MLP.com>

Dear Tomas, thank you very much. I installed r-devel r75201 and tested.

The machine with 88 cores has NUMA disabled. It therefore has 2 processor groups with 64 and 24 processors each.

require(parallel)
detectCores()
# [1] 88

This is great!

Then I went on to test with a simple 'foreach()' loop. I started with 64 processors (max limit of 1 processor group). I ran with a simple function of 0.5s sleep.

require(snow)
require(doSNOW)
require(foreach)

cl <- makeCluster(64L, "SOCK")
registerDoSNOW(cl)
system.time(foreach(i=1:64) %dopar% Sys.sleep(0.5))
# user  system elapsed 
# 0.06    0.00    0.64 
system.time(foreach(i=1:65) %dopar% Sys.sleep(0.5))
#    user  system elapsed 
#    0.03    0.01    1.04 
stopCluster(cl)

With a cluster of 64 processors and loop running with 64 iterations, it completed in ~.5s (0.64), and with 65 iterations, it took ~1s as expected.
 
cl <- makeCluster(65L, "SOCK")
registerDoSNOW(cl)
system.time(foreach(i=1:64) %dopar% Sys.sleep(0.5))
   user  system elapsed 
   0.03    0.02    0.61 
system.time(foreach(i=1:65) %dopar% Sys.sleep(0.5))
# Timing stopped at: 0.08 0 293
stopCluster(cl)

However, when I increased the cluster to have 65 processors, a loop with 64 iterations seem to complete as expected, but using all 65 processors to loop over 65 iterations didn't seem to complete. I stopped it after ~5mins. The same happens with the cluster started with any number between 65 and 88. It seems to me like we are still not being able to use >64 processors all at the same time even if detectCores() returns the right count now.

I'd appreciate your thoughts on this.

Best,
Arun.

-----Original Message-----
From: Tomas Kalibera <tomas.kalibera at gmail.com> 
Sent: 27 August 2018 19:43
To: Srinivasan, Arunkumar <Arunkumar.Srinivasan at uk.mlp.com>; r-devel at r-project.org
Subject: Re: [Rd] Get Logical processor count correctly whether NUMA is enabled or disabled

Dear Arun,

thank you for checking the workaround scripts.

I've modified detectCores() to use GetLogicalProcessorInformationEx. It is in revision 75198 of R-devel, could you please test it on your machines? For a binary, you can wait until the R-devel snapshot build gets to at least this svn revision.

Thanks for the link to the processor groups documentation. I don't have a machine to test this on, but I would hope that snow clusters (e.g. 
PSOCK) should work fine on systems with >64 logical processors as they spawn new processes (not just threads). Note that FORK clusters are not supported on Windows.

Thanks
Tomas

On 08/21/2018 02:53 PM, Srinivasan, Arunkumar wrote:
> Dear Tomas, thank you for looking into this. Here's the output:
>
> # number of logical processors - what detectCores() should return out 
> <- system("wmic cpu get numberoflogicalprocessors", intern=TRUE)
> [1] "NumberOfLogicalProcessors  \r" "22                         \r" "22                         \r"
> [4] "20                         \r" "22                         \r" "\r"
> sum(as.numeric(gsub("([0-9]+).*", "\\1", grep("[0-9]+[ \t]*", out, 
> value=TRUE)))) # [1] 86
>
> [I've asked the IT team to understand why one of the values is 20 instead of 22].
>
> # number of cores - what detectCores(FALSE) should return out <- 
> system("wmic cpu get numberofcores", intern=TRUE)
> [1] "NumberOfCores  \r" "22             \r" "22             \r" "20             \r" "22             \r"
> [6] "\r"
> sum(as.numeric(gsub("([0-9]+).*", "\\1", grep("[0-9]+[ \t]*", out, 
> value=TRUE)))) # [1] 86
>
> [Currently hyperthreading is disabled. So this output being identical to the previous output makes sense].
>
> system("wmic computersystem get numberofprocessors") 
> NumberOfProcessors
> 4
>
> In addition, I'd also bring to your attention this documentation: https://docs.microsoft.com/en-us/windows/desktop/ProcThread/processor-groups on processor groups which explain how one should go about running a process ro run on multiple groups (which seems to be different to NUMA). All this seems overly complicated to allow a process to use all cores by default TBH.
>
> Here's a project on Github 'fio' where the issue of running a process on more than 1 processor group has come up -  https://github.com/axboe/fio/issues/527 and is addressed - https://github.com/axboe/fio/blob/c479640d6208236744f0562b1e79535eec290e2b/os/os-windows-7.h . I am not sure though if this is entirely relevant since we would be forking new processes in R instead of allowing a single process to use all cores. Apologies if this is utterly irrelevant.
>
> Thank you,
> Arun.
>
> From: Tomas Kalibera <tomas.kalibera at gmail.com>
> Sent: 21 August 2018 11:50
> To: Srinivasan, Arunkumar <Arunkumar.Srinivasan at uk.mlp.com>; 
> r-devel at r-project.org
> Subject: Re: [Rd] Get Logical processor count correctly whether NUMA 
> is enabled or disabled
>
> Dear Arun,
>
> thank you for the report. I agree with the analysis, detectCores() will only report logical processors in the NUMA group in which R is running. I don't have a system to test on, could you please check these workarounds for me on your systems?
>
> # number of logical processors - what detectCores() should return out 
> <- system("wmic cpu get numberoflogicalprocessors", intern=TRUE) 
> sum(as.numeric(gsub("([0-9]+).*", "\\1", grep("[0-9]+[ \t]*", out, 
> value=TRUE))))
>
> # number of cores - what detectCores(FALSE) should return out <- 
> system("wmic cpu get numberofcores", intern=TRUE) 
> sum(as.numeric(gsub("([0-9]+).*", "\\1", grep("[0-9]+[ \t]*", out, 
> value=TRUE))))
>
> # number of physical processors - as a sanity check
>
> system("wmic computersystem get numberofprocessors")
>
> Thanks,
> Tomas
>
> On 08/17/2018 05:11 PM, Srinivasan, Arunkumar wrote:
> Dear R-devel list,
>
> R's detectCores() function internally calls "ncpus" function to get the total number of logical processors. However, this doesnot seem to take NUMA into account on Windows machines.
>
> On a machine having 48 processors (24 cores) in total and windows server 2012 installed, if NUMA is enabled and has 2 nodes (node 0 and node 1 each having 24 CPUs), then R's detectCores() only detects 24 instead of the total 48. If NUMA is disabled, detectCores() returns 48.
>
> Similarly, on a machine with 88 cores (176 processors) and windows server 2012, detectCores() with NUMA disabled only returns the maximum value of 64. If NUMA is enabled with 4 nodes (44 processors each), then detectCores() will only return 44. This is particularly limiting since we cannot get to use all processors by enabling/disabling NUMA in this case.
>
> We think this is because R's ncpus.c file uses "PSYSTEM_LOGICAL_PROCESSOR_INFORMATION" (https://msdn.microsoft.com/en-us/library/windows/desktop/ms683194(v=vs.85).aspx) instead of "PSYSTEM_LOGICAL_PROCESSOR_INFORMATION_EX" (https://msdn.microsoft.com/en-us/library/windows/desktop/dd405488(v=vs.85).aspx). Specifically, quoting from the first link:
>
> "On systems with more than 64 logical processors, the?GetLogicalProcessorInformation?function retrieves logical processor information about processors in the?https://msdn.microsoft.com/en-us/library/windows/desktop/dd405503(v=vs.85).aspx?to which the calling thread is currently assigned. Use the?https://msdn.microsoft.com/en-us/library/windows/desktop/dd405488(v=vs.85).aspx?function to retrieve information about processors in all processor groups on the system."
>
> Therefore, it might be possible to get the right count of total processors even with NUMA enabled by using "GetLogicalProcessorInformationEX".  It'd be nice to know what you think.
>
> Thank you very much,
> Arun.
>
> --
> Arun Srinivasan
> Analyst, Millennium Management LLC
> 50 Berkeley Street | London, W1J 8HD
>

######################################################################

The information contained in this communication is confidential and

intended only for the individual(s) named above. If you are not a named

addressee, please notify the sender immediately and delete this email

from your system and do not disclose the email or any part of it to any

person. The views expressed in this email are the views of the author

and do not necessarily represent the views of Millennium Capital Partners

LLP (MCP LLP) or any of its affiliates. Outgoing and incoming electronic

communications of MCP LLP and its affiliates, including telephone

communications, may be electronically archived and subject to review

and/or disclosure to someone other than the recipient. MCP LLP is

authorized and regulated by the Financial Conduct Authority. Millennium

Capital Partners LLP is a limited liability partnership registered in

England & Wales with number OC312897 and with its registered office at

50 Berkeley Street, London, W1J 8HD.

######################################################################

From h@wickh@m @ending from gm@il@com  Wed Aug 29 23:41:46 2018
From: h@wickh@m @ending from gm@il@com (Hadley Wickham)
Date: Wed, 29 Aug 2018 16:41:46 -0500
Subject: [Rd] conflicted: an alternative conflict resolution strategy
In-Reply-To: <CAMFmJs=5wNzMkDjy1K6hTX1Vs6Hgk+r2BKZBnw_Z8nZG2CQC7Q@mail.gmail.com>
References: <CABdHhvHyV7YXUp9fChvpH-Q7cpW81XhbfNDP8_KwzMFPmAJyHQ@mail.gmail.com>
 <CAMFmJs=5wNzMkDjy1K6hTX1Vs6Hgk+r2BKZBnw_Z8nZG2CQC7Q@mail.gmail.com>
Message-ID: <CABdHhvFL8NwF83u5KU8aFN++r6_wM4_-3_8mr1d3--2AxBpfnQ@mail.gmail.com>

>> conflicted applies a few heuristics to minimise false positives (at the
>> cost of introducing a few false negatives). The overarching goal is to
>> ensure that code behaves identically regardless of the order in which
>> packages are attached.
>>
>> -   A number of packages provide a function that appears to conflict
>>     with a function in a base package, but they follow the superset
>>     principle (i.e. they only extend the API, as explained to me by
>>     Herv? Pages).
>>
>>     conflicted assumes that packages adhere to the superset principle,
>>     which appears to be true in most of the cases that I?ve seen.
>
>
> It seems that you may be able to strengthen this heuristic from a blanket assumption to something more narrowly targeted by looking for one or more of the following to confirm likely-superset adherence
>
> matching or purely extending formals (ie all the named arguments of base::fun match including order, and there are new arguments in pkg::fun only if base::fun takes ...)
> explicit call to  base::fun in the body of pkg::fun
> UseMethod(funname) and at least one provided S3 method calls base::fun
> S4 generic creation using fun or base::fun as the seeding/default method body or called from at least one method

Oooh nice, idea I'll definitely try it out.

>> For
>>     example, the lubridate package provides `as.difftime()` and `date()`
>>     which extend the behaviour of base functions, and provides S4
>>     generics for the set operators.
>>
>>         conflict_scout(c("lubridate", "base"))
>>         #> 5 conflicts:
>>         #> * `as.difftime`: [lubridate]
>>         #> * `date`       : [lubridate]
>>         #> * `intersect`  : [lubridate]
>>         #> * `setdiff`    : [lubridate]
>>         #> * `union`      : [lubridate]
>>
>>     There are two popular functions that don?t adhere to this principle:
>>     `dplyr::filter()` and `dplyr::lag()` :(. conflicted handles these
>>     special cases so they correctly generate conflicts. (I sure wish I?d
>>     know about the subset principle when creating dplyr!)
>>
>>         conflict_scout(c("dplyr", "stats"))
>>         #> 2 conflicts:
>>         #> * `filter`: dplyr, stats
>>         #> * `lag`   : dplyr, stats
>>
>> -   Deprecated functions should never win a conflict, so conflicted
>>     checks for use of `.Deprecated()`. This rule is very useful when
>>     moving functions from one package to another. For example, many
>>     devtools functions were moved to usethis, and conflicted ensures
>>     that you always get the non-deprecated version, regardess of package
>>     attach order:
>
>
> I would completely believe this rule is useful for refactoring as you describe, but that is the "same function" case. For an end-user in the "different function same symbol" case it's not at all clear to me that the deprecated function should always win.
>
> People sometimes use deprecated functions. It's not great, and eventually they'll need to fix that for any given case, but imagine if you deprecated the filter verb in dplyr (I know this will never happen, but I think it's illustrative none the less).
>
> Consider a piece of code someone wrote before this hypothetical deprecation of filter. The fact that it's now deprecated certainly doesn't mean that they secretly wanted stats::filter all along, right? Conflicted acting as if it does will lead to them getting the exact kind of error you're looking to protect them from, and with even less ability to understand why because they are already doing "The right thing" to protect themselves by using conflicted in the first place...

Ah yes, good point. I'll add some heuristic to check that the function
name appears in the first argument of the .Deprecated call (assuming
that the call looks something like `.Deprecated("pkg::foo")`)

>> Finally, as mentioned above, the user can declare preferences:
>>
>>     conflict_prefer("select", "MASS")
>>     #> [conflicted] Will prefer MASS::select over any other package
>>     conflict_scout(c("dplyr", "MASS"))
>>     #> 1 conflict:
>>     #> * `select`: [MASS]
>>
>
> I deeply worry about people putting this kind of thing, or even just library(conflicted), in their .Rprofile and thus making their scripts substantially less reproducible. Is that a consequence you have thought about to this kind of functionality?

Yes, and I've already recommended against it in two places :)  I'm not
sure if there's any more I can do - people already put (e.g.)
`library(ggplot2)` in their .Rprofile, which is just as bad from a
reproducibility standpoint.

Thanks for the thoughtful feedback!

Hadley

-- 
http://hadley.nz


From h@wickh@m @ending from gm@il@com  Wed Aug 29 23:50:08 2018
From: h@wickh@m @ending from gm@il@com (Hadley Wickham)
Date: Wed, 29 Aug 2018 16:50:08 -0500
Subject: [Rd] Where does L come from?
In-Reply-To: <CABdHhvGa67SEpGkCffGpTu6P+u+0sKj-XOMP0ZNTGp0S6RmQqA@mail.gmail.com>
References: <CABdHhvGa67SEpGkCffGpTu6P+u+0sKj-XOMP0ZNTGp0S6RmQqA@mail.gmail.com>
Message-ID: <CABdHhvGKbXCypPBU8_wX08MwMjR7Cu42PW=UHe8cRetP70YngQ@mail.gmail.com>

Thanks for the great discussion everyone!
Hadley
On Sat, Aug 25, 2018 at 8:26 AM Hadley Wickham <h.wickham at gmail.com> wrote:
>
> Hi all,
>
> Would someone mind pointing to me to the inspiration for the use of
> the L suffix to mean "integer"?  This is obviously hard to google for,
> and the R language definition
> (https://cran.r-project.org/doc/manuals/r-release/R-lang.html#Constants)
> is silent.
>
> Hadley
>
> --
> http://hadley.nz



-- 
http://hadley.nz


From ripley @ending from @t@t@@ox@@c@uk  Thu Aug 30 08:14:23 2018
From: ripley @ending from @t@t@@ox@@c@uk (Prof Brian Ripley)
Date: Thu, 30 Aug 2018 07:14:23 +0100
Subject: [Rd] "utils::file.edit" does not understand "editor" with
 additional arguments
In-Reply-To: <6EF551C0-AC7B-412F-965C-36EECBDFBFEA@gmail.com>
References: <6EF551C0-AC7B-412F-965C-36EECBDFBFEA@gmail.com>
Message-ID: <5a8ac853-17c2-4f6e-9d8e-23d3804b3aa0@stats.ox.ac.uk>

We do not have the 'at a minimum' information requested by the posting 
guide, and I cannot reproduce anything like this on a Unix-alike.  Both 
file.edit and edit.default call the same underlying C code, and that 
single-quotes the 'editor' argument to allow for spaces in its path/name 
so I would not expect this to work.

Two workarounds:

1) Set an alias in your shell (e.g. in .bashrc) for 'subl -n'.  This is 
something widely needed on macOS where many editors are invoked by 'open 
-a', and I also use it for 'emacsclient -n'.

2) Make use of the ability to specify editor as an R function, invoking 
the external program by system2() etc.


On 28/08/2018 20:07, Randy Lai wrote:
> I am using Sublime Text as my editor. If I run `subl -n .Rprofile` in bash, a file would be opened in a new window.
> 
> Back in R, if I run this
> 
>> file.edit(".Rprofile", editor="'subl -n'")
> sh: 'subl -n': command not found
> Warning message:
> error in running command
> 
> However, the interesting bit happens when I run
> 
> edit(1:10, editor="'subl -n?")
> 
> It does open Sublime Text. It seems that `file.edit` and `edit` are behaving differently when ?editor? has additional arguments.
> 
> Randy

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Emeritus Professor of Applied Statistics, University of Oxford


From f@ridcher @ending from gm@il@com  Tue Aug 28 07:17:16 2018
From: f@ridcher @ending from gm@il@com (Faridedin Cheraghi)
Date: Tue, 28 Aug 2018 09:47:16 +0430
Subject: [Rd] build package with unicode (farsi) strings
Message-ID: <CAJTBV4V4x=tuv72GZtnQfESG_STHufEDdJfKkvCHkb2zh9XwJQ@mail.gmail.com>

Hi,

I have a R script file with Persian letters in it defined as a variable:

#' @export
letters_fa <- c('???','?','?','?','?','?','?','?','?','?','?','?')

I have specified the encoding field in my DESCRIPTION file of my package.

...
Encoding: UTF-8
...

I also included Sys.setlocale(locale="Persian") in my .RProfile, so it is
executed when RCMD is called. However, after a BUILD and INSTALL, when I
access the variable from the package, the characters are not printed
correctly:
> futils::letters_fa
 [1] "<d8><a7><d9><84><d9><81>" "<d8><a8>"                 "<d9><be>"
           "<d8><aa>"                 "<d8><ab>"
 [6] "<d8><ac>"                 "<da><86>"                 "<d8><ad>"
           "<d8><ae>"                 "<d8><b1>"
[11] "<d8><b2>"                 "<d8><af>"


thanks
Farid

	[[alternative HTML version deleted]]


From tr@xpl@yer @ending from gm@il@com  Tue Aug 28 16:15:09 2018
From: tr@xpl@yer @ending from gm@il@com (=?UTF-8?Q?Martin_M=C3=B8ller_Skarbiniks_Pedersen?=)
Date: Tue, 28 Aug 2018 16:15:09 +0200
Subject: [Rd] plotmath degree symbol
In-Reply-To: <d83487b0-3b9b-43ab-bfd7-6526ee63012a@uni-muenster.de>
References: <d83487b0-3b9b-43ab-bfd7-6526ee63012a@uni-muenster.de>
Message-ID: <CAGAA5beFhuiO4QDm0_NGagti1w7_R4SuyKKTQS2oa5ydiznaVg@mail.gmail.com>

On Fri, 24 Aug 2018 at 19:53, Edzer Pebesma
<edzer.pebesma at uni-muenster.de> wrote:
>
> In plotmath expressions, R's degree symbol, e.g. shown by
>
> plot(1, main = parse(text = "1*degree*C"))
>
> has sunk to halfway the text line, instead of touching its top. In older
> R versions this looked much better.

I can confirm this problem.

R version 3.5.1 (2018-07-02)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Ubuntu 18.04.1 LTS

Matrix products: default
BLAS: /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.7.1
LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.7.1

locale:
 [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
 [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
 [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8
 [7] LC_PAPER=en_US.UTF-8       LC_NAME=C
 [9] LC_ADDRESS=C               LC_TELEPHONE=C
[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

loaded via a namespace (and not attached):
[1] compiler_3.5.1


From thierry@onkelinx @ending from inbo@be  Thu Aug 30 09:05:35 2018
From: thierry@onkelinx @ending from inbo@be (Thierry Onkelinx)
Date: Thu, 30 Aug 2018 09:05:35 +0200
Subject: [Rd] build package with unicode (farsi) strings
In-Reply-To: <CAJTBV4V4x=tuv72GZtnQfESG_STHufEDdJfKkvCHkb2zh9XwJQ@mail.gmail.com>
References: <CAJTBV4V4x=tuv72GZtnQfESG_STHufEDdJfKkvCHkb2zh9XwJQ@mail.gmail.com>
Message-ID: <CAJuCY5zsiVvu_+4hq=6OmWdprNTC1Cu07Vq=zRZdzqfcGMObwg@mail.gmail.com>

Dear Farid,

Try using the ASCII notation. letters_fa <- c("\u0627", "\u0641"). The full
code table is available at https://www.utf8-chartable.de

Best regards,



ir. Thierry Onkelinx
Statisticus / Statistician

Vlaamse Overheid / Government of Flanders
INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
FOREST
Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
thierry.onkelinx at inbo.be
Havenlaan 88 bus 73, 1000 Brussel
www.inbo.be

///////////////////////////////////////////////////////////////////////////////////////////
To call in the statistician after the experiment is done may be no more
than asking him to perform a post-mortem examination: he may be able to say
what the experiment died of. ~ Sir Ronald Aylmer Fisher
The plural of anecdote is not data. ~ Roger Brinner
The combination of some data and an aching desire for an answer does not
ensure that a reasonable answer can be extracted from a given body of data.
~ John Tukey
///////////////////////////////////////////////////////////////////////////////////////////

<https://www.inbo.be>

2018-08-28 7:17 GMT+02:00 Faridedin Cheraghi <faridcher at gmail.com>:

> Hi,
>
> I have a R script file with Persian letters in it defined as a variable:
>
> #' @export
> letters_fa <- c('???','?','?','?','?','?','?','?','?','?','?','?')
>
> I have specified the encoding field in my DESCRIPTION file of my package.
>
> ...
> Encoding: UTF-8
> ...
>
> I also included Sys.setlocale(locale="Persian") in my .RProfile, so it is
> executed when RCMD is called. However, after a BUILD and INSTALL, when I
> access the variable from the package, the characters are not printed
> correctly:
> > futils::letters_fa
>  [1] "<d8><a7><d9><84><d9><81>" "<d8><a8>"                 "<d9><be>"
>            "<d8><aa>"                 "<d8><ab>"
>  [6] "<d8><ac>"                 "<da><86>"                 "<d8><ad>"
>            "<d8><ae>"                 "<d8><b1>"
> [11] "<d8><b2>"                 "<d8><af>"
>
>
> thanks
> Farid
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From emil@bode @ending from d@n@@kn@w@nl  Thu Aug 30 13:09:29 2018
From: emil@bode @ending from d@n@@kn@w@nl (Emil Bode)
Date: Thu, 30 Aug 2018 11:09:29 +0000
Subject: [Rd] ROBUSTNESS: x || y and x && y to give warning/error if
 length(x) != 1 or length(y) != 1
In-Reply-To: <CAFDcVCSEqHvfhD358GD3Fgz6FjuOiMiYHswz+pyTnAEoV0CyZw@mail.gmail.com>
References: <CAFDcVCSEqHvfhD358GD3Fgz6FjuOiMiYHswz+pyTnAEoV0CyZw@mail.gmail.com>
Message-ID: <0C3093D1-5FE0-4DCC-AFC0-CEA6FCBA4580@dans.knaw.nl>

I have to disagree, I think one of the advantages of '||' (or &&) is the lazy evaluation, i.e. you can use the first condition to "not care" about the second (and stop errors from being thrown).
So if I want to check if x is a length-one numeric with value a value between 0 and 1, I can do 'class(x)=='numeric' && length(x)==1 && x>0 && x<1'.
In your proposal, having x=c(1,2) would throw an error or multiple warnings.
Also code that relies on the second argument not being evaluated would break, as we need to evaluate y in order to know length(y)
There may be some benefit in checking for length(x) only, though that could also cause some false positives (e.g. 'x==-1 || length(x)==0' would be a bit ugly, but not necessarily wrong, same for someone too lazy to write x[1] instead of x).

And I don?t really see the advantage. The casting to length one is (I think), a feature, not a bug. If I have/need a length one x, and a length one y, why not use '|' and '&'? I have to admit I only use them in if-statements, and if I need an error to be thrown when x and y are not length one, I can use the shorter versions and then the if throws a warning (or an error for a length-0 or NA result).

I get it that for someone just starting in R, the differences between | and || can be confusing, but I guess that's just the price to pay for having a vectorized language.

Best regards, 
Emil Bode
 
Data-analyst
 
+31 6 43 83 89 33
emil.bode at dans.knaw.nl
 
DANS: Netherlands Institute for Permanent Access to Digital Research Resources
Anna van Saksenlaan 51 | 2593 HW Den Haag | +31 70 349 44 50 | info at dans.knaw.nl <mailto:info at dans.kn> | dans.knaw.nl <applewebdata://71F677F0-6872-45F3-A6C4-4972BF87185B/www.dans.knaw.nl>
DANS is an institute of the Dutch Academy KNAW <http://knaw.nl/nl> and funding organisation NWO <http://www.nwo.nl/>. 

?On 29/08/2018, 05:03, "R-devel on behalf of Henrik Bengtsson" <r-devel-bounces at r-project.org on behalf of henrik.bengtsson at gmail.com> wrote:

    # Issue
    
    'x || y' performs 'x[1] || y' for length(x) > 1.  For instance (here
    using R 3.5.1),
    
    > c(TRUE, TRUE) || FALSE
    [1] TRUE
    > c(TRUE, FALSE) || FALSE
    [1] TRUE
    > c(TRUE, NA) || FALSE
    [1] TRUE
    > c(FALSE, TRUE) || FALSE
    [1] FALSE
    
    This property is symmetric in LHS and RHS (i.e. 'y || x' behaves the
    same) and it also applies to 'x && y'.
    
    Note also how the above truncation of 'x' is completely silent -
    there's neither an error nor a warning being produced.
    
    
    # Discussion/Suggestion
    
    Using 'x || y' and 'x && y' with a non-scalar 'x' or 'y' is likely a
    mistake.  Either the code is written assuming 'x' and 'y' are scalars,
    or there is a coding error and vectorized versions 'x | y' and 'x & y'
    were intended.  Should 'x || y' always be considered an mistake if
    'length(x) != 1' or 'length(y) != 1'?  If so, should it be a warning
    or an error?  For instance,
    '''r
    > x <- c(TRUE, TRUE)
    > y <- FALSE
    > x || y
    
    Error in x || y : applying scalar operator || to non-scalar elements
    Execution halted
    
    What about the case where 'length(x) == 0' or 'length(y) == 0'?  Today
    'x || y' returns 'NA' in such cases, e.g.
    
    > logical(0) || c(FALSE, NA)
    [1] NA
    > logical(0) || logical(0)
    [1] NA
    > logical(0) && logical(0)
    [1] NA
    
    I don't know the background for this behavior, but I'm sure there is
    an argument behind that one.  Maybe it's simply that '||' and '&&'
    should always return a scalar logical and neither TRUE nor FALSE can
    be returned.
    
    /Henrik
    
    PS. This is in the same vein as
    https://mailman.stat.ethz.ch/pipermail/r-devel/2017-March/073817.html
    - in R (>=3.4.0) we now get that if (1:2 == 1) ... is an error if
    _R_CHECK_LENGTH_1_CONDITION_=true
    
    ______________________________________________
    R-devel at r-project.org mailing list
    https://stat.ethz.ch/mailman/listinfo/r-devel
    


From jori@mey@ @ending from gm@il@com  Thu Aug 30 13:56:48 2018
From: jori@mey@ @ending from gm@il@com (Joris Meys)
Date: Thu, 30 Aug 2018 13:56:48 +0200
Subject: [Rd] ROBUSTNESS: x || y and x && y to give warning/error if
 length(x) != 1 or length(y) != 1
In-Reply-To: <CAFDcVCSEqHvfhD358GD3Fgz6FjuOiMiYHswz+pyTnAEoV0CyZw@mail.gmail.com>
References: <CAFDcVCSEqHvfhD358GD3Fgz6FjuOiMiYHswz+pyTnAEoV0CyZw@mail.gmail.com>
Message-ID: <CAO1zAVaCyKOQLFi2161PB3ycHbr72ue3EtTa189Lz-C+rO1_0w@mail.gmail.com>

I have to agree with Emil here. && and || are short circuited like in C and
C++. That means that

TRUE || c(TRUE, FALSE)
FALSE && c(TRUE, FALSE)

cannot give an error because the second part is never evaluated. Throwing a
warning or error for

c(TRUE, FALSE) || TRUE

would mean that the operator gives a different result depending on the
order of the objects, breaking the symmetry. Also that would be undesirable.

Regarding logical(0): per the documentation, it is indeed so that ||, &&
and isTRUE always return a length-one logical vector. Hence the NA.

On a sidenote: there is no such thing as a scalar in R. What you call
scalar, is really a length-one vector. That seems like a detail, but is
important in understanding why this admittedly confusing behaviour actually
makes sense within the framework of R imho. I do understand your objections
and suggestions, but it would boil down to removing short circuited
operators from R.

My 2 cents.
Cheers
Joris

On Wed, Aug 29, 2018 at 5:03 AM Henrik Bengtsson <henrik.bengtsson at gmail.com>
wrote:

> # Issue
>
> 'x || y' performs 'x[1] || y' for length(x) > 1.  For instance (here
> using R 3.5.1),
>
> > c(TRUE, TRUE) || FALSE
> [1] TRUE
> > c(TRUE, FALSE) || FALSE
> [1] TRUE
> > c(TRUE, NA) || FALSE
> [1] TRUE
> > c(FALSE, TRUE) || FALSE
> [1] FALSE
>
> This property is symmetric in LHS and RHS (i.e. 'y || x' behaves the
> same) and it also applies to 'x && y'.
>
> Note also how the above truncation of 'x' is completely silent -
> there's neither an error nor a warning being produced.
>
>
> # Discussion/Suggestion
>
> Using 'x || y' and 'x && y' with a non-scalar 'x' or 'y' is likely a
> mistake.  Either the code is written assuming 'x' and 'y' are scalars,
> or there is a coding error and vectorized versions 'x | y' and 'x & y'
> were intended.  Should 'x || y' always be considered an mistake if
> 'length(x) != 1' or 'length(y) != 1'?  If so, should it be a warning
> or an error?  For instance,
> '''r
> > x <- c(TRUE, TRUE)
> > y <- FALSE
> > x || y
>
> Error in x || y : applying scalar operator || to non-scalar elements
> Execution halted
>
> What about the case where 'length(x) == 0' or 'length(y) == 0'?  Today
> 'x || y' returns 'NA' in such cases, e.g.
>
> > logical(0) || c(FALSE, NA)
> [1] NA
> > logical(0) || logical(0)
> [1] NA
> > logical(0) && logical(0)
> [1] NA
>
> I don't know the background for this behavior, but I'm sure there is
> an argument behind that one.  Maybe it's simply that '||' and '&&'
> should always return a scalar logical and neither TRUE nor FALSE can
> be returned.
>
> /Henrik
>
> PS. This is in the same vein as
> https://mailman.stat.ethz.ch/pipermail/r-devel/2017-March/073817.html
> - in R (>=3.4.0) we now get that if (1:2 == 1) ... is an error if
> _R_CHECK_LENGTH_1_CONDITION_=true
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


-- 
Joris Meys
Statistical consultant

Department of Data Analysis and Mathematical Modelling
Ghent University
Coupure Links 653, B-9000 Gent (Belgium)
<https://maps.google.com/?q=Coupure+links+653,%C2%A0B-9000+Gent,%C2%A0Belgium&entry=gmail&source=g>

-----------
Biowiskundedagen 2017-2018
http://www.biowiskundedagen.ugent.be/

-------------------------------
Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php

	[[alternative HTML version deleted]]


From toth@dene@ @ending from kogentum@hu  Thu Aug 30 14:03:55 2018
From: toth@dene@ @ending from kogentum@hu (=?UTF-8?B?RMOpbmVzIFTDs3Ro?=)
Date: Thu, 30 Aug 2018 14:03:55 +0200
Subject: [Rd] ROBUSTNESS: x || y and x && y to give warning/error if
 length(x) != 1 or length(y) != 1
In-Reply-To: <0C3093D1-5FE0-4DCC-AFC0-CEA6FCBA4580@dans.knaw.nl>
References: <CAFDcVCSEqHvfhD358GD3Fgz6FjuOiMiYHswz+pyTnAEoV0CyZw@mail.gmail.com>
 <0C3093D1-5FE0-4DCC-AFC0-CEA6FCBA4580@dans.knaw.nl>
Message-ID: <438ecf6e-7e56-9408-8b6d-3a70e8ba7b28@kogentum.hu>

Hi,

I absolutely second Henrik's suggestion.

On 08/30/2018 01:09 PM, Emil Bode wrote:
> I have to disagree, I think one of the advantages of '||' (or &&) is the lazy evaluation, i.e. you can use the first condition to "not care" about the second (and stop errors from being thrown).

I do not think Henrik's proposal implies that both arguments of `||` or 
`&&` should be evaluated before the evaluation of the condition. It 
implies that if an argument is evaluated, and its length does not equal 
one, it should return an error instead of the silent truncation of the 
argument.
So your argument is orthogonal to the issue.

> So if I want to check if x is a length-one numeric with value a value between 0 and 1, I can do 'class(x)=='numeric' && length(x)==1 && x>0 && x<1'.
> In your proposal, having x=c(1,2) would throw an error or multiple warnings.
> Also code that relies on the second argument not being evaluated would break, as we need to evaluate y in order to know length(y)
> There may be some benefit in checking for length(x) only, though that could also cause some false positives (e.g. 'x==-1 || length(x)==0' would be a bit ugly, but not necessarily wrong, same for someone too lazy to write x[1] instead of x).
> 
> And I don?t really see the advantage. The casting to length one is (I think), a feature, not a bug. If I have/need a length one x, and a length one y, why not use '|' and '&'? I have to admit I only use them in if-statements, and if I need an error to be thrown when x and y are not length one, I can use the shorter versions and then the if throws a warning (or an error for a length-0 or NA result).
> 
> I get it that for someone just starting in R, the differences between | and || can be confusing, but I guess that's just the price to pay for having a vectorized language.

I use R for about 10 years, and use regularly `||` and `&&` for the 
standard purpose (implemented in most programming languages for the same 
purpose, that is, no evaluation of all arguments if it is not required 
to decide whether the condition is TRUE). I can not recall any single 
case when I wanted to use them for the purpose to evaluate whether the 
*first* elements of vectors fulfill the given condition.

However, I regularly write mistakenly `||` or `&&` when I actually want 
to write `|` or `&`, and have no chance to spot the error because of the 
silent truncation of the arguments.


Regards,
Denes



> 
> Best regards,
> Emil Bode
>   
> Data-analyst
>   
> +31 6 43 83 89 33
> emil.bode at dans.knaw.nl
>   
> DANS: Netherlands Institute for Permanent Access to Digital Research Resources
> Anna van Saksenlaan 51 | 2593 HW Den Haag | +31 70 349 44 50 | info at dans.knaw.nl <mailto:info at dans.kn> | dans.knaw.nl <applewebdata://71F677F0-6872-45F3-A6C4-4972BF87185B/www.dans.knaw.nl>
> DANS is an institute of the Dutch Academy KNAW <http://knaw.nl/nl> and funding organisation NWO <http://www.nwo.nl/>.
> 
> ?On 29/08/2018, 05:03, "R-devel on behalf of Henrik Bengtsson" <r-devel-bounces at r-project.org on behalf of henrik.bengtsson at gmail.com> wrote:
> 
>      # Issue
>      
>      'x || y' performs 'x[1] || y' for length(x) > 1.  For instance (here
>      using R 3.5.1),
>      
>      > c(TRUE, TRUE) || FALSE
>      [1] TRUE
>      > c(TRUE, FALSE) || FALSE
>      [1] TRUE
>      > c(TRUE, NA) || FALSE
>      [1] TRUE
>      > c(FALSE, TRUE) || FALSE
>      [1] FALSE
>      
>      This property is symmetric in LHS and RHS (i.e. 'y || x' behaves the
>      same) and it also applies to 'x && y'.
>      
>      Note also how the above truncation of 'x' is completely silent -
>      there's neither an error nor a warning being produced.
>      
>      
>      # Discussion/Suggestion
>      
>      Using 'x || y' and 'x && y' with a non-scalar 'x' or 'y' is likely a
>      mistake.  Either the code is written assuming 'x' and 'y' are scalars,
>      or there is a coding error and vectorized versions 'x | y' and 'x & y'
>      were intended.  Should 'x || y' always be considered an mistake if
>      'length(x) != 1' or 'length(y) != 1'?  If so, should it be a warning
>      or an error?  For instance,
>      '''r
>      > x <- c(TRUE, TRUE)
>      > y <- FALSE
>      > x || y
>      
>      Error in x || y : applying scalar operator || to non-scalar elements
>      Execution halted
>      
>      What about the case where 'length(x) == 0' or 'length(y) == 0'?  Today
>      'x || y' returns 'NA' in such cases, e.g.
>      
>      > logical(0) || c(FALSE, NA)
>      [1] NA
>      > logical(0) || logical(0)
>      [1] NA
>      > logical(0) && logical(0)
>      [1] NA
>      
>      I don't know the background for this behavior, but I'm sure there is
>      an argument behind that one.  Maybe it's simply that '||' and '&&'
>      should always return a scalar logical and neither TRUE nor FALSE can
>      be returned.
>      
>      /Henrik
>      
>      PS. This is in the same vein as
>      https://mailman.stat.ethz.ch/pipermail/r-devel/2017-March/073817.html
>      - in R (>=3.4.0) we now get that if (1:2 == 1) ... is an error if
>      _R_CHECK_LENGTH_1_CONDITION_=true
>      
>      ______________________________________________
>      R-devel at r-project.org mailing list
>      https://stat.ethz.ch/mailman/listinfo/r-devel
>      
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From toth@dene@ @ending from kogentum@hu  Thu Aug 30 14:09:51 2018
From: toth@dene@ @ending from kogentum@hu (=?UTF-8?B?RMOpbmVzIFTDs3Ro?=)
Date: Thu, 30 Aug 2018 14:09:51 +0200
Subject: [Rd] ROBUSTNESS: x || y and x && y to give warning/error if
 length(x) != 1 or length(y) != 1
In-Reply-To: <CAO1zAVaCyKOQLFi2161PB3ycHbr72ue3EtTa189Lz-C+rO1_0w@mail.gmail.com>
References: <CAFDcVCSEqHvfhD358GD3Fgz6FjuOiMiYHswz+pyTnAEoV0CyZw@mail.gmail.com>
 <CAO1zAVaCyKOQLFi2161PB3ycHbr72ue3EtTa189Lz-C+rO1_0w@mail.gmail.com>
Message-ID: <c5c5721f-65fb-39f0-c7e9-77bb413fbb03@kogentum.hu>



On 08/30/2018 01:56 PM, Joris Meys wrote:
> I have to agree with Emil here. && and || are short circuited like in C and
> C++. That means that
> 
> TRUE || c(TRUE, FALSE)
> FALSE && c(TRUE, FALSE)
> 
> cannot give an error because the second part is never evaluated. Throwing a
> warning or error for
> 
> c(TRUE, FALSE) || TRUE
> 
> would mean that the operator gives a different result depending on the
> order of the objects, breaking the symmetry. Also that would be undesirable.

Note that `||` and `&&` have never been symmetric:

TRUE || stop() # returns TRUE
stop() || TRUE # returns an error


> 
> Regarding logical(0): per the documentation, it is indeed so that ||, &&
> and isTRUE always return a length-one logical vector. Hence the NA.
> 
> On a sidenote: there is no such thing as a scalar in R. What you call
> scalar, is really a length-one vector. That seems like a detail, but is
> important in understanding why this admittedly confusing behaviour actually
> makes sense within the framework of R imho. I do understand your objections
> and suggestions, but it would boil down to removing short circuited
> operators from R.
> 
> My 2 cents.
> Cheers
> Joris
> 
> On Wed, Aug 29, 2018 at 5:03 AM Henrik Bengtsson <henrik.bengtsson at gmail.com>
> wrote:
> 
>> # Issue
>>
>> 'x || y' performs 'x[1] || y' for length(x) > 1.  For instance (here
>> using R 3.5.1),
>>
>>> c(TRUE, TRUE) || FALSE
>> [1] TRUE
>>> c(TRUE, FALSE) || FALSE
>> [1] TRUE
>>> c(TRUE, NA) || FALSE
>> [1] TRUE
>>> c(FALSE, TRUE) || FALSE
>> [1] FALSE
>>
>> This property is symmetric in LHS and RHS (i.e. 'y || x' behaves the
>> same) and it also applies to 'x && y'.
>>
>> Note also how the above truncation of 'x' is completely silent -
>> there's neither an error nor a warning being produced.
>>
>>
>> # Discussion/Suggestion
>>
>> Using 'x || y' and 'x && y' with a non-scalar 'x' or 'y' is likely a
>> mistake.  Either the code is written assuming 'x' and 'y' are scalars,
>> or there is a coding error and vectorized versions 'x | y' and 'x & y'
>> were intended.  Should 'x || y' always be considered an mistake if
>> 'length(x) != 1' or 'length(y) != 1'?  If so, should it be a warning
>> or an error?  For instance,
>> '''r
>>> x <- c(TRUE, TRUE)
>>> y <- FALSE
>>> x || y
>>
>> Error in x || y : applying scalar operator || to non-scalar elements
>> Execution halted
>>
>> What about the case where 'length(x) == 0' or 'length(y) == 0'?  Today
>> 'x || y' returns 'NA' in such cases, e.g.
>>
>>> logical(0) || c(FALSE, NA)
>> [1] NA
>>> logical(0) || logical(0)
>> [1] NA
>>> logical(0) && logical(0)
>> [1] NA
>>
>> I don't know the background for this behavior, but I'm sure there is
>> an argument behind that one.  Maybe it's simply that '||' and '&&'
>> should always return a scalar logical and neither TRUE nor FALSE can
>> be returned.
>>
>> /Henrik
>>
>> PS. This is in the same vein as
>> https://mailman.stat.ethz.ch/pipermail/r-devel/2017-March/073817.html
>> - in R (>=3.4.0) we now get that if (1:2 == 1) ... is an error if
>> _R_CHECK_LENGTH_1_CONDITION_=true
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
> 
>


From jori@mey@ @ending from gm@il@com  Thu Aug 30 14:48:01 2018
From: jori@mey@ @ending from gm@il@com (Joris Meys)
Date: Thu, 30 Aug 2018 14:48:01 +0200
Subject: [Rd] ROBUSTNESS: x || y and x && y to give warning/error if
 length(x) != 1 or length(y) != 1
In-Reply-To: <c5c5721f-65fb-39f0-c7e9-77bb413fbb03@kogentum.hu>
References: <CAFDcVCSEqHvfhD358GD3Fgz6FjuOiMiYHswz+pyTnAEoV0CyZw@mail.gmail.com>
 <CAO1zAVaCyKOQLFi2161PB3ycHbr72ue3EtTa189Lz-C+rO1_0w@mail.gmail.com>
 <c5c5721f-65fb-39f0-c7e9-77bb413fbb03@kogentum.hu>
Message-ID: <CAO1zAVYgQyKP3AbyB_AdTPKogWGQs=cC+YaTtLuG2OSahzvrVA@mail.gmail.com>

On Thu, Aug 30, 2018 at 2:09 PM D?nes T?th <toth.denes at kogentum.hu> wrote:

> Note that `||` and `&&` have never been symmetric:
>
> TRUE || stop() # returns TRUE
> stop() || TRUE # returns an error
>
>
Fair point. So the suggestion would be to check whether x is of length 1
and whether y is of length 1 only when needed. I.e.

c(TRUE,FALSE) || TRUE

would give an error and

TRUE || c(TRUE, FALSE)

would pass.

Thought about it a bit more, and I can't come up with a use case where the
first line must pass. So if the short circuiting remains and the extra
check only gives a small performance penalty, adding the error could indeed
make some bugs more obvious.

Cheers
Joris

-- 
Joris Meys
Statistical consultant

Department of Data Analysis and Mathematical Modelling
Ghent University
Coupure Links 653, B-9000 Gent (Belgium)
<https://maps.google.com/?q=Coupure+links+653,%C2%A0B-9000+Gent,%C2%A0Belgium&entry=gmail&source=g>

-----------
Biowiskundedagen 2017-2018
http://www.biowiskundedagen.ugent.be/

-------------------------------
Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php

	[[alternative HTML version deleted]]


From emil@bode @ending from d@n@@kn@w@nl  Thu Aug 30 16:01:25 2018
From: emil@bode @ending from d@n@@kn@w@nl (Emil Bode)
Date: Thu, 30 Aug 2018 14:01:25 +0000
Subject: [Rd] ROBUSTNESS: x || y and x && y to give warning/error if
 length(x) != 1 or length(y) != 1
In-Reply-To: <CAO1zAVYgQyKP3AbyB_AdTPKogWGQs=cC+YaTtLuG2OSahzvrVA@mail.gmail.com>
References: <CAFDcVCSEqHvfhD358GD3Fgz6FjuOiMiYHswz+pyTnAEoV0CyZw@mail.gmail.com>
 <CAO1zAVaCyKOQLFi2161PB3ycHbr72ue3EtTa189Lz-C+rO1_0w@mail.gmail.com>
 <c5c5721f-65fb-39f0-c7e9-77bb413fbb03@kogentum.hu>
 <CAO1zAVYgQyKP3AbyB_AdTPKogWGQs=cC+YaTtLuG2OSahzvrVA@mail.gmail.com>
Message-ID: <A71C9076-683F-43DE-8DD7-E94F67828806@dans.knaw.nl>

Okay, I thought you always wanted to check the length, but if we can only check what's evaluated I mostly agree.

I still think there's not much wrong with how length-0 logicals are treated, as the return of NA in cases where the value matters is enough warning I think, and I can imagine some code like my previous example 'x==-1 || length(x)==0', which wouldn't need a warning.

But we could do a check for length being >1

Greetings, Emil


?On 30/08/2018, 14:55, "R-devel on behalf of Joris Meys" <r-devel-bounces at r-project.org on behalf of jorismeys at gmail.com> wrote:

    On Thu, Aug 30, 2018 at 2:09 PM D?nes T?th <toth.denes at kogentum.hu> wrote:
    
    > Note that `||` and `&&` have never been symmetric:
    >
    > TRUE || stop() # returns TRUE
    > stop() || TRUE # returns an error
    >
    >
    Fair point. So the suggestion would be to check whether x is of length 1
    and whether y is of length 1 only when needed. I.e.
    
    c(TRUE,FALSE) || TRUE
    
    would give an error and
    
    TRUE || c(TRUE, FALSE)
    
    would pass.
    
    Thought about it a bit more, and I can't come up with a use case where the
    first line must pass. So if the short circuiting remains and the extra
    check only gives a small performance penalty, adding the error could indeed
    make some bugs more obvious.
    
    Cheers
    Joris
    
    -- 
    Joris Meys
    Statistical consultant
    
    Department of Data Analysis and Mathematical Modelling
    Ghent University
    Coupure Links 653, B-9000 Gent (Belgium)
    <https://maps.google.com/?q=Coupure+links+653,%C2%A0B-9000+Gent,%C2%A0Belgium&entry=gmail&source=g>
    
    -----------
    Biowiskundedagen 2017-2018
    http://www.biowiskundedagen.ugent.be/
    
    -------------------------------
    Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php
    
    	[[alternative HTML version deleted]]
    
    ______________________________________________
    R-devel at r-project.org mailing list
    https://stat.ethz.ch/mailman/listinfo/r-devel
    


From h@wickh@m @ending from gm@il@com  Thu Aug 30 16:16:57 2018
From: h@wickh@m @ending from gm@il@com (Hadley Wickham)
Date: Thu, 30 Aug 2018 09:16:57 -0500
Subject: [Rd] ROBUSTNESS: x || y and x && y to give warning/error if
 length(x) != 1 or length(y) != 1
In-Reply-To: <CAFDcVCSEqHvfhD358GD3Fgz6FjuOiMiYHswz+pyTnAEoV0CyZw@mail.gmail.com>
References: <CAFDcVCSEqHvfhD358GD3Fgz6FjuOiMiYHswz+pyTnAEoV0CyZw@mail.gmail.com>
Message-ID: <CABdHhvF+aMURkcp1EGRaPgMCbVg4P_gLwsK30cOVvqS6Wi-+LA@mail.gmail.com>

I think this is an excellent idea as it eliminates a situation which
is almost certainly user error. Making it an error would break a small
amount of existing code (even if for the better), so perhaps it should
start as a warning, but be optionally upgraded to an error. It would
be nice to have a fixed date (R version) in the future when the
default will change to error.

In an ideal world, I think the following four cases should all return
the same error:

if (logical()) 1
#> Error in if (logical()) 1: argument is of length zero
if (c(TRUE, TRUE)) 1
#> Warning in if (c(TRUE, TRUE)) 1: the condition has length > 1 and only the
#> first element will be used
#> [1] 1

logical() || TRUE
#> [1] TRUE
c(TRUE, TRUE) || TRUE
#> [1] TRUE

i.e. I think that `if`, `&&`, and `||` should all check that their
input is a logical (or numeric) vector of length 1.

Hadley

On Tue, Aug 28, 2018 at 10:03 PM Henrik Bengtsson
<henrik.bengtsson at gmail.com> wrote:
>
> # Issue
>
> 'x || y' performs 'x[1] || y' for length(x) > 1.  For instance (here
> using R 3.5.1),
>
> > c(TRUE, TRUE) || FALSE
> [1] TRUE
> > c(TRUE, FALSE) || FALSE
> [1] TRUE
> > c(TRUE, NA) || FALSE
> [1] TRUE
> > c(FALSE, TRUE) || FALSE
> [1] FALSE
>
> This property is symmetric in LHS and RHS (i.e. 'y || x' behaves the
> same) and it also applies to 'x && y'.
>
> Note also how the above truncation of 'x' is completely silent -
> there's neither an error nor a warning being produced.
>
>
> # Discussion/Suggestion
>
> Using 'x || y' and 'x && y' with a non-scalar 'x' or 'y' is likely a
> mistake.  Either the code is written assuming 'x' and 'y' are scalars,
> or there is a coding error and vectorized versions 'x | y' and 'x & y'
> were intended.  Should 'x || y' always be considered an mistake if
> 'length(x) != 1' or 'length(y) != 1'?  If so, should it be a warning
> or an error?  For instance,
> '''r
> > x <- c(TRUE, TRUE)
> > y <- FALSE
> > x || y
>
> Error in x || y : applying scalar operator || to non-scalar elements
> Execution halted
>
> What about the case where 'length(x) == 0' or 'length(y) == 0'?  Today
> 'x || y' returns 'NA' in such cases, e.g.
>
> > logical(0) || c(FALSE, NA)
> [1] NA
> > logical(0) || logical(0)
> [1] NA
> > logical(0) && logical(0)
> [1] NA
>
> I don't know the background for this behavior, but I'm sure there is
> an argument behind that one.  Maybe it's simply that '||' and '&&'
> should always return a scalar logical and neither TRUE nor FALSE can
> be returned.
>
> /Henrik
>
> PS. This is in the same vein as
> https://mailman.stat.ethz.ch/pipermail/r-devel/2017-March/073817.html
> - in R (>=3.4.0) we now get that if (1:2 == 1) ... is an error if
> _R_CHECK_LENGTH_1_CONDITION_=true
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel



-- 
http://hadley.nz


From i@t@z@hn @ending from gm@il@com  Thu Aug 30 17:25:59 2018
From: i@t@z@hn @ending from gm@il@com (Ista Zahn)
Date: Thu, 30 Aug 2018 11:25:59 -0400
Subject: [Rd] build package with unicode (farsi) strings
In-Reply-To: <CAJuCY5zsiVvu_+4hq=6OmWdprNTC1Cu07Vq=zRZdzqfcGMObwg@mail.gmail.com>
References: <CAJTBV4V4x=tuv72GZtnQfESG_STHufEDdJfKkvCHkb2zh9XwJQ@mail.gmail.com>
 <CAJuCY5zsiVvu_+4hq=6OmWdprNTC1Cu07Vq=zRZdzqfcGMObwg@mail.gmail.com>
Message-ID: <CA+vqiLHqopUra-uOn3qt-zxFOUT0bscJpnhUYBit1sFwkT+xKw@mail.gmail.com>

On Thu, Aug 30, 2018 at 3:11 AM Thierry Onkelinx
<thierry.onkelinx at inbo.be> wrote:
>
> Dear Farid,
>
> Try using the ASCII notation. letters_fa <- c("\u0627", "\u0641").

... as recommend in the manual:
https://cran.r-project.org/doc/manuals/r-release/R-exts.html#Encoding-issues

Best,
Ista

The full
> code table is available at https://www.utf8-chartable.de
>
> Best regards,
>
>
>
> ir. Thierry Onkelinx
> Statisticus / Statistician
>
> Vlaamse Overheid / Government of Flanders
> INSTITUUT VOOR NATUUR- EN BOSONDERZOEK / RESEARCH INSTITUTE FOR NATURE AND
> FOREST
> Team Biometrie & Kwaliteitszorg / Team Biometrics & Quality Assurance
> thierry.onkelinx at inbo.be
> Havenlaan 88 bus 73, 1000 Brussel
> www.inbo.be
>
> ///////////////////////////////////////////////////////////////////////////////////////////
> To call in the statistician after the experiment is done may be no more
> than asking him to perform a post-mortem examination: he may be able to say
> what the experiment died of. ~ Sir Ronald Aylmer Fisher
> The plural of anecdote is not data. ~ Roger Brinner
> The combination of some data and an aching desire for an answer does not
> ensure that a reasonable answer can be extracted from a given body of data.
> ~ John Tukey
> ///////////////////////////////////////////////////////////////////////////////////////////
>
> <https://www.inbo.be>
>
> 2018-08-28 7:17 GMT+02:00 Faridedin Cheraghi <faridcher at gmail.com>:
>
> > Hi,
> >
> > I have a R script file with Persian letters in it defined as a variable:
> >
> > #' @export
> > letters_fa <- c('???','?','?','?','?','?','?','?','?','?','?','?')
> >
> > I have specified the encoding field in my DESCRIPTION file of my package.
> >
> > ...
> > Encoding: UTF-8
> > ...
> >
> > I also included Sys.setlocale(locale="Persian") in my .RProfile, so it is
> > executed when RCMD is called. However, after a BUILD and INSTALL, when I
> > access the variable from the package, the characters are not printed
> > correctly:
> > > futils::letters_fa
> >  [1] "<d8><a7><d9><84><d9><81>" "<d8><a8>"                 "<d9><be>"
> >            "<d8><aa>"                 "<d8><ab>"
> >  [6] "<d8><ac>"                 "<da><86>"                 "<d8><ad>"
> >            "<d8><ae>"                 "<d8><b1>"
> > [11] "<d8><b2>"                 "<d8><af>"
> >
> >
> > thanks
> > Farid
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From wdunl@p @ending from tibco@com  Thu Aug 30 17:44:43 2018
From: wdunl@p @ending from tibco@com (William Dunlap)
Date: Thu, 30 Aug 2018 08:44:43 -0700
Subject: [Rd] ROBUSTNESS: x || y and x && y to give warning/error if
 length(x) != 1 or length(y) != 1
In-Reply-To: <CABdHhvF+aMURkcp1EGRaPgMCbVg4P_gLwsK30cOVvqS6Wi-+LA@mail.gmail.com>
References: <CAFDcVCSEqHvfhD358GD3Fgz6FjuOiMiYHswz+pyTnAEoV0CyZw@mail.gmail.com>
 <CABdHhvF+aMURkcp1EGRaPgMCbVg4P_gLwsK30cOVvqS6Wi-+LA@mail.gmail.com>
Message-ID: <CAF8bMcZMW9HDC=Gxe-R-Fo1B_0uEm2kJOGDHjDCpZ4kVskiFDA@mail.gmail.com>

Should the following two functions should always give the same result,
except for possible differences in the 'call' component of the warning
or error message?:

  f0 <- function(x, y) x || y
  f1 <- function(x, y) if (x) { TRUE } else { if (y) {TRUE } else { FALSE }
}

And the same for the 'and' version?

  g0 <- function(x, y) x && y
  g1 <- function(x, y) if (x) { if (y) { TRUE } else { FALSE } } else {
FALSE }

The proposal is to make them act the same when length(x) or length(y) is
not 1.
Should they also act the same when x or y is NA?  'if (x)' currently stops
if is.na(x)
and 'x||y' does not.  Or should we continue with 'if' restricted to
bi-valued
logical and '||' and '&&' handling tri-valued logic?



Bill Dunlap
TIBCO Software
wdunlap tibco.com

On Thu, Aug 30, 2018 at 7:16 AM, Hadley Wickham <h.wickham at gmail.com> wrote:

> I think this is an excellent idea as it eliminates a situation which
> is almost certainly user error. Making it an error would break a small
> amount of existing code (even if for the better), so perhaps it should
> start as a warning, but be optionally upgraded to an error. It would
> be nice to have a fixed date (R version) in the future when the
> default will change to error.
>
> In an ideal world, I think the following four cases should all return
> the same error:
>
> if (logical()) 1
> #> Error in if (logical()) 1: argument is of length zero
> if (c(TRUE, TRUE)) 1
> #> Warning in if (c(TRUE, TRUE)) 1: the condition has length > 1 and only
> the
> #> first element will be used
> #> [1] 1
>
> logical() || TRUE
> #> [1] TRUE
> c(TRUE, TRUE) || TRUE
> #> [1] TRUE
>
> i.e. I think that `if`, `&&`, and `||` should all check that their
> input is a logical (or numeric) vector of length 1.
>
> Hadley
>
> On Tue, Aug 28, 2018 at 10:03 PM Henrik Bengtsson
> <henrik.bengtsson at gmail.com> wrote:
> >
> > # Issue
> >
> > 'x || y' performs 'x[1] || y' for length(x) > 1.  For instance (here
> > using R 3.5.1),
> >
> > > c(TRUE, TRUE) || FALSE
> > [1] TRUE
> > > c(TRUE, FALSE) || FALSE
> > [1] TRUE
> > > c(TRUE, NA) || FALSE
> > [1] TRUE
> > > c(FALSE, TRUE) || FALSE
> > [1] FALSE
> >
> > This property is symmetric in LHS and RHS (i.e. 'y || x' behaves the
> > same) and it also applies to 'x && y'.
> >
> > Note also how the above truncation of 'x' is completely silent -
> > there's neither an error nor a warning being produced.
> >
> >
> > # Discussion/Suggestion
> >
> > Using 'x || y' and 'x && y' with a non-scalar 'x' or 'y' is likely a
> > mistake.  Either the code is written assuming 'x' and 'y' are scalars,
> > or there is a coding error and vectorized versions 'x | y' and 'x & y'
> > were intended.  Should 'x || y' always be considered an mistake if
> > 'length(x) != 1' or 'length(y) != 1'?  If so, should it be a warning
> > or an error?  For instance,
> > '''r
> > > x <- c(TRUE, TRUE)
> > > y <- FALSE
> > > x || y
> >
> > Error in x || y : applying scalar operator || to non-scalar elements
> > Execution halted
> >
> > What about the case where 'length(x) == 0' or 'length(y) == 0'?  Today
> > 'x || y' returns 'NA' in such cases, e.g.
> >
> > > logical(0) || c(FALSE, NA)
> > [1] NA
> > > logical(0) || logical(0)
> > [1] NA
> > > logical(0) && logical(0)
> > [1] NA
> >
> > I don't know the background for this behavior, but I'm sure there is
> > an argument behind that one.  Maybe it's simply that '||' and '&&'
> > should always return a scalar logical and neither TRUE nor FALSE can
> > be returned.
> >
> > /Henrik
> >
> > PS. This is in the same vein as
> > https://mailman.stat.ethz.ch/pipermail/r-devel/2017-March/073817.html
> > - in R (>=3.4.0) we now get that if (1:2 == 1) ... is an error if
> > _R_CHECK_LENGTH_1_CONDITION_=true
> >
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
>
>
>
> --
> http://hadley.nz
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From m@echler @ending from @t@t@m@th@ethz@ch  Thu Aug 30 17:58:17 2018
From: m@echler @ending from @t@t@m@th@ethz@ch (Martin Maechler)
Date: Thu, 30 Aug 2018 17:58:17 +0200
Subject: [Rd] ROBUSTNESS: x || y and x && y to give warning/error if
 length(x) != 1 or length(y) != 1
In-Reply-To: <CAO1zAVYgQyKP3AbyB_AdTPKogWGQs=cC+YaTtLuG2OSahzvrVA@mail.gmail.com>
References: <CAFDcVCSEqHvfhD358GD3Fgz6FjuOiMiYHswz+pyTnAEoV0CyZw@mail.gmail.com>
 <CAO1zAVaCyKOQLFi2161PB3ycHbr72ue3EtTa189Lz-C+rO1_0w@mail.gmail.com>
 <c5c5721f-65fb-39f0-c7e9-77bb413fbb03@kogentum.hu>
 <CAO1zAVYgQyKP3AbyB_AdTPKogWGQs=cC+YaTtLuG2OSahzvrVA@mail.gmail.com>
Message-ID: <23432.5145.624837.968060@stat.math.ethz.ch>

>>>>> Joris Meys 
>>>>>     on Thu, 30 Aug 2018 14:48:01 +0200 writes:

    > On Thu, Aug 30, 2018 at 2:09 PM D?nes T?th
    > <toth.denes at kogentum.hu> wrote:
    >> Note that `||` and `&&` have never been symmetric:
    >> 
    >> TRUE || stop() # returns TRUE stop() || TRUE # returns an
    >> error
    >> 
    >> 
    > Fair point. So the suggestion would be to check whether x
    > is of length 1 and whether y is of length 1 only when
    > needed. I.e.

    > c(TRUE,FALSE) || TRUE

    > would give an error and

    > TRUE || c(TRUE, FALSE)

    > would pass.

    > Thought about it a bit more, and I can't come up with a
    > use case where the first line must pass. So if the short
    > circuiting remains and the extra check only gives a small
    > performance penalty, adding the error could indeed make
    > some bugs more obvious.

I agree "in theory".
Thank you, Henrik, for bringing it up!

In practice I think we should start having a warning signalled.
I have checked the source code in the mean time, and the check
is really very cheap
{ because it can/should be done after checking isNumber(): so
  then we know we have an atomic and can use XLENGTH() }


The 0-length case I don't think we should change as I do find
NA (is logical!) to be an appropriate logical answer.

Martin Maechler
ETH Zurich and R Core team.

    > Cheers Joris

    > -- 
    > Joris Meys Statistical consultant


From ruipb@rr@d@@ @ending from @@po@pt  Thu Aug 30 18:16:00 2018
From: ruipb@rr@d@@ @ending from @@po@pt (Rui Barradas)
Date: Thu, 30 Aug 2018 17:16:00 +0100
Subject: [Rd] ROBUSTNESS: x || y and x && y to give warning/error if
 length(x) != 1 or length(y) != 1
In-Reply-To: <CAF8bMcZMW9HDC=Gxe-R-Fo1B_0uEm2kJOGDHjDCpZ4kVskiFDA@mail.gmail.com>
References: <CAFDcVCSEqHvfhD358GD3Fgz6FjuOiMiYHswz+pyTnAEoV0CyZw@mail.gmail.com>
 <CABdHhvF+aMURkcp1EGRaPgMCbVg4P_gLwsK30cOVvqS6Wi-+LA@mail.gmail.com>
 <CAF8bMcZMW9HDC=Gxe-R-Fo1B_0uEm2kJOGDHjDCpZ4kVskiFDA@mail.gmail.com>
Message-ID: <834d32fa-a994-8bfb-0b5c-d4cc518b3171@sapo.pt>

Hello,

Inline.

?s 16:44 de 30/08/2018, William Dunlap via R-devel escreveu:
> Should the following two functions should always give the same result,
> except for possible differences in the 'call' component of the warning
> or error message?:
> 
>    f0 <- function(x, y) x || y
>    f1 <- function(x, y) if (x) { TRUE } else { if (y) {TRUE } else { FALSE }
> }
> 
> And the same for the 'and' version?
> 
>    g0 <- function(x, y) x && y
>    g1 <- function(x, y) if (x) { if (y) { TRUE } else { FALSE } } else {
> FALSE }
> 
> The proposal is to make them act the same when length(x) or length(y) is
> not 1.
> Should they also act the same when x or y is NA?  'if (x)' currently stops
> if is.na(x)
> and 'x||y' does not.  Or should we continue with 'if' restricted to
> bi-valued
> logical and '||' and '&&' handling tri-valued logic?

I expect R to continue to do


f0(FALSE, NA)    # [1] NA
f0(NA, FALSE)    # [1] NA

g0(TRUE, NA)    # [1] NA
g0(NA, TRUE)    # [1] NA

f1(FALSE, NA)
#Error in if (y) { : missing value where TRUE/FALSE needed
f1(NA, FALSE)
#Error in if (x) { : missing value where TRUE/FALSE needed

g1(TRUE, NA)
#Error in if (x) { : missing value where TRUE/FALSE needed
g1(NA, TRUE)
#Error in if (x) { : missing value where TRUE/FALSE needed



Please don't change this.
There's more to the logical operators than the operands' lengths. That 
issue needs to be fixed but it doesn't mean a radical change should happen.
And the same goes for 'if'. Here the problem is completely different, 
there's more to 'if' than '||' and '&&'. Any change should be done with 
increased care. (Which I'm sure will, as always.)

Rui Barradas


> 
> 
> 
> Bill Dunlap
> TIBCO Software
> wdunlap tibco.com
> 
> On Thu, Aug 30, 2018 at 7:16 AM, Hadley Wickham <h.wickham at gmail.com> wrote:
> 
>> I think this is an excellent idea as it eliminates a situation which
>> is almost certainly user error. Making it an error would break a small
>> amount of existing code (even if for the better), so perhaps it should
>> start as a warning, but be optionally upgraded to an error. It would
>> be nice to have a fixed date (R version) in the future when the
>> default will change to error.
>>
>> In an ideal world, I think the following four cases should all return
>> the same error:
>>
>> if (logical()) 1
>> #> Error in if (logical()) 1: argument is of length zero
>> if (c(TRUE, TRUE)) 1
>> #> Warning in if (c(TRUE, TRUE)) 1: the condition has length > 1 and only
>> the
>> #> first element will be used
>> #> [1] 1
>>
>> logical() || TRUE
>> #> [1] TRUE
>> c(TRUE, TRUE) || TRUE
>> #> [1] TRUE
>>
>> i.e. I think that `if`, `&&`, and `||` should all check that their
>> input is a logical (or numeric) vector of length 1.
>>
>> Hadley
>>
>> On Tue, Aug 28, 2018 at 10:03 PM Henrik Bengtsson
>> <henrik.bengtsson at gmail.com> wrote:
>>>
>>> # Issue
>>>
>>> 'x || y' performs 'x[1] || y' for length(x) > 1.  For instance (here
>>> using R 3.5.1),
>>>
>>>> c(TRUE, TRUE) || FALSE
>>> [1] TRUE
>>>> c(TRUE, FALSE) || FALSE
>>> [1] TRUE
>>>> c(TRUE, NA) || FALSE
>>> [1] TRUE
>>>> c(FALSE, TRUE) || FALSE
>>> [1] FALSE
>>>
>>> This property is symmetric in LHS and RHS (i.e. 'y || x' behaves the
>>> same) and it also applies to 'x && y'.
>>>
>>> Note also how the above truncation of 'x' is completely silent -
>>> there's neither an error nor a warning being produced.
>>>
>>>
>>> # Discussion/Suggestion
>>>
>>> Using 'x || y' and 'x && y' with a non-scalar 'x' or 'y' is likely a
>>> mistake.  Either the code is written assuming 'x' and 'y' are scalars,
>>> or there is a coding error and vectorized versions 'x | y' and 'x & y'
>>> were intended.  Should 'x || y' always be considered an mistake if
>>> 'length(x) != 1' or 'length(y) != 1'?  If so, should it be a warning
>>> or an error?  For instance,
>>> '''r
>>>> x <- c(TRUE, TRUE)
>>>> y <- FALSE
>>>> x || y
>>>
>>> Error in x || y : applying scalar operator || to non-scalar elements
>>> Execution halted
>>>
>>> What about the case where 'length(x) == 0' or 'length(y) == 0'?  Today
>>> 'x || y' returns 'NA' in such cases, e.g.
>>>
>>>> logical(0) || c(FALSE, NA)
>>> [1] NA
>>>> logical(0) || logical(0)
>>> [1] NA
>>>> logical(0) && logical(0)
>>> [1] NA
>>>
>>> I don't know the background for this behavior, but I'm sure there is
>>> an argument behind that one.  Maybe it's simply that '||' and '&&'
>>> should always return a scalar logical and neither TRUE nor FALSE can
>>> be returned.
>>>
>>> /Henrik
>>>
>>> PS. This is in the same vein as
>>> https://mailman.stat.ethz.ch/pipermail/r-devel/2017-March/073817.html
>>> - in R (>=3.4.0) we now get that if (1:2 == 1) ... is an error if
>>> _R_CHECK_LENGTH_1_CONDITION_=true
>>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>>
>>
>> --
>> http://hadley.nz
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From jori@mey@ @ending from gm@il@com  Thu Aug 30 18:24:02 2018
From: jori@mey@ @ending from gm@il@com (Joris Meys)
Date: Thu, 30 Aug 2018 18:24:02 +0200
Subject: [Rd] ROBUSTNESS: x || y and x && y to give warning/error if
 length(x) != 1 or length(y) != 1
In-Reply-To: <23432.5145.624837.968060@stat.math.ethz.ch>
References: <CAFDcVCSEqHvfhD358GD3Fgz6FjuOiMiYHswz+pyTnAEoV0CyZw@mail.gmail.com>
 <CAO1zAVaCyKOQLFi2161PB3ycHbr72ue3EtTa189Lz-C+rO1_0w@mail.gmail.com>
 <c5c5721f-65fb-39f0-c7e9-77bb413fbb03@kogentum.hu>
 <CAO1zAVYgQyKP3AbyB_AdTPKogWGQs=cC+YaTtLuG2OSahzvrVA@mail.gmail.com>
 <23432.5145.624837.968060@stat.math.ethz.ch>
Message-ID: <CAO1zAVbp4f9Hv1dMrqLy0LQCkirUFKTdmTp6-UHROQy51avRyw@mail.gmail.com>

On Thu, Aug 30, 2018 at 5:58 PM Martin Maechler <maechler at stat.math.ethz.ch>
wrote:

>
> I agree "in theory".
> Thank you, Henrik, for bringing it up!
>
> In practice I think we should start having a warning signalled.
>

I agree. I wouldn't know who would count on the automatic selection of the
first value, but better safe than sorry.


> I have checked the source code in the mean time, and the check
> is really very cheap
> { because it can/should be done after checking isNumber(): so
>   then we know we have an atomic and can use XLENGTH() }
>
>
That was my idea as well after going through the source code. I didn't want
to state it as I don't know enough of the code base and couldn't see if
there were complications I missed. Thank you for confirming!

Cheers
Joris
-- 
Joris Meys
Statistical consultant

Department of Data Analysis and Mathematical Modelling
Ghent University
Coupure Links 653, B-9000 Gent (Belgium)
<https://maps.google.com/?q=Coupure+links+653,%C2%A0B-9000+Gent,%C2%A0Belgium&entry=gmail&source=g>

-----------
Biowiskundedagen 2017-2018
http://www.biowiskundedagen.ugent.be/

-------------------------------
Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php

	[[alternative HTML version deleted]]


From h@wickh@m @ending from gm@il@com  Thu Aug 30 20:15:21 2018
From: h@wickh@m @ending from gm@il@com (Hadley Wickham)
Date: Thu, 30 Aug 2018 13:15:21 -0500
Subject: [Rd] ROBUSTNESS: x || y and x && y to give warning/error if
 length(x) != 1 or length(y) != 1
In-Reply-To: <23432.5145.624837.968060@stat.math.ethz.ch>
References: <CAFDcVCSEqHvfhD358GD3Fgz6FjuOiMiYHswz+pyTnAEoV0CyZw@mail.gmail.com>
 <CAO1zAVaCyKOQLFi2161PB3ycHbr72ue3EtTa189Lz-C+rO1_0w@mail.gmail.com>
 <c5c5721f-65fb-39f0-c7e9-77bb413fbb03@kogentum.hu>
 <CAO1zAVYgQyKP3AbyB_AdTPKogWGQs=cC+YaTtLuG2OSahzvrVA@mail.gmail.com>
 <23432.5145.624837.968060@stat.math.ethz.ch>
Message-ID: <CABdHhvHvjHNdD3XkU=qA8CzxnL6VFritQ2tPy7JrY8VfbnR4rg@mail.gmail.com>

On Thu, Aug 30, 2018 at 10:58 AM Martin Maechler
<maechler at stat.math.ethz.ch> wrote:
>
> >>>>> Joris Meys
> >>>>>     on Thu, 30 Aug 2018 14:48:01 +0200 writes:
>
>     > On Thu, Aug 30, 2018 at 2:09 PM D?nes T?th
>     > <toth.denes at kogentum.hu> wrote:
>     >> Note that `||` and `&&` have never been symmetric:
>     >>
>     >> TRUE || stop() # returns TRUE stop() || TRUE # returns an
>     >> error
>     >>
>     >>
>     > Fair point. So the suggestion would be to check whether x
>     > is of length 1 and whether y is of length 1 only when
>     > needed. I.e.
>
>     > c(TRUE,FALSE) || TRUE
>
>     > would give an error and
>
>     > TRUE || c(TRUE, FALSE)
>
>     > would pass.
>
>     > Thought about it a bit more, and I can't come up with a
>     > use case where the first line must pass. So if the short
>     > circuiting remains and the extra check only gives a small
>     > performance penalty, adding the error could indeed make
>     > some bugs more obvious.
>
> I agree "in theory".
> Thank you, Henrik, for bringing it up!
>
> In practice I think we should start having a warning signalled.
> I have checked the source code in the mean time, and the check
> is really very cheap
> { because it can/should be done after checking isNumber(): so
>   then we know we have an atomic and can use XLENGTH() }
>
>
> The 0-length case I don't think we should change as I do find
> NA (is logical!) to be an appropriate logical answer.

Can you explain your reasoning a bit more here? I'd like to understand
the general principle, because from my perspective it's more
parsimonious to say that the inputs to || and && must be length 1,
rather than to say that inputs could be length 0 or length 1, and in
the length 0 case they are replaced with NA.

Hadley

-- 
http://hadley.nz


From h@wickh@m @ending from gm@il@com  Fri Aug 31 00:18:17 2018
From: h@wickh@m @ending from gm@il@com (Hadley Wickham)
Date: Thu, 30 Aug 2018 17:18:17 -0500
Subject: [Rd] build package with unicode (farsi) strings
In-Reply-To: <CAJuCY5zsiVvu_+4hq=6OmWdprNTC1Cu07Vq=zRZdzqfcGMObwg@mail.gmail.com>
References: <CAJTBV4V4x=tuv72GZtnQfESG_STHufEDdJfKkvCHkb2zh9XwJQ@mail.gmail.com>
 <CAJuCY5zsiVvu_+4hq=6OmWdprNTC1Cu07Vq=zRZdzqfcGMObwg@mail.gmail.com>
Message-ID: <CABdHhvGdyEG-q9fsBmL8dJ7uqTdGEiD0t4u0fFVO0Jh3RnodTA@mail.gmail.com>

On Thu, Aug 30, 2018 at 2:11 AM Thierry Onkelinx
<thierry.onkelinx at inbo.be> wrote:
>
> Dear Farid,
>
> Try using the ASCII notation. letters_fa <- c("\u0627", "\u0641"). The full
> code table is available at https://www.utf8-chartable.de

It's a little easier to do this with code:

letters_fa <- c('???','?','?','?','?','?','?','?','?','?','?','?')
writeLines(stringi::stri_escape_unicode(letters_fa))
#> \u0627\u0644\u0641
#> \u0628
#> \u067e
#> \u062a
#> \u062b
#> \u062c
#> \u0686
#> \u062d
#> \u062e
#> \u0631
#> \u0632
#> \u062f

Hadley

-- 
http://hadley.nz


From henrik@bengt@@on @ending from gm@il@com  Fri Aug 31 01:18:29 2018
From: henrik@bengt@@on @ending from gm@il@com (Henrik Bengtsson)
Date: Thu, 30 Aug 2018 16:18:29 -0700
Subject: [Rd] Detecting whether a process exists or not by its PID?
Message-ID: <CAFDcVCS85ZWsfOwEn4-3Sev+eykSjkeNwxWq_h1ie+_dkt0B5g@mail.gmail.com>

Hi, I'd like to test whether a (localhost) PSOCK cluster node is still
running or not by its PID, e.g. it may have crashed / core dumped.
I'm ok with getting false-positive results due to *another* process
with the same PID has since started.

I can the PID of each cluster nodes by querying them for their
Sys.getpid(), e.g.

    pids <- parallel::clusterEvalQ(cl, Sys.getpid())

Is there a function in core R for testing whether a process with a
given PID exists or not? From trial'n'error, I found that on Linux:

  pid_exists <- function(pid) as.logical(tools::pskill(pid, signal = 0L))

returns TRUE for existing processes and FALSE otherwise, but I'm not
sure if I can trust this.  It's not a documented feature in
?tools::pskill, which also warns about 'signal' not being standardized
across OSes.

The other Linux alternative I can imagine is:

  pid_exists <- function(pid) system2("ps", args = c("--pid", pid),
stdout = FALSE) == 0L

Can I expect this to work on macOS as well?  What about other *nix systems?

And, finally, what can be done on Windows?

I'm sure there are packages on CRAN that provides this, but I'd like
to keep dependencies at a minimum.

I appreciate any feedback. Thxs,

Henrik


From c@@rdi@g@bor @ending from gm@il@com  Fri Aug 31 08:14:18 2018
From: c@@rdi@g@bor @ending from gm@il@com (=?UTF-8?B?R8OhYm9yIENzw6FyZGk=?=)
Date: Fri, 31 Aug 2018 08:14:18 +0200
Subject: [Rd] Detecting whether a process exists or not by its PID?
In-Reply-To: <CAFDcVCS85ZWsfOwEn4-3Sev+eykSjkeNwxWq_h1ie+_dkt0B5g@mail.gmail.com>
References: <CAFDcVCS85ZWsfOwEn4-3Sev+eykSjkeNwxWq_h1ie+_dkt0B5g@mail.gmail.com>
Message-ID: <CABtg=KmFS4TE3tyNVYc+zp2xXBOxdANsm==cbMtPzYHsKt8q0Q@mail.gmail.com>

On Fri, Aug 31, 2018 at 1:18 AM Henrik Bengtsson
<henrik.bengtsson at gmail.com> wrote:
[...]
>   pid_exists <- function(pid) as.logical(tools::pskill(pid, signal = 0L))
>
> returns TRUE for existing processes and FALSE otherwise, but I'm not
> sure if I can trust this.  It's not a documented feature in
> ?tools::pskill, which also warns about 'signal' not being standardized
> across OSes.

Yes, as long as tools::pskill() is willing to call a killl(0) system
call, AFAIK this will work fine on all UNIX systems.

> The other Linux alternative I can imagine is:
>
>   pid_exists <- function(pid) system2("ps", args = c("--pid", pid),
> stdout = FALSE) == 0L
>
> Can I expect this to work on macOS as well?  What about other *nix systems?

There is no --pid option on macOS. I think simply `ps <pid>` is
better, but some very minimal systems might not have ps at all.

> And, finally, what can be done on Windows?

You need to call OpenProcess from C, or find some base R function that
does that without messing up the process. Seems like tools::psnice()
does that.

> I'm sure there are packages on CRAN that provides this, but I'd like
> to keep dependencies at a minimum.

Yes, e.g. the ps package does this, and it does it properly, i.e. you
don't need to worry about pid reuse. Pid reuse does cause problems
quite frequently, especially on Windows, and especially on a system
that starts a lot of processes, like win-builder.

Gabor

> I appreciate any feedback. Thxs,
>
> Henrik
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From emil@bode @ending from d@n@@kn@w@nl  Fri Aug 31 11:48:57 2018
From: emil@bode @ending from d@n@@kn@w@nl (Emil Bode)
Date: Fri, 31 Aug 2018 09:48:57 +0000
Subject: [Rd] ROBUSTNESS: x || y and x && y to give warning/error if
 length(x) != 1 or length(y) != 1
In-Reply-To: <CABdHhvHvjHNdD3XkU=qA8CzxnL6VFritQ2tPy7JrY8VfbnR4rg@mail.gmail.com>
References: <CAFDcVCSEqHvfhD358GD3Fgz6FjuOiMiYHswz+pyTnAEoV0CyZw@mail.gmail.com>
 <CAO1zAVaCyKOQLFi2161PB3ycHbr72ue3EtTa189Lz-C+rO1_0w@mail.gmail.com>
 <c5c5721f-65fb-39f0-c7e9-77bb413fbb03@kogentum.hu>
 <CAO1zAVYgQyKP3AbyB_AdTPKogWGQs=cC+YaTtLuG2OSahzvrVA@mail.gmail.com>
 <23432.5145.624837.968060@stat.math.ethz.ch>
 <CABdHhvHvjHNdD3XkU=qA8CzxnL6VFritQ2tPy7JrY8VfbnR4rg@mail.gmail.com>
Message-ID: <CF5BD448-B846-43EE-AB33-F3BFD68C5E34@dans.knaw.nl>


?On 30/08/2018, 20:15, "R-devel on behalf of Hadley Wickham" <r-devel-bounces at r-project.org on behalf of h.wickham at gmail.com> wrote:

    On Thu, Aug 30, 2018 at 10:58 AM Martin Maechler
    <maechler at stat.math.ethz.ch> wrote:
    >
    > >>>>> Joris Meys
    > >>>>>     on Thu, 30 Aug 2018 14:48:01 +0200 writes:
    >
    >     > On Thu, Aug 30, 2018 at 2:09 PM D?nes T?th
    >     > <toth.denes at kogentum.hu> wrote:
    >     >> Note that `||` and `&&` have never been symmetric:
    >     >>
    >     >> TRUE || stop() # returns TRUE stop() || TRUE # returns an
    >     >> error
    >     >>
    >     >>
    >     > Fair point. So the suggestion would be to check whether x
    >     > is of length 1 and whether y is of length 1 only when
    >     > needed. I.e.
    >
    >     > c(TRUE,FALSE) || TRUE
    >
    >     > would give an error and
    >
    >     > TRUE || c(TRUE, FALSE)
    >
    >     > would pass.
    >
    >     > Thought about it a bit more, and I can't come up with a
    >     > use case where the first line must pass. So if the short
    >     > circuiting remains and the extra check only gives a small
    >     > performance penalty, adding the error could indeed make
    >     > some bugs more obvious.
    >
    > I agree "in theory".
    > Thank you, Henrik, for bringing it up!
    >
    > In practice I think we should start having a warning signalled.
    > I have checked the source code in the mean time, and the check
    > is really very cheap
    > { because it can/should be done after checking isNumber(): so
    >   then we know we have an atomic and can use XLENGTH() }
    >
    >
    > The 0-length case I don't think we should change as I do find
    > NA (is logical!) to be an appropriate logical answer.
    
    Can you explain your reasoning a bit more here? I'd like to understand
    the general principle, because from my perspective it's more
    parsimonious to say that the inputs to || and && must be length 1,
    rather than to say that inputs could be length 0 or length 1, and in
    the length 0 case they are replaced with NA.
    
    Hadley
    
I would say the value NA would cause warnings later on, that are easy to track down, so a return of NA is far less likely to cause problems than an unintended TRUE or FALSE. And I guess there would be some code reliant on 'logical(0) || TRUE' returning TRUE, that wouldn't necessarily be a mistake.

But I think it's hard to predict how exactly people are using functions. I personally can't imagine a situation where I'd use || or && outside an if-statement, so I'd rather have the current behaviour, because I'm not sure if I'm reliant on logical(0) || TRUE  somewhere in my code (even though that would be ugly code, it's not wrong per se)
But I could always rewrite it, so I believe it's more a question of how much would have to be rewritten. Maybe implement it first in devel, to see how many people would complain?

Emil Bode


    


From f@ridcher @ending from gm@il@com  Fri Aug 31 12:25:52 2018
From: f@ridcher @ending from gm@il@com (Farid Ch)
Date: Fri, 31 Aug 2018 10:25:52 +0000
Subject: [Rd] build package with unicode (farsi) strings
In-Reply-To: <CABdHhvGdyEG-q9fsBmL8dJ7uqTdGEiD0t4u0fFVO0Jh3RnodTA@mail.gmail.com>
References: <CAJTBV4V4x=tuv72GZtnQfESG_STHufEDdJfKkvCHkb2zh9XwJQ@mail.gmail.com>
 <CAJuCY5zsiVvu_+4hq=6OmWdprNTC1Cu07Vq=zRZdzqfcGMObwg@mail.gmail.com>,
 <CABdHhvGdyEG-q9fsBmL8dJ7uqTdGEiD0t4u0fFVO0Jh3RnodTA@mail.gmail.com>
Message-ID: <BN6PR18MB09486544D042B8153FAB5DC0F50F0@BN6PR18MB0948.namprd18.prod.outlook.com>

Thank you all for your valuable insights. The most viable workaround is a modification to the Hadley?s line of code:



stringi::stri_escape_unicode(letters_fa) %>%

paste0("'",.,"'",collapse=',') %>%

paste0('c(',.,')')



which then, the output string could be easily copied and pasted without manual editing. However, imagine you had to do this process to all of your English strings that you write daily! It is not that much productive. Is it?



I think R deserves a better support for internationalization and I know this implies fundamental revisions to the code to avoid the unecessary conversion to a (OS) native locale; i.e. directly reading/writing as unicode.



Farid



________________________________
From: Hadley Wickham <h.wickham at gmail.com>
Sent: Friday, August 31, 2018 2:48:17 AM
To: ONKELINX, Thierry
Cc: faridcher at gmail.com; r-devel at r-project.org
Subject: Re: [Rd] build package with unicode (farsi) strings

On Thu, Aug 30, 2018 at 2:11 AM Thierry Onkelinx
<thierry.onkelinx at inbo.be> wrote:
>
> Dear Farid,
>
> Try using the ASCII notation. letters_fa <- c("\u0627", "\u0641"). The full
> code table is available at https://www.utf8-chartable.de

It's a little easier to do this with code:

letters_fa <- c('???','?','?','?','?','?','?','?','?','?','?','?')
writeLines(stringi::stri_escape_unicode(letters_fa))
#> \u0627\u0644\u0641
#> \u0628
#> \u067e
#> \u062a
#> \u062b
#> \u062c
#> \u0686
#> \u062d
#> \u062e
#> \u0631
#> \u0632
#> \u062f

Hadley

--
http://hadley.nz

	[[alternative HTML version deleted]]


From tom@@@k@liber@ @ending from gm@il@com  Fri Aug 31 14:51:11 2018
From: tom@@@k@liber@ @ending from gm@il@com (Tomas Kalibera)
Date: Fri, 31 Aug 2018 14:51:11 +0200
Subject: [Rd] Detecting whether a process exists or not by its PID?
In-Reply-To: <CAFDcVCS85ZWsfOwEn4-3Sev+eykSjkeNwxWq_h1ie+_dkt0B5g@mail.gmail.com>
References: <CAFDcVCS85ZWsfOwEn4-3Sev+eykSjkeNwxWq_h1ie+_dkt0B5g@mail.gmail.com>
Message-ID: <1ef168ae-9889-de7e-15f2-631f755fa4aa@gmail.com>

On 08/31/2018 01:18 AM, Henrik Bengtsson wrote:
> Hi, I'd like to test whether a (localhost) PSOCK cluster node is still
> running or not by its PID, e.g. it may have crashed / core dumped.
> I'm ok with getting false-positive results due to *another* process
> with the same PID has since started.
kill(sig=0) is specified by POSIX but indeed as you say there is a race 
condition due to PID-reuse.? In principle, detecting that a worker 
process is still alive cannot be done correctly outside base R. At 
user-level I would probably consider some watchdog, e.g. the parallel 
tasks would be repeatedly touching a file.

In base R, one can do this correctly for forked processes via 
mcparallel/mccollect, not for PSOCK cluster workers which are based on 
system() (and I understand it would be a useful feature)

 > j <- mcparallel(Sys.sleep(1000))
 > mccollect(j, wait=FALSE)
NULL

# kill the child process

 > mccollect(j, wait=FALSE)
$`1542`
NULL

More details indeed in ?mcparallel. The key part is that the job must be 
started as non-detached and as soon as mccollect() collects is, 
mccollect() must never be called on it again.

Tomas

>
> I can the PID of each cluster nodes by querying them for their
> Sys.getpid(), e.g.
>
>      pids <- parallel::clusterEvalQ(cl, Sys.getpid())
>
> Is there a function in core R for testing whether a process with a
> given PID exists or not? From trial'n'error, I found that on Linux:
>
>    pid_exists <- function(pid) as.logical(tools::pskill(pid, signal = 0L))
>
> returns TRUE for existing processes and FALSE otherwise, but I'm not
> sure if I can trust this.  It's not a documented feature in
> ?tools::pskill, which also warns about 'signal' not being standardized
> across OSes.
>
> The other Linux alternative I can imagine is:
>
>    pid_exists <- function(pid) system2("ps", args = c("--pid", pid),
> stdout = FALSE) == 0L
>
> Can I expect this to work on macOS as well?  What about other *nix systems?
>
> And, finally, what can be done on Windows?
>
> I'm sure there are packages on CRAN that provides this, but I'd like
> to keep dependencies at a minimum.
>
> I appreciate any feedback. Thxs,
>
> Henrik
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From felix@gm@ern@t @ending from outlook@com  Thu Aug 30 11:27:38 2018
From: felix@gm@ern@t @ending from outlook@com (Felix Ernst)
Date: Thu, 30 Aug 2018 09:27:38 +0000
Subject: [Rd] compairing doubles
Message-ID: <AM0PR0502MB387404BC690F9AF421B6F5EAD6080@AM0PR0502MB3874.eurprd05.prod.outlook.com>

Dear all,

I a bit unsure, whether this qualifies as a bug, but it is definitly a strange behaviour. That why I wanted to discuss it.

With the following function, I want to test for evenly space numbers, starting from anywhere.

.is_continous_evenly_spaced <- function(n){
  if(length(n) < 2) return(FALSE)
  n <- n[order(n)]
  n <- n - min(n)
  step <- n[2] - n[1]
  test <- seq(from = min(n), to = max(n), by = step)
  if(length(n) == length(test) &&
     all(n == test)){
    return(TRUE)
  }
  return(FALSE)
}

> .is_continous_evenly_spaced(c(1,2,3,4))
[1] TRUE
> .is_continous_evenly_spaced(c(1,3,4,5))
[1] FALSE
> .is_continous_evenly_spaced(c(1,1.1,1.2,1.3))
[1] FALSE

I expect the result for 1 and 2, but not for 3. Upon Investigation it turns out, that n == test is TRUE for every pair, but not for the pair of 0.2.

The types reported are always double, however n[2] == 0.1 reports FALSE as well.

The whole problem is solved by switching from all(n == test) to all(as.character(n) == as.character(test)). However that is weird, isn?t it?

Does this work as intended? Thanks for any help, advise and suggestions in advance.

Best regards,
Felix


	[[alternative HTML version deleted]]


From mgiuli@no@m@il @ending from gm@il@com  Fri Aug 31 08:53:02 2018
From: mgiuli@no@m@il @ending from gm@il@com (Marco Giuliano)
Date: Fri, 31 Aug 2018 08:53:02 +0200
Subject: [Rd] Segfault when performing match on POSIXlt object
Message-ID: <CAA9Rc3PFDr_Rn9toAcwpp5MOvcdEbGMgfDnc7n6PFK+cHxZYAw@mail.gmail.com>

Hi All,
I found a possible unexpected behavior when performing match/%in% on
POSIXlt objects, e.g. :

  d <- as.POSIXlt('2018-01-01')

  # match(<anything>,<POSIXlt>) --> segfault
  match(0,d)

  # consequently also this fails :
  0 %in% d

REPORTED ERROR ON LINUX:
   *** caught segfault ***
  address 0x16dc2, cause 'memory not mapped'

Verified on 3.5.0 on linux, 3.5.1 on Windows.

I think this could be a bug, since even if that match operation makes no
sense, the R session is not supposed to crash with segmentation fault, but
rather throw an exception.

Thanks in advance

	[[alternative HTML version deleted]]


From c@@rdi@g@bor @ending from gm@il@com  Fri Aug 31 15:13:22 2018
From: c@@rdi@g@bor @ending from gm@il@com (=?UTF-8?B?R8OhYm9yIENzw6FyZGk=?=)
Date: Fri, 31 Aug 2018 15:13:22 +0200
Subject: [Rd] Detecting whether a process exists or not by its PID?
In-Reply-To: <1ef168ae-9889-de7e-15f2-631f755fa4aa@gmail.com>
References: <CAFDcVCS85ZWsfOwEn4-3Sev+eykSjkeNwxWq_h1ie+_dkt0B5g@mail.gmail.com>
 <1ef168ae-9889-de7e-15f2-631f755fa4aa@gmail.com>
Message-ID: <CABtg=KkpUWebM8o9uVeMuYAfjcMZ17ipQ-dneOfYk4enGgnFTQ@mail.gmail.com>

On Fri, Aug 31, 2018 at 2:51 PM Tomas Kalibera <tomas.kalibera at gmail.com> wrote:
[...]
> kill(sig=0) is specified by POSIX but indeed as you say there is a race
> condition due to PID-reuse.  In principle, detecting that a worker
> process is still alive cannot be done correctly outside base R.

I am not sure why you think so.

> At user-level I would probably consider some watchdog, e.g. the parallel
> tasks would be repeatedly touching a file.

I am pretty sure that there are simpler and better solutions. E.g. one
would be to
ask the worker process for its startup time (with as much precision as possible)
and then use the (pid, startup_time) pair as a unique id.

With this you can check if the process is still running, by checking
that the pid exists,
and that its startup time matches.

This is all very simple with the ps package, on Linux, macOS and Windows.

Gabor

> In base R, one can do this correctly for forked processes via
> mcparallel/mccollect, not for PSOCK cluster workers which are based on
> system() (and I understand it would be a useful feature)
>
>  > j <- mcparallel(Sys.sleep(1000))
>  > mccollect(j, wait=FALSE)
> NULL
>
> # kill the child process
>
>  > mccollect(j, wait=FALSE)
> $`1542`
> NULL
>
> More details indeed in ?mcparallel. The key part is that the job must be
> started as non-detached and as soon as mccollect() collects is,
> mccollect() must never be called on it again.
>
> Tomas
>
> >
> > I can the PID of each cluster nodes by querying them for their
> > Sys.getpid(), e.g.
> >
> >      pids <- parallel::clusterEvalQ(cl, Sys.getpid())
> >
> > Is there a function in core R for testing whether a process with a
> > given PID exists or not? From trial'n'error, I found that on Linux:
> >
> >    pid_exists <- function(pid) as.logical(tools::pskill(pid, signal = 0L))
> >
> > returns TRUE for existing processes and FALSE otherwise, but I'm not
> > sure if I can trust this.  It's not a documented feature in
> > ?tools::pskill, which also warns about 'signal' not being standardized
> > across OSes.
> >
> > The other Linux alternative I can imagine is:
> >
> >    pid_exists <- function(pid) system2("ps", args = c("--pid", pid),
> > stdout = FALSE) == 0L
> >
> > Can I expect this to work on macOS as well?  What about other *nix systems?
> >
> > And, finally, what can be done on Windows?
> >
> > I'm sure there are packages on CRAN that provides this, but I'd like
> > to keep dependencies at a minimum.
> >
> > I appreciate any feedback. Thxs,
> >
> > Henrik
> >
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From tom@@@k@liber@ @ending from gm@il@com  Fri Aug 31 15:35:29 2018
From: tom@@@k@liber@ @ending from gm@il@com (Tomas Kalibera)
Date: Fri, 31 Aug 2018 15:35:29 +0200
Subject: [Rd] Detecting whether a process exists or not by its PID?
In-Reply-To: <CABtg=KkpUWebM8o9uVeMuYAfjcMZ17ipQ-dneOfYk4enGgnFTQ@mail.gmail.com>
References: <CAFDcVCS85ZWsfOwEn4-3Sev+eykSjkeNwxWq_h1ie+_dkt0B5g@mail.gmail.com>
 <1ef168ae-9889-de7e-15f2-631f755fa4aa@gmail.com>
 <CABtg=KkpUWebM8o9uVeMuYAfjcMZ17ipQ-dneOfYk4enGgnFTQ@mail.gmail.com>
Message-ID: <cf3fb3b9-8523-c8c7-700d-bd2a75e5cbd8@gmail.com>

On 08/31/2018 03:13 PM, G?bor Cs?rdi wrote:
> On Fri, Aug 31, 2018 at 2:51 PM Tomas Kalibera <tomas.kalibera at gmail.com> wrote:
> [...]
>> kill(sig=0) is specified by POSIX but indeed as you say there is a race
>> condition due to PID-reuse.  In principle, detecting that a worker
>> process is still alive cannot be done correctly outside base R.
> I am not sure why you think so.
To avoid the race with PID re-use one needs access to signal handling, 
to blocking signals, to handling sigchld. system/system2 and 
mcparallel/mccollect in base R use these features and the interaction is 
still safe given the specific use in system/system2 and 
mcparallel/mccollect, yet would have to be re-visited if either of the 
two uses change. These features cannot be safely used outside of base R 
in contributed packages.

Tomas

>
>> At user-level I would probably consider some watchdog, e.g. the parallel
>> tasks would be repeatedly touching a file.
> I am pretty sure that there are simpler and better solutions. E.g. one
> would be to
> ask the worker process for its startup time (with as much precision as possible)
> and then use the (pid, startup_time) pair as a unique id.
>
> With this you can check if the process is still running, by checking
> that the pid exists,
> and that its startup time matches.
>
> This is all very simple with the ps package, on Linux, macOS and Windows.
>
> Gabor
>
>> In base R, one can do this correctly for forked processes via
>> mcparallel/mccollect, not for PSOCK cluster workers which are based on
>> system() (and I understand it would be a useful feature)
>>
>>   > j <- mcparallel(Sys.sleep(1000))
>>   > mccollect(j, wait=FALSE)
>> NULL
>>
>> # kill the child process
>>
>>   > mccollect(j, wait=FALSE)
>> $`1542`
>> NULL
>>
>> More details indeed in ?mcparallel. The key part is that the job must be
>> started as non-detached and as soon as mccollect() collects is,
>> mccollect() must never be called on it again.
>>
>> Tomas
>>
>>> I can the PID of each cluster nodes by querying them for their
>>> Sys.getpid(), e.g.
>>>
>>>       pids <- parallel::clusterEvalQ(cl, Sys.getpid())
>>>
>>> Is there a function in core R for testing whether a process with a
>>> given PID exists or not? From trial'n'error, I found that on Linux:
>>>
>>>     pid_exists <- function(pid) as.logical(tools::pskill(pid, signal = 0L))
>>>
>>> returns TRUE for existing processes and FALSE otherwise, but I'm not
>>> sure if I can trust this.  It's not a documented feature in
>>> ?tools::pskill, which also warns about 'signal' not being standardized
>>> across OSes.
>>>
>>> The other Linux alternative I can imagine is:
>>>
>>>     pid_exists <- function(pid) system2("ps", args = c("--pid", pid),
>>> stdout = FALSE) == 0L
>>>
>>> Can I expect this to work on macOS as well?  What about other *nix systems?
>>>
>>> And, finally, what can be done on Windows?
>>>
>>> I'm sure there are packages on CRAN that provides this, but I'd like
>>> to keep dependencies at a minimum.
>>>
>>> I appreciate any feedback. Thxs,
>>>
>>> Henrik
>>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel


From iuc@r @ending from fedor@project@org  Fri Aug 31 15:36:49 2018
From: iuc@r @ending from fedor@project@org (=?UTF-8?Q?I=C3=B1aki_Ucar?=)
Date: Fri, 31 Aug 2018 15:36:49 +0200
Subject: [Rd] compairing doubles
In-Reply-To: <AM0PR0502MB387404BC690F9AF421B6F5EAD6080@AM0PR0502MB3874.eurprd05.prod.outlook.com>
References: <AM0PR0502MB387404BC690F9AF421B6F5EAD6080@AM0PR0502MB3874.eurprd05.prod.outlook.com>
Message-ID: <CALEXWq3wSS4uaYu33oA-KUu7gSBuqKYF0ESemwvsbmp6KktOJg@mail.gmail.com>

El vie., 31 ago. 2018 a las 15:10, Felix Ernst
(<felix.gm.ernst at outlook.com>) escribi?:
>
> Dear all,
>
> I a bit unsure, whether this qualifies as a bug, but it is definitly a strange behaviour. That why I wanted to discuss it.
>
> With the following function, I want to test for evenly space numbers, starting from anywhere.
>
> .is_continous_evenly_spaced <- function(n){
>   if(length(n) < 2) return(FALSE)
>   n <- n[order(n)]
>   n <- n - min(n)
>   step <- n[2] - n[1]
>   test <- seq(from = min(n), to = max(n), by = step)
>   if(length(n) == length(test) &&
>      all(n == test)){
>     return(TRUE)
>   }
>   return(FALSE)
> }
>
> > .is_continous_evenly_spaced(c(1,2,3,4))
> [1] TRUE
> > .is_continous_evenly_spaced(c(1,3,4,5))
> [1] FALSE
> > .is_continous_evenly_spaced(c(1,1.1,1.2,1.3))
> [1] FALSE
>
> I expect the result for 1 and 2, but not for 3. Upon Investigation it turns out, that n == test is TRUE for every pair, but not for the pair of 0.2.
>
> The types reported are always double, however n[2] == 0.1 reports FALSE as well.
>
> The whole problem is solved by switching from all(n == test) to all(as.character(n) == as.character(test)). However that is weird, isn?t it?
>
> Does this work as intended? Thanks for any help, advise and suggestions in advance.

I guess this has something to do with how the sequence is built and
the inherent error of floating point arithmetic. In fact, if you
return test minus n, you'll get:

[1] 0.000000e+00 0.000000e+00 2.220446e-16 0.000000e+00

and the error gets bigger when you continue the sequence; e.g., this
is for c(1, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7):

[1] 0.000000e+00 0.000000e+00 2.220446e-16 2.220446e-16 4.440892e-16
[6] 4.440892e-16 4.440892e-16 0.000000e+00

So, independently of this is considered a bug or not, instead of

length(n) == length(test) && all(n == test)

I would use the following condition:

isTRUE(all.equal(n, test))

I?aki

>
> Best regards,
> Felix
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel



-- 
I?aki Ucar


From emil@bode @ending from d@n@@kn@w@nl  Fri Aug 31 15:45:49 2018
From: emil@bode @ending from d@n@@kn@w@nl (Emil Bode)
Date: Fri, 31 Aug 2018 13:45:49 +0000
Subject: [Rd] compairing doubles
In-Reply-To: <CALEXWq3wSS4uaYu33oA-KUu7gSBuqKYF0ESemwvsbmp6KktOJg@mail.gmail.com>
References: <AM0PR0502MB387404BC690F9AF421B6F5EAD6080@AM0PR0502MB3874.eurprd05.prod.outlook.com>
 <CALEXWq3wSS4uaYu33oA-KUu7gSBuqKYF0ESemwvsbmp6KktOJg@mail.gmail.com>
Message-ID: <95F39593-22F4-46B6-A766-8FF60F608C0C@dans.knaw.nl>

Agreed that's it's rounding error, and all.equal would be the way to go.
I wouldn't call it a bug, it's simply part of working with floating point numbers, any language has the same issue.

And while we're at it, I think the function can be a lot shorter:
.is_continous_evenly_spaced <- function(n){
  length(n)>1 && isTRUE(all.equal(n[order(n)], seq(from=min(n), to=max(n), length.out = length(n))))
}

Cheers, Emil

    El vie., 31 ago. 2018 a las 15:10, Felix Ernst
    (<felix.gm.ernst at outlook.com>) escribi?:
    >
    > Dear all,
    >
    > I a bit unsure, whether this qualifies as a bug, but it is definitly a strange behaviour. That why I wanted to discuss it.
    >
    > With the following function, I want to test for evenly space numbers, starting from anywhere.
    >
    > .is_continous_evenly_spaced <- function(n){
    >   if(length(n) < 2) return(FALSE)
    >   n <- n[order(n)]
    >   n <- n - min(n)
    >   step <- n[2] - n[1]
    >   test <- seq(from = min(n), to = max(n), by = step)
    >   if(length(n) == length(test) &&
    >      all(n == test)){
    >     return(TRUE)
    >   }
    >   return(FALSE)
    > }
    >
    > > .is_continous_evenly_spaced(c(1,2,3,4))
    > [1] TRUE
    > > .is_continous_evenly_spaced(c(1,3,4,5))
    > [1] FALSE
    > > .is_continous_evenly_spaced(c(1,1.1,1.2,1.3))
    > [1] FALSE
    >
    > I expect the result for 1 and 2, but not for 3. Upon Investigation it turns out, that n == test is TRUE for every pair, but not for the pair of 0.2.
    >
    > The types reported are always double, however n[2] == 0.1 reports FALSE as well.
    >
    > The whole problem is solved by switching from all(n == test) to all(as.character(n) == as.character(test)). However that is weird, isn?t it?
    >
    > Does this work as intended? Thanks for any help, advise and suggestions in advance.
    
    I guess this has something to do with how the sequence is built and
    the inherent error of floating point arithmetic. In fact, if you
    return test minus n, you'll get:
    
    [1] 0.000000e+00 0.000000e+00 2.220446e-16 0.000000e+00
    
    and the error gets bigger when you continue the sequence; e.g., this
    is for c(1, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7):
    
    [1] 0.000000e+00 0.000000e+00 2.220446e-16 2.220446e-16 4.440892e-16
    [6] 4.440892e-16 4.440892e-16 0.000000e+00
    
    So, independently of this is considered a bug or not, instead of
    
    length(n) == length(test) && all(n == test)
    
    I would use the following condition:
    
    isTRUE(all.equal(n, test))
    
    I?aki
    
    >
    > Best regards,
    > Felix
    >
    >
    >         [[alternative HTML version deleted]]
    >
    > ______________________________________________
    > R-devel at r-project.org mailing list
    > https://stat.ethz.ch/mailman/listinfo/r-devel
    
    
    
    -- 
    I?aki Ucar
    
    ______________________________________________
    R-devel at r-project.org mailing list
    https://stat.ethz.ch/mailman/listinfo/r-devel
    


From c@@rdi@g@bor @ending from gm@il@com  Fri Aug 31 15:48:44 2018
From: c@@rdi@g@bor @ending from gm@il@com (=?UTF-8?B?R8OhYm9yIENzw6FyZGk=?=)
Date: Fri, 31 Aug 2018 15:48:44 +0200
Subject: [Rd] Detecting whether a process exists or not by its PID?
In-Reply-To: <cf3fb3b9-8523-c8c7-700d-bd2a75e5cbd8@gmail.com>
References: <CAFDcVCS85ZWsfOwEn4-3Sev+eykSjkeNwxWq_h1ie+_dkt0B5g@mail.gmail.com>
 <1ef168ae-9889-de7e-15f2-631f755fa4aa@gmail.com>
 <CABtg=KkpUWebM8o9uVeMuYAfjcMZ17ipQ-dneOfYk4enGgnFTQ@mail.gmail.com>
 <cf3fb3b9-8523-c8c7-700d-bd2a75e5cbd8@gmail.com>
Message-ID: <CABtg=Kk6iXKtzuc9F558L8_PmP6r9iPDkynHEmERpRKHA+f8ig@mail.gmail.com>

On Fri, Aug 31, 2018 at 3:35 PM Tomas Kalibera <tomas.kalibera at gmail.com> wrote:
>
> On 08/31/2018 03:13 PM, G?bor Cs?rdi wrote:
> > On Fri, Aug 31, 2018 at 2:51 PM Tomas Kalibera <tomas.kalibera at gmail.com> wrote:
> > [...]
> >> kill(sig=0) is specified by POSIX but indeed as you say there is a race
> >> condition due to PID-reuse.  In principle, detecting that a worker
> >> process is still alive cannot be done correctly outside base R.
> > I am not sure why you think so.
> To avoid the race with PID re-use one needs access to signal handling,
> to blocking signals, to handling sigchld. system/system2 and
> mcparallel/mccollect in base R use these features and the interaction is
> still safe given the specific use in system/system2 and
> mcparallel/mccollect, yet would have to be re-visited if either of the
> two uses change. These features cannot be safely used outside of base R
> in contributed packages.

Yes, _in theory_ this is right, and of course this only works for
child processes.

_In practice_, you do not need signal handling. The startup time stamp
method is
completely fine, because it is practically impossible to have two
processes with the
same pid and the same (high precision) startup time. This method also
works for any
process (not just child processes), so for PSOCK clusters as well.

Gabor

[...]


From iuc@r @ending from fedor@project@org  Fri Aug 31 15:49:00 2018
From: iuc@r @ending from fedor@project@org (=?UTF-8?Q?I=C3=B1aki_Ucar?=)
Date: Fri, 31 Aug 2018 15:49:00 +0200
Subject: [Rd] compairing doubles
In-Reply-To: <CALEXWq3wSS4uaYu33oA-KUu7gSBuqKYF0ESemwvsbmp6KktOJg@mail.gmail.com>
References: <AM0PR0502MB387404BC690F9AF421B6F5EAD6080@AM0PR0502MB3874.eurprd05.prod.outlook.com>
 <CALEXWq3wSS4uaYu33oA-KUu7gSBuqKYF0ESemwvsbmp6KktOJg@mail.gmail.com>
Message-ID: <CALEXWq3rszbbeb2=0rGTtVFjMq7cOF4jqCy5yOC1G3XD_moFbA@mail.gmail.com>

FYI, more fun with floats:

> 0.1+0.1==0.2
[1] TRUE
> 0.1+0.1+0.1+0.1==0.4
[1] TRUE
> 0.1+0.1+0.1==0.3
[1] FALSE
> 0.1+0.1+0.1==0.1*3
[1] TRUE
> 0.3==0.1*3
[1] FALSE

?\_(?)_/?

But this is not R's fault. See: https://0.30000000000000004.com

I?aki

El vie., 31 ago. 2018 a las 15:36, I?aki Ucar
(<iucar at fedoraproject.org>) escribi?:
>
> El vie., 31 ago. 2018 a las 15:10, Felix Ernst
> (<felix.gm.ernst at outlook.com>) escribi?:
> >
> > Dear all,
> >
> > I a bit unsure, whether this qualifies as a bug, but it is definitly a strange behaviour. That why I wanted to discuss it.
> >
> > With the following function, I want to test for evenly space numbers, starting from anywhere.
> >
> > .is_continous_evenly_spaced <- function(n){
> >   if(length(n) < 2) return(FALSE)
> >   n <- n[order(n)]
> >   n <- n - min(n)
> >   step <- n[2] - n[1]
> >   test <- seq(from = min(n), to = max(n), by = step)
> >   if(length(n) == length(test) &&
> >      all(n == test)){
> >     return(TRUE)
> >   }
> >   return(FALSE)
> > }
> >
> > > .is_continous_evenly_spaced(c(1,2,3,4))
> > [1] TRUE
> > > .is_continous_evenly_spaced(c(1,3,4,5))
> > [1] FALSE
> > > .is_continous_evenly_spaced(c(1,1.1,1.2,1.3))
> > [1] FALSE
> >
> > I expect the result for 1 and 2, but not for 3. Upon Investigation it turns out, that n == test is TRUE for every pair, but not for the pair of 0.2.
> >
> > The types reported are always double, however n[2] == 0.1 reports FALSE as well.
> >
> > The whole problem is solved by switching from all(n == test) to all(as.character(n) == as.character(test)). However that is weird, isn?t it?
> >
> > Does this work as intended? Thanks for any help, advise and suggestions in advance.
>
> I guess this has something to do with how the sequence is built and
> the inherent error of floating point arithmetic. In fact, if you
> return test minus n, you'll get:
>
> [1] 0.000000e+00 0.000000e+00 2.220446e-16 0.000000e+00
>
> and the error gets bigger when you continue the sequence; e.g., this
> is for c(1, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7):
>
> [1] 0.000000e+00 0.000000e+00 2.220446e-16 2.220446e-16 4.440892e-16
> [6] 4.440892e-16 4.440892e-16 0.000000e+00
>
> So, independently of this is considered a bug or not, instead of
>
> length(n) == length(test) && all(n == test)
>
> I would use the following condition:
>
> isTRUE(all.equal(n, test))
>
> I?aki
>
> >
> > Best regards,
> > Felix
> >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
>
>
>
> --
> I?aki Ucar



-- 
I?aki Ucar


From m@echler @ending from @t@t@m@th@ethz@ch  Fri Aug 31 16:00:07 2018
From: m@echler @ending from @t@t@m@th@ethz@ch (Martin Maechler)
Date: Fri, 31 Aug 2018 16:00:07 +0200
Subject: [Rd] Segfault when performing match on POSIXlt object
In-Reply-To: <CAA9Rc3PFDr_Rn9toAcwpp5MOvcdEbGMgfDnc7n6PFK+cHxZYAw@mail.gmail.com>
References: <CAA9Rc3PFDr_Rn9toAcwpp5MOvcdEbGMgfDnc7n6PFK+cHxZYAw@mail.gmail.com>
Message-ID: <23433.18919.859126.521094@stat.math.ethz.ch>

>>>>> Marco Giuliano 
>>>>>     on Fri, 31 Aug 2018 08:53:02 +0200 writes:

    > Hi All, I found a possible unexpected behavior when
    > performing match/%in% on POSIXlt objects, e.g. :

    >   d <- as.POSIXlt('2018-01-01')

    >   # match(<anything>,<POSIXlt>) --> segfault match(0,d)

    >   # consequently also this fails : 0 %in% d

    > REPORTED ERROR ON LINUX: *** caught segfault *** address
    > 0x16dc2, cause 'memory not mapped'

    > Verified on 3.5.0 on linux, 3.5.1 on Windows.

Confirmed (Linux, I think all version >= 3.4.0, but not in R
3.3.3 (or earlier, presumably).

Note the segfault happens inspite of the match_transform() utility
which explicitly checks for "POSIXlt" and the code comment which
says that "POSIXlt" should have been transformed to character, 
but seems to fail in recent versions of R.

    > I think this could be a bug, since even if that match
    > operation makes no sense, the R session is not supposed to
    > crash with segmentation fault, but rather throw an
    > exception.

Definitely.  It is a bug.

    > Thanks in advance

Thank you for reporting!

Martin Maechler, ETH Zurich


From m@rk@v@nderloo @ending from gm@il@com  Fri Aug 31 16:00:06 2018
From: m@rk@v@nderloo @ending from gm@il@com (Mark van der Loo)
Date: Fri, 31 Aug 2018 16:00:06 +0200
Subject: [Rd] compairing doubles
In-Reply-To: <95F39593-22F4-46B6-A766-8FF60F608C0C@dans.knaw.nl>
References: <AM0PR0502MB387404BC690F9AF421B6F5EAD6080@AM0PR0502MB3874.eurprd05.prod.outlook.com>
 <CALEXWq3wSS4uaYu33oA-KUu7gSBuqKYF0ESemwvsbmp6KktOJg@mail.gmail.com>
 <95F39593-22F4-46B6-A766-8FF60F608C0C@dans.knaw.nl>
Message-ID: <CAOKDuOgykFYMA-ZT4kUxr_NKXBEBsxZV8s-3sAuQ3Motc5M_Fw@mail.gmail.com>

how about

is_evenly_spaced <- function(x,...) all.equal(diff(sort(x)),...)

(use ellipsis to set tolerance if necessary)


Op vr 31 aug. 2018 om 15:46 schreef Emil Bode <emil.bode at dans.knaw.nl>:

> Agreed that's it's rounding error, and all.equal would be the way to go.
> I wouldn't call it a bug, it's simply part of working with floating point
> numbers, any language has the same issue.
>
> And while we're at it, I think the function can be a lot shorter:
> .is_continous_evenly_spaced <- function(n){
>   length(n)>1 && isTRUE(all.equal(n[order(n)], seq(from=min(n), to=max(n),
> length.out = length(n))))
> }
>
> Cheers, Emil
>
>     El vie., 31 ago. 2018 a las 15:10, Felix Ernst
>     (<felix.gm.ernst at outlook.com>) escribi?:
>     >
>     > Dear all,
>     >
>     > I a bit unsure, whether this qualifies as a bug, but it is definitly
> a strange behaviour. That why I wanted to discuss it.
>     >
>     > With the following function, I want to test for evenly space
> numbers, starting from anywhere.
>     >
>     > .is_continous_evenly_spaced <- function(n){
>     >   if(length(n) < 2) return(FALSE)
>     >   n <- n[order(n)]
>     >   n <- n - min(n)
>     >   step <- n[2] - n[1]
>     >   test <- seq(from = min(n), to = max(n), by = step)
>     >   if(length(n) == length(test) &&
>     >      all(n == test)){
>     >     return(TRUE)
>     >   }
>     >   return(FALSE)
>     > }
>     >
>     > > .is_continous_evenly_spaced(c(1,2,3,4))
>     > [1] TRUE
>     > > .is_continous_evenly_spaced(c(1,3,4,5))
>     > [1] FALSE
>     > > .is_continous_evenly_spaced(c(1,1.1,1.2,1.3))
>     > [1] FALSE
>     >
>     > I expect the result for 1 and 2, but not for 3. Upon Investigation
> it turns out, that n == test is TRUE for every pair, but not for the pair
> of 0.2.
>     >
>     > The types reported are always double, however n[2] == 0.1 reports
> FALSE as well.
>     >
>     > The whole problem is solved by switching from all(n == test) to
> all(as.character(n) == as.character(test)). However that is weird, isn?t it?
>     >
>     > Does this work as intended? Thanks for any help, advise and
> suggestions in advance.
>
>     I guess this has something to do with how the sequence is built and
>     the inherent error of floating point arithmetic. In fact, if you
>     return test minus n, you'll get:
>
>     [1] 0.000000e+00 0.000000e+00 2.220446e-16 0.000000e+00
>
>     and the error gets bigger when you continue the sequence; e.g., this
>     is for c(1, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7):
>
>     [1] 0.000000e+00 0.000000e+00 2.220446e-16 2.220446e-16 4.440892e-16
>     [6] 4.440892e-16 4.440892e-16 0.000000e+00
>
>     So, independently of this is considered a bug or not, instead of
>
>     length(n) == length(test) && all(n == test)
>
>     I would use the following condition:
>
>     isTRUE(all.equal(n, test))
>
>     I?aki
>
>     >
>     > Best regards,
>     > Felix
>     >
>     >
>     >         [[alternative HTML version deleted]]
>     >
>     > ______________________________________________
>     > R-devel at r-project.org mailing list
>     > https://stat.ethz.ch/mailman/listinfo/r-devel
>
>
>
>     --
>     I?aki Ucar
>
>     ______________________________________________
>     R-devel at r-project.org mailing list
>     https://stat.ethz.ch/mailman/listinfo/r-devel
>
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From m@rk@v@nderloo @ending from gm@il@com  Fri Aug 31 16:04:10 2018
From: m@rk@v@nderloo @ending from gm@il@com (Mark van der Loo)
Date: Fri, 31 Aug 2018 16:04:10 +0200
Subject: [Rd] compairing doubles
In-Reply-To: <CAOKDuOgykFYMA-ZT4kUxr_NKXBEBsxZV8s-3sAuQ3Motc5M_Fw@mail.gmail.com>
References: <AM0PR0502MB387404BC690F9AF421B6F5EAD6080@AM0PR0502MB3874.eurprd05.prod.outlook.com>
 <CALEXWq3wSS4uaYu33oA-KUu7gSBuqKYF0ESemwvsbmp6KktOJg@mail.gmail.com>
 <95F39593-22F4-46B6-A766-8FF60F608C0C@dans.knaw.nl>
 <CAOKDuOgykFYMA-ZT4kUxr_NKXBEBsxZV8s-3sAuQ3Motc5M_Fw@mail.gmail.com>
Message-ID: <CAOKDuOi7vPD7Hg855V+i0=dNs99jxRi_Dacz0nS9gusN1Y3K2A@mail.gmail.com>

Sorry for the second e-mail: this is worth watching:
https://www.youtube.com/watch?v=3Bu7QUxzIbA&t=1s
It's Martin Maechler's talk at useR!2018. This kind of stuff should be
mandatory material for any aspiring programmer/data scientist/statistician.

-Mark




Op vr 31 aug. 2018 om 16:00 schreef Mark van der Loo <
mark.vanderloo at gmail.com>:

> how about
>
> is_evenly_spaced <- function(x,...) all.equal(diff(sort(x)),...)
>
> (use ellipsis to set tolerance if necessary)
>
>
> Op vr 31 aug. 2018 om 15:46 schreef Emil Bode <emil.bode at dans.knaw.nl>:
>
>> Agreed that's it's rounding error, and all.equal would be the way to go.
>> I wouldn't call it a bug, it's simply part of working with floating point
>> numbers, any language has the same issue.
>>
>> And while we're at it, I think the function can be a lot shorter:
>> .is_continous_evenly_spaced <- function(n){
>>   length(n)>1 && isTRUE(all.equal(n[order(n)], seq(from=min(n),
>> to=max(n), length.out = length(n))))
>> }
>>
>> Cheers, Emil
>>
>>     El vie., 31 ago. 2018 a las 15:10, Felix Ernst
>>     (<felix.gm.ernst at outlook.com>) escribi?:
>>     >
>>     > Dear all,
>>     >
>>     > I a bit unsure, whether this qualifies as a bug, but it is
>> definitly a strange behaviour. That why I wanted to discuss it.
>>     >
>>     > With the following function, I want to test for evenly space
>> numbers, starting from anywhere.
>>     >
>>     > .is_continous_evenly_spaced <- function(n){
>>     >   if(length(n) < 2) return(FALSE)
>>     >   n <- n[order(n)]
>>     >   n <- n - min(n)
>>     >   step <- n[2] - n[1]
>>     >   test <- seq(from = min(n), to = max(n), by = step)
>>     >   if(length(n) == length(test) &&
>>     >      all(n == test)){
>>     >     return(TRUE)
>>     >   }
>>     >   return(FALSE)
>>     > }
>>     >
>>     > > .is_continous_evenly_spaced(c(1,2,3,4))
>>     > [1] TRUE
>>     > > .is_continous_evenly_spaced(c(1,3,4,5))
>>     > [1] FALSE
>>     > > .is_continous_evenly_spaced(c(1,1.1,1.2,1.3))
>>     > [1] FALSE
>>     >
>>     > I expect the result for 1 and 2, but not for 3. Upon Investigation
>> it turns out, that n == test is TRUE for every pair, but not for the pair
>> of 0.2.
>>     >
>>     > The types reported are always double, however n[2] == 0.1 reports
>> FALSE as well.
>>     >
>>     > The whole problem is solved by switching from all(n == test) to
>> all(as.character(n) == as.character(test)). However that is weird, isn?t it?
>>     >
>>     > Does this work as intended? Thanks for any help, advise and
>> suggestions in advance.
>>
>>     I guess this has something to do with how the sequence is built and
>>     the inherent error of floating point arithmetic. In fact, if you
>>     return test minus n, you'll get:
>>
>>     [1] 0.000000e+00 0.000000e+00 2.220446e-16 0.000000e+00
>>
>>     and the error gets bigger when you continue the sequence; e.g., this
>>     is for c(1, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7):
>>
>>     [1] 0.000000e+00 0.000000e+00 2.220446e-16 2.220446e-16 4.440892e-16
>>     [6] 4.440892e-16 4.440892e-16 0.000000e+00
>>
>>     So, independently of this is considered a bug or not, instead of
>>
>>     length(n) == length(test) && all(n == test)
>>
>>     I would use the following condition:
>>
>>     isTRUE(all.equal(n, test))
>>
>>     I?aki
>>
>>     >
>>     > Best regards,
>>     > Felix
>>     >
>>     >
>>     >         [[alternative HTML version deleted]]
>>     >
>>     > ______________________________________________
>>     > R-devel at r-project.org mailing list
>>     > https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>>
>>
>>     --
>>     I?aki Ucar
>>
>>     ______________________________________________
>>     R-devel at r-project.org mailing list
>>     https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>

	[[alternative HTML version deleted]]


From m@echler @ending from @t@t@m@th@ethz@ch  Fri Aug 31 16:04:37 2018
From: m@echler @ending from @t@t@m@th@ethz@ch (Martin Maechler)
Date: Fri, 31 Aug 2018 16:04:37 +0200
Subject: [Rd] Segfault when performing match on POSIXlt object
In-Reply-To: <23433.18919.859126.521094@stat.math.ethz.ch>
References: <CAA9Rc3PFDr_Rn9toAcwpp5MOvcdEbGMgfDnc7n6PFK+cHxZYAw@mail.gmail.com>
 <23433.18919.859126.521094@stat.math.ethz.ch>
Message-ID: <23433.19189.841070.766209@stat.math.ethz.ch>

>>>>> Martin Maechler  on Fri, 31 Aug 2018 16:00:07 +0200 writes:

>>>>> Marco Giuliano on Fri, 31 Aug 2018 08:53:02 +0200 writes:

    >> Hi All, I found a possible unexpected behavior when
    >> performing match/%in% on POSIXlt objects, e.g. :

    >> d <- as.POSIXlt('2018-01-01')

    >> # match(<anything>,<POSIXlt>) --> segfault match(0,d)

    >> # consequently also this fails :

    > 0 %in% d

    >> REPORTED ERROR ON LINUX: *** caught segfault *** address
    >> 0x16dc2, cause 'memory not mapped'

    >> Verified on 3.5.0 on linux, 3.5.1 on Windows.

    > Confirmed (Linux, I think all version >= 3.4.0, but not in
    > R 3.3.3 (or earlier, presumably).

ooops that was an offset error:

      Bug in all versions >= 3.3.3,
      but not in 3.2.5 (or earlier, presumably)


From luke-tier@ey m@ili@g off uiow@@edu  Fri Aug 31 16:05:06 2018
From: luke-tier@ey m@ili@g off uiow@@edu (luke-tier@ey m@ili@g off uiow@@edu)
Date: Fri, 31 Aug 2018 09:05:06 -0500 (CDT)
Subject: [Rd] Detecting whether a process exists or not by its PID?
In-Reply-To: <CABtg=Kk6iXKtzuc9F558L8_PmP6r9iPDkynHEmERpRKHA+f8ig@mail.gmail.com>
References: <CAFDcVCS85ZWsfOwEn4-3Sev+eykSjkeNwxWq_h1ie+_dkt0B5g@mail.gmail.com>
 <1ef168ae-9889-de7e-15f2-631f755fa4aa@gmail.com>
 <CABtg=KkpUWebM8o9uVeMuYAfjcMZ17ipQ-dneOfYk4enGgnFTQ@mail.gmail.com>
 <cf3fb3b9-8523-c8c7-700d-bd2a75e5cbd8@gmail.com>
 <CABtg=Kk6iXKtzuc9F558L8_PmP6r9iPDkynHEmERpRKHA+f8ig@mail.gmail.com>
Message-ID: <alpine.DEB.2.21.1808310904090.2971@luke-Latitude-7480>

On Fri, 31 Aug 2018, G?bor Cs?rdi wrote:

> On Fri, Aug 31, 2018 at 3:35 PM Tomas Kalibera <tomas.kalibera at gmail.com> wrote:
>>
>> On 08/31/2018 03:13 PM, G?bor Cs?rdi wrote:
>>> On Fri, Aug 31, 2018 at 2:51 PM Tomas Kalibera <tomas.kalibera at gmail.com> wrote:
>>> [...]
>>>> kill(sig=0) is specified by POSIX but indeed as you say there is a race
>>>> condition due to PID-reuse.  In principle, detecting that a worker
>>>> process is still alive cannot be done correctly outside base R.
>>> I am not sure why you think so.
>> To avoid the race with PID re-use one needs access to signal handling,
>> to blocking signals, to handling sigchld. system/system2 and
>> mcparallel/mccollect in base R use these features and the interaction is
>> still safe given the specific use in system/system2 and
>> mcparallel/mccollect, yet would have to be re-visited if either of the
>> two uses change. These features cannot be safely used outside of base R
>> in contributed packages.
>
> Yes, _in theory_ this is right, and of course this only works for
> child processes.
>
> _In practice_, you do not need signal handling. The startup time stamp
> method is
> completely fine, because it is practically impossible to have two
> processes with the
> same pid and the same (high precision) startup time. This method also
> works for any
> process (not just child processes), so for PSOCK clusters as well.

PSOCK workers may not be running on the same host as the master process.

Best,

luke

>
> Gabor
>
> [...]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

-- 
Luke Tierney
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu

From iuc@r @ending from fedor@project@org  Fri Aug 31 16:13:43 2018
From: iuc@r @ending from fedor@project@org (=?UTF-8?Q?I=C3=B1aki_Ucar?=)
Date: Fri, 31 Aug 2018 16:13:43 +0200
Subject: [Rd] compairing doubles
In-Reply-To: <CAOKDuOgykFYMA-ZT4kUxr_NKXBEBsxZV8s-3sAuQ3Motc5M_Fw@mail.gmail.com>
References: <AM0PR0502MB387404BC690F9AF421B6F5EAD6080@AM0PR0502MB3874.eurprd05.prod.outlook.com>
 <CALEXWq3wSS4uaYu33oA-KUu7gSBuqKYF0ESemwvsbmp6KktOJg@mail.gmail.com>
 <95F39593-22F4-46B6-A766-8FF60F608C0C@dans.knaw.nl>
 <CAOKDuOgykFYMA-ZT4kUxr_NKXBEBsxZV8s-3sAuQ3Motc5M_Fw@mail.gmail.com>
Message-ID: <CALEXWq2ke081QLqbv89z=yfTuoocLw9dzQWx6-05gzSFqAD3vQ@mail.gmail.com>

El vie., 31 ago. 2018 a las 16:00, Mark van der Loo
(<mark.vanderloo at gmail.com>) escribi?:
>
> how about
>
> is_evenly_spaced <- function(x,...) all.equal(diff(sort(x)),...)

This doesn't work, because

1. all.equal does *not* return FALSE. Use of isTRUE or identical(.,
TRUE) is required if you want a boolean.
2. all.equal compares two objects, not elements in a vector.

I?aki

>
> (use ellipsis to set tolerance if necessary)
>
>
> Op vr 31 aug. 2018 om 15:46 schreef Emil Bode <emil.bode at dans.knaw.nl>:
>>
>> Agreed that's it's rounding error, and all.equal would be the way to go.
>> I wouldn't call it a bug, it's simply part of working with floating point numbers, any language has the same issue.
>>
>> And while we're at it, I think the function can be a lot shorter:
>> .is_continous_evenly_spaced <- function(n){
>>   length(n)>1 && isTRUE(all.equal(n[order(n)], seq(from=min(n), to=max(n), length.out = length(n))))
>> }
>>
>> Cheers, Emil
>>
>>     El vie., 31 ago. 2018 a las 15:10, Felix Ernst
>>     (<felix.gm.ernst at outlook.com>) escribi?:
>>     >
>>     > Dear all,
>>     >
>>     > I a bit unsure, whether this qualifies as a bug, but it is definitly a strange behaviour. That why I wanted to discuss it.
>>     >
>>     > With the following function, I want to test for evenly space numbers, starting from anywhere.
>>     >
>>     > .is_continous_evenly_spaced <- function(n){
>>     >   if(length(n) < 2) return(FALSE)
>>     >   n <- n[order(n)]
>>     >   n <- n - min(n)
>>     >   step <- n[2] - n[1]
>>     >   test <- seq(from = min(n), to = max(n), by = step)
>>     >   if(length(n) == length(test) &&
>>     >      all(n == test)){
>>     >     return(TRUE)
>>     >   }
>>     >   return(FALSE)
>>     > }
>>     >
>>     > > .is_continous_evenly_spaced(c(1,2,3,4))
>>     > [1] TRUE
>>     > > .is_continous_evenly_spaced(c(1,3,4,5))
>>     > [1] FALSE
>>     > > .is_continous_evenly_spaced(c(1,1.1,1.2,1.3))
>>     > [1] FALSE
>>     >
>>     > I expect the result for 1 and 2, but not for 3. Upon Investigation it turns out, that n == test is TRUE for every pair, but not for the pair of 0.2.
>>     >
>>     > The types reported are always double, however n[2] == 0.1 reports FALSE as well.
>>     >
>>     > The whole problem is solved by switching from all(n == test) to all(as.character(n) == as.character(test)). However that is weird, isn?t it?
>>     >
>>     > Does this work as intended? Thanks for any help, advise and suggestions in advance.
>>
>>     I guess this has something to do with how the sequence is built and
>>     the inherent error of floating point arithmetic. In fact, if you
>>     return test minus n, you'll get:
>>
>>     [1] 0.000000e+00 0.000000e+00 2.220446e-16 0.000000e+00
>>
>>     and the error gets bigger when you continue the sequence; e.g., this
>>     is for c(1, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7):
>>
>>     [1] 0.000000e+00 0.000000e+00 2.220446e-16 2.220446e-16 4.440892e-16
>>     [6] 4.440892e-16 4.440892e-16 0.000000e+00
>>
>>     So, independently of this is considered a bug or not, instead of
>>
>>     length(n) == length(test) && all(n == test)
>>
>>     I would use the following condition:
>>
>>     isTRUE(all.equal(n, test))
>>
>>     I?aki
>>
>>     >
>>     > Best regards,
>>     > Felix
>>     >
>>     >
>>     >         [[alternative HTML version deleted]]
>>     >
>>     > ______________________________________________
>>     > R-devel at r-project.org mailing list
>>     > https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>>
>>
>>     --
>>     I?aki Ucar
>>
>>     ______________________________________________
>>     R-devel at r-project.org mailing list
>>     https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel



-- 
I?aki Ucar


From m@rk@v@nderloo @ending from gm@il@com  Fri Aug 31 16:25:44 2018
From: m@rk@v@nderloo @ending from gm@il@com (Mark van der Loo)
Date: Fri, 31 Aug 2018 16:25:44 +0200
Subject: [Rd] compairing doubles
In-Reply-To: <CALEXWq2ke081QLqbv89z=yfTuoocLw9dzQWx6-05gzSFqAD3vQ@mail.gmail.com>
References: <AM0PR0502MB387404BC690F9AF421B6F5EAD6080@AM0PR0502MB3874.eurprd05.prod.outlook.com>
 <CALEXWq3wSS4uaYu33oA-KUu7gSBuqKYF0ESemwvsbmp6KktOJg@mail.gmail.com>
 <95F39593-22F4-46B6-A766-8FF60F608C0C@dans.knaw.nl>
 <CAOKDuOgykFYMA-ZT4kUxr_NKXBEBsxZV8s-3sAuQ3Motc5M_Fw@mail.gmail.com>
 <CALEXWq2ke081QLqbv89z=yfTuoocLw9dzQWx6-05gzSFqAD3vQ@mail.gmail.com>
Message-ID: <CAOKDuOg5bA0Uf5bmQX_9ZP6qbnO=hewb7cVbjEEZEvu5KOGC5w@mail.gmail.com>

Ah, my bad, you're right of course.

sum(abs(diff(diff( sort(x))))) < eps

for some reasonable eps then, would do as a oneliner, or

all(abs(diff(diff(sort(x)))) < eps)

or

max(abs(diff(diff(sort(x))))) < eps


-Mark

Op vr 31 aug. 2018 om 16:14 schreef I?aki Ucar <iucar at fedoraproject.org>:

> El vie., 31 ago. 2018 a las 16:00, Mark van der Loo
> (<mark.vanderloo at gmail.com>) escribi?:
> >
> > how about
> >
> > is_evenly_spaced <- function(x,...) all.equal(diff(sort(x)),...)
>
> This doesn't work, because
>
> 1. all.equal does *not* return FALSE. Use of isTRUE or identical(.,
> TRUE) is required if you want a boolean.
> 2. all.equal compares two objects, not elements in a vector.
>
> I?aki
>
> >
> > (use ellipsis to set tolerance if necessary)
> >
> >
> > Op vr 31 aug. 2018 om 15:46 schreef Emil Bode <emil.bode at dans.knaw.nl>:
> >>
> >> Agreed that's it's rounding error, and all.equal would be the way to go.
> >> I wouldn't call it a bug, it's simply part of working with floating
> point numbers, any language has the same issue.
> >>
> >> And while we're at it, I think the function can be a lot shorter:
> >> .is_continous_evenly_spaced <- function(n){
> >>   length(n)>1 && isTRUE(all.equal(n[order(n)], seq(from=min(n),
> to=max(n), length.out = length(n))))
> >> }
> >>
> >> Cheers, Emil
> >>
> >>     El vie., 31 ago. 2018 a las 15:10, Felix Ernst
> >>     (<felix.gm.ernst at outlook.com>) escribi?:
> >>     >
> >>     > Dear all,
> >>     >
> >>     > I a bit unsure, whether this qualifies as a bug, but it is
> definitly a strange behaviour. That why I wanted to discuss it.
> >>     >
> >>     > With the following function, I want to test for evenly space
> numbers, starting from anywhere.
> >>     >
> >>     > .is_continous_evenly_spaced <- function(n){
> >>     >   if(length(n) < 2) return(FALSE)
> >>     >   n <- n[order(n)]
> >>     >   n <- n - min(n)
> >>     >   step <- n[2] - n[1]
> >>     >   test <- seq(from = min(n), to = max(n), by = step)
> >>     >   if(length(n) == length(test) &&
> >>     >      all(n == test)){
> >>     >     return(TRUE)
> >>     >   }
> >>     >   return(FALSE)
> >>     > }
> >>     >
> >>     > > .is_continous_evenly_spaced(c(1,2,3,4))
> >>     > [1] TRUE
> >>     > > .is_continous_evenly_spaced(c(1,3,4,5))
> >>     > [1] FALSE
> >>     > > .is_continous_evenly_spaced(c(1,1.1,1.2,1.3))
> >>     > [1] FALSE
> >>     >
> >>     > I expect the result for 1 and 2, but not for 3. Upon
> Investigation it turns out, that n == test is TRUE for every pair, but not
> for the pair of 0.2.
> >>     >
> >>     > The types reported are always double, however n[2] == 0.1 reports
> FALSE as well.
> >>     >
> >>     > The whole problem is solved by switching from all(n == test) to
> all(as.character(n) == as.character(test)). However that is weird, isn?t it?
> >>     >
> >>     > Does this work as intended? Thanks for any help, advise and
> suggestions in advance.
> >>
> >>     I guess this has something to do with how the sequence is built and
> >>     the inherent error of floating point arithmetic. In fact, if you
> >>     return test minus n, you'll get:
> >>
> >>     [1] 0.000000e+00 0.000000e+00 2.220446e-16 0.000000e+00
> >>
> >>     and the error gets bigger when you continue the sequence; e.g., this
> >>     is for c(1, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7):
> >>
> >>     [1] 0.000000e+00 0.000000e+00 2.220446e-16 2.220446e-16 4.440892e-16
> >>     [6] 4.440892e-16 4.440892e-16 0.000000e+00
> >>
> >>     So, independently of this is considered a bug or not, instead of
> >>
> >>     length(n) == length(test) && all(n == test)
> >>
> >>     I would use the following condition:
> >>
> >>     isTRUE(all.equal(n, test))
> >>
> >>     I?aki
> >>
> >>     >
> >>     > Best regards,
> >>     > Felix
> >>     >
> >>     >
> >>     >         [[alternative HTML version deleted]]
> >>     >
> >>     > ______________________________________________
> >>     > R-devel at r-project.org mailing list
> >>     > https://stat.ethz.ch/mailman/listinfo/r-devel
> >>
> >>
> >>
> >>     --
> >>     I?aki Ucar
> >>
> >>     ______________________________________________
> >>     R-devel at r-project.org mailing list
> >>     https://stat.ethz.ch/mailman/listinfo/r-devel
> >>
> >>
> >> ______________________________________________
> >> R-devel at r-project.org mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-devel
>
>
>
> --
> I?aki Ucar
>

	[[alternative HTML version deleted]]


From @okol @ending from in@@-toulou@e@fr  Fri Aug 31 16:50:04 2018
From: @okol @ending from in@@-toulou@e@fr (Serguei Sokol)
Date: Fri, 31 Aug 2018 16:50:04 +0200
Subject: [Rd] compairing doubles
In-Reply-To: <CAOKDuOg5bA0Uf5bmQX_9ZP6qbnO=hewb7cVbjEEZEvu5KOGC5w@mail.gmail.com>
References: <AM0PR0502MB387404BC690F9AF421B6F5EAD6080@AM0PR0502MB3874.eurprd05.prod.outlook.com>
 <CALEXWq3wSS4uaYu33oA-KUu7gSBuqKYF0ESemwvsbmp6KktOJg@mail.gmail.com>
 <95F39593-22F4-46B6-A766-8FF60F608C0C@dans.knaw.nl>
 <CAOKDuOgykFYMA-ZT4kUxr_NKXBEBsxZV8s-3sAuQ3Motc5M_Fw@mail.gmail.com>
 <CALEXWq2ke081QLqbv89z=yfTuoocLw9dzQWx6-05gzSFqAD3vQ@mail.gmail.com>
 <CAOKDuOg5bA0Uf5bmQX_9ZP6qbnO=hewb7cVbjEEZEvu5KOGC5w@mail.gmail.com>
Message-ID: <e275b09d-39c8-a7e2-fc11-30c00034d1e4@insa-toulouse.fr>

Le 31/08/2018 ? 16:25, Mark van der Loo a ?crit?:
> Ah, my bad, you're right of course.
>
> sum(abs(diff(diff( sort(x))))) < eps
>
> for some reasonable eps then, would do as a oneliner, or
>
> all(abs(diff(diff(sort(x)))) < eps)
>
> or
>
> max(abs(diff(diff(sort(x))))) < eps
Or with only four function calls:
diff(range(diff(sort(x)))) < eps

Serguei.
>
>
> -Mark
>
> Op vr 31 aug. 2018 om 16:14 schreef I?aki Ucar <iucar at fedoraproject.org>:
>
>> El vie., 31 ago. 2018 a las 16:00, Mark van der Loo
>> (<mark.vanderloo at gmail.com>) escribi?:
>>> how about
>>>
>>> is_evenly_spaced <- function(x,...) all.equal(diff(sort(x)),...)
>> This doesn't work, because
>>
>> 1. all.equal does *not* return FALSE. Use of isTRUE or identical(.,
>> TRUE) is required if you want a boolean.
>> 2. all.equal compares two objects, not elements in a vector.
>>
>> I?aki
>>
>>> (use ellipsis to set tolerance if necessary)
>>>
>>>
>>> Op vr 31 aug. 2018 om 15:46 schreef Emil Bode <emil.bode at dans.knaw.nl>:
>>>> Agreed that's it's rounding error, and all.equal would be the way to go.
>>>> I wouldn't call it a bug, it's simply part of working with floating
>> point numbers, any language has the same issue.
>>>> And while we're at it, I think the function can be a lot shorter:
>>>> .is_continous_evenly_spaced <- function(n){
>>>>    length(n)>1 && isTRUE(all.equal(n[order(n)], seq(from=min(n),
>> to=max(n), length.out = length(n))))
>>>> }
>>>>
>>>> Cheers, Emil
>>>>
>>>>      El vie., 31 ago. 2018 a las 15:10, Felix Ernst
>>>>      (<felix.gm.ernst at outlook.com>) escribi?:
>>>>      >
>>>>      > Dear all,
>>>>      >
>>>>      > I a bit unsure, whether this qualifies as a bug, but it is
>> definitly a strange behaviour. That why I wanted to discuss it.
>>>>      >
>>>>      > With the following function, I want to test for evenly space
>> numbers, starting from anywhere.
>>>>      >
>>>>      > .is_continous_evenly_spaced <- function(n){
>>>>      >   if(length(n) < 2) return(FALSE)
>>>>      >   n <- n[order(n)]
>>>>      >   n <- n - min(n)
>>>>      >   step <- n[2] - n[1]
>>>>      >   test <- seq(from = min(n), to = max(n), by = step)
>>>>      >   if(length(n) == length(test) &&
>>>>      >      all(n == test)){
>>>>      >     return(TRUE)
>>>>      >   }
>>>>      >   return(FALSE)
>>>>      > }
>>>>      >
>>>>      > > .is_continous_evenly_spaced(c(1,2,3,4))
>>>>      > [1] TRUE
>>>>      > > .is_continous_evenly_spaced(c(1,3,4,5))
>>>>      > [1] FALSE
>>>>      > > .is_continous_evenly_spaced(c(1,1.1,1.2,1.3))
>>>>      > [1] FALSE
>>>>      >
>>>>      > I expect the result for 1 and 2, but not for 3. Upon
>> Investigation it turns out, that n == test is TRUE for every pair, but not
>> for the pair of 0.2.
>>>>      >
>>>>      > The types reported are always double, however n[2] == 0.1 reports
>> FALSE as well.
>>>>      >
>>>>      > The whole problem is solved by switching from all(n == test) to
>> all(as.character(n) == as.character(test)). However that is weird, isn?t it?
>>>>      >
>>>>      > Does this work as intended? Thanks for any help, advise and
>> suggestions in advance.
>>>>      I guess this has something to do with how the sequence is built and
>>>>      the inherent error of floating point arithmetic. In fact, if you
>>>>      return test minus n, you'll get:
>>>>
>>>>      [1] 0.000000e+00 0.000000e+00 2.220446e-16 0.000000e+00
>>>>
>>>>      and the error gets bigger when you continue the sequence; e.g., this
>>>>      is for c(1, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7):
>>>>
>>>>      [1] 0.000000e+00 0.000000e+00 2.220446e-16 2.220446e-16 4.440892e-16
>>>>      [6] 4.440892e-16 4.440892e-16 0.000000e+00
>>>>
>>>>      So, independently of this is considered a bug or not, instead of
>>>>
>>>>      length(n) == length(test) && all(n == test)
>>>>
>>>>      I would use the following condition:
>>>>
>>>>      isTRUE(all.equal(n, test))
>>>>
>>>>      I?aki
>>>>
>>>>      >
>>>>      > Best regards,
>>>>      > Felix
>>>>      >
>>>>      >
>>>>      >         [[alternative HTML version deleted]]
>>>>      >
>>>>      > ______________________________________________
>>>>      > R-devel at r-project.org mailing list
>>>>      > https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>
>>>>
>>>>
>>>>      --
>>>>      I?aki Ucar
>>>>
>>>>      ______________________________________________
>>>>      R-devel at r-project.org mailing list
>>>>      https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>
>>>>
>>>> ______________________________________________
>>>> R-devel at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>>
>> --
>> I?aki Ucar
>>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


-- 
Serguei Sokol
Ingenieur de recherche INRA

Cellule math?matiques
LISBP, INSA/INRA UMR 792, INSA/CNRS UMR 5504
135 Avenue de Rangueil
31077 Toulouse Cedex 04

tel: +33 5 62 25 01 27
email: sokol at insa-toulouse.fr
http://www.lisbp.fr


From mgiuli@no@m@il @ending from gm@il@com  Fri Aug 31 16:50:56 2018
From: mgiuli@no@m@il @ending from gm@il@com (Marco Giuliano)
Date: Fri, 31 Aug 2018 16:50:56 +0200
Subject: [Rd] Segfault when performing match on POSIXlt object
In-Reply-To: <23433.19189.841070.766209@stat.math.ethz.ch>
References: <CAA9Rc3PFDr_Rn9toAcwpp5MOvcdEbGMgfDnc7n6PFK+cHxZYAw@mail.gmail.com>
 <23433.18919.859126.521094@stat.math.ethz.ch>
 <23433.19189.841070.766209@stat.math.ethz.ch>
Message-ID: <CAA9Rc3N=Lji03ozt0qC=CeOa589ckQc9=iuq3hAmJfDLhmnkWw@mail.gmail.com>

Hi Martin,
should I file a formal bug report somewhere or you've already done it ?

On Fri, Aug 31, 2018 at 4:04 PM Martin Maechler <maechler at stat.math.ethz.ch>
wrote:

> >>>>> Martin Maechler  on Fri, 31 Aug 2018 16:00:07 +0200 writes:
>
> >>>>> Marco Giuliano on Fri, 31 Aug 2018 08:53:02 +0200 writes:
>
>     >> Hi All, I found a possible unexpected behavior when
>     >> performing match/%in% on POSIXlt objects, e.g. :
>
>     >> d <- as.POSIXlt('2018-01-01')
>
>     >> # match(<anything>,<POSIXlt>) --> segfault match(0,d)
>
>     >> # consequently also this fails :
>
>     > 0 %in% d
>
>     >> REPORTED ERROR ON LINUX: *** caught segfault *** address
>     >> 0x16dc2, cause 'memory not mapped'
>
>     >> Verified on 3.5.0 on linux, 3.5.1 on Windows.
>
>     > Confirmed (Linux, I think all version >= 3.4.0, but not in
>     > R 3.3.3 (or earlier, presumably).
>
> ooops that was an offset error:
>
>       Bug in all versions >= 3.3.3,
>       but not in 3.2.5 (or earlier, presumably)
>
>

	[[alternative HTML version deleted]]


From m@rc_@chw@rtz @ending from me@com  Fri Aug 31 16:00:38 2018
From: m@rc_@chw@rtz @ending from me@com (Marc Schwartz)
Date: Fri, 31 Aug 2018 10:00:38 -0400
Subject: [Rd] compairing doubles
In-Reply-To: <CALEXWq3wSS4uaYu33oA-KUu7gSBuqKYF0ESemwvsbmp6KktOJg@mail.gmail.com>
References: <AM0PR0502MB387404BC690F9AF421B6F5EAD6080@AM0PR0502MB3874.eurprd05.prod.outlook.com>
 <CALEXWq3wSS4uaYu33oA-KUu7gSBuqKYF0ESemwvsbmp6KktOJg@mail.gmail.com>
Message-ID: <E76D8E57-85D5-4BC5-A212-6FE54FAB7537@me.com>



> On Aug 31, 2018, at 9:36 AM, I?aki Ucar <iucar at fedoraproject.org> wrote:
> 
> El vie., 31 ago. 2018 a las 15:10, Felix Ernst
> (<felix.gm.ernst at outlook.com>) escribi?:
>> 
>> Dear all,
>> 
>> I a bit unsure, whether this qualifies as a bug, but it is definitly a strange behaviour. That why I wanted to discuss it.
>> 
>> With the following function, I want to test for evenly space numbers, starting from anywhere.
>> 
>> .is_continous_evenly_spaced <- function(n){
>>  if(length(n) < 2) return(FALSE)
>>  n <- n[order(n)]
>>  n <- n - min(n)
>>  step <- n[2] - n[1]
>>  test <- seq(from = min(n), to = max(n), by = step)
>>  if(length(n) == length(test) &&
>>     all(n == test)){
>>    return(TRUE)
>>  }
>>  return(FALSE)
>> }
>> 
>>> .is_continous_evenly_spaced(c(1,2,3,4))
>> [1] TRUE
>>> .is_continous_evenly_spaced(c(1,3,4,5))
>> [1] FALSE
>>> .is_continous_evenly_spaced(c(1,1.1,1.2,1.3))
>> [1] FALSE
>> 
>> I expect the result for 1 and 2, but not for 3. Upon Investigation it turns out, that n == test is TRUE for every pair, but not for the pair of 0.2.
>> 
>> The types reported are always double, however n[2] == 0.1 reports FALSE as well.
>> 
>> The whole problem is solved by switching from all(n == test) to all(as.character(n) == as.character(test)). However that is weird, isn?t it?
>> 
>> Does this work as intended? Thanks for any help, advise and suggestions in advance.
> 
> I guess this has something to do with how the sequence is built and
> the inherent error of floating point arithmetic. In fact, if you
> return test minus n, you'll get:
> 
> [1] 0.000000e+00 0.000000e+00 2.220446e-16 0.000000e+00
> 
> and the error gets bigger when you continue the sequence; e.g., this
> is for c(1, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7):
> 
> [1] 0.000000e+00 0.000000e+00 2.220446e-16 2.220446e-16 4.440892e-16
> [6] 4.440892e-16 4.440892e-16 0.000000e+00
> 
> So, independently of this is considered a bug or not, instead of
> 
> length(n) == length(test) && all(n == test)
> 
> I would use the following condition:
> 
> isTRUE(all.equal(n, test))
> 
> I?aki
> 
>> 
>> Best regards,
>> Felix


Hi,

This is essentially FAQ 7.31:

  https://cran.r-project.org/doc/FAQ/R-FAQ.html#Why-doesn_0027t-R-think-these-numbers-are-equal_003f <https://cran.r-project.org/doc/FAQ/R-FAQ.html#Why-doesn_0027t-R-think-these-numbers-are-equal_003f>

Review that and the references therein to gain some insights into binary representations of floating point numbers.

Rather than the more complicated code you have above, try the following:

evenlyspaced <- function(x) {
  gaps <- diff(sort(x))
  all(gaps[-1] == gaps[1])
}

Note the use of ?diff:

> diff(c(1, 2, 3, 4))
[1] 1 1 1

> diff(c(1, 3, 4, 5))
[1] 2 1 1

> diff(c(1, 1.1, 1.2, 1.3))
[1] 0.1 0.1 0.1

However, in reality, due to the floating point representation issues noted above:

> print(diff(c(1, 1.1, 1.2, 1.3)), 20)
[1] 0.100000000000000088818 0.099999999999999866773
[3] 0.100000000000000088818

So the differences between the numbers are not exactly 0.1.

Using the function above, you get:

> evenlyspaced(c(1, 2, 3, 4))
[1] TRUE

> evenlyspaced(c(1, 3, 4, 5))
[1] FALSE

> evenlyspaced(c(1, 1.1, 1.2, 1.3))
[1] FALSE

As has been noted, if you want the gap comparison to be based upon some margin of error, use ?all.equal rather than the explicit equals comparison that I have in the function above. Something along the lines of:

evenlyspaced <- function(x) {
  gaps <- diff(sort(x))
  all(sapply(gaps[-1], function(x) all.equal(x, gaps[1])))
}

On which case, you now get:

evenlyspaced(c(1, 1.1, 1.2, 1.3))
[1] TRUE


Regards,

Marc Schwartz


	[[alternative HTML version deleted]]


From iuc@r @ending from fedor@project@org  Fri Aug 31 17:31:19 2018
From: iuc@r @ending from fedor@project@org (=?UTF-8?Q?I=C3=B1aki_Ucar?=)
Date: Fri, 31 Aug 2018 17:31:19 +0200
Subject: [Rd] compairing doubles
In-Reply-To: <e275b09d-39c8-a7e2-fc11-30c00034d1e4@insa-toulouse.fr>
References: <AM0PR0502MB387404BC690F9AF421B6F5EAD6080@AM0PR0502MB3874.eurprd05.prod.outlook.com>
 <CALEXWq3wSS4uaYu33oA-KUu7gSBuqKYF0ESemwvsbmp6KktOJg@mail.gmail.com>
 <95F39593-22F4-46B6-A766-8FF60F608C0C@dans.knaw.nl>
 <CAOKDuOgykFYMA-ZT4kUxr_NKXBEBsxZV8s-3sAuQ3Motc5M_Fw@mail.gmail.com>
 <CALEXWq2ke081QLqbv89z=yfTuoocLw9dzQWx6-05gzSFqAD3vQ@mail.gmail.com>
 <CAOKDuOg5bA0Uf5bmQX_9ZP6qbnO=hewb7cVbjEEZEvu5KOGC5w@mail.gmail.com>
 <e275b09d-39c8-a7e2-fc11-30c00034d1e4@insa-toulouse.fr>
Message-ID: <CALEXWq3hNdLZwf9v=YZNTJ1FjTJfN6xuuUeSZ-zyZq1Sp+8yKA@mail.gmail.com>

El vie., 31 ago. 2018 a las 17:08, Serguei Sokol
(<sokol at insa-toulouse.fr>) escribi?:
>
> Le 31/08/2018 ? 16:25, Mark van der Loo a ?crit :
> > Ah, my bad, you're right of course.
> >
> > sum(abs(diff(diff( sort(x))))) < eps
> >
> > for some reasonable eps then, would do as a oneliner, or
> >
> > all(abs(diff(diff(sort(x)))) < eps)
> >
> > or
> >
> > max(abs(diff(diff(sort(x))))) < eps
> Or with only four function calls:
> diff(range(diff(sort(x)))) < eps

We may have a winner... :)

I?aki


From m@echler @ending from @t@t@m@th@ethz@ch  Fri Aug 31 18:48:10 2018
From: m@echler @ending from @t@t@m@th@ethz@ch (Martin Maechler)
Date: Fri, 31 Aug 2018 18:48:10 +0200
Subject: [Rd] Segfault when performing match on POSIXlt object
In-Reply-To: <CAA9Rc3N=Lji03ozt0qC=CeOa589ckQc9=iuq3hAmJfDLhmnkWw@mail.gmail.com>
References: <CAA9Rc3PFDr_Rn9toAcwpp5MOvcdEbGMgfDnc7n6PFK+cHxZYAw@mail.gmail.com>
 <23433.18919.859126.521094@stat.math.ethz.ch>
 <23433.19189.841070.766209@stat.math.ethz.ch>
 <CAA9Rc3N=Lji03ozt0qC=CeOa589ckQc9=iuq3hAmJfDLhmnkWw@mail.gmail.com>
Message-ID: <23433.29002.152055.941837@stat.math.ethz.ch>

>>>>> Marco Giuliano 
>>>>>     on Fri, 31 Aug 2018 16:50:56 +0200 writes:

    > Hi Martin, should I file a formal bug report somewhere or
    > you've already done it ?

No, I haven't, 
and as I may not address this bug further myself (in the near
future), it may be best if you file a formal report.

I will create an account for you on R's bugzilla - you will be
notified and can update your initial pseudo-random password.

Best,
Martin


From @pencer@gr@ve@ @ending from prod@y@e@com  Fri Aug 31 21:21:40 2018
From: @pencer@gr@ve@ @ending from prod@y@e@com (Spencer Graves)
Date: Fri, 31 Aug 2018 14:21:40 -0500
Subject: [Rd] svg ignores cex.axis in R3.5.1 on macOS
Message-ID: <ac8f9908-f064-d723-61a8-7e8ca085dd43@prodsyse.com>

 ????? Plots produced using svg in R 3.5.1 under macOS 10.13.6 ignores 
cex.axis=2.? Consider the following:


 > plot(1:2, cex.axis=2)
 > svg('svg_ignores_cex.axis.svg')
 > plot(1:2, cex.axis=2)
 > dev.off()
 > sessionInfo()
R version 3.5.1 (2018-07-02)
Platform: x86_64-apple-darwin15.6.0 (64-bit)
Running under: macOS High Sierra 10.13.6

Matrix products: default
BLAS: 
/Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
LAPACK: 
/Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] stats???? graphics? grDevices utils???? datasets? methods base

loaded via a namespace (and not attached):
[1] compiler_3.5.1


 ????? ** The axis labels are appropriately expanded with the first 
"plot(1:2, cex.axis=2)".? However, when I wrote that to an svg file and 
opened it in other applications (GIMP and Safari), the cex.axis request 
was ignored.? This also occurred inside RStudio on my Mac. It worked 
properly using R 3.2.1 under Windows 7.


 ????? Thanks,
 ????? Spencer Graves


From hp@ge@ @ending from fredhutch@org  Fri Aug 31 21:40:48 2018
From: hp@ge@ @ending from fredhutch@org (=?UTF-8?B?SGVydsOpIFBhZ8Oocw==?=)
Date: Fri, 31 Aug 2018 12:40:48 -0700
Subject: [Rd] Argument 'dim' misspelled in error message
Message-ID: <777dc8b8-368a-dcdb-0aaf-e484475ed90f@fredhutch.org>

Hi,

The following error message misspells the name of
the 'dim' argument:

   > array(integer(0), dim=integer(0))
   Error in array(integer(0), dim = integer(0)) :
     'dims' cannot be of length 0

The name of the argument is 'dim' not 'dims':

   > args(array)
   function (data = NA, dim = length(data), dimnames = NULL)
   NULL

Cheers,
H.

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fredhutch.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From @pencer@gr@ve@ @ending from prod@y@e@com  Fri Aug 31 22:02:14 2018
From: @pencer@gr@ve@ @ending from prod@y@e@com (Spencer Graves)
Date: Fri, 31 Aug 2018 15:02:14 -0500
Subject: [Rd] svg ignores cex.axis in R3.5.1 on macOS
In-Reply-To: <ac8f9908-f064-d723-61a8-7e8ca085dd43@prodsyse.com>
References: <ac8f9908-f064-d723-61a8-7e8ca085dd43@prodsyse.com>
Message-ID: <c506e0f9-bf78-cab1-3d38-ef8b0b1dd850@prodsyse.com>



On 2018-08-31 14:21, Spencer Graves wrote:
> Plots produced using svg in R 3.5.1 under macOS 10.13.6 ignores 
> cex.axis=2.? Consider the following:
>
>
> > plot(1:2, cex.axis=2)
> > svg('svg_ignores_cex.axis.svg')
> > plot(1:2, cex.axis=2)
> > dev.off()
> > sessionInfo()
> R version 3.5.1 (2018-07-02)
> Platform: x86_64-apple-darwin15.6.0 (64-bit)
> Running under: macOS High Sierra 10.13.6
>
> Matrix products: default
> BLAS: 
> /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
> LAPACK: 
> /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
>
> locale:
> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
>
> attached base packages:
> [1] stats???? graphics? grDevices utils???? datasets? methods base
>
> loaded via a namespace (and not attached):
> [1] compiler_3.5.1
>
>
> ????? ** The axis labels are appropriately expanded with the first 
> "plot(1:2, cex.axis=2)".? However, when I wrote that to an svg file 
> and opened it in other applications (GIMP and Safari), the cex.axis 
> request was ignored.? This also occurred inside RStudio on my Mac. It 
> worked properly using R 3.2.1 under Windows 7.


I just confirmed that when I created a file like this under Windows 7 
and brought it back to my Mac, it displayed fine.? I have not tried this 
with the current version of R under Windows 7 nor an old version of R on 
my Mac.? Thanks.? Spencer
>
>
> ????? Thanks,
> ????? Spencer Graves
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From mgiuli@no@m@il @ending from gm@il@com  Fri Aug 31 22:15:18 2018
From: mgiuli@no@m@il @ending from gm@il@com (Marco Giuliano)
Date: Fri, 31 Aug 2018 22:15:18 +0200
Subject: [Rd] Segfault when performing match on POSIXlt object
In-Reply-To: <23433.29002.152055.941837@stat.math.ethz.ch>
References: <CAA9Rc3PFDr_Rn9toAcwpp5MOvcdEbGMgfDnc7n6PFK+cHxZYAw@mail.gmail.com>
 <23433.18919.859126.521094@stat.math.ethz.ch>
 <23433.19189.841070.766209@stat.math.ethz.ch>
 <CAA9Rc3N=Lji03ozt0qC=CeOa589ckQc9=iuq3hAmJfDLhmnkWw@mail.gmail.com>
 <23433.29002.152055.941837@stat.math.ethz.ch>
Message-ID: <CAA9Rc3PLkKxJbnb_gQ8JN=PKQaO4TEAqRKvyw-NYcLC0oX2MvA@mail.gmail.com>

Bug report submitted :
https://bugs.r-project.org/bugzilla/show_bug.cgi?id=17459
Thanks!

On Fri, Aug 31, 2018 at 6:48 PM Martin Maechler <maechler at stat.math.ethz.ch>
wrote:

> >>>>> Marco Giuliano
> >>>>>     on Fri, 31 Aug 2018 16:50:56 +0200 writes:
>
>     > Hi Martin, should I file a formal bug report somewhere or
>     > you've already done it ?
>
> No, I haven't,
> and as I may not address this bug further myself (in the near
> future), it may be best if you file a formal report.
>
> I will create an account for you on R's bugzilla - you will be
> notified and can update your initial pseudo-random password.
>
> Best,
> Martin
>
>

	[[alternative HTML version deleted]]


