From @oko| @end|ng |rom |n@@-tou|ou@e@|r  Tue Oct  1 10:58:46 2019
From: @oko| @end|ng |rom |n@@-tou|ou@e@|r (Serguei Sokol)
Date: Tue, 1 Oct 2019 10:58:46 +0200
Subject: [Rd] Is missingness always passed on?
In-Reply-To: <1fc5777f-cc96-ceb7-f2e4-4475622ce77b-860@gmail.com>
References: <1fc5777f-cc96-ceb7-f2e4-4475622ce77b-860@gmail.com>
Message-ID: <34912961-8e67-4c78-865d-e524e207a684@insa-toulouse.fr>

Le 30/09/2019 ? 16:17, Duncan Murdoch a ?crit?:
>
> There's a StackOverflow question 
> https://stackoverflow.com/q/22024082/2554330 that references this text 
> from ?missing:
>
> "Currently missing can only be used in the immediate body of the 
> function that defines the argument, not in the body of a nested 
> function or a local call. This may change in the future."
>
> Someone pointed out (in https://stackoverflow.com/a/58169498/2554330) 
> that this isn't true in the examples they've tried:? missingness does 
> get passed along.? This example shows it (this is slightly different 
> than the SO example):
>
> f1 <- function(x, y, z){
> ? if(missing(x))
> ??? cat("f1: x is missing\n")
> ? if(missing(y))
> ??? cat("f1: y is missing\n")
> }
>
> f2 <- function(x, y, z){
> ? if(missing(z))
> ??? cat("f2: z is missing\n")
> ? f1(x, y)
> }
>
> f2()
>
> which produces
>
> f2: z is missing
> f1: x is missing
> f1: y is missing
>
> Is the documentation out of date?? That quote appears to have been 
> written in 2002.
Er, as far? as I understand the cited doc, it correctly describes what 
happened in your example: missing() is not working in a local call (here 
f1(x,y)).
In fact, what missing() of f1 is reporting it is still the situation of 
f2() call (i.e. immediate body of the function). See

f2(y=1)

produces

f2: z is missing
f1: x is missing

(the line about y missing disappeared from f1(x,y) call, what needed to 
be demonstrated).

Best,
Serguei.


From murdoch@dunc@n @end|ng |rom gm@||@com  Tue Oct  1 11:22:26 2019
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Tue, 1 Oct 2019 05:22:26 -0400
Subject: [Rd] Is missingness always passed on?
In-Reply-To: <34912961-8e67-4c78-865d-e524e207a684@insa-toulouse.fr>
References: <1fc5777f-cc96-ceb7-f2e4-4475622ce77b-860@gmail.com>
 <34912961-8e67-4c78-865d-e524e207a684@insa-toulouse.fr>
Message-ID: <40de20d9-7ec7-83a8-dc88-3438d1de9715@gmail.com>

On 01/10/2019 4:58 a.m., Serguei Sokol wrote:
> Le 30/09/2019 ? 16:17, Duncan Murdoch a ?crit?:
>>
>> There's a StackOverflow question
>> https://stackoverflow.com/q/22024082/2554330 that references this text
>> from ?missing:
>>
>> "Currently missing can only be used in the immediate body of the
>> function that defines the argument, not in the body of a nested
>> function or a local call. This may change in the future."
>>
>> Someone pointed out (in https://stackoverflow.com/a/58169498/2554330)
>> that this isn't true in the examples they've tried:? missingness does
>> get passed along.? This example shows it (this is slightly different
>> than the SO example):
>>
>> f1 <- function(x, y, z){
>>  ? if(missing(x))
>>  ??? cat("f1: x is missing\n")
>>  ? if(missing(y))
>>  ??? cat("f1: y is missing\n")
>> }
>>
>> f2 <- function(x, y, z){
>>  ? if(missing(z))
>>  ??? cat("f2: z is missing\n")
>>  ? f1(x, y)
>> }
>>
>> f2()
>>
>> which produces
>>
>> f2: z is missing
>> f1: x is missing
>> f1: y is missing
>>
>> Is the documentation out of date?? That quote appears to have been
>> written in 2002.
> Er, as far? as I understand the cited doc, it correctly describes what
> happened in your example: missing() is not working in a local call (here
> f1(x,y)).
> In fact, what missing() of f1 is reporting it is still the situation of
> f2() call (i.e. immediate body of the function). See
> 
> f2(y=1)
> 
> produces
> 
> f2: z is missing
> f1: x is missing
> 
> (the line about y missing disappeared from f1(x,y) call, what needed to
> be demonstrated).

Yes, that's a possible interpretation.  Another one is that missing 
should fail in this example:

f2 <- function(z){
    f1 <- function(){
      if(missing(z))
        cat("f1: z is missing\n")
    }
    f1()
}

and it does:

 > f2()
Error in missing(z) (from #3) : 'missing' can only be used for arguments

Here missing() is appearing in a nested function.  I'm really not sure 
which is the intended meaning of that paragraph:  what exactly is a 
"local call"?

Duncan Murdoch


From @oko| @end|ng |rom |n@@-tou|ou@e@|r  Tue Oct  1 11:27:43 2019
From: @oko| @end|ng |rom |n@@-tou|ou@e@|r (Serguei Sokol)
Date: Tue, 1 Oct 2019 11:27:43 +0200
Subject: [Rd] Is missingness always passed on?
In-Reply-To: <34912961-8e67-4c78-865d-e524e207a684@insa-toulouse.fr>
References: <1fc5777f-cc96-ceb7-f2e4-4475622ce77b-860@gmail.com>
 <34912961-8e67-4c78-865d-e524e207a684@insa-toulouse.fr>
Message-ID: <d4df4d3e-48b3-40b3-64ad-75f8d6c87709@insa-toulouse.fr>

Le 01/10/2019 ? 10:58, Serguei Sokol a ?crit?:
> Le 30/09/2019 ? 16:17, Duncan Murdoch a ?crit?:
>>
>> There's a StackOverflow question 
>> https://stackoverflow.com/q/22024082/2554330 that references this 
>> text from ?missing:
>>
>> "Currently missing can only be used in the immediate body of the 
>> function that defines the argument, not in the body of a nested 
>> function or a local call. This may change in the future."
>>
>> Someone pointed out (in https://stackoverflow.com/a/58169498/2554330) 
>> that this isn't true in the examples they've tried:? missingness does 
>> get passed along.? This example shows it (this is slightly different 
>> than the SO example):
>>
>> f1 <- function(x, y, z){
>> ? if(missing(x))
>> ??? cat("f1: x is missing\n")
>> ? if(missing(y))
>> ??? cat("f1: y is missing\n")
>> }
>>
>> f2 <- function(x, y, z){
>> ? if(missing(z))
>> ??? cat("f2: z is missing\n")
>> ? f1(x, y)
>> }
>>
>> f2()
>>
>> which produces
>>
>> f2: z is missing
>> f1: x is missing
>> f1: y is missing
>>
>> Is the documentation out of date?? That quote appears to have been 
>> written in 2002.
> Er, as far? as I understand the cited doc, it correctly describes what 
> happened in your example: missing() is not working in a local call 
> (here f1(x,y)).
> In fact, what missing() of f1 is reporting it is still the situation 
> of f2() call (i.e. immediate body of the function). See
>
> f2(y=1)
>
> produces
>
> f2: z is missing
> f1: x is missing
>
> (the line about y missing disappeared from f1(x,y) call, what needed 
> to be demonstrated).
Re-er, it seem that I was a little bit to fast in my conclusion. If we 
modify f2 to be

f2 <- function(x, y, z){
 ? if(missing(z))
 ??? cat("f2: z is missing\n")
 ? f1(x=1, y)
}

then f2() call gives

f2: z is missing
f1: y is missing

i.e. missing() of f1(x=1,y) call is reporting its own situation, not 
those of f2(). And the missingess of y seems to be inherited from f2() call.
Sorry to be hasty.

Serguei.


From @|ex @end|ng |rom bed@t@dr|ven@com  Tue Oct  1 10:37:16 2019
From: @|ex @end|ng |rom bed@t@dr|ven@com (Bertram, Alexander)
Date: Tue, 1 Oct 2019 10:37:16 +0200
Subject: [Rd] Is missingness always passed on?
In-Reply-To: <1fc5777f-cc96-ceb7-f2e4-4475622ce77b-860@gmail.com>
References: <1fc5777f-cc96-ceb7-f2e4-4475622ce77b-860@gmail.com>
Message-ID: <CAOdV3zCtz4_pOXF8J9smOCVDsd2YOvCBx_-qBr1MQBmUvx7sBQ@mail.gmail.com>

There is "missing with default" and "missing without default".

If an argument x is missing without a default, then missing(x) is true, if
you pass x to another function, it will pass the value of the "missing
argument". (which is different than simply being missing!)

If an argument x is missing _with_a default, then missing(x) is still true,
but if you pass x to another function, the default value is passed, not the
missing argument symbol.

If you add default arguments to your example, you'll see this effect:

f1 <- function(x, y, z){
   if(missing(x))
     cat("f1: x is missing\n")
   if(missing(y))
     cat("f1: y is missing\n")
}

f2 <- function(x, y, z){
   if(missing(z))
     cat("f2: z is missing\n")
   f1(x, y)
}

f2()

prints

f2: z is missing

The intersection of default values, and the representation of missing
without a default as a symbol yields some unexpected and complex behaviors.
Here are a few more fun examples:
https://github.com/bedatadriven/renjin/blob/master/tests/src/test/R/test.missing.R

Best,
Alex



On Tue, Oct 1, 2019, 10:27 Duncan Murdoch <murdoch.duncan at gmail.com> wrote:

>
> There's a StackOverflow question
> https://stackoverflow.com/q/22024082/2554330 that references this text
> from ?missing:
>
> "Currently missing can only be used in the immediate body of the
> function that defines the argument, not in the body of a nested function
> or a local call. This may change in the future."
>
> Someone pointed out (in https://stackoverflow.com/a/58169498/2554330)
> that this isn't true in the examples they've tried:  missingness does
> get passed along.  This example shows it (this is slightly different
> than the SO example):
>
> f1 <- function(x, y, z){
>    if(missing(x))
>      cat("f1: x is missing\n")
>    if(missing(y))
>      cat("f1: y is missing\n")
> }
>
> f2 <- function(x, y, z){
>    if(missing(z))
>      cat("f2: z is missing\n")
>    f1(x, y)
> }
>
> f2()
>
> which produces
>
> f2: z is missing
> f1: x is missing
> f1: y is missing
>
> Is the documentation out of date?  That quote appears to have been
> written in 2002.
>
> Duncan Murdoch
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From @|ex @end|ng |rom bed@t@dr|ven@com  Tue Oct  1 10:38:32 2019
From: @|ex @end|ng |rom bed@t@dr|ven@com (Bertram, Alexander)
Date: Tue, 1 Oct 2019 10:38:32 +0200
Subject: [Rd] Is missingness always passed on?
In-Reply-To: <CAOdV3zCtz4_pOXF8J9smOCVDsd2YOvCBx_-qBr1MQBmUvx7sBQ@mail.gmail.com>
References: <1fc5777f-cc96-ceb7-f2e4-4475622ce77b-860@gmail.com>
 <CAOdV3zCtz4_pOXF8J9smOCVDsd2YOvCBx_-qBr1MQBmUvx7sBQ@mail.gmail.com>
Message-ID: <CAOdV3zCRNXGx4W7FLmA5QhWqR90Qkp0NH5qbAcK5jemsW_XQPA@mail.gmail.com>

The updated example was meant to read:

f1 <- function(x = NULL, y = NULL, z = NULL){
  if(missing(x))
    cat("f1: x is missing\n")
  if(missing(y))
    cat("f1: y is missing\n")
}

f2 <- function(x = NULL, y = NULL, z = NULL){
  if(missing(z))
    cat("f2: z is missing\n")
  f1(x, y)
}

f2()

Alex

On Tue, Oct 1, 2019 at 10:37 AM Bertram, Alexander <alex at bedatadriven.com>
wrote:

> There is "missing with default" and "missing without default".
>
> If an argument x is missing without a default, then missing(x) is true, if
> you pass x to another function, it will pass the value of the "missing
> argument". (which is different than simply being missing!)
>
> If an argument x is missing _with_a default, then missing(x) is still
> true, but if you pass x to another function, the default value is passed,
> not the missing argument symbol.
>
> If you add default arguments to your example, you'll see this effect:
>
> f1 <- function(x, y, z){
>    if(missing(x))
>      cat("f1: x is missing\n")
>    if(missing(y))
>      cat("f1: y is missing\n")
> }
>
> f2 <- function(x, y, z){
>    if(missing(z))
>      cat("f2: z is missing\n")
>    f1(x, y)
> }
>
> f2()
>
> prints
>
> f2: z is missing
>
> The intersection of default values, and the representation of missing
> without a default as a symbol yields some unexpected and complex behaviors.
> Here are a few more fun examples:
>
> https://github.com/bedatadriven/renjin/blob/master/tests/src/test/R/test.missing.R
>
> Best,
> Alex
>
>
>
> On Tue, Oct 1, 2019, 10:27 Duncan Murdoch <murdoch.duncan at gmail.com>
> wrote:
>
>>
>> There's a StackOverflow question
>> https://stackoverflow.com/q/22024082/2554330 that references this text
>> from ?missing:
>>
>> "Currently missing can only be used in the immediate body of the
>> function that defines the argument, not in the body of a nested function
>> or a local call. This may change in the future."
>>
>> Someone pointed out (in https://stackoverflow.com/a/58169498/2554330)
>> that this isn't true in the examples they've tried:  missingness does
>> get passed along.  This example shows it (this is slightly different
>> than the SO example):
>>
>> f1 <- function(x, y, z){
>>    if(missing(x))
>>      cat("f1: x is missing\n")
>>    if(missing(y))
>>      cat("f1: y is missing\n")
>> }
>>
>> f2 <- function(x, y, z){
>>    if(missing(z))
>>      cat("f2: z is missing\n")
>>    f1(x, y)
>> }
>>
>> f2()
>>
>> which produces
>>
>> f2: z is missing
>> f1: x is missing
>> f1: y is missing
>>
>> Is the documentation out of date?  That quote appears to have been
>> written in 2002.
>>
>> Duncan Murdoch
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>

-- 
Alexander Bertram
Technical Director
*BeDataDriven BV*

Web: http://bedatadriven.com
Email: alex at bedatadriven.com
Tel. Nederlands: +31(0)647205388
Skype: akbertram

	[[alternative HTML version deleted]]


From jerome@|@uren@ @end|ng |rom u-bourgogne@|r  Tue Oct  1 11:47:38 2019
From: jerome@|@uren@ @end|ng |rom u-bourgogne@|r (=?utf-8?Q?J=C3=A9r=C3=B4me_LAURENS?=)
Date: Tue, 1 Oct 2019 11:47:38 +0200
Subject: [Rd] Improvement in hist command documentation
Message-ID: <86B01203-9D0C-45FA-A4F1-762E4B22C41E@u-bourgogne.fr>

Hi all,

In the histogram created with the `hist` command, the label of the y axis reads "Frequency ?.
Finding that the ylab key is used to change (localize) that word may seem difficult for people
who are beginners either in R or in english.

I suggest very minor modifications of the file hist.Rd at https://svn.r-project.org/R/trunk/src/library/graphics/man/hist.Rd.
1) make the default value for ylab explicit, similarly to main and xlab
2) make the word "label" explicitly appear in the documentation for the ylab key

That way, people looking either for the word "label" or the word "Frequency" in hist?s help will get more chance to find the ylab key.

Here is the diff with revision 77240

17c17
<      xlab = xname, ylab,
---
>      xlab = xname, ylab = "Frequency",
65,66c65,66
<   \item{main, xlab, ylab}{these arguments to \code{title} have useful
<     defaults here.}
---
>   \item{main, xlab, ylab}{to change the default value of the main title,
>     the label of the x axis or the label of the y axis.}

Thanks for your efforts.

JL


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Wed Oct  2 08:50:01 2019
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Wed, 2 Oct 2019 08:50:01 +0200
Subject: [Rd] Improvement in hist command documentation
In-Reply-To: <86B01203-9D0C-45FA-A4F1-762E4B22C41E@u-bourgogne.fr>
References: <86B01203-9D0C-45FA-A4F1-762E4B22C41E@u-bourgogne.fr>
Message-ID: <23956.18585.282365.676969@stat.math.ethz.ch>

>>>>> J?r?me LAURENS 
>>>>>     on Tue, 1 Oct 2019 11:47:38 +0200 writes:

    > Hi all,
    > In the histogram created with the `hist` command, the label of the y axis reads "Frequency ?.
    > Finding that the ylab key is used to change (localize) that word may seem difficult for people
    > who are beginners either in R or in english.

    > I suggest very minor modifications of the file hist.Rd at https://svn.r-project.org/R/trunk/src/library/graphics/man/hist.Rd.
    > 1) make the default value for ylab explicit, similarly to main and xlab
    > 2) make the word "label" explicitly appear in the documentation for the ylab key

    > That way, people looking either for the word "label" or the word "Frequency" in hist?s help will get more chance to find the ylab key.

    > Here is the diff with revision 77240

    > 17c17
    > <      xlab = xname, ylab,
    > ---
    >> xlab = xname, ylab = "Frequency",
    > 65,66c65,66
    > <   \item{main, xlab, ylab}{these arguments to \code{title} have useful
    > <     defaults here.}
    > ---
    >> \item{main, xlab, ylab}{to change the default value of the main title,
    >> the label of the x axis or the label of the y axis.}

    > Thanks for your efforts.
    > JL

Thank you, J?r?me, for helping to improve R's documentation.

I was hoping others would point out that your '1)' is wrong
unfortunately:  As the current doc say  - not optimally -

   " these arguments ... have useful defaults here "

and the default for ylab is not just "Frequency", and the whole
thing is a bit more subtle than you think notably because a
"histogram" object is constructed with a plot method, etc.
Please look at the source code the development of which code is
always visible here
  https://svn.r-project.org/R/trunk/src/library/graphics/R/hist.R

But you are right that the

 \item{main, xlab, ylab}{these arguments to \code{title} have useful
    defaults here.}

can be improved and I will do so, ensuring that 'label' will
appear there.

Best regards,

Martin Maechler,
ETH Zurich and R Core Team


From therne@u @end|ng |rom m@yo@edu  Fri Oct  4 16:19:51 2019
From: therne@u @end|ng |rom m@yo@edu (Therneau, Terry M., Ph.D.)
Date: Fri, 04 Oct 2019 09:19:51 -0500
Subject: [Rd] Error in [.terms
Message-ID: <771925$ch3q2b@ironport10.mayo.edu>

Martin,

 ? There are a couple of issues with [.terms that have bitten my survival code.? At the 
useR conference I promised you a detailed (readable) explanation, and have been lax in 
getting it to you. The error was first pointed out in a bugzilla note from 2016, by the 
way.? The current survival code works around these.

Consider the following formula:

<<testform>>=
library(survival)? # only to get access to the lung data set
test <- Surv(time, status) ~? age + offset(ph.ecog) + strata(inst)
tform <- terms(test, specials="strata")
mf <- model.frame(tform, data=lung)
mterm <- terms(mf)
@

The strata term is handled in a special way by coxph, and then needs to be removed from 
the model formula before calling model.matrix.
To do this the code uses essentially the following, which fails for the formula above.

<<strata>>=
strata <- attr(mterm, "specials")$strata - attr(mterm, "response")
X <- model.matrix(mterm[-strata], mf)
@

The root problem is the need for multiple subscripts.
\begin{itemize}
 ? \item The formula itself has length 5, with `~' as the first element
 ? \item The variables and predvars attributes are call objects, each a list() with 4 
elments: the response and all 3 predictors
 ? \item The term.labels attribute omits the resonse and the offset, so has? length 2
 ? \item The factors attribute has 4 rows and 2 columns
 ? \item The dataClasses attribute is a character vector of length 4
\end{itemize}

So the ideal result of? mterm[remove the specials] would use subscript of
\begin{itemize}
 ? \item [-5] on the formula itself, variables and predvars attributes
 ? \item [-2] for term.labels
 ? \item [-4 , -2, drop=FALSE] for factor attribute
 ? \item [-2] for order attribute
 ? \item [-4] for the dataClasses attribute
\end{itemize}

That will recreate the formula that ``would have been'' had there been no strata term.? 
Now look at the first portion of the code in models.R
<<>>=
`[.terms` <- function (termobj, i)
{
 ??? resp <- if (attr(termobj, "response")) termobj[[2L]]
 ??? newformula <- attr(termobj, "term.labels")[i]
 ??? if (length(newformula) == 0L) newformula <- "1"
 ??? newformula <- reformulate(newformula, resp, attr(termobj, "intercept"), 
environment(termobj))
 ??? result <- terms(newformula, specials = names(attr(termobj, "specials")))

 ??? # Edit the optional attributes
}
@

The use of reformulate() is a nice trick.? However, the index reported in the specials 
attribute is generated with reference to the variables
attribute, or equivalently the row names of the factors attribute, not with respect to the 
term.labels attribute. For consistency the second line should instead be
<<>>=
newformula <- row.names(attr(termobj, "factors"))[i]
@

Of course, this will break code for anyone else who has used [.terms and, like me, has 
been adjusting for the ``response is counted in specials but
not in term.labels'' feature.? R core will have to discuss/decide what is the right thing 
to do, and I'll adapt.

The reformulate trick breaks in another way, one that only appeared on my radar this week 
via a formula like the following.

<<form2>>=
Surv(time, status) ~ age + (sex=='male') + strata(inst)
@

In both the term.labels attribute and the row/col names of the factors attribute the 
parentheses disappear, and the result of the reformulate call is not a proper formula.? 
The + binds tighter than == leading to an error message that will confuse most users. We 
can argue, and I probably would, that the user should have used I(sex=='male').? But they 
didn't, and without the I() it is a legal formula, or at least one that currently works.? 
Fixing this issue is a lot harder.

An offset term causes issues in the 'Edit the optional attributes' part of the routine as 
well.? If you and/or R core will tell me what you think
the code should do, I'll create a patch.? My vote would be to use rownames per the above 
and ignore the () edge case.

The same basic code appears in drop.terms, by the way.

Terry T.


	[[alternative HTML version deleted]]


From @nto|ne@|@br| @end|ng |rom gm@||@com  Sat Oct  5 16:34:23 2019
From: @nto|ne@|@br| @end|ng |rom gm@||@com (Ant F)
Date: Sat, 5 Oct 2019 16:34:23 +0200
Subject: [Rd] should base R have a piping operator ?
Message-ID: <CAEKh8uidcEWA4tzmkpQxpF=q7r93qjf-HtiWLfbT91TFGOSzFQ@mail.gmail.com>

Dear R-devel,

The most popular piping operator sits in the package `magrittr` and is used
by a huge amount of users, and imported /reexported by more and more
packages too.

Many workflows don't even make much sense without pipes nowadays, so the
examples in the doc will use pipes, as do the README, vignettes etc. I
believe base R could have a piping operator so packages can use a pipe in
their code or doc and stay dependency free.

I don't suggest an operator based on complex heuristics, instead I suggest
a very simple and fast one (>10 times than magrittr in my tests) :

`%.%` <- function (e1, e2) {
  eval(substitute(e2), envir = list(. = e1), enclos = parent.frame())
}

iris %.% head(.) %.% dim(.)
#> [1] 6 5

The difference with magrittr is that the dots must all be explicit (which
sits with the choice of the name), and that special magrittr features such
as assignment in place and building functions with `. %>% head() %>% dim()`
are not supported.

Edge cases are not surprising:

```
x <- "a"
x %.% quote(.)
#> .
x %.% substitute(.)
#> [1] "a"

f1 <- function(y) function() eval(quote(y))
f2 <- x %.% f1(.)
f2()
#> [1] "a"
```

Looking forward for your thoughts on this,

Antoine

	[[alternative HTML version deleted]]


From je||@@@ry@n @end|ng |rom gm@||@com  Sat Oct  5 16:57:25 2019
From: je||@@@ry@n @end|ng |rom gm@||@com (Jeff Ryan)
Date: Sat, 5 Oct 2019 09:57:25 -0500
Subject: [Rd] should base R have a piping operator ?
In-Reply-To: <CAEKh8uidcEWA4tzmkpQxpF=q7r93qjf-HtiWLfbT91TFGOSzFQ@mail.gmail.com>
References: <CAEKh8uidcEWA4tzmkpQxpF=q7r93qjf-HtiWLfbT91TFGOSzFQ@mail.gmail.com>
Message-ID: <CABDUZc8WcFypnRQDGBvSb07BuYc_F-OXX-CnykrG2fzMd=tPDA@mail.gmail.com>

Is there some concrete example of your ?many workflows don?t even make much
sense without pipes nowadays? comment?

I don?t think I?m opposed to pipes in the absolute, but as I am now deep
into my second decade of using R I?ve done just fine without them. As I
would guess have the vast majority of users and code that is used
throughout the world.

Jeff

On Sat, Oct 5, 2019 at 09:34 Ant F <antoine.fabri at gmail.com> wrote:

> Dear R-devel,
>
> The most popular piping operator sits in the package `magrittr` and is used
> by a huge amount of users, and imported /reexported by more and more
> packages too.
>
> Many workflows don't even make much sense without pipes nowadays, so the
> examples in the doc will use pipes, as do the README, vignettes etc. I
> believe base R could have a piping operator so packages can use a pipe in
> their code or doc and stay dependency free.
>
> I don't suggest an operator based on complex heuristics, instead I suggest
> a very simple and fast one (>10 times than magrittr in my tests) :
>
> `%.%` <- function (e1, e2) {
>   eval(substitute(e2), envir = list(. = e1), enclos = parent.frame())
> }
>
> iris %.% head(.) %.% dim(.)
> #> [1] 6 5
>
> The difference with magrittr is that the dots must all be explicit (which
> sits with the choice of the name), and that special magrittr features such
> as assignment in place and building functions with `. %>% head() %>% dim()`
> are not supported.
>
> Edge cases are not surprising:
>
> ```
> x <- "a"
> x %.% quote(.)
> #> .
> x %.% substitute(.)
> #> [1] "a"
>
> f1 <- function(y) function() eval(quote(y))
> f2 <- x %.% f1(.)
> f2()
> #> [1] "a"
> ```
>
> Looking forward for your thoughts on this,
>
> Antoine
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From jmount @end|ng |rom w|n-vector@com  Sat Oct  5 16:59:27 2019
From: jmount @end|ng |rom w|n-vector@com (John Mount)
Date: Sat, 5 Oct 2019 07:59:27 -0700
Subject: [Rd] should base R have a piping operator ?
In-Reply-To: <CAEKh8uidcEWA4tzmkpQxpF=q7r93qjf-HtiWLfbT91TFGOSzFQ@mail.gmail.com>
References: <CAEKh8uidcEWA4tzmkpQxpF=q7r93qjf-HtiWLfbT91TFGOSzFQ@mail.gmail.com>
Message-ID: <859F02FC-2DCF-4FFA-A941-1A7285B5F362@win-vector.com>

Actually, base R already has a pipe fairly close to the one you describe: ->.;

	iris ->.; head(.) ->.; dim(.)
	# [1] 6 5

I've called it the Bizarro pipe ( http://www.win-vector.com/blog/2016/12/magrittrs-doppelganger/ <http://www.win-vector.com/blog/2016/12/magrittrs-doppelganger/> ), and for some reason we chickened out and didn't spend time on it in the dot pipe paper ( https://journal.r-project.org/archive/2018/RJ-2018-042/index.html <https://journal.r-project.org/archive/2018/RJ-2018-042/index.html> ).

For documentation Bizarro pipe has the advantage that one can work out how it works from the application itself, with out reference to a defining function.

> On Oct 5, 2019, at 7:34 AM, Ant F <antoine.fabri at gmail.com> wrote:
> 
> Dear R-devel,
> 
> The most popular piping operator sits in the package `magrittr` and is used
> by a huge amount of users, and imported /reexported by more and more
> packages too.
> 
> Many workflows don't even make much sense without pipes nowadays, so the
> examples in the doc will use pipes, as do the README, vignettes etc. I
> believe base R could have a piping operator so packages can use a pipe in
> their code or doc and stay dependency free.
> 
> I don't suggest an operator based on complex heuristics, instead I suggest
> a very simple and fast one (>10 times than magrittr in my tests) :
> 
> `%.%` <- function (e1, e2) {
>  eval(substitute(e2), envir = list(. = e1), enclos = parent.frame())
> }
> 
> iris %.% head(.) %.% dim(.)
> #> [1] 6 5
> 
> The difference with magrittr is that the dots must all be explicit (which
> sits with the choice of the name), and that special magrittr features such
> as assignment in place and building functions with `. %>% head() %>% dim()`
> are not supported.
> 
> Edge cases are not surprising:
> 
> ```
> x <- "a"
> x %.% quote(.)
> #> .
> x %.% substitute(.)
> #> [1] "a"
> 
> f1 <- function(y) function() eval(quote(y))
> f2 <- x %.% f1(.)
> f2()
> #> [1] "a"
> ```
> 
> Looking forward for your thoughts on this,
> 
> Antoine
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

---------------
John Mount
http://www.win-vector.com/ <http://www.win-vector.com/> 
Our book: Practical Data Science with R https://www.manning.com/books/practical-data-science-with-r-second-edition <http://www.manning.com/zumel/>





	[[alternative HTML version deleted]]


From hugh@m@rer@ @end|ng |rom gm@||@com  Sat Oct  5 17:03:25 2019
From: hugh@m@rer@ @end|ng |rom gm@||@com (Hugh Marera)
Date: Sat, 5 Oct 2019 17:03:25 +0200
Subject: [Rd] should base R have a piping operator ?
In-Reply-To: <CAEKh8uidcEWA4tzmkpQxpF=q7r93qjf-HtiWLfbT91TFGOSzFQ@mail.gmail.com>
References: <CAEKh8uidcEWA4tzmkpQxpF=q7r93qjf-HtiWLfbT91TFGOSzFQ@mail.gmail.com>
Message-ID: <CAGpovd2jxxWBUxPhMAR6ZPw1+UQzK7BzJVtps10cMBSp=EUdHQ@mail.gmail.com>

How is your argument different to, say,  "Should dplyr or data.table be
part of base R as they are the most popular data science packages and they
are used by a large number of users?"

Kind regards

On Sat, Oct 5, 2019 at 4:34 PM Ant F <antoine.fabri at gmail.com> wrote:

> Dear R-devel,
>
> The most popular piping operator sits in the package `magrittr` and is used
> by a huge amount of users, and imported /reexported by more and more
> packages too.
>
> Many workflows don't even make much sense without pipes nowadays, so the
> examples in the doc will use pipes, as do the README, vignettes etc. I
> believe base R could have a piping operator so packages can use a pipe in
> their code or doc and stay dependency free.
>
> I don't suggest an operator based on complex heuristics, instead I suggest
> a very simple and fast one (>10 times than magrittr in my tests) :
>
> `%.%` <- function (e1, e2) {
>   eval(substitute(e2), envir = list(. = e1), enclos = parent.frame())
> }
>
> iris %.% head(.) %.% dim(.)
> #> [1] 6 5
>
> The difference with magrittr is that the dots must all be explicit (which
> sits with the choice of the name), and that special magrittr features such
> as assignment in place and building functions with `. %>% head() %>% dim()`
> are not supported.
>
> Edge cases are not surprising:
>
> ```
> x <- "a"
> x %.% quote(.)
> #> .
> x %.% substitute(.)
> #> [1] "a"
>
> f1 <- function(y) function() eval(quote(y))
> f2 <- x %.% f1(.)
> f2()
> #> [1] "a"
> ```
>
> Looking forward for your thoughts on this,
>
> Antoine
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From d@v|dhughjone@ @end|ng |rom gm@||@com  Sat Oct  5 17:15:20 2019
From: d@v|dhughjone@ @end|ng |rom gm@||@com (David Hugh-Jones)
Date: Sat, 5 Oct 2019 16:15:20 +0100
Subject: [Rd] should base R have a piping operator ?
In-Reply-To: <CAEKh8uidcEWA4tzmkpQxpF=q7r93qjf-HtiWLfbT91TFGOSzFQ@mail.gmail.com>
References: <CAEKh8uidcEWA4tzmkpQxpF=q7r93qjf-HtiWLfbT91TFGOSzFQ@mail.gmail.com>
Message-ID: <CAARY7khPzSC7W9eV2JsF+5giPuuf+0PRo9MxR0pWCZiVgS4NYg@mail.gmail.com>

I +1 this idea, without judging the implementation details. The pipe
operator has proven vastly popular. Adding it would be
relatively easy (I think). Having it as part of the core would be a strong
guarantee of the future stability of this syntax.



On Sat, 5 Oct 2019 at 15:34, Ant F <antoine.fabri at gmail.com> wrote:

> Dear R-devel,
>
> The most popular piping operator sits in the package `magrittr` and is used
> by a huge amount of users, and imported /reexported by more and more
> packages too.
>
> Many workflows don't even make much sense without pipes nowadays, so the
> examples in the doc will use pipes, as do the README, vignettes etc. I
> believe base R could have a piping operator so packages can use a pipe in
> their code or doc and stay dependency free.
>
> I don't suggest an operator based on complex heuristics, instead I suggest
> a very simple and fast one (>10 times than magrittr in my tests) :
>
> `%.%` <- function (e1, e2) {
>   eval(substitute(e2), envir = list(. = e1), enclos = parent.frame())
> }
>
> iris %.% head(.) %.% dim(.)
> #> [1] 6 5
>
> The difference with magrittr is that the dots must all be explicit (which
> sits with the choice of the name), and that special magrittr features such
> as assignment in place and building functions with `. %>% head() %>% dim()`
> are not supported.
>
> Edge cases are not surprising:
>
> ```
> x <- "a"
> x %.% quote(.)
> #> .
> x %.% substitute(.)
> #> [1] "a"
>
> f1 <- function(y) function() eval(quote(y))
> f2 <- x %.% f1(.)
> f2()
> #> [1] "a"
> ```
>
> Looking forward for your thoughts on this,
>
> Antoine
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>
-- 
Sent from Gmail Mobile

	[[alternative HTML version deleted]]


From @nto|ne@|@br| @end|ng |rom gm@||@com  Sat Oct  5 17:48:03 2019
From: @nto|ne@|@br| @end|ng |rom gm@||@com (Ant F)
Date: Sat, 5 Oct 2019 17:48:03 +0200
Subject: [Rd] should base R have a piping operator ?
In-Reply-To: <859F02FC-2DCF-4FFA-A941-1A7285B5F362@win-vector.com>
References: <CAEKh8uidcEWA4tzmkpQxpF=q7r93qjf-HtiWLfbT91TFGOSzFQ@mail.gmail.com>
 <859F02FC-2DCF-4FFA-A941-1A7285B5F362@win-vector.com>
Message-ID: <CAEKh8uj_NsmKQcCge1oi0qp3j2AgCV3krMhGiKghmySfKPL=YA@mail.gmail.com>

Hi John,

Thanks, but the Bizzaro pipe comes with many flaws though :
* It's not a single operator
* It has a different precedence
* It cannot be used in a subcall
* The variable assigned to must be on the right
* It doesn't trigger indentation when going to the line
* It creates/overwrite a `.` variable in the worksace.

And it doesn't deal gracefully with some lazy evaluation edge cases such as
:

compose <- function(f, g) { function(x) g(f(x)) }
plus1   <- function(x) x + 1

plus2 <- plus1 %.% compose(.,plus1)
plus2(5)
#> [1] 7

plus1 ->.; compose(.,plus1) -> .; . -> plus2
plus2(5)
#> Error: C stack usage  15923776 is too close to the limit

What I propose on the other hand can always substitute any existing proper
pipe in their standard feature, as long as the dot is made explicit.

Best regards,

Antoine



Le sam. 5 oct. 2019 ? 16:59, John Mount <jmount at win-vector.com> a ?crit :

> Actually, base R already has a pipe fairly close to the one you describe:
> ->.;
>
> iris ->.; head(.) ->.; dim(.)
> # [1] 6 5
>
> I've called it the Bizarro pipe (
> http://www.win-vector.com/blog/2016/12/magrittrs-doppelganger/ ), and for
> some reason we chickened out and didn't spend time on it in the dot pipe
> paper ( https://journal.r-project.org/archive/2018/RJ-2018-042/index.html
>  ).
>
> For documentation Bizarro pipe has the advantage that one can work out how
> it works from the application itself, with out reference to a defining
> function.
>
> On Oct 5, 2019, at 7:34 AM, Ant F <antoine.fabri at gmail.com> wrote:
>
> Dear R-devel,
>
> The most popular piping operator sits in the package `magrittr` and is used
> by a huge amount of users, and imported /reexported by more and more
> packages too.
>
> Many workflows don't even make much sense without pipes nowadays, so the
> examples in the doc will use pipes, as do the README, vignettes etc. I
> believe base R could have a piping operator so packages can use a pipe in
> their code or doc and stay dependency free.
>
> I don't suggest an operator based on complex heuristics, instead I suggest
> a very simple and fast one (>10 times than magrittr in my tests) :
>
> `%.%` <- function (e1, e2) {
>  eval(substitute(e2), envir = list(. = e1), enclos = parent.frame())
> }
>
> iris %.% head(.) %.% dim(.)
> #> [1] 6 5
>
> The difference with magrittr is that the dots must all be explicit (which
> sits with the choice of the name), and that special magrittr features such
> as assignment in place and building functions with `. %>% head() %>% dim()`
> are not supported.
>
> Edge cases are not surprising:
>
> ```
> x <- "a"
> x %.% quote(.)
> #> .
> x %.% substitute(.)
> #> [1] "a"
>
> f1 <- function(y) function() eval(quote(y))
> f2 <- x %.% f1(.)
> f2()
> #> [1] "a"
> ```
>
> Looking forward for your thoughts on this,
>
> Antoine
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>
>
> ---------------
> John Mount
> http://www.win-vector.com/
> Our book: Practical Data Science with R
> https://www.manning.com/books/practical-data-science-with-r-second-edition
> <http://www.manning.com/zumel/>
>
>
>
>
>

	[[alternative HTML version deleted]]


From jmount @end|ng |rom w|n-vector@com  Sat Oct  5 18:01:00 2019
From: jmount @end|ng |rom w|n-vector@com (John Mount)
Date: Sat, 5 Oct 2019 09:01:00 -0700
Subject: [Rd] should base R have a piping operator ?
In-Reply-To: <CAEKh8uj_NsmKQcCge1oi0qp3j2AgCV3krMhGiKghmySfKPL=YA@mail.gmail.com>
References: <CAEKh8uidcEWA4tzmkpQxpF=q7r93qjf-HtiWLfbT91TFGOSzFQ@mail.gmail.com>
 <859F02FC-2DCF-4FFA-A941-1A7285B5F362@win-vector.com>
 <CAEKh8uj_NsmKQcCge1oi0qp3j2AgCV3krMhGiKghmySfKPL=YA@mail.gmail.com>
Message-ID: <D7D71F4B-18BD-4A67-B400-D55C800805D1@win-vector.com>

Many of those issues can be dealt with by introducing curly braces:

	compose <- function(f, g) { function(x) g(f(x)) }
	plus1 <- function(x) x + 1
	plus2 <- { plus1 ->.; compose(., plus1) }
	plus2(5)
	# [1] 7

And a lot of that is a point to note: we may not all agree on what cases are corner cases, and which ones should be handled in a given way.

> On Oct 5, 2019, at 8:48 AM, Ant F <antoine.fabri at gmail.com> wrote:
> 
> Hi John,
> 
> Thanks, but the Bizzaro pipe comes with many flaws though :
> * It's not a single operator
> * It has a different precedence
> * It cannot be used in a subcall
> * The variable assigned to must be on the right
> * It doesn't trigger indentation when going to the line
> * It creates/overwrite a `.` variable in the worksace. 
> 
> And it doesn't deal gracefully with some lazy evaluation edge cases such as : 
> 
> compose <- function(f, g) { function(x) g(f(x)) }
> plus1   <- function(x) x + 1
> 
> plus2 <- plus1 %.% compose(.,plus1)
> plus2(5)
> #> [1] 7
> 
> plus1 ->.; compose(.,plus1) -> .; . -> plus2
> plus2(5)
> #> Error: C stack usage  15923776 is too close to the limit
> 
> What I propose on the other hand can always substitute any existing proper pipe in their standard feature, as long as the dot is made explicit.
> 
> Best regards,
> 
> Antoine
> 
> 
> 
> Le sam. 5 oct. 2019 ? 16:59, John Mount <jmount at win-vector.com <mailto:jmount at win-vector.com>> a ?crit :
> Actually, base R already has a pipe fairly close to the one you describe: ->.;
> 
> 	iris ->.; head(.) ->.; dim(.)
> 	# [1] 6 5
> 
> I've called it the Bizarro pipe ( http://www.win-vector.com/blog/2016/12/magrittrs-doppelganger/ <http://www.win-vector.com/blog/2016/12/magrittrs-doppelganger/> ), and for some reason we chickened out and didn't spend time on it in the dot pipe paper ( https://journal.r-project.org/archive/2018/RJ-2018-042/index.html <https://journal.r-project.org/archive/2018/RJ-2018-042/index.html> ).
> 
> For documentation Bizarro pipe has the advantage that one can work out how it works from the application itself, with out reference to a defining function.
> 
>> On Oct 5, 2019, at 7:34 AM, Ant F <antoine.fabri at gmail.com <mailto:antoine.fabri at gmail.com>> wrote:
>> 
>> Dear R-devel,
>> 
>> The most popular piping operator sits in the package `magrittr` and is used
>> by a huge amount of users, and imported /reexported by more and more
>> packages too.
>> 
>> Many workflows don't even make much sense without pipes nowadays, so the
>> examples in the doc will use pipes, as do the README, vignettes etc. I
>> believe base R could have a piping operator so packages can use a pipe in
>> their code or doc and stay dependency free.
>> 
>> I don't suggest an operator based on complex heuristics, instead I suggest
>> a very simple and fast one (>10 times than magrittr in my tests) :
>> 
>> `%.%` <- function (e1, e2) {
>>  eval(substitute(e2), envir = list(. = e1), enclos = parent.frame())
>> }
>> 
>> iris %.% head(.) %.% dim(.)
>> #> [1] 6 5
>> 
>> The difference with magrittr is that the dots must all be explicit (which
>> sits with the choice of the name), and that special magrittr features such
>> as assignment in place and building functions with `. %>% head() %>% dim()`
>> are not supported.
>> 
>> Edge cases are not surprising:
>> 
>> ```
>> x <- "a"
>> x %.% quote(.)
>> #> .
>> x %.% substitute(.)
>> #> [1] "a"
>> 
>> f1 <- function(y) function() eval(quote(y))
>> f2 <- x %.% f1(.)
>> f2()
>> #> [1] "a"
>> ```
>> 
>> Looking forward for your thoughts on this,
>> 
>> Antoine
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-devel at r-project.org <mailto:R-devel at r-project.org> mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel <https://stat.ethz.ch/mailman/listinfo/r-devel>
> 
> ---------------
> John Mount
> http://www.win-vector.com/ <http://www.win-vector.com/> 
> Our book: Practical Data Science with R https://www.manning.com/books/practical-data-science-with-r-second-edition <http://www.manning.com/zumel/>
> 
> 
> 
> 

---------------
John Mount
http://www.win-vector.com/ <http://www.win-vector.com/> 
Our book: Practical Data Science with R https://www.manning.com/books/practical-data-science-with-r-second-edition <http://www.manning.com/zumel/>





	[[alternative HTML version deleted]]


From ru|pb@rr@d@@ @end|ng |rom @@po@pt  Sat Oct  5 18:03:04 2019
From: ru|pb@rr@d@@ @end|ng |rom @@po@pt (Rui Barradas)
Date: Sat, 5 Oct 2019 17:03:04 +0100
Subject: [Rd] should base R have a piping operator ?
In-Reply-To: <CAEKh8uj_NsmKQcCge1oi0qp3j2AgCV3krMhGiKghmySfKPL=YA@mail.gmail.com>
References: <CAEKh8uidcEWA4tzmkpQxpF=q7r93qjf-HtiWLfbT91TFGOSzFQ@mail.gmail.com>
 <859F02FC-2DCF-4FFA-A941-1A7285B5F362@win-vector.com>
 <CAEKh8uj_NsmKQcCge1oi0qp3j2AgCV3krMhGiKghmySfKPL=YA@mail.gmail.com>
Message-ID: <ce629286-825e-0315-4570-28c1c57dbfc0@sapo.pt>

Hello,

R is a functional language, pipes are not. There are even higher order 
functions such as [1] and [2].
Besides, packages are part of R, R couldn't live without them. I find 
pipes a good idea but I also find it better not to have them as part of 
base R. If you want to use them, load a package, if you don't, don't. 
This simple.

As for your example, compose, there is a StackOverflow question on it. 
See this answer [3].

[1] https://stat.ethz.ch/R-manual/R-devel/library/base/html/funprog.html
[2] https://stat.ethz.ch/R-manual/R-devel/library/base/html/Recall.html
[3] https://stackoverflow.com/a/52465956/8245406

Hope this helps,

Rui Barradas

?s 16:48 de 05/10/19, Ant F escreveu:
> Hi John,
> 
> Thanks, but the Bizzaro pipe comes with many flaws though :
> * It's not a single operator
> * It has a different precedence
> * It cannot be used in a subcall
> * The variable assigned to must be on the right
> * It doesn't trigger indentation when going to the line
> * It creates/overwrite a `.` variable in the worksace.
> 
> And it doesn't deal gracefully with some lazy evaluation edge cases such as
> :
> 
> compose <- function(f, g) { function(x) g(f(x)) }
> plus1   <- function(x) x + 1
> 
> plus2 <- plus1 %.% compose(.,plus1)
> plus2(5)
> #> [1] 7
> 
> plus1 ->.; compose(.,plus1) -> .; . -> plus2
> plus2(5)
> #> Error: C stack usage  15923776 is too close to the limit
> 
> What I propose on the other hand can always substitute any existing proper
> pipe in their standard feature, as long as the dot is made explicit.
> 
> Best regards,
> 
> Antoine
> 
> 
> 
> Le sam. 5 oct. 2019 ? 16:59, John Mount <jmount at win-vector.com> a ?crit :
> 
>> Actually, base R already has a pipe fairly close to the one you describe:
>> ->.;
>>
>> iris ->.; head(.) ->.; dim(.)
>> # [1] 6 5
>>
>> I've called it the Bizarro pipe (
>> http://www.win-vector.com/blog/2016/12/magrittrs-doppelganger/ ), and for
>> some reason we chickened out and didn't spend time on it in the dot pipe
>> paper ( https://journal.r-project.org/archive/2018/RJ-2018-042/index.html
>>   ).
>>
>> For documentation Bizarro pipe has the advantage that one can work out how
>> it works from the application itself, with out reference to a defining
>> function.
>>
>> On Oct 5, 2019, at 7:34 AM, Ant F <antoine.fabri at gmail.com> wrote:
>>
>> Dear R-devel,
>>
>> The most popular piping operator sits in the package `magrittr` and is used
>> by a huge amount of users, and imported /reexported by more and more
>> packages too.
>>
>> Many workflows don't even make much sense without pipes nowadays, so the
>> examples in the doc will use pipes, as do the README, vignettes etc. I
>> believe base R could have a piping operator so packages can use a pipe in
>> their code or doc and stay dependency free.
>>
>> I don't suggest an operator based on complex heuristics, instead I suggest
>> a very simple and fast one (>10 times than magrittr in my tests) :
>>
>> `%.%` <- function (e1, e2) {
>>   eval(substitute(e2), envir = list(. = e1), enclos = parent.frame())
>> }
>>
>> iris %.% head(.) %.% dim(.)
>> #> [1] 6 5
>>
>> The difference with magrittr is that the dots must all be explicit (which
>> sits with the choice of the name), and that special magrittr features such
>> as assignment in place and building functions with `. %>% head() %>% dim()`
>> are not supported.
>>
>> Edge cases are not surprising:
>>
>> ```
>> x <- "a"
>> x %.% quote(.)
>> #> .
>> x %.% substitute(.)
>> #> [1] "a"
>>
>> f1 <- function(y) function() eval(quote(y))
>> f2 <- x %.% f1(.)
>> f2()
>> #> [1] "a"
>> ```
>>
>> Looking forward for your thoughts on this,
>>
>> Antoine
>>
>> [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>>
>> ---------------
>> John Mount
>> http://www.win-vector.com/
>> Our book: Practical Data Science with R
>> https://www.manning.com/books/practical-data-science-with-r-second-edition
>> <http://www.manning.com/zumel/>
>>
>>
>>
>>
>>
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From |uc@r @end|ng |rom |edor@project@org  Sat Oct  5 18:30:25 2019
From: |uc@r @end|ng |rom |edor@project@org (=?UTF-8?Q?I=C3=B1aki_Ucar?=)
Date: Sat, 5 Oct 2019 18:30:25 +0200
Subject: [Rd] should base R have a piping operator ?
In-Reply-To: <CAGpovd2jxxWBUxPhMAR6ZPw1+UQzK7BzJVtps10cMBSp=EUdHQ@mail.gmail.com>
References: <CAEKh8uidcEWA4tzmkpQxpF=q7r93qjf-HtiWLfbT91TFGOSzFQ@mail.gmail.com>
 <CAGpovd2jxxWBUxPhMAR6ZPw1+UQzK7BzJVtps10cMBSp=EUdHQ@mail.gmail.com>
Message-ID: <CALEXWq0ZvGeTbeq-AeYTSTcTwR-TR+gGczqQhWupzB2h0wa+zg@mail.gmail.com>

On Sat, 5 Oct 2019 at 17:15, Hugh Marera <hugh.marera at gmail.com> wrote:
>
> How is your argument different to, say,  "Should dplyr or data.table be
> part of base R as they are the most popular data science packages and they
> are used by a large number of users?"

Two packages with many features, dozens of functions and under heavy
development to fix bugs, add new features and improve performance, vs.
a single operator with a limited and well-defined functionality, and a
reference implementation that hasn't changed in years (but certainly
hackish in a way that probably could only be improved from R itself).

Can't you really spot the difference?

I?aki


From |uc@r @end|ng |rom |edor@project@org  Sat Oct  5 18:45:44 2019
From: |uc@r @end|ng |rom |edor@project@org (=?UTF-8?Q?I=C3=B1aki_Ucar?=)
Date: Sat, 5 Oct 2019 18:45:44 +0200
Subject: [Rd] should base R have a piping operator ?
In-Reply-To: <ce629286-825e-0315-4570-28c1c57dbfc0@sapo.pt>
References: <CAEKh8uidcEWA4tzmkpQxpF=q7r93qjf-HtiWLfbT91TFGOSzFQ@mail.gmail.com>
 <859F02FC-2DCF-4FFA-A941-1A7285B5F362@win-vector.com>
 <CAEKh8uj_NsmKQcCge1oi0qp3j2AgCV3krMhGiKghmySfKPL=YA@mail.gmail.com>
 <ce629286-825e-0315-4570-28c1c57dbfc0@sapo.pt>
Message-ID: <CALEXWq3UPJ7D4hj8A2z6HeByo56xq39H3EPAe2m1HFgs8cdo=A@mail.gmail.com>

On Sat, 5 Oct 2019 at 18:10, Rui Barradas <ruipbarradas at sapo.pt> wrote:
>
> R is a functional language, pipes are not.

How would you classify them? Pipes are analogous to function
composition. In that sense, they are more functional than classes, and
R does have classes.

Anyway, I don't see "purity" as a valid argument either in favour or
against any given feature. Language classification may be useful for
theorists, but certainly not for practitioners.

I?aki


From @nto|ne@|@br| @end|ng |rom gm@||@com  Sat Oct  5 19:14:32 2019
From: @nto|ne@|@br| @end|ng |rom gm@||@com (Ant F)
Date: Sat, 5 Oct 2019 19:14:32 +0200
Subject: [Rd] should base R have a piping operator ?
In-Reply-To: <ce629286-825e-0315-4570-28c1c57dbfc0@sapo.pt>
References: <CAEKh8uidcEWA4tzmkpQxpF=q7r93qjf-HtiWLfbT91TFGOSzFQ@mail.gmail.com>
 <859F02FC-2DCF-4FFA-A941-1A7285B5F362@win-vector.com>
 <CAEKh8uj_NsmKQcCge1oi0qp3j2AgCV3krMhGiKghmySfKPL=YA@mail.gmail.com>
 <ce629286-825e-0315-4570-28c1c57dbfc0@sapo.pt>
Message-ID: <CAEKh8uj3xG_Y5wV7a1jH15227H4JO=GNqgKc+382Xu+A28Ci-w@mail.gmail.com>

Hi Rui,

> R is a functional language, pipes are not. There are even higher order
functions such as [1] and [2].

If they can be build in R, either R is not a functional language, or pipes
are part of a functional language. Could you elaborate ?
What point would you make against pipes that you couldn't make about other
operators or functions leveraging lazy evaluation ?
I don't understand either the references to Negate() and Recall().

I actually think that there are few things more "functional" than using
pipes :).

> Besides, packages are part of R, R couldn't live without them. I find
> pipes a good idea but I also find it better not to have them as part of
> base R. If you want to use them, load a package, if you don't, don't.
> This simple.

Simple enough, but more complicated than necessary.
I believe it's a fair argument to point than pipes are different. They
don't DO anything and yet are used by thousands of packages.
They are largely viewed as part of R and some users are surprised no to
find them in base R, in the remaining users many don't
even know that they come from magrittr but think they come from dplyr.

The fact that it feels odd to attach a package only to use the pipe is
highlighted by the fact that thousands of packages reexport it
, see https://github.com/search?q=filename%3Autils-pipe.R+magrittr . I
don't think it can be said of any other function.

Package developers could still use `usethis::use_pipe()` if they want to
associate their package
with magrittr's pipe, but they wouldn't "need" it to suggest to user than
piping is recommend way of using their functions in sequence,
and this would mean less red ink saying that this `%>%` pipe was masked by
that other `%>%` pipe.

I like the design of magrittr's pipe a lot and I'd continue to use it
either way, but I would write documentation and even functions with `%.%`
if it was available,
without worrying about dependencies, and less worried about suboptimal
performance.

Best regards,

Antoine










Le sam. 5 oct. 2019 ? 18:03, Rui Barradas <ruipbarradas at sapo.pt> a ?crit :

> Hello,
>
> R is a functional language, pipes are not. There are even higher order
> functions such as [1] and [2].
> Besides, packages are part of R, R couldn't live without them. I find
> pipes a good idea but I also find it better not to have them as part of
> base R. If you want to use them, load a package, if you don't, don't.
> This simple.
>
> As for your example, compose, there is a StackOverflow question on it.
> See this answer [3].
>
> [1] https://stat.ethz.ch/R-manual/R-devel/library/base/html/funprog.html
> [2] https://stat.ethz.ch/R-manual/R-devel/library/base/html/Recall.html
> [3] https://stackoverflow.com/a/52465956/8245406
>
> Hope this helps,
>
> Rui Barradas
>
> ?s 16:48 de 05/10/19, Ant F escreveu:
> > Hi John,
> >
> > Thanks, but the Bizzaro pipe comes with many flaws though :
> > * It's not a single operator
> > * It has a different precedence
> > * It cannot be used in a subcall
> > * The variable assigned to must be on the right
> > * It doesn't trigger indentation when going to the line
> > * It creates/overwrite a `.` variable in the worksace.
> >
> > And it doesn't deal gracefully with some lazy evaluation edge cases such
> as
> > :
> >
> > compose <- function(f, g) { function(x) g(f(x)) }
> > plus1   <- function(x) x + 1
> >
> > plus2 <- plus1 %.% compose(.,plus1)
> > plus2(5)
> > #> [1] 7
> >
> > plus1 ->.; compose(.,plus1) -> .; . -> plus2
> > plus2(5)
> > #> Error: C stack usage  15923776 is too close to the limit
> >
> > What I propose on the other hand can always substitute any existing
> proper
> > pipe in their standard feature, as long as the dot is made explicit.
> >
> > Best regards,
> >
> > Antoine
> >
> >
> >
> > Le sam. 5 oct. 2019 ? 16:59, John Mount <jmount at win-vector.com> a ?crit
> :
> >
> >> Actually, base R already has a pipe fairly close to the one you
> describe:
> >> ->.;
> >>
> >> iris ->.; head(.) ->.; dim(.)
> >> # [1] 6 5
> >>
> >> I've called it the Bizarro pipe (
> >> http://www.win-vector.com/blog/2016/12/magrittrs-doppelganger/ ), and
> for
> >> some reason we chickened out and didn't spend time on it in the dot pipe
> >> paper (
> https://journal.r-project.org/archive/2018/RJ-2018-042/index.html
> >>   ).
> >>
> >> For documentation Bizarro pipe has the advantage that one can work out
> how
> >> it works from the application itself, with out reference to a defining
> >> function.
> >>
> >> On Oct 5, 2019, at 7:34 AM, Ant F <antoine.fabri at gmail.com> wrote:
> >>
> >> Dear R-devel,
> >>
> >> The most popular piping operator sits in the package `magrittr` and is
> used
> >> by a huge amount of users, and imported /reexported by more and more
> >> packages too.
> >>
> >> Many workflows don't even make much sense without pipes nowadays, so the
> >> examples in the doc will use pipes, as do the README, vignettes etc. I
> >> believe base R could have a piping operator so packages can use a pipe
> in
> >> their code or doc and stay dependency free.
> >>
> >> I don't suggest an operator based on complex heuristics, instead I
> suggest
> >> a very simple and fast one (>10 times than magrittr in my tests) :
> >>
> >> `%.%` <- function (e1, e2) {
> >>   eval(substitute(e2), envir = list(. = e1), enclos = parent.frame())
> >> }
> >>
> >> iris %.% head(.) %.% dim(.)
> >> #> [1] 6 5
> >>
> >> The difference with magrittr is that the dots must all be explicit
> (which
> >> sits with the choice of the name), and that special magrittr features
> such
> >> as assignment in place and building functions with `. %>% head() %>%
> dim()`
> >> are not supported.
> >>
> >> Edge cases are not surprising:
> >>
> >> ```
> >> x <- "a"
> >> x %.% quote(.)
> >> #> .
> >> x %.% substitute(.)
> >> #> [1] "a"
> >>
> >> f1 <- function(y) function() eval(quote(y))
> >> f2 <- x %.% f1(.)
> >> f2()
> >> #> [1] "a"
> >> ```
> >>
> >> Looking forward for your thoughts on this,
> >>
> >> Antoine
> >>
> >> [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-devel at r-project.org mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-devel
> >>
> >>
> >> ---------------
> >> John Mount
> >> http://www.win-vector.com/
> >> Our book: Practical Data Science with R
> >>
> https://www.manning.com/books/practical-data-science-with-r-second-edition
> >> <http://www.manning.com/zumel/>
> >>
> >>
> >>
> >>
> >>
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
> >
>

	[[alternative HTML version deleted]]


From hugh@m@rer@ @end|ng |rom gm@||@com  Sat Oct  5 19:53:43 2019
From: hugh@m@rer@ @end|ng |rom gm@||@com (Hugh Marera)
Date: Sat, 5 Oct 2019 19:53:43 +0200
Subject: [Rd] should base R have a piping operator ?
In-Reply-To: <CALEXWq0ZvGeTbeq-AeYTSTcTwR-TR+gGczqQhWupzB2h0wa+zg@mail.gmail.com>
References: <CAEKh8uidcEWA4tzmkpQxpF=q7r93qjf-HtiWLfbT91TFGOSzFQ@mail.gmail.com>
 <CAGpovd2jxxWBUxPhMAR6ZPw1+UQzK7BzJVtps10cMBSp=EUdHQ@mail.gmail.com>
 <CALEXWq0ZvGeTbeq-AeYTSTcTwR-TR+gGczqQhWupzB2h0wa+zg@mail.gmail.com>
Message-ID: <CAGpovd29V3rXSyB_SMBnbKfb3dzAFPF66ck9iU6XPB_+bS+8WQ@mail.gmail.com>

I exaggerated the comparison for effect. However, it is not very difficult
to find functions in dplyr or data.table or indeed other packages that one
may wish to be in base R. Examples, for me, could include
data.table::fread, dplyr::group_by & dplyr::summari[sZ]e combo, etc. Also,
the "popularity" of magrittr::`%>%` is mostly attributable to the tidyverse
(an advanced superset of R). Many R users don't even know that they are
installing the magrittr package.

On Sat, Oct 5, 2019 at 6:30 PM I?aki Ucar <iucar at fedoraproject.org> wrote:

> On Sat, 5 Oct 2019 at 17:15, Hugh Marera <hugh.marera at gmail.com> wrote:
> >
> > How is your argument different to, say,  "Should dplyr or data.table be
> > part of base R as they are the most popular data science packages and
> they
> > are used by a large number of users?"
>
> Two packages with many features, dozens of functions and under heavy
> development to fix bugs, add new features and improve performance, vs.
> a single operator with a limited and well-defined functionality, and a
> reference implementation that hasn't changed in years (but certainly
> hackish in a way that probably could only be improved from R itself).
>
> Can't you really spot the difference?
>
> I?aki
>

	[[alternative HTML version deleted]]


From |uc@r @end|ng |rom |edor@project@org  Sat Oct  5 20:08:48 2019
From: |uc@r @end|ng |rom |edor@project@org (=?UTF-8?Q?I=C3=B1aki_Ucar?=)
Date: Sat, 5 Oct 2019 20:08:48 +0200
Subject: [Rd] should base R have a piping operator ?
In-Reply-To: <CAGpovd29V3rXSyB_SMBnbKfb3dzAFPF66ck9iU6XPB_+bS+8WQ@mail.gmail.com>
References: <CAEKh8uidcEWA4tzmkpQxpF=q7r93qjf-HtiWLfbT91TFGOSzFQ@mail.gmail.com>
 <CAGpovd2jxxWBUxPhMAR6ZPw1+UQzK7BzJVtps10cMBSp=EUdHQ@mail.gmail.com>
 <CALEXWq0ZvGeTbeq-AeYTSTcTwR-TR+gGczqQhWupzB2h0wa+zg@mail.gmail.com>
 <CAGpovd29V3rXSyB_SMBnbKfb3dzAFPF66ck9iU6XPB_+bS+8WQ@mail.gmail.com>
Message-ID: <CALEXWq1riqkhSgiycKRSq4c2Q4hKCw_o-p2DZu4kfrwAsDO2+w@mail.gmail.com>

On Sat, 5 Oct 2019 at 19:54, Hugh Marera <hugh.marera at gmail.com> wrote:
>
> [...] it is not very difficult to find functions in dplyr or data.table or indeed other packages that one may wish to be in base R. Examples, for me, could include data.table::fread

You have utils::read.table and the like.

> dplyr::group_by & dplyr::summari[sZ]e combo

base::tapply, base::by, stats::aggregate.

> [...] Many R users don't even know that they are installing the magrittr package.

And that's one of the reasons why the proposal makes sense. Another
one is that the pipe plays well with many base R functions, such as
subset, transform, merge, aggregate and reshape.

I?aki


From @nto|ne@|@br| @end|ng |rom gm@||@com  Sat Oct  5 20:26:25 2019
From: @nto|ne@|@br| @end|ng |rom gm@||@com (Ant F)
Date: Sat, 5 Oct 2019 20:26:25 +0200
Subject: [Rd] should base R have a piping operator ?
In-Reply-To: <CAGpovd29V3rXSyB_SMBnbKfb3dzAFPF66ck9iU6XPB_+bS+8WQ@mail.gmail.com>
References: <CAEKh8uidcEWA4tzmkpQxpF=q7r93qjf-HtiWLfbT91TFGOSzFQ@mail.gmail.com>
 <CAGpovd2jxxWBUxPhMAR6ZPw1+UQzK7BzJVtps10cMBSp=EUdHQ@mail.gmail.com>
 <CALEXWq0ZvGeTbeq-AeYTSTcTwR-TR+gGczqQhWupzB2h0wa+zg@mail.gmail.com>
 <CAGpovd29V3rXSyB_SMBnbKfb3dzAFPF66ck9iU6XPB_+bS+8WQ@mail.gmail.com>
Message-ID: <CAEKh8ugr0FNryRVmM64pBPxjP=4xLrC0T2OsUD27mSASAyHpVQ@mail.gmail.com>

Yes but this exageration precisely misses the point.

Concerning your examples:

* I love fread but I think it makes a lot of subjective choices that are
best associated with a package. I think it
changed a lot with time and can still change, and we have great developers
willing to maintain it and be reactive
regarding feature requests or bug reports

*.group_by() adds a class that works only (or mostly) with tidyverse verbs,
that's very easy to dismiss it as an inclusion in base R.

* summarize is an alternative to aggregate, that would be very confusing to
have both

Now to be fair to your argument we could think of other functions such as
data.table::rleid() which I believe base R misses deeply,
and there is nothing wrong with packaged functions making their way to base
R.

Maybe there's an existing list of criteria for inclusion, in base R but if
not I can make one up for the sake of this discussion :) :
* 1) the functionality should not already exist
* 2) the function should be general enough
* 3) the function should have a large amount of potential of users
* 4) the function should be robust, and not require extensive maintenance
* 5) the function should be stable, we shouldn't expect new features ever 2
months
* 6) the function should have an intuitive interface in the context of the
rest ot base R

I guess 1 and 6 could be held against my proposal, because :
(1) everything can be done without pipes
(6) They are somewhat surprising (though with explicit dots not that much,
and not more surprising than say `bquote()`)

In my opinion the + offset the -.

I wouldn't advise taking magrittr's pipe (providing the license allows so)
for instance, because it makes a lot of design choices and has a complex
behavior, what I propose is 2 lines of code very unlikely to evolve or
require maintenance.

Antoine

PS: I just receive the digest once a day so If you don't "reply all" I can
only react later.

Le sam. 5 oct. 2019 ? 19:54, Hugh Marera <hugh.marera at gmail.com> a ?crit :

> I exaggerated the comparison for effect. However, it is not very difficult
> to find functions in dplyr or data.table or indeed other packages that one
> may wish to be in base R. Examples, for me, could include
> data.table::fread, dplyr::group_by & dplyr::summari[sZ]e combo, etc. Also,
> the "popularity" of magrittr::`%>%` is mostly attributable to the tidyverse
> (an advanced superset of R). Many R users don't even know that they are
> installing the magrittr package.
>
> On Sat, Oct 5, 2019 at 6:30 PM I?aki Ucar <iucar at fedoraproject.org> wrote:
>
>> On Sat, 5 Oct 2019 at 17:15, Hugh Marera <hugh.marera at gmail.com> wrote:
>> >
>> > How is your argument different to, say,  "Should dplyr or data.table be
>> > part of base R as they are the most popular data science packages and
>> they
>> > are used by a large number of users?"
>>
>> Two packages with many features, dozens of functions and under heavy
>> development to fix bugs, add new features and improve performance, vs.
>> a single operator with a limited and well-defined functionality, and a
>> reference implementation that hasn't changed in years (but certainly
>> hackish in a way that probably could only be improved from R itself).
>>
>> Can't you really spot the difference?
>>
>> I?aki
>>
>

	[[alternative HTML version deleted]]


From g@bembecker @end|ng |rom gm@||@com  Sun Oct  6 01:50:10 2019
From: g@bembecker @end|ng |rom gm@||@com (Gabriel Becker)
Date: Sun, 6 Oct 2019 10:50:10 +1100
Subject: [Rd] should base R have a piping operator ?
In-Reply-To: <CAEKh8ugr0FNryRVmM64pBPxjP=4xLrC0T2OsUD27mSASAyHpVQ@mail.gmail.com>
References: <CAEKh8uidcEWA4tzmkpQxpF=q7r93qjf-HtiWLfbT91TFGOSzFQ@mail.gmail.com>
 <CAGpovd2jxxWBUxPhMAR6ZPw1+UQzK7BzJVtps10cMBSp=EUdHQ@mail.gmail.com>
 <CALEXWq0ZvGeTbeq-AeYTSTcTwR-TR+gGczqQhWupzB2h0wa+zg@mail.gmail.com>
 <CAGpovd29V3rXSyB_SMBnbKfb3dzAFPF66ck9iU6XPB_+bS+8WQ@mail.gmail.com>
 <CAEKh8ugr0FNryRVmM64pBPxjP=4xLrC0T2OsUD27mSASAyHpVQ@mail.gmail.com>
Message-ID: <CAD4oTHHgSUr5TfLaTHxAM94E-Sqw9bW6gON7KdW324VEr524Lw@mail.gmail.com>

Hi all,

I think there's some nuance here that makes makes me agree partially with
each "side".

The pipe is inarguably extremely popular. Many probably think of it as a
core feature of R, along with the tidyverse that (as was pointed out)
largely surrounds it and drives its popularity. Whether its a good or bad
thing that they think that doesn't change the fact that by my estimation
that Ant is correct that they do. BUT, I don't agree with him that that, by
itself, is a reason to put it in base R in the form that it exists now. For
the current form, there aren't really any major downsides that I see to
having people just use the package version.

Sure it may be a little weird, but it doesn't ever really stop the
people from using it or present a significant barrier. Another major point
is that many (most?) base R functions are not necessarily tooled to be
endomorphic, which in my personal opinion is *largely* the only place that
the pipes are really compelling.

That was for pipes as the exist in package space, though. There is another
way the pipe could go into base R that could not be done in package space
and has the potential to mitigate some pretty serious downsides to the
pipes relating to debugging, which would be to implement them in the parser.

If

iris %>% group_by(Species) %>% summarize(mean_sl = mean(Sepal.Length)) %>%
filter(mean_sl > 5)


were *parsed* as, for example, into

local({
            . = group_by(iris, Species)

            ._tmp2 = summarize(., mean_sl = mean(Sepal.Length))

            filter(., mean_sl > 5)
       })




Then debuggiing (once you knew that) would be much easier but behavaior
would be the same as it is now. There could even be some sort of
step-through-pipe debugger at that point added as well for additional
convenience.

There is some minor precedent for that type of transformative parsing:

> expr = parse(text = "5 -> x")

> expr

expression(5 -> x)

> expr[[1]]

x <- 5


Though thats a much more minor transformation.

All of that said, I believe Jim Hester (cc'ed) suggested something along
these lines at the RSummit a couple of years ago, and thus far R-core has
not shown much appetite for changing things in the parser.

Without that changing, I'd have to say that my vote, for whatever its
worth, comes down on the side of pipes being fine in packages. A summary of
my reasoning being that it only makes sense for them to go into R itself if
doing so fixes an issue that cna't be fixed with them in package space.

Best,
~G



On Sun, Oct 6, 2019 at 5:26 AM Ant F <antoine.fabri at gmail.com> wrote:

> Yes but this exageration precisely misses the point.
>
> Concerning your examples:
>
> * I love fread but I think it makes a lot of subjective choices that are
> best associated with a package. I think it
> changed a lot with time and can still change, and we have great developers
> willing to maintain it and be reactive
> regarding feature requests or bug reports
>
> *.group_by() adds a class that works only (or mostly) with tidyverse verbs,
> that's very easy to dismiss it as an inclusion in base R.
>
> * summarize is an alternative to aggregate, that would be very confusing to
> have both
>
> Now to be fair to your argument we could think of other functions such as
> data.table::rleid() which I believe base R misses deeply,
> and there is nothing wrong with packaged functions making their way to base
> R.
>
> Maybe there's an existing list of criteria for inclusion, in base R but if
> not I can make one up for the sake of this discussion :) :
> * 1) the functionality should not already exist
> * 2) the function should be general enough
> * 3) the function should have a large amount of potential of users
> * 4) the function should be robust, and not require extensive maintenance
> * 5) the function should be stable, we shouldn't expect new features ever 2
> months
> * 6) the function should have an intuitive interface in the context of the
> rest ot base R
>
> I guess 1 and 6 could be held against my proposal, because :
> (1) everything can be done without pipes
> (6) They are somewhat surprising (though with explicit dots not that much,
> and not more surprising than say `bquote()`)
>
> In my opinion the + offset the -.
>
> I wouldn't advise taking magrittr's pipe (providing the license allows so)
> for instance, because it makes a lot of design choices and has a complex
> behavior, what I propose is 2 lines of code very unlikely to evolve or
> require maintenance.
>
> Antoine
>
> PS: I just receive the digest once a day so If you don't "reply all" I can
> only react later.
>
> Le sam. 5 oct. 2019 ? 19:54, Hugh Marera <hugh.marera at gmail.com> a ?crit :
>
> > I exaggerated the comparison for effect. However, it is not very
> difficult
> > to find functions in dplyr or data.table or indeed other packages that
> one
> > may wish to be in base R. Examples, for me, could include
> > data.table::fread, dplyr::group_by & dplyr::summari[sZ]e combo, etc.
> Also,
> > the "popularity" of magrittr::`%>%` is mostly attributable to the
> tidyverse
> > (an advanced superset of R). Many R users don't even know that they are
> > installing the magrittr package.
> >
> > On Sat, Oct 5, 2019 at 6:30 PM I?aki Ucar <iucar at fedoraproject.org>
> wrote:
> >
> >> On Sat, 5 Oct 2019 at 17:15, Hugh Marera <hugh.marera at gmail.com> wrote:
> >> >
> >> > How is your argument different to, say,  "Should dplyr or data.table
> be
> >> > part of base R as they are the most popular data science packages and
> >> they
> >> > are used by a large number of users?"
> >>
> >> Two packages with many features, dozens of functions and under heavy
> >> development to fix bugs, add new features and improve performance, vs.
> >> a single operator with a limited and well-defined functionality, and a
> >> reference implementation that hasn't changed in years (but certainly
> >> hackish in a way that probably could only be improved from R itself).
> >>
> >> Can't you really spot the difference?
> >>
> >> I?aki
> >>
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From jor|@mey@ @end|ng |rom gm@||@com  Sun Oct  6 10:13:53 2019
From: jor|@mey@ @end|ng |rom gm@||@com (Joris Meys)
Date: Sun, 6 Oct 2019 10:13:53 +0200
Subject: [Rd] should base R have a piping operator ?
In-Reply-To: <CAD4oTHHgSUr5TfLaTHxAM94E-Sqw9bW6gON7KdW324VEr524Lw@mail.gmail.com>
References: <CAEKh8uidcEWA4tzmkpQxpF=q7r93qjf-HtiWLfbT91TFGOSzFQ@mail.gmail.com>
 <CAGpovd2jxxWBUxPhMAR6ZPw1+UQzK7BzJVtps10cMBSp=EUdHQ@mail.gmail.com>
 <CALEXWq0ZvGeTbeq-AeYTSTcTwR-TR+gGczqQhWupzB2h0wa+zg@mail.gmail.com>
 <CAGpovd29V3rXSyB_SMBnbKfb3dzAFPF66ck9iU6XPB_+bS+8WQ@mail.gmail.com>
 <CAEKh8ugr0FNryRVmM64pBPxjP=4xLrC0T2OsUD27mSASAyHpVQ@mail.gmail.com>
 <CAD4oTHHgSUr5TfLaTHxAM94E-Sqw9bW6gON7KdW324VEr524Lw@mail.gmail.com>
Message-ID: <CAO1zAVZONKgBBcTJVBdZaJpVc6w90QndVsRwsfimVi7UKaDaxw@mail.gmail.com>

I'm largely with Gabriel Becker on this one: if pipes enter base R, they
should be a well thought out and integrated part of the language.

I do see merit though in providing a pipe in base R. Reason is mainly that
right now there's not a single pipe. A pipe function exists in different
packages, and it's not impossible that at one point piping operators might
behave slightly different depending on the package you load. So I hope
someone from RStudio is reading this thread and decides to do the heavy
lifting for R core. After all, it really is mainly their packages that
would benefit from it. I can't think of a non-tidyverse package that's
easier to use with pipes than without.

Best
Joris

On Sun, Oct 6, 2019 at 1:50 AM Gabriel Becker <gabembecker at gmail.com> wrote:

> Hi all,
>
> I think there's some nuance here that makes makes me agree partially with
> each "side".
>
> The pipe is inarguably extremely popular. Many probably think of it as a
> core feature of R, along with the tidyverse that (as was pointed out)
> largely surrounds it and drives its popularity. Whether its a good or bad
> thing that they think that doesn't change the fact that by my estimation
> that Ant is correct that they do. BUT, I don't agree with him that that, by
> itself, is a reason to put it in base R in the form that it exists now. For
> the current form, there aren't really any major downsides that I see to
> having people just use the package version.
>
> Sure it may be a little weird, but it doesn't ever really stop the
> people from using it or present a significant barrier. Another major point
> is that many (most?) base R functions are not necessarily tooled to be
> endomorphic, which in my personal opinion is *largely* the only place that
> the pipes are really compelling.
>
> That was for pipes as the exist in package space, though. There is another
> way the pipe could go into base R that could not be done in package space
> and has the potential to mitigate some pretty serious downsides to the
> pipes relating to debugging, which would be to implement them in the
> parser.
>
> If
>
> iris %>% group_by(Species) %>% summarize(mean_sl = mean(Sepal.Length)) %>%
> filter(mean_sl > 5)
>
>
> were *parsed* as, for example, into
>
> local({
>             . = group_by(iris, Species)
>
>             ._tmp2 = summarize(., mean_sl = mean(Sepal.Length))
>
>             filter(., mean_sl > 5)
>        })
>
>
>
>
> Then debuggiing (once you knew that) would be much easier but behavaior
> would be the same as it is now. There could even be some sort of
> step-through-pipe debugger at that point added as well for additional
> convenience.
>
> There is some minor precedent for that type of transformative parsing:
>
> > expr = parse(text = "5 -> x")
>
> > expr
>
> expression(5 -> x)
>
> > expr[[1]]
>
> x <- 5
>
>
> Though thats a much more minor transformation.
>
> All of that said, I believe Jim Hester (cc'ed) suggested something along
> these lines at the RSummit a couple of years ago, and thus far R-core has
> not shown much appetite for changing things in the parser.
>
> Without that changing, I'd have to say that my vote, for whatever its
> worth, comes down on the side of pipes being fine in packages. A summary of
> my reasoning being that it only makes sense for them to go into R itself if
> doing so fixes an issue that cna't be fixed with them in package space.
>
> Best,
> ~G
>
>
>
> On Sun, Oct 6, 2019 at 5:26 AM Ant F <antoine.fabri at gmail.com> wrote:
>
> > Yes but this exageration precisely misses the point.
> >
> > Concerning your examples:
> >
> > * I love fread but I think it makes a lot of subjective choices that are
> > best associated with a package. I think it
> > changed a lot with time and can still change, and we have great
> developers
> > willing to maintain it and be reactive
> > regarding feature requests or bug reports
> >
> > *.group_by() adds a class that works only (or mostly) with tidyverse
> verbs,
> > that's very easy to dismiss it as an inclusion in base R.
> >
> > * summarize is an alternative to aggregate, that would be very confusing
> to
> > have both
> >
> > Now to be fair to your argument we could think of other functions such as
> > data.table::rleid() which I believe base R misses deeply,
> > and there is nothing wrong with packaged functions making their way to
> base
> > R.
> >
> > Maybe there's an existing list of criteria for inclusion, in base R but
> if
> > not I can make one up for the sake of this discussion :) :
> > * 1) the functionality should not already exist
> > * 2) the function should be general enough
> > * 3) the function should have a large amount of potential of users
> > * 4) the function should be robust, and not require extensive maintenance
> > * 5) the function should be stable, we shouldn't expect new features
> ever 2
> > months
> > * 6) the function should have an intuitive interface in the context of
> the
> > rest ot base R
> >
> > I guess 1 and 6 could be held against my proposal, because :
> > (1) everything can be done without pipes
> > (6) They are somewhat surprising (though with explicit dots not that
> much,
> > and not more surprising than say `bquote()`)
> >
> > In my opinion the + offset the -.
> >
> > I wouldn't advise taking magrittr's pipe (providing the license allows
> so)
> > for instance, because it makes a lot of design choices and has a complex
> > behavior, what I propose is 2 lines of code very unlikely to evolve or
> > require maintenance.
> >
> > Antoine
> >
> > PS: I just receive the digest once a day so If you don't "reply all" I
> can
> > only react later.
> >
> > Le sam. 5 oct. 2019 ? 19:54, Hugh Marera <hugh.marera at gmail.com> a
> ?crit :
> >
> > > I exaggerated the comparison for effect. However, it is not very
> > difficult
> > > to find functions in dplyr or data.table or indeed other packages that
> > one
> > > may wish to be in base R. Examples, for me, could include
> > > data.table::fread, dplyr::group_by & dplyr::summari[sZ]e combo, etc.
> > Also,
> > > the "popularity" of magrittr::`%>%` is mostly attributable to the
> > tidyverse
> > > (an advanced superset of R). Many R users don't even know that they are
> > > installing the magrittr package.
> > >
> > > On Sat, Oct 5, 2019 at 6:30 PM I?aki Ucar <iucar at fedoraproject.org>
> > wrote:
> > >
> > >> On Sat, 5 Oct 2019 at 17:15, Hugh Marera <hugh.marera at gmail.com>
> wrote:
> > >> >
> > >> > How is your argument different to, say,  "Should dplyr or data.table
> > be
> > >> > part of base R as they are the most popular data science packages
> and
> > >> they
> > >> > are used by a large number of users?"
> > >>
> > >> Two packages with many features, dozens of functions and under heavy
> > >> development to fix bugs, add new features and improve performance, vs.
> > >> a single operator with a limited and well-defined functionality, and a
> > >> reference implementation that hasn't changed in years (but certainly
> > >> hackish in a way that probably could only be improved from R itself).
> > >>
> > >> Can't you really spot the difference?
> > >>
> > >> I?aki
> > >>
> > >
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


-- 
Joris Meys
Statistical consultant

Department of Data Analysis and Mathematical Modelling
Ghent University
Coupure Links 653, B-9000 Gent (Belgium)
<https://maps.google.com/?q=Coupure+links+653,%C2%A0B-9000+Gent,%C2%A0Belgium&entry=gmail&source=g>

-----------
Biowiskundedagen 2018-2019
http://www.biowiskundedagen.ugent.be/

-------------------------------
Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php

	[[alternative HTML version deleted]]


From |uc@r @end|ng |rom |edor@project@org  Sun Oct  6 11:33:57 2019
From: |uc@r @end|ng |rom |edor@project@org (=?UTF-8?Q?I=C3=B1aki_Ucar?=)
Date: Sun, 6 Oct 2019 11:33:57 +0200
Subject: [Rd] should base R have a piping operator ?
In-Reply-To: <CAO1zAVZONKgBBcTJVBdZaJpVc6w90QndVsRwsfimVi7UKaDaxw@mail.gmail.com>
References: <CAEKh8uidcEWA4tzmkpQxpF=q7r93qjf-HtiWLfbT91TFGOSzFQ@mail.gmail.com>
 <CAGpovd2jxxWBUxPhMAR6ZPw1+UQzK7BzJVtps10cMBSp=EUdHQ@mail.gmail.com>
 <CALEXWq0ZvGeTbeq-AeYTSTcTwR-TR+gGczqQhWupzB2h0wa+zg@mail.gmail.com>
 <CAGpovd29V3rXSyB_SMBnbKfb3dzAFPF66ck9iU6XPB_+bS+8WQ@mail.gmail.com>
 <CAEKh8ugr0FNryRVmM64pBPxjP=4xLrC0T2OsUD27mSASAyHpVQ@mail.gmail.com>
 <CAD4oTHHgSUr5TfLaTHxAM94E-Sqw9bW6gON7KdW324VEr524Lw@mail.gmail.com>
 <CAO1zAVZONKgBBcTJVBdZaJpVc6w90QndVsRwsfimVi7UKaDaxw@mail.gmail.com>
Message-ID: <CALEXWq0F7UuP72nFFp2f8Y1iO=RjRGzAOBgh1DfpXbiFo-O-AQ@mail.gmail.com>

On Sun, 6 Oct 2019 at 10:30, Joris Meys <jorismeys at gmail.com> wrote:
>
> I'm largely with Gabriel Becker on this one: if pipes enter base R, they
> should be a well thought out and integrated part of the language.
>
> I do see merit though in providing a pipe in base R. Reason is mainly that
> right now there's not a single pipe. A pipe function exists in different
> packages, and it's not impossible that at one point piping operators might
> behave slightly different depending on the package you load. So I hope
> someone from RStudio is reading this thread and decides to do the heavy
> lifting for R core. After all, it really is mainly their packages that
> would benefit from it.

Completely agree with Gabriel and Joris.

> I can't think of a non-tidyverse package that's
> easier to use with pipes than without.

I can give you one (disclaimer: it's one of my packages): simmer,
which is specifically designed to work with pipes, and has nothing to
do with the tidyverse.

I?aki


From suh@rto_@@ggo@o m@iii@g oii y@hoo@com  Sun Oct  6 13:55:54 2019
From: suh@rto_@@ggo@o m@iii@g oii y@hoo@com (suh@rto_@@ggo@o m@iii@g oii y@hoo@com)
Date: Sun, 6 Oct 2019 11:55:54 +0000 (UTC)
Subject: [Rd] Strange "no-echo" in place of "slave"
Message-ID: <2074176959.2758422.1570362954895@mail.yahoo.com>

SVN revision replaces "slave" with "no-echo" in R devel.


In each of the following, "no-echo" is rather strange to me.

- src/gnuwin32/README.Rterm
3) As a no-echo process for ESS mode in NTEmacs with flag --ess.

- src/library/grDevices/src/qdCocoa.m
/* the no-echo thread work until this is NO */


From g@bembecker @end|ng |rom gm@||@com  Sun Oct  6 14:00:16 2019
From: g@bembecker @end|ng |rom gm@||@com (Gabriel Becker)
Date: Sun, 6 Oct 2019 23:00:16 +1100
Subject: [Rd] Strange "no-echo" in place of "slave"
In-Reply-To: <2074176959.2758422.1570362954895@mail.yahoo.com>
References: <2074176959.2758422.1570362954895@mail.yahoo.com>
Message-ID: <CAD4oTHEo0KqtEiSNhzc9CeEjAmrt3ieHe3=Na2yMsUhMKK37UA@mail.gmail.com>

As far as I know, not being involved with the effort at all, they are
removing the term 'slave' and replacing it with 'no-echo' which is intended
to be fully synonmyous with the meaning of the old 'slave' term.

~G

On Sun, Oct 6, 2019 at 10:56 PM suharto_anggono--- via R-devel <
r-devel at r-project.org> wrote:

> SVN revision replaces "slave" with "no-echo" in R devel.
>
>
> In each of the following, "no-echo" is rather strange to me.
>
> - src/gnuwin32/README.Rterm
> 3) As a no-echo process for ESS mode in NTEmacs with flag --ess.
>
> - src/library/grDevices/src/qdCocoa.m
> /* the no-echo thread work until this is NO */
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From suh@rto_@@ggo@o m@iii@g oii y@hoo@com  Sun Oct  6 14:14:23 2019
From: suh@rto_@@ggo@o m@iii@g oii y@hoo@com (suh@rto_@@ggo@o m@iii@g oii y@hoo@com)
Date: Sun, 6 Oct 2019 12:14:23 +0000 (UTC)
Subject: [Rd] Wrong explanation on 'ylab' in hist.Rd
Message-ID: <36467073.2771795.1570364063916@mail.yahoo.com>

Description?of?arguments?main,?xlab,?ylab?in?hist.Rd?in?current?R?devel?and?R?patched?ends?with?this.
the?default?\code{ylab}?is?\code{"Frequency"}?iff?\code{probability}?is?true

In?fact,?if?'probability'?is?true,?the?histogram?doesn't?represent?frequencies.

It?should?be
the?default?\code{ylab}?is?\code{"Frequency"}?iff?\code{freq}?is?true


From pd@|gd @end|ng |rom gm@||@com  Sun Oct  6 16:13:58 2019
From: pd@|gd @end|ng |rom gm@||@com (peter dalgaard)
Date: Sun, 6 Oct 2019 16:13:58 +0200
Subject: [Rd] Strange "no-echo" in place of "slave"
In-Reply-To: <CAD4oTHEo0KqtEiSNhzc9CeEjAmrt3ieHe3=Na2yMsUhMKK37UA@mail.gmail.com>
References: <2074176959.2758422.1570362954895@mail.yahoo.com>
 <CAD4oTHEo0KqtEiSNhzc9CeEjAmrt3ieHe3=Na2yMsUhMKK37UA@mail.gmail.com>
Message-ID: <BE5D03DD-25FD-47C7-B92A-29CE99F2BD7A@gmail.com>

The first of Suharno's examples can be viewed that way, because R under ESS is not a "slave" in the technical sense, just a situation where you do not want keyboard input to be echoed. "Non-echoing" might have been better language though.

The 2nd example really is of the master/slave variety, and to my knowledge nothing to do with echoing, so looks like a search-and-replace oversight. Presumably, "worker thread" (or thereabouts) would be a better replacement.

-pd

> On 6 Oct 2019, at 14:00 , Gabriel Becker <gabembecker at gmail.com> wrote:
> 
> As far as I know, not being involved with the effort at all, they are
> removing the term 'slave' and replacing it with 'no-echo' which is intended
> to be fully synonmyous with the meaning of the old 'slave' term.
> 
> ~G
> 
> On Sun, Oct 6, 2019 at 10:56 PM suharto_anggono--- via R-devel <
> r-devel at r-project.org> wrote:
> 
>> SVN revision replaces "slave" with "no-echo" in R devel.
>> 
>> 
>> In each of the following, "no-echo" is rather strange to me.
>> 
>> - src/gnuwin32/README.Rterm
>> 3) As a no-echo process for ESS mode in NTEmacs with flag --ess.
>> 
>> - src/library/grDevices/src/qdCocoa.m
>> /* the no-echo thread work until this is NO */
>> 
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From jmount @end|ng |rom w|n-vector@com  Sun Oct  6 18:58:46 2019
From: jmount @end|ng |rom w|n-vector@com (John Mount)
Date: Sun, 6 Oct 2019 09:58:46 -0700
Subject: [Rd] should base R have a piping operator ?
In-Reply-To: <CAD4oTHHgSUr5TfLaTHxAM94E-Sqw9bW6gON7KdW324VEr524Lw@mail.gmail.com>
References: <CAEKh8uidcEWA4tzmkpQxpF=q7r93qjf-HtiWLfbT91TFGOSzFQ@mail.gmail.com>
 <CAGpovd2jxxWBUxPhMAR6ZPw1+UQzK7BzJVtps10cMBSp=EUdHQ@mail.gmail.com>
 <CALEXWq0ZvGeTbeq-AeYTSTcTwR-TR+gGczqQhWupzB2h0wa+zg@mail.gmail.com>
 <CAGpovd29V3rXSyB_SMBnbKfb3dzAFPF66ck9iU6XPB_+bS+8WQ@mail.gmail.com>
 <CAEKh8ugr0FNryRVmM64pBPxjP=4xLrC0T2OsUD27mSASAyHpVQ@mail.gmail.com>
 <CAD4oTHHgSUr5TfLaTHxAM94E-Sqw9bW6gON7KdW324VEr524Lw@mail.gmail.com>
Message-ID: <427982A4-477B-4A80-BE46-06A9FAFB32E2@win-vector.com>

Except for the isolation of local() R pretty much already has the parsing transformation you mention.


as.list(parse(text="

iris ->.; 
  group_by(., Species) ->.; 
  summarize(., mean_sl = mean(Sepal.Length)) ->.;
  filter(., mean_sl > 5)

"))

#> [[1]]
#> . <- iris
#> 
#> [[2]]
#> . <- group_by(., Species)
#> 
#> [[3]]
#> . <- summarize(., mean_sl = mean(Sepal.Length))
#> 
#> [[4]]
#> filter(., mean_sl > 5)


<sup>Created on 2019-10-06 by the [reprex package](https://reprex.tidyverse.org) (v0.3.0)</sup>

> On Oct 5, 2019, at 4:50 PM, Gabriel Becker <gabembecker at gmail.com> wrote:
> 
> 
> iris %>% group_by(Species) %>% summarize(mean_sl = mean(Sepal.Length)) %>%
> filter(mean_sl > 5)
> 
> 
> were *parsed* as, for example, into
> 
> local({
>            . = group_by(iris, Species)
> 
>            ._tmp2 = summarize(., mean_sl = mean(Sepal.Length))
> 
>            filter(., mean_sl > 5)
>       })
> 
> 
> 
> 
> Then debuggiing (once you knew that) would be much easier but behavaior
> would be the same as it is now. There could even be some sort of
> step-through-pipe debugger at that point added as well for additional
> convenience.
> 
> There is some minor precedent for that type of transformative parsing:
> 
>> expr = parse(text = "5 -> x")
> 
>> expr
> 
> expression(5 -> x)
> 
>> expr[[1]]
> 
> x <- 5
> 
> 
> Though thats a much more minor transformation.


From murdoch@dunc@n @end|ng |rom gm@||@com  Sun Oct  6 22:56:29 2019
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Sun, 6 Oct 2019 16:56:29 -0400
Subject: [Rd] should base R have a piping operator ?
In-Reply-To: <CAD4oTHHgSUr5TfLaTHxAM94E-Sqw9bW6gON7KdW324VEr524Lw@mail.gmail.com>
References: <CAEKh8uidcEWA4tzmkpQxpF=q7r93qjf-HtiWLfbT91TFGOSzFQ@mail.gmail.com>
 <CAGpovd2jxxWBUxPhMAR6ZPw1+UQzK7BzJVtps10cMBSp=EUdHQ@mail.gmail.com>
 <CALEXWq0ZvGeTbeq-AeYTSTcTwR-TR+gGczqQhWupzB2h0wa+zg@mail.gmail.com>
 <CAGpovd29V3rXSyB_SMBnbKfb3dzAFPF66ck9iU6XPB_+bS+8WQ@mail.gmail.com>
 <CAEKh8ugr0FNryRVmM64pBPxjP=4xLrC0T2OsUD27mSASAyHpVQ@mail.gmail.com>
 <CAD4oTHHgSUr5TfLaTHxAM94E-Sqw9bW6gON7KdW324VEr524Lw@mail.gmail.com>
Message-ID: <09db6cf4-c2db-e47f-9b0e-4ad14116609e@gmail.com>

On 05/10/2019 7:50 p.m., Gabriel Becker wrote:
> Hi all,
> 
> I think there's some nuance here that makes makes me agree partially with
> each "side".
> 
> The pipe is inarguably extremely popular. Many probably think of it as a
> core feature of R, along with the tidyverse that (as was pointed out)
> largely surrounds it and drives its popularity. Whether its a good or bad
> thing that they think that doesn't change the fact that by my estimation
> that Ant is correct that they do. BUT, I don't agree with him that that, by
> itself, is a reason to put it in base R in the form that it exists now. For
> the current form, there aren't really any major downsides that I see to
> having people just use the package version.
> 
> Sure it may be a little weird, but it doesn't ever really stop the
> people from using it or present a significant barrier. Another major point
> is that many (most?) base R functions are not necessarily tooled to be
> endomorphic, which in my personal opinion is *largely* the only place that
> the pipes are really compelling.
> 
> That was for pipes as the exist in package space, though. There is another
> way the pipe could go into base R that could not be done in package space
> and has the potential to mitigate some pretty serious downsides to the
> pipes relating to debugging, which would be to implement them in the parser.

Actually, that could be done in package space too:  just write a 
function to do the transformation.  That is, something like

    transformPipe( a %>% b %>% c )

could convert the original expression into one like yours below.  This 
could be done by a smart IDE like RStudio without the user typing anything.

A really strong argument for doing this in a package instead of Bison/C 
code in the parser is the help page ?magrittr::"%>%".  There are so many 
special cases there that it's certainly hard and possibly impossible for 
the parser to do the transformation:  I think some parts of the 
transformation depend on run-time values, not syntax.

Of course, a simpler operator like Antoine's would be easier, but that 
would break code that uses magrittr pipes, and I think those are the 
most commonly accepted ones.

So a workable plan would be for all the pipe authors to agree on syntax 
for transformPipe(), and then for IDE authors to support it.  R Core 
doesn't need to be involved at all unless they want to update Rgui or 
R.app or command line R.

Duncan Murdoch

> 
> If
> 
> iris %>% group_by(Species) %>% summarize(mean_sl = mean(Sepal.Length)) %>%
> filter(mean_sl > 5)
> 
> 
> were *parsed* as, for example, into
> 
> local({
>              . = group_by(iris, Species)
> 
>              ._tmp2 = summarize(., mean_sl = mean(Sepal.Length))
> 
>              filter(., mean_sl > 5)
>         })
> 
> 
> 
> 
> Then debuggiing (once you knew that) would be much easier but behavaior
> would be the same as it is now. There could even be some sort of
> step-through-pipe debugger at that point added as well for additional
> convenience.
> 
> There is some minor precedent for that type of transformative parsing:
> 
>> expr = parse(text = "5 -> x")
> 
>> expr
> 
> expression(5 -> x)
> 
>> expr[[1]]
> 
> x <- 5
> 
> 
> Though thats a much more minor transformation.
> 
> All of that said, I believe Jim Hester (cc'ed) suggested something along
> these lines at the RSummit a couple of years ago, and thus far R-core has
> not shown much appetite for changing things in the parser.
> 
> Without that changing, I'd have to say that my vote, for whatever its
> worth, comes down on the side of pipes being fine in packages. A summary of
> my reasoning being that it only makes sense for them to go into R itself if
> doing so fixes an issue that cna't be fixed with them in package space.
> 
> Best,
> ~G
> 
> 
> 
> On Sun, Oct 6, 2019 at 5:26 AM Ant F <antoine.fabri at gmail.com> wrote:
> 
>> Yes but this exageration precisely misses the point.
>>
>> Concerning your examples:
>>
>> * I love fread but I think it makes a lot of subjective choices that are
>> best associated with a package. I think it
>> changed a lot with time and can still change, and we have great developers
>> willing to maintain it and be reactive
>> regarding feature requests or bug reports
>>
>> *.group_by() adds a class that works only (or mostly) with tidyverse verbs,
>> that's very easy to dismiss it as an inclusion in base R.
>>
>> * summarize is an alternative to aggregate, that would be very confusing to
>> have both
>>
>> Now to be fair to your argument we could think of other functions such as
>> data.table::rleid() which I believe base R misses deeply,
>> and there is nothing wrong with packaged functions making their way to base
>> R.
>>
>> Maybe there's an existing list of criteria for inclusion, in base R but if
>> not I can make one up for the sake of this discussion :) :
>> * 1) the functionality should not already exist
>> * 2) the function should be general enough
>> * 3) the function should have a large amount of potential of users
>> * 4) the function should be robust, and not require extensive maintenance
>> * 5) the function should be stable, we shouldn't expect new features ever 2
>> months
>> * 6) the function should have an intuitive interface in the context of the
>> rest ot base R
>>
>> I guess 1 and 6 could be held against my proposal, because :
>> (1) everything can be done without pipes
>> (6) They are somewhat surprising (though with explicit dots not that much,
>> and not more surprising than say `bquote()`)
>>
>> In my opinion the + offset the -.
>>
>> I wouldn't advise taking magrittr's pipe (providing the license allows so)
>> for instance, because it makes a lot of design choices and has a complex
>> behavior, what I propose is 2 lines of code very unlikely to evolve or
>> require maintenance.
>>
>> Antoine
>>
>> PS: I just receive the digest once a day so If you don't "reply all" I can
>> only react later.
>>
>> Le sam. 5 oct. 2019 ? 19:54, Hugh Marera <hugh.marera at gmail.com> a ?crit :
>>
>>> I exaggerated the comparison for effect. However, it is not very
>> difficult
>>> to find functions in dplyr or data.table or indeed other packages that
>> one
>>> may wish to be in base R. Examples, for me, could include
>>> data.table::fread, dplyr::group_by & dplyr::summari[sZ]e combo, etc.
>> Also,
>>> the "popularity" of magrittr::`%>%` is mostly attributable to the
>> tidyverse
>>> (an advanced superset of R). Many R users don't even know that they are
>>> installing the magrittr package.
>>>
>>> On Sat, Oct 5, 2019 at 6:30 PM I?aki Ucar <iucar at fedoraproject.org>
>> wrote:
>>>
>>>> On Sat, 5 Oct 2019 at 17:15, Hugh Marera <hugh.marera at gmail.com> wrote:
>>>>>
>>>>> How is your argument different to, say,  "Should dplyr or data.table
>> be
>>>>> part of base R as they are the most popular data science packages and
>>>> they
>>>>> are used by a large number of users?"
>>>>
>>>> Two packages with many features, dozens of functions and under heavy
>>>> development to fix bugs, add new features and improve performance, vs.
>>>> a single operator with a limited and well-defined functionality, and a
>>>> reference implementation that hasn't changed in years (but certainly
>>>> hackish in a way that probably could only be improved from R itself).
>>>>
>>>> Can't you really spot the difference?
>>>>
>>>> I?aki
>>>>
>>>
>>
>>          [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From @nto|ne@|@br| @end|ng |rom gm@||@com  Mon Oct  7 00:31:18 2019
From: @nto|ne@|@br| @end|ng |rom gm@||@com (Ant F)
Date: Mon, 7 Oct 2019 00:31:18 +0200
Subject: [Rd] should base R have a piping operator ?
In-Reply-To: <09db6cf4-c2db-e47f-9b0e-4ad14116609e@gmail.com>
References: <CAEKh8uidcEWA4tzmkpQxpF=q7r93qjf-HtiWLfbT91TFGOSzFQ@mail.gmail.com>
 <CAGpovd2jxxWBUxPhMAR6ZPw1+UQzK7BzJVtps10cMBSp=EUdHQ@mail.gmail.com>
 <CALEXWq0ZvGeTbeq-AeYTSTcTwR-TR+gGczqQhWupzB2h0wa+zg@mail.gmail.com>
 <CAGpovd29V3rXSyB_SMBnbKfb3dzAFPF66ck9iU6XPB_+bS+8WQ@mail.gmail.com>
 <CAEKh8ugr0FNryRVmM64pBPxjP=4xLrC0T2OsUD27mSASAyHpVQ@mail.gmail.com>
 <CAD4oTHHgSUr5TfLaTHxAM94E-Sqw9bW6gON7KdW324VEr524Lw@mail.gmail.com>
 <09db6cf4-c2db-e47f-9b0e-4ad14116609e@gmail.com>
Message-ID: <CAEKh8ugCPFppQrn4Z1SVDnJ78epquxkE5_jniCPtJOB-ST3E6A@mail.gmail.com>

As a matter of fact I played a few days ago with this idea of transforming
the pipe chain to a sequence of calls such as the one Gabriel proposed.

My proposed debugging method was to use a debugging pipe

calling iris %>% head %B>% dim %>% length will open place you right at the
browser call below :

#> Called from: (function (.) #> {#>     on.exit(rm(.))#>     . <-
head(.)#>     browser()#>     . <- dim(.)#>     . <- length(.)#>
.#> })(iris)

https://github.com/moodymudskipper/pipe/blob/master/README.md

Regarding breaking code, it would only if the pipe if named the same.

To be clear, I like the fact that magrittr exists as an external package
and that it can evolve with the thought and input of the tidyverse crew and
I wouldn't want a base pipe to replace it.
I think package developers would code and document using the base pipe
(unless they have a strong preference for a packaged pipe), and that users
would use interactively the pipe they prefer, which is usually magrittr's
pipe among current choices.

Thanks all for the good points,

Antoine



Le dim. 6 oct. 2019 ? 22:56, Duncan Murdoch <murdoch.duncan at gmail.com> a
?crit :

> On 05/10/2019 7:50 p.m., Gabriel Becker wrote:
> > Hi all,
> >
> > I think there's some nuance here that makes makes me agree partially with
> > each "side".
> >
> > The pipe is inarguably extremely popular. Many probably think of it as a
> > core feature of R, along with the tidyverse that (as was pointed out)
> > largely surrounds it and drives its popularity. Whether its a good or bad
> > thing that they think that doesn't change the fact that by my estimation
> > that Ant is correct that they do. BUT, I don't agree with him that that,
> by
> > itself, is a reason to put it in base R in the form that it exists now.
> For
> > the current form, there aren't really any major downsides that I see to
> > having people just use the package version.
> >
> > Sure it may be a little weird, but it doesn't ever really stop the
> > people from using it or present a significant barrier. Another major
> point
> > is that many (most?) base R functions are not necessarily tooled to be
> > endomorphic, which in my personal opinion is *largely* the only place
> that
> > the pipes are really compelling.
> >
> > That was for pipes as the exist in package space, though. There is
> another
> > way the pipe could go into base R that could not be done in package space
> > and has the potential to mitigate some pretty serious downsides to the
> > pipes relating to debugging, which would be to implement them in the
> parser.
>
> Actually, that could be done in package space too:  just write a
> function to do the transformation.  That is, something like
>
>     transformPipe( a %>% b %>% c )
>
> could convert the original expression into one like yours below.  This
> could be done by a smart IDE like RStudio without the user typing anything.
>
> A really strong argument for doing this in a package instead of Bison/C
> code in the parser is the help page ?magrittr::"%>%".  There are so many
> special cases there that it's certainly hard and possibly impossible for
> the parser to do the transformation:  I think some parts of the
> transformation depend on run-time values, not syntax.
>
> Of course, a simpler operator like Antoine's would be easier, but that
> would break code that uses magrittr pipes, and I think those are the
> most commonly accepted ones.
>
> So a workable plan would be for all the pipe authors to agree on syntax
> for transformPipe(), and then for IDE authors to support it.  R Core
> doesn't need to be involved at all unless they want to update Rgui or
> R.app or command line R.
>
> Duncan Murdoch
>
> >
> > If
> >
> > iris %>% group_by(Species) %>% summarize(mean_sl = mean(Sepal.Length))
> %>%
> > filter(mean_sl > 5)
> >
> >
> > were *parsed* as, for example, into
> >
> > local({
> >              . = group_by(iris, Species)
> >
> >              ._tmp2 = summarize(., mean_sl = mean(Sepal.Length))
> >
> >              filter(., mean_sl > 5)
> >         })
> >
> >
> >
> >
> > Then debuggiing (once you knew that) would be much easier but behavaior
> > would be the same as it is now. There could even be some sort of
> > step-through-pipe debugger at that point added as well for additional
> > convenience.
> >
> > There is some minor precedent for that type of transformative parsing:
> >
> >> expr = parse(text = "5 -> x")
> >
> >> expr
> >
> > expression(5 -> x)
> >
> >> expr[[1]]
> >
> > x <- 5
> >
> >
> > Though thats a much more minor transformation.
> >
> > All of that said, I believe Jim Hester (cc'ed) suggested something along
> > these lines at the RSummit a couple of years ago, and thus far R-core has
> > not shown much appetite for changing things in the parser.
> >
> > Without that changing, I'd have to say that my vote, for whatever its
> > worth, comes down on the side of pipes being fine in packages. A summary
> of
> > my reasoning being that it only makes sense for them to go into R itself
> if
> > doing so fixes an issue that cna't be fixed with them in package space.
> >
> > Best,
> > ~G
> >
> >
> >
> > On Sun, Oct 6, 2019 at 5:26 AM Ant F <antoine.fabri at gmail.com> wrote:
> >
> >> Yes but this exageration precisely misses the point.
> >>
> >> Concerning your examples:
> >>
> >> * I love fread but I think it makes a lot of subjective choices that are
> >> best associated with a package. I think it
> >> changed a lot with time and can still change, and we have great
> developers
> >> willing to maintain it and be reactive
> >> regarding feature requests or bug reports
> >>
> >> *.group_by() adds a class that works only (or mostly) with tidyverse
> verbs,
> >> that's very easy to dismiss it as an inclusion in base R.
> >>
> >> * summarize is an alternative to aggregate, that would be very
> confusing to
> >> have both
> >>
> >> Now to be fair to your argument we could think of other functions such
> as
> >> data.table::rleid() which I believe base R misses deeply,
> >> and there is nothing wrong with packaged functions making their way to
> base
> >> R.
> >>
> >> Maybe there's an existing list of criteria for inclusion, in base R but
> if
> >> not I can make one up for the sake of this discussion :) :
> >> * 1) the functionality should not already exist
> >> * 2) the function should be general enough
> >> * 3) the function should have a large amount of potential of users
> >> * 4) the function should be robust, and not require extensive
> maintenance
> >> * 5) the function should be stable, we shouldn't expect new features
> ever 2
> >> months
> >> * 6) the function should have an intuitive interface in the context of
> the
> >> rest ot base R
> >>
> >> I guess 1 and 6 could be held against my proposal, because :
> >> (1) everything can be done without pipes
> >> (6) They are somewhat surprising (though with explicit dots not that
> much,
> >> and not more surprising than say `bquote()`)
> >>
> >> In my opinion the + offset the -.
> >>
> >> I wouldn't advise taking magrittr's pipe (providing the license allows
> so)
> >> for instance, because it makes a lot of design choices and has a complex
> >> behavior, what I propose is 2 lines of code very unlikely to evolve or
> >> require maintenance.
> >>
> >> Antoine
> >>
> >> PS: I just receive the digest once a day so If you don't "reply all" I
> can
> >> only react later.
> >>
> >> Le sam. 5 oct. 2019 ? 19:54, Hugh Marera <hugh.marera at gmail.com> a
> ?crit :
> >>
> >>> I exaggerated the comparison for effect. However, it is not very
> >> difficult
> >>> to find functions in dplyr or data.table or indeed other packages that
> >> one
> >>> may wish to be in base R. Examples, for me, could include
> >>> data.table::fread, dplyr::group_by & dplyr::summari[sZ]e combo, etc.
> >> Also,
> >>> the "popularity" of magrittr::`%>%` is mostly attributable to the
> >> tidyverse
> >>> (an advanced superset of R). Many R users don't even know that they are
> >>> installing the magrittr package.
> >>>
> >>> On Sat, Oct 5, 2019 at 6:30 PM I?aki Ucar <iucar at fedoraproject.org>
> >> wrote:
> >>>
> >>>> On Sat, 5 Oct 2019 at 17:15, Hugh Marera <hugh.marera at gmail.com>
> wrote:
> >>>>>
> >>>>> How is your argument different to, say,  "Should dplyr or data.table
> >> be
> >>>>> part of base R as they are the most popular data science packages and
> >>>> they
> >>>>> are used by a large number of users?"
> >>>>
> >>>> Two packages with many features, dozens of functions and under heavy
> >>>> development to fix bugs, add new features and improve performance, vs.
> >>>> a single operator with a limited and well-defined functionality, and a
> >>>> reference implementation that hasn't changed in years (but certainly
> >>>> hackish in a way that probably could only be improved from R itself).
> >>>>
> >>>> Can't you really spot the difference?
> >>>>
> >>>> I?aki
> >>>>
> >>>
> >>
> >>          [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-devel at r-project.org mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-devel
> >>
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
> >
>
>

	[[alternative HTML version deleted]]


From g@br|e|@guzm@n@25u @end|ng |rom gm@||@com  Mon Oct  7 06:43:02 2019
From: g@br|e|@guzm@n@25u @end|ng |rom gm@||@com (Gabriel Guzman)
Date: Mon, 7 Oct 2019 00:43:02 -0400
Subject: [Rd] Changing HTML Error code message
Message-ID: <CAN7Zm=FVijKOU_3MJD5dPxuipH+tyiSQP2Y=NGrO5yco3gbDdg@mail.gmail.com>

Hello,

I'm relatively new to using R and shiny. Currently, I'm getting the Error:
Conflict (HTTP  409) when trying to access an html file from dropbox and
this is fine, I know the reason. What I do have a problem with is trying to
find a way to change Error code message. Instead of simply "Error: Conflict
(HTTP  409)", I would a message a client might be able to understand. I've
tried a couple forms of validation and try-catches. I'm including the code
and simple html files to recreate the problem. Any and all suggestions are
welcome. Thank you in advance for your help.



library(shiny)
library(rdrop2)
library(httr)

ui <- # Define UI for dataset viewer application

  shinyUI(pageWithSidebar(

    headerPanel("Test DropBox html Docs to Shiny"),

    sidebarPanel(

      selectInput("Cat", "Choose a Category:",

                  choices = c("A", "B", "C")),

      selectInput("Year", "Choose a Year:",

                  choices = c("2012", "2011")),

      downloadButton("downFile", "Download File"),

      width = 2),


    mainPanel(

      tabsetPanel(type = "tabs",

                  tabPanel("Html Pages", htmlOutput("viewReport"))), width
= 10)
    )
  )

#IMPORTANT: The two lines below needs to be run just one time unless the
token is deleted

# Create Token

# token <- drop_auth()

# Save token

# saveRDS(token, "droptoken.rds")

token <- readRDS("droptoken.rds")


server <- shinyServer(function(input, output) {


# ---------------------------------------------------
  filePutReport <- reactive(

    paste(input$Cat, "_", input$Year, "_Doc.html", sep = "")

  )

  filePutReport2 <- reactive({
    # Search if the file exists in DropBox


    drop_download(path = paste("shiny_docs/shinydbtest/", filePutReport(),
sep = ""),
                  overwrite = TRUE, local_path = "./www",
             dtoken = token)

      filePutReport()

  })

  # Show Html Pages

  output$viewReport <- renderUI({

    tags$iframe(seamless = "seamless", width = "1400", height = "1000",

                src = filePutReport2()
                )
  })


  ###
  output$downFile <- downloadHandler(
    # generate bins based on input$bins from ui.R
    filename = function() {

      paste0(filePutReport() )
    },

    content = function(file){

      file.copy(from = paste0("./www/", filePutReport2() ), to = file,
overwrite = TRUE)
    }

  )

})

shinyApp(ui = ui, server = server)

From ||one| @end|ng |rom r@tud|o@com  Mon Oct  7 10:22:30 2019
From: ||one| @end|ng |rom r@tud|o@com (Lionel Henry)
Date: Mon, 7 Oct 2019 10:22:30 +0200
Subject: [Rd] should base R have a piping operator ?
In-Reply-To: <CAD4oTHHgSUr5TfLaTHxAM94E-Sqw9bW6gON7KdW324VEr524Lw@mail.gmail.com>
References: <CAEKh8uidcEWA4tzmkpQxpF=q7r93qjf-HtiWLfbT91TFGOSzFQ@mail.gmail.com>
 <CAGpovd2jxxWBUxPhMAR6ZPw1+UQzK7BzJVtps10cMBSp=EUdHQ@mail.gmail.com>
 <CALEXWq0ZvGeTbeq-AeYTSTcTwR-TR+gGczqQhWupzB2h0wa+zg@mail.gmail.com>
 <CAGpovd29V3rXSyB_SMBnbKfb3dzAFPF66ck9iU6XPB_+bS+8WQ@mail.gmail.com>
 <CAEKh8ugr0FNryRVmM64pBPxjP=4xLrC0T2OsUD27mSASAyHpVQ@mail.gmail.com>
 <CAD4oTHHgSUr5TfLaTHxAM94E-Sqw9bW6gON7KdW324VEr524Lw@mail.gmail.com>
Message-ID: <95648642-FCC6-49BE-93D9-8872172946E6@rstudio.com>

Hi Gabe,

> There is another way the pipe could go into base R that could not be
> done in package space and has the potential to mitigate some pretty
> serious downsides to the pipes relating to debugging

I assume you're thinking about the large stack trace of the magrittr
pipe? You don't need a parser transformation to solve this problem
though, the pipe could be implemented as a regular function with a
very limited impact on the stack. And if implemented as a SPECIALSXP,
it would be completely invisible. We've been planning to rewrite %>%
to fix the performance and the stack print, it's just low priority.

About the semantics of local evaluation that were proposed in this
thread, I think that wouldn't be right. A native pipe should be
consistent with other control flow constructs like `if` and `for` and
evaluate in the current environment. In that case, the `.` binding, if
any, would be restored to its original value in `on.exit()` (or through
unwind-protection if implemented in C).

Best,
Lionel


> On 6 Oct 2019, at 01:50, Gabriel Becker <gabembecker at gmail.com> wrote:
> 
> Hi all,
> 
> I think there's some nuance here that makes makes me agree partially with
> each "side".
> 
> The pipe is inarguably extremely popular. Many probably think of it as a
> core feature of R, along with the tidyverse that (as was pointed out)
> largely surrounds it and drives its popularity. Whether its a good or bad
> thing that they think that doesn't change the fact that by my estimation
> that Ant is correct that they do. BUT, I don't agree with him that that, by
> itself, is a reason to put it in base R in the form that it exists now. For
> the current form, there aren't really any major downsides that I see to
> having people just use the package version.
> 
> Sure it may be a little weird, but it doesn't ever really stop the
> people from using it or present a significant barrier. Another major point
> is that many (most?) base R functions are not necessarily tooled to be
> endomorphic, which in my personal opinion is *largely* the only place that
> the pipes are really compelling.
> 
> That was for pipes as the exist in package space, though. There is another
> way the pipe could go into base R that could not be done in package space
> and has the potential to mitigate some pretty serious downsides to the
> pipes relating to debugging, which would be to implement them in the parser.
> 
> If
> 
> iris %>% group_by(Species) %>% summarize(mean_sl = mean(Sepal.Length)) %>%
> filter(mean_sl > 5)
> 
> 
> were *parsed* as, for example, into
> 
> local({
>            . = group_by(iris, Species)
> 
>            ._tmp2 = summarize(., mean_sl = mean(Sepal.Length))
> 
>            filter(., mean_sl > 5)
>       })
> 
> 
> 
> 
> Then debuggiing (once you knew that) would be much easier but behavaior
> would be the same as it is now. There could even be some sort of
> step-through-pipe debugger at that point added as well for additional
> convenience.
> 
> There is some minor precedent for that type of transformative parsing:
> 
>> expr = parse(text = "5 -> x")
> 
>> expr
> 
> expression(5 -> x)
> 
>> expr[[1]]
> 
> x <- 5
> 
> 
> Though thats a much more minor transformation.
> 
> All of that said, I believe Jim Hester (cc'ed) suggested something along
> these lines at the RSummit a couple of years ago, and thus far R-core has
> not shown much appetite for changing things in the parser.
> 
> Without that changing, I'd have to say that my vote, for whatever its
> worth, comes down on the side of pipes being fine in packages. A summary of
> my reasoning being that it only makes sense for them to go into R itself if
> doing so fixes an issue that cna't be fixed with them in package space.
> 
> Best,
> ~G
> 
> 
> 
> On Sun, Oct 6, 2019 at 5:26 AM Ant F <antoine.fabri at gmail.com> wrote:
> 
>> Yes but this exageration precisely misses the point.
>> 
>> Concerning your examples:
>> 
>> * I love fread but I think it makes a lot of subjective choices that are
>> best associated with a package. I think it
>> changed a lot with time and can still change, and we have great developers
>> willing to maintain it and be reactive
>> regarding feature requests or bug reports
>> 
>> *.group_by() adds a class that works only (or mostly) with tidyverse verbs,
>> that's very easy to dismiss it as an inclusion in base R.
>> 
>> * summarize is an alternative to aggregate, that would be very confusing to
>> have both
>> 
>> Now to be fair to your argument we could think of other functions such as
>> data.table::rleid() which I believe base R misses deeply,
>> and there is nothing wrong with packaged functions making their way to base
>> R.
>> 
>> Maybe there's an existing list of criteria for inclusion, in base R but if
>> not I can make one up for the sake of this discussion :) :
>> * 1) the functionality should not already exist
>> * 2) the function should be general enough
>> * 3) the function should have a large amount of potential of users
>> * 4) the function should be robust, and not require extensive maintenance
>> * 5) the function should be stable, we shouldn't expect new features ever 2
>> months
>> * 6) the function should have an intuitive interface in the context of the
>> rest ot base R
>> 
>> I guess 1 and 6 could be held against my proposal, because :
>> (1) everything can be done without pipes
>> (6) They are somewhat surprising (though with explicit dots not that much,
>> and not more surprising than say `bquote()`)
>> 
>> In my opinion the + offset the -.
>> 
>> I wouldn't advise taking magrittr's pipe (providing the license allows so)
>> for instance, because it makes a lot of design choices and has a complex
>> behavior, what I propose is 2 lines of code very unlikely to evolve or
>> require maintenance.
>> 
>> Antoine
>> 
>> PS: I just receive the digest once a day so If you don't "reply all" I can
>> only react later.
>> 
>> Le sam. 5 oct. 2019 ? 19:54, Hugh Marera <hugh.marera at gmail.com> a ?crit :
>> 
>>> I exaggerated the comparison for effect. However, it is not very
>> difficult
>>> to find functions in dplyr or data.table or indeed other packages that
>> one
>>> may wish to be in base R. Examples, for me, could include
>>> data.table::fread, dplyr::group_by & dplyr::summari[sZ]e combo, etc.
>> Also,
>>> the "popularity" of magrittr::`%>%` is mostly attributable to the
>> tidyverse
>>> (an advanced superset of R). Many R users don't even know that they are
>>> installing the magrittr package.
>>> 
>>> On Sat, Oct 5, 2019 at 6:30 PM I?aki Ucar <iucar at fedoraproject.org>
>> wrote:
>>> 
>>>> On Sat, 5 Oct 2019 at 17:15, Hugh Marera <hugh.marera at gmail.com> wrote:
>>>>> 
>>>>> How is your argument different to, say,  "Should dplyr or data.table
>> be
>>>>> part of base R as they are the most popular data science packages and
>>>> they
>>>>> are used by a large number of users?"
>>>> 
>>>> Two packages with many features, dozens of functions and under heavy
>>>> development to fix bugs, add new features and improve performance, vs.
>>>> a single operator with a limited and well-defined functionality, and a
>>>> reference implementation that hasn't changed in years (but certainly
>>>> hackish in a way that probably could only be improved from R itself).
>>>> 
>>>> Can't you really spot the difference?
>>>> 
>>>> I?aki
>>>> 
>>> 
>> 
>>        [[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From murdoch@dunc@n @end|ng |rom gm@||@com  Mon Oct  7 13:47:41 2019
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Mon, 7 Oct 2019 07:47:41 -0400
Subject: [Rd] should base R have a piping operator ?
In-Reply-To: <95648642-FCC6-49BE-93D9-8872172946E6@rstudio.com>
References: <CAEKh8uidcEWA4tzmkpQxpF=q7r93qjf-HtiWLfbT91TFGOSzFQ@mail.gmail.com>
 <CAGpovd2jxxWBUxPhMAR6ZPw1+UQzK7BzJVtps10cMBSp=EUdHQ@mail.gmail.com>
 <CALEXWq0ZvGeTbeq-AeYTSTcTwR-TR+gGczqQhWupzB2h0wa+zg@mail.gmail.com>
 <CAGpovd29V3rXSyB_SMBnbKfb3dzAFPF66ck9iU6XPB_+bS+8WQ@mail.gmail.com>
 <CAEKh8ugr0FNryRVmM64pBPxjP=4xLrC0T2OsUD27mSASAyHpVQ@mail.gmail.com>
 <CAD4oTHHgSUr5TfLaTHxAM94E-Sqw9bW6gON7KdW324VEr524Lw@mail.gmail.com>
 <95648642-FCC6-49BE-93D9-8872172946E6@rstudio.com>
Message-ID: <056d3c18-4d2b-0456-3e9f-6b496d733816@gmail.com>

On 07/10/2019 4:22 a.m., Lionel Henry wrote:
> Hi Gabe,
> 
>> There is another way the pipe could go into base R that could not be
>> done in package space and has the potential to mitigate some pretty
>> serious downsides to the pipes relating to debugging
> 
> I assume you're thinking about the large stack trace of the magrittr
> pipe? You don't need a parser transformation to solve this problem
> though, the pipe could be implemented as a regular function with a
> very limited impact on the stack. And if implemented as a SPECIALSXP,
> it would be completely invisible. We've been planning to rewrite %>%
> to fix the performance and the stack print, it's just low priority.

I don't know what Gabe had in mind, but the downside to pipes that I see 
is that they are single statements.  I'd like the debugger to be able to 
single step through one stage at a time.  I'd like to be able to set a 
breakpoint on line 3 in

   a %>%
   b %>%
   c %>%
   d

and be able to examine the intermediate result of evaluating b before 
piping it into c.  (Or maybe that's off by one:  maybe I'd prefer to 
examine the inputs to d if I put a breakpoint there.  I'd have to try it 
to find out which feels more natural.)


> About the semantics of local evaluation that were proposed in this
> thread, I think that wouldn't be right. A native pipe should be
> consistent with other control flow constructs like `if` and `for` and
> evaluate in the current environment. In that case, the `.` binding, if
> any, would be restored to its original value in `on.exit()` (or through
> unwind-protection if implemented in C).

That makes sense.

Duncan Murdoch

> 
> Best,
> Lionel
> 
> 
>> On 6 Oct 2019, at 01:50, Gabriel Becker <gabembecker at gmail.com> wrote:
>>
>> Hi all,
>>
>> I think there's some nuance here that makes makes me agree partially with
>> each "side".
>>
>> The pipe is inarguably extremely popular. Many probably think of it as a
>> core feature of R, along with the tidyverse that (as was pointed out)
>> largely surrounds it and drives its popularity. Whether its a good or bad
>> thing that they think that doesn't change the fact that by my estimation
>> that Ant is correct that they do. BUT, I don't agree with him that that, by
>> itself, is a reason to put it in base R in the form that it exists now. For
>> the current form, there aren't really any major downsides that I see to
>> having people just use the package version.
>>
>> Sure it may be a little weird, but it doesn't ever really stop the
>> people from using it or present a significant barrier. Another major point
>> is that many (most?) base R functions are not necessarily tooled to be
>> endomorphic, which in my personal opinion is *largely* the only place that
>> the pipes are really compelling.
>>
>> That was for pipes as the exist in package space, though. There is another
>> way the pipe could go into base R that could not be done in package space
>> and has the potential to mitigate some pretty serious downsides to the
>> pipes relating to debugging, which would be to implement them in the parser.
>>
>> If
>>
>> iris %>% group_by(Species) %>% summarize(mean_sl = mean(Sepal.Length)) %>%
>> filter(mean_sl > 5)
>>
>>
>> were *parsed* as, for example, into
>>
>> local({
>>             . = group_by(iris, Species)
>>
>>             ._tmp2 = summarize(., mean_sl = mean(Sepal.Length))
>>
>>             filter(., mean_sl > 5)
>>        })
>>
>>
>>
>>
>> Then debuggiing (once you knew that) would be much easier but behavaior
>> would be the same as it is now. There could even be some sort of
>> step-through-pipe debugger at that point added as well for additional
>> convenience.
>>
>> There is some minor precedent for that type of transformative parsing:
>>
>>> expr = parse(text = "5 -> x")
>>
>>> expr
>>
>> expression(5 -> x)
>>
>>> expr[[1]]
>>
>> x <- 5
>>
>>
>> Though thats a much more minor transformation.
>>
>> All of that said, I believe Jim Hester (cc'ed) suggested something along
>> these lines at the RSummit a couple of years ago, and thus far R-core has
>> not shown much appetite for changing things in the parser.
>>
>> Without that changing, I'd have to say that my vote, for whatever its
>> worth, comes down on the side of pipes being fine in packages. A summary of
>> my reasoning being that it only makes sense for them to go into R itself if
>> doing so fixes an issue that cna't be fixed with them in package space.
>>
>> Best,
>> ~G
>>
>>
>>
>> On Sun, Oct 6, 2019 at 5:26 AM Ant F <antoine.fabri at gmail.com> wrote:
>>
>>> Yes but this exageration precisely misses the point.
>>>
>>> Concerning your examples:
>>>
>>> * I love fread but I think it makes a lot of subjective choices that are
>>> best associated with a package. I think it
>>> changed a lot with time and can still change, and we have great developers
>>> willing to maintain it and be reactive
>>> regarding feature requests or bug reports
>>>
>>> *.group_by() adds a class that works only (or mostly) with tidyverse verbs,
>>> that's very easy to dismiss it as an inclusion in base R.
>>>
>>> * summarize is an alternative to aggregate, that would be very confusing to
>>> have both
>>>
>>> Now to be fair to your argument we could think of other functions such as
>>> data.table::rleid() which I believe base R misses deeply,
>>> and there is nothing wrong with packaged functions making their way to base
>>> R.
>>>
>>> Maybe there's an existing list of criteria for inclusion, in base R but if
>>> not I can make one up for the sake of this discussion :) :
>>> * 1) the functionality should not already exist
>>> * 2) the function should be general enough
>>> * 3) the function should have a large amount of potential of users
>>> * 4) the function should be robust, and not require extensive maintenance
>>> * 5) the function should be stable, we shouldn't expect new features ever 2
>>> months
>>> * 6) the function should have an intuitive interface in the context of the
>>> rest ot base R
>>>
>>> I guess 1 and 6 could be held against my proposal, because :
>>> (1) everything can be done without pipes
>>> (6) They are somewhat surprising (though with explicit dots not that much,
>>> and not more surprising than say `bquote()`)
>>>
>>> In my opinion the + offset the -.
>>>
>>> I wouldn't advise taking magrittr's pipe (providing the license allows so)
>>> for instance, because it makes a lot of design choices and has a complex
>>> behavior, what I propose is 2 lines of code very unlikely to evolve or
>>> require maintenance.
>>>
>>> Antoine
>>>
>>> PS: I just receive the digest once a day so If you don't "reply all" I can
>>> only react later.
>>>
>>> Le sam. 5 oct. 2019 ? 19:54, Hugh Marera <hugh.marera at gmail.com> a ?crit :
>>>
>>>> I exaggerated the comparison for effect. However, it is not very
>>> difficult
>>>> to find functions in dplyr or data.table or indeed other packages that
>>> one
>>>> may wish to be in base R. Examples, for me, could include
>>>> data.table::fread, dplyr::group_by & dplyr::summari[sZ]e combo, etc.
>>> Also,
>>>> the "popularity" of magrittr::`%>%` is mostly attributable to the
>>> tidyverse
>>>> (an advanced superset of R). Many R users don't even know that they are
>>>> installing the magrittr package.
>>>>
>>>> On Sat, Oct 5, 2019 at 6:30 PM I?aki Ucar <iucar at fedoraproject.org>
>>> wrote:
>>>>
>>>>> On Sat, 5 Oct 2019 at 17:15, Hugh Marera <hugh.marera at gmail.com> wrote:
>>>>>>
>>>>>> How is your argument different to, say,  "Should dplyr or data.table
>>> be
>>>>>> part of base R as they are the most popular data science packages and
>>>>> they
>>>>>> are used by a large number of users?"
>>>>>
>>>>> Two packages with many features, dozens of functions and under heavy
>>>>> development to fix bugs, add new features and improve performance, vs.
>>>>> a single operator with a limited and well-defined functionality, and a
>>>>> reference implementation that hasn't changed in years (but certainly
>>>>> hackish in a way that probably could only be improved from R itself).
>>>>>
>>>>> Can't you really spot the difference?
>>>>>
>>>>> I?aki
>>>>>
>>>>
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>
>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From ||one| @end|ng |rom r@tud|o@com  Mon Oct  7 14:38:19 2019
From: ||one| @end|ng |rom r@tud|o@com (Lionel Henry)
Date: Mon, 7 Oct 2019 14:38:19 +0200
Subject: [Rd] should base R have a piping operator ?
In-Reply-To: <056d3c18-4d2b-0456-3e9f-6b496d733816@gmail.com>
References: <CAEKh8uidcEWA4tzmkpQxpF=q7r93qjf-HtiWLfbT91TFGOSzFQ@mail.gmail.com>
 <CAGpovd2jxxWBUxPhMAR6ZPw1+UQzK7BzJVtps10cMBSp=EUdHQ@mail.gmail.com>
 <CALEXWq0ZvGeTbeq-AeYTSTcTwR-TR+gGczqQhWupzB2h0wa+zg@mail.gmail.com>
 <CAGpovd29V3rXSyB_SMBnbKfb3dzAFPF66ck9iU6XPB_+bS+8WQ@mail.gmail.com>
 <CAEKh8ugr0FNryRVmM64pBPxjP=4xLrC0T2OsUD27mSASAyHpVQ@mail.gmail.com>
 <CAD4oTHHgSUr5TfLaTHxAM94E-Sqw9bW6gON7KdW324VEr524Lw@mail.gmail.com>
 <95648642-FCC6-49BE-93D9-8872172946E6@rstudio.com>
 <056d3c18-4d2b-0456-3e9f-6b496d733816@gmail.com>
Message-ID: <4126370A-8386-406C-AD94-B8A99E2AE52E@rstudio.com>

> 
> On 7 Oct 2019, at 13:47, Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
> 
> On 07/10/2019 4:22 a.m., Lionel Henry wrote:
>> Hi Gabe,
>>> There is another way the pipe could go into base R that could not be
>>> done in package space and has the potential to mitigate some pretty
>>> serious downsides to the pipes relating to debugging
>> I assume you're thinking about the large stack trace of the magrittr
>> pipe? You don't need a parser transformation to solve this problem
>> though, the pipe could be implemented as a regular function with a
>> very limited impact on the stack. And if implemented as a SPECIALSXP,
>> it would be completely invisible. We've been planning to rewrite %>%
>> to fix the performance and the stack print, it's just low priority.
> 
> I don't know what Gabe had in mind, but the downside to pipes that I see is that they are single statements.  I'd like the debugger to be able to single step through one stage at a time.  I'd like to be able to set a breakpoint on line 3 in
> 
>  a %>%
>  b %>%
>  c %>%
>  d
> 
> and be able to examine the intermediate result of evaluating b before piping it into c.  (Or maybe that's off by one:  maybe I'd prefer to examine the inputs to d if I put a breakpoint there.  I'd have to try it to find out which feels more natural.)

In order to place a breakpoint on line 3, I think you'll need to wrap
`c()` in curly braces and insert a `browser()` call. And at that point
you're changing the semantics of `c()` and you'll need to manually
write the placeholder for the input:

a() |>
  b() |>
  { browser(); c(.) } |>
  d()

I don't see any way around this. I guess it could be done behind the 
scenes by the IDE when a breakpoint is set though. Note that this
doesn't require any changes to the parser and already works with the
magrittr pipe.

Then there's the issue of continuing to step-debug through the
pipeline. This could be achieved by parsing `a |> b()` as `{a} |>
{b()}`. so that each sub-expression carries source references. In
general, there are metaprogramming patterns that would be made easier
if calls to `function` or `if` always had a body wrapped in `{`. It is
too late to change historical operators but maybe it makes sense for
newer ones?

Lionel


	[[alternative HTML version deleted]]


From therne@u @end|ng |rom m@yo@edu  Mon Oct  7 15:00:01 2019
From: therne@u @end|ng |rom m@yo@edu (Therneau, Terry M., Ph.D.)
Date: Mon, 07 Oct 2019 08:00:01 -0500
Subject: [Rd] error in [.terms
Message-ID: <771925$chdf2n@ironport10.mayo.edu>

As a footnote, the update.formula function shares one of the flaws I mentioned in the 
earlier post

 > test <-? y ~ x1 + (x2=='abc') + x3
 > update(test, . ~ .-x3)
y ~ x1 + x2 == "abc"

The original formula is valid but the updated one is not.

Terry T.


	[[alternative HTML version deleted]]


From murdoch@dunc@n @end|ng |rom gm@||@com  Mon Oct  7 15:36:03 2019
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Mon, 7 Oct 2019 09:36:03 -0400
Subject: [Rd] should base R have a piping operator ?
In-Reply-To: <4126370A-8386-406C-AD94-B8A99E2AE52E@rstudio.com>
References: <CAEKh8uidcEWA4tzmkpQxpF=q7r93qjf-HtiWLfbT91TFGOSzFQ@mail.gmail.com>
 <CAGpovd2jxxWBUxPhMAR6ZPw1+UQzK7BzJVtps10cMBSp=EUdHQ@mail.gmail.com>
 <CALEXWq0ZvGeTbeq-AeYTSTcTwR-TR+gGczqQhWupzB2h0wa+zg@mail.gmail.com>
 <CAGpovd29V3rXSyB_SMBnbKfb3dzAFPF66ck9iU6XPB_+bS+8WQ@mail.gmail.com>
 <CAEKh8ugr0FNryRVmM64pBPxjP=4xLrC0T2OsUD27mSASAyHpVQ@mail.gmail.com>
 <CAD4oTHHgSUr5TfLaTHxAM94E-Sqw9bW6gON7KdW324VEr524Lw@mail.gmail.com>
 <95648642-FCC6-49BE-93D9-8872172946E6@rstudio.com>
 <056d3c18-4d2b-0456-3e9f-6b496d733816@gmail.com>
 <4126370A-8386-406C-AD94-B8A99E2AE52E@rstudio.com>
Message-ID: <032922f2-37a7-5197-04e8-b9ac51990e91@gmail.com>

On 07/10/2019 8:38 a.m., Lionel Henry wrote:
>>
>> On 7 Oct 2019, at 13:47, Duncan Murdoch <murdoch.duncan at gmail.com 
>> <mailto:murdoch.duncan at gmail.com>> wrote:
>>
>> On 07/10/2019 4:22 a.m., Lionel Henry wrote:
>>> Hi Gabe,
>>>> There is another way the pipe could go into base R that could not be
>>>> done in package space and has the potential to mitigate some pretty
>>>> serious downsides to the pipes relating to debugging
>>> I assume you're thinking about the large stack trace of the magrittr
>>> pipe? You don't need a parser transformation to solve this problem
>>> though, the pipe could be implemented as a regular function with a
>>> very limited impact on the stack. And if implemented as a SPECIALSXP,
>>> it would be completely invisible. We've been planning to rewrite %>%
>>> to fix the performance and the stack print, it's just low priority.
>>
>> I don't know what Gabe had in mind, but the downside to pipes that I 
>> see is that they are single statements. ?I'd like the debugger to be 
>> able to single step through one stage at a time. ?I'd like to be able 
>> to set a breakpoint on line 3 in
>>
>> ?a %>%
>> ?b %>%
>> ?c %>%
>> ?d
>>
>> and be able to examine the intermediate result of evaluating b before 
>> piping it into c. ?(Or maybe that's off by one: ?maybe I'd prefer to 
>> examine the inputs to d if I put a breakpoint there. ?I'd have to try 
>> it to find out which feels more natural.)
> 
> In order to place a breakpoint on line 3, I think you'll need to wrap
> `c()` in curly braces and insert a `browser()` call. And at that point
> you're changing the semantics of `c()` and you'll need to manually
> write the placeholder for the input:
> 
> a() |>
>  ? b() |>
>  ? { browser(); c(.) } |>
>  ? d()
> 
> I don't see any way around this. I guess it could be done behind the
> scenes by the IDE when a breakpoint is set though. Note that this
> doesn't require any changes to the parser and already works with the
> magrittr pipe.

Yes, I was hoping this would happen behind the scenes.  I agree that the 
parser doesn't need to be changed, but the IDE would need to break up 
the statement into 3 or more equivalent statements for this to work with 
no changes to core R.  I think that could be done after parsing at 
run-time, as described in my earlier message.

Duncan Murdoch

P.S.  Were you just using |> to save typing, or is there a proposal to 
add a new operator to the language?  That would need parser changes.


>
> Then there's the issue of continuing to step-debug through the
> pipeline. This could be achieved by parsing `a |> b()` as `{a} |>
> {b()}`. so that each sub-expression carries source references. In
> general, there are metaprogramming patterns that would be made easier
> if calls to `function` or `if` always had a body wrapped in `{`. It is
> too late to change historical operators but maybe it makes sense for
> newer ones?
> 
> Lionel
>


From ||one| @end|ng |rom r@tud|o@com  Mon Oct  7 16:28:20 2019
From: ||one| @end|ng |rom r@tud|o@com (Lionel Henry)
Date: Mon, 7 Oct 2019 16:28:20 +0200
Subject: [Rd] should base R have a piping operator ?
In-Reply-To: <032922f2-37a7-5197-04e8-b9ac51990e91@gmail.com>
References: <CAEKh8uidcEWA4tzmkpQxpF=q7r93qjf-HtiWLfbT91TFGOSzFQ@mail.gmail.com>
 <CAGpovd2jxxWBUxPhMAR6ZPw1+UQzK7BzJVtps10cMBSp=EUdHQ@mail.gmail.com>
 <CALEXWq0ZvGeTbeq-AeYTSTcTwR-TR+gGczqQhWupzB2h0wa+zg@mail.gmail.com>
 <CAGpovd29V3rXSyB_SMBnbKfb3dzAFPF66ck9iU6XPB_+bS+8WQ@mail.gmail.com>
 <CAEKh8ugr0FNryRVmM64pBPxjP=4xLrC0T2OsUD27mSASAyHpVQ@mail.gmail.com>
 <CAD4oTHHgSUr5TfLaTHxAM94E-Sqw9bW6gON7KdW324VEr524Lw@mail.gmail.com>
 <95648642-FCC6-49BE-93D9-8872172946E6@rstudio.com>
 <056d3c18-4d2b-0456-3e9f-6b496d733816@gmail.com>
 <4126370A-8386-406C-AD94-B8A99E2AE52E@rstudio.com>
 <032922f2-37a7-5197-04e8-b9ac51990e91@gmail.com>
Message-ID: <B82B864E-63D4-470A-8AC9-045E116B6D71@rstudio.com>


> On 7 Oct 2019, at 15:36, Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
> 
>  I think that could be done after parsing at run-time, as described in my earlier message.

Good point.


> P.S.  Were you just using |> to save typing, or is there a proposal to add a new operator to the language?  That would need parser changes.

Just a hypothetical native pipe for which the parser would automatically
wrap the arguments in srcref-carrying braces. Then we get step-debugging
of pipelines in all editors.


Best,
Lionel
	[[alternative HTML version deleted]]


From |uke-t|erney @end|ng |rom u|ow@@edu  Mon Oct  7 17:04:49 2019
From: |uke-t|erney @end|ng |rom u|ow@@edu (Tierney, Luke)
Date: Mon, 7 Oct 2019 15:04:49 +0000
Subject: [Rd] [External] Re:  should base R have a piping operator ?
In-Reply-To: <95648642-FCC6-49BE-93D9-8872172946E6@rstudio.com>
References: <CAEKh8uidcEWA4tzmkpQxpF=q7r93qjf-HtiWLfbT91TFGOSzFQ@mail.gmail.com>
 <CAGpovd2jxxWBUxPhMAR6ZPw1+UQzK7BzJVtps10cMBSp=EUdHQ@mail.gmail.com>
 <CALEXWq0ZvGeTbeq-AeYTSTcTwR-TR+gGczqQhWupzB2h0wa+zg@mail.gmail.com>
 <CAGpovd29V3rXSyB_SMBnbKfb3dzAFPF66ck9iU6XPB_+bS+8WQ@mail.gmail.com>
 <CAEKh8ugr0FNryRVmM64pBPxjP=4xLrC0T2OsUD27mSASAyHpVQ@mail.gmail.com>
 <CAD4oTHHgSUr5TfLaTHxAM94E-Sqw9bW6gON7KdW324VEr524Lw@mail.gmail.com>
 <95648642-FCC6-49BE-93D9-8872172946E6@rstudio.com>
Message-ID: <alpine.DEB.2.21.1910071001350.14507@luke-Latitude-7480>

On Mon, 7 Oct 2019, Lionel Henry wrote:

> Hi Gabe,
>
>> There is another way the pipe could go into base R that could not be
>> done in package space and has the potential to mitigate some pretty
>> serious downsides to the pipes relating to debugging
>
> I assume you're thinking about the large stack trace of the magrittr
> pipe? You don't need a parser transformation to solve this problem
> though, the pipe could be implemented as a regular function with a
> very limited impact on the stack. And if implemented as a SPECIALSXP,
> it would be completely invisible. We've been planning to rewrite %>%
> to fix the performance and the stack print, it's just low priority.
>
> About the semantics of local evaluation that were proposed in this
> thread, I think that wouldn't be right. A native pipe should be
> consistent with other control flow constructs like `if` and `for` and
> evaluate in the current environment. In that case, the `.` binding, if
> any, would be restored to its original value in `on.exit()` (or through
> unwind-protection if implemented in C).
>

Sorry to be blunt but adding/removing a variable from a caller's
environment is a horrible design idea. Think about what happens if an
argument in a pipe stage contains a pipe. (Not completely
unreasonable, e.g. for a left_join). We already have such a design
lurking in (at least) one place in base code and it keeps biting. It's
pretty high on my list to be expunged.

If a variable is to be used it needs to be in its own
scope/environment.  There is another option, which is to rewrite the
pipe as a nested call and evaluate that in the parent frame. Not likely
to be much worse for debugging and might even be better.  Some
tinkering with these ideas is at

https://gitlab.com/luke-tierney/pipes

All that said, there is nothing that can be done with pipes that can't
be done without them. They may be the most visible aspect of the
tidyverse but they are also the least essential. I don't find them
useful, mostly because they make debugging harder and add to the
cognitive load of figuring out what is actually going on in the
evaluation process. So I don't use them in my work or my teaching (I
do mention them in teaching so students can understand them when they
see them). Many people clearly like them, and that's fine. But they
are not in any way, shape, or form essential.

I can't speak for all of R core on this, but this is how I look at the
question of inclusion in base: R core developer time is a (very)
scarce resource. Any part of that resource that is used to incorporate
and maintain in base something that can be implemented reasonably well
in a package is then not available for improving and maintaining parts
of R that have to be in base. There would need to be extremely strong
reasons for reallocating resources in this way and I just don't see
how that case can be made here.

It is certainly possible that thinking about pipes might suggest tome
useful low level primitives to add that would have to live in base and
might be useful in other contexts. Those might be worth considering.
[Some kind of 'exec()' or aving an 'exec()' or 'tailcall()' primitive
to reuse a call frame for example.]

Best,

luke

> Best,
> Lionel
>
>
>> On 6 Oct 2019, at 01:50, Gabriel Becker <gabembecker at gmail.com> wrote:
>>
>> Hi all,
>>
>> I think there's some nuance here that makes makes me agree partially with
>> each "side".
>>
>> The pipe is inarguably extremely popular. Many probably think of it as a
>> core feature of R, along with the tidyverse that (as was pointed out)
>> largely surrounds it and drives its popularity. Whether its a good or bad
>> thing that they think that doesn't change the fact that by my estimation
>> that Ant is correct that they do. BUT, I don't agree with him that that, by
>> itself, is a reason to put it in base R in the form that it exists now. For
>> the current form, there aren't really any major downsides that I see to
>> having people just use the package version.
>>
>> Sure it may be a little weird, but it doesn't ever really stop the
>> people from using it or present a significant barrier. Another major point
>> is that many (most?) base R functions are not necessarily tooled to be
>> endomorphic, which in my personal opinion is *largely* the only place that
>> the pipes are really compelling.
>>
>> That was for pipes as the exist in package space, though. There is another
>> way the pipe could go into base R that could not be done in package space
>> and has the potential to mitigate some pretty serious downsides to the
>> pipes relating to debugging, which would be to implement them in the parser.
>>
>> If
>>
>> iris %>% group_by(Species) %>% summarize(mean_sl = mean(Sepal.Length)) %>%
>> filter(mean_sl > 5)
>>
>>
>> were *parsed* as, for example, into
>>
>> local({
>>            . = group_by(iris, Species)
>>
>>            ._tmp2 = summarize(., mean_sl = mean(Sepal.Length))
>>
>>            filter(., mean_sl > 5)
>>       })
>>
>>
>>
>>
>> Then debuggiing (once you knew that) would be much easier but behavaior
>> would be the same as it is now. There could even be some sort of
>> step-through-pipe debugger at that point added as well for additional
>> convenience.
>>
>> There is some minor precedent for that type of transformative parsing:
>>
>>> expr = parse(text = "5 -> x")
>>
>>> expr
>>
>> expression(5 -> x)
>>
>>> expr[[1]]
>>
>> x <- 5
>>
>>
>> Though thats a much more minor transformation.
>>
>> All of that said, I believe Jim Hester (cc'ed) suggested something along
>> these lines at the RSummit a couple of years ago, and thus far R-core has
>> not shown much appetite for changing things in the parser.
>>
>> Without that changing, I'd have to say that my vote, for whatever its
>> worth, comes down on the side of pipes being fine in packages. A summary of
>> my reasoning being that it only makes sense for them to go into R itself if
>> doing so fixes an issue that cna't be fixed with them in package space.
>>
>> Best,
>> ~G
>>
>>
>>
>> On Sun, Oct 6, 2019 at 5:26 AM Ant F <antoine.fabri at gmail.com> wrote:
>>
>>> Yes but this exageration precisely misses the point.
>>>
>>> Concerning your examples:
>>>
>>> * I love fread but I think it makes a lot of subjective choices that are
>>> best associated with a package. I think it
>>> changed a lot with time and can still change, and we have great developers
>>> willing to maintain it and be reactive
>>> regarding feature requests or bug reports
>>>
>>> *.group_by() adds a class that works only (or mostly) with tidyverse verbs,
>>> that's very easy to dismiss it as an inclusion in base R.
>>>
>>> * summarize is an alternative to aggregate, that would be very confusing to
>>> have both
>>>
>>> Now to be fair to your argument we could think of other functions such as
>>> data.table::rleid() which I believe base R misses deeply,
>>> and there is nothing wrong with packaged functions making their way to base
>>> R.
>>>
>>> Maybe there's an existing list of criteria for inclusion, in base R but if
>>> not I can make one up for the sake of this discussion :) :
>>> * 1) the functionality should not already exist
>>> * 2) the function should be general enough
>>> * 3) the function should have a large amount of potential of users
>>> * 4) the function should be robust, and not require extensive maintenance
>>> * 5) the function should be stable, we shouldn't expect new features ever 2
>>> months
>>> * 6) the function should have an intuitive interface in the context of the
>>> rest ot base R
>>>
>>> I guess 1 and 6 could be held against my proposal, because :
>>> (1) everything can be done without pipes
>>> (6) They are somewhat surprising (though with explicit dots not that much,
>>> and not more surprising than say `bquote()`)
>>>
>>> In my opinion the + offset the -.
>>>
>>> I wouldn't advise taking magrittr's pipe (providing the license allows so)
>>> for instance, because it makes a lot of design choices and has a complex
>>> behavior, what I propose is 2 lines of code very unlikely to evolve or
>>> require maintenance.
>>>
>>> Antoine
>>>
>>> PS: I just receive the digest once a day so If you don't "reply all" I can
>>> only react later.
>>>
>>> Le sam. 5 oct. 2019 ? 19:54, Hugh Marera <hugh.marera at gmail.com> a ?crit :
>>>
>>>> I exaggerated the comparison for effect. However, it is not very
>>> difficult
>>>> to find functions in dplyr or data.table or indeed other packages that
>>> one
>>>> may wish to be in base R. Examples, for me, could include
>>>> data.table::fread, dplyr::group_by & dplyr::summari[sZ]e combo, etc.
>>> Also,
>>>> the "popularity" of magrittr::`%>%` is mostly attributable to the
>>> tidyverse
>>>> (an advanced superset of R). Many R users don't even know that they are
>>>> installing the magrittr package.
>>>>
>>>> On Sat, Oct 5, 2019 at 6:30 PM I?aki Ucar <iucar at fedoraproject.org>
>>> wrote:
>>>>
>>>>> On Sat, 5 Oct 2019 at 17:15, Hugh Marera <hugh.marera at gmail.com> wrote:
>>>>>>
>>>>>> How is your argument different to, say,  "Should dplyr or data.table
>>> be
>>>>>> part of base R as they are the most popular data science packages and
>>>>> they
>>>>>> are used by a large number of users?"
>>>>>
>>>>> Two packages with many features, dozens of functions and under heavy
>>>>> development to fix bugs, add new features and improve performance, vs.
>>>>> a single operator with a limited and well-defined functionality, and a
>>>>> reference implementation that hasn't changed in years (but certainly
>>>>> hackish in a way that probably could only be improved from R itself).
>>>>>
>>>>> Can't you really spot the difference?
>>>>>
>>>>> I?aki
>>>>>
>>>>
>>>
>>>        [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>
>>
>> 	[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

-- 
Luke Tierney
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu

From ||one| @end|ng |rom r@tud|o@com  Mon Oct  7 17:22:38 2019
From: ||one| @end|ng |rom r@tud|o@com (Lionel Henry)
Date: Mon, 7 Oct 2019 17:22:38 +0200
Subject: [Rd] [External] Re:  should base R have a piping operator ?
In-Reply-To: <alpine.DEB.2.21.1910071001350.14507@luke-Latitude-7480>
References: <CAEKh8uidcEWA4tzmkpQxpF=q7r93qjf-HtiWLfbT91TFGOSzFQ@mail.gmail.com>
 <CAGpovd2jxxWBUxPhMAR6ZPw1+UQzK7BzJVtps10cMBSp=EUdHQ@mail.gmail.com>
 <CALEXWq0ZvGeTbeq-AeYTSTcTwR-TR+gGczqQhWupzB2h0wa+zg@mail.gmail.com>
 <CAGpovd29V3rXSyB_SMBnbKfb3dzAFPF66ck9iU6XPB_+bS+8WQ@mail.gmail.com>
 <CAEKh8ugr0FNryRVmM64pBPxjP=4xLrC0T2OsUD27mSASAyHpVQ@mail.gmail.com>
 <CAD4oTHHgSUr5TfLaTHxAM94E-Sqw9bW6gON7KdW324VEr524Lw@mail.gmail.com>
 <95648642-FCC6-49BE-93D9-8872172946E6@rstudio.com>
 <alpine.DEB.2.21.1910071001350.14507@luke-Latitude-7480>
Message-ID: <ABABA4EA-88E4-4E22-B256-214050E8720A@rstudio.com>



> On 7 Oct 2019, at 17:04, Tierney, Luke <luke-tierney at uiowa.edu> wrote:
> 
>  Think about what happens if an
> argument in a pipe stage contains a pipe. (Not completely
> unreasonable, e.g. for a left_join).

It should work exactly as it does in a local environment.

```
`%foo%` <- function(x, y) {
  env <- parent.frame()

  # Use `:=` to avoid partial matching on .env/.frame
  rlang::scoped_bindings(. := x, .env = env)

  eval(substitute(y), env)
}

"A" %foo% {
  print(.)
  "B" %foo% print(.)
  print(.)
}
#> [1] "A"
#> [1] "B"
#> [1] "A"

print(.)
#> Error in print(.) : object '.' not found

```

The advantage is that side effects (such as assigning variables or calling
`return()`) will occur in the expected environment. I don't see it causing
problems except in artificial cases. Am I missing something?

I agree that restraining the pipe to a single placeholder (to avoid
double evaluation) would be a good design too.

I can't access https://gitlab.com/luke-tierney/pipes, it appears to be 404.

Best,
Lionel


From |uke-t|erney @end|ng |rom u|ow@@edu  Mon Oct  7 18:17:46 2019
From: |uke-t|erney @end|ng |rom u|ow@@edu (Tierney, Luke)
Date: Mon, 7 Oct 2019 16:17:46 +0000
Subject: [Rd] [External] Re:  should base R have a piping operator ?
In-Reply-To: <ABABA4EA-88E4-4E22-B256-214050E8720A@rstudio.com>
References: <CAEKh8uidcEWA4tzmkpQxpF=q7r93qjf-HtiWLfbT91TFGOSzFQ@mail.gmail.com>
 <CAGpovd2jxxWBUxPhMAR6ZPw1+UQzK7BzJVtps10cMBSp=EUdHQ@mail.gmail.com>
 <CALEXWq0ZvGeTbeq-AeYTSTcTwR-TR+gGczqQhWupzB2h0wa+zg@mail.gmail.com>
 <CAGpovd29V3rXSyB_SMBnbKfb3dzAFPF66ck9iU6XPB_+bS+8WQ@mail.gmail.com>
 <CAEKh8ugr0FNryRVmM64pBPxjP=4xLrC0T2OsUD27mSASAyHpVQ@mail.gmail.com>
 <CAD4oTHHgSUr5TfLaTHxAM94E-Sqw9bW6gON7KdW324VEr524Lw@mail.gmail.com>
 <95648642-FCC6-49BE-93D9-8872172946E6@rstudio.com>
 <alpine.DEB.2.21.1910071001350.14507@luke-Latitude-7480>
 <ABABA4EA-88E4-4E22-B256-214050E8720A@rstudio.com>
Message-ID: <alpine.DEB.2.21.1910071052330.14507@luke-Latitude-7480>

On Mon, 7 Oct 2019, Lionel Henry wrote:

>
>
>> On 7 Oct 2019, at 17:04, Tierney, Luke <luke-tierney at uiowa.edu> wrote:
>>
>>  Think about what happens if an
>> argument in a pipe stage contains a pipe. (Not completely
>> unreasonable, e.g. for a left_join).
>
> It should work exactly as it does in a local environment.
>
> ```
> `%foo%` <- function(x, y) {
>  env <- parent.frame()
>
>  # Use `:=` to avoid partial matching on .env/.frame
>  rlang::scoped_bindings(. := x, .env = env)
>
>  eval(substitute(y), env)
> }
>
> "A" %foo% {
>  print(.)
>  "B" %foo% print(.)
>  print(.)
> }
> #> [1] "A"
> #> [1] "B"
> #> [1] "A"
>
> print(.)
> #> Error in print(.) : object '.' not found
>
> ```
>
> The advantage is that side effects (such as assigning variables or calling
> `return()`) will occur in the expected environment.

You get the assignment behavior with the nested call approach. (Not
that doing this is necessarily a good idea).

> I don't see it causing
> problems except in artificial cases. Am I missing something?

Here is a stylized example:

f <- function(x, y) {
     assign("xx", x, parent.frame())
     on.exit(rm(xx, envir = parent.frame()))
     y
     get("xx") + 1
}

## This is fine:
> f(1, 2) 
[1] 2

## This is not:
> f(1, f(1, 2))
Error in get("xx") : object 'xx' not found

If you play these games whether you get the result you want, or an
obvious error, or just the wrong answer depends on argument evaluation
order and the like. You really don't want to go there. Not to mention
that you would be telling users they are not allowed to use '.' as a
variable name for their own purposes or you would be polluting their
environment with some other artificial symbol that they would see in
debugging. Just don't.

Anything going in base needs to worry even about artificial cases.
Yes, there are things in base that don't meet that standard. No, that
is not a reason to add more.

> I agree that restraining the pipe to a single placeholder (to avoid
> double evaluation) would be a good design too.
>
> I can't access https://gitlab.com/luke-tierney/pipes, it appears to be 404.

Should be able to get there now. Needed to change the visibility ---
still learning my way around gitlab.

Best,

luke

> Best,
> Lionel
>
>

-- 
Luke Tierney
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu


From kev|nu@hey @end|ng |rom gm@||@com  Mon Oct  7 18:42:13 2019
From: kev|nu@hey @end|ng |rom gm@||@com (Kevin Ushey)
Date: Mon, 7 Oct 2019 09:42:13 -0700
Subject: [Rd] [External] Re: should base R have a piping operator ?
In-Reply-To: <alpine.DEB.2.21.1910071052330.14507@luke-Latitude-7480>
References: <CAEKh8uidcEWA4tzmkpQxpF=q7r93qjf-HtiWLfbT91TFGOSzFQ@mail.gmail.com>
 <CAGpovd2jxxWBUxPhMAR6ZPw1+UQzK7BzJVtps10cMBSp=EUdHQ@mail.gmail.com>
 <CALEXWq0ZvGeTbeq-AeYTSTcTwR-TR+gGczqQhWupzB2h0wa+zg@mail.gmail.com>
 <CAGpovd29V3rXSyB_SMBnbKfb3dzAFPF66ck9iU6XPB_+bS+8WQ@mail.gmail.com>
 <CAEKh8ugr0FNryRVmM64pBPxjP=4xLrC0T2OsUD27mSASAyHpVQ@mail.gmail.com>
 <CAD4oTHHgSUr5TfLaTHxAM94E-Sqw9bW6gON7KdW324VEr524Lw@mail.gmail.com>
 <95648642-FCC6-49BE-93D9-8872172946E6@rstudio.com>
 <alpine.DEB.2.21.1910071001350.14507@luke-Latitude-7480>
 <ABABA4EA-88E4-4E22-B256-214050E8720A@rstudio.com>
 <alpine.DEB.2.21.1910071052330.14507@luke-Latitude-7480>
Message-ID: <CAJXgQP0qBoY+4pmfUfoiSTtJ+KNm25k8umyYkL2s8fxu0GH2Tw@mail.gmail.com>

IMHO, if base R were to include a pipe operator, I think it should be much
simpler than the magrittr pipe. It should satisfy the property that:

    x |> f(...)   is equivalent to   f(x, ...)

Except, perhaps, in terms of when the promise for 'x' gets forced. We
shouldn't need to mess with bindings in environments to make that work.

My understanding is that the '.' placeholder is used so that the magrittr
pipe can be adapted to functions that aren't endomorphic or otherwise
easily pipeable. I would argue that:

1. Users could just create their own pipable wrapper functions if so
required, or
2. Users could use magrittr to get some of the 'extensions' to the pipe
operator (with the noted caveats).

Best,
Kevin

	[[alternative HTML version deleted]]


From ||one| @end|ng |rom r@tud|o@com  Mon Oct  7 21:11:15 2019
From: ||one| @end|ng |rom r@tud|o@com (Lionel Henry)
Date: Mon, 7 Oct 2019 21:11:15 +0200
Subject: [Rd] [External] Re: should base R have a piping operator ?
In-Reply-To: <CAJXgQP0qBoY+4pmfUfoiSTtJ+KNm25k8umyYkL2s8fxu0GH2Tw@mail.gmail.com>
References: <CAEKh8uidcEWA4tzmkpQxpF=q7r93qjf-HtiWLfbT91TFGOSzFQ@mail.gmail.com>
 <CAGpovd2jxxWBUxPhMAR6ZPw1+UQzK7BzJVtps10cMBSp=EUdHQ@mail.gmail.com>
 <CALEXWq0ZvGeTbeq-AeYTSTcTwR-TR+gGczqQhWupzB2h0wa+zg@mail.gmail.com>
 <CAGpovd29V3rXSyB_SMBnbKfb3dzAFPF66ck9iU6XPB_+bS+8WQ@mail.gmail.com>
 <CAEKh8ugr0FNryRVmM64pBPxjP=4xLrC0T2OsUD27mSASAyHpVQ@mail.gmail.com>
 <CAD4oTHHgSUr5TfLaTHxAM94E-Sqw9bW6gON7KdW324VEr524Lw@mail.gmail.com>
 <95648642-FCC6-49BE-93D9-8872172946E6@rstudio.com>
 <alpine.DEB.2.21.1910071001350.14507@luke-Latitude-7480>
 <ABABA4EA-88E4-4E22-B256-214050E8720A@rstudio.com>
 <alpine.DEB.2.21.1910071052330.14507@luke-Latitude-7480>
 <CAJXgQP0qBoY+4pmfUfoiSTtJ+KNm25k8umyYkL2s8fxu0GH2Tw@mail.gmail.com>
Message-ID: <74AC0AE9-4594-40BC-B873-40C6D4DA34A4@rstudio.com>

Hi Kevin,

> On 7 Oct 2019, at 18:42, Kevin Ushey <kevinushey at gmail.com> wrote:
> 
> My understanding is that the '.' placeholder is used so that the magrittr pipe can be adapted to functions that aren't endomorphic or otherwise easily pipeable. I would argue that:
> 
> 1. Users could just create their own pipable wrapper functions if so required, or
> 2. Users could use magrittr to get some of the 'extensions' to the pipe operator (with the noted caveats).


Another advantage of the placeholder is that it represents an obvious
binding to inspect while debugging. It would be useful to be able to
inspect all intermediate values in a pipeline by stepping with
sequences of `n` and `.` commands.

Lionel


	[[alternative HTML version deleted]]


From ||one| @end|ng |rom r@tud|o@com  Mon Oct  7 21:14:21 2019
From: ||one| @end|ng |rom r@tud|o@com (Lionel Henry)
Date: Mon, 7 Oct 2019 21:14:21 +0200
Subject: [Rd] [External] Re:  should base R have a piping operator ?
In-Reply-To: <alpine.DEB.2.21.1910071052330.14507@luke-Latitude-7480>
References: <CAEKh8uidcEWA4tzmkpQxpF=q7r93qjf-HtiWLfbT91TFGOSzFQ@mail.gmail.com>
 <CAGpovd2jxxWBUxPhMAR6ZPw1+UQzK7BzJVtps10cMBSp=EUdHQ@mail.gmail.com>
 <CALEXWq0ZvGeTbeq-AeYTSTcTwR-TR+gGczqQhWupzB2h0wa+zg@mail.gmail.com>
 <CAGpovd29V3rXSyB_SMBnbKfb3dzAFPF66ck9iU6XPB_+bS+8WQ@mail.gmail.com>
 <CAEKh8ugr0FNryRVmM64pBPxjP=4xLrC0T2OsUD27mSASAyHpVQ@mail.gmail.com>
 <CAD4oTHHgSUr5TfLaTHxAM94E-Sqw9bW6gON7KdW324VEr524Lw@mail.gmail.com>
 <95648642-FCC6-49BE-93D9-8872172946E6@rstudio.com>
 <alpine.DEB.2.21.1910071001350.14507@luke-Latitude-7480>
 <ABABA4EA-88E4-4E22-B256-214050E8720A@rstudio.com>
 <alpine.DEB.2.21.1910071052330.14507@luke-Latitude-7480>
Message-ID: <5B3D10F5-9E85-463E-933D-A125843E2B3F@rstudio.com>

On 7 Oct 2019, at 18:17, Tierney, Luke <luke-tierney at uiowa.edu> wrote:

> Here is a stylized example:

The previous value of the binding should only be restored if it
existed:

g <- function(x, y) {
  rlang::scoped_bindings(xx = x, .env = parent.frame())
  y
  get("xx") + 10
}

# Good
g(1, 2)
#> [1] 11

# Still good?
g(1, g(1, 2))
#> [1] 11


> If you play these games whether you get the result you want, or an
> obvious error, or just the wrong answer depends on argument evaluation
> order and the like.

I think the surprises are limited because the pattern has stack-like
semantics. We get in a new context where `.` gains a new meaning, and
when we exit the previous meaning is restored.

One example where this could lead to unexpected behaviour is trying to
capture the value of the placeholder in a closure:

f <- function(x) {
  x %>% {
    identity(function() .)
  }
}

# This makes sense:
f("A")()
#> Error: object '.' not found

# This doesn't:
"B" %>% { f("A")() }
#> [1] "B"


> Not to mention that you would be telling users they are not allowed
> to use '.' as a variable name for their own purposes or you would be
> polluting their environment with some other artificial symbol that
> they would see in debugging.

That's a good point. Debugging allows to move up the call stack before
the context is exited, so you'd see the last value of `.` in examples
of nested pipes like `foo %>% bar( f %>% g() )`. That could be confusing.


> Anything going in base needs to worry even about artificial cases.
> Yes, there are things in base that don't meet that standard. No, that
> is not a reason to add more.

Agreed. What I meant by artificial cases is functions making
questionable assumptions after peeking into foreign contexts etc.

I'm worried about what happens with important language constructs like
`<-` and `return()` when code is evaluated in a local context. That
said, I think binding pipe values to `.` is more important than these
particular semantics because the placeholder is an obvious binding to
inspect while debug-stepping through a pipeline. So evaluating in a
child is probably preferable to giving up the placeholder altogether.

Best,
Lionel


From |uke-t|erney @end|ng |rom u|ow@@edu  Mon Oct  7 21:41:30 2019
From: |uke-t|erney @end|ng |rom u|ow@@edu (Tierney, Luke)
Date: Mon, 7 Oct 2019 19:41:30 +0000
Subject: [Rd] [External] Re:  should base R have a piping operator ?
In-Reply-To: <5B3D10F5-9E85-463E-933D-A125843E2B3F@rstudio.com>
References: <CAEKh8uidcEWA4tzmkpQxpF=q7r93qjf-HtiWLfbT91TFGOSzFQ@mail.gmail.com>
 <CAGpovd2jxxWBUxPhMAR6ZPw1+UQzK7BzJVtps10cMBSp=EUdHQ@mail.gmail.com>
 <CALEXWq0ZvGeTbeq-AeYTSTcTwR-TR+gGczqQhWupzB2h0wa+zg@mail.gmail.com>
 <CAGpovd29V3rXSyB_SMBnbKfb3dzAFPF66ck9iU6XPB_+bS+8WQ@mail.gmail.com>
 <CAEKh8ugr0FNryRVmM64pBPxjP=4xLrC0T2OsUD27mSASAyHpVQ@mail.gmail.com>
 <CAD4oTHHgSUr5TfLaTHxAM94E-Sqw9bW6gON7KdW324VEr524Lw@mail.gmail.com>
 <95648642-FCC6-49BE-93D9-8872172946E6@rstudio.com>
 <alpine.DEB.2.21.1910071001350.14507@luke-Latitude-7480>
 <ABABA4EA-88E4-4E22-B256-214050E8720A@rstudio.com>
 <alpine.DEB.2.21.1910071052330.14507@luke-Latitude-7480>
 <5B3D10F5-9E85-463E-933D-A125843E2B3F@rstudio.com>
Message-ID: <alpine.DEB.2.21.1910071430550.14507@luke-Latitude-7480>

Yes you can make my little example work by implementing dynamic
scope with a stack for saving/restoring binding values. Given R's
reflection capabilities and rm() with an envir argument that has its
own issues. If you want to try to get this right and maintain it in
your own packages that is up to you. I can't see the cost/benefit
calculation justifying having it in base.

Best,

luke

On Mon, 7 Oct 2019, Lionel Henry wrote:

> On 7 Oct 2019, at 18:17, Tierney, Luke <luke-tierney at uiowa.edu> wrote:
>
>> Here is a stylized example:
>
> The previous value of the binding should only be restored if it
> existed:
>
> g <- function(x, y) {
>  rlang::scoped_bindings(xx = x, .env = parent.frame())
>  y
>  get("xx") + 10
> }
>
> # Good
> g(1, 2)
> #> [1] 11
>
> # Still good?
> g(1, g(1, 2))
> #> [1] 11
>
>
>> If you play these games whether you get the result you want, or an
>> obvious error, or just the wrong answer depends on argument evaluation
>> order and the like.
>
> I think the surprises are limited because the pattern has stack-like
> semantics. We get in a new context where `.` gains a new meaning, and
> when we exit the previous meaning is restored.
>
> One example where this could lead to unexpected behaviour is trying to
> capture the value of the placeholder in a closure:
>
> f <- function(x) {
>  x %>% {
>    identity(function() .)
>  }
> }
>
> # This makes sense:
> f("A")()
> #> Error: object '.' not found
>
> # This doesn't:
> "B" %>% { f("A")() }
> #> [1] "B"
>
>
>> Not to mention that you would be telling users they are not allowed
>> to use '.' as a variable name for their own purposes or you would be
>> polluting their environment with some other artificial symbol that
>> they would see in debugging.
>
> That's a good point. Debugging allows to move up the call stack before
> the context is exited, so you'd see the last value of `.` in examples
> of nested pipes like `foo %>% bar( f %>% g() )`. That could be confusing.
>
>
>> Anything going in base needs to worry even about artificial cases.
>> Yes, there are things in base that don't meet that standard. No, that
>> is not a reason to add more.
>
> Agreed. What I meant by artificial cases is functions making
> questionable assumptions after peeking into foreign contexts etc.
>
> I'm worried about what happens with important language constructs like
> `<-` and `return()` when code is evaluated in a local context. That
> said, I think binding pipe values to `.` is more important than these
> particular semantics because the placeholder is an obvious binding to
> inspect while debug-stepping through a pipeline. So evaluating in a
> child is probably preferable to giving up the placeholder altogether.
>
> Best,
> Lionel
>

-- 
Luke Tierney
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu


From |uke-t|erney @end|ng |rom u|ow@@edu  Mon Oct  7 22:53:32 2019
From: |uke-t|erney @end|ng |rom u|ow@@edu (Tierney, Luke)
Date: Mon, 7 Oct 2019 20:53:32 +0000
Subject: [Rd] [External] Re:  should base R have a piping operator ?
In-Reply-To: <alpine.DEB.2.21.1910071052330.14507@luke-Latitude-7480>
References: <CAEKh8uidcEWA4tzmkpQxpF=q7r93qjf-HtiWLfbT91TFGOSzFQ@mail.gmail.com>
 <CAGpovd2jxxWBUxPhMAR6ZPw1+UQzK7BzJVtps10cMBSp=EUdHQ@mail.gmail.com>
 <CALEXWq0ZvGeTbeq-AeYTSTcTwR-TR+gGczqQhWupzB2h0wa+zg@mail.gmail.com>
 <CAGpovd29V3rXSyB_SMBnbKfb3dzAFPF66ck9iU6XPB_+bS+8WQ@mail.gmail.com>
 <CAEKh8ugr0FNryRVmM64pBPxjP=4xLrC0T2OsUD27mSASAyHpVQ@mail.gmail.com>
 <CAD4oTHHgSUr5TfLaTHxAM94E-Sqw9bW6gON7KdW324VEr524Lw@mail.gmail.com>
 <95648642-FCC6-49BE-93D9-8872172946E6@rstudio.com>
 <alpine.DEB.2.21.1910071001350.14507@luke-Latitude-7480>
 <ABABA4EA-88E4-4E22-B256-214050E8720A@rstudio.com>
 <alpine.DEB.2.21.1910071052330.14507@luke-Latitude-7480>
Message-ID: <alpine.DEB.2.21.1910071549450.14507@luke-Latitude-7480>

Just for the record, and not that using return() calls like this is
necessarily a good idea, it is possible to make a nested-call-based
pipe that handles return() calls the way you want using delayedAssign.
I've added it to the end of the file on gitlab.

Time to move on to the stuff I've been avoiding ...

Best,

luke

On Mon, 7 Oct 2019, Tierney, Luke wrote:

> On Mon, 7 Oct 2019, Lionel Henry wrote:
>
>>
>>
>>> On 7 Oct 2019, at 17:04, Tierney, Luke <luke-tierney at uiowa.edu> wrote:
>>>
>>>  Think about what happens if an
>>> argument in a pipe stage contains a pipe. (Not completely
>>> unreasonable, e.g. for a left_join).
>>
>> It should work exactly as it does in a local environment.
>>
>> ```
>> `%foo%` <- function(x, y) {
>>  env <- parent.frame()
>>
>>  # Use `:=` to avoid partial matching on .env/.frame
>>  rlang::scoped_bindings(. := x, .env = env)
>>
>>  eval(substitute(y), env)
>> }
>>
>> "A" %foo% {
>>  print(.)
>>  "B" %foo% print(.)
>>  print(.)
>> }
>> #> [1] "A"
>> #> [1] "B"
>> #> [1] "A"
>>
>> print(.)
>> #> Error in print(.) : object '.' not found
>>
>> ```
>>
>> The advantage is that side effects (such as assigning variables or calling
>> `return()`) will occur in the expected environment.
>
> You get the assignment behavior with the nested call approach. (Not
> that doing this is necessarily a good idea).
>
>> I don't see it causing
>> problems except in artificial cases. Am I missing something?
>
> Here is a stylized example:
>
> f <- function(x, y) {
>     assign("xx", x, parent.frame())
>     on.exit(rm(xx, envir = parent.frame()))
>     y
>     get("xx") + 1
> }
>
> ## This is fine:
>> f(1, 2)
> [1] 2
>
> ## This is not:
>> f(1, f(1, 2))
> Error in get("xx") : object 'xx' not found
>
> If you play these games whether you get the result you want, or an
> obvious error, or just the wrong answer depends on argument evaluation
> order and the like. You really don't want to go there. Not to mention
> that you would be telling users they are not allowed to use '.' as a
> variable name for their own purposes or you would be polluting their
> environment with some other artificial symbol that they would see in
> debugging. Just don't.
>
> Anything going in base needs to worry even about artificial cases.
> Yes, there are things in base that don't meet that standard. No, that
> is not a reason to add more.
>
>> I agree that restraining the pipe to a single placeholder (to avoid
>> double evaluation) would be a good design too.
>>
>> I can't access https://gitlab.com/luke-tierney/pipes, it appears to be 404.
>
> Should be able to get there now. Needed to change the visibility ---
> still learning my way around gitlab.
>
> Best,
>
> luke
>
>> Best,
>> Lionel
>>
>>
>
>

-- 
Luke Tierney
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Tue Oct  8 09:01:33 2019
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Tue, 8 Oct 2019 09:01:33 +0200
Subject: [Rd] Wrong explanation on 'ylab' in hist.Rd
In-Reply-To: <36467073.2771795.1570364063916@mail.yahoo.com>
References: <36467073.2771795.1570364063916@mail.yahoo.com>
Message-ID: <23964.13389.55553.884305@stat.math.ethz.ch>

>>>>> suharto anggono--- via R-devel 
>>>>>     on Sun, 6 Oct 2019 12:14:23 +0000 writes:

    > Description of arguments main, xlab, ylab in hist.Rd in current R devel and R patched ends with this.
    > the default \code{ylab} is \code{"Frequency"} iff \code{probability} is true

    > In fact, if 'probability' is true, the histogram doesn't represent frequencies.

    > It should be
    > the default \code{ylab} is \code{"Frequency"} iff \code{freq} is true

yes, of course. <.. blush ..>


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Tue Oct  8 09:23:47 2019
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Tue, 8 Oct 2019 09:23:47 +0200
Subject: [Rd] Strange "no-echo" in place of "slave"
In-Reply-To: <BE5D03DD-25FD-47C7-B92A-29CE99F2BD7A@gmail.com>
References: <2074176959.2758422.1570362954895@mail.yahoo.com>
 <CAD4oTHEo0KqtEiSNhzc9CeEjAmrt3ieHe3=Na2yMsUhMKK37UA@mail.gmail.com>
 <BE5D03DD-25FD-47C7-B92A-29CE99F2BD7A@gmail.com>
Message-ID: <23964.14723.714441.966553@stat.math.ethz.ch>

>>>>> peter dalgaard 
>>>>>     on Sun, 6 Oct 2019 16:13:58 +0200 writes:

    > The first of Suharno's examples can be viewed that way,
    > because R under ESS is not a "slave" in the technical
    > sense, just a situation where you do not want keyboard
    > input to be echoed. "Non-echoing" might have been better
    > language though.

    > The 2nd example really is of the master/slave variety,
    > and to my knowledge nothing to do with echoing, so looks
    > like a search-and-replace oversight. Presumably, "worker
    > thread" (or thereabouts) would be a better replacement.

    > -pd

Yes indeed, thank you,  Suharto, Gabe and Peter.

I've updated the 2nd example (using "worker")

Martin


    >> On 6 Oct 2019, at 14:00 , Gabriel Becker
    >> <gabembecker at gmail.com> wrote:
    >> 
    >> As far as I know, not being involved with the effort at
    >> all, they are removing the term 'slave' and replacing it
    >> with 'no-echo' which is intended to be fully synonmyous
    >> with the meaning of the old 'slave' term.
    >> 
    >> ~G
    >> 
    >> On Sun, Oct 6, 2019 at 10:56 PM suharto_anggono--- via
    >> R-devel < r-devel at r-project.org> wrote:
    >> 
    >>> SVN revision replaces "slave" with "no-echo" in R devel.
    >>> 
    >>> 
    >>> In each of the following, "no-echo" is rather strange to
    >>> me.
    >>> 
    >>> - src/gnuwin32/README.Rterm 3) As a no-echo process for
    >>> ESS mode in NTEmacs with flag --ess.
    >>> 
    >>> - src/library/grDevices/src/qdCocoa.m /* the no-echo
    >>> thread work until this is NO */
    >>>


From c@@rd|@g@bor @end|ng |rom gm@||@com  Wed Oct  9 00:12:37 2019
From: c@@rd|@g@bor @end|ng |rom gm@||@com (=?UTF-8?B?R8OhYm9yIENzw6FyZGk=?=)
Date: Tue, 8 Oct 2019 23:12:37 +0100
Subject: [Rd] \Sexpr{} within \examples{}
Message-ID: <CABtg=K=AUALpjh0J6HjobSoC8gkYO8csix6KFo-Jcz4zazPedw@mail.gmail.com>

Hi all,

I am trying to use \Sexpr{} within \examples{} in the manual, and it
all works fine, except that I get an error and then a NOTE from R CMD
check when it checks for long lines in the manual.

The long line check calls `Rd2txt` with fragment = TRUE, because it
only checks the \usage{} and \examples{} sections, but fragment = TRUE
does not evaluate \Sexpr{}, and then an unevaluated \Sexpr{} is not
allowed in \examples{}. (Well, sometimes, see below.)

Apart from the check NOTE, I would think it could be better to
evaluate the \Sexpr{} in/before the long line check. Otherwise it is
surely not checking the text that'll appear in the manual in the end.

Maybe it would make sense to call prepare_Rd() before the check, to
evaluate the \Sexpr{} calls?

Here is a reproducible example, tools:::.check_Rd_line_widths()
eventually calls Rd2txt() with fragment = TRUE, via
tools:::find_wide_Rd_lines_in_Rd_object, so I'll just call Rd2txt()
directly here, for simplicity:

rd <- "
\\name{foo}
\\title{Title}
\\description{Desc.}
\\examples{
  \\Sexpr[stage=render]{\"# foobar\"}
}"

rd <- tools::parse_Rd(con <- textConnection(rd)); close(con)

tools::Rd2txt(rd)
pos <- which(tools:::RdTags(rd) == "\\examples")
tools::Rd2txt(rd[pos], fragment = TRUE)

#> Examples:
#>
#> Error: 6: Tag \Sexpr not expected in code block

# This fails. After prepare_Rd() it works fine:

rd2 <- tools:::prepare_Rd(rd, stages = "render")
pos2 <- which(tools:::RdTags(rd2) == "\\examples")
tools::Rd2txt(rd2[pos2], fragment = TRUE)

#> Examples:
#>
#>      # foobar

Interestingly, if Rd2txt() is called with the \examples tag only,
instead of the 1-element list consisting of the \examples tag, like
above and in tools:::find_wide_Rd_lines_in_Rd_object, it also works
fine, although it does not render the \Sexpr{}, of course:

tools::Rd2txt(rd[[pos]], fragment = TRUE)

#> \Sexpr[stage=render]{"# foobar"}

Do you think it would make sense to change the line width check, so
\Sexpr{} within \examples{} will not generate a NOTE?

Thank you,
Gabor


From konr@d@rudo|ph @end|ng |rom gm@||@com  Wed Oct  9 21:22:13 2019
From: konr@d@rudo|ph @end|ng |rom gm@||@com (Konrad Rudolph)
Date: Wed, 9 Oct 2019 20:22:13 +0100
Subject: [Rd] S3 lookup rules changed in R 3.6.1
Message-ID: <CAM2gKPZfigRJL1yfyMq-hC3K6QVcswx05NM8HaJ41HBzom4fOQ@mail.gmail.com>

tl;dr: S3 lookup no longer works in custom non-namespace environments as of
R 3.6.1. Is this a bug?

I am implementing S3 dispatch for generic methods in environments that are
not
packages. I am trying to emulate the R package namespace mechanism by
having a
?namespace? environment that defines generics and methods, but only exposes
the
generics themselves, not the methods.

To make S3 lookup work when using the generics, I am using
`registerS3method`.
While this method itself has no extensive documentation, the documentation
of
`UseMethod` contains this relevant passage:

> Namespaces can register methods for generic functions. To support this,
> ?UseMethod? and ?NextMethod? search for methods in two places: in the
> environment in which the generic function is called, and in the
registration
> data base for the environment in which the generic is defined (typically a
> namespace). So methods for a generic function need to be available in the
> environment of the call to the generic, or they must be registered. (It
does
> not matter whether they are visible in the environment in which the
generic is
> defined.) As from R 3.5.0, the registration data base is searched after
the
> top level environment (see ?topenv?) of the calling environment (but
before
> the parents of the top level environment).

This used to work but it stopped working in R 3.6.1 and I cannot figure out
(a)
why, and (b) how to fix it. Unfortunately I am unable to find the relevant
information by reading the R source code, even when ?diff?ing what seem to
be
the only even remotely relevant changes [1].

The R NEWS merely list the following change for R 3.6.0:

> * S3method() directives in ?NAMESPACE? can now also be used to perform
delayed
>   S3 method registration.
> [?]
> * Method dispatch uses more relevant environments when looking up class
>   definitions.

Unfortunately it is not clear to me what exactly this means.

Here?s a minimal example code that works under R 3.5.3 but breaks under
R 3.6.1
(I don?t know about 3.6.0).

```
# Define ?package namespace?:
ns = new.env(parent = .BaseNamespaceEnv)
local(envir = ns, {
    test = function (x) UseMethod('test')
    test.default = function (x) message('test.default')
    test.foo = function (x) message('test.foo')

    .__S3MethodsTable__. = new.env(parent = .BaseNamespaceEnv)
    .__S3MethodsTable__.$test.default = test.default
    .__S3MethodsTable__.$test.foo = test.foo

    # Or, equivalently:
    # registerS3method('test', 'default', test.default)
    # registerS3method('test', 'foo', test.foo)
})

# Expose generic publicly:
test = ns$test

# Usage:
test(1)
test(structure(1, class = 'foo'))
```

Output in R up to 3.5.3:

```
test.default
test.foo
```

Output in R 3.6.1:

```
Error in UseMethod("test") :
  no applicable method for 'test' applied to an object of class
"c('double', 'numeric')"
```

It?s worth noting that the output of `.S3methods` is the same for all R
versions, and from my understanding of its output, this *should* indicate
that
S3 lookup should behave identically, too. Furthermore, lookup via
`getS3method`
succeeds in all R versions, and (again, in my understanding) the logic of
this
function should be identical to the logic of R?s internal S3 dispatch:

```
getS3method('test', 'default')(1)
getS3method('test', 'foo')(1)
```

Conversely, specialising an existing generic from a loaded package works.
E.g.:

```
local(envir = ns, {
    print.foo = function (x) message('print.foo')
    registerS3method('print', 'foo', print.foo)
})

print(structure(1, class = 'foo'))
```

This prints ?print.foo? in all R versions as expected.

So my question is: Why do the `test(?)` calls in R 3.6.1 no longer trigger
S3
method lookup in the generic function?s environment? Is this behaviour by
design
or is it a bug? If it?s by design, why does `getS3method` still use the old
behaviour? And, most importantly, how can I fix my definition of `ns` to
make
S3 dispatch for non-exposed methods work again?

? actually I just found a workaround:

```
ns$.packageName = 'not important'
```

This marks `ns` as a package namespace. To me, the documentation seems to
imply
that this shouldn?t be necessary (and it previously wasn?t). Furthermore,
the
code for `registerS3method` explicitly supports non-package namespace
environments. Unfortunately this workaround is not satisfactory because
pretending that the environment is a package namespace, when it really
isn?t,
might break other things.

[1] See r75273; there?s also r74625, which changes the actual lookup
mechanism
    used by `UseMethod`, but that seems even less relevant, because it is
    disabled unless a specific environment variable is set.

-- 
Konrad Rudolph

	[[alternative HTML version deleted]]


From murdoch@dunc@n @end|ng |rom gm@||@com  Thu Oct 10 00:23:53 2019
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Wed, 9 Oct 2019 18:23:53 -0400
Subject: [Rd] S3 lookup rules changed in R 3.6.1
In-Reply-To: <CAM2gKPZfigRJL1yfyMq-hC3K6QVcswx05NM8HaJ41HBzom4fOQ@mail.gmail.com>
References: <CAM2gKPZfigRJL1yfyMq-hC3K6QVcswx05NM8HaJ41HBzom4fOQ@mail.gmail.com>
Message-ID: <004fe769-cfbd-73e5-a567-cbe7928c1229@gmail.com>

On 09/10/2019 3:22 p.m., Konrad Rudolph wrote:
> tl;dr: S3 lookup no longer works in custom non-namespace environments as of
> R 3.6.1. Is this a bug?

I don't know whether this was intentional or not, but a binary search 
through the svn commits finds that the errors started in this one:

------------------------------------------------------------------------
r75127 | hornik | 2018-08-13 09:58:47 -0400 (Mon, 13 Aug 2018) | 2 lines
Changed paths:
    M /trunk/src/main/objects.c
    M /trunk/tests/reg-tests-1a.R

Have S3 methods lookup by default look for the S3 registry in the topenv
of the generic.
------------------------------------------------------------------------

Duncan Murdoch

> 
> I am implementing S3 dispatch for generic methods in environments that are
> not
> packages. I am trying to emulate the R package namespace mechanism by
> having a
> ?namespace? environment that defines generics and methods, but only exposes
> the
> generics themselves, not the methods.
> 
> To make S3 lookup work when using the generics, I am using
> `registerS3method`.
> While this method itself has no extensive documentation, the documentation
> of
> `UseMethod` contains this relevant passage:
> 
>> Namespaces can register methods for generic functions. To support this,
>> ?UseMethod? and ?NextMethod? search for methods in two places: in the
>> environment in which the generic function is called, and in the
> registration
>> data base for the environment in which the generic is defined (typically a
>> namespace). So methods for a generic function need to be available in the
>> environment of the call to the generic, or they must be registered. (It
> does
>> not matter whether they are visible in the environment in which the
> generic is
>> defined.) As from R 3.5.0, the registration data base is searched after
> the
>> top level environment (see ?topenv?) of the calling environment (but
> before
>> the parents of the top level environment).
> 
> This used to work but it stopped working in R 3.6.1 and I cannot figure out
> (a)
> why, and (b) how to fix it. Unfortunately I am unable to find the relevant
> information by reading the R source code, even when ?diff?ing what seem to
> be
> the only even remotely relevant changes [1].
> 
> The R NEWS merely list the following change for R 3.6.0:
> 
>> * S3method() directives in ?NAMESPACE? can now also be used to perform
> delayed
>>    S3 method registration.
>> [?]
>> * Method dispatch uses more relevant environments when looking up class
>>    definitions.
> 
> Unfortunately it is not clear to me what exactly this means.
> 
> Here?s a minimal example code that works under R 3.5.3 but breaks under
> R 3.6.1
> (I don?t know about 3.6.0).
> 
> ```
> # Define ?package namespace?:
> ns = new.env(parent = .BaseNamespaceEnv)
> local(envir = ns, {
>      test = function (x) UseMethod('test')
>      test.default = function (x) message('test.default')
>      test.foo = function (x) message('test.foo')
> 
>      .__S3MethodsTable__. = new.env(parent = .BaseNamespaceEnv)
>      .__S3MethodsTable__.$test.default = test.default
>      .__S3MethodsTable__.$test.foo = test.foo
> 
>      # Or, equivalently:
>      # registerS3method('test', 'default', test.default)
>      # registerS3method('test', 'foo', test.foo)
> })
> 
> # Expose generic publicly:
> test = ns$test
> 
> # Usage:
> test(1)
> test(structure(1, class = 'foo'))
> ```
> 
> Output in R up to 3.5.3:
> 
> ```
> test.default
> test.foo
> ```
> 
> Output in R 3.6.1:
> 
> ```
> Error in UseMethod("test") :
>    no applicable method for 'test' applied to an object of class
> "c('double', 'numeric')"
> ```
> 
> It?s worth noting that the output of `.S3methods` is the same for all R
> versions, and from my understanding of its output, this *should* indicate
> that
> S3 lookup should behave identically, too. Furthermore, lookup via
> `getS3method`
> succeeds in all R versions, and (again, in my understanding) the logic of
> this
> function should be identical to the logic of R?s internal S3 dispatch:
> 
> ```
> getS3method('test', 'default')(1)
> getS3method('test', 'foo')(1)
> ```
> 
> Conversely, specialising an existing generic from a loaded package works.
> E.g.:
> 
> ```
> local(envir = ns, {
>      print.foo = function (x) message('print.foo')
>      registerS3method('print', 'foo', print.foo)
> })
> 
> print(structure(1, class = 'foo'))
> ```
> 
> This prints ?print.foo? in all R versions as expected.
> 
> So my question is: Why do the `test(?)` calls in R 3.6.1 no longer trigger
> S3
> method lookup in the generic function?s environment? Is this behaviour by
> design
> or is it a bug? If it?s by design, why does `getS3method` still use the old
> behaviour? And, most importantly, how can I fix my definition of `ns` to
> make
> S3 dispatch for non-exposed methods work again?
> 
> ? actually I just found a workaround:
> 
> ```
> ns$.packageName = 'not important'
> ```
> 
> This marks `ns` as a package namespace. To me, the documentation seems to
> imply
> that this shouldn?t be necessary (and it previously wasn?t). Furthermore,
> the
> code for `registerS3method` explicitly supports non-package namespace
> environments. Unfortunately this workaround is not satisfactory because
> pretending that the environment is a package namespace, when it really
> isn?t,
> might break other things.
> 
> [1] See r75273; there?s also r74625, which changes the actual lookup
> mechanism
>      used by `UseMethod`, but that seems even less relevant, because it is
>      disabled unless a specific environment variable is set.
>


From henr|k@bengt@@on @end|ng |rom gm@||@com  Thu Oct 10 07:09:05 2019
From: henr|k@bengt@@on @end|ng |rom gm@||@com (Henrik Bengtsson)
Date: Wed, 9 Oct 2019 22:09:05 -0700
Subject: [Rd] R-specific environment variables: Naming convention?
Message-ID: <CAFDcVCQYRYrsEtE8aVqvbO380US9S-01T39+D89WTQ5DxWUQFg@mail.gmail.com>

In base R, there are lots of environment variables with either prefix
'R_' or '_R_', e.g. R_ENABLE_JIT and _R_RNG_VERSION_.  I always
considered R_* variables to be "public" and _R_*_ ones being
"internal" but realized I don't have a reference for this.  Is this
true, or is there another reason? Is the difference between the two
kinds documented anywhere?

Thank you,

Henrik


From @@nd@|ndhu @end|ng |rom gm@||@com  Wed Oct  9 13:49:20 2019
From: @@nd@|ndhu @end|ng |rom gm@||@com (Sandeep Sindhu)
Date: Wed, 9 Oct 2019 17:19:20 +0530
Subject: [Rd] R plot window repaint and close issue on CentOS7.2
Message-ID: <CANtPyGzgGWikgs3v=bZKu-2W+G2CggwGvLw-dVvYV3=nERT-og@mail.gmail.com>

Hi,

I have embedded R into Qt application (DyLoad R.dll and set bunch of
callbacks e.g  ReadConsole, WriteConsole etc). On CentOS7.2, the plot
command works fine, plots data nicely, but when i try to close it by
clicking on close tool button it doesn't closes. Also, when maximized, the
plot window becomes blank i.e. it doesn't repaint.

I am using R-3.6.0. Pls not that on Windows, it works just fine.

Appreciate any comment

Thanks

Sandy

	[[alternative HTML version deleted]]


From konr@d@rudo|ph @end|ng |rom gm@||@com  Thu Oct 10 09:43:49 2019
From: konr@d@rudo|ph @end|ng |rom gm@||@com (Konrad Rudolph)
Date: Thu, 10 Oct 2019 08:43:49 +0100
Subject: [Rd] S3 lookup rules changed in R 3.6.1
In-Reply-To: <004fe769-cfbd-73e5-a567-cbe7928c1229@gmail.com>
References: <CAM2gKPZfigRJL1yfyMq-hC3K6QVcswx05NM8HaJ41HBzom4fOQ@mail.gmail.com>
 <004fe769-cfbd-73e5-a567-cbe7928c1229@gmail.com>
Message-ID: <CAM2gKPYV3UuCEwQEa9ObR2rSLBW6POjDgupU9XBq=2OYWn0V0g@mail.gmail.com>

Oh, I had missed that that code path is now enabled by default. It?s worth
noting that the commented-out test in that commit also still succeeds if
invoked via `getS3method`. So at the very least there?s now an
inconsistency in the lookup performed by R internally (via `UseMethod`) and
`getS3method`, which is probably unintentional.

I see how the change is beneficial by preventing surprising behaviour in a
corner case. Unfortunately it also breaks at least one published package
[1], and if I understand correctly it no longer conforms to the documented
behaviour (quoted in my initial message), which even explicitly mentions
non-namespace environments.

[1] https://github.com/klmr/modules/issues/147

On Wed, Oct 9, 2019 at 11:23 PM Duncan Murdoch <murdoch.duncan at gmail.com>
wrote:

> On 09/10/2019 3:22 p.m., Konrad Rudolph wrote:
> > tl;dr: S3 lookup no longer works in custom non-namespace environments as
> of
> > R 3.6.1. Is this a bug?
>
> I don't know whether this was intentional or not, but a binary search
> through the svn commits finds that the errors started in this one:
>
> ------------------------------------------------------------------------
> r75127 | hornik | 2018-08-13 09:58:47 -0400 (Mon, 13 Aug 2018) | 2 lines
> Changed paths:
>     M /trunk/src/main/objects.c
>     M /trunk/tests/reg-tests-1a.R
>
> Have S3 methods lookup by default look for the S3 registry in the topenv
> of the generic.
> ------------------------------------------------------------------------
>
> Duncan Murdoch
>
> >
> > I am implementing S3 dispatch for generic methods in environments that
> are
> > not
> > packages. I am trying to emulate the R package namespace mechanism by
> > having a
> > ?namespace? environment that defines generics and methods, but only
> exposes
> > the
> > generics themselves, not the methods.
> >
> > To make S3 lookup work when using the generics, I am using
> > `registerS3method`.
> > While this method itself has no extensive documentation, the
> documentation
> > of
> > `UseMethod` contains this relevant passage:
> >
> >> Namespaces can register methods for generic functions. To support this,
> >> ?UseMethod? and ?NextMethod? search for methods in two places: in the
> >> environment in which the generic function is called, and in the
> > registration
> >> data base for the environment in which the generic is defined
> (typically a
> >> namespace). So methods for a generic function need to be available in
> the
> >> environment of the call to the generic, or they must be registered. (It
> > does
> >> not matter whether they are visible in the environment in which the
> > generic is
> >> defined.) As from R 3.5.0, the registration data base is searched after
> > the
> >> top level environment (see ?topenv?) of the calling environment (but
> > before
> >> the parents of the top level environment).
> >
> > This used to work but it stopped working in R 3.6.1 and I cannot figure
> out
> > (a)
> > why, and (b) how to fix it. Unfortunately I am unable to find the
> relevant
> > information by reading the R source code, even when ?diff?ing what seem
> to
> > be
> > the only even remotely relevant changes [1].
> >
> > The R NEWS merely list the following change for R 3.6.0:
> >
> >> * S3method() directives in ?NAMESPACE? can now also be used to perform
> > delayed
> >>    S3 method registration.
> >> [?]
> >> * Method dispatch uses more relevant environments when looking up class
> >>    definitions.
> >
> > Unfortunately it is not clear to me what exactly this means.
> >
> > Here?s a minimal example code that works under R 3.5.3 but breaks under
> > R 3.6.1
> > (I don?t know about 3.6.0).
> >
> > ```
> > # Define ?package namespace?:
> > ns = new.env(parent = .BaseNamespaceEnv)
> > local(envir = ns, {
> >      test = function (x) UseMethod('test')
> >      test.default = function (x) message('test.default')
> >      test.foo = function (x) message('test.foo')
> >
> >      .__S3MethodsTable__. = new.env(parent = .BaseNamespaceEnv)
> >      .__S3MethodsTable__.$test.default = test.default
> >      .__S3MethodsTable__.$test.foo = test.foo
> >
> >      # Or, equivalently:
> >      # registerS3method('test', 'default', test.default)
> >      # registerS3method('test', 'foo', test.foo)
> > })
> >
> > # Expose generic publicly:
> > test = ns$test
> >
> > # Usage:
> > test(1)
> > test(structure(1, class = 'foo'))
> > ```
> >
> > Output in R up to 3.5.3:
> >
> > ```
> > test.default
> > test.foo
> > ```
> >
> > Output in R 3.6.1:
> >
> > ```
> > Error in UseMethod("test") :
> >    no applicable method for 'test' applied to an object of class
> > "c('double', 'numeric')"
> > ```
> >
> > It?s worth noting that the output of `.S3methods` is the same for all R
> > versions, and from my understanding of its output, this *should* indicate
> > that
> > S3 lookup should behave identically, too. Furthermore, lookup via
> > `getS3method`
> > succeeds in all R versions, and (again, in my understanding) the logic of
> > this
> > function should be identical to the logic of R?s internal S3 dispatch:
> >
> > ```
> > getS3method('test', 'default')(1)
> > getS3method('test', 'foo')(1)
> > ```
> >
> > Conversely, specialising an existing generic from a loaded package works.
> > E.g.:
> >
> > ```
> > local(envir = ns, {
> >      print.foo = function (x) message('print.foo')
> >      registerS3method('print', 'foo', print.foo)
> > })
> >
> > print(structure(1, class = 'foo'))
> > ```
> >
> > This prints ?print.foo? in all R versions as expected.
> >
> > So my question is: Why do the `test(?)` calls in R 3.6.1 no longer
> trigger
> > S3
> > method lookup in the generic function?s environment? Is this behaviour by
> > design
> > or is it a bug? If it?s by design, why does `getS3method` still use the
> old
> > behaviour? And, most importantly, how can I fix my definition of `ns` to
> > make
> > S3 dispatch for non-exposed methods work again?
> >
> > ? actually I just found a workaround:
> >
> > ```
> > ns$.packageName = 'not important'
> > ```
> >
> > This marks `ns` as a package namespace. To me, the documentation seems to
> > imply
> > that this shouldn?t be necessary (and it previously wasn?t). Furthermore,
> > the
> > code for `registerS3method` explicitly supports non-package namespace
> > environments. Unfortunately this workaround is not satisfactory because
> > pretending that the environment is a package namespace, when it really
> > isn?t,
> > might break other things.
> >
> > [1] See r75273; there?s also r74625, which changes the actual lookup
> > mechanism
> >      used by `UseMethod`, but that seems even less relevant, because it
> is
> >      disabled unless a specific environment variable is set.
> >
>
>

-- 
Konrad Rudolph

	[[alternative HTML version deleted]]


From tom@@@k@||ber@ @end|ng |rom gm@||@com  Thu Oct 10 11:00:35 2019
From: tom@@@k@||ber@ @end|ng |rom gm@||@com (Tomas Kalibera)
Date: Thu, 10 Oct 2019 11:00:35 +0200
Subject: [Rd] R-specific environment variables: Naming convention?
In-Reply-To: <CAFDcVCQYRYrsEtE8aVqvbO380US9S-01T39+D89WTQ5DxWUQFg@mail.gmail.com>
References: <CAFDcVCQYRYrsEtE8aVqvbO380US9S-01T39+D89WTQ5DxWUQFg@mail.gmail.com>
Message-ID: <08f627da-acaf-1992-2cb8-a89f65e0d1f4@gmail.com>

In principle, what is documented in Writing R Extensions or in the 
manual pages of the internal help system is "public", including 
environment variables. If it is not documented there, it is not 
"public". In addition, the documentation may say that something is 
experimental, better avoided, etc, so needs to read the context, not 
just "grep" the documentation.

Moreover, it is always better to avoid using environment variables 
whenever possible, even the "public" ones, they may interact with child 
processes one is not aware of and in unexpected ways (including child 
processes spawned by R internally, such as during package checking and 
installation). For the same reason they may be set/re-set/cleared when 
child processes are spawned. They may interact in unexpected ways when 
they impact related things. Debugging issues related to environment 
variables is often hard and time consuming.

If an environment variable name starts with _R, it is an indication that 
it is not "public", but there may be several inconsistencies. One should 
always check the documentation; it is not only about whether the 
variable is "public", but also what is the public behavior - and for 
that one needs the documentation.

Best
Tomas

On 10/10/19 7:09 AM, Henrik Bengtsson wrote:
> In base R, there are lots of environment variables with either prefix
> 'R_' or '_R_', e.g. R_ENABLE_JIT and _R_RNG_VERSION_.  I always
> considered R_* variables to be "public" and _R_*_ ones being
> "internal" but realized I don't have a reference for this.  Is this
> true, or is there another reason? Is the difference between the two
> kinds documented anywhere?
>
> Thank you,
>
> Henrik
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From r@m|ro @end|ng |rom prec|@|onb|o@@@@y@com  Thu Oct 10 23:13:42 2019
From: r@m|ro @end|ng |rom prec|@|onb|o@@@@y@com (Ramiro Barrantes)
Date: Thu, 10 Oct 2019 21:13:42 +0000
Subject: [Rd] How to refer to my package from another package DESCRIPTION
Message-ID: <1EBF248F-6759-4D01-99E9-BE482097AC19@precisionbioassay.com>

Hello, I am developing some packages. Package1 requires Package2, both of which I developed:

Right now Package2 is installed properly here:

/myHome/libraries/Package2

Package1 needs Package2.  Package1 refers to Package2 both in the Roxygen @import field for one of the  functions, and also in the Imports section in the DESCRIPTION

However, when I do ?R CMD check Package1? I get:

* checking loading without being on the library search path ... WARNING
Error: package or namespace load failed for ?Package1? in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]):
 there is no package called ?Package2?

It can?t find it.  However, it does not seem like the correct thing is to have R_LIBS_USER=/myHome/libraries

How should I go about this?

Thanks in advance!



	[[alternative HTML version deleted]]


From murdoch@dunc@n @end|ng |rom gm@||@com  Thu Oct 10 23:41:59 2019
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Thu, 10 Oct 2019 17:41:59 -0400
Subject: [Rd] 
 How to refer to my package from another package DESCRIPTION
In-Reply-To: <1EBF248F-6759-4D01-99E9-BE482097AC19@precisionbioassay.com>
References: <1EBF248F-6759-4D01-99E9-BE482097AC19@precisionbioassay.com>
Message-ID: <9d3665fd-3637-2103-b63e-84d8806f1c66@gmail.com>

On 10/10/2019 5:13 p.m., Ramiro Barrantes wrote:
> Hello, I am developing some packages. Package1 requires Package2, both of which I developed:
> 
> Right now Package2 is installed properly here:
> 
> /myHome/libraries/Package2
> 
> Package1 needs Package2.  Package1 refers to Package2 both in the Roxygen @import field for one of the  functions, and also in the Imports section in the DESCRIPTION
> 
> However, when I do ?R CMD check Package1? I get:
> 
> * checking loading without being on the library search path ... WARNING
> Error: package or namespace load failed for ?Package1? in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]):
>   there is no package called ?Package2?
> 
> It can?t find it.  However, it does not seem like the correct thing is to have R_LIBS_USER=/myHome/libraries

Why not?

Duncan Murdoch

> 
> How should I go about this?
> 
> Thanks in advance!
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From r@m|ro @end|ng |rom prec|@|onb|o@@@@y@com  Fri Oct 11 00:06:51 2019
From: r@m|ro @end|ng |rom prec|@|onb|o@@@@y@com (Ramiro Barrantes)
Date: Thu, 10 Oct 2019 22:06:51 +0000
Subject: [Rd] 
 How to refer to my package from another package DESCRIPTION
In-Reply-To: <9d3665fd-3637-2103-b63e-84d8806f1c66@gmail.com>
References: <1EBF248F-6759-4D01-99E9-BE482097AC19@precisionbioassay.com>
 <9d3665fd-3637-2103-b63e-84d8806f1c66@gmail.com>
Message-ID: <3EC487E3-926C-4D4C-8675-E2F012841244@precisionbioassay.com>

Because it does not work.  It says:

"""
* checking loading without being on the library search path ... WARNING                                             
Error: package or namespace load failed for ?Package1? in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]):                                                                                                    
 there is no package called ?Package2?                                                                                  
Execution halted                                                                                                    
                                                                                                                    
It looks like this package has a loading problem when not on .libPaths:                                             
see the messages for details.                                         
"""

Any ideas?


?On 10/10/19, 5:42 PM, "Duncan Murdoch" <murdoch.duncan at gmail.com> wrote:

    On 10/10/2019 5:13 p.m., Ramiro Barrantes wrote:
    > Hello, I am developing some packages. Package1 requires Package2, both of which I developed:
    > 
    > Right now Package2 is installed properly here:
    > 
    > /myHome/libraries/Package2
    > 
    > Package1 needs Package2.  Package1 refers to Package2 both in the Roxygen @import field for one of the  functions, and also in the Imports section in the DESCRIPTION
    > 
    > However, when I do ?R CMD check Package1? I get:
    > 
    > * checking loading without being on the library search path ... WARNING
    > Error: package or namespace load failed for ?Package1? in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]):
    >   there is no package called ?Package2?
    > 
    > It can?t find it.  However, it does not seem like the correct thing is to have R_LIBS_USER=/myHome/libraries
    
    Why not?
    
    Duncan Murdoch
    
    > 
    > How should I go about this?
    > 
    > Thanks in advance!
    > 
    > 
    > 
    > 	[[alternative HTML version deleted]]
    > 
    > ______________________________________________
    > R-devel at r-project.org mailing list
    > https://stat.ethz.ch/mailman/listinfo/r-devel
    > 
    


From morg@n@em@||box @end|ng |rom gm@||@com  Fri Oct 11 12:44:16 2019
From: morg@n@em@||box @end|ng |rom gm@||@com (Morgan Morgan)
Date: Fri, 11 Oct 2019 11:44:16 +0100
Subject: [Rd] New matrix function
Message-ID: <CAL0QV_OHN1+c2ZMO28v9gB1Y-0c+2zL57bDCgcZse13bZ8--eQ@mail.gmail.com>

Hi All,

I was looking for a function to find a small matrix inside a larger matrix
in R similar to the one described in the following link:

https://www.mathworks.com/matlabcentral/answers/194708-index-a-small-matrix-in-a-larger-matrix

I couldn't find anything.

The above function can be seen as a "generalisation" of the "which"
function as well as the function described in the following post:

https://coolbutuseless.github.io/2018/04/03/finding-a-length-n-needle-in-a-haystack/

Would be possible to add such a function to base R?

I am happy to work with someone from the R core team (if you wish) and
suggest an implementation in C.

Thank you
Best regards,
Morgan

	[[alternative HTML version deleted]]


From murdoch@dunc@n @end|ng |rom gm@||@com  Fri Oct 11 11:45:46 2019
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Fri, 11 Oct 2019 05:45:46 -0400
Subject: [Rd] New matrix function
In-Reply-To: <CAL0QV_OHN1+c2ZMO28v9gB1Y-0c+2zL57bDCgcZse13bZ8--eQ@mail.gmail.com>
References: <CAL0QV_OHN1+c2ZMO28v9gB1Y-0c+2zL57bDCgcZse13bZ8--eQ@mail.gmail.com>
Message-ID: <43cbad8a-af09-0a9d-c66d-2476aee555aa@gmail.com>

On 11/10/2019 6:44 a.m., Morgan Morgan wrote:
> Hi All,
> 
> I was looking for a function to find a small matrix inside a larger matrix
> in R similar to the one described in the following link:
> 
> https://www.mathworks.com/matlabcentral/answers/194708-index-a-small-matrix-in-a-larger-matrix
> 
> I couldn't find anything.
> 
> The above function can be seen as a "generalisation" of the "which"
> function as well as the function described in the following post:
> 
> https://coolbutuseless.github.io/2018/04/03/finding-a-length-n-needle-in-a-haystack/
> 
> Would be possible to add such a function to base R?
> 
> I am happy to work with someone from the R core team (if you wish) and
> suggest an implementation in C.

That seems like it would sometimes be a useful function, and maybe 
someone will point out a package that already contains it.  But if not, 
why would it belong in base R?

Duncan Murdoch


From @pencer@gr@ve@ @end|ng |rom prod@y@e@com  Fri Oct 11 12:04:01 2019
From: @pencer@gr@ve@ @end|ng |rom prod@y@e@com (Spencer Graves)
Date: Fri, 11 Oct 2019 05:04:01 -0500
Subject: [Rd] New matrix function
In-Reply-To: <43cbad8a-af09-0a9d-c66d-2476aee555aa@gmail.com>
References: <CAL0QV_OHN1+c2ZMO28v9gB1Y-0c+2zL57bDCgcZse13bZ8--eQ@mail.gmail.com>
 <43cbad8a-af09-0a9d-c66d-2476aee555aa@gmail.com>
Message-ID: <17e1e4b8-a3d3-bb0a-bfd5-1ae8a62b9009@prodsyse.com>



On 2019-10-11 04:45, Duncan Murdoch wrote:
> On 11/10/2019 6:44 a.m., Morgan Morgan wrote:
>> Hi All,
>>
>> I was looking for a function to find a small matrix inside a larger 
>> matrix
>> in R similar to the one described in the following link:
>>
>> https://www.mathworks.com/matlabcentral/answers/194708-index-a-small-matrix-in-a-larger-matrix 
>>
>>
>> I couldn't find anything.
>>
>> The above function can be seen as a "generalisation" of the "which"
>> function as well as the function described in the following post:
>>
>> https://coolbutuseless.github.io/2018/04/03/finding-a-length-n-needle-in-a-haystack/ 
>>
>>
>> Would be possible to add such a function to base R?
>>
>> I am happy to work with someone from the R core team (if you wish) and
>> suggest an implementation in C.
>
> That seems like it would sometimes be a useful function, and maybe 
> someone will point out a package that already contains it.? But if 
> not, why would it belong in base R?


 ????? The natural thing could be to add it to another existing package.


 ????? A list of different search tools appear in the Wikiversity 
article on "Searching R packages".[1]? I especially like the "sos" 
package, which includes a vignette, [2] but I also use RDocumentation 
and occasionally Rseek.? Google Advanced Search[3] is also very good;? 
I've used that for other things, but not searching for R packages.


 ????? I've had modest luck suggesting additions to other packages if I 
write the function and documentation with good examples that tend to 
ensure quality.? Some maintainers reject my suggestions;? other have 
accepted them.


 ????? Spencer Graves


[1]
https://en.wikiversity.org/wiki/Searching_R_Packages


[2] Caveat:? I wrote both that Wikiversity article and the "sos" 
package, so I'm biased.


[3]
https://www.google.com/advanced_search

>
> Duncan Murdoch
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From morg@n@em@||box @end|ng |rom gm@||@com  Fri Oct 11 17:16:01 2019
From: morg@n@em@||box @end|ng |rom gm@||@com (Morgan Morgan)
Date: Fri, 11 Oct 2019 16:16:01 +0100
Subject: [Rd] New matrix function
In-Reply-To: <43cbad8a-af09-0a9d-c66d-2476aee555aa@gmail.com>
References: <CAL0QV_OHN1+c2ZMO28v9gB1Y-0c+2zL57bDCgcZse13bZ8--eQ@mail.gmail.com>
 <43cbad8a-af09-0a9d-c66d-2476aee555aa@gmail.com>
Message-ID: <CAL0QV_PcL25UttJNJF9ED-xj5z1gBHHSHozG584=WDW6uzUXRw@mail.gmail.com>

On Fri, 11 Oct 2019 10:45 Duncan Murdoch, <murdoch.duncan at gmail.com> wrote:

> On 11/10/2019 6:44 a.m., Morgan Morgan wrote:
> > Hi All,
> >
> > I was looking for a function to find a small matrix inside a larger
> matrix
> > in R similar to the one described in the following link:
> >
> >
> https://www.mathworks.com/matlabcentral/answers/194708-index-a-small-matrix-in-a-larger-matrix
> >
> > I couldn't find anything.
> >
> > The above function can be seen as a "generalisation" of the "which"
> > function as well as the function described in the following post:
> >
> >
> https://coolbutuseless.github.io/2018/04/03/finding-a-length-n-needle-in-a-haystack/
> >
> > Would be possible to add such a function to base R?
> >
> > I am happy to work with someone from the R core team (if you wish) and
> > suggest an implementation in C.
>
> That seems like it would sometimes be a useful function, and maybe
> someone will point out a package that already contains it.  But if not,
> why would it belong in base R?
>

If someone already implemented it, that would great indeed. I think it is a
very general and basic function, hence base R could be a good place for it?

But this is probably not a good reason; maybe someone from the R core team
can shed some light on how they decide whether or not to include a function
in base R?


> Duncan Murdoch
>

	[[alternative HTML version deleted]]


From @zwj|08 @end|ng |rom gm@||@com  Fri Oct 11 14:48:31 2019
From: @zwj|08 @end|ng |rom gm@||@com (Wang Jiefei)
Date: Fri, 11 Oct 2019 08:48:31 -0400
Subject: [Rd] New matrix function
In-Reply-To: <CAL0QV_PcL25UttJNJF9ED-xj5z1gBHHSHozG584=WDW6uzUXRw@mail.gmail.com>
References: <CAL0QV_OHN1+c2ZMO28v9gB1Y-0c+2zL57bDCgcZse13bZ8--eQ@mail.gmail.com>
 <43cbad8a-af09-0a9d-c66d-2476aee555aa@gmail.com>
 <CAL0QV_PcL25UttJNJF9ED-xj5z1gBHHSHozG584=WDW6uzUXRw@mail.gmail.com>
Message-ID: <CAGiFhPN-Zgjkc0g=4sbuSXMg11-9iYWk6ZfDKpJBG4nXv6nG1A@mail.gmail.com>

Hi Morgan,

I think there is a discussion on how developers include a function in base
R in another thread:

https://stat.ethz.ch/pipermail/r-devel/2019-October/078551.html


Best,
Jiefei

On Fri, Oct 11, 2019 at 8:19 AM Morgan Morgan <morgan.emailbox at gmail.com>
wrote:

> On Fri, 11 Oct 2019 10:45 Duncan Murdoch, <murdoch.duncan at gmail.com>
> wrote:
>
> > On 11/10/2019 6:44 a.m., Morgan Morgan wrote:
> > > Hi All,
> > >
> > > I was looking for a function to find a small matrix inside a larger
> > matrix
> > > in R similar to the one described in the following link:
> > >
> > >
> >
> https://www.mathworks.com/matlabcentral/answers/194708-index-a-small-matrix-in-a-larger-matrix
> > >
> > > I couldn't find anything.
> > >
> > > The above function can be seen as a "generalisation" of the "which"
> > > function as well as the function described in the following post:
> > >
> > >
> >
> https://coolbutuseless.github.io/2018/04/03/finding-a-length-n-needle-in-a-haystack/
> > >
> > > Would be possible to add such a function to base R?
> > >
> > > I am happy to work with someone from the R core team (if you wish) and
> > > suggest an implementation in C.
> >
> > That seems like it would sometimes be a useful function, and maybe
> > someone will point out a package that already contains it.  But if not,
> > why would it belong in base R?
> >
>
> If someone already implemented it, that would great indeed. I think it is a
> very general and basic function, hence base R could be a good place for it?
>
> But this is probably not a good reason; maybe someone from the R core team
> can shed some light on how they decide whether or not to include a function
> in base R?
>
>
> > Duncan Murdoch
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From |@wrence@m|ch@e| @end|ng |rom gene@com  Fri Oct 11 14:53:25 2019
From: |@wrence@m|ch@e| @end|ng |rom gene@com (Michael Lawrence)
Date: Fri, 11 Oct 2019 05:53:25 -0700
Subject: [Rd] New matrix function
In-Reply-To: <CAL0QV_PcL25UttJNJF9ED-xj5z1gBHHSHozG584=WDW6uzUXRw@mail.gmail.com>
References: <CAL0QV_OHN1+c2ZMO28v9gB1Y-0c+2zL57bDCgcZse13bZ8--eQ@mail.gmail.com>
 <43cbad8a-af09-0a9d-c66d-2476aee555aa@gmail.com>
 <CAL0QV_PcL25UttJNJF9ED-xj5z1gBHHSHozG584=WDW6uzUXRw@mail.gmail.com>
Message-ID: <CAOQ5NyesrhA_yALGN8=kmO-T8vM5HqDAjiLooJUVLpGceHhp0w@mail.gmail.com>

Thanks for this interesting suggestion, Morgan. While there is no strict
criteria for base R inclusion, one criterion relevant in this case is that
the usefulness of a feature be proven in the package space first.

Michael


On Fri, Oct 11, 2019 at 5:19 AM Morgan Morgan <morgan.emailbox at gmail.com>
wrote:

> On Fri, 11 Oct 2019 10:45 Duncan Murdoch, <murdoch.duncan at gmail.com>
> wrote:
>
> > On 11/10/2019 6:44 a.m., Morgan Morgan wrote:
> > > Hi All,
> > >
> > > I was looking for a function to find a small matrix inside a larger
> > matrix
> > > in R similar to the one described in the following link:
> > >
> > >
> >
> https://www.mathworks.com/matlabcentral/answers/194708-index-a-small-matrix-in-a-larger-matrix
> > >
> > > I couldn't find anything.
> > >
> > > The above function can be seen as a "generalisation" of the "which"
> > > function as well as the function described in the following post:
> > >
> > >
> >
> https://coolbutuseless.github.io/2018/04/03/finding-a-length-n-needle-in-a-haystack/
> > >
> > > Would be possible to add such a function to base R?
> > >
> > > I am happy to work with someone from the R core team (if you wish) and
> > > suggest an implementation in C.
> >
> > That seems like it would sometimes be a useful function, and maybe
> > someone will point out a package that already contains it.  But if not,
> > why would it belong in base R?
> >
>
> If someone already implemented it, that would great indeed. I think it is a
> very general and basic function, hence base R could be a good place for it?
>
> But this is probably not a good reason; maybe someone from the R core team
> can shed some light on how they decide whether or not to include a function
> in base R?
>
>
> > Duncan Murdoch
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


-- 
Michael Lawrence
Scientist, Bioinformatics and Computational Biology
Genentech, A Member of the Roche Group
Office +1 (650) 225-7760
michafla at gene.com

Join Genentech on LinkedIn | Twitter | Facebook | Instagram | YouTube

	[[alternative HTML version deleted]]


From morg@n@em@||box @end|ng |rom gm@||@com  Fri Oct 11 18:53:59 2019
From: morg@n@em@||box @end|ng |rom gm@||@com (Morgan Morgan)
Date: Fri, 11 Oct 2019 17:53:59 +0100
Subject: [Rd] New matrix function
In-Reply-To: <CAOQ5NyesrhA_yALGN8=kmO-T8vM5HqDAjiLooJUVLpGceHhp0w@mail.gmail.com>
References: <CAL0QV_OHN1+c2ZMO28v9gB1Y-0c+2zL57bDCgcZse13bZ8--eQ@mail.gmail.com>
 <43cbad8a-af09-0a9d-c66d-2476aee555aa@gmail.com>
 <CAL0QV_PcL25UttJNJF9ED-xj5z1gBHHSHozG584=WDW6uzUXRw@mail.gmail.com>
 <CAOQ5NyesrhA_yALGN8=kmO-T8vM5HqDAjiLooJUVLpGceHhp0w@mail.gmail.com>
Message-ID: <CAL0QV_Mi_4uL-Ssc8CqtOZGVTfu46KfVbDdZD3TF_cDd-dvXKA@mail.gmail.com>

How do you prove usefulness of a feature?
Do you have an example of a feature that has been added after proving to be
useful in the package space first?

Thank you,
Morgan

On Fri, 11 Oct 2019 13:53 Michael Lawrence, <lawrence.michael at gene.com>
wrote:

> Thanks for this interesting suggestion, Morgan. While there is no strict
> criteria for base R inclusion, one criterion relevant in this case is that
> the usefulness of a feature be proven in the package space first.
>
> Michael
>
>
> On Fri, Oct 11, 2019 at 5:19 AM Morgan Morgan <morgan.emailbox at gmail.com>
> wrote:
>
>> On Fri, 11 Oct 2019 10:45 Duncan Murdoch, <murdoch.duncan at gmail.com>
>> wrote:
>>
>> > On 11/10/2019 6:44 a.m., Morgan Morgan wrote:
>> > > Hi All,
>> > >
>> > > I was looking for a function to find a small matrix inside a larger
>> > matrix
>> > > in R similar to the one described in the following link:
>> > >
>> > >
>> >
>> https://www.mathworks.com/matlabcentral/answers/194708-index-a-small-matrix-in-a-larger-matrix
>> > >
>> > > I couldn't find anything.
>> > >
>> > > The above function can be seen as a "generalisation" of the "which"
>> > > function as well as the function described in the following post:
>> > >
>> > >
>> >
>> https://coolbutuseless.github.io/2018/04/03/finding-a-length-n-needle-in-a-haystack/
>> > >
>> > > Would be possible to add such a function to base R?
>> > >
>> > > I am happy to work with someone from the R core team (if you wish) and
>> > > suggest an implementation in C.
>> >
>> > That seems like it would sometimes be a useful function, and maybe
>> > someone will point out a package that already contains it.  But if not,
>> > why would it belong in base R?
>> >
>>
>> If someone already implemented it, that would great indeed. I think it is
>> a
>> very general and basic function, hence base R could be a good place for
>> it?
>>
>> But this is probably not a good reason; maybe someone from the R core team
>> can shed some light on how they decide whether or not to include a
>> function
>> in base R?
>>
>>
>> > Duncan Murdoch
>> >
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>
>
> --
> Michael Lawrence
> Scientist, Bioinformatics and Computational Biology
> Genentech, A Member of the Roche Group
> Office +1 (650) 225-7760
> michafla at gene.com
>
> Join Genentech on LinkedIn | Twitter | Facebook | Instagram | YouTube
>

	[[alternative HTML version deleted]]


From jor|@mey@ @end|ng |rom gm@||@com  Fri Oct 11 16:27:49 2019
From: jor|@mey@ @end|ng |rom gm@||@com (Joris Meys)
Date: Fri, 11 Oct 2019 16:27:49 +0200
Subject: [Rd] New matrix function
In-Reply-To: <CAL0QV_Mi_4uL-Ssc8CqtOZGVTfu46KfVbDdZD3TF_cDd-dvXKA@mail.gmail.com>
References: <CAL0QV_OHN1+c2ZMO28v9gB1Y-0c+2zL57bDCgcZse13bZ8--eQ@mail.gmail.com>
 <43cbad8a-af09-0a9d-c66d-2476aee555aa@gmail.com>
 <CAL0QV_PcL25UttJNJF9ED-xj5z1gBHHSHozG584=WDW6uzUXRw@mail.gmail.com>
 <CAOQ5NyesrhA_yALGN8=kmO-T8vM5HqDAjiLooJUVLpGceHhp0w@mail.gmail.com>
 <CAL0QV_Mi_4uL-Ssc8CqtOZGVTfu46KfVbDdZD3TF_cDd-dvXKA@mail.gmail.com>
Message-ID: <CAO1zAVaxj-5QRwuJnapjgGDGMAC+=t2CnHRuzXvgXcFb-t809Q@mail.gmail.com>

On Fri, Oct 11, 2019 at 3:55 PM Morgan Morgan <morgan.emailbox at gmail.com>
wrote:

> How do you prove usefulness of a feature?
> Do you have an example of a feature that has been added after proving to be
> useful in the package space first?
>
> Thank you,
> Morgan
>

The parallel package (a base package like utils, stats, ...) was added as a
drop-in replacement of the packages snow and multicore for parallel
computing. That's one example, but sure there's more.

Kind regards
Joris

-- 
Joris Meys
Statistical consultant

Department of Data Analysis and Mathematical Modelling
Ghent University
Coupure Links 653, B-9000 Gent (Belgium)
<https://maps.google.com/?q=Coupure+links+653,%C2%A0B-9000+Gent,%C2%A0Belgium&entry=gmail&source=g>

-----------
Biowiskundedagen 2018-2019
http://www.biowiskundedagen.ugent.be/

-------------------------------
Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php

	[[alternative HTML version deleted]]


From @vr@h@m@@d|er @end|ng |rom gm@||@com  Fri Oct 11 16:30:27 2019
From: @vr@h@m@@d|er @end|ng |rom gm@||@com (Avraham Adler)
Date: Fri, 11 Oct 2019 10:30:27 -0400
Subject: [Rd] New matrix function
In-Reply-To: <CAL0QV_Mi_4uL-Ssc8CqtOZGVTfu46KfVbDdZD3TF_cDd-dvXKA@mail.gmail.com>
References: <CAL0QV_OHN1+c2ZMO28v9gB1Y-0c+2zL57bDCgcZse13bZ8--eQ@mail.gmail.com>
 <43cbad8a-af09-0a9d-c66d-2476aee555aa@gmail.com>
 <CAL0QV_PcL25UttJNJF9ED-xj5z1gBHHSHozG584=WDW6uzUXRw@mail.gmail.com>
 <CAOQ5NyesrhA_yALGN8=kmO-T8vM5HqDAjiLooJUVLpGceHhp0w@mail.gmail.com>
 <CAL0QV_Mi_4uL-Ssc8CqtOZGVTfu46KfVbDdZD3TF_cDd-dvXKA@mail.gmail.com>
Message-ID: <CAL6gwn+cKv=MJzLdqExP749frcPC0w9SR5vP4+YjARa=FzxqZg@mail.gmail.com>

It?s rather difficult. For example, the base R Kendall tau is written with
the naive O(n^2). The much faster O(n log n) implementation was programmed
and is in the pcaPP package. When I say much faster, I mean that my
implementation in Excel VBA was faster than R for 10,000 or so pairs.
R-Core decided not to implement that code, and instead made a note about
the faster implementation living in pcaPP in the help for ?cor?. See [1]
for the 2012 discussion. My point is it?s really really difficult to get
something in Base R. Develop it well, put it in a package, and you have
basically the same result.

Avi

[1] https://stat.ethz.ch/pipermail/r-devel/2012-June/064351.html

On Fri, Oct 11, 2019 at 9:55 AM Morgan Morgan <morgan.emailbox at gmail.com>
wrote:

> How do you prove usefulness of a feature?
> Do you have an example of a feature that has been added after proving to be
> useful in the package space first?
>
> Thank you,
> Morgan
>
> On Fri, 11 Oct 2019 13:53 Michael Lawrence, <lawrence.michael at gene.com>
> wrote:
>
> > Thanks for this interesting suggestion, Morgan. While there is no strict
> > criteria for base R inclusion, one criterion relevant in this case is
> that
> > the usefulness of a feature be proven in the package space first.
> >
> > Michael
> >
> >
> > On Fri, Oct 11, 2019 at 5:19 AM Morgan Morgan <morgan.emailbox at gmail.com
> >
> > wrote:
> >
> >> On Fri, 11 Oct 2019 10:45 Duncan Murdoch, <murdoch.duncan at gmail.com>
> >> wrote:
> >>
> >> > On 11/10/2019 6:44 a.m., Morgan Morgan wrote:
> >> > > Hi All,
> >> > >
> >> > > I was looking for a function to find a small matrix inside a larger
> >> > matrix
> >> > > in R similar to the one described in the following link:
> >> > >
> >> > >
> >> >
> >>
> https://www.mathworks.com/matlabcentral/answers/194708-index-a-small-matrix-in-a-larger-matrix
> >> > >
> >> > > I couldn't find anything.
> >> > >
> >> > > The above function can be seen as a "generalisation" of the "which"
> >> > > function as well as the function described in the following post:
> >> > >
> >> > >
> >> >
> >>
> https://coolbutuseless.github.io/2018/04/03/finding-a-length-n-needle-in-a-haystack/
> >> > >
> >> > > Would be possible to add such a function to base R?
> >> > >
> >> > > I am happy to work with someone from the R core team (if you wish)
> and
> >> > > suggest an implementation in C.
> >> >
> >> > That seems like it would sometimes be a useful function, and maybe
> >> > someone will point out a package that already contains it.  But if
> not,
> >> > why would it belong in base R?
> >> >
> >>
> >> If someone already implemented it, that would great indeed. I think it
> is
> >> a
> >> very general and basic function, hence base R could be a good place for
> >> it?
> >>
> >> But this is probably not a good reason; maybe someone from the R core
> team
> >> can shed some light on how they decide whether or not to include a
> >> function
> >> in base R?
> >>
> >>
> >> > Duncan Murdoch
> >> >
> >>
> >>         [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-devel at r-project.org mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-devel
> >>
> >
> >
> > --
> > Michael Lawrence
> > Scientist, Bioinformatics and Computational Biology
> > Genentech, A Member of the Roche Group
> > Office +1 (650) 225-7760
> > michafla at gene.com
> >
> > Join Genentech on LinkedIn | Twitter | Facebook | Instagram | YouTube
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>
-- 
Sent from Gmail Mobile

	[[alternative HTML version deleted]]


From morg@n@em@||box @end|ng |rom gm@||@com  Fri Oct 11 19:41:17 2019
From: morg@n@em@||box @end|ng |rom gm@||@com (Morgan Morgan)
Date: Fri, 11 Oct 2019 18:41:17 +0100
Subject: [Rd] New matrix function
In-Reply-To: <CAO1zAVaxj-5QRwuJnapjgGDGMAC+=t2CnHRuzXvgXcFb-t809Q@mail.gmail.com>
References: <CAL0QV_OHN1+c2ZMO28v9gB1Y-0c+2zL57bDCgcZse13bZ8--eQ@mail.gmail.com>
 <43cbad8a-af09-0a9d-c66d-2476aee555aa@gmail.com>
 <CAL0QV_PcL25UttJNJF9ED-xj5z1gBHHSHozG584=WDW6uzUXRw@mail.gmail.com>
 <CAOQ5NyesrhA_yALGN8=kmO-T8vM5HqDAjiLooJUVLpGceHhp0w@mail.gmail.com>
 <CAL0QV_Mi_4uL-Ssc8CqtOZGVTfu46KfVbDdZD3TF_cDd-dvXKA@mail.gmail.com>
 <CAO1zAVaxj-5QRwuJnapjgGDGMAC+=t2CnHRuzXvgXcFb-t809Q@mail.gmail.com>
Message-ID: <CAL0QV_OT2TnFmCq98z9cvpK1TfY0NZ8y6JdBTR87yXEZUpHPeQ@mail.gmail.com>

I think you are confusing package and function here. Plus some of the R
Core packages, that you mention, contain functions that should probably be
replaced by functions with better implementation from packages on CRAN.

Best regards
Morgan

On Fri, 11 Oct 2019 15:22 Joris Meys, <jorismeys at gmail.com> wrote:

>
>
> On Fri, Oct 11, 2019 at 3:55 PM Morgan Morgan <morgan.emailbox at gmail.com>
> wrote:
>
>> How do you prove usefulness of a feature?
>> Do you have an example of a feature that has been added after proving to
>> be
>> useful in the package space first?
>>
>> Thank you,
>> Morgan
>>
>
> The parallel package (a base package like utils, stats, ...) was added as
> a drop-in replacement of the packages snow and multicore for parallel
> computing. That's one example, but sure there's more.
>
> Kind regards
> Joris
>
> --
> Joris Meys
> Statistical consultant
>
> Department of Data Analysis and Mathematical Modelling
> Ghent University
> Coupure Links 653, B-9000 Gent (Belgium)
>
> <https://maps.google.com/?q=Coupure+links+653,%C2%A0B-9000+Gent,%C2%A0Belgium&entry=gmail&source=g>
>
> -----------
> Biowiskundedagen 2018-2019
> http://www.biowiskundedagen.ugent.be/
>
> -------------------------------
> Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php
>

	[[alternative HTML version deleted]]


From @vr@h@m@@d|er @end|ng |rom gm@||@com  Fri Oct 11 16:50:56 2019
From: @vr@h@m@@d|er @end|ng |rom gm@||@com (Avraham Adler)
Date: Fri, 11 Oct 2019 10:50:56 -0400
Subject: [Rd] New matrix function
In-Reply-To: <CAL0QV_OT2TnFmCq98z9cvpK1TfY0NZ8y6JdBTR87yXEZUpHPeQ@mail.gmail.com>
References: <CAL0QV_OHN1+c2ZMO28v9gB1Y-0c+2zL57bDCgcZse13bZ8--eQ@mail.gmail.com>
 <43cbad8a-af09-0a9d-c66d-2476aee555aa@gmail.com>
 <CAL0QV_PcL25UttJNJF9ED-xj5z1gBHHSHozG584=WDW6uzUXRw@mail.gmail.com>
 <CAOQ5NyesrhA_yALGN8=kmO-T8vM5HqDAjiLooJUVLpGceHhp0w@mail.gmail.com>
 <CAL0QV_Mi_4uL-Ssc8CqtOZGVTfu46KfVbDdZD3TF_cDd-dvXKA@mail.gmail.com>
 <CAO1zAVaxj-5QRwuJnapjgGDGMAC+=t2CnHRuzXvgXcFb-t809Q@mail.gmail.com>
 <CAL0QV_OT2TnFmCq98z9cvpK1TfY0NZ8y6JdBTR87yXEZUpHPeQ@mail.gmail.com>
Message-ID: <CAL6gwn+NQyYP0_dLnrZ7EKq3UPetkLkRntExhSucC-afmKqcig@mail.gmail.com>

Perhaps. But aren?t you looking to implementation a function which finds a
submatrix? If I?m confused, please accept my apologies.

Avi

On Fri, Oct 11, 2019 at 10:43 AM Morgan Morgan <morgan.emailbox at gmail.com>
wrote:

> I think you are confusing package and function here. Plus some of the R
> Core packages, that you mention, contain functions that should probably be
> replaced by functions with better implementation from packages on CRAN.
>
> Best regards
> Morgan
>
> On Fri, 11 Oct 2019 15:22 Joris Meys, <jorismeys at gmail.com> wrote:
>
> >
> >
> > On Fri, Oct 11, 2019 at 3:55 PM Morgan Morgan <morgan.emailbox at gmail.com
> >
> > wrote:
> >
> >> How do you prove usefulness of a feature?
> >> Do you have an example of a feature that has been added after proving to
> >> be
> >> useful in the package space first?
> >>
> >> Thank you,
> >> Morgan
> >>
> >
> > The parallel package (a base package like utils, stats, ...) was added as
> > a drop-in replacement of the packages snow and multicore for parallel
> > computing. That's one example, but sure there's more.
> >
> > Kind regards
> > Joris
> >
> > --
> > Joris Meys
> > Statistical consultant
> >
> > Department of Data Analysis and Mathematical Modelling
> > Ghent University
> > Coupure Links 653, B-9000 Gent (Belgium)
> >
> > <
> https://maps.google.com/?q=Coupure+links+653,%C2%A0B-9000+Gent,%C2%A0Belgium&entry=gmail&source=g
> >
> >
> > -----------
> > Biowiskundedagen 2018-2019
> > http://www.biowiskundedagen.ugent.be/
> >
> > -------------------------------
> > Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>
-- 
Sent from Gmail Mobile

	[[alternative HTML version deleted]]


From ggrothend|eck @end|ng |rom gm@||@com  Fri Oct 11 16:46:41 2019
From: ggrothend|eck @end|ng |rom gm@||@com (Gabor Grothendieck)
Date: Fri, 11 Oct 2019 10:46:41 -0400
Subject: [Rd] New matrix function
In-Reply-To: <CAL0QV_OHN1+c2ZMO28v9gB1Y-0c+2zL57bDCgcZse13bZ8--eQ@mail.gmail.com>
References: <CAL0QV_OHN1+c2ZMO28v9gB1Y-0c+2zL57bDCgcZse13bZ8--eQ@mail.gmail.com>
Message-ID: <CAP01uRn5KxHwB3o+9E9f2b+i-jAzpvXn-D5eNitqn57-oWVTPw@mail.gmail.com>

Using the example in the link here are two one-liners:

  A <- c(2,3,4,1,2,3,4,1,1,2)
  x <- c(1,2)

  # 1 - zoo
  library(zoo)
  which( rollapply(A, length(x), identical, x, fill = FALSE, align = "left") )
  ## [1] 4 9

  # 2 - Base R using conversion to character
  gregexpr(paste(x, collapse = ""), paste(A, collapse = ""))[[1]]
  ## [1] 4 9
  ...snip ...

On Fri, Oct 11, 2019 at 3:45 AM Morgan Morgan <morgan.emailbox at gmail.com> wrote:
>
> Hi All,
>
> I was looking for a function to find a small matrix inside a larger matrix
> in R similar to the one described in the following link:
>
> https://www.mathworks.com/matlabcentral/answers/194708-index-a-small-matrix-in-a-larger-matrix
>
> I couldn't find anything.
>
> The above function can be seen as a "generalisation" of the "which"
> function as well as the function described in the following post:
>
> https://coolbutuseless.github.io/2018/04/03/finding-a-length-n-needle-in-a-haystack/
>
> Would be possible to add such a function to base R?
>
> I am happy to work with someone from the R core team (if you wish) and
> suggest an implementation in C.
>
> Thank you
> Best regards,
> Morgan
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel



-- 
Statistics & Software Consulting
GKX Group, GKX Associates Inc.
tel: 1-877-GKX-GROUP
email: ggrothendieck at gmail.com


From morg@n@em@||box @end|ng |rom gm@||@com  Fri Oct 11 19:52:22 2019
From: morg@n@em@||box @end|ng |rom gm@||@com (Morgan Morgan)
Date: Fri, 11 Oct 2019 18:52:22 +0100
Subject: [Rd] New matrix function
In-Reply-To: <CAL6gwn+cKv=MJzLdqExP749frcPC0w9SR5vP4+YjARa=FzxqZg@mail.gmail.com>
References: <CAL0QV_OHN1+c2ZMO28v9gB1Y-0c+2zL57bDCgcZse13bZ8--eQ@mail.gmail.com>
 <43cbad8a-af09-0a9d-c66d-2476aee555aa@gmail.com>
 <CAL0QV_PcL25UttJNJF9ED-xj5z1gBHHSHozG584=WDW6uzUXRw@mail.gmail.com>
 <CAOQ5NyesrhA_yALGN8=kmO-T8vM5HqDAjiLooJUVLpGceHhp0w@mail.gmail.com>
 <CAL0QV_Mi_4uL-Ssc8CqtOZGVTfu46KfVbDdZD3TF_cDd-dvXKA@mail.gmail.com>
 <CAL6gwn+cKv=MJzLdqExP749frcPC0w9SR5vP4+YjARa=FzxqZg@mail.gmail.com>
Message-ID: <CAL0QV_N=LiXjw6xD-=+o-PGicAOj02h=KE9TW0id7=280Y6CnQ@mail.gmail.com>

Your answer makes much more sense to me.
I will probably end up adding the function to a package.
Some processes and decisions on how R is developed seems to be obscure to
me.

Thank you
Morgan

On Fri, 11 Oct 2019 15:30 Avraham Adler, <avraham.adler at gmail.com> wrote:

> It?s rather difficult. For example, the base R Kendall tau is written with
> the naive O(n^2). The much faster O(n log n) implementation was programmed
> and is in the pcaPP package. When I say much faster, I mean that my
> implementation in Excel VBA was faster than R for 10,000 or so pairs.
> R-Core decided not to implement that code, and instead made a note about
> the faster implementation living in pcaPP in the help for ?cor?. See [1]
> for the 2012 discussion. My point is it?s really really difficult to get
> something in Base R. Develop it well, put it in a package, and you have
> basically the same result.
>
> Avi
>
> [1] https://stat.ethz.ch/pipermail/r-devel/2012-June/064351.html
>
> On Fri, Oct 11, 2019 at 9:55 AM Morgan Morgan <morgan.emailbox at gmail.com>
> wrote:
>
>> How do you prove usefulness of a feature?
>> Do you have an example of a feature that has been added after proving to
>> be
>> useful in the package space first?
>>
>> Thank you,
>> Morgan
>>
>> On Fri, 11 Oct 2019 13:53 Michael Lawrence, <lawrence.michael at gene.com>
>> wrote:
>>
>> > Thanks for this interesting suggestion, Morgan. While there is no strict
>> > criteria for base R inclusion, one criterion relevant in this case is
>> that
>> > the usefulness of a feature be proven in the package space first.
>> >
>> > Michael
>> >
>> >
>> > On Fri, Oct 11, 2019 at 5:19 AM Morgan Morgan <
>> morgan.emailbox at gmail.com>
>> > wrote:
>> >
>> >> On Fri, 11 Oct 2019 10:45 Duncan Murdoch, <murdoch.duncan at gmail.com>
>> >> wrote:
>> >>
>> >> > On 11/10/2019 6:44 a.m., Morgan Morgan wrote:
>> >> > > Hi All,
>> >> > >
>> >> > > I was looking for a function to find a small matrix inside a larger
>> >> > matrix
>> >> > > in R similar to the one described in the following link:
>> >> > >
>> >> > >
>> >> >
>> >>
>> https://www.mathworks.com/matlabcentral/answers/194708-index-a-small-matrix-in-a-larger-matrix
>> >> > >
>> >> > > I couldn't find anything.
>> >> > >
>> >> > > The above function can be seen as a "generalisation" of the "which"
>> >> > > function as well as the function described in the following post:
>> >> > >
>> >> > >
>> >> >
>> >>
>> https://coolbutuseless.github.io/2018/04/03/finding-a-length-n-needle-in-a-haystack/
>> >> > >
>> >> > > Would be possible to add such a function to base R?
>> >> > >
>> >> > > I am happy to work with someone from the R core team (if you wish)
>> and
>> >> > > suggest an implementation in C.
>> >> >
>> >> > That seems like it would sometimes be a useful function, and maybe
>> >> > someone will point out a package that already contains it.  But if
>> not,
>> >> > why would it belong in base R?
>> >> >
>> >>
>> >> If someone already implemented it, that would great indeed. I think it
>> is
>> >> a
>> >> very general and basic function, hence base R could be a good place for
>> >> it?
>> >>
>> >> But this is probably not a good reason; maybe someone from the R core
>> team
>> >> can shed some light on how they decide whether or not to include a
>> >> function
>> >> in base R?
>> >>
>> >>
>> >> > Duncan Murdoch
>> >> >
>> >>
>> >>         [[alternative HTML version deleted]]
>> >>
>> >> ______________________________________________
>> >> R-devel at r-project.org mailing list
>> >> https://stat.ethz.ch/mailman/listinfo/r-devel
>> >>
>> >
>> >
>> > --
>> > Michael Lawrence
>> > Scientist, Bioinformatics and Computational Biology
>> > Genentech, A Member of the Roche Group
>> > Office +1 (650) 225-7760
>> > michafla at gene.com
>> >
>> > Join Genentech on LinkedIn | Twitter | Facebook | Instagram | YouTube
>> >
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
> --
> Sent from Gmail Mobile
>

	[[alternative HTML version deleted]]


From jor|@mey@ @end|ng |rom gm@||@com  Fri Oct 11 17:00:01 2019
From: jor|@mey@ @end|ng |rom gm@||@com (Joris Meys)
Date: Fri, 11 Oct 2019 17:00:01 +0200
Subject: [Rd] New matrix function
In-Reply-To: <CAL0QV_OT2TnFmCq98z9cvpK1TfY0NZ8y6JdBTR87yXEZUpHPeQ@mail.gmail.com>
References: <CAL0QV_OHN1+c2ZMO28v9gB1Y-0c+2zL57bDCgcZse13bZ8--eQ@mail.gmail.com>
 <43cbad8a-af09-0a9d-c66d-2476aee555aa@gmail.com>
 <CAL0QV_PcL25UttJNJF9ED-xj5z1gBHHSHozG584=WDW6uzUXRw@mail.gmail.com>
 <CAOQ5NyesrhA_yALGN8=kmO-T8vM5HqDAjiLooJUVLpGceHhp0w@mail.gmail.com>
 <CAL0QV_Mi_4uL-Ssc8CqtOZGVTfu46KfVbDdZD3TF_cDd-dvXKA@mail.gmail.com>
 <CAO1zAVaxj-5QRwuJnapjgGDGMAC+=t2CnHRuzXvgXcFb-t809Q@mail.gmail.com>
 <CAL0QV_OT2TnFmCq98z9cvpK1TfY0NZ8y6JdBTR87yXEZUpHPeQ@mail.gmail.com>
Message-ID: <CAO1zAVYwRmR2m=i=ez2r=x+hFT7Nra6ujn+y=C5AmQmDDg1zqA@mail.gmail.com>

As a package is a collection of functions, and as "base" is not used to
refer to the base package but to all packages that form "base R" (so utils,
graphics, stats, methods, parallel, ...), the functions in the parallel
package are one example of functions incorporated in "base R" from a
package. Actually two, because the functions in parallel are based on both
snow and multicore.

This has no relation to changes in implementation (which often don't happen
due to back-compatibility issues in edge cases).

Kind regards
Joris

On Fri, Oct 11, 2019 at 4:42 PM Morgan Morgan <morgan.emailbox at gmail.com>
wrote:

> I think you are confusing package and function here. Plus some of the R
> Core packages, that you mention, contain functions that should probably be
> replaced by functions with better implementation from packages on CRAN.
>
> Best regards
> Morgan
>
> On Fri, 11 Oct 2019 15:22 Joris Meys, <jorismeys at gmail.com> wrote:
>
>>
>>
>> On Fri, Oct 11, 2019 at 3:55 PM Morgan Morgan <morgan.emailbox at gmail.com>
>> wrote:
>>
>>> How do you prove usefulness of a feature?
>>> Do you have an example of a feature that has been added after proving to
>>> be
>>> useful in the package space first?
>>>
>>> Thank you,
>>> Morgan
>>>
>>
>> The parallel package (a base package like utils, stats, ...) was added as
>> a drop-in replacement of the packages snow and multicore for parallel
>> computing. That's one example, but sure there's more.
>>
>> Kind regards
>> Joris
>>
>> --
>> Joris Meys
>> Statistical consultant
>>
>> Department of Data Analysis and Mathematical Modelling
>> Ghent University
>> Coupure Links 653, B-9000 Gent (Belgium)
>>
>> <https://maps.google.com/?q=Coupure+links+653,%C2%A0B-9000+Gent,%C2%A0Belgium&entry=gmail&source=g>
>>
>> -----------
>> Biowiskundedagen 2018-2019
>> http://www.biowiskundedagen.ugent.be/
>>
>> -------------------------------
>> Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php
>>
>

-- 
Joris Meys
Statistical consultant

Department of Data Analysis and Mathematical Modelling
Ghent University
Coupure Links 653, B-9000 Gent (Belgium)
<https://maps.google.com/?q=Coupure+links+653,%C2%A0B-9000+Gent,%C2%A0Belgium&entry=gmail&source=g>

-----------
Biowiskundedagen 2018-2019
http://www.biowiskundedagen.ugent.be/

-------------------------------
Disclaimer : http://helpdesk.ugent.be/e-maildisclaimer.php

	[[alternative HTML version deleted]]


From ggrothend|eck @end|ng |rom gm@||@com  Fri Oct 11 16:57:24 2019
From: ggrothend|eck @end|ng |rom gm@||@com (Gabor Grothendieck)
Date: Fri, 11 Oct 2019 10:57:24 -0400
Subject: [Rd] New matrix function
In-Reply-To: <CAP01uRn5KxHwB3o+9E9f2b+i-jAzpvXn-D5eNitqn57-oWVTPw@mail.gmail.com>
References: <CAL0QV_OHN1+c2ZMO28v9gB1Y-0c+2zL57bDCgcZse13bZ8--eQ@mail.gmail.com>
 <CAP01uRn5KxHwB3o+9E9f2b+i-jAzpvXn-D5eNitqn57-oWVTPw@mail.gmail.com>
Message-ID: <CAP01uRm73EP-RcL+3s7QQiDfEfEOzZwOx_Jp84qhU3NvoF+_+g@mail.gmail.com>

Also note that the functionality discussed could be regarded as a generalization
of matrix multiplication where *  and + are general functions and in this case
we have * replaced by == and + replaced by &.

On Fri, Oct 11, 2019 at 10:46 AM Gabor Grothendieck
<ggrothendieck at gmail.com> wrote:
>
> Using the example in the link here are two one-liners:
>
>   A <- c(2,3,4,1,2,3,4,1,1,2)
>   x <- c(1,2)
>
>   # 1 - zoo
>   library(zoo)
>   which( rollapply(A, length(x), identical, x, fill = FALSE, align = "left") )
>   ## [1] 4 9
>
>   # 2 - Base R using conversion to character
>   gregexpr(paste(x, collapse = ""), paste(A, collapse = ""))[[1]]
>   ## [1] 4 9
>   ...snip ...
>
> On Fri, Oct 11, 2019 at 3:45 AM Morgan Morgan <morgan.emailbox at gmail.com> wrote:
> >
> > Hi All,
> >
> > I was looking for a function to find a small matrix inside a larger matrix
> > in R similar to the one described in the following link:
> >
> > https://www.mathworks.com/matlabcentral/answers/194708-index-a-small-matrix-in-a-larger-matrix
> >
> > I couldn't find anything.
> >
> > The above function can be seen as a "generalisation" of the "which"
> > function as well as the function described in the following post:
> >
> > https://coolbutuseless.github.io/2018/04/03/finding-a-length-n-needle-in-a-haystack/
> >
> > Would be possible to add such a function to base R?
> >
> > I am happy to work with someone from the R core team (if you wish) and
> > suggest an implementation in C.
> >
> > Thank you
> > Best regards,
> > Morgan
> >
> >         [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
>
>
>
> --
> Statistics & Software Consulting
> GKX Group, GKX Associates Inc.
> tel: 1-877-GKX-GROUP
> email: ggrothendieck at gmail.com



-- 
Statistics & Software Consulting
GKX Group, GKX Associates Inc.
tel: 1-877-GKX-GROUP
email: ggrothendieck at gmail.com


From ggrothend|eck @end|ng |rom gm@||@com  Fri Oct 11 17:04:43 2019
From: ggrothend|eck @end|ng |rom gm@||@com (Gabor Grothendieck)
Date: Fri, 11 Oct 2019 11:04:43 -0400
Subject: [Rd] New matrix function
In-Reply-To: <CAP01uRm73EP-RcL+3s7QQiDfEfEOzZwOx_Jp84qhU3NvoF+_+g@mail.gmail.com>
References: <CAL0QV_OHN1+c2ZMO28v9gB1Y-0c+2zL57bDCgcZse13bZ8--eQ@mail.gmail.com>
 <CAP01uRn5KxHwB3o+9E9f2b+i-jAzpvXn-D5eNitqn57-oWVTPw@mail.gmail.com>
 <CAP01uRm73EP-RcL+3s7QQiDfEfEOzZwOx_Jp84qhU3NvoF+_+g@mail.gmail.com>
Message-ID: <CAP01uR=9F9ZyhaSVkwz-cQrDuXbxnTD-YMebdZvR_B_o6L4cpg@mail.gmail.com>

I pressed return too soon.

If we had such a multiply then

   which(embed(A, x) %==.&% reverse(x))

On Fri, Oct 11, 2019 at 10:57 AM Gabor Grothendieck
<ggrothendieck at gmail.com> wrote:
>
> Also note that the functionality discussed could be regarded as a generalization
> of matrix multiplication where *  and + are general functions and in this case
> we have * replaced by == and + replaced by &.
>
> On Fri, Oct 11, 2019 at 10:46 AM Gabor Grothendieck
> <ggrothendieck at gmail.com> wrote:
> >
> > Using the example in the link here are two one-liners:
> >
> >   A <- c(2,3,4,1,2,3,4,1,1,2)
> >   x <- c(1,2)
> >
> >   # 1 - zoo
> >   library(zoo)
> >   which( rollapply(A, length(x), identical, x, fill = FALSE, align = "left") )
> >   ## [1] 4 9
> >
> >   # 2 - Base R using conversion to character
> >   gregexpr(paste(x, collapse = ""), paste(A, collapse = ""))[[1]]
> >   ## [1] 4 9
> >   ...snip ...
> >
> > On Fri, Oct 11, 2019 at 3:45 AM Morgan Morgan <morgan.emailbox at gmail.com> wrote:
> > >
> > > Hi All,
> > >
> > > I was looking for a function to find a small matrix inside a larger matrix
> > > in R similar to the one described in the following link:
> > >
> > > https://www.mathworks.com/matlabcentral/answers/194708-index-a-small-matrix-in-a-larger-matrix
> > >
> > > I couldn't find anything.
> > >
> > > The above function can be seen as a "generalisation" of the "which"
> > > function as well as the function described in the following post:
> > >
> > > https://coolbutuseless.github.io/2018/04/03/finding-a-length-n-needle-in-a-haystack/
> > >
> > > Would be possible to add such a function to base R?
> > >
> > > I am happy to work with someone from the R core team (if you wish) and
> > > suggest an implementation in C.
> > >
> > > Thank you
> > > Best regards,
> > > Morgan
> > >
> > >         [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-devel at r-project.org mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-devel
> >
> >
> >
> > --
> > Statistics & Software Consulting
> > GKX Group, GKX Associates Inc.
> > tel: 1-877-GKX-GROUP
> > email: ggrothendieck at gmail.com
>
>
>
> --
> Statistics & Software Consulting
> GKX Group, GKX Associates Inc.
> tel: 1-877-GKX-GROUP
> email: ggrothendieck at gmail.com



-- 
Statistics & Software Consulting
GKX Group, GKX Associates Inc.
tel: 1-877-GKX-GROUP
email: ggrothendieck at gmail.com


From hp@ge@ @end|ng |rom |redhutch@org  Fri Oct 11 20:39:04 2019
From: hp@ge@ @end|ng |rom |redhutch@org (Pages, Herve)
Date: Fri, 11 Oct 2019 18:39:04 +0000
Subject: [Rd] New matrix function
In-Reply-To: <CAL0QV_OHN1+c2ZMO28v9gB1Y-0c+2zL57bDCgcZse13bZ8--eQ@mail.gmail.com>
References: <CAL0QV_OHN1+c2ZMO28v9gB1Y-0c+2zL57bDCgcZse13bZ8--eQ@mail.gmail.com>
Message-ID: <65fd0c92-3711-cb7e-09cf-35f9896663b8@fredhutch.org>

Has someone looked into the image processing area for this? That sounds 
a little bit too high-level for base R to me (and I would be surprised 
if any mainstream programming language had this kind of functionality 
built-in).

H.

On 10/11/19 03:44, Morgan Morgan wrote:
> Hi All,
> 
> I was looking for a function to find a small matrix inside a larger matrix
> in R similar to the one described in the following link:
> 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__www.mathworks.com_matlabcentral_answers_194708-2Dindex-2Da-2Dsmall-2Dmatrix-2Din-2Da-2Dlarger-2Dmatrix&d=DwICAg&c=eRAMFD45gAfqt84VtBcfhQ&r=BK7q3XeAvimeWdGbWY_wJYbW0WYiZvSXAJJKaaPhzWA&m=v96tqHMO3CLNBS7KTmdshM371i6W_v8_2H5bdVy_KHo&s=9Eu0WySIEzrWuYXFhwhHETpZQzi6hHLd84DZsbZsXYY&e=
> 
> I couldn't find anything.
> 
> The above function can be seen as a "generalisation" of the "which"
> function as well as the function described in the following post:
> 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__coolbutuseless.github.io_2018_04_03_finding-2Da-2Dlength-2Dn-2Dneedle-2Din-2Da-2Dhaystack_&d=DwICAg&c=eRAMFD45gAfqt84VtBcfhQ&r=BK7q3XeAvimeWdGbWY_wJYbW0WYiZvSXAJJKaaPhzWA&m=v96tqHMO3CLNBS7KTmdshM371i6W_v8_2H5bdVy_KHo&s=qZ3SJ8t8zEDA-em4WT7gBmN66qvvCKKKXRJunoF6P3k&e=
> 
> Would be possible to add such a function to base R?
> 
> I am happy to work with someone from the R core team (if you wish) and
> suggest an implementation in C.
> 
> Thank you
> Best regards,
> Morgan
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Ddevel&d=DwICAg&c=eRAMFD45gAfqt84VtBcfhQ&r=BK7q3XeAvimeWdGbWY_wJYbW0WYiZvSXAJJKaaPhzWA&m=v96tqHMO3CLNBS7KTmdshM371i6W_v8_2H5bdVy_KHo&s=tyVSs9EYVBd_dmVm1LSC23GhUzbBv8ULvtsveo-COoU&e=
> 

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fredhutch.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319

From ggrothend|eck @end|ng |rom gm@||@com  Sat Oct 12 00:35:27 2019
From: ggrothend|eck @end|ng |rom gm@||@com (Gabor Grothendieck)
Date: Fri, 11 Oct 2019 18:35:27 -0400
Subject: [Rd] New matrix function
In-Reply-To: <65fd0c92-3711-cb7e-09cf-35f9896663b8@fredhutch.org>
References: <CAL0QV_OHN1+c2ZMO28v9gB1Y-0c+2zL57bDCgcZse13bZ8--eQ@mail.gmail.com>
 <65fd0c92-3711-cb7e-09cf-35f9896663b8@fredhutch.org>
Message-ID: <CAP01uRkmkD+yrZVwAtgR-ZcvVbGHXmtf4cU5-mA4+31-zV8Xzg@mail.gmail.com>

The link you posted used the same inputs as in my example. If that is
not what you meant maybe
a different example is needed.
Regards.

On Fri, Oct 11, 2019 at 2:39 PM Pages, Herve <hpages at fredhutch.org> wrote:
>
> Has someone looked into the image processing area for this? That sounds
> a little bit too high-level for base R to me (and I would be surprised
> if any mainstream programming language had this kind of functionality
> built-in).
>
> H.
>
> On 10/11/19 03:44, Morgan Morgan wrote:
> > Hi All,
> >
> > I was looking for a function to find a small matrix inside a larger matrix
> > in R similar to the one described in the following link:
> >
> > https://urldefense.proofpoint.com/v2/url?u=https-3A__www.mathworks.com_matlabcentral_answers_194708-2Dindex-2Da-2Dsmall-2Dmatrix-2Din-2Da-2Dlarger-2Dmatrix&d=DwICAg&c=eRAMFD45gAfqt84VtBcfhQ&r=BK7q3XeAvimeWdGbWY_wJYbW0WYiZvSXAJJKaaPhzWA&m=v96tqHMO3CLNBS7KTmdshM371i6W_v8_2H5bdVy_KHo&s=9Eu0WySIEzrWuYXFhwhHETpZQzi6hHLd84DZsbZsXYY&e=
> >
> > I couldn't find anything.
> >
> > The above function can be seen as a "generalisation" of the "which"
> > function as well as the function described in the following post:
> >
> > https://urldefense.proofpoint.com/v2/url?u=https-3A__coolbutuseless.github.io_2018_04_03_finding-2Da-2Dlength-2Dn-2Dneedle-2Din-2Da-2Dhaystack_&d=DwICAg&c=eRAMFD45gAfqt84VtBcfhQ&r=BK7q3XeAvimeWdGbWY_wJYbW0WYiZvSXAJJKaaPhzWA&m=v96tqHMO3CLNBS7KTmdshM371i6W_v8_2H5bdVy_KHo&s=qZ3SJ8t8zEDA-em4WT7gBmN66qvvCKKKXRJunoF6P3k&e=
> >
> > Would be possible to add such a function to base R?
> >
> > I am happy to work with someone from the R core team (if you wish) and
> > suggest an implementation in C.
> >
> > Thank you
> > Best regards,
> > Morgan
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Ddevel&d=DwICAg&c=eRAMFD45gAfqt84VtBcfhQ&r=BK7q3XeAvimeWdGbWY_wJYbW0WYiZvSXAJJKaaPhzWA&m=v96tqHMO3CLNBS7KTmdshM371i6W_v8_2H5bdVy_KHo&s=tyVSs9EYVBd_dmVm1LSC23GhUzbBv8ULvtsveo-COoU&e=
> >
>
> --
> Herv? Pag?s
>
> Program in Computational Biology
> Division of Public Health Sciences
> Fred Hutchinson Cancer Research Center
> 1100 Fairview Ave. N, M1-B514
> P.O. Box 19024
> Seattle, WA 98109-1024
>
> E-mail: hpages at fredhutch.org
> Phone:  (206) 667-5791
> Fax:    (206) 667-1319
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel



-- 
Statistics & Software Consulting
GKX Group, GKX Associates Inc.
tel: 1-877-GKX-GROUP
email: ggrothendieck at gmail.com


From morg@n@em@||box @end|ng |rom gm@||@com  Sat Oct 12 05:14:28 2019
From: morg@n@em@||box @end|ng |rom gm@||@com (Morgan Morgan)
Date: Sat, 12 Oct 2019 04:14:28 +0100
Subject: [Rd] New matrix function
In-Reply-To: <CAP01uRkmkD+yrZVwAtgR-ZcvVbGHXmtf4cU5-mA4+31-zV8Xzg@mail.gmail.com>
References: <CAL0QV_OHN1+c2ZMO28v9gB1Y-0c+2zL57bDCgcZse13bZ8--eQ@mail.gmail.com>
 <65fd0c92-3711-cb7e-09cf-35f9896663b8@fredhutch.org>
 <CAP01uRkmkD+yrZVwAtgR-ZcvVbGHXmtf4cU5-mA4+31-zV8Xzg@mail.gmail.com>
Message-ID: <CAL0QV_Pn3zinBDi32Q=k95yjtfehZ4OCiPH7W5BpG1cmoSDB7Q@mail.gmail.com>

Basically the problem is to find the position of a submatrix inside a
larger matrix. Here are some links describing the problem:

https://stackoverflow.com/questions/10529278/fastest-way-to-find-a-m-x-n-submatrix-in-m-x-n-matrix

https://stackoverflow.com/questions/16750739/find-a-matrix-in-a-big-matrix

Best
Morgan

On Fri, 11 Oct 2019 23:36 Gabor Grothendieck, <ggrothendieck at gmail.com>
wrote:

> The link you posted used the same inputs as in my example. If that is
> not what you meant maybe
> a different example is needed.
> Regards.
>
> On Fri, Oct 11, 2019 at 2:39 PM Pages, Herve <hpages at fredhutch.org> wrote:
> >
> > Has someone looked into the image processing area for this? That sounds
> > a little bit too high-level for base R to me (and I would be surprised
> > if any mainstream programming language had this kind of functionality
> > built-in).
> >
> > H.
> >
> > On 10/11/19 03:44, Morgan Morgan wrote:
> > > Hi All,
> > >
> > > I was looking for a function to find a small matrix inside a larger
> matrix
> > > in R similar to the one described in the following link:
> > >
> > >
> https://urldefense.proofpoint.com/v2/url?u=https-3A__www.mathworks.com_matlabcentral_answers_194708-2Dindex-2Da-2Dsmall-2Dmatrix-2Din-2Da-2Dlarger-2Dmatrix&d=DwICAg&c=eRAMFD45gAfqt84VtBcfhQ&r=BK7q3XeAvimeWdGbWY_wJYbW0WYiZvSXAJJKaaPhzWA&m=v96tqHMO3CLNBS7KTmdshM371i6W_v8_2H5bdVy_KHo&s=9Eu0WySIEzrWuYXFhwhHETpZQzi6hHLd84DZsbZsXYY&e=
> > >
> > > I couldn't find anything.
> > >
> > > The above function can be seen as a "generalisation" of the "which"
> > > function as well as the function described in the following post:
> > >
> > >
> https://urldefense.proofpoint.com/v2/url?u=https-3A__coolbutuseless.github.io_2018_04_03_finding-2Da-2Dlength-2Dn-2Dneedle-2Din-2Da-2Dhaystack_&d=DwICAg&c=eRAMFD45gAfqt84VtBcfhQ&r=BK7q3XeAvimeWdGbWY_wJYbW0WYiZvSXAJJKaaPhzWA&m=v96tqHMO3CLNBS7KTmdshM371i6W_v8_2H5bdVy_KHo&s=qZ3SJ8t8zEDA-em4WT7gBmN66qvvCKKKXRJunoF6P3k&e=
> > >
> > > Would be possible to add such a function to base R?
> > >
> > > I am happy to work with someone from the R core team (if you wish) and
> > > suggest an implementation in C.
> > >
> > > Thank you
> > > Best regards,
> > > Morgan
> > >
> > >       [[alternative HTML version deleted]]
> > >
> > > ______________________________________________
> > > R-devel at r-project.org mailing list
> > >
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Ddevel&d=DwICAg&c=eRAMFD45gAfqt84VtBcfhQ&r=BK7q3XeAvimeWdGbWY_wJYbW0WYiZvSXAJJKaaPhzWA&m=v96tqHMO3CLNBS7KTmdshM371i6W_v8_2H5bdVy_KHo&s=tyVSs9EYVBd_dmVm1LSC23GhUzbBv8ULvtsveo-COoU&e=
> > >
> >
> > --
> > Herv? Pag?s
> >
> > Program in Computational Biology
> > Division of Public Health Sciences
> > Fred Hutchinson Cancer Research Center
> > 1100 Fairview Ave. N, M1-B514
> > P.O. Box 19024
> > Seattle, WA 98109-1024
> >
> > E-mail: hpages at fredhutch.org
> > Phone:  (206) 667-5791
> > Fax:    (206) 667-1319
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
>
>
>
> --
> Statistics & Software Consulting
> GKX Group, GKX Associates Inc.
> tel: 1-877-GKX-GROUP
> email: ggrothendieck at gmail.com
>

	[[alternative HTML version deleted]]


From j@goreck| @end|ng |rom w|t@edu@p|  Thu Oct 17 22:51:52 2019
From: j@goreck| @end|ng |rom w|t@edu@p| (Jan Gorecki)
Date: Thu, 17 Oct 2019 22:51:52 +0200
Subject: [Rd] contrib.url and R-devel numbering 3.7 vs 4.0
Message-ID: <CAOO9MKWQ5-wgrsV2PeW9iH-e=DbhRh7C=oT-UWzHiS1RGcVx-A@mail.gmail.com>

Dear R-devel,

Due to the numbering change of R-devel from 3.7 to 4.0 there seems to
be an issue happening in a helper function contrib.url, maybe some
others too.
When running on a Windows r77294 (2019-10-15)

    contrib.url("https://cloud.r-project.org", type="binary")
    #[1] "https://cloud.r-project.org/bin/windows/contrib/4.0"

Resulting string is actually not a the proper one. As of now CRAN
still uses 3.7.
It is not clear if migration to 4.0 urls will happen or R-devel CRAN
will stay as 3.7.

Anyway I would like to request, again, to consider my suggestion from
May 2018 that makes contrib.url more flexible, allowing to handle
issue discussed in this email.
https://bugs.r-project.org/bugzilla/show_bug.cgi?id=17420

Best Regards,
Jan Gorecki


From g@bembecker @end|ng |rom gm@||@com  Fri Oct 18 20:59:52 2019
From: g@bembecker @end|ng |rom gm@||@com (Gabriel Becker)
Date: Fri, 18 Oct 2019 11:59:52 -0700
Subject: [Rd] 
 head.matrix can return 1000s of columns -- limit to n or add
 new argument?
In-Reply-To: <23937.1791.389379.103733@stat.math.ethz.ch>
References: <CAPRVBcxsAkcgr3mDc4skTe8DTxJaYe2xyhk3QMk3odW1MGcyNA@mail.gmail.com>
 <CAB8pepxS2mRtd+522OwjixnM6ZjYxb+X0dc4vMGn6QxdFYBmVQ@mail.gmail.com>
 <CAD4oTHH=LpeEj6h6xYdOGsreS-GDCGdgJV5wpaUR3Pjuh5_M1w@mail.gmail.com>
 <CAPRVBczYZHO3JaFVfNeNNV0-YMRvX3CUYhUvLCjophDEoP+XgQ@mail.gmail.com>
 <23935.14449.965488.684663@stat.math.ethz.ch>
 <CAPRVBcwMY=GAVfFeCwowQsxP31Y=_B2ducosThN+8V91WESB8Q@mail.gmail.com>
 <20449_1568703158_x8H6qIDG009669_f8285123-371f-5d4b-1281-6cc085dba122@fredhutch.org>
 <22204_1568723392_x8HCSG3F015456_674779E0-B1F6-4DC2-9F1C-D5D48B044B66@mcmaster.ca>
 <AD725242-0FB7-4004-9F16-304E345A363A@mcmaster.ca>
 <23937.1791.389379.103733@stat.math.ethz.ch>
Message-ID: <CAD4oTHF_e4rtHu0S5bUu38+kQSEHXwun3Sv9-CbK+xN9tSc2hA@mail.gmail.com>

Hi Martin et al.

Sorry for not getting back onto this sooner. I've been pretty well buried
under travel plus being sick for a bit, but I will be happy to roll up a
patch for this, including documentation and put it into a wishlist item.

I'll aim to do that at some point next week.

Thanks @Martin Maechler <maechler at stat.math.ethz.ch> for engaging with us
and being willing to consider the patch.

Best,
~G

On Tue, Sep 17, 2019 at 9:17 AM Martin Maechler <maechler at stat.math.ethz.ch>
wrote:

> >>>>> Fox, John
> >>>>>     on Tue, 17 Sep 2019 12:32:13 +0000 writes:
>
>     > Dear Herve,
>     > Sorry, I should have said "matrices" rather than "data frames" --
> brief() has methods for both.
>
>     > Best,
>     > John
>
>     > -----------------------------
>     > John Fox, Professor Emeritus
>     > McMaster University
>     > Hamilton, Ontario, Canada
>     > Web: http::/socserv.mcmaster.ca/jfox
>
>     >> On Sep 17, 2019, at 8:29 AM, Fox, John <jfox at mcmaster.ca> wrote:
>     >>
>     >> Dear Herve,
>     >>
>     >> The brief() generic function in the car package does something very
> similar to that for data frames (and has methods for other classes of
> objects as well).
>     >>
>     >> Best,
>     >> John
>     >>
>     >> -----------------------------
>     >> John Fox, Professor Emeritus
>     >> McMaster University
>     >> Hamilton, Ontario, Canada
>     >> Web: http::/socserv.mcmaster.ca/jfox
>     >>
>     >>> On Sep 17, 2019, at 2:52 AM, Pages, Herve <hpages at fredhutch.org>
> wrote:
>     >>>
>     >>> Hi,
>     >>>
>     >>> Alternatively, how about a new glance() generic that would do
> something
>     >>> like this:
>     >>>
>     >>>> library(DelayedArray)
>     >>>> glance <- DelayedArray:::show_compact_array
>     >>>
>     >>>> M <- matrix(rnorm(1e6), nrow = 1000L, ncol = 2000L)
>     >>>> glance(M)
>     >>> <1000 x 2000> matrix object of type "double":
>     >>> [,1]        [,2]        [,3] ...    [,1999]    [,2000]
>     >>> [1,]  -0.8854896   1.8010288   1.3051341   . -0.4473593  0.4684985
>     >>> [2,]  -0.8563415  -0.7102768  -0.9309155   . -1.8743504  0.4300557
>     >>> [3,]   1.0558159  -0.5956583   1.2689806   .  2.7292249  0.2608300
>     >>> [4,]   0.7547356   0.1465714   0.1798959   . -0.1778017  1.3417423
>     >>> [5,]   0.8037360  -2.7081809   0.9766657   . -0.9902788  0.1741957
>     >>> ...           .           .           .   .          .          .
>     >>> [996,]  0.67220752  0.07804320 -0.38743454   .  0.4438639
> -0.8130713
>     >>> [997,] -0.67349962 -1.15292067 -0.54505567   .  0.4630923
> -1.6287694
>     >>> [998,]  0.03374595 -1.68061325 -0.88458368   . -0.2890962
> 0.2552267
>     >>> [999,]  0.47861492  1.25530912  0.19436708   . -0.5193121
> -1.1695501
>     >>> [1000,]  1.52819218  2.23253275 -1.22051720   . -1.0342430
> -0.1703396
>     >>>
>     >>>> A <- array(rnorm(1e6), c(50, 20, 10, 100))
>     >>>> glance(A)
>     >>> <50 x 20 x 10 x 100> array object of type "double":
>     >>> ,,1,1
>     >>> [,1]       [,2]       [,3] ...      [,19]      [,20]
>     >>> [1,] 0.78319619 0.82258390 0.09122269   .  1.7288189  0.7968574
>     >>> [2,] 2.80687459 0.63709640 0.80844430   . -0.3963161 -1.2768284
>     >>> ...          .          .          .   .          .          .
>     >>> [49,] -1.0696320 -0.1698111  2.0082890   .  0.4488292  0.5215745
>     >>> [50,] -0.7012526 -2.0818229  0.7750518   .  0.3189076  0.1437394
>     >>>
>     >>> ...
>     >>>
>     >>> ,,10,100
>     >>> [,1]       [,2]       [,3] ...      [,19]      [,20]
>     >>> [1,]  0.5360649  0.5491561 -0.4098350   .  0.7647435  0.5640699
>     >>> [2,]  0.7924093 -0.7395815 -1.3792913   .  0.1980287 -0.2897026
>     >>> ...          .          .          .   .          .          .
>     >>> [49,]  0.6266209  0.3778512  1.4995778   . -0.3820651 -1.4241691
>     >>> [50,]  1.9218715  3.5475949  0.5963763   .  0.4005210  0.4385623
>     >>>
>     >>> H.
>
> Thank you, Herv? and John.
> Both glance() and brief() are nice, and I think a version of one of
> them could also make a nice addition to the 'utils' package.
>
> However, there's a principal difference between them and the
> proposed generalized head {or tail} :
> The latter really does *return* a sub matrix/array of chosen
> dimensions with modified dimnames and that *object* then is
> printed if not assigned.
>
> OTOH,  glance() and brief() rather are versions of print()
> and I think have a dedicated "display-only" purpose {yes, I see they do
> return something; glance() returning a character object, brief()
> returning the principal argument invisibly, the same as any
> "correct" print() method..}
>
> From the above, I think it may make sense to entertain both a
> generalization of head() and one such a glance() / brief()
> /.. function which for a matrix shows all 4 corners of the
> matrix of data frame.
>
> There's another important criterion here:  __Simplicity__ in the
> code that's added (and will have to be maintained as part of R
> "forever" into the future)...
> AFAICS, the DelayedArray stuff is beatifully modular, but
> possibly also much entangled in the dependent packages and classes we
> cannot require for 'utils'.
>
> The current source for head() and tail() and all their methods
> in utils is just 83 lines of code  {file utils/R/head.R minus
> the initial mostly copyright comments}.
> I am very reluctant to consider blowing that up by factors...
>
>
> Martin
>
>     >>> On 9/16/19 00:54, Michael Chirico wrote:
>     >>>> Awesome. Gabe, since you already have a workshopped version,
> would you like
>     >>>> to proceed? Feel free to ping me to review the patch once it's
> posted.
>     >>>>
>     >>>> On Mon, Sep 16, 2019 at 3:26 PM Martin Maechler <
> maechler at stat.math.ethz.ch>
>     >>>> wrote:
>     >>>>
>     >>>>>>>>>> Michael Chirico
>     >>>>>>>>>> on Sun, 15 Sep 2019 20:52:34 +0800 writes:
>     >>>>>
> >>>>> Finally read in detail your response Gabe. Looks great,
> >>>>> and I agree it's quite intuitive, as well as agree against
> >>>>> non-recycling.
>     >>>>>
> >>>>> Once the length(n) == length(dim(x)) behavior is enabled,
> >>>>> I don't think there's any need/desire to have head() do
> >>>>> x[1:6,1:6] anymore. head(x, c(6, 6)) is quite clear for
> >>>>> those familiar with head(x, 6), it would seem to me.
>     >>>>>
> >>>>> Mike C
>     >>>>>
>     >>>>> Thank you, Gabe, and Michael.
>     >>>>> I did like Gabe's proposal already back in July but was
>     >>>>> busy and/or vacationing then ...
>     >>>>>
>     >>>>> If you submit this with a patch (that includes changes to both
>     >>>>> *.R and *.Rd , including some example) as "wishlist" item to R's
>     >>>>> bugzilla, I'm willing/happy to check and commit this to R-devel.
>     >>>>>
>     >>>>> Martin
>     >>>>>
>     >>>>>
> >>>>> On Sat, Jul 13, 2019 at 8:35 AM Gabriel Becker
> >>>>> <gabembecker at gmail.com> wrote:
>     >>>>>
>     >>>>>>> Hi Michael and Abby,
>     >>>>>>>
>     >>>>>>> So one thing that could happen that would be backwards
>     >>>>>>> compatible (with the exception of something that was an
>     >>>>>>> error no longer being an error) is head and tail could
>     >>>>>>> take vectors of length (dim(x)) rather than integers of
>     >>>>>>> length for n, with the default being n=6 being equivalent
>     >>>>>>> to n = c(6, dim(x)[2], <...>, dim(x)[k]), at least for
>     >>>>>>> the deprecation cycle, if not permanently. It not
>     >>>>>>> recycling would be unexpected based on the behavior of
>     >>>>>>> many R functions but would preserve the current behavior
>     >>>>>>> while granting more fine-grained control to users that
>     >>>>>>> feel they need it.
>     >>>>>>>
>     >>>>>>> A rapidly thrown-together prototype of such a method for
>     >>>>>>> the head of a matrix case is as follows:
>     >>>>>>>
>     >>>>>>> head2 = function(x, n = 6L, ...) { indvecs =
>     >>>>>>> lapply(seq_along(dim(x)), function(i) { if(length(n) >=
>     >>>>>>> i) { ni = n[i] } else { ni = dim(x)[i] } if(ni < 0L) ni =
>     >>>>>>> max(nrow(x) + ni, 0L) else ni = min(ni, dim(x)[i])
>     >>>>>>> seq_len(ni) }) lstargs = c(list(x),indvecs, drop = FALSE)
>     >>>>>>> do.call("[", lstargs) }
>     >>>>>>>
>     >>>>>>>
>     >>>>>>>> mat = matrix(1:100, 10, 10)
>     >>>>>>>
>     >>>>>>>> *head(mat)*
>     >>>>>>>
>     >>>>>>> [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
>     >>>>>>>
>     >>>>>>> [1,] 1 11 21 31 41 51 61 71 81 91
>     >>>>>>>
>     >>>>>>> [2,] 2 12 22 32 42 52 62 72 82 92
>     >>>>>>>
>     >>>>>>> [3,] 3 13 23 33 43 53 63 73 83 93
>     >>>>>>>
>     >>>>>>> [4,] 4 14 24 34 44 54 64 74 84 94
>     >>>>>>>
>     >>>>>>> [5,] 5 15 25 35 45 55 65 75 85 95
>     >>>>>>>
>     >>>>>>> [6,] 6 16 26 36 46 56 66 76 86 96
>     >>>>>>>
>     >>>>>>>> *head2(mat)*
>     >>>>>>>
>     >>>>>>> [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
>     >>>>>>>
>     >>>>>>> [1,] 1 11 21 31 41 51 61 71 81 91
>     >>>>>>>
>     >>>>>>> [2,] 2 12 22 32 42 52 62 72 82 92
>     >>>>>>>
>     >>>>>>> [3,] 3 13 23 33 43 53 63 73 83 93
>     >>>>>>>
>     >>>>>>> [4,] 4 14 24 34 44 54 64 74 84 94
>     >>>>>>>
>     >>>>>>> [5,] 5 15 25 35 45 55 65 75 85 95
>     >>>>>>>
>     >>>>>>> [6,] 6 16 26 36 46 56 66 76 86 96
>     >>>>>>>
>     >>>>>>>> *head2(mat, c(2, 3))*
>     >>>>>>>
>     >>>>>>> [,1] [,2] [,3]
>     >>>>>>>
>     >>>>>>> [1,] 1 11 21
>     >>>>>>>
>     >>>>>>> [2,] 2 12 22
>     >>>>>>>
>     >>>>>>>> *head2(mat, c(2, -9))*
>     >>>>>>>
>     >>>>>>> [,1]
>     >>>>>>>
>     >>>>>>> [1,] 1
>     >>>>>>>
>     >>>>>>> [2,] 2
>     >>>>>>>
>     >>>>>>>
>     >>>>>>> Now one thing to keep in mind here, is that I think we'd
>     >>>>>>> either a) have to make the non-recycling behavior
>     >>>>>>> permanent, or b) have head treat data.frames and matrices
>     >>>>>>> different with respect to the subsets they grab (which
>     >>>>>>> strikes me as a *Bad Plan *(tm)).
>     >>>>>>>
>     >>>>>>> So I don't think the default behavior would ever be
>     >>>>>>> mat[1:6, 1:6], not because of backwards compatibility,
>     >>>>>>> but because at least in my intuition that is just not
>     >>>>>>> what head on a data.frame should do by default, and I
>     >>>>>>> think the behaviors for the basic rectangular datatypes
>     >>>>>>> should "stick together". I mean, also because of
>     >>>>>>> backwards compatibility, but that could *in theory*
>     >>>>>>> change across a long enough deprecation cycle, but the
>     >>>>>>> conceptually right thing to do with a data.frame probably
>     >>>>>>> won't.
>     >>>>>>>
>     >>>>>>> All of that said, is head(mat, c(6, 6)) really that much
>     >>>>>>> easier to type/better than just mat[1:6, 1:6, drop=FALSE]
>     >>>>>>> (I know this will behave differently if any of the dims
>     >>>>>>> of mat are less than 6, but if so why are you heading it
>     >>>>>>> in the first place ;) )? I don't really have a strong
>     >>>>>>> feeling on the answer to that.
>     >>>>>>>
>     >>>>>>> I'm happy to put a patch for head.matrix,
>     >>>>>>> head.data.frame, tail.matrix and tail.data.frame, plus
>     >>>>>>> documentation, if people on R-core are interested in
>     >>>>>>> this.
>     >>>>>>>
>     >>>>>>> Note, as most here probably know, and as alluded to
>     >>>>>>> above, length(n) > 1 for head or tail currently give an
>     >>>>>>> error, so this would be an extension of the existing
>     >>>>>>> functionality in the mathematical extension sense, where
>     >>>>>>> all existing behavior would remain identical, but the
>     >>>>>>> support/valid parameter space would grow.
>     >>>>>>>
>     >>>>>>> Best, ~G
>     >>>>>>>
>     >>>>>>>
>     >>>>>>> On Fri, Jul 12, 2019 at 4:03 PM Abby Spurdle
>     >>>>>>> <spurdle.a at gmail.com> wrote:
>     >>>>>>>
>     >>>>>>>>> I assume there are lots of backwards-compatibility
>     >>>>>>>> issues as well as valid > use cases for this behavior,
>     >>>>>>>> so I guess defaulting to M[1:6, 1:6] is out of > the
>     >>>>>>>> question.
>     >>>>>>>>
>     >>>>>>>> Agree.
>     >>>>>>>>
>     >>>>>>>>> Is there any scope for adding a new argument to
>     >>>>>>>> head.matrix that would > allow this flexibility?
>     >>>>>>>>
>     >>>>>>>> I agree with what you're trying to achieve.  However,
>     >>>>>>>> I'm not sure this is as simple as you're suggesting.
>     >>>>>>>>
>     >>>>>>>> What if the user wants "head" in rows but "tail" in
>     >>>>>>>> columns.  Or "head" in rows, and both "head" and "tail"
>     >>>>>>>> in columns.  With head and tail alone, there's a
>     >>>>>>>> combinatorial explosion.
>     >>>>>>>>
>     >>>>>>>> Also, when using tail on an unnamed matrix, it may be
>     >>>>>>>> desirable to name rows and columns.
>     >>>>>>>>
>     >>>>>>>> And all of this assumes standard matrix objects.  Add in
>     >>>>>>>> a matrix subclasses and related objects, and things get
>     >>>>>>>> more complex still.
>     >>>>>>>>
>     >>>>>>>> As I suggested in a another thread, a few days ago, I'm
>     >>>>>>>> planning to write an R package for matrices and
>     >>>>>>>> matrix-like objects (possibly extending the Matrix
>     >>>>>>>> package), with an initial emphasis on subsetting,
>     >>>>>>>> printing and formatting.  So, I'm interested to hear
>     >>>>>>>> more suggestions on this topic.
>     >>>>>>>>
>     >>>>>>>> [[alternative HTML version deleted]]
>     >>>>>>>>
>     >>>>>>>> ______________________________________________
>     >>>>>>>> R-devel at r-project.org mailing list
>     >>>>>>>>
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Ddevel&d=DwICAg&c=eRAMFD45gAfqt84VtBcfhQ&r=BK7q3XeAvimeWdGbWY_wJYbW0WYiZvSXAJJKaaPhzWA&m=sOZlR-nzy_f_Sje6VGA6IXYQM01BO39OQ2zqA8mtaGI&s=VyNGYbk1jJJqirYBwnhKX60dCp31ArtS62RmXKn86O4&e=
>     >>>>>>>>
>     >>>>>>>
>     >>>>>
> >>>>> [[alternative HTML version deleted]]
>     >>>>>
> >>>>> ______________________________________________
> >>>>> R-devel at r-project.org mailing list
> >>>>>
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Ddevel&d=DwICAg&c=eRAMFD45gAfqt84VtBcfhQ&r=BK7q3XeAvimeWdGbWY_wJYbW0WYiZvSXAJJKaaPhzWA&m=sOZlR-nzy_f_Sje6VGA6IXYQM01BO39OQ2zqA8mtaGI&s=VyNGYbk1jJJqirYBwnhKX60dCp31ArtS62RmXKn86O4&e=
>     >>>>>
>     >>>>
>     >>>> [[alternative HTML version deleted]]
>     >>>>
>     >>>> ______________________________________________
>     >>>> R-devel at r-project.org mailing list
>     >>>>
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Ddevel&d=DwICAg&c=eRAMFD45gAfqt84VtBcfhQ&r=BK7q3XeAvimeWdGbWY_wJYbW0WYiZvSXAJJKaaPhzWA&m=sOZlR-nzy_f_Sje6VGA6IXYQM01BO39OQ2zqA8mtaGI&s=VyNGYbk1jJJqirYBwnhKX60dCp31ArtS62RmXKn86O4&e=
>     >>>>
>     >>>
>     >>> --
>     >>> Herv? Pag?s
>     >>>
>     >>> Program in Computational Biology
>     >>> Division of Public Health Sciences
>     >>> Fred Hutchinson Cancer Research Center
>     >>> 1100 Fairview Ave. N, M1-B514
>     >>> P.O. Box 19024
>     >>> Seattle, WA 98109-1024
>     >>>
>     >>> E-mail: hpages at fredhutch.org
>     >>> Phone:  (206) 667-5791
>     >>> Fax:    (206) 667-1319
>     >>> ______________________________________________
>     >>> R-devel at r-project.org mailing list
>     >>> https://stat.ethz.ch/mailman/listinfo/r-devel
>     >>
>     >> ______________________________________________
>     >> R-devel at r-project.org mailing list
>     >> https://stat.ethz.ch/mailman/listinfo/r-devel
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From ph||@m@th@|og|c @end|ng |rom gm@||@com  Fri Oct 18 01:50:36 2019
From: ph||@m@th@|og|c @end|ng |rom gm@||@com (Adam Frank)
Date: Thu, 17 Oct 2019 19:50:36 -0400
Subject: [Rd] Cannot install dplyr
Message-ID: <CAL1ABgiGMF=3T3sVCL2kTa9UmzieONfAfazky5Xvy1RHMbcGWA@mail.gmail.com>

So I've tried many methods to get dyplr installed, I'll detail each of the
methods and issues below.  I'm running a new Linux machine with Pop!_OS.

*From the package manager*

I tried installing r-base from the package manager but it delivers a version
so out of date that when I try to install dyplr it throws an error.  As far
as I can tell from researching the error, the only resolution is to not
install from the package manager.

*From precompiled binaries*

I don't fully get what I'm doing but as I understand it (I am not an
especially tech-savvy person) the precompiled binaries are located here:
Binaries <https://mirrors.nics.utk.edu/cran/>    However, given that my OS
is none of those listed, I was wary of trying this.  I tried anyway,
guessing using the Ubuntu trusty one might do something, got a public key
error.

```
W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu trusty/ Release:
The following signatures couldn't be verified because the public key is not
available: NO_PUBKEY 51716619E084DAB9
```

I searched the error, don't really understand what I read about how to
resolve it.  But I'm guessing it has something to do with needing to already
have a public key and I don't.  So maybe this solution isn't meant for me.

*Compiling from sources*

Trying to follow any (and all) instructions I could find on building from
source files, I downloaded from here:  R souce
<https://cran.r-project.org/src/base/R-3/>  .  I unzipped, navigated in, ran
`sudo ./configure`.  After chasing a lot of errors and trying to resolve
them I eventually got it installed.  Then downloaded RStudio and tried to
install it, but it couldn't find R.  After researching the error I found a
suggestion to run `./configure` with the `--enable-R-shlib`.  Did that, now
I run `make` and get

```
[Some other stuff up above here]
/usr/bin/ld: ../unix/X11.o: relocation R_X86_64_PC32 against symbol
`R_GUIType' can not be used when making a shared object; recompile with
-fPIC
/usr/bin/ld: final link failed: nonrepresentable section on output
collect2: error: ld returned 1 exit status
make[3]: *** [Makefile:177: libR.so] Error 1
make[3]: Leaving directory '/home/addem/Downloads/R-3.6.1/src/main'
make[2]: *** [Makefile:135: R] Error 2
make[2]: Leaving directory '/home/addem/Downloads/R-3.6.1/src/main'
make[1]: *** [Makefile:28: R] Error 1
make[1]: Leaving directory '/home/addem/Downloads/R-3.6.1/src'
make: *** [Makefile:61: R] Error 1
```

After searching this error I can't find a resolution.

-----

And just to be clear about what I'm trying to do here: All I want is R
running with RStudio, and is able to install the dplyr package.  If this can
be done in any way, I don't care which way it's done.

If any log files or whatever would be useful for any one of these three
installation attempts, just ask.

Thanks.

	[[alternative HTML version deleted]]


From edd @end|ng |rom deb|@n@org  Sat Oct 19 17:13:58 2019
From: edd @end|ng |rom deb|@n@org (Dirk Eddelbuettel)
Date: Sat, 19 Oct 2019 10:13:58 -0500
Subject: [Rd] Cannot install dplyr
In-Reply-To: <CAL1ABgiGMF=3T3sVCL2kTa9UmzieONfAfazky5Xvy1RHMbcGWA@mail.gmail.com>
References: <CAL1ABgiGMF=3T3sVCL2kTa9UmzieONfAfazky5Xvy1RHMbcGWA@mail.gmail.com>
Message-ID: <23979.10294.489867.923604@rob.eddelbuettel.com>


Adam,

This can indeed be very frustrating.  But depending on the choices you make
it does not have to be that way.  Most Linux distributions have 'something'
it may not be comprehensive or current. So most of the time you need to be
able to compile from source. Which is not that hard, and can be
learned. Maybe there is a user group near you.

Otherwise and not so well known is that Michael Rutter is (single-handedly)
keeping an effort we once called 'cran2deb' alive (in what we could call the
third iteration of this; I had been involved in some earlier ones [1]).

So if you run Ubuntu LTS you get over 4,000 CRAN packages ready-made.  But
you should really run Ubuntu LTS (currently 18.04) or a compatiable release
(ie a new Ubuntu release).

That maybe be best seen rather than read so to complement earlier blog posts
I made a video showing how even installation of the entire tidyverse (and its
80+ packages including dplyr), or rstan (which is also heavy 'from source'
due to loads of C++ compilation), boils down to a single command (!!) and a
mere two minutes of automated downloading and unpackaging.  See

   http://dirk.eddelbuettel.com/blog/2019/06/09#022_rocker_and_ppas

for a few slides and a link to the video you could also jump at directly at

   https://www.youtube.com/watch?v=qIjWirNma-8

Having something like this _comprehensively_ across a few distros is clearly
needed.

But the only active effort I know of appears to be an RStudio effort related
to their their (commercial only, AFAIK) package manager.  Needless to say, I
would much rather have this as an open community effort, but sadly, wishing
alone does not make it so. [1]

Dirk

   
[1] And one potential implementation effort was halted when the R Consortium
ISC lead decided it was better to withdraw the grant they had given me (as
PI) and a few others than to further support the effort.  

-- 
http://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From edd @end|ng |rom deb|@n@org  Sat Oct 19 17:30:00 2019
From: edd @end|ng |rom deb|@n@org (Dirk Eddelbuettel)
Date: Sat, 19 Oct 2019 10:30:00 -0500
Subject: [Rd] Cannot install dplyr
In-Reply-To: <23979.10294.489867.923604@rob.eddelbuettel.com>
References: <CAL1ABgiGMF=3T3sVCL2kTa9UmzieONfAfazky5Xvy1RHMbcGWA@mail.gmail.com>
 <23979.10294.489867.923604@rob.eddelbuettel.com>
Message-ID: <23979.11256.950398.14846@rob.eddelbuettel.com>


Adam,

Apologies for typos in previous email -- and follow-ups should probably go to
the r-sig-debian list which support R on .deb based distros. I am not sure
have readers from the derivative distro you use, but there is a fairly wide
spread of users there beyond "just" Debian or Ubuntu. Subscribe first, then
post. 

Best,  Dirk

-- 
http://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From tr@ver@c @end|ng |rom gm@||@com  Mon Oct 21 02:47:56 2019
From: tr@ver@c @end|ng |rom gm@||@com (Travers Ching)
Date: Sun, 20 Oct 2019 17:47:56 -0700
Subject: [Rd] S4SXP type vs S4 object bit?
Message-ID: <CAPLMX9FMdNTDjQSy-HvCUVrn3ph3rh0-YCTrXhFa5FycL8bjLw-113@mail.gmail.com>

I'm trying to understand the R internals a bit better and reading over the
documentation.

I see that there is a bit related to whether an object is S4
(S4_OBJECT_MASK), and also the object type S4SXP (25).  The documentation
makes clear that these two things aren't the same.

But in practice, will the S4-bit and object type ever disagree for S4
objects?  I know that one can set the bit manually in C; are there any
practical applications for doing so?

Thank you
Travers

	[[alternative HTML version deleted]]


From |@wrence@m|ch@e| @end|ng |rom gene@com  Tue Oct 22 17:00:51 2019
From: |@wrence@m|ch@e| @end|ng |rom gene@com (Michael Lawrence)
Date: Tue, 22 Oct 2019 08:00:51 -0700
Subject: [Rd] S4SXP type vs S4 object bit?
In-Reply-To: <CAPLMX9FMdNTDjQSy-HvCUVrn3ph3rh0-YCTrXhFa5FycL8bjLw-113@mail.gmail.com>
References: <CAPLMX9FMdNTDjQSy-HvCUVrn3ph3rh0-YCTrXhFa5FycL8bjLw-113@mail.gmail.com>
Message-ID: <CAOQ5NyeE-Q-7gC6-YBUJ=ZfPxi10cSaXBAWfSBymvMnLK5u+Kg@mail.gmail.com>

Yes, any object of a class that derives from a basic type, like an
atomic vector for example, will be of the basic SEXP type, with the S4
bit set. This means that a class can extend "integer" and objects of
that class can be treated as any ordinary integer vector. S4SXP is
only for objects that do not derive from another basic type.

Michael

On Tue, Oct 22, 2019 at 1:28 AM Travers Ching <traversc at gmail.com> wrote:
>
> I'm trying to understand the R internals a bit better and reading over the
> documentation.
>
> I see that there is a bit related to whether an object is S4
> (S4_OBJECT_MASK), and also the object type S4SXP (25).  The documentation
> makes clear that these two things aren't the same.
>
> But in practice, will the S4-bit and object type ever disagree for S4
> objects?  I know that one can set the bit manually in C; are there any
> practical applications for doing so?
>
> Thank you
> Travers
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel



-- 
Michael Lawrence
Scientist, Bioinformatics and Computational Biology
Genentech, A Member of the Roche Group
Office +1 (650) 225-7760
michafla at gene.com

Join Genentech on LinkedIn | Twitter | Facebook | Instagram | YouTube


From @zwj|08 @end|ng |rom gm@||@com  Tue Oct 22 17:14:28 2019
From: @zwj|08 @end|ng |rom gm@||@com (Wang Jiefei)
Date: Tue, 22 Oct 2019 11:14:28 -0400
Subject: [Rd] S4SXP type vs S4 object bit?
In-Reply-To: <CAPLMX9FMdNTDjQSy-HvCUVrn3ph3rh0-YCTrXhFa5FycL8bjLw-113@mail.gmail.com>
References: <CAPLMX9FMdNTDjQSy-HvCUVrn3ph3rh0-YCTrXhFa5FycL8bjLw-113@mail.gmail.com>
Message-ID: <CAGiFhPNv-JvWVC_bhQ5rFzyOJm9nWq_tuqAuJW-9i--swdLgWQ@mail.gmail.com>

Hi Travers,

Just an additional remarks to Michael's answer, if your S4 class inherits
from R's basic types, say integer, the resulting object will be an INTSXP.
If your S4 class does not inherit from any class, it will be an S4SXP. You
can think about this question from the object-oriented framework: If one
class inherits the integer class, what should R do to make all the integer
related functions compatible with the new class at C level?

Best,
Jiefei

On Tue, Oct 22, 2019 at 4:28 AM Travers Ching <traversc at gmail.com> wrote:

> I'm trying to understand the R internals a bit better and reading over the
> documentation.
>
> I see that there is a bit related to whether an object is S4
> (S4_OBJECT_MASK), and also the object type S4SXP (25).  The documentation
> makes clear that these two things aren't the same.
>
> But in practice, will the S4-bit and object type ever disagree for S4
> objects?  I know that one can set the bit manually in C; are there any
> practical applications for doing so?
>
> Thank you
> Travers
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From tr@ver@c @end|ng |rom gm@||@com  Tue Oct 22 18:42:31 2019
From: tr@ver@c @end|ng |rom gm@||@com (Travers Ching)
Date: Tue, 22 Oct 2019 09:42:31 -0700
Subject: [Rd] S4SXP type vs S4 object bit?
In-Reply-To: <CAGiFhPNv-JvWVC_bhQ5rFzyOJm9nWq_tuqAuJW-9i--swdLgWQ@mail.gmail.com>
References: <CAPLMX9FMdNTDjQSy-HvCUVrn3ph3rh0-YCTrXhFa5FycL8bjLw-113@mail.gmail.com>
 <CAGiFhPNv-JvWVC_bhQ5rFzyOJm9nWq_tuqAuJW-9i--swdLgWQ@mail.gmail.com>
Message-ID: <CAPLMX9GSnT0-zcxZUg9B6tBi-xnL1toiKW700cLLx8UjbkJRnQ@mail.gmail.com>

Thanks you Jiefei and Michael!

Travers

On Tue, Oct 22, 2019 at 8:14 AM Wang Jiefei <szwjf08 at gmail.com> wrote:

> Hi Travers,
>
> Just an additional remarks to Michael's answer, if your S4 class inherits
> from R's basic types, say integer, the resulting object will be an INTSXP.
> If your S4 class does not inherit from any class, it will be an S4SXP. You
> can think about this question from the object-oriented framework: If one
> class inherits the integer class, what should R do to make all the integer
> related functions compatible with the new class at C level?
>
> Best,
> Jiefei
>
> On Tue, Oct 22, 2019 at 4:28 AM Travers Ching <traversc at gmail.com> wrote:
>
>> I'm trying to understand the R internals a bit better and reading over the
>> documentation.
>>
>> I see that there is a bit related to whether an object is S4
>> (S4_OBJECT_MASK), and also the object type S4SXP (25).  The documentation
>> makes clear that these two things aren't the same.
>>
>> But in practice, will the S4-bit and object type ever disagree for S4
>> objects?  I know that one can set the bit manually in C; are there any
>> practical applications for doing so?
>>
>> Thank you
>> Travers
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>

	[[alternative HTML version deleted]]


From @zwj|08 @end|ng |rom gm@||@com  Wed Oct 23 06:45:29 2019
From: @zwj|08 @end|ng |rom gm@||@com (Wang Jiefei)
Date: Wed, 23 Oct 2019 00:45:29 -0400
Subject: [Rd] Unexpected behavior when using macro to loop over vector
Message-ID: <CAGiFhPNAvjx4_jNPc9VJVy3vJ3NhCMCPnvEt5whh5P6Ca-nuDA@mail.gmail.com>

Hi all,

I found an unexpected behavior when I was trying to use the macro defined
in "R_ext/Itermacros.h"  to loop over an atomic vector. Here is a minimum
example:

C++ code
```
#include "R_ext/Itermacros.h"
#define GET_REGION_BUFSIZE 2
//Redefine the macro since C++ is not happy with the implicit type
conversion
#define ITERATE_BY_REGION_PARTIAL(sx, px, idx, nb, etype, vtype, \
 strt, nfull, expr) do { \
const etype *px = (etype*)DATAPTR_OR_NULL(sx); \
if (px != NULL) { \
   R_xlen_t __ibr_n__ = strt + nfull; \
   R_xlen_t nb = __ibr_n__; \
   for (R_xlen_t idx = strt; idx < __ibr_n__; idx += nb) { \
expr \
    } \
} \
else ITERATE_BY_REGION_PARTIAL0(sx, px, idx, nb, etype, vtype, \
strt, nfull, expr); \
    } while (0)
// [[Rcpp::export]]
void C_testPrint(SEXP x) {
ITERATE_BY_REGION_PARTIAL(x, ptr, idx, nbatch, double, REAL, 1, 4, {
for (R_xlen_t i = 0; i < nbatch; i++)
Rprintf("idx: %lld, i: %lld, ptr:%f\n", idx, i, ptr[i]);
});
}
```

The function C_testPrint loops over its argument x and prints out one value
of x at each loop. The loop starts from the second element and ends in the
fifth element of x. I also redefine the buffer size to see the effect of
it. Here is my R code:

R code
```
> C_testPrint(as.numeric(1:10))
idx: 1, i: 0, ptr:2.000000
idx: 1, i: 1, ptr:3.000000
idx: 3, i: 0, ptr:4.000000
idx: 3, i: 1, ptr:5.000000
> C_testPrint(c(1,2,3,4,5,6,7,8,9,10))
idx: 1, i: 0, ptr:1.000000
idx: 1, i: 1, ptr:2.000000
idx: 1, i: 2, ptr:3.000000
idx: 1, i: 3, ptr:4.000000
idx: 1, i: 4, ptr:5.000000
```

There are two problems in the outputs:
1. The numbers of lines are different
2. The starting indices are not the same.

>From my understanding, the first output seems correct to me. The second is
not unexpected. I believe the differences are due to the accessibility of
the data pointer. Did I misunderstand and misuse the macro? Or is it a bug
in R? Here is my session info. My R is a bit outdated but the macro seems
unchanged in R 4.0. Thanks

```
> sessionInfo()
R Under development (unstable) (2019-08-22 r77060)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows >= 8 x64 (build 9200)
```

	[[alternative HTML version deleted]]


From c@@rd|@g@bor @end|ng |rom gm@||@com  Fri Oct 25 00:20:38 2019
From: c@@rd|@g@bor @end|ng |rom gm@||@com (=?UTF-8?B?R8OhYm9yIENzw6FyZGk=?=)
Date: Thu, 24 Oct 2019 23:20:38 +0100
Subject: [Rd] Built field in binary packages missing the platform
Message-ID: <CABtg=KmDPcFjGVUt2-S=eyrCJr4he4BhxCsHm9wUn+fcWyfNoA@mail.gmail.com>

It seems that for binary packages that do not contain compiled code,
the "Built` field looks like this:

Built: R 3.6.0; ; 2019-09-23 13:56:59 UTC; unix

I.e., the second part, the platform string is missing. For a package
that has compiled code, it is present:

Built: R 3.6.0; x86_64-apple-darwin15.6.0; 2019-04-26 19:49:57 UTC;
        unix

This is true for all platforms I tried, Linux, Windows and macOS.

I think it would be great to include the platform in binary packages,
even if they do not have compiled code, because they can still be
platform dependent. Then the Built field could be a simple way to know
what platform a binary package was created for and/or installed on.

Is the platform missing intentionally for some reason?

Thanks,
Gabor


From tom@@@k@||ber@ @end|ng |rom gm@||@com  Fri Oct 25 11:01:51 2019
From: tom@@@k@||ber@ @end|ng |rom gm@||@com (Tomas Kalibera)
Date: Fri, 25 Oct 2019 11:01:51 +0200
Subject: [Rd] Unexpected behavior when using macro to loop over vector
In-Reply-To: <CAGiFhPNAvjx4_jNPc9VJVy3vJ3NhCMCPnvEt5whh5P6Ca-nuDA@mail.gmail.com>
References: <CAGiFhPNAvjx4_jNPc9VJVy3vJ3NhCMCPnvEt5whh5P6Ca-nuDA@mail.gmail.com>
Message-ID: <8c418e9b-66fc-98d9-0e4f-e4d29504ac82@gmail.com>

On 10/23/19 6:45 AM, Wang Jiefei wrote:
> Hi all,
>
> I found an unexpected behavior when I was trying to use the macro defined
> in "R_ext/Itermacros.h"  to loop over an atomic vector. Here is a minimum
> example:
>
> C++ code
> ```
> #include "R_ext/Itermacros.h"
> #define GET_REGION_BUFSIZE 2
> //Redefine the macro since C++ is not happy with the implicit type
> conversion
> #define ITERATE_BY_REGION_PARTIAL(sx, px, idx, nb, etype, vtype, \
>   strt, nfull, expr) do { \
> const etype *px = (etype*)DATAPTR_OR_NULL(sx); \
> if (px != NULL) { \
>     R_xlen_t __ibr_n__ = strt + nfull; \
>     R_xlen_t nb = __ibr_n__; \
>     for (R_xlen_t idx = strt; idx < __ibr_n__; idx += nb) { \
> expr \
>      } \
> } \
> else ITERATE_BY_REGION_PARTIAL0(sx, px, idx, nb, etype, vtype, \
> strt, nfull, expr); \
>      } while (0)
> // [[Rcpp::export]]
> void C_testPrint(SEXP x) {
> ITERATE_BY_REGION_PARTIAL(x, ptr, idx, nbatch, double, REAL, 1, 4, {
> for (R_xlen_t i = 0; i < nbatch; i++)
> Rprintf("idx: %lld, i: %lld, ptr:%f\n", idx, i, ptr[i]);

You need to index "ptr" by "idx + i", not by "i". Have a look at how the 
macros are used in R, e.g. printvector.c.

Best,
Tomas

> });
> }
> ```
>
> The function C_testPrint loops over its argument x and prints out one value
> of x at each loop. The loop starts from the second element and ends in the
> fifth element of x. I also redefine the buffer size to see the effect of
> it. Here is my R code:
>
> R code
> ```
>> C_testPrint(as.numeric(1:10))
> idx: 1, i: 0, ptr:2.000000
> idx: 1, i: 1, ptr:3.000000
> idx: 3, i: 0, ptr:4.000000
> idx: 3, i: 1, ptr:5.000000
>> C_testPrint(c(1,2,3,4,5,6,7,8,9,10))
> idx: 1, i: 0, ptr:1.000000
> idx: 1, i: 1, ptr:2.000000
> idx: 1, i: 2, ptr:3.000000
> idx: 1, i: 3, ptr:4.000000
> idx: 1, i: 4, ptr:5.000000
> ```
>
> There are two problems in the outputs:
> 1. The numbers of lines are different
> 2. The starting indices are not the same.
>
>  From my understanding, the first output seems correct to me. The second is
> not unexpected. I believe the differences are due to the accessibility of
> the data pointer. Did I misunderstand and misuse the macro? Or is it a bug
> in R? Here is my session info. My R is a bit outdated but the macro seems
> unchanged in R 4.0. Thanks
>
> ```
>> sessionInfo()
> R Under development (unstable) (2019-08-22 r77060)
> Platform: x86_64-w64-mingw32/x64 (64-bit)
> Running under: Windows >= 8 x64 (build 9200)
> ```
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From tom@@@k@||ber@ @end|ng |rom gm@||@com  Fri Oct 25 17:13:10 2019
From: tom@@@k@||ber@ @end|ng |rom gm@||@com (Tomas Kalibera)
Date: Fri, 25 Oct 2019 17:13:10 +0200
Subject: [Rd] Unexpected behavior when using macro to loop over vector
In-Reply-To: <8c418e9b-66fc-98d9-0e4f-e4d29504ac82@gmail.com>
References: <CAGiFhPNAvjx4_jNPc9VJVy3vJ3NhCMCPnvEt5whh5P6Ca-nuDA@mail.gmail.com>
 <8c418e9b-66fc-98d9-0e4f-e4d29504ac82@gmail.com>
Message-ID: <1fbf4de9-580b-6d3c-28d2-6f0dd7c7f35e@gmail.com>

On 10/25/19 11:01 AM, Tomas Kalibera wrote:
> On 10/23/19 6:45 AM, Wang Jiefei wrote:
>> Hi all,
>>
>> I found an unexpected behavior when I was trying to use the macro 
>> defined
>> in "R_ext/Itermacros.h"? to loop over an atomic vector. Here is a 
>> minimum
>> example:
>>
>> C++ code
>> ```
>> #include "R_ext/Itermacros.h"
>> #define GET_REGION_BUFSIZE 2
>> //Redefine the macro since C++ is not happy with the implicit type
>> conversion
>> #define ITERATE_BY_REGION_PARTIAL(sx, px, idx, nb, etype, vtype, \
>> ? strt, nfull, expr) do { \
>> const etype *px = (etype*)DATAPTR_OR_NULL(sx); \
>> if (px != NULL) { \
>> ??? R_xlen_t __ibr_n__ = strt + nfull; \
>> ??? R_xlen_t nb = __ibr_n__; \
>> ??? for (R_xlen_t idx = strt; idx < __ibr_n__; idx += nb) { \
>> expr \
>> ???? } \
>> } \
>> else ITERATE_BY_REGION_PARTIAL0(sx, px, idx, nb, etype, vtype, \
>> strt, nfull, expr); \
>> ???? } while (0)
>> // [[Rcpp::export]]
>> void C_testPrint(SEXP x) {
>> ITERATE_BY_REGION_PARTIAL(x, ptr, idx, nbatch, double, REAL, 1, 4, {
>> for (R_xlen_t i = 0; i < nbatch; i++)
>> Rprintf("idx: %lld, i: %lld, ptr:%f\n", idx, i, ptr[i]);
>
> You need to index "ptr" by "idx + i", not by "i". Have a look at how 
> the macros are used in R, e.g. printvector.c.

Actually, the macro should do this for you, we will investigate/fix. 
Thanks for the report!

Best
Tomas

>
> Best,
> Tomas
>
>> });
>> }
>> ```
>>
>> The function C_testPrint loops over its argument x and prints out one 
>> value
>> of x at each loop. The loop starts from the second element and ends 
>> in the
>> fifth element of x. I also redefine the buffer size to see the effect of
>> it. Here is my R code:
>>
>> R code
>> ```
>>> C_testPrint(as.numeric(1:10))
>> idx: 1, i: 0, ptr:2.000000
>> idx: 1, i: 1, ptr:3.000000
>> idx: 3, i: 0, ptr:4.000000
>> idx: 3, i: 1, ptr:5.000000
>>> C_testPrint(c(1,2,3,4,5,6,7,8,9,10))
>> idx: 1, i: 0, ptr:1.000000
>> idx: 1, i: 1, ptr:2.000000
>> idx: 1, i: 2, ptr:3.000000
>> idx: 1, i: 3, ptr:4.000000
>> idx: 1, i: 4, ptr:5.000000
>> ```
>>
>> There are two problems in the outputs:
>> 1. The numbers of lines are different
>> 2. The starting indices are not the same.
>>
>> ?From my understanding, the first output seems correct to me. The 
>> second is
>> not unexpected. I believe the differences are due to the 
>> accessibility of
>> the data pointer. Did I misunderstand and misuse the macro? Or is it 
>> a bug
>> in R? Here is my session info. My R is a bit outdated but the macro 
>> seems
>> unchanged in R 4.0. Thanks
>>
>> ```
>>> sessionInfo()
>> R Under development (unstable) (2019-08-22 r77060)
>> Platform: x86_64-w64-mingw32/x64 (64-bit)
>> Running under: Windows >= 8 x64 (build 9200)
>> ```
>>
>> ????[[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>
>


From @zwj|08 @end|ng |rom gm@||@com  Fri Oct 25 17:20:33 2019
From: @zwj|08 @end|ng |rom gm@||@com (Wang Jiefei)
Date: Fri, 25 Oct 2019 11:20:33 -0400
Subject: [Rd] Unexpected behavior when using macro to loop over vector
In-Reply-To: <1fbf4de9-580b-6d3c-28d2-6f0dd7c7f35e@gmail.com>
References: <CAGiFhPNAvjx4_jNPc9VJVy3vJ3NhCMCPnvEt5whh5P6Ca-nuDA@mail.gmail.com>
 <8c418e9b-66fc-98d9-0e4f-e4d29504ac82@gmail.com>
 <1fbf4de9-580b-6d3c-28d2-6f0dd7c7f35e@gmail.com>
Message-ID: <CAGiFhPO29mRww443MOtEqo8iXFFB0KW=eBe2ocnwckAjnKroZA@mail.gmail.com>

Thank you, Tomas. I appreciate your help. BTW, could you also add an
explicit type conversion in " ITERATE_BY_REGION_PARTIAL" macro while you
are fixing the bug? C++ compiler does not happy with the implicit
conversion from void* to T* somehow and I have to redefine it before using
the macro.

Best,
Jiefei

On Fri, Oct 25, 2019 at 11:13 AM Tomas Kalibera <tomas.kalibera at gmail.com>
wrote:

> On 10/25/19 11:01 AM, Tomas Kalibera wrote:
> > On 10/23/19 6:45 AM, Wang Jiefei wrote:
> >> Hi all,
> >>
> >> I found an unexpected behavior when I was trying to use the macro
> >> defined
> >> in "R_ext/Itermacros.h"  to loop over an atomic vector. Here is a
> >> minimum
> >> example:
> >>
> >> C++ code
> >> ```
> >> #include "R_ext/Itermacros.h"
> >> #define GET_REGION_BUFSIZE 2
> >> //Redefine the macro since C++ is not happy with the implicit type
> >> conversion
> >> #define ITERATE_BY_REGION_PARTIAL(sx, px, idx, nb, etype, vtype, \
> >>   strt, nfull, expr) do { \
> >> const etype *px = (etype*)DATAPTR_OR_NULL(sx); \
> >> if (px != NULL) { \
> >>     R_xlen_t __ibr_n__ = strt + nfull; \
> >>     R_xlen_t nb = __ibr_n__; \
> >>     for (R_xlen_t idx = strt; idx < __ibr_n__; idx += nb) { \
> >> expr \
> >>      } \
> >> } \
> >> else ITERATE_BY_REGION_PARTIAL0(sx, px, idx, nb, etype, vtype, \
> >> strt, nfull, expr); \
> >>      } while (0)
> >> // [[Rcpp::export]]
> >> void C_testPrint(SEXP x) {
> >> ITERATE_BY_REGION_PARTIAL(x, ptr, idx, nbatch, double, REAL, 1, 4, {
> >> for (R_xlen_t i = 0; i < nbatch; i++)
> >> Rprintf("idx: %lld, i: %lld, ptr:%f\n", idx, i, ptr[i]);
> >
> > You need to index "ptr" by "idx + i", not by "i". Have a look at how
> > the macros are used in R, e.g. printvector.c.
>
> Actually, the macro should do this for you, we will investigate/fix.
> Thanks for the report!
>
> Best
> Tomas
>
> >
> > Best,
> > Tomas
> >
> >> });
> >> }
> >> ```
> >>
> >> The function C_testPrint loops over its argument x and prints out one
> >> value
> >> of x at each loop. The loop starts from the second element and ends
> >> in the
> >> fifth element of x. I also redefine the buffer size to see the effect of
> >> it. Here is my R code:
> >>
> >> R code
> >> ```
> >>> C_testPrint(as.numeric(1:10))
> >> idx: 1, i: 0, ptr:2.000000
> >> idx: 1, i: 1, ptr:3.000000
> >> idx: 3, i: 0, ptr:4.000000
> >> idx: 3, i: 1, ptr:5.000000
> >>> C_testPrint(c(1,2,3,4,5,6,7,8,9,10))
> >> idx: 1, i: 0, ptr:1.000000
> >> idx: 1, i: 1, ptr:2.000000
> >> idx: 1, i: 2, ptr:3.000000
> >> idx: 1, i: 3, ptr:4.000000
> >> idx: 1, i: 4, ptr:5.000000
> >> ```
> >>
> >> There are two problems in the outputs:
> >> 1. The numbers of lines are different
> >> 2. The starting indices are not the same.
> >>
> >>  From my understanding, the first output seems correct to me. The
> >> second is
> >> not unexpected. I believe the differences are due to the
> >> accessibility of
> >> the data pointer. Did I misunderstand and misuse the macro? Or is it
> >> a bug
> >> in R? Here is my session info. My R is a bit outdated but the macro
> >> seems
> >> unchanged in R 4.0. Thanks
> >>
> >> ```
> >>> sessionInfo()
> >> R Under development (unstable) (2019-08-22 r77060)
> >> Platform: x86_64-w64-mingw32/x64 (64-bit)
> >> Running under: Windows >= 8 x64 (build 9200)
> >> ```
> >>
> >>     [[alternative HTML version deleted]]
> >>
> >> ______________________________________________
> >> R-devel at r-project.org mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-devel
> >
> >
>
>

	[[alternative HTML version deleted]]


From tom@@@k@||ber@ @end|ng |rom gm@||@com  Fri Oct 25 19:19:36 2019
From: tom@@@k@||ber@ @end|ng |rom gm@||@com (Tomas Kalibera)
Date: Fri, 25 Oct 2019 19:19:36 +0200
Subject: [Rd] Unexpected behavior when using macro to loop over vector
In-Reply-To: <CAGiFhPO29mRww443MOtEqo8iXFFB0KW=eBe2ocnwckAjnKroZA@mail.gmail.com>
References: <CAGiFhPNAvjx4_jNPc9VJVy3vJ3NhCMCPnvEt5whh5P6Ca-nuDA@mail.gmail.com>
 <8c418e9b-66fc-98d9-0e4f-e4d29504ac82@gmail.com>
 <1fbf4de9-580b-6d3c-28d2-6f0dd7c7f35e@gmail.com>
 <CAGiFhPO29mRww443MOtEqo8iXFFB0KW=eBe2ocnwckAjnKroZA@mail.gmail.com>
Message-ID: <03918072-c25a-6abe-a03c-6f8318dab845@gmail.com>

On 10/25/19 5:20 PM, Wang Jiefei wrote:
> Thank you, Tomas. I appreciate your help. BTW, could you also add an 
> explicit type conversion in " ITERATE_BY_REGION_PARTIAL" macro while 
> you are fixing the bug? C++ compiler does not happy with the implicit 
> conversion from void* to T* somehow and I have to redefine it before 
> using the macro.

Yes, C++ requires a cast from void to non-void pointer. Fixed now in 
R-devel (77331): your example iterator should now work also for the 
non-compact sequence.

Best
Tomas

>
> Best,
> Jiefei
>
> On Fri, Oct 25, 2019 at 11:13 AM Tomas Kalibera 
> <tomas.kalibera at gmail.com <mailto:tomas.kalibera at gmail.com>> wrote:
>
>     On 10/25/19 11:01 AM, Tomas Kalibera wrote:
>     > On 10/23/19 6:45 AM, Wang Jiefei wrote:
>     >> Hi all,
>     >>
>     >> I found an unexpected behavior when I was trying to use the macro
>     >> defined
>     >> in "R_ext/Itermacros.h"? to loop over an atomic vector. Here is a
>     >> minimum
>     >> example:
>     >>
>     >> C++ code
>     >> ```
>     >> #include "R_ext/Itermacros.h"
>     >> #define GET_REGION_BUFSIZE 2
>     >> //Redefine the macro since C++ is not happy with the implicit type
>     >> conversion
>     >> #define ITERATE_BY_REGION_PARTIAL(sx, px, idx, nb, etype, vtype, \
>     >> ? strt, nfull, expr) do { \
>     >> const etype *px = (etype*)DATAPTR_OR_NULL(sx); \
>     >> if (px != NULL) { \
>     >> ??? R_xlen_t __ibr_n__ = strt + nfull; \
>     >> ??? R_xlen_t nb = __ibr_n__; \
>     >> ??? for (R_xlen_t idx = strt; idx < __ibr_n__; idx += nb) { \
>     >> expr \
>     >> ???? } \
>     >> } \
>     >> else ITERATE_BY_REGION_PARTIAL0(sx, px, idx, nb, etype, vtype, \
>     >> strt, nfull, expr); \
>     >> ???? } while (0)
>     >> // [[Rcpp::export]]
>     >> void C_testPrint(SEXP x) {
>     >> ITERATE_BY_REGION_PARTIAL(x, ptr, idx, nbatch, double, REAL, 1,
>     4, {
>     >> for (R_xlen_t i = 0; i < nbatch; i++)
>     >> Rprintf("idx: %lld, i: %lld, ptr:%f\n", idx, i, ptr[i]);
>     >
>     > You need to index "ptr" by "idx + i", not by "i". Have a look at
>     how
>     > the macros are used in R, e.g. printvector.c.
>
>     Actually, the macro should do this for you, we will investigate/fix.
>     Thanks for the report!
>
>     Best
>     Tomas
>
>     >
>     > Best,
>     > Tomas
>     >
>     >> });
>     >> }
>     >> ```
>     >>
>     >> The function C_testPrint loops over its argument x and prints
>     out one
>     >> value
>     >> of x at each loop. The loop starts from the second element and
>     ends
>     >> in the
>     >> fifth element of x. I also redefine the buffer size to see the
>     effect of
>     >> it. Here is my R code:
>     >>
>     >> R code
>     >> ```
>     >>> C_testPrint(as.numeric(1:10))
>     >> idx: 1, i: 0, ptr:2.000000
>     >> idx: 1, i: 1, ptr:3.000000
>     >> idx: 3, i: 0, ptr:4.000000
>     >> idx: 3, i: 1, ptr:5.000000
>     >>> C_testPrint(c(1,2,3,4,5,6,7,8,9,10))
>     >> idx: 1, i: 0, ptr:1.000000
>     >> idx: 1, i: 1, ptr:2.000000
>     >> idx: 1, i: 2, ptr:3.000000
>     >> idx: 1, i: 3, ptr:4.000000
>     >> idx: 1, i: 4, ptr:5.000000
>     >> ```
>     >>
>     >> There are two problems in the outputs:
>     >> 1. The numbers of lines are different
>     >> 2. The starting indices are not the same.
>     >>
>     >> ?From my understanding, the first output seems correct to me. The
>     >> second is
>     >> not unexpected. I believe the differences are due to the
>     >> accessibility of
>     >> the data pointer. Did I misunderstand and misuse the macro? Or
>     is it
>     >> a bug
>     >> in R? Here is my session info. My R is a bit outdated but the
>     macro
>     >> seems
>     >> unchanged in R 4.0. Thanks
>     >>
>     >> ```
>     >>> sessionInfo()
>     >> R Under development (unstable) (2019-08-22 r77060)
>     >> Platform: x86_64-w64-mingw32/x64 (64-bit)
>     >> Running under: Windows >= 8 x64 (build 9200)
>     >> ```
>     >>
>     >> ????[[alternative HTML version deleted]]
>     >>
>     >> ______________________________________________
>     >> R-devel at r-project.org <mailto:R-devel at r-project.org> mailing list
>     >> https://stat.ethz.ch/mailman/listinfo/r-devel
>     >
>     >
>


	[[alternative HTML version deleted]]


From jog @end|ng |rom tecn|co@pt  Mon Oct 28 14:13:54 2019
From: jog @end|ng |rom tecn|co@pt (=?UTF-8?Q?Jo=c3=a3o_Coelho_Garcia?=)
Date: Mon, 28 Oct 2019 13:13:54 +0000
Subject: [Rd] interception of expression evaluation. What is the right way
 to proceed?
Message-ID: <e9524b00-74dd-f249-f818-7b2f8aaa9b1d@tecnico.pt>

Hi all!

I am a computer science lecturer at IST (www.tecnico.ulisboa.pt) andhave 
been developing a voluntary computing environment for R together with 
some students. The idea is to allow R users to share their CPU time in 
the community. Both for security of the volunteers (to create a sandbox) 
and for debugging support, we have extended the R interpreter with a 
small intervention in the eval function.

My question is the following:

- Is there any way to transparently intercept all expressions in the R 
runtime? and in a way we should not have touched the internals of the 
interpreter..

- If changing the interpreter is the way to go, what is the correct way 
to propose this change to the R community?

 ??? Thank you for all help,

 ??? Best regards,

 ??? ??? Jo?o Garcia


From g@bembecker @end|ng |rom gm@||@com  Tue Oct 29 20:43:15 2019
From: g@bembecker @end|ng |rom gm@||@com (Gabriel Becker)
Date: Tue, 29 Oct 2019 12:43:15 -0700
Subject: [Rd] 
 head.matrix can return 1000s of columns -- limit to n or add
 new argument?
In-Reply-To: <CAD4oTHF_e4rtHu0S5bUu38+kQSEHXwun3Sv9-CbK+xN9tSc2hA@mail.gmail.com>
References: <CAPRVBcxsAkcgr3mDc4skTe8DTxJaYe2xyhk3QMk3odW1MGcyNA@mail.gmail.com>
 <CAB8pepxS2mRtd+522OwjixnM6ZjYxb+X0dc4vMGn6QxdFYBmVQ@mail.gmail.com>
 <CAD4oTHH=LpeEj6h6xYdOGsreS-GDCGdgJV5wpaUR3Pjuh5_M1w@mail.gmail.com>
 <CAPRVBczYZHO3JaFVfNeNNV0-YMRvX3CUYhUvLCjophDEoP+XgQ@mail.gmail.com>
 <23935.14449.965488.684663@stat.math.ethz.ch>
 <CAPRVBcwMY=GAVfFeCwowQsxP31Y=_B2ducosThN+8V91WESB8Q@mail.gmail.com>
 <20449_1568703158_x8H6qIDG009669_f8285123-371f-5d4b-1281-6cc085dba122@fredhutch.org>
 <22204_1568723392_x8HCSG3F015456_674779E0-B1F6-4DC2-9F1C-D5D48B044B66@mcmaster.ca>
 <AD725242-0FB7-4004-9F16-304E345A363A@mcmaster.ca>
 <23937.1791.389379.103733@stat.math.ethz.ch>
 <CAD4oTHF_e4rtHu0S5bUu38+kQSEHXwun3Sv9-CbK+xN9tSc2hA@mail.gmail.com>
Message-ID: <CAD4oTHFf7Zp+Xe3jvv7oirLWgZm7ZiSNWZvbrEsqJGfrL6Q2wQ@mail.gmail.com>

Hi all,

So I've started working on this and I ran into something that I didn't
know, namely that for x a multi-dimensional (2+) array, head(x) and tail(x)
ignore dimension completely, treat x as an atomic vector, and return an
(unclassed) atomic vector:

> x = array(100, c(4, 5, 5))

> dim(x)

[1] 4 5 5

> head(x, 1)

[1] 100

> class(head(x))

[1] "numeric"


(For a 1d array, it does return another 1d array).

When extending head/tail to understand multiple dimensions as discussed in
this thread, then, should the behavior for 2+d arrays be explicitly
retained, or should head and tail do the analogous thing (with a head(<2d
array>) behaving the same as head(<matrix>), which honestly is what I
expected to already be happening)?

Are people using/relying on this behavior in their code, and if so, why/for
what?

Even more generally, one way forward is to have the default methods check
for dimensions, and use length if it is null:

tail.default <- tail.data.frame <- function(x, n = 6L, ...)
{
    if(any(n == 0))
        stop("n must be non-zero or unspecified for all dimensions")
    if(!is.null(dim(x)))
        dimsx <- dim(x)
    else
        dimsx <- length(x)

    ## this returns a list of vectors of indices in each
    ## dimension, regardless of length of the the n
    ## argument
    sel <- lapply(seq_along(dimsx), function(i) {
        dxi <- dimsx[i]
        ## select all indices (full dim) if not specified
        ni <- if(length(n) >= i) n[i] else dxi
        ## handle negative ns
        ni <- if (ni < 0L) max(dxi + ni, 0L) else min(ni, dxi)
        seq.int(to = dxi, length.out = ni)
    })
    args <- c(list(x), sel, drop = FALSE)
    do.call("[", args)
}


I think this precludes the need for a separate data.frame method at all,
actually, though (I would think) tail.data.frame would still be defined and
exported for backwards compatibility. (the matrix method has some extra
bits so my current conception of it is still separate, though it might not
NEED to be).

The question then becomes, should head/tail always return something with
the same dimensionally (number of dims) it got, or should data.frame and
matrix be special cased in this regard, as they are now?

What are people's thoughts?
~G

	[[alternative HTML version deleted]]


From j@goreck| @end|ng |rom w|t@edu@p|  Wed Oct 30 06:31:57 2019
From: j@goreck| @end|ng |rom w|t@edu@p| (Jan Gorecki)
Date: Wed, 30 Oct 2019 11:01:57 +0530
Subject: [Rd] 
 head.matrix can return 1000s of columns -- limit to n or add
 new argument?
In-Reply-To: <CAD4oTHFf7Zp+Xe3jvv7oirLWgZm7ZiSNWZvbrEsqJGfrL6Q2wQ@mail.gmail.com>
References: <CAPRVBcxsAkcgr3mDc4skTe8DTxJaYe2xyhk3QMk3odW1MGcyNA@mail.gmail.com>
 <CAB8pepxS2mRtd+522OwjixnM6ZjYxb+X0dc4vMGn6QxdFYBmVQ@mail.gmail.com>
 <CAD4oTHH=LpeEj6h6xYdOGsreS-GDCGdgJV5wpaUR3Pjuh5_M1w@mail.gmail.com>
 <CAPRVBczYZHO3JaFVfNeNNV0-YMRvX3CUYhUvLCjophDEoP+XgQ@mail.gmail.com>
 <23935.14449.965488.684663@stat.math.ethz.ch>
 <CAPRVBcwMY=GAVfFeCwowQsxP31Y=_B2ducosThN+8V91WESB8Q@mail.gmail.com>
 <20449_1568703158_x8H6qIDG009669_f8285123-371f-5d4b-1281-6cc085dba122@fredhutch.org>
 <22204_1568723392_x8HCSG3F015456_674779E0-B1F6-4DC2-9F1C-D5D48B044B66@mcmaster.ca>
 <AD725242-0FB7-4004-9F16-304E345A363A@mcmaster.ca>
 <23937.1791.389379.103733@stat.math.ethz.ch>
 <CAD4oTHF_e4rtHu0S5bUu38+kQSEHXwun3Sv9-CbK+xN9tSc2hA@mail.gmail.com>
 <CAD4oTHFf7Zp+Xe3jvv7oirLWgZm7ZiSNWZvbrEsqJGfrL6Q2wQ@mail.gmail.com>
Message-ID: <CAOO9MKWMEbtoPtEc+H8XuiCfC1oJhdJR8aK0OWPk66wDHWq3RQ@mail.gmail.com>

Gabriel,
My view is rather radical.

- head/tail should return object having same number of dimensions
- data.frame should be a special case
- matrix should be handled as 2D array

P.S. idea of accepting `n` argument as a vector of corresponding
dimensions is a brilliant one

On Wed, Oct 30, 2019 at 1:13 AM Gabriel Becker <gabembecker at gmail.com> wrote:
>
> Hi all,
>
> So I've started working on this and I ran into something that I didn't
> know, namely that for x a multi-dimensional (2+) array, head(x) and tail(x)
> ignore dimension completely, treat x as an atomic vector, and return an
> (unclassed) atomic vector:
>
> > x = array(100, c(4, 5, 5))
>
> > dim(x)
>
> [1] 4 5 5
>
> > head(x, 1)
>
> [1] 100
>
> > class(head(x))
>
> [1] "numeric"
>
>
> (For a 1d array, it does return another 1d array).
>
> When extending head/tail to understand multiple dimensions as discussed in
> this thread, then, should the behavior for 2+d arrays be explicitly
> retained, or should head and tail do the analogous thing (with a head(<2d
> array>) behaving the same as head(<matrix>), which honestly is what I
> expected to already be happening)?
>
> Are people using/relying on this behavior in their code, and if so, why/for
> what?
>
> Even more generally, one way forward is to have the default methods check
> for dimensions, and use length if it is null:
>
> tail.default <- tail.data.frame <- function(x, n = 6L, ...)
> {
>     if(any(n == 0))
>         stop("n must be non-zero or unspecified for all dimensions")
>     if(!is.null(dim(x)))
>         dimsx <- dim(x)
>     else
>         dimsx <- length(x)
>
>     ## this returns a list of vectors of indices in each
>     ## dimension, regardless of length of the the n
>     ## argument
>     sel <- lapply(seq_along(dimsx), function(i) {
>         dxi <- dimsx[i]
>         ## select all indices (full dim) if not specified
>         ni <- if(length(n) >= i) n[i] else dxi
>         ## handle negative ns
>         ni <- if (ni < 0L) max(dxi + ni, 0L) else min(ni, dxi)
>         seq.int(to = dxi, length.out = ni)
>     })
>     args <- c(list(x), sel, drop = FALSE)
>     do.call("[", args)
> }
>
>
> I think this precludes the need for a separate data.frame method at all,
> actually, though (I would think) tail.data.frame would still be defined and
> exported for backwards compatibility. (the matrix method has some extra
> bits so my current conception of it is still separate, though it might not
> NEED to be).
>
> The question then becomes, should head/tail always return something with
> the same dimensionally (number of dims) it got, or should data.frame and
> matrix be special cased in this regard, as they are now?
>
> What are people's thoughts?
> ~G
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From cr@n @end|ng |rom wr|g@de  Wed Oct 30 08:28:19 2019
From: cr@n @end|ng |rom wr|g@de (Marvin Wright)
Date: Wed, 30 Oct 2019 08:28:19 +0100
Subject: [Rd] set.seed() in a package
Message-ID: <B880EAC4-01D4-4588-BAE2-D4337A10378A@wrig.de>

Hi all, 

I recently found several calls of set.seed() in a CRAN package. These calls are in a plot function, which could lead to unexpected behaviour. See https://github.com/sammo3182/interplot/issues/33 <https://github.com/sammo3182/interplot/issues/33> for a description of the problem. 

I checked the CRAN repository policies and could not find anything about this. I would have expected a policy against setting fixed seeds somewhere in a package. Am I missing something? 

Best, 

Marvin
	[[alternative HTML version deleted]]


From tdhock5 @end|ng |rom gm@||@com  Tue Oct 29 01:17:32 2019
From: tdhock5 @end|ng |rom gm@||@com (Toby Hocking)
Date: Mon, 28 Oct 2019 17:17:32 -0700
Subject: [Rd] stats::reshape quadratic in number of input columns
Message-ID: <CALK03d1pCRTsz=n0YBmVjWnrA0N9hafQh=Es7WBaU27s-81V2Q-1502@mail.gmail.com>

Hi R-core,

I have been performance testing R packages for wide-to-tall data reshaping
and for the most part I see they differ by constant factors.

However in one test, which involves converting into multiple output
columns, I see that stats::reshape is in fact quadratic in the number of
input columns. For example take the iris data, which has 4 input columns to
reshape, and the desired output has columns named
Species,Sepal,Petal,dimension (where part is either Length or Width). Of
course there is no performance issue with N=4 input columns in the original
iris data, but I made larger versions of this reshaping problem by making
copies of the input columns. The results
https://github.com/tdhock/nc-article#28-oct-2019 show that the quadratic
time complexity results in significant slowdowns after about N=10,000 input
columns to reshape. (e.g. several minutes for stats::reshape versus several
seconds for data.table::melt)

For a fix, I would suggest looking into how they implemented the same
operation in the data.table package, which in my test shows computation
times that seem to be linear.

Toby

	[[alternative HTML version deleted]]


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Wed Oct 30 12:29:45 2019
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Wed, 30 Oct 2019 12:29:45 +0100
Subject: [Rd] 
 head.matrix can return 1000s of columns -- limit to n or add
 new argument?
In-Reply-To: <CAD4oTHFf7Zp+Xe3jvv7oirLWgZm7ZiSNWZvbrEsqJGfrL6Q2wQ@mail.gmail.com>
References: <CAPRVBcxsAkcgr3mDc4skTe8DTxJaYe2xyhk3QMk3odW1MGcyNA@mail.gmail.com>
 <CAB8pepxS2mRtd+522OwjixnM6ZjYxb+X0dc4vMGn6QxdFYBmVQ@mail.gmail.com>
 <CAD4oTHH=LpeEj6h6xYdOGsreS-GDCGdgJV5wpaUR3Pjuh5_M1w@mail.gmail.com>
 <CAPRVBczYZHO3JaFVfNeNNV0-YMRvX3CUYhUvLCjophDEoP+XgQ@mail.gmail.com>
 <23935.14449.965488.684663@stat.math.ethz.ch>
 <CAPRVBcwMY=GAVfFeCwowQsxP31Y=_B2ducosThN+8V91WESB8Q@mail.gmail.com>
 <20449_1568703158_x8H6qIDG009669_f8285123-371f-5d4b-1281-6cc085dba122@fredhutch.org>
 <22204_1568723392_x8HCSG3F015456_674779E0-B1F6-4DC2-9F1C-D5D48B044B66@mcmaster.ca>
 <AD725242-0FB7-4004-9F16-304E345A363A@mcmaster.ca>
 <23937.1791.389379.103733@stat.math.ethz.ch>
 <CAD4oTHF_e4rtHu0S5bUu38+kQSEHXwun3Sv9-CbK+xN9tSc2hA@mail.gmail.com>
 <CAD4oTHFf7Zp+Xe3jvv7oirLWgZm7ZiSNWZvbrEsqJGfrL6Q2wQ@mail.gmail.com>
Message-ID: <23993.29737.690600.313456@stat.math.ethz.ch>

>>>>> Gabriel Becker 
>>>>>     on Tue, 29 Oct 2019 12:43:15 -0700 writes:

    > Hi all,
    > So I've started working on this and I ran into something that I didn't
    > know, namely that for x a multi-dimensional (2+) array, head(x) and tail(x)
    > ignore dimension completely, treat x as an atomic vector, and return an
    > (unclassed) atomic vector:

Well, that's  (3+), not "2+" .

But I did write (on Sep 17 in this thread!)

  > The current source for head() and tail() and all their methods
  > in utils is just 83 lines of code  {file utils/R/head.R minus
  > the initial mostly copyright comments}.

and if've ever looked at these few dozen of R code lines, you'll
have seen that we just added two simple utilities with a few
reasonable simple methods.  To treat non-matrix (i.e. non-2d)
arrays as vectors, is typically not unreasonable in R, but
indeed with your proposals (in this thread), such non-2d arrays
should be treated differently either via new  head.array() /
tail.array() methods ((or -- only if it can be done more nicely -- by
the default method)).

Note however the following  historical quirk :

> sapply(setNames(,1:5), function(K) inherits(array(pi, dim=1:K), "array"))
    1     2     3     4     5 
 TRUE FALSE  TRUE  TRUE  TRUE 

(Is this something we should consider changing for R 4.0.0 -- to
 have it TRUE also for 2d-arrays aka matrix objects ??)

The consequence of that is that
currently, "often"   foo.matrix is just a copy of foo.array  in
the case the latter exists:
"base" examples: foo in {unique, duplicated, anyDuplicated}.

So I propose you change current  head.matrix and tail.matrix  to
head.array and tail.array
(and then have   head.matrix <- head.array  etc, at least if the
 above quirk must remain, or remains (which I currently guess to
 be the case)).


    >> x = array(100, c(4, 5, 5))

    >> dim(x)

    > [1] 4 5 5

    >> head(x, 1)

    > [1] 100

    >> class(head(x))

    > [1] "numeric"


    > (For a 1d array, it does return another 1d array).

    > When extending head/tail to understand multiple dimensions as discussed in
    > this thread, then, should the behavior for 2+d arrays be explicitly
    > retained, or should head and tail do the analogous thing (with a head(<2d
    array> ) behaving the same as head(<matrix>), which honestly is what I
    > expected to already be happening)?

    > Are people using/relying on this behavior in their code, and if so, why/for
    > what?

    > Even more generally, one way forward is to have the default methods check
    > for dimensions, and use length if it is null:

    > tail.default <- tail.data.frame <- function(x, n = 6L, ...)
    > {
    > if(any(n == 0))
    > stop("n must be non-zero or unspecified for all dimensions")
    > if(!is.null(dim(x)))
    > dimsx <- dim(x)
    > else
    > dimsx <- length(x)

    > ## this returns a list of vectors of indices in each
    > ## dimension, regardless of length of the the n
    > ## argument
    > sel <- lapply(seq_along(dimsx), function(i) {
    > dxi <- dimsx[i]
    > ## select all indices (full dim) if not specified
    > ni <- if(length(n) >= i) n[i] else dxi
    > ## handle negative ns
    > ni <- if (ni < 0L) max(dxi + ni, 0L) else min(ni, dxi)
    > seq.int(to = dxi, length.out = ni)
    > })
    > args <- c(list(x), sel, drop = FALSE)
    > do.call("[", args)
    > }


    > I think this precludes the need for a separate data.frame method at all,
    > actually, though (I would think) tail.data.frame would still be defined and
    > exported for backwards compatibility. (the matrix method has some extra
    > bits so my current conception of it is still separate, though it might not
    > NEED to be).

    > The question then becomes, should head/tail always return something with
    > the same dimensionally (number of dims) it got, or should data.frame and
    > matrix be special cased in this regard, as they are now?

    > What are people's thoughts?
    > ~G

    > [[alternative HTML version deleted]]


From murdoch@dunc@n @end|ng |rom gm@||@com  Wed Oct 30 13:46:47 2019
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Wed, 30 Oct 2019 08:46:47 -0400
Subject: [Rd] set.seed() in a package
In-Reply-To: <B880EAC4-01D4-4588-BAE2-D4337A10378A@wrig.de>
References: <B880EAC4-01D4-4588-BAE2-D4337A10378A@wrig.de>
Message-ID: <7e3c5f47-e9b4-0e18-573f-5499f444c21a@gmail.com>

On 30/10/2019 3:28 a.m., Marvin Wright wrote:
> Hi all,
> 
> I recently found several calls of set.seed() in a CRAN package. These calls are in a plot function, which could lead to unexpected behaviour. See https://github.com/sammo3182/interplot/issues/33 <https://github.com/sammo3182/interplot/issues/33> for a description of the problem.
> 
> I checked the CRAN repository policies and could not find anything about this. I would have expected a policy against setting fixed seeds somewhere in a package. Am I missing something?

set.seed() writes .Random.seed in the user's global environment, which 
violates this policy:

- Packages should not modify the global environment (user?s workspace).

However, every call to a random number generator creates or modifies 
.Random.seed as well, and most of those are expected and shouldn't be 
flagged.  And interplot() is documented to do random simulations, so it 
would be expected to change the seed:  the issue is that given the same 
inputs it always changes it to the same thing.  I think that would be 
quite hard for a test to detect.

Should it be a policy with no test?  Maybe, because I agree with you 
that interplot()'s set.seed(324) is bad practice.

Duncan Murdoch


From pd@|gd @end|ng |rom gm@||@com  Wed Oct 30 14:08:29 2019
From: pd@|gd @end|ng |rom gm@||@com (peter dalgaard)
Date: Wed, 30 Oct 2019 14:08:29 +0100
Subject: [Rd] set.seed() in a package
In-Reply-To: <7e3c5f47-e9b4-0e18-573f-5499f444c21a@gmail.com>
References: <B880EAC4-01D4-4588-BAE2-D4337A10378A@wrig.de>
 <7e3c5f47-e9b4-0e18-573f-5499f444c21a@gmail.com>
Message-ID: <03E4EF62-11A2-47EE-854F-FF872F19E2FA@gmail.com>

We commit a similar sin in the help pages, e.g.

example(set.seed) ; runif(2)
example(set.seed) ; runif(2)

gives you the same random uniforms both times. (Of course it isn't that much of an issue, since you would rarely be running examples before any serious simulations.)

You can fairly easily work around that by saving and restoring .Random.seed. I wonder if that isn't also true of the cases using set.seed() for other reasons? 

-pd


> On 30 Oct 2019, at 13:46 , Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
> 
> On 30/10/2019 3:28 a.m., Marvin Wright wrote:
>> Hi all,
>> I recently found several calls of set.seed() in a CRAN package. These calls are in a plot function, which could lead to unexpected behaviour. See https://github.com/sammo3182/interplot/issues/33 <https://github.com/sammo3182/interplot/issues/33> for a description of the problem.
>> I checked the CRAN repository policies and could not find anything about this. I would have expected a policy against setting fixed seeds somewhere in a package. Am I missing something?
> 
> set.seed() writes .Random.seed in the user's global environment, which violates this policy:
> 
> - Packages should not modify the global environment (user?s workspace).
> 
> However, every call to a random number generator creates or modifies .Random.seed as well, and most of those are expected and shouldn't be flagged.  And interplot() is documented to do random simulations, so it would be expected to change the seed:  the issue is that given the same inputs it always changes it to the same thing.  I think that would be quite hard for a test to detect.
> 
> Should it be a policy with no test?  Maybe, because I agree with you that interplot()'s set.seed(324) is bad practice.
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From murdoch@dunc@n @end|ng |rom gm@||@com  Wed Oct 30 14:22:37 2019
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Wed, 30 Oct 2019 09:22:37 -0400
Subject: [Rd] set.seed() in a package
In-Reply-To: <03E4EF62-11A2-47EE-854F-FF872F19E2FA@gmail.com>
References: <B880EAC4-01D4-4588-BAE2-D4337A10378A@wrig.de>
 <7e3c5f47-e9b4-0e18-573f-5499f444c21a@gmail.com>
 <03E4EF62-11A2-47EE-854F-FF872F19E2FA@gmail.com>
Message-ID: <e2f26e0b-5499-1eb4-55f6-c063c15c2622@gmail.com>

On 30/10/2019 9:08 a.m., peter dalgaard wrote:
> We commit a similar sin in the help pages, e.g.
> 
> example(set.seed) ; runif(2)
> example(set.seed) ; runif(2)
> 
> gives you the same random uniforms both times. (Of course it isn't that much of an issue, since you would rarely be running examples before any serious simulations.)

I think it's pretty common in example code, and that's justifiable.  But 
it could be avoided by using withr::with_seed() or something equivalent.

Duncan Murdoch

> 
> You can fairly easily work around that by saving and restoring .Random.seed. I wonder if that isn't also true of the cases using set.seed() for other reasons?
> 
> -pd
> 
> 
>> On 30 Oct 2019, at 13:46 , Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
>>
>> On 30/10/2019 3:28 a.m., Marvin Wright wrote:
>>> Hi all,
>>> I recently found several calls of set.seed() in a CRAN package. These calls are in a plot function, which could lead to unexpected behaviour. See https://github.com/sammo3182/interplot/issues/33 <https://github.com/sammo3182/interplot/issues/33> for a description of the problem.
>>> I checked the CRAN repository policies and could not find anything about this. I would have expected a policy against setting fixed seeds somewhere in a package. Am I missing something?
>>
>> set.seed() writes .Random.seed in the user's global environment, which violates this policy:
>>
>> - Packages should not modify the global environment (user?s workspace).
>>
>> However, every call to a random number generator creates or modifies .Random.seed as well, and most of those are expected and shouldn't be flagged.  And interplot() is documented to do random simulations, so it would be expected to change the seed:  the issue is that given the same inputs it always changes it to the same thing.  I think that would be quite hard for a test to detect.
>>
>> Should it be a policy with no test?  Maybe, because I agree with you that interplot()'s set.seed(324) is bad practice.
>>
>> Duncan Murdoch
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From henr|k@bengt@@on @end|ng |rom gm@||@com  Wed Oct 30 17:50:18 2019
From: henr|k@bengt@@on @end|ng |rom gm@||@com (Henrik Bengtsson)
Date: Wed, 30 Oct 2019 09:50:18 -0700
Subject: [Rd] set.seed() in a package
In-Reply-To: <e2f26e0b-5499-1eb4-55f6-c063c15c2622@gmail.com>
References: <B880EAC4-01D4-4588-BAE2-D4337A10378A@wrig.de>
 <7e3c5f47-e9b4-0e18-573f-5499f444c21a@gmail.com>
 <03E4EF62-11A2-47EE-854F-FF872F19E2FA@gmail.com>
 <e2f26e0b-5499-1eb4-55f6-c063c15c2622@gmail.com>
Message-ID: <CAFDcVCQM3uF6tg78WQkEb4F_u-Sc3e13qNidVfya6Rgz2QR-gQ@mail.gmail.com>

> On 30/10/2019 9:08 a.m., peter dalgaard wrote:
> > You can fairly easily work around that by saving and restoring .Random.seed.

This is actually quite tedious to get correct; it requires you to
under how and when .Random.seed is set, and what are valid values on
.Random.seed.   For instance, a common mistake (me too) is to reset to
.GlobalEnv$.Random.seed <- NULL in a fresh R session but this will
produce a warning on: ".Random.seed' is not an integer vector but of
type 'NULL', so ignored".  You end up having to do things such as:

  oseed <- .GlobalEnv$.Random.seed
  on.exit({
    if (is.null(oseed)) {
      rm(list=".Random.seed", envir= .GlobalEnv)
    } else {
      assign(".Random.seed", value=oseed, envir= .GlobalEnv)
    }
  })

to avoid that warning.  So, having support functions for this in base
R would be helpful, e.g.

  oseed <- base::getRandomSeed()
  on.exit(base::setRandomSeed(oseed))

Back to Marvin's point/question.  I think it would be useful if the
CRAN Policies would explicitly say that functions must not change the
random seed to a fixed one, without undoing it, unless the user
specifies it via an argument.  If not, there's a great risk it will
mess up statistical analysis.  Also, if there would a way to test
against such practices, which I think is really hard, I would be the
first one backing it up.

On a related note; there are packages that forward the .Random.seed
when loaded.  This is also an unfortunate behavior because you will
give different RNGs depending on that package was already loaded
before you called a function that depends on it or not.  For example,
if pkgA forwards the .Random.seed when loaded, the following is *not*
reproducible:

# User might or might not have loaded pkgA already
if (runif(1) < 0.5) loadNamespace(pkgA)

set.seed(0xBEEF)
loadNamespace(pkgA)
y <- runif(1)

I ran into this problem when doing some strict testing.  I argue this
also falls into the set of "bad" practices to be avoided.

/Henrik

On Wed, Oct 30, 2019 at 6:23 AM Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
>
> On 30/10/2019 9:08 a.m., peter dalgaard wrote:
> > We commit a similar sin in the help pages, e.g.
> >
> > example(set.seed) ; runif(2)
> > example(set.seed) ; runif(2)
> >
> > gives you the same random uniforms both times. (Of course it isn't that much of an issue, since you would rarely be running examples before any serious simulations.)
>
> I think it's pretty common in example code, and that's justifiable.  But
> it could be avoided by using withr::with_seed() or something equivalent.
>
> Duncan Murdoch
>
> >
> > You can fairly easily work around that by saving and restoring .Random.seed. I wonder if that isn't also true of the cases using set.seed() for other reasons?
> >
> > -pd
> >
> >
> >> On 30 Oct 2019, at 13:46 , Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
> >>
> >> On 30/10/2019 3:28 a.m., Marvin Wright wrote:
> >>> Hi all,
> >>> I recently found several calls of set.seed() in a CRAN package. These calls are in a plot function, which could lead to unexpected behaviour. See https://github.com/sammo3182/interplot/issues/33 <https://github.com/sammo3182/interplot/issues/33> for a description of the problem.
> >>> I checked the CRAN repository policies and could not find anything about this. I would have expected a policy against setting fixed seeds somewhere in a package. Am I missing something?
> >>
> >> set.seed() writes .Random.seed in the user's global environment, which violates this policy:
> >>
> >> - Packages should not modify the global environment (user?s workspace).
> >>
> >> However, every call to a random number generator creates or modifies .Random.seed as well, and most of those are expected and shouldn't be flagged.  And interplot() is documented to do random simulations, so it would be expected to change the seed:  the issue is that given the same inputs it always changes it to the same thing.  I think that would be quite hard for a test to detect.
> >>
> >> Should it be a policy with no test?  Maybe, because I agree with you that interplot()'s set.seed(324) is bad practice.
> >>
> >> Duncan Murdoch
> >>
> >> ______________________________________________
> >> R-devel at r-project.org mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-devel
> >
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From henr|k@bengt@@on @end|ng |rom gm@||@com  Wed Oct 30 17:59:07 2019
From: henr|k@bengt@@on @end|ng |rom gm@||@com (Henrik Bengtsson)
Date: Wed, 30 Oct 2019 09:59:07 -0700
Subject: [Rd] set.seed() in a package
In-Reply-To: <CAFDcVCQM3uF6tg78WQkEb4F_u-Sc3e13qNidVfya6Rgz2QR-gQ@mail.gmail.com>
References: <B880EAC4-01D4-4588-BAE2-D4337A10378A@wrig.de>
 <7e3c5f47-e9b4-0e18-573f-5499f444c21a@gmail.com>
 <03E4EF62-11A2-47EE-854F-FF872F19E2FA@gmail.com>
 <e2f26e0b-5499-1eb4-55f6-c063c15c2622@gmail.com>
 <CAFDcVCQM3uF6tg78WQkEb4F_u-Sc3e13qNidVfya6Rgz2QR-gQ@mail.gmail.com>
Message-ID: <CAFDcVCT5DMz1edX0CQbt45YdDJxeq0pT5cS=UAZ9k6RDUfCMSw@mail.gmail.com>

Forgot to say: For,

  oseed <- base::getRandomSeed()
  on.exit(base::setRandomSeed(oseed))

one could upgrade set.seed() to take this role, e.g.

  oseed <- set.seed(0xBEEF)
  on.exit(set.seed(oseed))

Current, set.seed() always return NULL.

BTW, and my memory might be bad, I think I mentioned this in the past
but was told that you cannot reset the RNG state for all types of RNG
kinds.  That might complicate things, but on the other hand, that
could be checked for at run-time by the above functions.

/Henrik

On Wed, Oct 30, 2019 at 9:50 AM Henrik Bengtsson
<henrik.bengtsson at gmail.com> wrote:
>
> > On 30/10/2019 9:08 a.m., peter dalgaard wrote:
> > > You can fairly easily work around that by saving and restoring .Random.seed.
>
> This is actually quite tedious to get correct; it requires you to
> under how and when .Random.seed is set, and what are valid values on
> .Random.seed.   For instance, a common mistake (me too) is to reset to
> .GlobalEnv$.Random.seed <- NULL in a fresh R session but this will
> produce a warning on: ".Random.seed' is not an integer vector but of
> type 'NULL', so ignored".  You end up having to do things such as:
>
>   oseed <- .GlobalEnv$.Random.seed
>   on.exit({
>     if (is.null(oseed)) {
>       rm(list=".Random.seed", envir= .GlobalEnv)
>     } else {
>       assign(".Random.seed", value=oseed, envir= .GlobalEnv)
>     }
>   })
>
> to avoid that warning.  So, having support functions for this in base
> R would be helpful, e.g.
>
>   oseed <- base::getRandomSeed()
>   on.exit(base::setRandomSeed(oseed))
>
> Back to Marvin's point/question.  I think it would be useful if the
> CRAN Policies would explicitly say that functions must not change the
> random seed to a fixed one, without undoing it, unless the user
> specifies it via an argument.  If not, there's a great risk it will
> mess up statistical analysis.  Also, if there would a way to test
> against such practices, which I think is really hard, I would be the
> first one backing it up.
>
> On a related note; there are packages that forward the .Random.seed
> when loaded.  This is also an unfortunate behavior because you will
> give different RNGs depending on that package was already loaded
> before you called a function that depends on it or not.  For example,
> if pkgA forwards the .Random.seed when loaded, the following is *not*
> reproducible:
>
> # User might or might not have loaded pkgA already
> if (runif(1) < 0.5) loadNamespace(pkgA)
>
> set.seed(0xBEEF)
> loadNamespace(pkgA)
> y <- runif(1)
>
> I ran into this problem when doing some strict testing.  I argue this
> also falls into the set of "bad" practices to be avoided.
>
> /Henrik
>
> On Wed, Oct 30, 2019 at 6:23 AM Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
> >
> > On 30/10/2019 9:08 a.m., peter dalgaard wrote:
> > > We commit a similar sin in the help pages, e.g.
> > >
> > > example(set.seed) ; runif(2)
> > > example(set.seed) ; runif(2)
> > >
> > > gives you the same random uniforms both times. (Of course it isn't that much of an issue, since you would rarely be running examples before any serious simulations.)
> >
> > I think it's pretty common in example code, and that's justifiable.  But
> > it could be avoided by using withr::with_seed() or something equivalent.
> >
> > Duncan Murdoch
> >
> > >
> > > You can fairly easily work around that by saving and restoring .Random.seed. I wonder if that isn't also true of the cases using set.seed() for other reasons?
> > >
> > > -pd
> > >
> > >
> > >> On 30 Oct 2019, at 13:46 , Duncan Murdoch <murdoch.duncan at gmail.com> wrote:
> > >>
> > >> On 30/10/2019 3:28 a.m., Marvin Wright wrote:
> > >>> Hi all,
> > >>> I recently found several calls of set.seed() in a CRAN package. These calls are in a plot function, which could lead to unexpected behaviour. See https://github.com/sammo3182/interplot/issues/33 <https://github.com/sammo3182/interplot/issues/33> for a description of the problem.
> > >>> I checked the CRAN repository policies and could not find anything about this. I would have expected a policy against setting fixed seeds somewhere in a package. Am I missing something?
> > >>
> > >> set.seed() writes .Random.seed in the user's global environment, which violates this policy:
> > >>
> > >> - Packages should not modify the global environment (user?s workspace).
> > >>
> > >> However, every call to a random number generator creates or modifies .Random.seed as well, and most of those are expected and shouldn't be flagged.  And interplot() is documented to do random simulations, so it would be expected to change the seed:  the issue is that given the same inputs it always changes it to the same thing.  I think that would be quite hard for a test to detect.
> > >>
> > >> Should it be a policy with no test?  Maybe, because I agree with you that interplot()'s set.seed(324) is bad practice.
> > >>
> > >> Duncan Murdoch
> > >>
> > >> ______________________________________________
> > >> R-devel at r-project.org mailing list
> > >> https://stat.ethz.ch/mailman/listinfo/r-devel
> > >
> >
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel


From g@bembecker @end|ng |rom gm@||@com  Thu Oct 31 20:46:35 2019
From: g@bembecker @end|ng |rom gm@||@com (Gabriel Becker)
Date: Thu, 31 Oct 2019 12:46:35 -0700
Subject: [Rd] 
 head.matrix can return 1000s of columns -- limit to n or add
 new argument?
In-Reply-To: <23993.29737.690600.313456@stat.math.ethz.ch>
References: <CAPRVBcxsAkcgr3mDc4skTe8DTxJaYe2xyhk3QMk3odW1MGcyNA@mail.gmail.com>
 <CAB8pepxS2mRtd+522OwjixnM6ZjYxb+X0dc4vMGn6QxdFYBmVQ@mail.gmail.com>
 <CAD4oTHH=LpeEj6h6xYdOGsreS-GDCGdgJV5wpaUR3Pjuh5_M1w@mail.gmail.com>
 <CAPRVBczYZHO3JaFVfNeNNV0-YMRvX3CUYhUvLCjophDEoP+XgQ@mail.gmail.com>
 <23935.14449.965488.684663@stat.math.ethz.ch>
 <CAPRVBcwMY=GAVfFeCwowQsxP31Y=_B2ducosThN+8V91WESB8Q@mail.gmail.com>
 <20449_1568703158_x8H6qIDG009669_f8285123-371f-5d4b-1281-6cc085dba122@fredhutch.org>
 <22204_1568723392_x8HCSG3F015456_674779E0-B1F6-4DC2-9F1C-D5D48B044B66@mcmaster.ca>
 <AD725242-0FB7-4004-9F16-304E345A363A@mcmaster.ca>
 <23937.1791.389379.103733@stat.math.ethz.ch>
 <CAD4oTHF_e4rtHu0S5bUu38+kQSEHXwun3Sv9-CbK+xN9tSc2hA@mail.gmail.com>
 <CAD4oTHFf7Zp+Xe3jvv7oirLWgZm7ZiSNWZvbrEsqJGfrL6Q2wQ@mail.gmail.com>
 <23993.29737.690600.313456@stat.math.ethz.ch>
Message-ID: <CAD4oTHH=BNz=_8h=5H9FFr8fDR40dr_ZBAHBbL6=7RUxmd=8SA@mail.gmail.com>

Hi Martin,


On Wed, Oct 30, 2019 at 4:30 AM Martin Maechler <maechler at stat.math.ethz.ch>
wrote:

> >>>>> Gabriel Becker
> >>>>>     on Tue, 29 Oct 2019 12:43:15 -0700 writes:
>
>     > Hi all,
>     > So I've started working on this and I ran into something that I
> didn't
>     > know, namely that for x a multi-dimensional (2+) array, head(x) and
> tail(x)
>     > ignore dimension completely, treat x as an atomic vector, and return
> an
>     > (unclassed) atomic vector:
>
> Well, that's  (3+), not "2+" .
>

You're correct, of course. Apologies for that.

>
> But I did write (on Sep 17 in this thread!)
>
>   > The current source for head() and tail() and all their methods
>   > in utils is just 83 lines of code  {file utils/R/head.R minus
>   > the initial mostly copyright comments}.
>
> and if've ever looked at these few dozen of R code lines, you'll
> have seen that we just added two simple utilities with a few
> reasonable simple methods.  To treat non-matrix (i.e. non-2d)
> arrays as vectors, is typically not unreasonable in R, but
> indeed with your proposals (in this thread), such non-2d arrays
> should be treated differently either via new  head.array() /
> tail.array() methods ((or -- only if it can be done more nicely -- by
> the default method)).
>

I hope you didn't construe my describing surprise (which was honest)  as a
criticism. It just quite literally not what I thought head(array(100, c(25,
2, 2))) would have done based on what head.matrix does is all.


>
> Note however the following  historical quirk :
>
> > sapply(setNames(,1:5), function(K) inherits(array(pi, dim=1:K), "array"))
>     1     2     3     4     5
>  TRUE FALSE  TRUE  TRUE  TRUE
>
> (Is this something we should consider changing for R 4.0.0 -- to
>  have it TRUE also for 2d-arrays aka matrix objects ??)
>

That is pretty odd. IMHO It would be quite nice from a design perspective
to fix that, but I do wonder, as I infer you do as well, how much code it
would break.

Changing this would cause problems in any case where a generic has an array
method but no matrix method, as well as any code that explicitly checks for
inherits from "array" assuming matrices won't return true, correct? My
intuition is that the former would be pretty rare, though it might be a fun
little problem to figure it out.  The latter is ...probably also fairly
rare? My intuition on that one is less strong though.


>
> The consequence of that is that
> currently, "often"   foo.matrix is just a copy of foo.array  in
> the case the latter exists:
> "base" examples: foo in {unique, duplicated, anyDuplicated}.
>
> So I propose you change current  head.matrix and tail.matrix  to
> head.array and tail.array
> (and then have   head.matrix <- head.array  etc, at least if the
>  above quirk must remain, or remains (which I currently guess to
>  be the case)).
>
>

Absolutely, will do. I'm gratified we're going after the more general
approach. Thanks for working with us on this.

Best,
~G


>
>     >> x = array(100, c(4, 5, 5))
>
>     >> dim(x)
>
>     > [1] 4 5 5
>
>     >> head(x, 1)
>
>     > [1] 100
>
>     >> class(head(x))
>
>     > [1] "numeric"
>
>
>     > (For a 1d array, it does return another 1d array).
>
>     > When extending head/tail to understand multiple dimensions as
> discussed in
>     > this thread, then, should the behavior for 2+d arrays be explicitly
>     > retained, or should head and tail do the analogous thing (with a
> head(<2d
>     array> ) behaving the same as head(<matrix>), which honestly is what I
>     > expected to already be happening)?
>
>     > Are people using/relying on this behavior in their code, and if so,
> why/for
>     > what?
>
>     > Even more generally, one way forward is to have the default methods
> check
>     > for dimensions, and use length if it is null:
>
>     > tail.default <- tail.data.frame <- function(x, n = 6L, ...)
>     > {
>     > if(any(n == 0))
>     > stop("n must be non-zero or unspecified for all dimensions")
>     > if(!is.null(dim(x)))
>     > dimsx <- dim(x)
>     > else
>     > dimsx <- length(x)
>
>     > ## this returns a list of vectors of indices in each
>     > ## dimension, regardless of length of the the n
>     > ## argument
>     > sel <- lapply(seq_along(dimsx), function(i) {
>     > dxi <- dimsx[i]
>     > ## select all indices (full dim) if not specified
>     > ni <- if(length(n) >= i) n[i] else dxi
>     > ## handle negative ns
>     > ni <- if (ni < 0L) max(dxi + ni, 0L) else min(ni, dxi)
>     > seq.int(to = dxi, length.out = ni)
>     > })
>     > args <- c(list(x), sel, drop = FALSE)
>     > do.call("[", args)
>     > }
>
>
>     > I think this precludes the need for a separate data.frame method at
> all,
>     > actually, though (I would think) tail.data.frame would still be
> defined and
>     > exported for backwards compatibility. (the matrix method has some
> extra
>     > bits so my current conception of it is still separate, though it
> might not
>     > NEED to be).
>
>     > The question then becomes, should head/tail always return something
> with
>     > the same dimensionally (number of dims) it got, or should data.frame
> and
>     > matrix be special cased in this regard, as they are now?
>
>     > What are people's thoughts?
>     > ~G
>
>     > [[alternative HTML version deleted]]
>
>

	[[alternative HTML version deleted]]


From hp@ge@ @end|ng |rom |redhutch@org  Thu Oct 31 22:02:07 2019
From: hp@ge@ @end|ng |rom |redhutch@org (Pages, Herve)
Date: Thu, 31 Oct 2019 21:02:07 +0000
Subject: [Rd] 
 head.matrix can return 1000s of columns -- limit to n or add
 new argument?
In-Reply-To: <23993.29737.690600.313456@stat.math.ethz.ch>
References: <CAPRVBcxsAkcgr3mDc4skTe8DTxJaYe2xyhk3QMk3odW1MGcyNA@mail.gmail.com>
 <CAB8pepxS2mRtd+522OwjixnM6ZjYxb+X0dc4vMGn6QxdFYBmVQ@mail.gmail.com>
 <CAD4oTHH=LpeEj6h6xYdOGsreS-GDCGdgJV5wpaUR3Pjuh5_M1w@mail.gmail.com>
 <CAPRVBczYZHO3JaFVfNeNNV0-YMRvX3CUYhUvLCjophDEoP+XgQ@mail.gmail.com>
 <23935.14449.965488.684663@stat.math.ethz.ch>
 <CAPRVBcwMY=GAVfFeCwowQsxP31Y=_B2ducosThN+8V91WESB8Q@mail.gmail.com>
 <20449_1568703158_x8H6qIDG009669_f8285123-371f-5d4b-1281-6cc085dba122@fredhutch.org>
 <22204_1568723392_x8HCSG3F015456_674779E0-B1F6-4DC2-9F1C-D5D48B044B66@mcmaster.ca>
 <AD725242-0FB7-4004-9F16-304E345A363A@mcmaster.ca>
 <23937.1791.389379.103733@stat.math.ethz.ch>
 <CAD4oTHF_e4rtHu0S5bUu38+kQSEHXwun3Sv9-CbK+xN9tSc2hA@mail.gmail.com>
 <CAD4oTHFf7Zp+Xe3jvv7oirLWgZm7ZiSNWZvbrEsqJGfrL6Q2wQ@mail.gmail.com>
 <23993.29737.690600.313456@stat.math.ethz.ch>
Message-ID: <2f877775-521a-7fc1-b68b-d233d4a50bea@fredhutch.org>

On 10/30/19 04:29, Martin Maechler wrote:
>>>>>> Gabriel Becker
>>>>>>      on Tue, 29 Oct 2019 12:43:15 -0700 writes:
> 
>      > Hi all,
>      > So I've started working on this and I ran into something that I didn't
>      > know, namely that for x a multi-dimensional (2+) array, head(x) and tail(x)
>      > ignore dimension completely, treat x as an atomic vector, and return an
>      > (unclassed) atomic vector:
> 
> Well, that's  (3+), not "2+" .
> 
> But I did write (on Sep 17 in this thread!)
> 
>    > The current source for head() and tail() and all their methods
>    > in utils is just 83 lines of code  {file utils/R/head.R minus
>    > the initial mostly copyright comments}.
> 
> and if've ever looked at these few dozen of R code lines, you'll
> have seen that we just added two simple utilities with a few
> reasonable simple methods.  To treat non-matrix (i.e. non-2d)
> arrays as vectors, is typically not unreasonable in R, but
> indeed with your proposals (in this thread), such non-2d arrays
> should be treated differently either via new  head.array() /
> tail.array() methods ((or -- only if it can be done more nicely -- by
> the default method)).
> 
> Note however the following  historical quirk :
> 
>> sapply(setNames(,1:5), function(K) inherits(array(pi, dim=1:K), "array"))
>      1     2     3     4     5
>   TRUE FALSE  TRUE  TRUE  TRUE
> 
> (Is this something we should consider changing for R 4.0.0 -- to
>   have it TRUE also for 2d-arrays aka matrix objects ??)

That would be awesome! More generally I wonder how feasible it would be 
to fix all these inheritance quirks where inherits(x, "something"), 
is(x, "something"), and is.something(x) disagree. They've been such a 
nuisance for so many years...

Thanks,
H.


> 
> The consequence of that is that
> currently, "often"   foo.matrix is just a copy of foo.array  in
> the case the latter exists:
> "base" examples: foo in {unique, duplicated, anyDuplicated}.
> 
> So I propose you change current  head.matrix and tail.matrix  to
> head.array and tail.array
> (and then have   head.matrix <- head.array  etc, at least if the
>   above quirk must remain, or remains (which I currently guess to
>   be the case)).
> 
> 
>      >> x = array(100, c(4, 5, 5))
> 
>      >> dim(x)
> 
>      > [1] 4 5 5
> 
>      >> head(x, 1)
> 
>      > [1] 100
> 
>      >> class(head(x))
> 
>      > [1] "numeric"
> 
> 
>      > (For a 1d array, it does return another 1d array).
> 
>      > When extending head/tail to understand multiple dimensions as discussed in
>      > this thread, then, should the behavior for 2+d arrays be explicitly
>      > retained, or should head and tail do the analogous thing (with a head(<2d
>      array> ) behaving the same as head(<matrix>), which honestly is what I
>      > expected to already be happening)?
> 
>      > Are people using/relying on this behavior in their code, and if so, why/for
>      > what?
> 
>      > Even more generally, one way forward is to have the default methods check
>      > for dimensions, and use length if it is null:
> 
>      > tail.default <- tail.data.frame <- function(x, n = 6L, ...)
>      > {
>      > if(any(n == 0))
>      > stop("n must be non-zero or unspecified for all dimensions")
>      > if(!is.null(dim(x)))
>      > dimsx <- dim(x)
>      > else
>      > dimsx <- length(x)
> 
>      > ## this returns a list of vectors of indices in each
>      > ## dimension, regardless of length of the the n
>      > ## argument
>      > sel <- lapply(seq_along(dimsx), function(i) {
>      > dxi <- dimsx[i]
>      > ## select all indices (full dim) if not specified
>      > ni <- if(length(n) >= i) n[i] else dxi
>      > ## handle negative ns
>      > ni <- if (ni < 0L) max(dxi + ni, 0L) else min(ni, dxi)
>      > seq.int(to = dxi, length.out = ni)
>      > })
>      > args <- c(list(x), sel, drop = FALSE)
>      > do.call("[", args)
>      > }
> 
> 
>      > I think this precludes the need for a separate data.frame method at all,
>      > actually, though (I would think) tail.data.frame would still be defined and
>      > exported for backwards compatibility. (the matrix method has some extra
>      > bits so my current conception of it is still separate, though it might not
>      > NEED to be).
> 
>      > The question then becomes, should head/tail always return something with
>      > the same dimensionally (number of dims) it got, or should data.frame and
>      > matrix be special cased in this regard, as they are now?
> 
>      > What are people's thoughts?
>      > ~G
> 
>      > [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Ddevel&d=DwICAg&c=eRAMFD45gAfqt84VtBcfhQ&r=BK7q3XeAvimeWdGbWY_wJYbW0WYiZvSXAJJKaaPhzWA&m=Xl_11U8w8hVRbuqAPQkz0uSW02kokK9EUPhOopxw0d8&s=vyKU4VkWLb_fGG6KeDPPjVM5_nLhav6UiX7NkzgqsuE&e=
> 

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fredhutch.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319

From @purd|e@@ @end|ng |rom gm@||@com  Thu Oct 31 22:23:46 2019
From: @purd|e@@ @end|ng |rom gm@||@com (Abby Spurdle)
Date: Fri, 1 Nov 2019 10:23:46 +1300
Subject: [Rd] 
 head.matrix can return 1000s of columns -- limit to n or add
 new argument?
In-Reply-To: <2f877775-521a-7fc1-b68b-d233d4a50bea@fredhutch.org>
References: <CAPRVBcxsAkcgr3mDc4skTe8DTxJaYe2xyhk3QMk3odW1MGcyNA@mail.gmail.com>
 <CAB8pepxS2mRtd+522OwjixnM6ZjYxb+X0dc4vMGn6QxdFYBmVQ@mail.gmail.com>
 <CAD4oTHH=LpeEj6h6xYdOGsreS-GDCGdgJV5wpaUR3Pjuh5_M1w@mail.gmail.com>
 <CAPRVBczYZHO3JaFVfNeNNV0-YMRvX3CUYhUvLCjophDEoP+XgQ@mail.gmail.com>
 <23935.14449.965488.684663@stat.math.ethz.ch>
 <CAPRVBcwMY=GAVfFeCwowQsxP31Y=_B2ducosThN+8V91WESB8Q@mail.gmail.com>
 <20449_1568703158_x8H6qIDG009669_f8285123-371f-5d4b-1281-6cc085dba122@fredhutch.org>
 <22204_1568723392_x8HCSG3F015456_674779E0-B1F6-4DC2-9F1C-D5D48B044B66@mcmaster.ca>
 <AD725242-0FB7-4004-9F16-304E345A363A@mcmaster.ca>
 <23937.1791.389379.103733@stat.math.ethz.ch>
 <CAD4oTHF_e4rtHu0S5bUu38+kQSEHXwun3Sv9-CbK+xN9tSc2hA@mail.gmail.com>
 <CAD4oTHFf7Zp+Xe3jvv7oirLWgZm7ZiSNWZvbrEsqJGfrL6Q2wQ@mail.gmail.com>
 <23993.29737.690600.313456@stat.math.ethz.ch>
 <2f877775-521a-7fc1-b68b-d233d4a50bea@fredhutch.org>
Message-ID: <CAB8pepy7+vo=H207iAWP8C0DeR3Fa4ayUbbv4jQZddHMm=d+eA@mail.gmail.com>

On Fri, Nov 1, 2019 at 10:02 AM Pages, Herve <hpages at fredhutch.org> wrote:
> That would be awesome! More generally I wonder how feasible it would be
> to fix all these inheritance quirks where inherits(x, "something"),
> is(x, "something"), and is.something(x) disagree. They've been such a
> nuisance for so many years...

This matter was raised in March:
https://stat.ethz.ch/pipermail/r-devel/2019-March/077457.html

In principle, I agree.
However, I'm not sure it's possible without causing compatibility problems.
Not to mention all the disagreement about what's the correct approach.

And I should probably apologize for incorrectly suggesting that there
was a non-backward-compatible design flaw...


From pd@|gd @end|ng |rom gm@||@com  Thu Oct 31 23:04:29 2019
From: pd@|gd @end|ng |rom gm@||@com (peter dalgaard)
Date: Thu, 31 Oct 2019 23:04:29 +0100
Subject: [Rd] 
 head.matrix can return 1000s of columns -- limit to n or add
 new argument?
In-Reply-To: <23993.29737.690600.313456@stat.math.ethz.ch>
References: <CAPRVBcxsAkcgr3mDc4skTe8DTxJaYe2xyhk3QMk3odW1MGcyNA@mail.gmail.com>
 <CAB8pepxS2mRtd+522OwjixnM6ZjYxb+X0dc4vMGn6QxdFYBmVQ@mail.gmail.com>
 <CAD4oTHH=LpeEj6h6xYdOGsreS-GDCGdgJV5wpaUR3Pjuh5_M1w@mail.gmail.com>
 <CAPRVBczYZHO3JaFVfNeNNV0-YMRvX3CUYhUvLCjophDEoP+XgQ@mail.gmail.com>
 <23935.14449.965488.684663@stat.math.ethz.ch>
 <CAPRVBcwMY=GAVfFeCwowQsxP31Y=_B2ducosThN+8V91WESB8Q@mail.gmail.com>
 <20449_1568703158_x8H6qIDG009669_f8285123-371f-5d4b-1281-6cc085dba122@fredhutch.org>
 <22204_1568723392_x8HCSG3F015456_674779E0-B1F6-4DC2-9F1C-D5D48B044B66@mcmaster.ca>
 <AD725242-0FB7-4004-9F16-304E345A363A@mcmaster.ca>
 <23937.1791.389379.103733@stat.math.ethz.ch>
 <CAD4oTHF_e4rtHu0S5bUu38+kQSEHXwun3Sv9-CbK+xN9tSc2hA@mail.gmail.com>
 <CAD4oTHFf7Zp+Xe3jvv7oirLWgZm7ZiSNWZvbrEsqJGfrL6Q2wQ@mail.gmail.com>
 <23993.29737.690600.313456@stat.math.ethz.ch>
Message-ID: <38105A28-07AE-48F6-A4A4-0C1FCCCBFBDC@gmail.com>

Hmm, the problem I see here is that these implied classes are all inherently one-off. We also have 

> inherits(matrix(1,1,1),"numeric")
[1] FALSE
> is.numeric(matrix(1,1,1))
[1] TRUE
> inherits(1L,"numeric")
[1] FALSE
> is.numeric(1L)
[1] TRUE

and if we start fixing one, we might need to fix all. 

For method dispatch, we do have inheritance, e.g.

> foo.numeric <- function(x) x + 1
> foo <- function(x) UseMethod("foo")
> foo(1)
[1] 2
> foo(1L)
[1] 2
> foo(matrix(1,1,1))
     [,1]
[1,]    2
> foo.integer <- function(x) x + 2
> foo(1)
[1] 2
> foo(1L)
[1] 3
> foo(matrix(1,1,1))
     [,1]
[1,]    2
> foo(matrix(1L,1,1))
     [,1]
[1,]    3

but these are not all automatic: "integer" implies "numeric", but "matrix" does not imply "numeric", much less "integer".

Also, we seem to have a rule that inherits(x, c) iff c %in% class(x), which would break -- unless we change class(x) to return the whole set of inherited classes, which I sense that we'd rather not do....

-pd

> On 30 Oct 2019, at 12:29 , Martin Maechler <maechler at stat.math.ethz.ch> wrote:
> 
> Note however the following  historical quirk :
> 
>> sapply(setNames(,1:5), function(K) inherits(array(pi, dim=1:K), "array"))
>    1     2     3     4     5 
> TRUE FALSE  TRUE  TRUE  TRUE 
> 
> (Is this something we should consider changing for R 4.0.0 -- to
> have it TRUE also for 2d-arrays aka matrix objects ??)

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


