From henr|k@bengt@@on @end|ng |rom gm@||@com  Sat May  1 03:40:48 2021
From: henr|k@bengt@@on @end|ng |rom gm@||@com (Henrik Bengtsson)
Date: Fri, 30 Apr 2021 18:40:48 -0700
Subject: [Rd] R compilation on old(ish) CentOS
In-Reply-To: <1E8063DB-0F65-49CF-8D63-C6786D9BB880@gmail.com>
References: <1ae5a21f-a74e-b532-1359-a55856d4f365@gmail.com>
 <1E8063DB-0F65-49CF-8D63-C6786D9BB880@gmail.com>
Message-ID: <CAFDcVCTRLWpan0emJd_Ri1-iUhoxQVA_8p8+WQ2HZa86bP3Tfw@mail.gmail.com>

Ben, it's most like what Peter says.  I can confirm it works; I just
installed https://cran.r-project.org/src/base-prerelease/R-latest.tar.gz
on an up-to-date CentOS 7.9.2009 system using the vanilla gcc (GCC)
4.8.5 that comes with that version and R compiles just fine and it
passes 'make check' too.

Since R is trying to move toward C++14 support by default, I agree
with I?aki, you might wanr to build and run R with a newer version of
gcc.  gcc 4.8.5 will only give you C++11 support.  RedHat's Software
Collections (SCL) devtoolset:s is the easiest way to do this. I've
done this too and can confirm that gcc 7.3.1 that comes with SCL
devtoolset/7 is sufficient to get C++14 support.  I'm sharing my
installation with lots of users, so I'm make it all transparent to the
end-user with environment modules, i.e. 'module load r/4.1.0' is all
the user needs to know.

/Henrik

On Thu, Apr 29, 2021 at 7:28 AM Peter Dalgaard <pdalgd at gmail.com> wrote:
>
> You may want to check out your checkout....
>
> I see:
>
> Peter-Dalgaards-iMac:R pd$ grep newsock src/main/connections.c
>     con = R_newsock(host, port, server, serverfd, open, timeout, options);
>
> but your file seems to have lost the ", options" bit somehow. Also, mine is line 3488, not 3477.
>
> Maybe you have an old file getting in the way?
>
> - Peter
>
> > On 29 Apr 2021, at 15:58 , Ben Bolker <bbolker at gmail.com> wrote:
> >
> >  I probably don't want to go down this rabbit hole very far, but if anyone has any *quick* ideas ...
> >
> >  Attempting to build R from scratch with a fresh SVN checkout on a somewhat out-of-date CentOS system (for which I don't have root access, although I can bug people if I care enough).
> >
> >  ../r-devel/configure; make
> >
> > ends with
> >
> > gcc -std=gnu99 -I../../../r-devel/trunk/src/extra  -I. -I../../src/include -I../../../r-devel/trunk/src/include -I/usr/local/include -I../../../r-devel/trunk/src/nmath -DHAVE_CONFIG_H  -fopenmp  -g -O2  -c ../../../r-devel/trunk/src/main/connections.c -o connections.o
> > ../../../r-devel/trunk/src/main/connections.c: In function ?do_sockconn?:
> > ../../../r-devel/trunk/src/main/connections.c:3477:5: error: too few arguments to function ?R_newsock?
> >     con = R_newsock(host, port, server, serverfd, open, timeout);
> >     ^
> > In file included from ../../../r-devel/trunk/src/main/connections.c:80:0:
> > ../../../r-devel/trunk/src/include/Rconnections.h:83:13: note: declared here
> > Rconnection R_newsock(const char *host, int port, int server, int serverfd, const char * const mode, int timeout, int options);
> >             ^
> > make[3]: *** [connections.o] Error 1
> >
> >  Any suggestions for a quick fix/diagnosis?
> >
> >  cheers
> >    Ben Bolker
> >
> > ====
> >
> >
> > $ gcc --version
> > gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-39)
> >
> > $ lsb_release -a
> > LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch
> > Distributor ID:       CentOS
> > Description:  CentOS Linux release 7.8.2003 (Core)
> > Release:      7.8.2003
> > Codename:     Core
> >
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From |uc@r @end|ng |rom |edor@project@org  Sat May  1 11:15:35 2021
From: |uc@r @end|ng |rom |edor@project@org (=?UTF-8?Q?I=C3=B1aki_Ucar?=)
Date: Sat, 1 May 2021 11:15:35 +0200
Subject: [Rd] R compilation on old(ish) CentOS
In-Reply-To: <CAFDcVCTRLWpan0emJd_Ri1-iUhoxQVA_8p8+WQ2HZa86bP3Tfw@mail.gmail.com>
References: <1ae5a21f-a74e-b532-1359-a55856d4f365@gmail.com>
 <1E8063DB-0F65-49CF-8D63-C6786D9BB880@gmail.com>
 <CAFDcVCTRLWpan0emJd_Ri1-iUhoxQVA_8p8+WQ2HZa86bP3Tfw@mail.gmail.com>
Message-ID: <CALEXWq3vufK7yrB2mwYYaDD-yB-jkt9MnVGMs-gY4yHzO-ugBw@mail.gmail.com>

On Sat, 1 May 2021 at 03:41, Henrik Bengtsson
<henrik.bengtsson at gmail.com> wrote:
>
> Ben, it's most like what Peter says.  I can confirm it works; I just
> installed https://cran.r-project.org/src/base-prerelease/R-latest.tar.gz
> on an up-to-date CentOS 7.9.2009 system using the vanilla gcc (GCC)
> 4.8.5 that comes with that version and R compiles just fine and it
> passes 'make check' too.

It's not that you can't compile R with gcc 4.8.5, it's that you'll
have a hard time installing many packages. And that's why EPEL 7 has R
3.6 and cannot be updated to 4.

> Since R is trying to move toward C++14 support by default, I agree
> with I?aki, you might wanr to build and run R with a newer version of
> gcc.  gcc 4.8.5 will only give you C++11 support.  RedHat's Software
> Collections (SCL) devtoolset:s is the easiest way to do this. I've
> done this too and can confirm that gcc 7.3.1 that comes with SCL
> devtoolset/7 is sufficient to get C++14 support.  I'm sharing my
> installation with lots of users, so I'm make it all transparent to the
> end-user with environment modules, i.e. 'module load r/4.1.0' is all
> the user needs to know.

--
I?aki ?car


From bbo|ker @end|ng |rom gm@||@com  Sat May  1 13:37:47 2021
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Sat, 1 May 2021 07:37:47 -0400
Subject: [Rd] R compilation on old(ish) CentOS
In-Reply-To: <CAFDcVCTRLWpan0emJd_Ri1-iUhoxQVA_8p8+WQ2HZa86bP3Tfw@mail.gmail.com>
References: <1ae5a21f-a74e-b532-1359-a55856d4f365@gmail.com>
 <1E8063DB-0F65-49CF-8D63-C6786D9BB880@gmail.com>
 <CAFDcVCTRLWpan0emJd_Ri1-iUhoxQVA_8p8+WQ2HZa86bP3Tfw@mail.gmail.com>
Message-ID: <353a1f68-27c0-5b7a-e620-79ccbb54ffae@gmail.com>

   Thanks -- yes, I can confirm that it installs OK after erasing and 
checking out SVN from scratch.

On 4/30/21 9:40 PM, Henrik Bengtsson wrote:
> Ben, it's most like what Peter says.  I can confirm it works; I just
> installed https://cran.r-project.org/src/base-prerelease/R-latest.tar.gz
> on an up-to-date CentOS 7.9.2009 system using the vanilla gcc (GCC)
> 4.8.5 that comes with that version and R compiles just fine and it
> passes 'make check' too.
> 
> Since R is trying to move toward C++14 support by default, I agree
> with I?aki, you might wanr to build and run R with a newer version of
> gcc.  gcc 4.8.5 will only give you C++11 support.  RedHat's Software
> Collections (SCL) devtoolset:s is the easiest way to do this. I've
> done this too and can confirm that gcc 7.3.1 that comes with SCL
> devtoolset/7 is sufficient to get C++14 support.  I'm sharing my
> installation with lots of users, so I'm make it all transparent to the
> end-user with environment modules, i.e. 'module load r/4.1.0' is all
> the user needs to know.
> 
> /Henrik
> 
> On Thu, Apr 29, 2021 at 7:28 AM Peter Dalgaard <pdalgd at gmail.com> wrote:
>>
>> You may want to check out your checkout....
>>
>> I see:
>>
>> Peter-Dalgaards-iMac:R pd$ grep newsock src/main/connections.c
>>      con = R_newsock(host, port, server, serverfd, open, timeout, options);
>>
>> but your file seems to have lost the ", options" bit somehow. Also, mine is line 3488, not 3477.
>>
>> Maybe you have an old file getting in the way?
>>
>> - Peter
>>
>>> On 29 Apr 2021, at 15:58 , Ben Bolker <bbolker at gmail.com> wrote:
>>>
>>>   I probably don't want to go down this rabbit hole very far, but if anyone has any *quick* ideas ...
>>>
>>>   Attempting to build R from scratch with a fresh SVN checkout on a somewhat out-of-date CentOS system (for which I don't have root access, although I can bug people if I care enough).
>>>
>>>   ../r-devel/configure; make
>>>
>>> ends with
>>>
>>> gcc -std=gnu99 -I../../../r-devel/trunk/src/extra  -I. -I../../src/include -I../../../r-devel/trunk/src/include -I/usr/local/include -I../../../r-devel/trunk/src/nmath -DHAVE_CONFIG_H  -fopenmp  -g -O2  -c ../../../r-devel/trunk/src/main/connections.c -o connections.o
>>> ../../../r-devel/trunk/src/main/connections.c: In function ?do_sockconn?:
>>> ../../../r-devel/trunk/src/main/connections.c:3477:5: error: too few arguments to function ?R_newsock?
>>>      con = R_newsock(host, port, server, serverfd, open, timeout);
>>>      ^
>>> In file included from ../../../r-devel/trunk/src/main/connections.c:80:0:
>>> ../../../r-devel/trunk/src/include/Rconnections.h:83:13: note: declared here
>>> Rconnection R_newsock(const char *host, int port, int server, int serverfd, const char * const mode, int timeout, int options);
>>>              ^
>>> make[3]: *** [connections.o] Error 1
>>>
>>>   Any suggestions for a quick fix/diagnosis?
>>>
>>>   cheers
>>>     Ben Bolker
>>>
>>> ====
>>>
>>>
>>> $ gcc --version
>>> gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-39)
>>>
>>> $ lsb_release -a
>>> LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch
>>> Distributor ID:       CentOS
>>> Description:  CentOS Linux release 7.8.2003 (Core)
>>> Release:      7.8.2003
>>> Codename:     Core
>>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>> --
>> Peter Dalgaard, Professor,
>> Center for Statistics, Copenhagen Business School
>> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
>> Phone: (+45)38153501
>> Office: A 4.23
>> Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel


From gz@p@t@w@|nberg @end|ng |rom gm@||@com  Mon May  3 20:48:49 2021
From: gz@p@t@w@|nberg @end|ng |rom gm@||@com (Gustavo Zapata Wainberg)
Date: Mon, 3 May 2021 20:48:49 +0200
Subject: [Rd] Inconsistency in median()
Message-ID: <CAFQ8Lqn=bn6pebC7gv13vHMx2PrApM7_Zo-HrxV+C7oVxjM0EA@mail.gmail.com>

Hi!

I'm wrinting this post because there is an inconsistency when median() is
calculated for even or odd vectors. For odd vectors, attributes (such as
labels added with Hmisc) are kept after running median(), but this is not
the case if the vector is even, in this last case attributes are lost.

I know that this is due to median() using mean() to obtain the result when
the vector is even, and mean() always takes attributes off vectors.

Don't you think that attributes should be kept in both cases? And, going
further, shouldn't mean() keep attributes as well? I have looked in R's
Bugzilla and I didn't find an entry related to this issue.

Please, let me know if you consider that this issue should be posted in R's
bugzilla.

Here is an example with code.

rndvar <- rnorm(n = 100)

Hmisc::label(rndvar) <- "A label for RNDVAR"

str(median(rndvar[-c(1,2)]))

Returns: "num 0.0368"

str(median(rndvar[-1]))

Returns:
 'labelled' num 0.0322
 - attr(*, "label")= chr "A label for RNDVAR"

Thanks in advance!

Gustavo Zapata-Wainberg

	[[alternative HTML version deleted]]


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Tue May  4 17:57:05 2021
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Tue, 4 May 2021 17:57:05 +0200
Subject: [Rd] Inconsistency in median()
In-Reply-To: <CAFQ8Lqn=bn6pebC7gv13vHMx2PrApM7_Zo-HrxV+C7oVxjM0EA@mail.gmail.com>
References: <CAFQ8Lqn=bn6pebC7gv13vHMx2PrApM7_Zo-HrxV+C7oVxjM0EA@mail.gmail.com>
Message-ID: <24721.28369.583490.768453@stat.math.ethz.ch>

>>>>> Gustavo Zapata Wainberg 
>>>>>     on Mon, 3 May 2021 20:48:49 +0200 writes:

    > Hi!

    > I'm wrinting this post because there is an inconsistency
    > when median() is calculated for even or odd vectors. For
    > odd vectors, attributes (such as labels added with Hmisc)
    > are kept after running median(), but this is not the case
    > if the vector is even, in this last case attributes are
    > lost.

    > I know that this is due to median() using mean() to obtain
    > the result when the vector is even, and mean() always
    > takes attributes off vectors.

Yes, and this has been the design of  median()  for ever :

If n := length(x)  is odd,  the median is "the middle" observation,
                   and should  equal to x[j] for j = (n+1)/2
		   and hence e.g., is well defined for an ordered factor.

When  n  is even
     however, median() must be the mean of "the two middle" observations,
       which is e.g., not even *defined* for an ordered factor.

We *could* talk of the so called lo-median  or hi-median
(terms probably coined by John W. Tukey) because (IIRC), these
are equal to each other and to the median for odd n, but
are   equal to  x[j]  and  x[j+1]   j=n/2  for even n *and* are
still "of the same kind" as x[]  itself.

Interestingly, for the mad() { = the median absolute deviation from the median}
we *do* allow to specify logical 'low' and 'high',
but that for the "outer" median in MAD's definition, not the
inner one.

## From <Rsrc>/src/library/stats/R/mad.R :

mad <- function(x, center = median(x), constant = 1.4826,
                na.rm = FALSE, low = FALSE, high = FALSE)
{
    if(na.rm)
	x <- x[!is.na(x)]
    n <- length(x)
    constant *
        if((low || high) && n%%2 == 0) {
            if(low && high) stop("'low' and 'high' cannot be both TRUE")
            n2 <- n %/% 2 + as.integer(high)
            sort(abs(x - center), partial = n2)[n2]
        }
        else median(abs(x - center))
}




    > Don't you think that attributes should be kept in both
    > cases? 

well, not all attributes can be kept.
Note that for *named* vectors x,  x[j] can (and does) keep the name,
but there's definitely no sensible name to give to (x[j] + x[j+1])/2

I'm willing to collaborate with some, considering
to extend  median.default()  making  hi-median and lo-median
available to the user.
Both of these will always return x[j] for some j and hence keep
all (sensible!) attributes (well, if the `[`-method for the
corresponding class has been defined correctly; I've encountered
quite a few cases where people created vector-like classes but
did not provide a "correct"  subsetting method (typically you
should make sure both a `[[` and `[` method works!).

Best regards,
Martin

Martin Maechler
ETH Zurich  and  R Core team

    > And, going further, shouldn't mean() keep
    > attributes as well? I have looked in R's Bugzilla and I
    > didn't find an entry related to this issue.

    > Please, let me know if you consider that this issue should
    > be posted in R's bugzilla.

    > Here is an example with code.

    > rndvar <- rnorm(n = 100)

    > Hmisc::label(rndvar) <- "A label for RNDVAR"

    > str(median(rndvar[-c(1,2)]))

    > Returns: "num 0.0368"

    > str(median(rndvar[-1]))

    > Returns: 'labelled' num 0.0322 - attr(*, "label")= chr "A
    > label for RNDVAR"

    > Thanks in advance!

    > Gustavo Zapata-Wainberg

    > 	[[alternative HTML version deleted]]

    > ______________________________________________
    > R-devel at r-project.org mailing list
    > https://stat.ethz.ch/mailman/listinfo/r-devel


From henr|k@bengt@@on @end|ng |rom gm@||@com  Tue May  4 18:31:25 2021
From: henr|k@bengt@@on @end|ng |rom gm@||@com (Henrik Bengtsson)
Date: Tue, 4 May 2021 09:31:25 -0700
Subject: [Rd] Testing R build when using --without-recommended-packages?
Message-ID: <CAFDcVCS8_8c0g8CXAmHd96XH49YYUX7jt8J9Bj3guWkX8X=Knw@mail.gmail.com>

I'm on Linux (Ubuntu 18.04). How do I check an R build when using
--without-recommended-packages? 'make check' assumes 'recommended'
packages are installed, so that fails without them available.

DETAILS:

When I build R from source without 'recommended' packages:

curl -O https://cran.r-project.org/src/base-prerelease/R-latest.tar.gz
tar xvfz R-latest.tar.gz
cd R-beta
./configure --enable-memory-profiling --enable-R-shlib --prefix="$PREFIX"
make

I cannot figure out how to validate the build.  Following Section
'Installation' of 'R Installation and Administration', I run:

make check

results in:

Testing examples for package ?stats?
Error: testing 'stats' failed
Execution halted

This is because those tests assume 'MASS' is installed;

$ cat /path/to/tests/Examples/stats-Ex.Rout.fail

> utils::data(muscle, package = "MASS")
Error in find.package(package, lib.loc, verbose = verbose) :
  there is no package called ?MASS?
Calls: <Anonymous> -> find.package
Execution halted

BTW, isn't this a bug? Shouldn't this example run conditionally on
'MASS' being installed, because 'MASS' is a suggested package here;

Package: stats
Version: 4.1.0
...
Imports: utils, grDevices, graphics
Suggests: MASS, Matrix, SuppDists, methods, stats4

/Henrik


From edd @end|ng |rom deb|@n@org  Tue May  4 19:16:26 2021
From: edd @end|ng |rom deb|@n@org (Dirk Eddelbuettel)
Date: Tue, 4 May 2021 12:16:26 -0500
Subject: [Rd] Testing R build when using --without-recommended-packages?
In-Reply-To: <CAFDcVCS8_8c0g8CXAmHd96XH49YYUX7jt8J9Bj3guWkX8X=Knw@mail.gmail.com>
References: <CAFDcVCS8_8c0g8CXAmHd96XH49YYUX7jt8J9Bj3guWkX8X=Knw@mail.gmail.com>
Message-ID: <24721.33130.749929.947885@rob.eddelbuettel.com>


On 4 May 2021 at 09:31, Henrik Bengtsson wrote:
| I'm on Linux (Ubuntu 18.04). How do I check an R build when using
| --without-recommended-packages? 'make check' assumes 'recommended'
| packages are installed, so that fails without them available.

[...]

| BTW, isn't this a bug? Shouldn't this example run conditionally on
| 'MASS' being installed, because 'MASS' is a suggested package here;

The 'R-admin' manual in Section 1.2 "Getting patched and development
versions" ends on 

  If downloading manually from CRAN, do ensure that you have the correct
  versions of the recommended packages: if the number in the file VERSION
  is ?x.y.z? you need to download the contents of
  ?https://CRAN.R-project.org/src/contrib/dir?, where dir is
  ?x.y.z/Recommended? for r-devel or x.y-patched/Recommended for r-patched,
  respectively, to directory src/library/Recommended in the sources you
  have unpacked. After downloading manually you need to execute
  tools/link-recommended from the top level of the sources to make the
  requisite links in src/library/Recommended. A suitable incantation from
  the top level of the R sources using wget might be (for the correct
  value of dir)

      wget -r -l1 --no-parent -A\*.gz -nd -P src/library/Recommended \
        https://CRAN.R-project.org/src/contrib/dir
      ./tools/link-recommended

Dirk

-- 
https://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From henr|k@bengt@@on @end|ng |rom gm@||@com  Tue May  4 20:07:26 2021
From: henr|k@bengt@@on @end|ng |rom gm@||@com (Henrik Bengtsson)
Date: Tue, 4 May 2021 11:07:26 -0700
Subject: [Rd] Testing R build when using --without-recommended-packages?
In-Reply-To: <24721.33130.749929.947885@rob.eddelbuettel.com>
References: <CAFDcVCS8_8c0g8CXAmHd96XH49YYUX7jt8J9Bj3guWkX8X=Knw@mail.gmail.com>
 <24721.33130.749929.947885@rob.eddelbuettel.com>
Message-ID: <CAFDcVCTQ4mNK35NGaZEsiTDynuoxVk1VUCRGpgaGjMh3v++ESA@mail.gmail.com>

Thanks, but I don't understand. That's what I usually do when I build
R with 'recommended' packages.  But here, I explicitly do *not* want
to build and install 'recommended' packages with the R installation.
So, I'm going down the --without-recommended-packages path on purpose
and I'm looking for a way to validate such an installation.

If your comment is on the 'stats' examples' hard dependency on 'MASS'
despite it's being a suggested packages, I still don't follow.

/Henrik

On Tue, May 4, 2021 at 10:16 AM Dirk Eddelbuettel <edd at debian.org> wrote:
>
>
> On 4 May 2021 at 09:31, Henrik Bengtsson wrote:
> | I'm on Linux (Ubuntu 18.04). How do I check an R build when using
> | --without-recommended-packages? 'make check' assumes 'recommended'
> | packages are installed, so that fails without them available.
>
> [...]
>
> | BTW, isn't this a bug? Shouldn't this example run conditionally on
> | 'MASS' being installed, because 'MASS' is a suggested package here;
>
> The 'R-admin' manual in Section 1.2 "Getting patched and development
> versions" ends on
>
>   If downloading manually from CRAN, do ensure that you have the correct
>   versions of the recommended packages: if the number in the file VERSION
>   is ?x.y.z? you need to download the contents of
>   ?https://CRAN.R-project.org/src/contrib/dir?, where dir is
>   ?x.y.z/Recommended? for r-devel or x.y-patched/Recommended for r-patched,
>   respectively, to directory src/library/Recommended in the sources you
>   have unpacked. After downloading manually you need to execute
>   tools/link-recommended from the top level of the sources to make the
>   requisite links in src/library/Recommended. A suitable incantation from
>   the top level of the R sources using wget might be (for the correct
>   value of dir)
>
>       wget -r -l1 --no-parent -A\*.gz -nd -P src/library/Recommended \
>         https://CRAN.R-project.org/src/contrib/dir
>       ./tools/link-recommended
>
> Dirk
>
> --
> https://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From edd @end|ng |rom deb|@n@org  Tue May  4 20:17:12 2021
From: edd @end|ng |rom deb|@n@org (Dirk Eddelbuettel)
Date: Tue, 4 May 2021 13:17:12 -0500
Subject: [Rd] Testing R build when using --without-recommended-packages?
In-Reply-To: <CAFDcVCTQ4mNK35NGaZEsiTDynuoxVk1VUCRGpgaGjMh3v++ESA@mail.gmail.com>
References: <CAFDcVCS8_8c0g8CXAmHd96XH49YYUX7jt8J9Bj3guWkX8X=Knw@mail.gmail.com>
 <24721.33130.749929.947885@rob.eddelbuettel.com>
 <CAFDcVCTQ4mNK35NGaZEsiTDynuoxVk1VUCRGpgaGjMh3v++ESA@mail.gmail.com>
Message-ID: <24721.36776.942664.909986@rob.eddelbuettel.com>


On 4 May 2021 at 11:07, Henrik Bengtsson wrote:
| Thanks, but I don't understand. That's what I usually do when I build
| R with 'recommended' packages.  But here, I explicitly do *not* want
| to build and install 'recommended' packages with the R installation.
| So, I'm going down the --without-recommended-packages path on purpose
| and I'm looking for a way to validate such an installation.

I understand the desire, and am sympathetic, but for all+ years I have been
building R (or R-devel) from source this has never been optional. Nor has any
optionality (for the build of R has a whole) been documented, at least as far
as I know.

Dirk

-- 
https://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From henr|k@bengt@@on @end|ng |rom gm@||@com  Tue May  4 20:25:01 2021
From: henr|k@bengt@@on @end|ng |rom gm@||@com (Henrik Bengtsson)
Date: Tue, 4 May 2021 11:25:01 -0700
Subject: [Rd] Testing R build when using --without-recommended-packages?
In-Reply-To: <24721.36776.942664.909986@rob.eddelbuettel.com>
References: <CAFDcVCS8_8c0g8CXAmHd96XH49YYUX7jt8J9Bj3guWkX8X=Knw@mail.gmail.com>
 <24721.33130.749929.947885@rob.eddelbuettel.com>
 <CAFDcVCTQ4mNK35NGaZEsiTDynuoxVk1VUCRGpgaGjMh3v++ESA@mail.gmail.com>
 <24721.36776.942664.909986@rob.eddelbuettel.com>
Message-ID: <CAFDcVCTkwZtY5jVHuorT15Yc35aGMJLsyeo3p3AmeUG3oUmsHA@mail.gmail.com>

FWIW,

$ ./configure --help
...
  --with-recommended-packages
                          use/install recommended R packages [yes]

/Henrik

On Tue, May 4, 2021 at 11:17 AM Dirk Eddelbuettel <edd at debian.org> wrote:
>
>
> On 4 May 2021 at 11:07, Henrik Bengtsson wrote:
> | Thanks, but I don't understand. That's what I usually do when I build
> | R with 'recommended' packages.  But here, I explicitly do *not* want
> | to build and install 'recommended' packages with the R installation.
> | So, I'm going down the --without-recommended-packages path on purpose
> | and I'm looking for a way to validate such an installation.
>
> I understand the desire, and am sympathetic, but for all+ years I have been
> building R (or R-devel) from source this has never been optional. Nor has any
> optionality (for the build of R has a whole) been documented, at least as far
> as I know.
>
> Dirk
>
> --
> https://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From edd @end|ng |rom deb|@n@org  Tue May  4 21:22:09 2021
From: edd @end|ng |rom deb|@n@org (Dirk Eddelbuettel)
Date: Tue, 4 May 2021 14:22:09 -0500
Subject: [Rd] Testing R build when using --without-recommended-packages?
In-Reply-To: <CAFDcVCTkwZtY5jVHuorT15Yc35aGMJLsyeo3p3AmeUG3oUmsHA@mail.gmail.com>
References: <CAFDcVCS8_8c0g8CXAmHd96XH49YYUX7jt8J9Bj3guWkX8X=Knw@mail.gmail.com>
 <24721.33130.749929.947885@rob.eddelbuettel.com>
 <CAFDcVCTQ4mNK35NGaZEsiTDynuoxVk1VUCRGpgaGjMh3v++ESA@mail.gmail.com>
 <24721.36776.942664.909986@rob.eddelbuettel.com>
 <CAFDcVCTkwZtY5jVHuorT15Yc35aGMJLsyeo3p3AmeUG3oUmsHA@mail.gmail.com>
Message-ID: <24721.40673.20828.844418@rob.eddelbuettel.com>


On 4 May 2021 at 11:25, Henrik Bengtsson wrote:
| FWIW,
| 
| $ ./configure --help
| ...
|   --with-recommended-packages
|                           use/install recommended R packages [yes]

Of course. But look at the verb in your Subject: no optionality _in testing_ there.

You obviously need to be able to build R itself to then build the recommended
packages you need for testing.

Dirk

-- 
https://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From henr|k@bengt@@on @end|ng |rom gm@||@com  Tue May  4 22:03:39 2021
From: henr|k@bengt@@on @end|ng |rom gm@||@com (Henrik Bengtsson)
Date: Tue, 4 May 2021 13:03:39 -0700
Subject: [Rd] Testing R build when using --without-recommended-packages?
In-Reply-To: <24721.40673.20828.844418@rob.eddelbuettel.com>
References: <CAFDcVCS8_8c0g8CXAmHd96XH49YYUX7jt8J9Bj3guWkX8X=Knw@mail.gmail.com>
 <24721.33130.749929.947885@rob.eddelbuettel.com>
 <CAFDcVCTQ4mNK35NGaZEsiTDynuoxVk1VUCRGpgaGjMh3v++ESA@mail.gmail.com>
 <24721.36776.942664.909986@rob.eddelbuettel.com>
 <CAFDcVCTkwZtY5jVHuorT15Yc35aGMJLsyeo3p3AmeUG3oUmsHA@mail.gmail.com>
 <24721.40673.20828.844418@rob.eddelbuettel.com>
Message-ID: <CAFDcVCS2SgCkVPULH4ovMPX8cgegNuQkY1cA-fW6jpPRHDNeBw@mail.gmail.com>

Two questions to R Core:

1. Is R designed so that 'recommended' packages are optional, or
should that be considered uncharted territories?

2. Can such an R build/installation be validated using existing check methods?


--

Dirk, it's not clear to me whether you know for sure, or you draw
conclusions based your long experience and reading. I think it's very
important that others don't find this thread later on and read your
comments as if they're the "truth" (unless they are).  I haven't
re-read it from start to finish, but there are passages in 'R
Installation and Administration' suggesting you can build and install
R without 'recommended' packages.  For example, post-installation,
Section 'Testing an Installation' suggests you can run (after making
sure `make install-tests`):

cd tests
../bin/R CMD make check

but they fail the same way.  The passage continuous "... and other
useful targets are test-BasePackages and test-Recommended to run tests
of the standard and recommended packages (if installed) respectively."
(*).  So, to me that hints at 'recommended' packages are optional just
as they're "Priority: recommended".  Further down, there's also a
mentioning of:

$ R_LIBS_USER="" R --vanilla
> Sys.setenv(LC_COLLATE = "C", LC_TIME = "C", LANGUAGE = "en")
> tools::testInstalledPackages(scope = "base")

which also produces errors when 'recommended' packages are missing,
e.g. "Failed with error:  'there is no package called 'nlme'".

(*) BTW, '../bin/R CMD make test-BasePackages' gives "make: *** No
rule to make target 'test-BasePackages'.  Stop."

Thanks,

/Henrik

On Tue, May 4, 2021 at 12:22 PM Dirk Eddelbuettel <edd at debian.org> wrote:
>
>
> On 4 May 2021 at 11:25, Henrik Bengtsson wrote:
> | FWIW,
> |
> | $ ./configure --help
> | ...
> |   --with-recommended-packages
> |                           use/install recommended R packages [yes]
>
> Of course. But look at the verb in your Subject: no optionality _in testing_ there.
>
> You obviously need to be able to build R itself to then build the recommended
> packages you need for testing.
>
> Dirk
>
> --
> https://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From g@bembecker @end|ng |rom gm@||@com  Tue May  4 23:10:12 2021
From: g@bembecker @end|ng |rom gm@||@com (Gabriel Becker)
Date: Tue, 4 May 2021 14:10:12 -0700
Subject: [Rd] Testing R build when using --without-recommended-packages?
In-Reply-To: <CAFDcVCS2SgCkVPULH4ovMPX8cgegNuQkY1cA-fW6jpPRHDNeBw@mail.gmail.com>
References: <CAFDcVCS8_8c0g8CXAmHd96XH49YYUX7jt8J9Bj3guWkX8X=Knw@mail.gmail.com>
 <24721.33130.749929.947885@rob.eddelbuettel.com>
 <CAFDcVCTQ4mNK35NGaZEsiTDynuoxVk1VUCRGpgaGjMh3v++ESA@mail.gmail.com>
 <24721.36776.942664.909986@rob.eddelbuettel.com>
 <CAFDcVCTkwZtY5jVHuorT15Yc35aGMJLsyeo3p3AmeUG3oUmsHA@mail.gmail.com>
 <24721.40673.20828.844418@rob.eddelbuettel.com>
 <CAFDcVCS2SgCkVPULH4ovMPX8cgegNuQkY1cA-fW6jpPRHDNeBw@mail.gmail.com>
Message-ID: <CAD4oTHG3U=tDeX2fEsnf0QPksTZYuJ7GDey45nNBE3DnCDb86w@mail.gmail.com>

Hi Henrik,

A couple of things. Firstly, so far asI have ever heard, it's valid that a
package have hard dependencies in its tests for packages listed only in
Suggests.  In fact, that is one of the stated purposes of Suggests. An
argument could be made, I suppose, that the base packages should be under
stricter guidelines, but stats isn't violating the letter or intention of
Suggests by doing this.


Secondly, I don't have time to dig through the make files/administration
docs, but I do know that R CMD check has --no-stop-on-error, so you can
either separately or as part of make check, use that option for stats (and
elsewhere as needed?) and just know that the stats tests that depend on
MASS are "false positive" (or, more accurately, missing value) test
results, rather than real positives, and go from there.

You could also "patch" the tests as part of your build process. Somewhere I
worked had to do that for parts of the internet tests that were unable to
get through the firewall.

Best,
~G



On Tue, May 4, 2021 at 1:04 PM Henrik Bengtsson <henrik.bengtsson at gmail.com>
wrote:

> Two questions to R Core:
>
> 1. Is R designed so that 'recommended' packages are optional, or
> should that be considered uncharted territories?
>
> 2. Can such an R build/installation be validated using existing check
> methods?
>
>
> --
>
> Dirk, it's not clear to me whether you know for sure, or you draw
> conclusions based your long experience and reading. I think it's very
> important that others don't find this thread later on and read your
> comments as if they're the "truth" (unless they are).  I haven't
> re-read it from start to finish, but there are passages in 'R
> Installation and Administration' suggesting you can build and install
> R without 'recommended' packages.  For example, post-installation,
> Section 'Testing an Installation' suggests you can run (after making
> sure `make install-tests`):
>
> cd tests
> ../bin/R CMD make check
>
> but they fail the same way.  The passage continuous "... and other
> useful targets are test-BasePackages and test-Recommended to run tests
> of the standard and recommended packages (if installed) respectively."
> (*).  So, to me that hints at 'recommended' packages are optional just
> as they're "Priority: recommended".  Further down, there's also a
> mentioning of:
>
> $ R_LIBS_USER="" R --vanilla
> > Sys.setenv(LC_COLLATE = "C", LC_TIME = "C", LANGUAGE = "en")
> > tools::testInstalledPackages(scope = "base")
>
> which also produces errors when 'recommended' packages are missing,
> e.g. "Failed with error:  'there is no package called 'nlme'".
>
> (*) BTW, '../bin/R CMD make test-BasePackages' gives "make: *** No
> rule to make target 'test-BasePackages'.  Stop."
>
> Thanks,
>
> /Henrik
>
> On Tue, May 4, 2021 at 12:22 PM Dirk Eddelbuettel <edd at debian.org> wrote:
> >
> >
> > On 4 May 2021 at 11:25, Henrik Bengtsson wrote:
> > | FWIW,
> > |
> > | $ ./configure --help
> > | ...
> > |   --with-recommended-packages
> > |                           use/install recommended R packages [yes]
> >
> > Of course. But look at the verb in your Subject: no optionality _in
> testing_ there.
> >
> > You obviously need to be able to build R itself to then build the
> recommended
> > packages you need for testing.
> >
> > Dirk
> >
> > --
> > https://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Tue May  4 23:22:36 2021
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Tue, 4 May 2021 17:22:36 -0400
Subject: [Rd] Testing R build when using --without-recommended-packages?
In-Reply-To: <CAD4oTHG3U=tDeX2fEsnf0QPksTZYuJ7GDey45nNBE3DnCDb86w@mail.gmail.com>
References: <CAFDcVCS8_8c0g8CXAmHd96XH49YYUX7jt8J9Bj3guWkX8X=Knw@mail.gmail.com>
 <24721.33130.749929.947885@rob.eddelbuettel.com>
 <CAFDcVCTQ4mNK35NGaZEsiTDynuoxVk1VUCRGpgaGjMh3v++ESA@mail.gmail.com>
 <24721.36776.942664.909986@rob.eddelbuettel.com>
 <CAFDcVCTkwZtY5jVHuorT15Yc35aGMJLsyeo3p3AmeUG3oUmsHA@mail.gmail.com>
 <24721.40673.20828.844418@rob.eddelbuettel.com>
 <CAFDcVCS2SgCkVPULH4ovMPX8cgegNuQkY1cA-fW6jpPRHDNeBw@mail.gmail.com>
 <CAD4oTHG3U=tDeX2fEsnf0QPksTZYuJ7GDey45nNBE3DnCDb86w@mail.gmail.com>
Message-ID: <280c17af-1eda-84a8-42a0-e6f4990f9f3e@gmail.com>


   Sorry if this has been pointed out already, but some relevant text 
from 
https://cran.r-project.org/doc/manuals/r-release/R-exts.html#Suggested-packages

 > Note that someone wanting to run the examples/tests/vignettes may not 
have a suggested package available (and it may not even be possible to 
install it for that platform). The recommendation used to be to make 
their use conditional via if(require("pkgname")): this is OK if that 
conditioning is done in examples/tests/vignettes, although using 
if(requireNamespace("pkgname")) is preferred, if possible.

...

 > Some people have assumed that a ?recommended? package in ?Suggests? 
can safely be used unconditionally, but this is not so. (R can be 
installed without recommended packages, and which packages are 
?recommended? may change.)



On 5/4/21 5:10 PM, Gabriel Becker wrote:
> Hi Henrik,
> 
> A couple of things. Firstly, so far asI have ever heard, it's valid that a
> package have hard dependencies in its tests for packages listed only in
> Suggests.  In fact, that is one of the stated purposes of Suggests. An
> argument could be made, I suppose, that the base packages should be under
> stricter guidelines, but stats isn't violating the letter or intention of
> Suggests by doing this.
> 
> 
> Secondly, I don't have time to dig through the make files/administration
> docs, but I do know that R CMD check has --no-stop-on-error, so you can
> either separately or as part of make check, use that option for stats (and
> elsewhere as needed?) and just know that the stats tests that depend on
> MASS are "false positive" (or, more accurately, missing value) test
> results, rather than real positives, and go from there.
> 
> You could also "patch" the tests as part of your build process. Somewhere I
> worked had to do that for parts of the internet tests that were unable to
> get through the firewall.
> 
> Best,
> ~G
> 
> 
> 
> On Tue, May 4, 2021 at 1:04 PM Henrik Bengtsson <henrik.bengtsson at gmail.com>
> wrote:
> 
>> Two questions to R Core:
>>
>> 1. Is R designed so that 'recommended' packages are optional, or
>> should that be considered uncharted territories?
>>
>> 2. Can such an R build/installation be validated using existing check
>> methods?
>>
>>
>> --
>>
>> Dirk, it's not clear to me whether you know for sure, or you draw
>> conclusions based your long experience and reading. I think it's very
>> important that others don't find this thread later on and read your
>> comments as if they're the "truth" (unless they are).  I haven't
>> re-read it from start to finish, but there are passages in 'R
>> Installation and Administration' suggesting you can build and install
>> R without 'recommended' packages.  For example, post-installation,
>> Section 'Testing an Installation' suggests you can run (after making
>> sure `make install-tests`):
>>
>> cd tests
>> ../bin/R CMD make check
>>
>> but they fail the same way.  The passage continuous "... and other
>> useful targets are test-BasePackages and test-Recommended to run tests
>> of the standard and recommended packages (if installed) respectively."
>> (*).  So, to me that hints at 'recommended' packages are optional just
>> as they're "Priority: recommended".  Further down, there's also a
>> mentioning of:
>>
>> $ R_LIBS_USER="" R --vanilla
>>> Sys.setenv(LC_COLLATE = "C", LC_TIME = "C", LANGUAGE = "en")
>>> tools::testInstalledPackages(scope = "base")
>>
>> which also produces errors when 'recommended' packages are missing,
>> e.g. "Failed with error:  'there is no package called 'nlme'".
>>
>> (*) BTW, '../bin/R CMD make test-BasePackages' gives "make: *** No
>> rule to make target 'test-BasePackages'.  Stop."
>>
>> Thanks,
>>
>> /Henrik
>>
>> On Tue, May 4, 2021 at 12:22 PM Dirk Eddelbuettel <edd at debian.org> wrote:
>>>
>>>
>>> On 4 May 2021 at 11:25, Henrik Bengtsson wrote:
>>> | FWIW,
>>> |
>>> | $ ./configure --help
>>> | ...
>>> |   --with-recommended-packages
>>> |                           use/install recommended R packages [yes]
>>>
>>> Of course. But look at the verb in your Subject: no optionality _in
>> testing_ there.
>>>
>>> You obviously need to be able to build R itself to then build the
>> recommended
>>> packages you need for testing.
>>>
>>> Dirk
>>>
>>> --
>>> https://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From edd @end|ng |rom deb|@n@org  Tue May  4 23:38:19 2021
From: edd @end|ng |rom deb|@n@org (Dirk Eddelbuettel)
Date: Tue, 4 May 2021 16:38:19 -0500
Subject: [Rd] Testing R build when using --without-recommended-packages?
In-Reply-To: <CAD4oTHG3U=tDeX2fEsnf0QPksTZYuJ7GDey45nNBE3DnCDb86w@mail.gmail.com>
References: <CAFDcVCS8_8c0g8CXAmHd96XH49YYUX7jt8J9Bj3guWkX8X=Knw@mail.gmail.com>
 <24721.33130.749929.947885@rob.eddelbuettel.com>
 <CAFDcVCTQ4mNK35NGaZEsiTDynuoxVk1VUCRGpgaGjMh3v++ESA@mail.gmail.com>
 <24721.36776.942664.909986@rob.eddelbuettel.com>
 <CAFDcVCTkwZtY5jVHuorT15Yc35aGMJLsyeo3p3AmeUG3oUmsHA@mail.gmail.com>
 <24721.40673.20828.844418@rob.eddelbuettel.com>
 <CAFDcVCS2SgCkVPULH4ovMPX8cgegNuQkY1cA-fW6jpPRHDNeBw@mail.gmail.com>
 <CAD4oTHG3U=tDeX2fEsnf0QPksTZYuJ7GDey45nNBE3DnCDb86w@mail.gmail.com>
Message-ID: <24721.48843.56458.417087@rob.eddelbuettel.com>


On 4 May 2021 at 14:10, Gabriel Becker wrote:
| A couple of things. Firstly, so far asI have ever heard, it's valid that a
| package have hard dependencies in its tests for packages listed only in
| Suggests.  In fact, that is one of the stated purposes of Suggests. An
| argument could be made, I suppose, that the base packages should be under
| stricter guidelines, but stats isn't violating the letter or intention of
| Suggests by doing this.

Like Ben, I also take the other side here and point you e.g. to the extended
discussion between Duncan and myself on r-package-devel (look for the thread
with Subject: "winUCRT failures").

Optional packages need testing for presence before they are used, and we are
slowly moving in that direction. Repeating "but that's not what we used to
do" is of limited interest as things do sometimes change for the better.

Dirk

-- 
https://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From g@bembecker @end|ng |rom gm@||@com  Tue May  4 23:40:22 2021
From: g@bembecker @end|ng |rom gm@||@com (Gabriel Becker)
Date: Tue, 4 May 2021 14:40:22 -0700
Subject: [Rd] Testing R build when using --without-recommended-packages?
In-Reply-To: <280c17af-1eda-84a8-42a0-e6f4990f9f3e@gmail.com>
References: <CAFDcVCS8_8c0g8CXAmHd96XH49YYUX7jt8J9Bj3guWkX8X=Knw@mail.gmail.com>
 <24721.33130.749929.947885@rob.eddelbuettel.com>
 <CAFDcVCTQ4mNK35NGaZEsiTDynuoxVk1VUCRGpgaGjMh3v++ESA@mail.gmail.com>
 <24721.36776.942664.909986@rob.eddelbuettel.com>
 <CAFDcVCTkwZtY5jVHuorT15Yc35aGMJLsyeo3p3AmeUG3oUmsHA@mail.gmail.com>
 <24721.40673.20828.844418@rob.eddelbuettel.com>
 <CAFDcVCS2SgCkVPULH4ovMPX8cgegNuQkY1cA-fW6jpPRHDNeBw@mail.gmail.com>
 <CAD4oTHG3U=tDeX2fEsnf0QPksTZYuJ7GDey45nNBE3DnCDb86w@mail.gmail.com>
 <280c17af-1eda-84a8-42a0-e6f4990f9f3e@gmail.com>
Message-ID: <CAD4oTHEVst9d3JXMnHkkziDZi_H83Ojv629M73UnHrZMCa1rNQ@mail.gmail.com>

Hmm, that's fair enough Ben, I stand corrected.  I will say that this seems
to be a pretty "soft" recommendation, as these things go, given that it
isn't tested for by R CMD check, including with the -as-cran extensions. In
principle, it seems like it could be, similar checks are made in package
code for inappropriate external-package-symbol usage/

Either way, though, I suppose I have a number of packages which have been
invisibly non-best-practices compliant for their entire lifetimes (or at
least, the portion of that where they had tests/vignettes...).

Best,
~G

On Tue, May 4, 2021 at 2:22 PM Ben Bolker <bbolker at gmail.com> wrote:

>
>    Sorry if this has been pointed out already, but some relevant text
> from
>
> https://cran.r-project.org/doc/manuals/r-release/R-exts.html#Suggested-packages
>
>  > Note that someone wanting to run the examples/tests/vignettes may not
> have a suggested package available (and it may not even be possible to
> install it for that platform). The recommendation used to be to make
> their use conditional via if(require("pkgname")): this is OK if that
> conditioning is done in examples/tests/vignettes, although using
> if(requireNamespace("pkgname")) is preferred, if possible.
>
> ...
>
>  > Some people have assumed that a ?recommended? package in ?Suggests?
> can safely be used unconditionally, but this is not so. (R can be
> installed without recommended packages, and which packages are
> ?recommended? may change.)
>
>
>
> On 5/4/21 5:10 PM, Gabriel Becker wrote:
> > Hi Henrik,
> >
> > A couple of things. Firstly, so far asI have ever heard, it's valid that
> a
> > package have hard dependencies in its tests for packages listed only in
> > Suggests.  In fact, that is one of the stated purposes of Suggests. An
> > argument could be made, I suppose, that the base packages should be under
> > stricter guidelines, but stats isn't violating the letter or intention of
> > Suggests by doing this.
> >
> >
> > Secondly, I don't have time to dig through the make files/administration
> > docs, but I do know that R CMD check has --no-stop-on-error, so you can
> > either separately or as part of make check, use that option for stats
> (and
> > elsewhere as needed?) and just know that the stats tests that depend on
> > MASS are "false positive" (or, more accurately, missing value) test
> > results, rather than real positives, and go from there.
> >
> > You could also "patch" the tests as part of your build process.
> Somewhere I
> > worked had to do that for parts of the internet tests that were unable to
> > get through the firewall.
> >
> > Best,
> > ~G
> >
> >
> >
> > On Tue, May 4, 2021 at 1:04 PM Henrik Bengtsson <
> henrik.bengtsson at gmail.com>
> > wrote:
> >
> >> Two questions to R Core:
> >>
> >> 1. Is R designed so that 'recommended' packages are optional, or
> >> should that be considered uncharted territories?
> >>
> >> 2. Can such an R build/installation be validated using existing check
> >> methods?
> >>
> >>
> >> --
> >>
> >> Dirk, it's not clear to me whether you know for sure, or you draw
> >> conclusions based your long experience and reading. I think it's very
> >> important that others don't find this thread later on and read your
> >> comments as if they're the "truth" (unless they are).  I haven't
> >> re-read it from start to finish, but there are passages in 'R
> >> Installation and Administration' suggesting you can build and install
> >> R without 'recommended' packages.  For example, post-installation,
> >> Section 'Testing an Installation' suggests you can run (after making
> >> sure `make install-tests`):
> >>
> >> cd tests
> >> ../bin/R CMD make check
> >>
> >> but they fail the same way.  The passage continuous "... and other
> >> useful targets are test-BasePackages and test-Recommended to run tests
> >> of the standard and recommended packages (if installed) respectively."
> >> (*).  So, to me that hints at 'recommended' packages are optional just
> >> as they're "Priority: recommended".  Further down, there's also a
> >> mentioning of:
> >>
> >> $ R_LIBS_USER="" R --vanilla
> >>> Sys.setenv(LC_COLLATE = "C", LC_TIME = "C", LANGUAGE = "en")
> >>> tools::testInstalledPackages(scope = "base")
> >>
> >> which also produces errors when 'recommended' packages are missing,
> >> e.g. "Failed with error:  'there is no package called 'nlme'".
> >>
> >> (*) BTW, '../bin/R CMD make test-BasePackages' gives "make: *** No
> >> rule to make target 'test-BasePackages'.  Stop."
> >>
> >> Thanks,
> >>
> >> /Henrik
> >>
> >> On Tue, May 4, 2021 at 12:22 PM Dirk Eddelbuettel <edd at debian.org>
> wrote:
> >>>
> >>>
> >>> On 4 May 2021 at 11:25, Henrik Bengtsson wrote:
> >>> | FWIW,
> >>> |
> >>> | $ ./configure --help
> >>> | ...
> >>> |   --with-recommended-packages
> >>> |                           use/install recommended R packages [yes]
> >>>
> >>> Of course. But look at the verb in your Subject: no optionality _in
> >> testing_ there.
> >>>
> >>> You obviously need to be able to build R itself to then build the
> >> recommended
> >>> packages you need for testing.
> >>>
> >>> Dirk
> >>>
> >>> --
> >>> https://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org
> >>
> >> ______________________________________________
> >> R-devel at r-project.org mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-devel
> >>
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
> >
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Wed May  5 11:13:02 2021
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Wed, 5 May 2021 11:13:02 +0200
Subject: [Rd] Testing R build when using --without-recommended-packages?
In-Reply-To: <CAD4oTHEVst9d3JXMnHkkziDZi_H83Ojv629M73UnHrZMCa1rNQ@mail.gmail.com>
References: <CAFDcVCS8_8c0g8CXAmHd96XH49YYUX7jt8J9Bj3guWkX8X=Knw@mail.gmail.com>
 <24721.33130.749929.947885@rob.eddelbuettel.com>
 <CAFDcVCTQ4mNK35NGaZEsiTDynuoxVk1VUCRGpgaGjMh3v++ESA@mail.gmail.com>
 <24721.36776.942664.909986@rob.eddelbuettel.com>
 <CAFDcVCTkwZtY5jVHuorT15Yc35aGMJLsyeo3p3AmeUG3oUmsHA@mail.gmail.com>
 <24721.40673.20828.844418@rob.eddelbuettel.com>
 <CAFDcVCS2SgCkVPULH4ovMPX8cgegNuQkY1cA-fW6jpPRHDNeBw@mail.gmail.com>
 <CAD4oTHG3U=tDeX2fEsnf0QPksTZYuJ7GDey45nNBE3DnCDb86w@mail.gmail.com>
 <280c17af-1eda-84a8-42a0-e6f4990f9f3e@gmail.com>
 <CAD4oTHEVst9d3JXMnHkkziDZi_H83Ojv629M73UnHrZMCa1rNQ@mail.gmail.com>
Message-ID: <24722.24990.41734.536374@stat.math.ethz.ch>

>>>>> Gabriel Becker 
>>>>>     on Tue, 4 May 2021 14:40:22 -0700 writes:

    > Hmm, that's fair enough Ben, I stand corrected.  I will say that this seems
    > to be a pretty "soft" recommendation, as these things go, given that it
    > isn't tested for by R CMD check, including with the -as-cran extensions. In
    > principle, it seems like it could be, similar checks are made in package
    > code for inappropriate external-package-symbol usage/

    > Either way, though, I suppose I have a number of packages which have been
    > invisibly non-best-practices compliant for their entire lifetimes (or at
    > least, the portion of that where they had tests/vignettes...).

    > Best,
    > ~G

    > On Tue, May 4, 2021 at 2:22 PM Ben Bolker <bbolker at gmail.com> wrote:

    >> Sorry if this has been pointed out already, but some relevant text
    >> from
    >> 
    >> https://cran.r-project.org/doc/manuals/r-release/R-exts.html#Suggested-packages
    >> 
    >> > Note that someone wanting to run the examples/tests/vignettes may not
    >> have a suggested package available (and it may not even be possible to
    >> install it for that platform). The recommendation used to be to make
    >> their use conditional via if(require("pkgname")): this is OK if that
    >> conditioning is done in examples/tests/vignettes, although using
    >> if(requireNamespace("pkgname")) is preferred, if possible.
    >> 
    >> ...
    >> 
    >> > Some people have assumed that a ?recommended? package in ?Suggests?
    >> can safely be used unconditionally, but this is not so. (R can be
    >> installed without recommended packages, and which packages are
    >> ?recommended? may change.)


Thank you all (Henrik, Gabe, Dirk & Ben) !

I think it would be a good community effort  and worth the time
also of R core to further move into the right direction
as Dirk suggested.

I think we all agree it would be nice if Henrik (and anybody)
could use  'make check' on R's own sources after using
 --without-recommended-packages

Even one more piece of evidence is the   tests/README   file in
the R sources.  It has much more but simply starts with

---------------------------------------------------------------------------
There is a hierarchy of check targets:

     make check

for all builders.  If this works one can be reasonably happy R is working
and do `make install' (or the equivalent).

    make check-devel

for people changing the code: this runs things like the demos and
no-segfault which might be broken by code changes, and checks on the
documentation (effectively R CMD check on each of the base packages).
This needs recommended packages installed.

    make check-all

runs all the checks, those in check-devel plus tests of the recommended
packages.

Note that for complete testing you will need a number of other
......................
......................

---------------------------------------------------------------------------

So, our (R core) own intent has been that   'make check'  should
run w/o rec.packages  but further checking not.

So, yes, please, you are encouraged to send patches against the
R devel trunk  to fix such examples and tests.

Best,
Martin


From murdoch@dunc@n @end|ng |rom gm@||@com  Wed May  5 11:42:54 2021
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Wed, 5 May 2021 05:42:54 -0400
Subject: [Rd] Testing R build when using --without-recommended-packages?
In-Reply-To: <24722.24990.41734.536374@stat.math.ethz.ch>
References: <CAFDcVCS8_8c0g8CXAmHd96XH49YYUX7jt8J9Bj3guWkX8X=Knw@mail.gmail.com>
 <24721.33130.749929.947885@rob.eddelbuettel.com>
 <CAFDcVCTQ4mNK35NGaZEsiTDynuoxVk1VUCRGpgaGjMh3v++ESA@mail.gmail.com>
 <24721.36776.942664.909986@rob.eddelbuettel.com>
 <CAFDcVCTkwZtY5jVHuorT15Yc35aGMJLsyeo3p3AmeUG3oUmsHA@mail.gmail.com>
 <24721.40673.20828.844418@rob.eddelbuettel.com>
 <CAFDcVCS2SgCkVPULH4ovMPX8cgegNuQkY1cA-fW6jpPRHDNeBw@mail.gmail.com>
 <CAD4oTHG3U=tDeX2fEsnf0QPksTZYuJ7GDey45nNBE3DnCDb86w@mail.gmail.com>
 <280c17af-1eda-84a8-42a0-e6f4990f9f3e@gmail.com>
 <CAD4oTHEVst9d3JXMnHkkziDZi_H83Ojv629M73UnHrZMCa1rNQ@mail.gmail.com>
 <24722.24990.41734.536374@stat.math.ethz.ch>
Message-ID: <421b7333-2ca8-1984-f21e-3b665cf98a3a@gmail.com>

On 05/05/2021 5:13 a.m., Martin Maechler wrote:
>>>>>> Gabriel Becker
>>>>>>      on Tue, 4 May 2021 14:40:22 -0700 writes:
> 
>      > Hmm, that's fair enough Ben, I stand corrected.  I will say that this seems
>      > to be a pretty "soft" recommendation, as these things go, given that it
>      > isn't tested for by R CMD check, including with the -as-cran extensions. In
>      > principle, it seems like it could be, similar checks are made in package
>      > code for inappropriate external-package-symbol usage/
> 
>      > Either way, though, I suppose I have a number of packages which have been
>      > invisibly non-best-practices compliant for their entire lifetimes (or at
>      > least, the portion of that where they had tests/vignettes...).
> 
>      > Best,
>      > ~G
> 
>      > On Tue, May 4, 2021 at 2:22 PM Ben Bolker <bbolker at gmail.com> wrote:
> 
>      >> Sorry if this has been pointed out already, but some relevant text
>      >> from
>      >>
>      >> https://cran.r-project.org/doc/manuals/r-release/R-exts.html#Suggested-packages
>      >>
>      >> > Note that someone wanting to run the examples/tests/vignettes may not
>      >> have a suggested package available (and it may not even be possible to
>      >> install it for that platform). The recommendation used to be to make
>      >> their use conditional via if(require("pkgname")): this is OK if that
>      >> conditioning is done in examples/tests/vignettes, although using
>      >> if(requireNamespace("pkgname")) is preferred, if possible.
>      >>
>      >> ...
>      >>
>      >> > Some people have assumed that a ?recommended? package in ?Suggests?
>      >> can safely be used unconditionally, but this is not so. (R can be
>      >> installed without recommended packages, and which packages are
>      >> ?recommended? may change.)
> 
> 
> Thank you all (Henrik, Gabe, Dirk & Ben) !
> 
> I think it would be a good community effort  and worth the time
> also of R core to further move into the right direction
> as Dirk suggested.
> 
> I think we all agree it would be nice if Henrik (and anybody)
> could use  'make check' on R's own sources after using
>   --without-recommended-packages
> 
> Even one more piece of evidence is the   tests/README   file in
> the R sources.  It has much more but simply starts with
> 
> ---------------------------------------------------------------------------
> There is a hierarchy of check targets:
> 
>       make check
> 
> for all builders.  If this works one can be reasonably happy R is working
> and do `make install' (or the equivalent).
> 
>      make check-devel
> 
> for people changing the code: this runs things like the demos and
> no-segfault which might be broken by code changes, and checks on the
> documentation (effectively R CMD check on each of the base packages).
> This needs recommended packages installed.
> 
>      make check-all
> 
> runs all the checks, those in check-devel plus tests of the recommended
> packages.
> 
> Note that for complete testing you will need a number of other
> ......................
> ......................
> 
> ---------------------------------------------------------------------------
> 
> So, our (R core) own intent has been that   'make check'  should
> run w/o rec.packages  but further checking not.
> 
> So, yes, please, you are encouraged to send patches against the
> R devel trunk  to fix such examples and tests.

I think it would be useful to issue some kind of warning if tests are 
skipped.  As mentioned earlier, this is impossible in user-contributed 
packages, which can only return OK or ERROR from their tests.

Duncan Murdoch


From edd @end|ng |rom deb|@n@org  Wed May  5 14:03:51 2021
From: edd @end|ng |rom deb|@n@org (Dirk Eddelbuettel)
Date: Wed, 5 May 2021 07:03:51 -0500
Subject: [Rd] Testing R build when using --without-recommended-packages?
In-Reply-To: <421b7333-2ca8-1984-f21e-3b665cf98a3a@gmail.com>
References: <CAFDcVCS8_8c0g8CXAmHd96XH49YYUX7jt8J9Bj3guWkX8X=Knw@mail.gmail.com>
 <24721.33130.749929.947885@rob.eddelbuettel.com>
 <CAFDcVCTQ4mNK35NGaZEsiTDynuoxVk1VUCRGpgaGjMh3v++ESA@mail.gmail.com>
 <24721.36776.942664.909986@rob.eddelbuettel.com>
 <CAFDcVCTkwZtY5jVHuorT15Yc35aGMJLsyeo3p3AmeUG3oUmsHA@mail.gmail.com>
 <24721.40673.20828.844418@rob.eddelbuettel.com>
 <CAFDcVCS2SgCkVPULH4ovMPX8cgegNuQkY1cA-fW6jpPRHDNeBw@mail.gmail.com>
 <CAD4oTHG3U=tDeX2fEsnf0QPksTZYuJ7GDey45nNBE3DnCDb86w@mail.gmail.com>
 <280c17af-1eda-84a8-42a0-e6f4990f9f3e@gmail.com>
 <CAD4oTHEVst9d3JXMnHkkziDZi_H83Ojv629M73UnHrZMCa1rNQ@mail.gmail.com>
 <24722.24990.41734.536374@stat.math.ethz.ch>
 <421b7333-2ca8-1984-f21e-3b665cf98a3a@gmail.com>
Message-ID: <24722.35239.677540.295797@rob.eddelbuettel.com>


On 5 May 2021 at 05:42, Duncan Murdoch wrote:
| I think it would be useful to issue some kind of warning if tests are 
| skipped.  As mentioned earlier, this is impossible in user-contributed 
| packages, which can only return OK or ERROR from their tests.

Seconded!

Dirk

-- 
https://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From gz@p@t@w@|nberg @end|ng |rom gm@||@com  Wed May  5 16:28:17 2021
From: gz@p@t@w@|nberg @end|ng |rom gm@||@com (Gustavo Zapata Wainberg)
Date: Wed, 5 May 2021 16:28:17 +0200
Subject: [Rd] Inconsistency in median()
In-Reply-To: <24721.28369.583490.768453@stat.math.ethz.ch>
References: <CAFQ8Lqn=bn6pebC7gv13vHMx2PrApM7_Zo-HrxV+C7oVxjM0EA@mail.gmail.com>
 <24721.28369.583490.768453@stat.math.ethz.ch>
Message-ID: <CAFQ8Lq=XTF=rMEqmkcaByuz5iYhcojjo5RdLKJSmigM2JFDhSw@mail.gmail.com>

Hi, thanks Dr. M?chler for your prompt response!

I agree with your explanations about this issue. But I was thinking of
something like adding an argument to median() and mean() that could keep
the attributes of the variables if set to TRUE.

Thanks again.

Best regards

El mar, 4 may 2021 a las 17:57, Martin Maechler (<maechler at stat.math.ethz.ch>)
escribi?:

> >>>>> Gustavo Zapata Wainberg
> >>>>>     on Mon, 3 May 2021 20:48:49 +0200 writes:
>
>     > Hi!
>
>     > I'm wrinting this post because there is an inconsistency
>     > when median() is calculated for even or odd vectors. For
>     > odd vectors, attributes (such as labels added with Hmisc)
>     > are kept after running median(), but this is not the case
>     > if the vector is even, in this last case attributes are
>     > lost.
>
>     > I know that this is due to median() using mean() to obtain
>     > the result when the vector is even, and mean() always
>     > takes attributes off vectors.
>
> Yes, and this has been the design of  median()  for ever :
>
> If n := length(x)  is odd,  the median is "the middle" observation,
>                    and should  equal to x[j] for j = (n+1)/2
>                    and hence e.g., is well defined for an ordered factor.
>
> When  n  is even
>      however, median() must be the mean of "the two middle" observations,
>        which is e.g., not even *defined* for an ordered factor.
>
> We *could* talk of the so called lo-median  or hi-median
> (terms probably coined by John W. Tukey) because (IIRC), these
> are equal to each other and to the median for odd n, but
> are   equal to  x[j]  and  x[j+1]   j=n/2  for even n *and* are
> still "of the same kind" as x[]  itself.
>
> Interestingly, for the mad() { = the median absolute deviation from the
> median}
> we *do* allow to specify logical 'low' and 'high',
> but that for the "outer" median in MAD's definition, not the
> inner one.
>
> ## From <Rsrc>/src/library/stats/R/mad.R :
>
> mad <- function(x, center = median(x), constant = 1.4826,
>                 na.rm = FALSE, low = FALSE, high = FALSE)
> {
>     if(na.rm)
>         x <- x[!is.na(x)]
>     n <- length(x)
>     constant *
>         if((low || high) && n%%2 == 0) {
>             if(low && high) stop("'low' and 'high' cannot be both TRUE")
>             n2 <- n %/% 2 + as.integer(high)
>             sort(abs(x - center), partial = n2)[n2]
>         }
>         else median(abs(x - center))
> }
>
>
>
>
>     > Don't you think that attributes should be kept in both
>     > cases?
>
> well, not all attributes can be kept.
> Note that for *named* vectors x,  x[j] can (and does) keep the name,
> but there's definitely no sensible name to give to (x[j] + x[j+1])/2
>
> I'm willing to collaborate with some, considering
> to extend  median.default()  making  hi-median and lo-median
> available to the user.
> Both of these will always return x[j] for some j and hence keep
> all (sensible!) attributes (well, if the `[`-method for the
> corresponding class has been defined correctly; I've encountered
> quite a few cases where people created vector-like classes but
> did not provide a "correct"  subsetting method (typically you
> should make sure both a `[[` and `[` method works!).
>
> Best regards,
> Martin
>
> Martin Maechler
> ETH Zurich  and  R Core team
>
>     > And, going further, shouldn't mean() keep
>     > attributes as well? I have looked in R's Bugzilla and I
>     > didn't find an entry related to this issue.
>
>     > Please, let me know if you consider that this issue should
>     > be posted in R's bugzilla.
>
>     > Here is an example with code.
>
>     > rndvar <- rnorm(n = 100)
>
>     > Hmisc::label(rndvar) <- "A label for RNDVAR"
>
>     > str(median(rndvar[-c(1,2)]))
>
>     > Returns: "num 0.0368"
>
>     > str(median(rndvar[-1]))
>
>     > Returns: 'labelled' num 0.0322 - attr(*, "label")= chr "A
>     > label for RNDVAR"
>
>     > Thanks in advance!
>
>     > Gustavo Zapata-Wainberg
>
>     >   [[alternative HTML version deleted]]
>
>     > ______________________________________________
>     > R-devel at r-project.org mailing list
>     > https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From dw|n@em|u@ @end|ng |rom comc@@t@net  Wed May  5 19:11:58 2021
From: dw|n@em|u@ @end|ng |rom comc@@t@net (David Winsemius)
Date: Wed, 5 May 2021 11:11:58 -0600
Subject: [Rd] Inconsistency in median()
In-Reply-To: <CAFQ8Lq=XTF=rMEqmkcaByuz5iYhcojjo5RdLKJSmigM2JFDhSw@mail.gmail.com>
References: <CAFQ8Lq=XTF=rMEqmkcaByuz5iYhcojjo5RdLKJSmigM2JFDhSw@mail.gmail.com>
Message-ID: <79EC9CF7-489D-45F9-B76F-982491AF0AEC@comcast.net>

It would almost trivial to make a wrapper tha first captures attributes, runs median, and then returns the Re-attribute-ed value.

David. 

Sent from my iPhone

> On May 5, 2021, at 8:29 AM, Gustavo Zapata Wainberg <gzapatawainberg at gmail.com> wrote:
> 
> ?Hi, thanks Dr. M?chler for your prompt response!
> 
> I agree with your explanations about this issue. But I was thinking of
> something like adding an argument to median() and mean() that could keep
> the attributes of the variables if set to TRUE.
> 
> Thanks again.
> 
> Best regards
> 
> El mar, 4 may 2021 a las 17:57, Martin Maechler (<maechler at stat.math.ethz.ch>)
> escribi?:
> 
>>>>>>> Gustavo Zapata Wainberg
>>>>>>>    on Mon, 3 May 2021 20:48:49 +0200 writes:
>> 
>>> Hi!
>> 
>>> I'm wrinting this post because there is an inconsistency
>>> when median() is calculated for even or odd vectors. For
>>> odd vectors, attributes (such as labels added with Hmisc)
>>> are kept after running median(), but this is not the case
>>> if the vector is even, in this last case attributes are
>>> lost.
>> 
>>> I know that this is due to median() using mean() to obtain
>>> the result when the vector is even, and mean() always
>>> takes attributes off vectors.
>> 
>> Yes, and this has been the design of  median()  for ever :
>> 
>> If n := length(x)  is odd,  the median is "the middle" observation,
>>                   and should  equal to x[j] for j = (n+1)/2
>>                   and hence e.g., is well defined for an ordered factor.
>> 
>> When  n  is even
>>     however, median() must be the mean of "the two middle" observations,
>>       which is e.g., not even *defined* for an ordered factor.
>> 
>> We *could* talk of the so called lo-median  or hi-median
>> (terms probably coined by John W. Tukey) because (IIRC), these
>> are equal to each other and to the median for odd n, but
>> are   equal to  x[j]  and  x[j+1]   j=n/2  for even n *and* are
>> still "of the same kind" as x[]  itself.
>> 
>> Interestingly, for the mad() { = the median absolute deviation from the
>> median}
>> we *do* allow to specify logical 'low' and 'high',
>> but that for the "outer" median in MAD's definition, not the
>> inner one.
>> 
>> ## From <Rsrc>/src/library/stats/R/mad.R :
>> 
>> mad <- function(x, center = median(x), constant = 1.4826,
>>                na.rm = FALSE, low = FALSE, high = FALSE)
>> {
>>    if(na.rm)
>>        x <- x[!is.na(x)]
>>    n <- length(x)
>>    constant *
>>        if((low || high) && n%%2 == 0) {
>>            if(low && high) stop("'low' and 'high' cannot be both TRUE")
>>            n2 <- n %/% 2 + as.integer(high)
>>            sort(abs(x - center), partial = n2)[n2]
>>        }
>>        else median(abs(x - center))
>> }
>> 
>> 
>> 
>> 
>>> Don't you think that attributes should be kept in both
>>> cases?
>> 
>> well, not all attributes can be kept.
>> Note that for *named* vectors x,  x[j] can (and does) keep the name,
>> but there's definitely no sensible name to give to (x[j] + x[j+1])/2
>> 
>> I'm willing to collaborate with some, considering
>> to extend  median.default()  making  hi-median and lo-median
>> available to the user.
>> Both of these will always return x[j] for some j and hence keep
>> all (sensible!) attributes (well, if the `[`-method for the
>> corresponding class has been defined correctly; I've encountered
>> quite a few cases where people created vector-like classes but
>> did not provide a "correct"  subsetting method (typically you
>> should make sure both a `[[` and `[` method works!).
>> 
>> Best regards,
>> Martin
>> 
>> Martin Maechler
>> ETH Zurich  and  R Core team
>> 
>>> And, going further, shouldn't mean() keep
>>> attributes as well? I have looked in R's Bugzilla and I
>>> didn't find an entry related to this issue.
>> 
>>> Please, let me know if you consider that this issue should
>>> be posted in R's bugzilla.
>> 
>>> Here is an example with code.
>> 
>>> rndvar <- rnorm(n = 100)
>> 
>>> Hmisc::label(rndvar) <- "A label for RNDVAR"
>> 
>>> str(median(rndvar[-c(1,2)]))
>> 
>>> Returns: "num 0.0368"
>> 
>>> str(median(rndvar[-1]))
>> 
>>> Returns: 'labelled' num 0.0322 - attr(*, "label")= chr "A
>>> label for RNDVAR"
>> 
>>> Thanks in advance!
>> 
>>> Gustavo Zapata-Wainberg
>> 
>>>  [[alternative HTML version deleted]]
>> 
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>> 
> 
>    [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From henr|k@bengt@@on @end|ng |rom gm@||@com  Wed May  5 21:27:48 2021
From: henr|k@bengt@@on @end|ng |rom gm@||@com (Henrik Bengtsson)
Date: Wed, 5 May 2021 12:27:48 -0700
Subject: [Rd] Testing R build when using --without-recommended-packages?
In-Reply-To: <24722.24990.41734.536374@stat.math.ethz.ch>
References: <CAFDcVCS8_8c0g8CXAmHd96XH49YYUX7jt8J9Bj3guWkX8X=Knw@mail.gmail.com>
 <24721.33130.749929.947885@rob.eddelbuettel.com>
 <CAFDcVCTQ4mNK35NGaZEsiTDynuoxVk1VUCRGpgaGjMh3v++ESA@mail.gmail.com>
 <24721.36776.942664.909986@rob.eddelbuettel.com>
 <CAFDcVCTkwZtY5jVHuorT15Yc35aGMJLsyeo3p3AmeUG3oUmsHA@mail.gmail.com>
 <24721.40673.20828.844418@rob.eddelbuettel.com>
 <CAFDcVCS2SgCkVPULH4ovMPX8cgegNuQkY1cA-fW6jpPRHDNeBw@mail.gmail.com>
 <CAD4oTHG3U=tDeX2fEsnf0QPksTZYuJ7GDey45nNBE3DnCDb86w@mail.gmail.com>
 <280c17af-1eda-84a8-42a0-e6f4990f9f3e@gmail.com>
 <CAD4oTHEVst9d3JXMnHkkziDZi_H83Ojv629M73UnHrZMCa1rNQ@mail.gmail.com>
 <24722.24990.41734.536374@stat.math.ethz.ch>
Message-ID: <CAFDcVCS6BNF_gpgvpkZCU+X6-t4348VP=QzfCF_DcaOFANTGEg@mail.gmail.com>

On Wed, May 5, 2021 at 2:13 AM Martin Maechler
<maechler at stat.math.ethz.ch> wrote:
>
> >>>>> Gabriel Becker
> >>>>>     on Tue, 4 May 2021 14:40:22 -0700 writes:
>
>     > Hmm, that's fair enough Ben, I stand corrected.  I will say that this seems
>     > to be a pretty "soft" recommendation, as these things go, given that it
>     > isn't tested for by R CMD check, including with the -as-cran extensions. In
>     > principle, it seems like it could be, similar checks are made in package
>     > code for inappropriate external-package-symbol usage/
>
>     > Either way, though, I suppose I have a number of packages which have been
>     > invisibly non-best-practices compliant for their entire lifetimes (or at
>     > least, the portion of that where they had tests/vignettes...).
>
>     > Best,
>     > ~G
>
>     > On Tue, May 4, 2021 at 2:22 PM Ben Bolker <bbolker at gmail.com> wrote:
>
>     >> Sorry if this has been pointed out already, but some relevant text
>     >> from
>     >>
>     >> https://cran.r-project.org/doc/manuals/r-release/R-exts.html#Suggested-packages
>     >>
>     >> > Note that someone wanting to run the examples/tests/vignettes may not
>     >> have a suggested package available (and it may not even be possible to
>     >> install it for that platform). The recommendation used to be to make
>     >> their use conditional via if(require("pkgname")): this is OK if that
>     >> conditioning is done in examples/tests/vignettes, although using
>     >> if(requireNamespace("pkgname")) is preferred, if possible.
>     >>
>     >> ...
>     >>
>     >> > Some people have assumed that a ?recommended? package in ?Suggests?
>     >> can safely be used unconditionally, but this is not so. (R can be
>     >> installed without recommended packages, and which packages are
>     >> ?recommended? may change.)
>
>
> Thank you all (Henrik, Gabe, Dirk & Ben) !
>
> I think it would be a good community effort  and worth the time
> also of R core to further move into the right direction
> as Dirk suggested.
>
> I think we all agree it would be nice if Henrik (and anybody)
> could use  'make check' on R's own sources after using
>  --without-recommended-packages
>
> Even one more piece of evidence is the   tests/README   file in
> the R sources.  It has much more but simply starts with
>
> ---------------------------------------------------------------------------
> There is a hierarchy of check targets:
>
>      make check
>
> for all builders.  If this works one can be reasonably happy R is working
> and do `make install' (or the equivalent).
>
>     make check-devel
>
> for people changing the code: this runs things like the demos and
> no-segfault which might be broken by code changes, and checks on the
> documentation (effectively R CMD check on each of the base packages).
> This needs recommended packages installed.
>
>     make check-all
>
> runs all the checks, those in check-devel plus tests of the recommended
> packages.
>
> Note that for complete testing you will need a number of other
> ......................
> ......................
>
> ---------------------------------------------------------------------------
>
> So, our (R core) own intent has been that   'make check'  should
> run w/o rec.packages  but further checking not.
>
> So, yes, please, you are encouraged to send patches against the
> R devel trunk  to fix such examples and tests.

Thanks Martin!  Thanks for confirming and for being open to patches.
This encourages me to try to patch what we've got so that 'make check'
and 'make check-devel' can complete also without 'recommended'
packages.

/Henrik

>
> Best,
> Martin
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From j@goreck| @end|ng |rom w|t@edu@p|  Mon May 10 12:42:09 2021
From: j@goreck| @end|ng |rom w|t@edu@p| (Jan Gorecki)
Date: Mon, 10 May 2021 12:42:09 +0200
Subject: [Rd] R-devel new warning: no longer be an S4 object
Message-ID: <CAOO9MKVd0EYf8knZuZhbhAGy-J80b2qovhDAmR08Qe3077GGMg@mail.gmail.com>

Hi R-devs,
R 4.0.5 gives no warning. Is it expected? Searching the news for "I("
doesn't give any info. Thanks

z = I(getClass("MethodDefinition"))
Warning message:
In `class<-`(x, unique.default(c("AsIs", oldClass(x)))) :
  Setting class(x) to multiple strings ("AsIs", "classRepresentation",
...); result will no longer be an S4 object

	[[alternative HTML version deleted]]


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Mon May 10 14:07:55 2021
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Mon, 10 May 2021 14:07:55 +0200
Subject: [Rd] R-devel new warning: no longer be an S4 object
In-Reply-To: <CAOO9MKVd0EYf8knZuZhbhAGy-J80b2qovhDAmR08Qe3077GGMg@mail.gmail.com>
References: <CAOO9MKVd0EYf8knZuZhbhAGy-J80b2qovhDAmR08Qe3077GGMg@mail.gmail.com>
Message-ID: <24729.8731.146971.424258@stat.math.ethz.ch>

>>>>> Jan Gorecki 
>>>>>     on Mon, 10 May 2021 12:42:09 +0200 writes:

    > Hi R-devs,
    > R 4.0.5 gives no warning. Is it expected? Searching the news for "I("
    > doesn't give any info. Thanks

    > z = I(getClass("MethodDefinition"))

Now what exactly did you intend with the above line ?

I'm bold and say (for the moment) that the above line has always
been very dubious if not misleading,
and this "fact" is now finally revealed by the warning

    > Warning message:
    > In `class<-`(x, unique.default(c("AsIs", oldClass(x)))) :
    > Setting class(x) to multiple strings ("AsIs", "classRepresentation",
    > ...); result will no longer be an S4 object

So, yes, the change has been on purpose to warn about problems,
you'd get later when trying to work with 'z'.


    > [[alternative HTML version deleted]]

   (your fault: do use plain text aka   MIME time 'text/plain'))


From bbo|ker @end|ng |rom gm@||@com  Mon May 17 19:20:19 2021
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Mon, 17 May 2021 13:20:19 -0400
Subject: [Rd] base R pipe documentation
Message-ID: <b8cb6ce2-14ed-eb07-d5da-9c2b22cb9d9d@gmail.com>

   As of right now, as far as I can tell, the documentation for the new 
native |> pipe still says that it's experimental.

https://github.com/wch/r-source/blob/trunk/src/library/base/man/pipeOp.Rd#L45

      *Pipe support is experimental and may change prior to release.*

Also still in the 4-1 branch:

https://github.com/wch/r-source/blob/R-4-1-branch/src/library/base/man/pipeOp.Rd#L45

   (The corresponding comment in the NEWS file has been fixed in the 
last 24 hours, but hasn't propagated to the online/HTML version on the 
developer page yet ...)

   As a "wish list" item, if there are any particularly 
salient/important  differences between the |> pipe and the %>% magrittr 
pipe, it would be great to have those documented (I know that 
documenting the difference between a base-R operator and the one that's 
implemented in a non-Recommended package is a little weird, but it would 
be helpful in this case ...)  I know I could go back to the mailing list 
discussion at 
https://hypatia.math.ethz.ch/pipermail/r-devel/2020-December/080173.html 
and try to figure it out for myself ...

   cheers
    Ben Bolker


From pd@|gd @end|ng |rom gm@||@com  Mon May 17 21:52:31 2021
From: pd@|gd @end|ng |rom gm@||@com (peter dalgaard)
Date: Mon, 17 May 2021 21:52:31 +0200
Subject: [Rd] base R pipe documentation
In-Reply-To: <b8cb6ce2-14ed-eb07-d5da-9c2b22cb9d9d@gmail.com>
References: <b8cb6ce2-14ed-eb07-d5da-9c2b22cb9d9d@gmail.com>
Message-ID: <5CFA3DF7-7BEC-45C0-B1C6-3EB6CB1F78FE@gmail.com>

OK, zapped the \note for 4-1-branch (for now). This close to release, changes have to be very small and safe to be allowed in, so anything more than that will have to wait for a later version.

- Peter

> On 17 May 2021, at 19:20 , Ben Bolker <bbolker at gmail.com> wrote:
> 
>  As of right now, as far as I can tell, the documentation for the new native |> pipe still says that it's experimental.
> 
> https://github.com/wch/r-source/blob/trunk/src/library/base/man/pipeOp.Rd#L45
> 
>     *Pipe support is experimental and may change prior to release.*
> 
> Also still in the 4-1 branch:
> 
> https://github.com/wch/r-source/blob/R-4-1-branch/src/library/base/man/pipeOp.Rd#L45
> 
>  (The corresponding comment in the NEWS file has been fixed in the last 24 hours, but hasn't propagated to the online/HTML version on the developer page yet ...)
> 
>  As a "wish list" item, if there are any particularly salient/important  differences between the |> pipe and the %>% magrittr pipe, it would be great to have those documented (I know that documenting the difference between a base-R operator and the one that's implemented in a non-Recommended package is a little weird, but it would be helpful in this case ...)  I know I could go back to the mailing list discussion at https://hypatia.math.ethz.ch/pipermail/r-devel/2020-December/080173.html and try to figure it out for myself ...
> 
>  cheers
>   Ben Bolker
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From dm|ch@e|p@rr|@h @end|ng |rom net@c@pe@net  Tue May 18 04:05:04 2021
From: dm|ch@e|p@rr|@h @end|ng |rom net@c@pe@net (Dmichael Parrish)
Date: Tue, 18 May 2021 02:05:04 +0000 (UTC)
Subject: [Rd] `mode`
References: <435593545.930811.1621303504980.ref@mail.yahoo.com>
Message-ID: <435593545.930811.1621303504980@mail.yahoo.com>

Hello,
Kindly revise the documentation for `mode` to reflect
foo <- function () {}
typeof(foo)
# [1] "closure"
mode(foo)# [1] "function"


`help(mode)` states:
Modes have the same set of names as types (see typeof) except that

??? types "integer" and "double" are returned as "numeric".

??? types "special" and "builtin" are returned as "function".

??? type "symbol" is called mode "name".

??? type "language" is returned as "(" or "call".

I am presently reading `help(mode)` on:

write.dcf(R.Version())
# platform: x86_64-w64-mingw32
# arch: x86_64
# os: mingw32
# system: x86_64, mingw32
# status:
# major: 4
# minor: 0.3
# year: 2020
# month: 10
# day: 10
# svn rev: 79318
# language: R
# version.string: R version 4.0.3 (2020-10-10)
# nickname: Bunny-Wunnies Freak Out

 
__________
Hmo < 0.1 L tanh kd ---Miche, 1951 / I have placed the sand for the bound of the sea... and though the waves thereof toss themselves... they cannot pass over it ---YHWH, ca. 600 B.C. (Jer. 5:22)
	[[alternative HTML version deleted]]


From m@ech|er @end|ng |rom @t@t@m@th@ethz@ch  Tue May 18 09:24:01 2021
From: m@ech|er @end|ng |rom @t@t@m@th@ethz@ch (Martin Maechler)
Date: Tue, 18 May 2021 09:24:01 +0200
Subject: [Rd] `mode`
In-Reply-To: <435593545.930811.1621303504980@mail.yahoo.com>
References: <435593545.930811.1621303504980.ref@mail.yahoo.com>
 <435593545.930811.1621303504980@mail.yahoo.com>
Message-ID: <24739.27537.776056.240203@stat.math.ethz.ch>

>>>>> Dmichael Parrish via R-devel 
>>>>>     on Tue, 18 May 2021 02:05:04 +0000 (UTC) writes:

    > Hello, Kindly revise the documentation for `mode` to
    > reflect foo <- function () {} typeof(foo) # [1] "closure"
    > mode(foo)# [1] "function"


    > `help(mode)` states: Modes have the same set of names as
    > types (see typeof) except that

    > ??? types "integer" and "double" are returned as
    > "numeric".

    > ??? types "special" and "builtin" are returned as
    > "function".

    > ??? type "symbol" is called mode "name".

    > ??? type "language" is returned as "(" or "call".


Indeed, that help file is missing  "closure", ...
amazingly, for all the history of R (of 25+ years).

Thank you!

I've already fixed this in the sources' trunk (svn rev 80321) a
minute ago; of course this will not make it anymore into R 4.1.0
but in its "patched" version, and then 4.1.1 and one.

With thankful regards,
Martin

--
Martin Maechler
ETH Zurich  and  R Core team


    > I am presently reading `help(mode)` on:

    > write.dcf(R.Version()) # platform: x86_64-w64-mingw32 #
    > arch: x86_64 # os: mingw32 # system: x86_64, mingw32 #
    > status: # major: 4 # minor: 0.3 # year: 2020 # month: 10 #
    > day: 10 # svn rev: 79318 # language: R # version.string: R
    > version 4.0.3 (2020-10-10) # nickname: Bunny-Wunnies Freak
    > Out

 
    > __________ Hmo < 0.1 L tanh kd ---Miche, 1951 / I have
    > placed the sand for the bound of the sea... and though the
    > waves thereof toss themselves... they cannot pass over it
    > ---YHWH, ca. 600 B.C. (Jer. 5:22)


From JH@rm@e @end|ng |rom roku@com  Tue May 18 16:39:17 2021
From: JH@rm@e @end|ng |rom roku@com (Jorgen Harmse)
Date: Tue, 18 May 2021 14:39:17 +0000
Subject: [Rd] Add to Documentation of atan2.
Message-ID: <1B4CC4B2-94B0-4386-B355-76543C2EFD1C@roku.com>

The current documentation says that atan2(y,x) is the angle between the x-axis and the vector from the origin to (x,y), but what does this mean when x & y are complex? The function seems to pick theta with Re(theta) between -pi and pi and with tan(theta) (approximately) equal to y/x, but that leaves 2 (sometimes 3) options, and there must be a set (branch region with 3 real dimensions?) on which the function is discontinuous. Please add details.

Even for real inputs, it might help to spell out the behaviour on the negative x-axis. It mostly matches the branch-cut rules for the other functions, but atan2(0,0)==0 is a unexpected.

I also suggest ?See Also? links from trigonometric functions to hyperbolic functions and from hyperbolic functions to exponential & logarithmic functions.

Regards,
Jorgen Harmse.



> R.version.string

[1] "R version 4.0.4 (2021-02-15)"






	[[alternative HTML version deleted]]


From w||||@mwdun|@p @end|ng |rom gm@||@com  Tue May 18 20:08:39 2021
From: w||||@mwdun|@p @end|ng |rom gm@||@com (Bill Dunlap)
Date: Tue, 18 May 2021 11:08:39 -0700
Subject: [Rd] base R pipe documentation
In-Reply-To: <b8cb6ce2-14ed-eb07-d5da-9c2b22cb9d9d@gmail.com>
References: <b8cb6ce2-14ed-eb07-d5da-9c2b22cb9d9d@gmail.com>
Message-ID: <CAHqSRuQdEs-Jj79kwKhmyycAvL+NF-pOrJU_iuUq+rfyRU6dYg@mail.gmail.com>

It would be nice to have "|>" listed in the precedence table in
help(Syntax).  I think it has the same precedence as "%any%" and both are
left-associative.
  > quote( a |> f1() %any% f2())
  f1(a) %any% f2()
  > quote( a %any% f1() |> f2())
  f2(a %any% f1())

help(`|>`) does mention magrittr's pipe operator so the user can compare
and contrast them.

-Bill

On Mon, May 17, 2021 at 10:20 AM Ben Bolker <bbolker at gmail.com> wrote:

>    As of right now, as far as I can tell, the documentation for the new
> native |> pipe still says that it's experimental.
>
>
> https://github.com/wch/r-source/blob/trunk/src/library/base/man/pipeOp.Rd#L45
>
>       *Pipe support is experimental and may change prior to release.*
>
> Also still in the 4-1 branch:
>
>
> https://github.com/wch/r-source/blob/R-4-1-branch/src/library/base/man/pipeOp.Rd#L45
>
>    (The corresponding comment in the NEWS file has been fixed in the
> last 24 hours, but hasn't propagated to the online/HTML version on the
> developer page yet ...)
>
>    As a "wish list" item, if there are any particularly
> salient/important  differences between the |> pipe and the %>% magrittr
> pipe, it would be great to have those documented (I know that
> documenting the difference between a base-R operator and the one that's
> implemented in a non-Recommended package is a little weird, but it would
> be helpful in this case ...)  I know I could go back to the mailing list
> discussion at
> https://hypatia.math.ethz.ch/pipermail/r-devel/2020-December/080173.html
> and try to figure it out for myself ...
>
>    cheers
>     Ben Bolker
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Tue May 18 20:30:13 2021
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Tue, 18 May 2021 14:30:13 -0400
Subject: [Rd] Add to Documentation of atan2.
In-Reply-To: <1B4CC4B2-94B0-4386-B355-76543C2EFD1C@roku.com>
References: <1B4CC4B2-94B0-4386-B355-76543C2EFD1C@roku.com>
Message-ID: <b58e40e6-e6cf-bafe-e80d-5f9e47f8d3ed@gmail.com>

   Can you dig through into the code, see what's going on, and suggest a 
documentation patch?  To get you started, the code for the complex 
version of atan2 is in

https://svn.r-project.org/R/trunk/src/main/complex.c

z_atan2 is at line 669 (the first argument is a pointer to the result, 
args 2 [csn] and 3 [ccs] are pointers to the arguments of atan2())

  In generic cases the computation is

	dr = catan(dcsn / dccs);
	if(creal(dccs) < 0) dr += M_PI;
	if(creal(dr) > M_PI) dr -= 2 * M_PI;

where dcsn, dccs are converted versions of the args.

catan() is *either* taken from system libraries or is defined at line 489.

   On my system (Ubuntu), 'man 3 catan' gives documentation on the 
function, and says "The real part of y is chosen in the interval 
[-pi/2,pi/2]" - but that _could_ be system-dependent.

    cheers
    Ben Bolker

On 5/18/21 10:39 AM, Jorgen Harmse via R-devel wrote:
> The current documentation says that atan2(y,x) is the angle between the x-axis and the vector from the origin to (x,y), but what does this mean when x & y are complex? The function seems to pick theta with Re(theta) between -pi and pi and with tan(theta) (approximately) equal to y/x, but that leaves 2 (sometimes 3) options, and there must be a set (branch region with 3 real dimensions?) on which the function is discontinuous. Please add details.
> 
> Even for real inputs, it might help to spell out the behaviour on the negative x-axis. It mostly matches the branch-cut rules for the other functions, but atan2(0,0)==0 is a unexpected.
> 
> I also suggest ?See Also? links from trigonometric functions to hyperbolic functions and from hyperbolic functions to exponential & logarithmic functions.
> 
> Regards,
> Jorgen Harmse.
> 
> 
> 
>> R.version.string
> 
> [1] "R version 4.0.4 (2021-02-15)"
> 
> 
> 
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From @te|@nML @end|ng |rom co||oc@t|on@@de  Tue May 18 23:12:02 2021
From: @te|@nML @end|ng |rom co||oc@t|on@@de (Stefan Evert)
Date: Tue, 18 May 2021 23:12:02 +0200
Subject: [Rd] Add to Documentation of atan2.
In-Reply-To: <b58e40e6-e6cf-bafe-e80d-5f9e47f8d3ed@gmail.com>
References: <1B4CC4B2-94B0-4386-B355-76543C2EFD1C@roku.com>
 <b58e40e6-e6cf-bafe-e80d-5f9e47f8d3ed@gmail.com>
Message-ID: <3DCD5EE4-74A2-4BA8-AC50-AA97A91E2E7C@collocations.de>



> On 18 May 2021, at 20:30, Ben Bolker <bbolker at gmail.com> wrote:
> 
>  On my system (Ubuntu), 'man 3 catan' gives documentation on the function, and says "The real part of y is chosen in the interval [-pi/2,pi/2]" - but that _could_ be system-dependent.

My copy of "C in a Nutshell" suggests that this requirement is part of the C99 standard (specified for catanh).

Best,
Stefan

From rpru|m @end|ng |rom c@|v|n@edu  Thu May 20 06:02:32 2021
From: rpru|m @end|ng |rom c@|v|n@edu (Randall Pruim)
Date: Thu, 20 May 2021 04:02:32 +0000
Subject: [Rd] {splines} package gone missing?
Message-ID: <5A066EFD-798C-41F3-9271-A53DF8F1CBD2@calvin.edu>


https://cran.r-project.org/web/packages/splines/index.html claims the the {splines} package has been archived.  If I follow the link there to the archives, the newest version shown is from 1999.  It seems like something has gone wrong with this package.

I checked on another mirror and {splines} is missing there as well.

?rjp


From pd@|gd @end|ng |rom gm@||@com  Thu May 20 09:14:07 2021
From: pd@|gd @end|ng |rom gm@||@com (peter dalgaard)
Date: Thu, 20 May 2021 09:14:07 +0200
Subject: [Rd] {splines} package gone missing?
In-Reply-To: <5A066EFD-798C-41F3-9271-A53DF8F1CBD2@calvin.edu>
References: <5A066EFD-798C-41F3-9271-A53DF8F1CBD2@calvin.edu>
Message-ID: <2D9CEF37-DD35-4D5E-9D42-16143E0A3C63@gmail.com>

It is part of base R, so comes with the R sources:

Peters-MacBook-Air:R pd$ ls src/library/
Makefile.in   compiler/     grid/         splines/      tools/
Makefile.win  datasets/     methods/      stats/        translations/
Recommended/  grDevices/    parallel/     stats4/       utils/
base/         graphics/     profile/      tcltk/        

- pd

> On 20 May 2021, at 06:02 , Randall Pruim <rpruim at calvin.edu> wrote:
> 
> 
> https://cran.r-project.org/web/packages/splines/index.html claims the the {splines} package has been archived.  If I follow the link there to the archives, the newest version shown is from 1999.  It seems like something has gone wrong with this package.
> 
> I checked on another mirror and {splines} is missing there as well.
> 
> ?rjp
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From pd@|gd @end|ng |rom gm@||@com  Tue May 18 10:06:24 2021
From: pd@|gd @end|ng |rom gm@||@com (Peter Dalgaard)
Date: Tue, 18 May 2021 10:06:24 +0200
Subject: [Rd] R 4.1.0 is released
Message-ID: <9C835984-B6BF-439C-A01A-3E5906D720B6@gmail.com>


The build system rolled up R-4.1.0.tar.gz (codename "Camp Pontanezen") this morning.

This is a major update, notably containing the new native pipe operator "|>" and 
shorthand inline functions "\(x) x+1".

The list below details the changes in this release. 

You can get the source code from

https://cran.r-project.org/src/base/R-4/R-4.1.0.tar.gz

or wait for it to be mirrored at a CRAN site nearer to you.

Binaries for various platforms will appear in due course.


For the R Core Team,

Peter Dalgaard


These are the checksums (md5 and SHA-256) for the freshly created files, in case you wish
to check that they are uncorrupted:

MD5 (AUTHORS) = b9c44f9f78cab3184ad9898bebc854b4
MD5 (COPYING) = eb723b61539feef013de476e68b5c50a
MD5 (COPYING.LIB) = a6f89e2100d9b6cdffcea4f398e37343
MD5 (FAQ) = 639fbbba9998cae70ef058be42b80a52
MD5 (INSTALL) = 7893f754308ca31f1ccf62055090ad7b
MD5 (NEWS) = b02805558a8315f1a93c7f7d7cd879c1
MD5 (NEWS.0) = bfcd7c147251b5474d96848c6f57e5a8
MD5 (NEWS.1) = eb78c4d053ec9c32b815cf0c2ebea801
MD5 (NEWS.2) = a767f7809324c73c49eaff47d14bce81
MD5 (NEWS.3) = e55ed2c8a547b827b46e08eb7137ba23
MD5 (R-latest.tar.gz) = bd80f97d0e46a71408f5bc25652a0203
MD5 (README) = f468f281c919665e276a1b691decbbe6
MD5 (RESOURCES) = 529223fd3ffef95731d0a87353108435
MD5 (THANKS) = 251d20510bfc3cc93b82c5a99f7efcc6
MD5 (VERSION-INFO.dcf) = 1f3cf39735afb48fea434bca2b7bf483
MD5 (R-4/R-4.1.0.tar.gz) = bd80f97d0e46a71408f5bc25652a0203

2cde824a7b18958e5f06b391c801c8288be0f84fa8934b7ddefef23c67e60c09  AUTHORS
e6d6a009505e345fe949e1310334fcb0747f28dae2856759de102ab66b722cb4  COPYING
6095e9ffa777dd22839f7801aa845b31c9ed07f3d6bf8a26dc5d2dec8ccc0ef3  COPYING.LIB
0dce85f38b9d6351a1b63f057dfbc7f572966245add12946482e57e60d41547c  FAQ
f87461be6cbaecc4dce44ac58e5bd52364b0491ccdadaf846cb9b452e9550f31  INSTALL
20e7185fb5af0f2ac825b27c16afd57ff206726117c6ac5cf7c6f230337af093  NEWS
4e21b62f515b749f80997063fceab626d7258c7d650e81a662ba8e0640f12f62  NEWS.0
12b30c724117b1b2b11484673906a6dcd48a361f69fc420b36194f9218692d01  NEWS.1
ba74618bc3f4c0e336dca13d472402a1863d12ba6f7f91a1782bc469ee986f6d  NEWS.2
1910a2405300b9bc7c76beeb0753a5249cf799afe175ce28f8d782fab723e012  NEWS.3
e8e68959d7282ca147360fc9644ada9bd161bab781bab14d33b8999a95182781  R-latest.tar.gz
2fdd3e90f23f32692d4b3a0c0452f2c219a10882033d1774f8cadf25886c3ddc  README
408737572ecc6e1135fdb2cf7a9dbb1a6cb27967c757f1771b8c39d1fd2f1ab9  RESOURCES
c9c7cb32308b4e560a22c858819ade9de524a602abd4e92d1c328c89f8037d73  THANKS
2f834a058dcfcdfb7eedf45aa0d897ea1a7c43a9460ffd22f73b60919cb1bf57  VERSION-INFO.dcf
e8e68959d7282ca147360fc9644ada9bd161bab781bab14d33b8999a95182781  R-4/R-4.1.0.tar.gz

This is the relevant part of the NEWS file

CHANGES IN R 4.1.0:

  FUTURE DIRECTIONS:

    * It is planned that the 4.1.x series will be the last to support
      32-bit Windows, with production of binary packages for that
      series continuing until early 2023.

  SIGNIFICANT USER-VISIBLE CHANGES:

    * Data set esoph in package datasets now provides the correct
      numbers of controls; previously it had the numbers of cases added
      to these.  (Reported by Alexander Fowler in PR#17964.)

  NEW FEATURES:

    * www.omegahat.net is no longer one of the repositories known by
      default to setRepositories().  (Nowadays it only provides source
      packages and is often unavailable.)

    * Function package_dependencies() (in package tools) can now use
      different dependency types for direct and recursive dependencies.

    * The checking of the size of tarball in R CMD check --as-cran
      <pkg> may be tweaked via the new environment variable
      _R_CHECK_CRAN_INCOMING_TARBALL_THRESHOLD_, as suggested in
      PR#17777 by Jan Gorecki.

    * Using c() to combine a factor with other factors now gives a
      factor, an ordered factor when combining ordered factors with
      identical levels.

    * apply() gains a simplify argument to allow disabling of
      simplification of results.

    * The format() method for class "ftable" gets a new option justify.
      (Suggested by Thomas Soeiro.)

    * New ...names() utility.  (Proposed by Neal Fultz in PR#17705.)

    * type.convert() now warns when its as.is argument is not
      specified, as the help file always said it _should_.  In that
      case, the default is changed to TRUE in line with its change in
      read.table() (related to stringsAsFactor) in R 4.0.0.

    * When printing list arrays, classed objects are now shown _via_
      their format() value if this is a short enough character string,
      or by giving the first elements of their class vector and their
      length.

    * capabilities() gets new entry "Rprof" which is TRUE when R has
      been configured with the equivalent of --enable-R-profiling (as
      it is by default).  (Related to Michael Orlitzky's report
      PR#17836.)

    * str(xS4) now also shows extraneous attributes of an S4 object
      xS4.

    * Rudimentary support for vi-style tags in rtags() and R CMD rtags
      has been added.  (Based on a patch from Neal Fultz in PR#17214.)

    * checkRdContents() is now exported from tools; it and also
      checkDocFiles() have a new option chkInternal allowing to check
      Rd files marked with keyword "internal" as well.  The latter can
      be activated for R CMD check via environment variable
      _R_CHECK_RD_INTERNAL_TOO_.

    * New functions numToBits() and numToInts() extend the raw
      conversion utilities to (double precision) numeric.

    * Functions URLencode() and URLdecode() in package utils now work
      on vectors of URIs.  (Based on patch from Bob Rudis submitted
      with PR#17873.)

    * path.expand() can expand ~user on most Unix-alikes even when
      readline is not in use.  It tries harder to expand ~, for example
      should environment variable HOME be unset.

    * For HTML help (both dynamic and static), Rd file links to help
      pages in external packages are now treated as references to
      topics rather than file names, and fall back to a file link only
      if the topic is not found in the target package. The earlier rule
      which prioritized file names over topics can be restored by
      setting the environment variable _R_HELP_LINKS_TO_TOPICS_ to a
      false value.

    * c() now removes NULL arguments before dispatching to methods,
      thus simplifying the implementation of c() methods, _but_ for
      back compatibility keeps NULL when it is the first argument.
      (From a report and patch proposal by Lionel Henry in PR#17900.)

    * Vectorize()'s result function's environment no longer keeps
      unneeded objects.

    * Function ...elt() now propagates visibility consistently with
      ..n.  (Thanks to Lionel Henry's PR#17905.)

    * capture.output() no longer uses non-standard evaluation to
      evaluate its arguments.  This makes evaluation of functions like
      parent.frame() more consistent.  (Thanks to Lionel Henry's
      PR#17907.)

    * packBits(bits, type="double") now works as inverse of
      numToBits().  (Thanks to Bill Dunlap's proposal in PR#17914.)

    * curlGetHeaders() has two new arguments, timeout to specify the
      timeout for that call (overriding getOption("timeout")) and TLS
      to specify the minimum TLS protocol version to be used for
      https:// URIs (_inter alia_ providing a means to check for sites
      using deprecated TLS versions 1.0 and 1.1).

    * For nls(), an optional constant scaleOffset may be added to the
      denominator of the relative offset convergence test for cases
      where the fit of a model is expected to be exact, thanks to a
      proposal by John Nash.  nls(*, trace=TRUE) now also shows the
      convergence criterion.

    * Numeric differentiation _via_ numericDeriv() gets new optional
      arguments eps and central, the latter for taking central divided
      differences.  The latter can be activated for nls() via
      nls.control(nDcentral = TRUE).

    * nls() now passes the trace and control arguments to getInitial(),
      notably for all self-starting models, so these can also be fit in
      zero-noise situations via a scaleOffset.  For this reason, the
      initial function of a selfStart model must now have ... in its
      argument list.

    * bquote(splice = TRUE) can now splice expression vectors with
      attributes: this makes it possible to splice the result of
      parse(keep.source = TRUE).  (Report and patch provided by Lionel
      Henry in PR#17869.)

    * textConnection() gets an optional name argument.

    * get(), exists(), and get0() now signal an error if the first
      argument has length greater than 1.  Previously additional
      elements were silently ignored.  (Suggested by Antoine Fabri on
      R-devel.)

    * R now provides a shorthand notation for creating functions, e.g.
      \(x) x + 1 is parsed as function(x) x + 1.

    * R now provides a simple native forward pipe syntax |>.  The
      simple form of the forward pipe inserts the left-hand side as the
      first argument in the right-hand side call.  The pipe
      implementation as a syntax transformation was motivated by
      suggestions from Jim Hester and Lionel Henry.

    * all.equal(f, g) for functions now by default also compares their
      environment(.)s, notably via new all.equal method for class
      function.  Comparison of nls() fits, e.g., may now need
      all.equal(m1, m2, check.environment = FALSE).

    * .libPaths() gets a new option include.site, allowing to _not_
      include the site library.  (Thanks to Dario Strbenac's suggestion
      and Gabe Becker's PR#18016.)

    * Lithuanian translations are now available.  (Thanks to Rimantas
      Zakauskas.)

    * names() now works for DOTSXP objects.  On the other hand, in
      R-lang, the R language manual, we now warn against relying on the
      structure or even existence of such dot-dot-dot objects.

    * all.equal() no longer gives an error on DOTSXP objects.

    * capabilities("cairo") now applies only to the file-based devices
      as it is now possible (if very unusual) to build R with Cairo
      support for those but not for X11().

    * There is optional support for tracing the progress of
      loadNamespace() - see its help.

    * (Not Windows.)  l10n_info() reports an additional element, the
      name of the encoding as reported by the OS (which may differ from
      the encoding part (if any) of the result from
      Sys.getlocale("LC_CTYPE").

    * New function gregexec() which generalizes regexec() to find _all_
      disjoint matches and well as all substrings corresponding to
      parenthesized subexpressions of the given regular expression.
      (Contributed by Brodie Gaslam.)

    * New function charClass() in package utils to query the
      wide-character classification functions in use (such as
      iswprint).

    * The names of quantile()'s result no longer depend on the global
      getOption("digits"), but quantile() gets a new optional argument
      digits = 7 instead.

    * grep(), sub(), regexp and variants work considerably faster for
      long factors with few levels.  (Thanks to Michael Chirico's
      PR#18063.)

    * Provide grouping of x11() graphics windows within a window
      manager such as Gnome or Unity; thanks to a patch by Ivan Krylov
      posted to R-devel.

    * The split() method for class data.frame now allows the f argument
      to be specified as a formula.

    * sprintf now warns on arguments unused by the format string.

    * New palettes "Rocket" and "Mako" for hcl.colors() (approximating
      palettes of the same name from the 'viridisLite' package).

      Contributed by Achim Zeileis.

    * The base environment and its namespace are now locked (so one can
      no longer add bindings to these or remove from these).

    * Rterm handling of multi-byte characters has been improved,
      allowing use of such characters when supported by the current
      locale.

    * Rterm now accepts ALT+ +xxxxxxxx sequences to enter Unicode
      characters as hex digits.

    * Environment variable LC_ALL on Windows now takes precedence over
      LC_CTYPE and variables for other supported categories, matching
      the POSIX behaviour.

    * duplicated() and anyDuplicated() are now optimized for integer
      and real vectors that are known to be sorted via the ALTREP
      framework. Contributed by Gabriel Becker via PR#17993.

  GRAPHICS:

    * The graphics engine version, R_GE_version, has been bumped to 14
      and so packages that provide graphics devices should be
      reinstalled.

    * Graphics devices should now specify deviceVersion to indicate
      what version of the graphics engine they support.

    * Graphics devices can now specify deviceClip.  If TRUE, the
      graphics engine will never perform any clipping of output itself.

      The clipping that the graphics engine does perform (for both
      canClip = TRUE and canClip = FALSE) has been improved to avoid
      producing unnecessary artifacts in clipped output.

    * The grid package now allows gpar(fill) to be a linearGradient(),
      a radialGradient(), or a pattern().  The viewport(clip) can now
      also be a grob, which defines a clipping path, and there is a new
      viewport(mask) that can also be a grob, which defines a mask.

      These new features are only supported so far on the Cairo-based
      graphics devices and on the pdf() device.

    * (Not Windows.)  A warning is given when a Cairo-based type is
      specified for a png(), jpeg(), tiff() or bmp() device but Cairo
      is unsupported (so type = "Xlib" is tried instead).

    * grSoftVersion() now reports the versions of FreeType and
      FontConfig if they are used directly (not _via_ Pango), as is
      most commonly done on macOS.

  C-LEVEL FACILITIES:

    * The _standalone_ libRmath math library and R's C API now provide
      log1pexp() again as documented, and gain log1mexp().

  INSTALLATION on a UNIX-ALIKE:

    * configure checks for a program pkgconf if program pkg-config is
      not found.  These are now only looked for on the path (like
      almost all other programs) so if needed specify a full path to
      the command in PKG_CONFIG, for example in file config.site.

    * C99 function iswblank is required - it was last seen missing ca
      2003 so the workaround has been removed.

    * There are new configure options --with-internal-iswxxxxx,
      --with-internal-towlower and --with-internal-wcwidth which allows
      the system functions for wide-character classification,
      case-switching and width (wcwidth and wcswidth) to be replaced by
      internal ones.  The first has long been used on macOS, AIX (and
      Windows) but this enables it to be unselected there and selected
      for other platforms (it is the new default on Solaris).  The
      second is new in this version of R and is selected by default on
      macOS and Solaris.  The third has long been the default and
      remains so as it contains customizations for East Asian
      languages.

      System versions of these functions are often minimally
      implemented (sometimes only for ASCII characters) and may not
      cover the full range of Unicode points: for example Solaris (and
      Windows) only cover the Basic Multilingual Plane.

    * Cairo installations without X11 are more likely to be detected by
      configure, when the file-based Cairo graphics devices will be
      available but not X11(type = "cairo").

    * There is a new configure option --with-static-cairo which is the
      default on macOS.  This should be used when only static cairo
      (and where relevant, Pango) libraries are available.

    * Cairo-based graphics devices on platforms without Pango but with
      FreeType/FontConfig will make use of the latter for font
      selection.

  LINK-TIME OPTIMIZATION on a UNIX-ALIKE:

    * Configuring with flag --enable-lto=R now also uses LTO when
      installing the recommended packages.

    * R CMD INSTALL and R CMD SHLIB have a new flag --use-LTO to use
      LTO when compiling code, for use with R configured with
      --enable-lto=R.  For R configured with --enable-lto, they have
      the new flag --no-use-LTO.

      Packages can opt in or out of LTO compilation _via_ a UseLTO
      field in the DESCRIPTION file.  (As usual this can be overridden
      by the command-line flags.)

  BUILDING R on Windows:

    * for GCC >= 8, FC_LEN_T is defined in config.h and hence character
      lengths are passed from C to Fortran in _inter alia_ BLAS and
      LAPACK calls.

    * There is a new text file src/gnuwin32/README.compilation, which
      outlines how C/Fortran code compilation is organized and
      documents new features:

        * R can be built with Link-Time Optimization with a suitable
          compiler - doing so with GCC 9.2 showed several
          inconsistencies which have been corrected.

        * There is support for cross-compiling the C and Fortran code
          in R and standard packages on suitable (Linux) platforms.
          This is mainly intended to allow developers to test later
          versions of compilers - for example using GCC 9.2 or 10.x has
          detected issues that GCC 8.3 in Rtools40 does not.

        * There is experimental support for cross-building R packages
          with C, C++ and/or Fortran code.

    * The R installer can now be optionally built to support a single
      architecture (only 64-bit or only 32-bit).

  PACKAGE INSTALLATION:

    * The default C++ standard has been changed to C++14 where
      available (which it is on all currently checked platforms): if
      not (as before) C++11 is used if available otherwise C++ is not
      supported.

      Packages which specify C++11 will still be installed using C++11.

      C++14 compilers may give deprecation warnings, most often for
      std::random_shuffle (deprecated in C++14 and removed in C++17).
      Either specify C++11 (see 'Writing R Extensions') or modernize
      the code and if needed specify C++14.  The latter has been
      supported since R 3.4.0 so the package's DESCRIPTION would need
      to include something like

           Depends: R (>= 3.4)
      
  PACKAGE INSTALLATION on Windows:

    * R CMD INSTALL and R CMD SHLIB make use of their flag --use-LTO
      when the LTO_OPT make macro is set in file etc/${R_ARCH}/Makeconf
      or in a personal/site Makevars file.  (For details see 'Writing R
      Extensions' SS4.5.)

      This provides a valuable check on code consistency.  It does work
      with GCC 8.3 as in Rtools40, but that does not detect everything
      the CRAN checks with current GCC do.

  PACKAGE INSTALLATION on macOS:

    * The default personal library directory on builds with
      --enable-aqua (including CRAN builds) now differs by CPU type,
      one of

            ~/Library/R/x86_64/x.y/library
            ~/Library/R/arm64/x.y/library
      
      This uses the CPU type R (and hence the packages) were built for,
      so when a x86_64 build of R is run under Rosetta emulation on an
      arm64 Mac, the first is used.

  UTILITIES:

    * R CMD check can now scan package functions for bogus return
      statements, which were possibly intended as return() calls (wish
      of PR#17180, patch by Sebastian Meyer). This check can be
      activated via the new environment variable
      _R_CHECK_BOGUS_RETURN_, true for --as-cran.

    * R CMD build omits tarballs and binaries of previous builds from
      the top-level package directory.  (PR#17828, patch by Sebastian
      Meyer.)

    * R CMD check now runs sanity checks on the use of LazyData, for
      example that a data directory is present and that
      LazyDataCompression is not specified without LazyData and has a
      documented value.  For packages with large LazyData databases
      without specifying LazyDataCompression, there is a reference to
      the code given in 'Writing R Extensions' SS1.1.6 to test the
      choice of compression (as in all the CRAN packages tested a
      non-default method was preferred).

    * R CMD build removes LazyData and LazyDataCompression fields from
      the DESCRIPTION file of packages without a data directory.

  ENCODING-RELATED CHANGES:

    * The parser now treats \Unnnnnnnn escapes larger than the upper
      limit for Unicode points (\U10FFFF) as an error as they cannot be
      represented by valid UTF-8.

      Where such escapes are used for outputting non-printable
      (including unassigned) characters, 6 hex digits are used (rather
      than 8 with leading zeros).  For clarity, braces are used, for
      example \U{0effff}.

    * The parser now looks for non-ASCII spaces on Solaris (as
      previously on most other OSes).

    * There are warnings (including from the parser) on the use of
      unpaired surrogate Unicode points such as \uD834.  (These cannot
      be converted to valid UTF-8.)

    * Functions nchar(), tolower(), toupper() and chartr() and those
      using regular expressions have more support for inputs with a
      marked Latin-1 encoding.

    * The character-classification functions used (by default) to
      replace the system iswxxxxx functions on Windows, macOS and AIX
      have been updated to Unicode 13.0.0.

      The character-width tables have been updated to include new
      assignments in Unicode 13.0.0.

    * The code for evaluating default (extended) regular expressions
      now uses the same character-classification functions as the rest
      of R (previously they differed on Windows, macOS and AIX).

    * There is a build-time option to replace the system's
      wide-character wctrans C function by tables shipped with R: use
      configure option --with-internal-towlower or (on Windows)
      -DUSE_RI18N_CASE in CFLAGS when building R.  This may be needed
      to allow tolower() and toupper() to work with Unicode characters
      beyond the Basic Multilingual Plane where not supported by system
      functions (e.g. on Solaris where it is the new default).

    * R is more careful when truncating UTF-8 and other multi-byte
      strings that are too long to be printed, passed to the system or
      libraries or placed into an internal buffer.  Truncation will no
      longer produce incomplete multibyte characters.

  DEPRECATED AND DEFUNCT:

    * Function plclust() from the package stats and
      package.dependencies(), pkgDepends(), getDepList(),
      installFoundDepends(), and vignetteDepends() from package tools
      are defunct.

    * Defunct functions checkNEWS() and readNEWS() from package tools
      and CRAN.packages() from utils have been removed.

    * R CMD config CXXCPP is defunct (it was deprecated in R 3.6.2).

    * parallel::detectCores() drops support for Irix (retired in 2013).

    * The LINPACK argument to chol.default(), chol2inv(),
      solve.default() and svd() has been defunct since R 3.1.0.  It was
      silently ignored up to R 4.0.3 but now gives an error.

    * Subsetting/indexing, such as ddd[*] or ddd$x on a DOTSXP
      (dot-dot-dot) object ddd has been disabled; it worked by accident
      only and was undocumented.

  BUG FIXES:

    * Many more C-level allocations (mainly by malloc and strdup) are
      checked for success with suitable alternative actions.

    * Bug fix for replayPlot(); this was turning off graphics engine
      display list recording if a recorded plot was replayed in the
      same session.  The impact of the bug became visible if resize the
      device after replay OR if attempted another savePlot() after
      replay (empty display list means empty screen on resize or empty
      saved plot).

    * R CMD check etc now warn when a package exports non-existing S4
      classes or methods, also in case of no "methods" presence.
      (Reported by Alex Bertram; reproducible example and patch by
      Sebastian Meyer in PR#16662.)

    * boxplot() now also accepts calls for labels such as ylab, the
      same as plot().  (Reported by Marius Hofert.)

    * The help page for xtabs() now correctly states that addNA is
      setting na.action = na.pass among others.  (Reported as PR#17770
      by Thomas Soeiro.)

    * The R CMD check <pkg> gives a longer and more comprehensible
      message when DESCRIPTION misses dependencies, e.g., in Imports:.
      (Thanks to the contributors of PR#17179.)

    * update.default() now calls the generic update() on the formula to
      work correctly for models with extended formulas.  (As reported
      and suggested by Neal Fultz in PR#17865.)

    * The horizontal position of leaves in a dendrogram is now correct
      also with center = FALSE.  (PR#14938, patch from Sebastian
      Meyer.)

    * all.equal.POSIXt() no longer warns about and subsequently ignores
      inconsistent "tzone" attributes, but describes the difference in
      its return value (PR#17277).  This check can be disabled _via_
      the new argument check.tzone = FALSE as suggested by Sebastian
      Meyer.

    * as.POSIXct() now populates the "tzone" attribute from its tz
      argument when x is a logical vector consisting entirely of NA
      values.

    * x[[2^31]] <- v now works.  (Thanks to the report and patch by
      Suharto Anggono in PR#17330.)

    * In log-scale graphics, axis() ticks and label positions are now
      computed more carefully and symmetrically in their range,
      typically providing _more_ ticks, fulfilling wishes in PR#17936.
      The change really corresponds to an improved axisTicks() (package
      grDevices), potentially influencing grid and lattice, for
      example.

    * qnorm(<very large negative>, log.p=TRUE) is now correct to at
      least five digits where it was catastrophically wrong,
      previously.

    * sum(df) and similar "Summary"- and "Math"-group member functions
      now work for data frames df with logical columns, notably also of
      zero rows.  (Reported to R-devel by Martin "b706".)

    * unsplit() had trouble with tibbles due to unsound use of rep(NA,
      len)-indexing, which should use NA_integer_ (Reported to R-devel
      by Mario Annau.)

    * pnorm(x, log.p = TRUE) underflows to -Inf slightly later.

    * show(<hidden S4 generic>) prints better and without quotes for
      non-hidden S4 generics.

    * read.table() and relatives treated an "NA" column name as missing
      when check.names = FALSE PR#18007.

    * Parsing strings containing UTF-16 surrogate pairs such as
      "\uD834\uDD1E" works better on some (uncommon) platforms.
      sprintf("%X", utf8ToInt("\uD834\uDD1E")) should now give "1D11E"
      on all platforms.

    * identical(x,y) is no longer true for differing DOTSXP objects,
      fixing PR#18032.

    * str() now works correctly for DOTSXP and related exotics, even
      when these are doomed.

      Additionally, it no longer fails for lists with a class and
      "irregular" method definitions such that e.g. lapply(*) will
      necessarily fail, as currently for different igraph objects.

    * Too long lines in environment files (e.g. Renviron) no longer
      crash R. This limit has been increased to 100,000 bytes.
      (PR#18001.)

    * There is a further workaround for FreeType giving incorrect
      italic font faces with cairo-based graphics devices on macOS.

    * add_datalist(*, force = TRUE) (from package tools) now actually
      updates an existing data/datalist file for new content.  (Thanks
      to a report and patch by Sebastian Meyer in PR#18048.)

    * cut.Date() and cut.POSIXt() could produce an empty last interval
      for breaks = "months" or breaks = "years".  (Reported as PR#18053
      by Christopher Carbone.)

    * Detection of the encoding of 'regular' macOS locales such as
      en_US (which is UTF-8) had been broken by a macOS change:
      fortunately these are now rarely used with en_US.UTF-8 being
      preferred.

    * sub() and gsub(pattern, repl, x, *) now keep attributes of x such
      as names() also when pattern is NA (PR#18079).

    * Time differences ("difftime" objects) get a replacement and a
      rep() method to keep "units" consistent.  (Thanks to a report and
      patch by Nicolas Bennett in PR#18066.)

    * The \RdOpts macro, setting defaults for \Sexpr options in an Rd
      file, had been ineffective since R 2.12.0: it now works again.
      (Thanks to a report and patch by Sebastian Meyer in PR#18073.)

    * mclapply and pvec no longer accidentally terminate parallel
      processes started before by mcparallel or related calls in
      package parallel (PR#18078).

    * grep and other functions for evaluating (extended) regular
      expressions handle in Unicode also strings not explicitly flagged
      UTF-8, but flagged native when running in UTF-8 locale.

    * Fixed a crash in fifo implementation on Windows (PR#18031).

    * Binary mode in fifo on Windows is now properly detected from
      argument open (PR#15600, PR#18031).

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com

______________________________________________
R-devel at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-devel


From rpru|m @end|ng |rom c@|v|n@edu  Thu May 20 13:01:01 2021
From: rpru|m @end|ng |rom c@|v|n@edu (Randall Pruim)
Date: Thu, 20 May 2021 11:01:01 +0000
Subject: [Rd] {splines} package gone missing?
In-Reply-To: <2D9CEF37-DD35-4D5E-9D42-16143E0A3C63@gmail.com>
References: <5A066EFD-798C-41F3-9271-A53DF8F1CBD2@calvin.edu>
 <2D9CEF37-DD35-4D5E-9D42-16143E0A3C63@gmail.com>
Message-ID: <43A9CDF6-9311-4997-B3B6-FD7CB6F9BCE2@calvin.edu>

Thanks.  I actually sort of checked for this:


row.names(installed.packages(priority = "base"))
 [1] "base"      "compiler"  "datasets"  "graphics"  "grDevices" "grid"      "methods"   "parallel"  "stats"     "stats4"    "tcltk"
[12] "tools"     "utils"

But, of course, if the package is missing on my system (a newly installed 4.1 on an RStudio server), then it won?t be listed here.

I?ll have to figure out what went wrong with the install.  I?ll probably start by having our sysadmin simply reinstall R 4.1 and hope that that takes care of the problem.

Looks like profile is missing as well.

?rjp

On May 20, 2021, at 3:14 AM, peter dalgaard <pdalgd at gmail.com<mailto:pdalgd at gmail.com>> wrote:

It is part of base R, so comes with the R sources:

Peters-MacBook-Air:R pd$ ls src/library/
Makefile.in   compiler/     grid/         splines/      tools/
Makefile.win<http://Makefile.win>  datasets/     methods/      stats/        translations/
Recommended/  grDevices/    parallel/     stats4/       utils/
base/         graphics/     profile/      tcltk/

- pd

On 20 May 2021, at 06:02 , Randall Pruim <rpruim at calvin.edu<mailto:rpruim at calvin.edu>> wrote:


https://urldefense.proofpoint.com/v2/url?u=https-3A__cran.r-2Dproject.org_web_packages_splines_index.html&d=DwIFaQ&c=4rZ6NPIETe-LE5i2KBR4rw&r=S6U-baLhvGcJ7iUQX_KZ6K2om1TTOeUI_-mjRpTrm00&m=GW4Zim0TvggrmjKt0HtW7il0NwJJaworM4-LL3H2FyI&s=69EYlZJd6di726T20ILKnX7f4GWM-VaUplRjNXfGolQ&e=  claims the the {splines} package has been archived.  If I follow the link there to the archives, the newest version shown is from 1999.  It seems like something has gone wrong with this package.

I checked on another mirror and {splines} is missing there as well.

?rjp

______________________________________________
R-devel at r-project.org<mailto:R-devel at r-project.org> mailing list
https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Ddevel&d=DwIFaQ&c=4rZ6NPIETe-LE5i2KBR4rw&r=S6U-baLhvGcJ7iUQX_KZ6K2om1TTOeUI_-mjRpTrm00&m=GW4Zim0TvggrmjKt0HtW7il0NwJJaworM4-LL3H2FyI&s=Jen0Ht23Vg-znRGxp8YPRc-qCbG0uLYGAyyECxU6kFg&e=

--
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk<mailto:pd.mes at cbs.dk>  Priv: PDalgd at gmail.com<mailto:PDalgd at gmail.com>



	[[alternative HTML version deleted]]


From bbo|ker @end|ng |rom gm@||@com  Thu May 20 13:12:58 2021
From: bbo|ker @end|ng |rom gm@||@com (Ben Bolker)
Date: Thu, 20 May 2021 07:12:58 -0400
Subject: [Rd] {splines} package gone missing?
In-Reply-To: <43A9CDF6-9311-4997-B3B6-FD7CB6F9BCE2@calvin.edu>
References: <5A066EFD-798C-41F3-9271-A53DF8F1CBD2@calvin.edu>
 <2D9CEF37-DD35-4D5E-9D42-16143E0A3C63@gmail.com>
 <43A9CDF6-9311-4997-B3B6-FD7CB6F9BCE2@calvin.edu>
Message-ID: <CABghstRW_f-FKX5FWV78nc73x88i1efgFkgKQc+Vw16RuNrb4w@mail.gmail.com>

splines is 'recommended' ( not sure about capitalization), not "base'

On Thu, May 20, 2021, 7:02 AM Randall Pruim <rpruim at calvin.edu> wrote:

> Thanks.  I actually sort of checked for this:
>
>
> row.names(installed.packages(priority = "base"))
>  [1] "base"      "compiler"  "datasets"  "graphics"  "grDevices" "grid"
>   "methods"   "parallel"  "stats"     "stats4"    "tcltk"
> [12] "tools"     "utils"
>
> But, of course, if the package is missing on my system (a newly installed
> 4.1 on an RStudio server), then it won?t be listed here.
>
> I?ll have to figure out what went wrong with the install.  I?ll probably
> start by having our sysadmin simply reinstall R 4.1 and hope that that
> takes care of the problem.
>
> Looks like profile is missing as well.
>
> ?rjp
>
> On May 20, 2021, at 3:14 AM, peter dalgaard <pdalgd at gmail.com<mailto:
> pdalgd at gmail.com>> wrote:
>
> It is part of base R, so comes with the R sources:
>
> Peters-MacBook-Air:R pd$ ls src/library/
> Makefile.in   compiler/     grid/         splines/      tools/
> Makefile.win<http://Makefile.win>  datasets/     methods/      stats/
>     translations/
> Recommended/  grDevices/    parallel/     stats4/       utils/
> base/         graphics/     profile/      tcltk/
>
> - pd
>
> On 20 May 2021, at 06:02 , Randall Pruim <rpruim at calvin.edu<mailto:
> rpruim at calvin.edu>> wrote:
>
>
>
> https://urldefense.proofpoint.com/v2/url?u=https-3A__cran.r-2Dproject.org_web_packages_splines_index.html&d=DwIFaQ&c=4rZ6NPIETe-LE5i2KBR4rw&r=S6U-baLhvGcJ7iUQX_KZ6K2om1TTOeUI_-mjRpTrm00&m=GW4Zim0TvggrmjKt0HtW7il0NwJJaworM4-LL3H2FyI&s=69EYlZJd6di726T20ILKnX7f4GWM-VaUplRjNXfGolQ&e=
> claims the the {splines} package has been archived.  If I follow the link
> there to the archives, the newest version shown is from 1999.  It seems
> like something has gone wrong with this package.
>
> I checked on another mirror and {splines} is missing there as well.
>
> ?rjp
>
> ______________________________________________
> R-devel at r-project.org<mailto:R-devel at r-project.org> mailing list
>
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Ddevel&d=DwIFaQ&c=4rZ6NPIETe-LE5i2KBR4rw&r=S6U-baLhvGcJ7iUQX_KZ6K2om1TTOeUI_-mjRpTrm00&m=GW4Zim0TvggrmjKt0HtW7il0NwJJaworM4-LL3H2FyI&s=Jen0Ht23Vg-znRGxp8YPRc-qCbG0uLYGAyyECxU6kFg&e=
>
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk<mailto:pd.mes at cbs.dk>  Priv: PDalgd at gmail.com<mailto:
> PDalgd at gmail.com>
>
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From pd@|gd @end|ng |rom gm@||@com  Thu May 20 13:22:31 2021
From: pd@|gd @end|ng |rom gm@||@com (peter dalgaard)
Date: Thu, 20 May 2021 13:22:31 +0200
Subject: [Rd] {splines} package gone missing?
In-Reply-To: <CABghstRW_f-FKX5FWV78nc73x88i1efgFkgKQc+Vw16RuNrb4w@mail.gmail.com>
References: <5A066EFD-798C-41F3-9271-A53DF8F1CBD2@calvin.edu>
 <2D9CEF37-DD35-4D5E-9D42-16143E0A3C63@gmail.com>
 <43A9CDF6-9311-4997-B3B6-FD7CB6F9BCE2@calvin.edu>
 <CABghstRW_f-FKX5FWV78nc73x88i1efgFkgKQc+Vw16RuNrb4w@mail.gmail.com>
Message-ID: <7332685F-3B37-4F5F-88AD-A10F0840F756@gmail.com>

No, it really is base R, not Recommended:

Peters-MacBook-Air:BUILD pd$ ls ../R/src/library/Recommended/
KernSmooth.tgz			foreign.tgz
KernSmooth_2.23-20.tar.gz	foreign_0.8-81.tar.gz
MASS.tgz			lattice.tgz
MASS_7.3-54.tar.gz		lattice_0.20-44.tar.gz
Makefile.in			mgcv.tgz
Makefile.win			mgcv_1.8-35.tar.gz
Matrix.tgz			nlme.tgz
Matrix_1.3-3.tar.gz		nlme_3.1-152.tar.gz
boot.tgz			nnet.tgz
boot_1.3-28.tar.gz		nnet_7.3-16.tar.gz
class.tgz			rpart.tgz
class_7.3-19.tar.gz		rpart_4.1-15.tar.gz
cluster.tgz			spatial.tgz
cluster_2.1.2.tar.gz		spatial_7.3-14.tar.gz
codetools.tgz			survival.tgz
codetools_0.2-18.tar.gz		survival_3.2-11.tar.gz
Peters-MacBook-Air:BUILD pd$ ls ../R/src/library/
Makefile.in	compiler	grid		splines		tools
Makefile.win	datasets	methods		stats		translations
Recommended	grDevices	parallel	stats4		utils
base		graphics	profile		tcltk
> row.names(installed.packages(priority = "base")) 
 [1] "base"      "compiler"  "datasets"  "graphics"  "grDevices" "grid"     
 [7] "methods"   "parallel"  "splines"   "stats"     "stats4"    "tcltk"    
[13] "tools"     "utils"    

'profile' is not an actual package, though.

- Peter


> On 20 May 2021, at 13:12 , Ben Bolker <bbolker at gmail.com> wrote:
> 
> splines is 'recommended' ( not sure about capitalization), not "base'
> 
> On Thu, May 20, 2021, 7:02 AM Randall Pruim <rpruim at calvin.edu> wrote:
> Thanks.  I actually sort of checked for this:
> 
> 
> row.names(installed.packages(priority = "base"))
>  [1] "base"      "compiler"  "datasets"  "graphics"  "grDevices" "grid"      "methods"   "parallel"  "stats"     "stats4"    "tcltk"
> [12] "tools"     "utils"
> 
> But, of course, if the package is missing on my system (a newly installed 4.1 on an RStudio server), then it won?t be listed here.
> 
> I?ll have to figure out what went wrong with the install.  I?ll probably start by having our sysadmin simply reinstall R 4.1 and hope that that takes care of the problem.
> 
> Looks like profile is missing as well.
> 
> ?rjp
> 
> On May 20, 2021, at 3:14 AM, peter dalgaard <pdalgd at gmail.com<mailto:pdalgd at gmail.com>> wrote:
> 
> It is part of base R, so comes with the R sources:
> 
> Peters-MacBook-Air:R pd$ ls src/library/
> Makefile.in   compiler/     grid/         splines/      tools/
> Makefile.win<http://Makefile.win>  datasets/     methods/      stats/        translations/
> Recommended/  grDevices/    parallel/     stats4/       utils/
> base/         graphics/     profile/      tcltk/
> 
> - pd
> 
> On 20 May 2021, at 06:02 , Randall Pruim <rpruim at calvin.edu<mailto:rpruim at calvin.edu>> wrote:
> 
> 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__cran.r-2Dproject.org_web_packages_splines_index.html&d=DwIFaQ&c=4rZ6NPIETe-LE5i2KBR4rw&r=S6U-baLhvGcJ7iUQX_KZ6K2om1TTOeUI_-mjRpTrm00&m=GW4Zim0TvggrmjKt0HtW7il0NwJJaworM4-LL3H2FyI&s=69EYlZJd6di726T20ILKnX7f4GWM-VaUplRjNXfGolQ&e=  claims the the {splines} package has been archived.  If I follow the link there to the archives, the newest version shown is from 1999.  It seems like something has gone wrong with this package.
> 
> I checked on another mirror and {splines} is missing there as well.
> 
> ?rjp
> 
> ______________________________________________
> R-devel at r-project.org<mailto:R-devel at r-project.org> mailing list
> https://urldefense.proofpoint.com/v2/url?u=https-3A__stat.ethz.ch_mailman_listinfo_r-2Ddevel&d=DwIFaQ&c=4rZ6NPIETe-LE5i2KBR4rw&r=S6U-baLhvGcJ7iUQX_KZ6K2om1TTOeUI_-mjRpTrm00&m=GW4Zim0TvggrmjKt0HtW7il0NwJJaworM4-LL3H2FyI&s=Jen0Ht23Vg-znRGxp8YPRc-qCbG0uLYGAyyECxU6kFg&e=
> 
> --
> Peter Dalgaard, Professor,
> Center for Statistics, Copenhagen Business School
> Solbjerg Plads 3, 2000 Frederiksberg, Denmark
> Phone: (+45)38153501
> Office: A 4.23
> Email: pd.mes at cbs.dk<mailto:pd.mes at cbs.dk>  Priv: PDalgd at gmail.com<mailto:PDalgd at gmail.com>
> 
> 
> 
>         [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Office: A 4.23
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From m@rc_@chw@rtz @end|ng |rom me@com  Fri May 21 17:01:16 2021
From: m@rc_@chw@rtz @end|ng |rom me@com (Marc Schwartz)
Date: Fri, 21 May 2021 11:01:16 -0400
Subject: [Rd] Status of "**" operator
Message-ID: <086d30f6-9080-cb15-cc48-a45049475ac7@me.com>

Hi All,

I was just sent some older R code from circa 2004, which contains the 
use of the "**" operator, which is parsed as "^".

 From looking at ?"**", I see the following in the Note section:

"** is translated in the parser to ^, but this was undocumented for many 
years. It appears as an index entry in Becker et al (1988), pointing to 
the help for Deprecated but is not actually mentioned on that page. Even 
though it had been deprecated in S for 20 years, it was still accepted 
in R in 2008."


In using R 4.1.0:

 > 2**3
[1] 8

the operator is still accepted in 2021.

Thus, has there been any discussion regarding the deprecation of this 
operator, or should the help file at least be updated to reflect the 
status in 2021?

Thanks,

Marc Schwartz


From edd @end|ng |rom deb|@n@org  Fri May 21 19:07:28 2021
From: edd @end|ng |rom deb|@n@org (Dirk Eddelbuettel)
Date: Fri, 21 May 2021 12:07:28 -0500
Subject: [Rd] {splines} package gone missing?
In-Reply-To: <CABghstRW_f-FKX5FWV78nc73x88i1efgFkgKQc+Vw16RuNrb4w@mail.gmail.com>
References: <5A066EFD-798C-41F3-9271-A53DF8F1CBD2@calvin.edu>
 <2D9CEF37-DD35-4D5E-9D42-16143E0A3C63@gmail.com>
 <43A9CDF6-9311-4997-B3B6-FD7CB6F9BCE2@calvin.edu>
 <CABghstRW_f-FKX5FWV78nc73x88i1efgFkgKQc+Vw16RuNrb4w@mail.gmail.com>
Message-ID: <24743.59600.555668.338808@rob.eddelbuettel.com>


Randall,

On 20 May 2021 at 07:12, Ben Bolker wrote:
| splines is 'recommended' ( not sure about capitalization), not "base'

If you install 'r-base' rather than just the narrower 'r-base-core' you
also get 'r-recommended'. So please do that, or just do

   sudo apt install r-recommended

The granularity offered is thought of as a feature even if it stares at you
right now with very humongous insect eyes...

Dirk

-- 
https://dirk.eddelbuettel.com | @eddelbuettel | edd at debian.org


From rpru|m @end|ng |rom c@|v|n@edu  Fri May 21 19:39:58 2021
From: rpru|m @end|ng |rom c@|v|n@edu (Randall Pruim)
Date: Fri, 21 May 2021 17:39:58 +0000
Subject: [Rd] {splines} package gone missing?
In-Reply-To: <24743.59600.555668.338808@rob.eddelbuettel.com>
References: <5A066EFD-798C-41F3-9271-A53DF8F1CBD2@calvin.edu>
 <2D9CEF37-DD35-4D5E-9D42-16143E0A3C63@gmail.com>
 <43A9CDF6-9311-4997-B3B6-FD7CB6F9BCE2@calvin.edu>
 <CABghstRW_f-FKX5FWV78nc73x88i1efgFkgKQc+Vw16RuNrb4w@mail.gmail.com>
 <24743.59600.555668.338808@rob.eddelbuettel.com>
Message-ID: <8090BEAA-2633-4881-8322-2FF343E6AA2F@calvin.edu>

Thanks everyone.

We are already back up and running.  We just did a redo of the install and started from scratch.  Actually, not quite from scratch since the packages in /usr/local that I had updated were not harmed.

I?m guessing this was all a side effect of an attempt to clean up from someone previously having installed contributed packages in amongst the base packages.  I?m guess splines got caught in the dragnet when cleaning up the mess.

?rjp


> On May 21, 2021, at 1:07 PM, Dirk Eddelbuettel <edd at debian.org> wrote:
> 
> 
> Randall,
> 
> On 20 May 2021 at 07:12, Ben Bolker wrote:
> | splines is 'recommended' ( not sure about capitalization), not "base'
> 
> If you install 'r-base' rather than just the narrower 'r-base-core' you
> also get 'r-recommended'. So please do that, or just do
> 
>   sudo apt install r-recommended
> 
> The granularity offered is thought of as a feature even if it stares at you
> right now with very humongous insect eyes...
> 
> Dirk
> 
> -- 
> https://urldefense.proofpoint.com/v2/url?u=https-3A__dirk.eddelbuettel.com&d=DwICAg&c=4rZ6NPIETe-LE5i2KBR4rw&r=S6U-baLhvGcJ7iUQX_KZ6K2om1TTOeUI_-mjRpTrm00&m=l1obx7tzJNQb2j6J4_wfeDktT--xuNcUP2IrIfMN5go&s=PcRNlKxDRkHI7d0Wt3SK1T5PjhYO3YJMXiy0MsXJvXE&e=  | @eddelbuettel | edd at debian.org


From r|p|ey @end|ng |rom @t@t@@ox@@c@uk  Sat May 22 10:42:32 2021
From: r|p|ey @end|ng |rom @t@t@@ox@@c@uk (Prof Brian Ripley)
Date: Sat, 22 May 2021 09:42:32 +0100
Subject: [Rd] Status of "**" operator
In-Reply-To: <086d30f6-9080-cb15-cc48-a45049475ac7@me.com>
References: <086d30f6-9080-cb15-cc48-a45049475ac7@me.com>
Message-ID: <f9c32c23-4291-1651-de1a-0edfc1510e3c@stats.ox.ac.uk>

On 21/05/2021 16:01, Marc Schwartz via R-devel wrote:
> Hi All,
> 
> I was just sent some older R code from circa 2004, which contains the 
> use of the "**" operator, which is parsed as "^".
> 
>  From looking at ?"**", I see the following in the Note section:
> 
> "** is translated in the parser to ^, but this was undocumented for many 
> years. It appears as an index entry in Becker et al (1988), pointing to 
> the help for Deprecated but is not actually mentioned on that page. Even 
> though it had been deprecated in S for 20 years, it was still accepted 
> in R in 2008."
> 
> 
> In using R 4.1.0:
> 
>  > 2**3
> [1] 8
> 
> the operator is still accepted in 2021.
> 
> Thus, has there been any discussion regarding the deprecation of this 
> operator, or should the help file at least be updated to reflect the 
> status in 2021?

Not really: there is no guarantee that it will be accepted for all of 
2021.  As it is deprecated and AFAIK untested, it could be broken at any 
time.  OTOH, no one has mentioned in my hearing a wish to actually 
remove it.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Emeritus Professor of Applied Statistics, University of Oxford


From du@@@@dr|@n @end|ng |rom un|buc@ro  Sun May 23 09:56:07 2021
From: du@@@@dr|@n @end|ng |rom un|buc@ro (=?UTF-8?B?QWRyaWFuIER1yJlh?=)
Date: Sun, 23 May 2021 10:56:07 +0300
Subject: [Rd] 1954 from NA
Message-ID: <CAJ=0CtDvHor+DZsmagY07CEjayGGTy+srb8aZqutLFLrTamfJg@mail.gmail.com>

Dear R devs,

I am probably missing something obvious, but still trying to understand why
the 1954 from the definition of an NA has to fill 32 bits when it normally
doesn't need more than 16.

Wouldn't the code below achieve exactly the same thing?

typedef union
{
    double value;
    unsigned short word[4];
} ieee_double;


#ifdef WORDS_BIGENDIAN
static CONST int hw = 0;
static CONST int lw = 3;
#else  /* !WORDS_BIGENDIAN */
static CONST int hw = 3;
static CONST int lw = 0;
#endif /* WORDS_BIGENDIAN */


static double R_ValueOfNA(void)
{
    volatile ieee_double x;
    x.word[hw] = 0x7ff0;
    x.word[lw] = 1954;
    return x.value;
}

This question has to do with the tagged NA values from package haven, on
which I want to improve. Every available bit counts, especially if
multi-byte characters are going to be involved.

Best wishes,
-- 
Adrian Dusa
University of Bucharest
Romanian Social Data Archive
Soseaua Panduri nr. 90-92
050663 Bucharest sector 5
Romania
https://adriandusa.eu

	[[alternative HTML version deleted]]


From brod|e@g@@|@m @end|ng |rom y@hoo@com  Sun May 23 15:20:23 2021
From: brod|e@g@@|@m @end|ng |rom y@hoo@com (brodie gaslam)
Date: Sun, 23 May 2021 13:20:23 +0000 (UTC)
Subject: [Rd] 1954 from NA
In-Reply-To: <CAJ=0CtDvHor+DZsmagY07CEjayGGTy+srb8aZqutLFLrTamfJg@mail.gmail.com>
References: <CAJ=0CtDvHor+DZsmagY07CEjayGGTy+srb8aZqutLFLrTamfJg@mail.gmail.com>
Message-ID: <1630288902.772422.1621776023778@mail.yahoo.com>

This is because the NA in question is NA_real_, which
is encoded in double precision IEEE-754, which uses
64 bits.? The "1954" is just part of the NA.? The NA
must also conform to the NaN encoding for double precision
numbers, which requires that the "beginning" portion of
the number be "0x7ff0" (well, I think it should be "0x7ff8"
but that's a different story), as you can see here:

? ? x.word[hw] = 0x7ff0;
? ? x.word[lw] = 1954;

Both those components are part of the same double precision
value.? They are just accessed this way to make it easy to
set the high bits (63-32) and the low bits (31-0).

So NA is not just 1954, its 0x7ff0 0000 & 1954 (note I'm
mixing hex and decimals here).

In IEEE 754 double precision encoding numbers that start
with 0x7ff are all NaNs.? The rest of the number except for
the first bit which designates "quiet" vs "signaling" NaNs can
be anything.? R has taken advantage of that to designate the
R NA by setting the lower bits to be 1954.

Note I'm being pretty loose about endianess, etc. here, but
hopefully this conveys the problem.

In terms of your proposal, I'm not entirely sure what you gain.
You're still attempting to generate a 64 bit representation
in the end.? If all you need is to encode the fact that there
was an NA, and restore it later as a 64 bit NA, then you can do
whatever you want so long as the end result conforms to the
expected encoding.

In terms of using 'short' here (which again, I don't see the
need for as you're using it to generate the final 64 bit encoding),
I see two possible problems.? You're adding the dependency that
short will be 16 bits.? We already have the (implicit) assumption
in R that double is 64 bits, and explicit that int is 32 bits.
But I think you'd be going a bit on a limb assuming that short
is 16 bits (not sure).? More important, if short is indeed 16 bits,
I think in:

??? x.word[hw] = 0x7ff0;

You overflow short.

Best,

B.



On Sunday, May 23, 2021, 8:56:18 AM EDT, Adrian Du?a <dusa.adrian at unibuc.ro> wrote: 





Dear R devs,

I am probably missing something obvious, but still trying to understand why
the 1954 from the definition of an NA has to fill 32 bits when it normally
doesn't need more than 16.

Wouldn't the code below achieve exactly the same thing?

typedef union
{
? ? double value;
? ? unsigned short word[4];
} ieee_double;


#ifdef WORDS_BIGENDIAN
static CONST int hw = 0;
static CONST int lw = 3;
#else? /* !WORDS_BIGENDIAN */
static CONST int hw = 3;
static CONST int lw = 0;
#endif /* WORDS_BIGENDIAN */


static double R_ValueOfNA(void)
{
? ? volatile ieee_double x;
? ? x.word[hw] = 0x7ff0;
? ? x.word[lw] = 1954;
? ? return x.value;
}

This question has to do with the tagged NA values from package haven, on
which I want to improve. Every available bit counts, especially if
multi-byte characters are going to be involved.

Best wishes,
-- 
Adrian Dusa
University of Bucharest
Romanian Social Data Archive
Soseaua Panduri nr. 90-92
050663 Bucharest sector 5
Romania
https://adriandusa.eu

??? [[alternative HTML version deleted]]

______________________________________________
R-devel at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-devel


From brod|e@g@@|@m @end|ng |rom y@hoo@com  Sun May 23 15:30:20 2021
From: brod|e@g@@|@m @end|ng |rom y@hoo@com (brodie gaslam)
Date: Sun, 23 May 2021 13:30:20 +0000 (UTC)
Subject: [Rd] 1954 from NA
In-Reply-To: <1630288902.772422.1621776023778@mail.yahoo.com>
References: <CAJ=0CtDvHor+DZsmagY07CEjayGGTy+srb8aZqutLFLrTamfJg@mail.gmail.com>
 <1630288902.772422.1621776023778@mail.yahoo.com>
Message-ID: <1847408368.765062.1621776620165@mail.yahoo.com>

I should add, I don't know that you can rely on this
particular encoding of R's NA.? If I were trying to restore
an NA from some external format, I would just generate an
R NA via e.g NA_real_ in the R session I'm restoring the 
external data into, and not try to hand assemble one.

Best,

B.


On Sunday, May 23, 2021, 9:23:54 AM EDT, brodie gaslam via R-devel <r-devel at r-project.org> wrote: 





This is because the NA in question is NA_real_, which
is encoded in double precision IEEE-754, which uses
64 bits.? The "1954" is just part of the NA.? The NA
must also conform to the NaN encoding for double precision
numbers, which requires that the "beginning" portion of
the number be "0x7ff0" (well, I think it should be "0x7ff8"
but that's a different story), as you can see here:

? ? x.word[hw] = 0x7ff0;
? ? x.word[lw] = 1954;

Both those components are part of the same double precision
value.? They are just accessed this way to make it easy to
set the high bits (63-32) and the low bits (31-0).

So NA is not just 1954, its 0x7ff0 0000 & 1954 (note I'm
mixing hex and decimals here).

In IEEE 754 double precision encoding numbers that start
with 0x7ff are all NaNs.? The rest of the number except for
the first bit which designates "quiet" vs "signaling" NaNs can
be anything.? R has taken advantage of that to designate the
R NA by setting the lower bits to be 1954.

Note I'm being pretty loose about endianess, etc. here, but
hopefully this conveys the problem.

In terms of your proposal, I'm not entirely sure what you gain.
You're still attempting to generate a 64 bit representation
in the end.? If all you need is to encode the fact that there
was an NA, and restore it later as a 64 bit NA, then you can do
whatever you want so long as the end result conforms to the
expected encoding.

In terms of using 'short' here (which again, I don't see the
need for as you're using it to generate the final 64 bit encoding),
I see two possible problems.? You're adding the dependency that
short will be 16 bits.? We already have the (implicit) assumption
in R that double is 64 bits, and explicit that int is 32 bits.
But I think you'd be going a bit on a limb assuming that short
is 16 bits (not sure).? More important, if short is indeed 16 bits,
I think in:

??? x.word[hw] = 0x7ff0;

You overflow short.

Best,

B.



On Sunday, May 23, 2021, 8:56:18 AM EDT, Adrian Du?a <dusa.adrian at unibuc.ro> wrote: 





Dear R devs,

I am probably missing something obvious, but still trying to understand why
the 1954 from the definition of an NA has to fill 32 bits when it normally
doesn't need more than 16.

Wouldn't the code below achieve exactly the same thing?

typedef union
{
? ? double value;
? ? unsigned short word[4];
} ieee_double;


#ifdef WORDS_BIGENDIAN
static CONST int hw = 0;
static CONST int lw = 3;
#else? /* !WORDS_BIGENDIAN */
static CONST int hw = 3;
static CONST int lw = 0;
#endif /* WORDS_BIGENDIAN */


static double R_ValueOfNA(void)
{
? ? volatile ieee_double x;
? ? x.word[hw] = 0x7ff0;
? ? x.word[lw] = 1954;
? ? return x.value;
}

This question has to do with the tagged NA values from package haven, on
which I want to improve. Every available bit counts, especially if
multi-byte characters are going to be involved.

Best wishes,
-- 
Adrian Dusa
University of Bucharest
Romanian Social Data Archive
Soseaua Panduri nr. 90-92
050663 Bucharest sector 5
Romania
https://adriandusa.eu

??? [[alternative HTML version deleted]]

______________________________________________
R-devel at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-devel


______________________________________________
R-devel at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-devel


From m@rk@v@nder|oo @end|ng |rom gm@||@com  Sun May 23 16:31:43 2021
From: m@rk@v@nder|oo @end|ng |rom gm@||@com (Mark van der Loo)
Date: Sun, 23 May 2021 16:31:43 +0200
Subject: [Rd] 1954 from NA
In-Reply-To: <1847408368.765062.1621776620165@mail.yahoo.com>
References: <CAJ=0CtDvHor+DZsmagY07CEjayGGTy+srb8aZqutLFLrTamfJg@mail.gmail.com>
 <1630288902.772422.1621776023778@mail.yahoo.com>
 <1847408368.765062.1621776620165@mail.yahoo.com>
Message-ID: <CAOKDuOgVFUT87hex9tGxwAJJ+8JhvHceHjAOMyQBmXeYhnUKog@mail.gmail.com>

I wrote about this once over here:
http://www.markvanderloo.eu/yaRb/2012/07/08/representation-of-numerical-nas-in-r-and-the-1954-enigma/

-M



Op zo 23 mei 2021 15:33 schreef brodie gaslam via R-devel <
r-devel at r-project.org>:

> I should add, I don't know that you can rely on this
> particular encoding of R's NA.  If I were trying to restore
> an NA from some external format, I would just generate an
> R NA via e.g NA_real_ in the R session I'm restoring the
> external data into, and not try to hand assemble one.
>
> Best,
>
> B.
>
>
> On Sunday, May 23, 2021, 9:23:54 AM EDT, brodie gaslam via R-devel <
> r-devel at r-project.org> wrote:
>
>
>
>
>
> This is because the NA in question is NA_real_, which
> is encoded in double precision IEEE-754, which uses
> 64 bits.  The "1954" is just part of the NA.  The NA
> must also conform to the NaN encoding for double precision
> numbers, which requires that the "beginning" portion of
> the number be "0x7ff0" (well, I think it should be "0x7ff8"
> but that's a different story), as you can see here:
>
>     x.word[hw] = 0x7ff0;
>     x.word[lw] = 1954;
>
> Both those components are part of the same double precision
> value.  They are just accessed this way to make it easy to
> set the high bits (63-32) and the low bits (31-0).
>
> So NA is not just 1954, its 0x7ff0 0000 & 1954 (note I'm
> mixing hex and decimals here).
>
> In IEEE 754 double precision encoding numbers that start
> with 0x7ff are all NaNs.  The rest of the number except for
> the first bit which designates "quiet" vs "signaling" NaNs can
> be anything.  R has taken advantage of that to designate the
> R NA by setting the lower bits to be 1954.
>
> Note I'm being pretty loose about endianess, etc. here, but
> hopefully this conveys the problem.
>
> In terms of your proposal, I'm not entirely sure what you gain.
> You're still attempting to generate a 64 bit representation
> in the end.  If all you need is to encode the fact that there
> was an NA, and restore it later as a 64 bit NA, then you can do
> whatever you want so long as the end result conforms to the
> expected encoding.
>
> In terms of using 'short' here (which again, I don't see the
> need for as you're using it to generate the final 64 bit encoding),
> I see two possible problems.  You're adding the dependency that
> short will be 16 bits.  We already have the (implicit) assumption
> in R that double is 64 bits, and explicit that int is 32 bits.
> But I think you'd be going a bit on a limb assuming that short
> is 16 bits (not sure).  More important, if short is indeed 16 bits,
> I think in:
>
>     x.word[hw] = 0x7ff0;
>
> You overflow short.
>
> Best,
>
> B.
>
>
>
> On Sunday, May 23, 2021, 8:56:18 AM EDT, Adrian Du?a <
> dusa.adrian at unibuc.ro> wrote:
>
>
>
>
>
> Dear R devs,
>
> I am probably missing something obvious, but still trying to understand why
> the 1954 from the definition of an NA has to fill 32 bits when it normally
> doesn't need more than 16.
>
> Wouldn't the code below achieve exactly the same thing?
>
> typedef union
> {
>     double value;
>     unsigned short word[4];
> } ieee_double;
>
>
> #ifdef WORDS_BIGENDIAN
> static CONST int hw = 0;
> static CONST int lw = 3;
> #else  /* !WORDS_BIGENDIAN */
> static CONST int hw = 3;
> static CONST int lw = 0;
> #endif /* WORDS_BIGENDIAN */
>
>
> static double R_ValueOfNA(void)
> {
>     volatile ieee_double x;
>     x.word[hw] = 0x7ff0;
>     x.word[lw] = 1954;
>     return x.value;
> }
>
> This question has to do with the tagged NA values from package haven, on
> which I want to improve. Every available bit counts, especially if
> multi-byte characters are going to be involved.
>
> Best wishes,
> --
> Adrian Dusa
> University of Bucharest
> Romanian Social Data Archive
> Soseaua Panduri nr. 90-92
> 050663 Bucharest sector 5
> Romania
> https://adriandusa.eu
>
>     [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From du@@@@dr|@n @end|ng |rom gm@||@com  Sun May 23 16:45:10 2021
From: du@@@@dr|@n @end|ng |rom gm@||@com (=?UTF-8?B?QWRyaWFuIER1yJlh?=)
Date: Sun, 23 May 2021 17:45:10 +0300
Subject: [Rd] 1954 from NA
In-Reply-To: <1847408368.765062.1621776620165@mail.yahoo.com>
References: <CAJ=0CtDvHor+DZsmagY07CEjayGGTy+srb8aZqutLFLrTamfJg@mail.gmail.com>
 <1630288902.772422.1621776023778@mail.yahoo.com>
 <1847408368.765062.1621776620165@mail.yahoo.com>
Message-ID: <CAJ=0CtCoOdijsw2ZPw4TQz2aCrSn+rXKAE3LL_e2FxjO2uO62A@mail.gmail.com>

On Sun, May 23, 2021 at 4:33 PM brodie gaslam via R-devel <
r-devel at r-project.org> wrote:

> I should add, I don't know that you can rely on this
> particular encoding of R's NA.  If I were trying to restore
> an NA from some external format, I would just generate an
> R NA via e.g NA_real_ in the R session I'm restoring the
> external data into, and not try to hand assemble one.
>

Thanks for your answer, Brodie, especially on Sunday (much appreciated).
The aim is not to reconstruct an NA, but to "tag" an NA (and yes, I was
referring to an NA_real_ of course), as seen in action here:
https://github.com/tidyverse/haven/blob/master/src/tagged_na.c

That code:
- preserves the first part 0x7ff0
- preserves the last part 1954
- adds one additional byte to store (tag) a character provided in the SEXP
vector

That is precisely my understanding, that doubles starting with 0x7ff are
all NaNs. My question was related to the additional part 1954 from the low
bits: why does it need 32 bits?

The binary value of 1954 is 11110100010, which is represented by 11 bits
occupying at most 2 bytes... So why does it need 4 bytes?

Re. the possible overflow, I am not sure: 0x7ff0 is the decimal 32752, or
the binary 111111111110000.
That is just about enough to fit in the available 16 bits (actually 15 to
leave one for the sign bit), so I don't really understand why it would. And
in any case, the union definition uses an unsigned short which (if my
understanding is correct) should certainly not overflow:

typedef union
{
    double value;
    unsigned short word[4];
} ieee_double;

What is gained with this proposal: 16 additional bits to do something with.
For the moment, only 16 are available (from the lower part of the high 32
bits). If the value 1954 would be checked as a short instead of an int, the
other 16 bits would become available. And those bits could be extremely
valuable to tag multi-byte characters, for instance, but also higher
numbers than 32767.

Best wishes,
Adrian

	[[alternative HTML version deleted]]


From brod|e@g@@|@m @end|ng |rom y@hoo@com  Sun May 23 17:19:04 2021
From: brod|e@g@@|@m @end|ng |rom y@hoo@com (brodie gaslam)
Date: Sun, 23 May 2021 15:19:04 +0000 (UTC)
Subject: [Rd] 1954 from NA
In-Reply-To: <CAJ=0CtCoOdijsw2ZPw4TQz2aCrSn+rXKAE3LL_e2FxjO2uO62A@mail.gmail.com>
References: <CAJ=0CtDvHor+DZsmagY07CEjayGGTy+srb8aZqutLFLrTamfJg@mail.gmail.com>
 <1630288902.772422.1621776023778@mail.yahoo.com>
 <1847408368.765062.1621776620165@mail.yahoo.com>
 <CAJ=0CtCoOdijsw2ZPw4TQz2aCrSn+rXKAE3LL_e2FxjO2uO62A@mail.gmail.com>
Message-ID: <1521833885.791949.1621783144724@mail.yahoo.com>

> On Sunday, May 23, 2021, 10:45:22 AM EDT, Adrian Du?a <dusa.adrian at gmail.com> wrote:
>
> On Sun, May 23, 2021 at 4:33 PM brodie gaslam via R-devel <r-devel at r-project.org> wrote:
> > I should add, I don't know that you can rely on this
> > particular encoding of R's NA.? If I were trying to restore
> > an NA from some external format, I would just generate an
> > R NA via e.g NA_real_ in the R session I'm restoring the
> > external data into, and not try to hand assemble one.
>
> Thanks for your answer, Brodie, especially on Sunday (much appreciated).

Maybe I shouldn't answer on Sunday given I've said several wrong things...

> The aim is not to reconstruct an NA, but to "tag" an NA (and yes, I was
> referring to an NA_real_ of course), as seen in action here:
> https://github.com/tidyverse/haven/blob/master/src/tagged_na.c
>
> That code:
> - preserves the first part 0x7ff0
> - preserves the last part 1954
> - adds one additional byte to store (tag) a character provided in the SEXP vector
>
> That is precisely my understanding, that doubles starting with 0x7ff are
> all NaNs. My question was related to the additional part 1954 from the
> low bits: why does it need 32 bits?

It probably doesn't need 32 bits.? The code is trying to set all 64 bits.
It seems natural to do the high 32 bit, and then the low.? But I'm not R
Core so don't listen to me too closely.

> The binary value of 1954 is 11110100010, which is represented by 11 bits
> occupying at most 2 bytes... So why does it need 4 bytes?
>
> Re. the possible overflow, I am not sure: 0x7ff0 is the decimal 32752,
> or the binary 111111111110000.

You are right, I had a moment and wrongly counted hex digits as bytes
instead of half-bytes.

> That is just about enough to fit in the available 16 bits (actually 15
> to leave one for the sign bit), so I don't really understand why it
> would. And in > any case, the union definition uses an unsigned short
> which (if my understanding is correct) should certainly not overflow:
>
> typedef union
> {
>???? double value;
>???? unsigned short word[4];
> } ieee_double;
>
> What is gained with this proposal: 16 additional bits to do something
> with. For the moment, only 16 are available (from the lower part of the
> high 32 bits). If the value 1954 would be checked as a short instead of
> an int, the other 16 bits would become available. And those bits could
> be extremely valuable to tag multi-byte characters, for instance, but
> also higher numbers than 32767.

Note that the stability of the payload portion of NaNs is questionable:

https://developer.r-project.org/Blog/public/2020/11/02/will-r-work-on-apple-silicon/#nanan-payload-propagation

Also, if I understand correctly, you would be asking R core to formalize
the internal representation of the R NA, which I don't think is public?
So that you can use those internal bits for your own purposes with a
guarantee that R will not disturb them?? Obviously only they can answer
that.

Apologies for confusing the issue.

B,

PS: the other obviously wrong thing I said was the NA was 0x7ff0 0000 &
1954 when it is really 0x7ff0 0000 0000 0000 & 1954 when.


From tom@@@k@||ber@ @end|ng |rom gm@||@com  Sun May 23 18:59:00 2021
From: tom@@@k@||ber@ @end|ng |rom gm@||@com (Tomas Kalibera)
Date: Sun, 23 May 2021 18:59:00 +0200
Subject: [Rd] 1954 from NA
In-Reply-To: <CAJ=0CtDvHor+DZsmagY07CEjayGGTy+srb8aZqutLFLrTamfJg@mail.gmail.com>
References: <CAJ=0CtDvHor+DZsmagY07CEjayGGTy+srb8aZqutLFLrTamfJg@mail.gmail.com>
Message-ID: <eede7930-8b70-a0a7-2c14-9b651878ecc6@gmail.com>

TLDR: tagging R NAs is not possible.

External software should not depend on how R currently implements NA, 
this may change at any time. Tagging of NA is not supported in R (if it 
were, it would have been documented). It would not be possible to 
implement such tagging reliably with the current implementation of NA in R.

NaN payload propagation is not standardized. Compilers are free to and 
do optimize code not preserving/achieving any specific propagation. 
CPUs/FPUs differ in how they propagate in binary operations, some zero 
the payload on any operation. Virtualized environments, binary 
translations, etc, may not preserve it in any way, either. ?NA has 
disclaimers about this, an NA may become NaN (payload lost) even in 
unary operations and also in binary operations not involving other NaN/NAs.

Writing any new software that would depend on that anything specific 
happens to the NaN payloads would not be a good idea. One can only 
reliably use the NaN payload bits for storage, that is if one avoids any 
computation at all, avoids passing the values to any external code 
unaware of such tagging (including R), etc. If such software wants any 
NaN to be understood as NA by R, it would have to use the documented R 
API for this (so essentially translating) - but given the problems 
mentioned above, there is really no point in doing that, because such 
NAs become NaNs at any time.

Best
Tomas

On 5/23/21 9:56 AM, Adrian Du?a wrote:
> Dear R devs,
>
> I am probably missing something obvious, but still trying to understand why
> the 1954 from the definition of an NA has to fill 32 bits when it normally
> doesn't need more than 16.
>
> Wouldn't the code below achieve exactly the same thing?
>
> typedef union
> {
>      double value;
>      unsigned short word[4];
> } ieee_double;
>
>
> #ifdef WORDS_BIGENDIAN
> static CONST int hw = 0;
> static CONST int lw = 3;
> #else  /* !WORDS_BIGENDIAN */
> static CONST int hw = 3;
> static CONST int lw = 0;
> #endif /* WORDS_BIGENDIAN */
>
>
> static double R_ValueOfNA(void)
> {
>      volatile ieee_double x;
>      x.word[hw] = 0x7ff0;
>      x.word[lw] = 1954;
>      return x.value;
> }
>
> This question has to do with the tagged NA values from package haven, on
> which I want to improve. Every available bit counts, especially if
> multi-byte characters are going to be involved.
>
> Best wishes,


From du@@@@dr|@n @end|ng |rom gm@||@com  Sun May 23 20:04:28 2021
From: du@@@@dr|@n @end|ng |rom gm@||@com (=?UTF-8?B?QWRyaWFuIER1yJlh?=)
Date: Sun, 23 May 2021 21:04:28 +0300
Subject: [Rd] 1954 from NA
In-Reply-To: <eede7930-8b70-a0a7-2c14-9b651878ecc6@gmail.com>
References: <CAJ=0CtDvHor+DZsmagY07CEjayGGTy+srb8aZqutLFLrTamfJg@mail.gmail.com>
 <eede7930-8b70-a0a7-2c14-9b651878ecc6@gmail.com>
Message-ID: <CAJ=0CtCprzMyNzjuhneBxyWG1X3g1KDQ+TJE3J6QuhVXG8wZpA@mail.gmail.com>

Dear Tomas,

I understand that perfectly, but that is fine.
The payload is not going to be used in any computations anyways, it is
strictly an information carrier that differentiates between different types
of (tagged) NA values.

Having only one NA value in R is extremely limiting for the social
sciences, where multiple missing values may exist, because respondents:
- did not know what to respond, or
- did not want to respond, or perhaps
- the question did not apply in a given situation etc.

All of these need to be captured, stored, and most importantly treated as
if they would be regular missing values. Whether the payload might be lost
in computations makes no difference: they were supposed to be "missing
values" anyways.

The original question is how the payload is currently stored: as an
unsigned int of 32 bits, or as an unsigned short of 16 bits. If the R
internals would not be affected (and I see no reason why they would be), it
would allow an entire universe for the social sciences that is not
currently available and which all other major statistical packages do offer.

Thank you very much, your attention is greatly appreciated,
Adrian

On Sun, May 23, 2021 at 7:59 PM Tomas Kalibera <tomas.kalibera at gmail.com>
wrote:

> TLDR: tagging R NAs is not possible.
>
> External software should not depend on how R currently implements NA,
> this may change at any time. Tagging of NA is not supported in R (if it
> were, it would have been documented). It would not be possible to
> implement such tagging reliably with the current implementation of NA in R.
>
> NaN payload propagation is not standardized. Compilers are free to and
> do optimize code not preserving/achieving any specific propagation.
> CPUs/FPUs differ in how they propagate in binary operations, some zero
> the payload on any operation. Virtualized environments, binary
> translations, etc, may not preserve it in any way, either. ?NA has
> disclaimers about this, an NA may become NaN (payload lost) even in
> unary operations and also in binary operations not involving other NaN/NAs.
>
> Writing any new software that would depend on that anything specific
> happens to the NaN payloads would not be a good idea. One can only
> reliably use the NaN payload bits for storage, that is if one avoids any
> computation at all, avoids passing the values to any external code
> unaware of such tagging (including R), etc. If such software wants any
> NaN to be understood as NA by R, it would have to use the documented R
> API for this (so essentially translating) - but given the problems
> mentioned above, there is really no point in doing that, because such
> NAs become NaNs at any time.
>
> Best
> Tomas
>
> On 5/23/21 9:56 AM, Adrian Du?a wrote:
> > Dear R devs,
> >
> > I am probably missing something obvious, but still trying to understand
> why
> > the 1954 from the definition of an NA has to fill 32 bits when it
> normally
> > doesn't need more than 16.
> >
> > Wouldn't the code below achieve exactly the same thing?
> >
> > typedef union
> > {
> >      double value;
> >      unsigned short word[4];
> > } ieee_double;
> >
> >
> > #ifdef WORDS_BIGENDIAN
> > static CONST int hw = 0;
> > static CONST int lw = 3;
> > #else  /* !WORDS_BIGENDIAN */
> > static CONST int hw = 3;
> > static CONST int lw = 0;
> > #endif /* WORDS_BIGENDIAN */
> >
> >
> > static double R_ValueOfNA(void)
> > {
> >      volatile ieee_double x;
> >      x.word[hw] = 0x7ff0;
> >      x.word[lw] = 1954;
> >      return x.value;
> > }
> >
> > This question has to do with the tagged NA values from package haven, on
> > which I want to improve. Every available bit counts, especially if
> > multi-byte characters are going to be involved.
> >
> > Best wishes,
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From tom@@@k@||ber@ @end|ng |rom gm@||@com  Sun May 23 21:14:54 2021
From: tom@@@k@||ber@ @end|ng |rom gm@||@com (Tomas Kalibera)
Date: Sun, 23 May 2021 21:14:54 +0200
Subject: [Rd] 1954 from NA
In-Reply-To: <CAJ=0CtCprzMyNzjuhneBxyWG1X3g1KDQ+TJE3J6QuhVXG8wZpA@mail.gmail.com>
References: <CAJ=0CtDvHor+DZsmagY07CEjayGGTy+srb8aZqutLFLrTamfJg@mail.gmail.com>
 <eede7930-8b70-a0a7-2c14-9b651878ecc6@gmail.com>
 <CAJ=0CtCprzMyNzjuhneBxyWG1X3g1KDQ+TJE3J6QuhVXG8wZpA@mail.gmail.com>
Message-ID: <0aa3352e-62a0-a8b9-fa6a-a3a994ab8e55@gmail.com>


On 5/23/21 8:04 PM, Adrian Du?a wrote:
> Dear Tomas,
>
> I understand that perfectly, but that is fine.
> The payload is not going to be used in any computations anyways, it is 
> strictly an information carrier that differentiates between different 
> types of (tagged) NA values.
Good, but unfortunately the delineation between computation and 
non-computation is not always transparent. Even if an operation doesn't 
look like "computation" on the high-level, it may internally involve 
computation - so, really, an R NA can become R NaN and vice versa, at 
any point (this is not a "feature", but it is how things are now).
> Having only one NA value in R is extremely limiting for the social 
> sciences, where multiple missing values may exist, because respondents:
> - did not know what to respond, or
> - did not want to respond, or perhaps
> - the question did not apply in a given situation etc.
>
> All of these need to be captured, stored, and most importantly treated 
> as if they would be regular missing values. Whether the payload might 
> be lost in computations makes no difference: they were supposed to be 
> "missing values" anyways.

Ok, then I would probably keep the meta-data on the missing values on 
the side to implement such missing values in such code, and treat them 
explicitly in supported operations.

But. in principle, you can use the floating-point NaN payloads, and you 
can pass such values to R. You just need to be prepared that not only 
you would loose your payloads/tags, but also the difference between R NA 
and R NaNs. Thanks to value semantics of R, you would not loose the tags 
in input values with proper reference counts (e.g. marked immutable), 
because those values will not be modified.

Best
Tomas

> The original question is how the payload is currently stored: as an 
> unsigned int of 32 bits, or as an unsigned short of 16 bits. If the R 
> internals would not be affected (and I see no reason why they would 
> be), it would allow an entire universe for the social sciences that is 
> not currently available and which all other major statistical packages 
> do offer.

>
> Thank you very much, your attention is greatly appreciated,
> Adrian
>
> On Sun, May 23, 2021 at 7:59 PM Tomas Kalibera 
> <tomas.kalibera at gmail.com <mailto:tomas.kalibera at gmail.com>> wrote:
>
>     TLDR: tagging R NAs is not possible.
>
>     External software should not depend on how R currently implements NA,
>     this may change at any time. Tagging of NA is not supported in R
>     (if it
>     were, it would have been documented). It would not be possible to
>     implement such tagging reliably with the current implementation of
>     NA in R.
>
>     NaN payload propagation is not standardized. Compilers are free to
>     and
>     do optimize code not preserving/achieving any specific propagation.
>     CPUs/FPUs differ in how they propagate in binary operations, some
>     zero
>     the payload on any operation. Virtualized environments, binary
>     translations, etc, may not preserve it in any way, either. ?NA has
>     disclaimers about this, an NA may become NaN (payload lost) even in
>     unary operations and also in binary operations not involving other
>     NaN/NAs.
>
>     Writing any new software that would depend on that anything specific
>     happens to the NaN payloads would not be a good idea. One can only
>     reliably use the NaN payload bits for storage, that is if one
>     avoids any
>     computation at all, avoids passing the values to any external code
>     unaware of such tagging (including R), etc. If such software wants
>     any
>     NaN to be understood as NA by R, it would have to use the
>     documented R
>     API for this (so essentially translating) - but given the problems
>     mentioned above, there is really no point in doing that, because such
>     NAs become NaNs at any time.
>
>     Best
>     Tomas
>
>     On 5/23/21 9:56 AM, Adrian Du?a wrote:
>     > Dear R devs,
>     >
>     > I am probably missing something obvious, but still trying to
>     understand why
>     > the 1954 from the definition of an NA has to fill 32 bits when
>     it normally
>     > doesn't need more than 16.
>     >
>     > Wouldn't the code below achieve exactly the same thing?
>     >
>     > typedef union
>     > {
>     >? ? ? double value;
>     >? ? ? unsigned short word[4];
>     > } ieee_double;
>     >
>     >
>     > #ifdef WORDS_BIGENDIAN
>     > static CONST int hw = 0;
>     > static CONST int lw = 3;
>     > #else? /* !WORDS_BIGENDIAN */
>     > static CONST int hw = 3;
>     > static CONST int lw = 0;
>     > #endif /* WORDS_BIGENDIAN */
>     >
>     >
>     > static double R_ValueOfNA(void)
>     > {
>     >? ? ? volatile ieee_double x;
>     >? ? ? x.word[hw] = 0x7ff0;
>     >? ? ? x.word[lw] = 1954;
>     >? ? ? return x.value;
>     > }
>     >
>     > This question has to do with the tagged NA values from package
>     haven, on
>     > which I want to improve. Every available bit counts, especially if
>     > multi-byte characters are going to be involved.
>     >
>     > Best wishes,
>
>     ______________________________________________
>     R-devel at r-project.org <mailto:R-devel at r-project.org> mailing list
>     https://stat.ethz.ch/mailman/listinfo/r-devel
>     <https://stat.ethz.ch/mailman/listinfo/r-devel>
>

	[[alternative HTML version deleted]]


From @v|gro@@ @end|ng |rom ver|zon@net  Sun May 23 21:21:31 2021
From: @v|gro@@ @end|ng |rom ver|zon@net (Avi Gross)
Date: Sun, 23 May 2021 15:21:31 -0400
Subject: [Rd] 1954 from NA
In-Reply-To: <CAJ=0CtCprzMyNzjuhneBxyWG1X3g1KDQ+TJE3J6QuhVXG8wZpA@mail.gmail.com>
References: <CAJ=0CtDvHor+DZsmagY07CEjayGGTy+srb8aZqutLFLrTamfJg@mail.gmail.com>
 <eede7930-8b70-a0a7-2c14-9b651878ecc6@gmail.com>
 <CAJ=0CtCprzMyNzjuhneBxyWG1X3g1KDQ+TJE3J6QuhVXG8wZpA@mail.gmail.com>
Message-ID: <008a01d75008$d6612490$83236db0$@verizon.net>

Arguably, R was not developed to satisfy some needs in the way intended.

When I have had to work with datasets from some of the social sciences I have had to adapt to subtleties in how they did things with software like SPSS in which an NA was done using an out of bounds marker like 999 or "." or even a blank cell. The problem is that R has a concept where data such as integers or floating point numbers is not stored as text normally but in their own formats and a vector by definition can only contain ONE data type. So the various forms of NA as well as Nan and Inf had to be grafted on to be considered VALID to share the same storage area as if they sort of were an integer or floating point number or text or whatever.

It does strike me as possible to simply have a column that is something like a factor that can contain as many NA excuses as you wish such as "NOT ANSWERED" to "CANNOT READ THE SQUIGLE" to "NOT SURE" to "WILL BE FILLED IN LATER" to "I DON'T SPEAK ENGLISH AND CANNOT ANSWER STUPID QUESTIONS". This additional column would presumably only have content when the other column has an NA. Your queries and other changes would work on something like a data.frame where both such columns coexisted.

Note reading in data with multiple NA reasons may take extra work. If your errors codes are text, it will all become text. If the errors are 999 and 998 and 997, it may all be treated as numeric and you may not want to convert all such codes to an NA immediately. Rather, you would use the first vector/column to make the second vector and THEN replace everything that should be an NA with an actual NA and reparse the entire vector to become properly numeric unless you like working with text and will convert to numbers as needed on the fly.

Now this form of annotation may not be pleasing but I suggest that an implementation that does allow annotation may use up space too. Of course, if your NA values are rare and space is only used then, you might save space. But if you could make a factor column and have it use the smallest int it can get as a basis, it may be a way to save on space.

People who have done work with R, especially those using the tidyverse, are quite used to using one column to explain another. So if you are asked to say tabulate what percent of missing values are due to reasons A/B/C then the added columns works fine for that calculation too.


-----Original Message-----
From: R-devel <r-devel-bounces at r-project.org> On Behalf Of Adrian Du?a
Sent: Sunday, May 23, 2021 2:04 PM
To: Tomas Kalibera <tomas.kalibera at gmail.com>
Cc: r-devel <r-devel at r-project.org>
Subject: Re: [Rd] 1954 from NA

Dear Tomas,

I understand that perfectly, but that is fine.
The payload is not going to be used in any computations anyways, it is strictly an information carrier that differentiates between different types of (tagged) NA values.

Having only one NA value in R is extremely limiting for the social sciences, where multiple missing values may exist, because respondents:
- did not know what to respond, or
- did not want to respond, or perhaps
- the question did not apply in a given situation etc.

All of these need to be captured, stored, and most importantly treated as if they would be regular missing values. Whether the payload might be lost in computations makes no difference: they were supposed to be "missing values" anyways.

The original question is how the payload is currently stored: as an unsigned int of 32 bits, or as an unsigned short of 16 bits. If the R internals would not be affected (and I see no reason why they would be), it would allow an entire universe for the social sciences that is not currently available and which all other major statistical packages do offer.

Thank you very much, your attention is greatly appreciated, Adrian

On Sun, May 23, 2021 at 7:59 PM Tomas Kalibera <tomas.kalibera at gmail.com>
wrote:

> TLDR: tagging R NAs is not possible.
>
> External software should not depend on how R currently implements NA, 
> this may change at any time. Tagging of NA is not supported in R (if 
> it were, it would have been documented). It would not be possible to 
> implement such tagging reliably with the current implementation of NA in R.
>
> NaN payload propagation is not standardized. Compilers are free to and 
> do optimize code not preserving/achieving any specific propagation.
> CPUs/FPUs differ in how they propagate in binary operations, some zero 
> the payload on any operation. Virtualized environments, binary 
> translations, etc, may not preserve it in any way, either. ?NA has 
> disclaimers about this, an NA may become NaN (payload lost) even in 
> unary operations and also in binary operations not involving other NaN/NAs.
>
> Writing any new software that would depend on that anything specific 
> happens to the NaN payloads would not be a good idea. One can only 
> reliably use the NaN payload bits for storage, that is if one avoids 
> any computation at all, avoids passing the values to any external code 
> unaware of such tagging (including R), etc. If such software wants any 
> NaN to be understood as NA by R, it would have to use the documented R 
> API for this (so essentially translating) - but given the problems 
> mentioned above, there is really no point in doing that, because such 
> NAs become NaNs at any time.
>
> Best
> Tomas
>
> On 5/23/21 9:56 AM, Adrian Du?a wrote:
> > Dear R devs,
> >
> > I am probably missing something obvious, but still trying to 
> > understand
> why
> > the 1954 from the definition of an NA has to fill 32 bits when it
> normally
> > doesn't need more than 16.
> >
> > Wouldn't the code below achieve exactly the same thing?
> >
> > typedef union
> > {
> >      double value;
> >      unsigned short word[4];
> > } ieee_double;
> >
> >
> > #ifdef WORDS_BIGENDIAN
> > static CONST int hw = 0;
> > static CONST int lw = 3;
> > #else  /* !WORDS_BIGENDIAN */
> > static CONST int hw = 3;
> > static CONST int lw = 0;
> > #endif /* WORDS_BIGENDIAN */
> >
> >
> > static double R_ValueOfNA(void)
> > {
> >      volatile ieee_double x;
> >      x.word[hw] = 0x7ff0;
> >      x.word[lw] = 1954;
> >      return x.value;
> > }
> >
> > This question has to do with the tagged NA values from package 
> > haven, on which I want to improve. Every available bit counts, 
> > especially if multi-byte characters are going to be involved.
> >
> > Best wishes,
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]

______________________________________________
R-devel at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-devel


From m|n@h@|| @end|ng |rom um|ch@edu  Mon May 24 04:58:35 2021
From: m|n@h@|| @end|ng |rom um|ch@edu (Greg Minshall)
Date: Mon, 24 May 2021 05:58:35 +0300
Subject: [Rd] 1954 from NA
In-Reply-To: Your message of "Sun, 23 May 2021 15:21:31 -0400."
 <008a01d75008$d6612490$83236db0$@verizon.net>
Message-ID: <3385535.1621825115@apollo2.minshall.org>

+1

Avi Gross via R-devel <r-devel at r-project.org> wrote:

> Arguably, R was not developed to satisfy some needs in the way intended.
> 
> When I have had to work with datasets from some of the social sciences I have had to adapt to subtleties in how they did things with software like SPSS in which an NA was done using an out of bounds marker like 999 or "." or even a blank cell. The problem is that R has a concept where data such as integers or floating point numbers is not stored as text normally but in their own formats and a vector by definition can only contain ONE data type. So the various forms of NA as well as Nan and Inf had to be grafted on to be considered VALID to share the same storage area as if they sort of were an integer or floating point number or text or whatever.
> 
> It does strike me as possible to simply have a column that is something like a factor that can contain as many NA excuses as you wish such as "NOT ANSWERED" to "CANNOT READ THE SQUIGLE" to "NOT SURE" to "WILL BE FILLED IN LATER" to "I DON'T SPEAK ENGLISH AND CANNOT ANSWER STUPID QUESTIONS". This additional column would presumably only have content when the other column has an NA. Your queries and other changes would work on something like a data.frame where both such columns coexisted.
> 
> Note reading in data with multiple NA reasons may take extra work. If your errors codes are text, it will all become text. If the errors are 999 and 998 and 997, it may all be treated as numeric and you may not want to convert all such codes to an NA immediately. Rather, you would use the first vector/column to make the second vector and THEN replace everything that should be an NA with an actual NA and reparse the entire vector to become properly numeric unless you like working with text and will convert to numbers as needed on the fly.
> 
> Now this form of annotation may not be pleasing but I suggest that an implementation that does allow annotation may use up space too. Of course, if your NA values are rare and space is only used then, you might save space. But if you could make a factor column and have it use the smallest int it can get as a basis, it may be a way to save on space.
> 
> People who have done work with R, especially those using the tidyverse, are quite used to using one column to explain another. So if you are asked to say tabulate what percent of missing values are due to reasons A/B/C then the added columns works fine for that calculation too.
> 
> 
> -----Original Message-----
> From: R-devel <r-devel-bounces at r-project.org> On Behalf Of Adrian Du?a
> Sent: Sunday, May 23, 2021 2:04 PM
> To: Tomas Kalibera <tomas.kalibera at gmail.com>
> Cc: r-devel <r-devel at r-project.org>
> Subject: Re: [Rd] 1954 from NA
> 
> Dear Tomas,
> 
> I understand that perfectly, but that is fine.
> The payload is not going to be used in any computations anyways, it is strictly an information carrier that differentiates between different types of (tagged) NA values.
> 
> Having only one NA value in R is extremely limiting for the social sciences, where multiple missing values may exist, because respondents:
> - did not know what to respond, or
> - did not want to respond, or perhaps
> - the question did not apply in a given situation etc.
> 
> All of these need to be captured, stored, and most importantly treated as if they would be regular missing values. Whether the payload might be lost in computations makes no difference: they were supposed to be "missing values" anyways.
> 
> The original question is how the payload is currently stored: as an unsigned int of 32 bits, or as an unsigned short of 16 bits. If the R internals would not be affected (and I see no reason why they would be), it would allow an entire universe for the social sciences that is not currently available and which all other major statistical packages do offer.
> 
> Thank you very much, your attention is greatly appreciated, Adrian
> 
> On Sun, May 23, 2021 at 7:59 PM Tomas Kalibera <tomas.kalibera at gmail.com>
> wrote:
> 
> > TLDR: tagging R NAs is not possible.
> >
> > External software should not depend on how R currently implements NA, 
> > this may change at any time. Tagging of NA is not supported in R (if 
> > it were, it would have been documented). It would not be possible to 
> > implement such tagging reliably with the current implementation of NA in R.
> >
> > NaN payload propagation is not standardized. Compilers are free to and 
> > do optimize code not preserving/achieving any specific propagation.
> > CPUs/FPUs differ in how they propagate in binary operations, some zero 
> > the payload on any operation. Virtualized environments, binary 
> > translations, etc, may not preserve it in any way, either. ?NA has 
> > disclaimers about this, an NA may become NaN (payload lost) even in 
> > unary operations and also in binary operations not involving other NaN/NAs.
> >
> > Writing any new software that would depend on that anything specific 
> > happens to the NaN payloads would not be a good idea. One can only 
> > reliably use the NaN payload bits for storage, that is if one avoids 
> > any computation at all, avoids passing the values to any external code 
> > unaware of such tagging (including R), etc. If such software wants any 
> > NaN to be understood as NA by R, it would have to use the documented R 
> > API for this (so essentially translating) - but given the problems 
> > mentioned above, there is really no point in doing that, because such 
> > NAs become NaNs at any time.
> >
> > Best
> > Tomas
> >
> > On 5/23/21 9:56 AM, Adrian Du?a wrote:
> > > Dear R devs,
> > >
> > > I am probably missing something obvious, but still trying to 
> > > understand
> > why
> > > the 1954 from the definition of an NA has to fill 32 bits when it
> > normally
> > > doesn't need more than 16.
> > >
> > > Wouldn't the code below achieve exactly the same thing?
> > >
> > > typedef union
> > > {
> > >      double value;
> > >      unsigned short word[4];
> > > } ieee_double;
> > >
> > >
> > > #ifdef WORDS_BIGENDIAN
> > > static CONST int hw = 0;
> > > static CONST int lw = 3;
> > > #else  /* !WORDS_BIGENDIAN */
> > > static CONST int hw = 3;
> > > static CONST int lw = 0;
> > > #endif /* WORDS_BIGENDIAN */
> > >
> > >
> > > static double R_ValueOfNA(void)
> > > {
> > >      volatile ieee_double x;
> > >      x.word[hw] = 0x7ff0;
> > >      x.word[lw] = 1954;
> > >      return x.value;
> > > }
> > >
> > > This question has to do with the tagged NA values from package 
> > > haven, on which I want to improve. Every available bit counts, 
> > > especially if multi-byte characters are going to be involved.
> > >
> > > Best wishes,
> >
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
> >
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 


From du@@@@dr|@n @end|ng |rom gm@||@com  Mon May 24 11:46:35 2021
From: du@@@@dr|@n @end|ng |rom gm@||@com (=?UTF-8?B?QWRyaWFuIER1yJlh?=)
Date: Mon, 24 May 2021 12:46:35 +0300
Subject: [Rd] 1954 from NA
In-Reply-To: <0aa3352e-62a0-a8b9-fa6a-a3a994ab8e55@gmail.com>
References: <CAJ=0CtDvHor+DZsmagY07CEjayGGTy+srb8aZqutLFLrTamfJg@mail.gmail.com>
 <eede7930-8b70-a0a7-2c14-9b651878ecc6@gmail.com>
 <CAJ=0CtCprzMyNzjuhneBxyWG1X3g1KDQ+TJE3J6QuhVXG8wZpA@mail.gmail.com>
 <0aa3352e-62a0-a8b9-fa6a-a3a994ab8e55@gmail.com>
Message-ID: <CAJ=0CtBJY8HcxzPX=5N65uXE410mQhnNDoG+dr7hYCtZhaTsQA@mail.gmail.com>

On Sun, May 23, 2021 at 10:14 PM Tomas Kalibera <tomas.kalibera at gmail.com>
wrote:

> [...]
>
> Good, but unfortunately the delineation between computation and
> non-computation is not always transparent. Even if an operation doesn't
> look like "computation" on the high-level, it may internally involve
> computation - so, really, an R NA can become R NaN and vice versa, at any
> point (this is not a "feature", but it is how things are now).
>

I see.
Well, this is a risk we'll have to consider when the time comes. For the
moment, storing some metadata within the payload seems to work.



> [...]
>
> Ok, then I would probably keep the meta-data on the missing values on the
> side to implement such missing values in such code, and treat them
> explicitly in supported operations.
>
> But. in principle, you can use the floating-point NaN payloads, and you
> can pass such values to R. You just need to be prepared that not only you
> would loose your payloads/tags, but also the difference between R NA and R
> NaNs. Thanks to value semantics of R, you would not loose the tags in input
> values with proper reference counts (e.g. marked immutable), because those
> values will not be modified.
>
NaNs are fine of course, but then some (social science?) users might get
confused about the difference between NAs and NaNs, and for this reason
only I would still like to preserve the 1954 payload.
If at all possible, however, the extra 16 bits from this payload would make
a whole lot of a difference.

Please forgive my persistence, but would it be possible to use an unsigned
short instead of an unsigned int for the 1954 payload?
That is, if it doesn't break anything, but I don't really see what it
could. The corresponding check function seems to work just fine and it
doesn't need to be changed at all:

int R_IsNA(double x)
{
    if (isnan(x)) {
ieee_double y;
y.value = x;
return (y.word[lw] == 1954);
    }
    return 0;
}

Best wishes,
Adrian

	[[alternative HTML version deleted]]


From tom@@@k@||ber@ @end|ng |rom gm@||@com  Mon May 24 12:31:53 2021
From: tom@@@k@||ber@ @end|ng |rom gm@||@com (Tomas Kalibera)
Date: Mon, 24 May 2021 12:31:53 +0200
Subject: [Rd] 1954 from NA
In-Reply-To: <CAJ=0CtBJY8HcxzPX=5N65uXE410mQhnNDoG+dr7hYCtZhaTsQA@mail.gmail.com>
References: <CAJ=0CtDvHor+DZsmagY07CEjayGGTy+srb8aZqutLFLrTamfJg@mail.gmail.com>
 <eede7930-8b70-a0a7-2c14-9b651878ecc6@gmail.com>
 <CAJ=0CtCprzMyNzjuhneBxyWG1X3g1KDQ+TJE3J6QuhVXG8wZpA@mail.gmail.com>
 <0aa3352e-62a0-a8b9-fa6a-a3a994ab8e55@gmail.com>
 <CAJ=0CtBJY8HcxzPX=5N65uXE410mQhnNDoG+dr7hYCtZhaTsQA@mail.gmail.com>
Message-ID: <45cb76eb-40e9-f9e1-8dd3-c9042b4e529a@gmail.com>


On 5/24/21 11:46 AM, Adrian Du?a wrote:
> On Sun, May 23, 2021 at 10:14 PM Tomas Kalibera 
> <tomas.kalibera at gmail.com <mailto:tomas.kalibera at gmail.com>> wrote:
>
>     [...]
>
>     Good, but unfortunately the delineation between computation and
>     non-computation is not always transparent. Even if an operation
>     doesn't look like "computation" on the high-level, it may
>     internally involve computation - so, really, an R NA can become R
>     NaN and vice versa, at any point (this is not a "feature", but it
>     is how things are now).
>
>
> I see.
> Well, this is a risk we'll have to consider when the time?comes. For 
> the moment, storing some metadata within the payload seems to work.
>
>>     [...]
>
>     Ok, then I would probably keep the meta-data on the missing values
>     on the side to implement such missing values in such code, and
>     treat them explicitly in supported operations.
>
>     But. in principle, you can use the floating-point NaN payloads,
>     and you can pass such values to R. You just need to be prepared
>     that not only you would loose your payloads/tags, but also the
>     difference between R NA and R NaNs. Thanks to value semantics of
>     R, you would not loose the tags in input values with proper
>     reference counts (e.g. marked immutable), because those values
>     will not be modified.
>
> NaNs are fine of?course, but then some (social science?) users might 
> get confused about the difference between NAs and NaNs, and for this 
> reason only I would still like to preserve the 1954 payload.
> If at all possible, however, the extra 16 bits from this payload would 
> make a whole lot of a difference.
>
> Please forgive my persistence, but would it be possible to use an 
> unsigned short instead of an unsigned int for the 1954 payload?
> That is, if it doesn't break anything, but I don't really see what it 
> could. The corresponding check function seems to work just fine and it 
> doesn't need to be changed at all:
>
> int R_IsNA(double x)
> {
> ? ? if (isnan(x)) {
> ieee_double y;
> y.value = x;
> return (y.word[lw] == 1954);
> ? ? }
> ? ? return 0;
> }

For the reasons I explained, I would be against such a change. Keeping 
the data on the side, as also recommended by others on this list, would 
allow you for a reliable implementation. I don't want to support fragile 
package code building on unspecified R internals, and in this case 
particularly internals that themselves have not stood the test of time, 
so are at high risk of change.

Best
Tomas

>
> Best wishes,
> Adrian
>
>
>

	[[alternative HTML version deleted]]


From du@@@@dr|@n @end|ng |rom un|buc@ro  Mon May 24 11:26:12 2021
From: du@@@@dr|@n @end|ng |rom un|buc@ro (=?UTF-8?B?QWRyaWFuIER1yJlh?=)
Date: Mon, 24 May 2021 12:26:12 +0300
Subject: [Rd] 1954 from NA
In-Reply-To: <008a01d75008$d6612490$83236db0$@verizon.net>
References: <CAJ=0CtDvHor+DZsmagY07CEjayGGTy+srb8aZqutLFLrTamfJg@mail.gmail.com>
 <eede7930-8b70-a0a7-2c14-9b651878ecc6@gmail.com>
 <CAJ=0CtCprzMyNzjuhneBxyWG1X3g1KDQ+TJE3J6QuhVXG8wZpA@mail.gmail.com>
 <008a01d75008$d6612490$83236db0$@verizon.net>
Message-ID: <CAJ=0CtCfRMJM6wq1ggkSv2BNfX+CGnW2N-SZAN8poOG3ev=nUA@mail.gmail.com>

Hmm...
If it was only one column then your solution is neat. But with 5-600
variables, each of which can contain multiple missing values, to double
this number of variables just to describe NA values seems to me excessive.
Not to mention we should be able to quickly convert / import / export from
one software package to another. This would imply maintaining some sort of
metadata reference of which explanatory additional factor describes which
original variable.

All of this strikes me as a lot of hassle compared to storing some
information within a tagged NA value... I just need a little bit more bits
to play with.

Best wishes,
Adrian

On Sun, May 23, 2021 at 10:21 PM Avi Gross via R-devel <
r-devel at r-project.org> wrote:

> Arguably, R was not developed to satisfy some needs in the way intended.
>
> When I have had to work with datasets from some of the social sciences I
> have had to adapt to subtleties in how they did things with software like
> SPSS in which an NA was done using an out of bounds marker like 999 or "."
> or even a blank cell. The problem is that R has a concept where data such
> as integers or floating point numbers is not stored as text normally but in
> their own formats and a vector by definition can only contain ONE data
> type. So the various forms of NA as well as Nan and Inf had to be grafted
> on to be considered VALID to share the same storage area as if they sort of
> were an integer or floating point number or text or whatever.
>
> It does strike me as possible to simply have a column that is something
> like a factor that can contain as many NA excuses as you wish such as "NOT
> ANSWERED" to "CANNOT READ THE SQUIGLE" to "NOT SURE" to "WILL BE FILLED IN
> LATER" to "I DON'T SPEAK ENGLISH AND CANNOT ANSWER STUPID QUESTIONS". This
> additional column would presumably only have content when the other column
> has an NA. Your queries and other changes would work on something like a
> data.frame where both such columns coexisted.
>
> Note reading in data with multiple NA reasons may take extra work. If your
> errors codes are text, it will all become text. If the errors are 999 and
> 998 and 997, it may all be treated as numeric and you may not want to
> convert all such codes to an NA immediately. Rather, you would use the
> first vector/column to make the second vector and THEN replace everything
> that should be an NA with an actual NA and reparse the entire vector to
> become properly numeric unless you like working with text and will convert
> to numbers as needed on the fly.
>
> Now this form of annotation may not be pleasing but I suggest that an
> implementation that does allow annotation may use up space too. Of course,
> if your NA values are rare and space is only used then, you might save
> space. But if you could make a factor column and have it use the smallest
> int it can get as a basis, it may be a way to save on space.
>
> People who have done work with R, especially those using the tidyverse,
> are quite used to using one column to explain another. So if you are asked
> to say tabulate what percent of missing values are due to reasons A/B/C
> then the added columns works fine for that calculation too.
>

-- 
Adrian Dusa
University of Bucharest
Romanian Social Data Archive
Soseaua Panduri nr. 90-92
050663 Bucharest sector 5
Romania
https://adriandusa.eu

	[[alternative HTML version deleted]]


From m|n@h@|| @end|ng |rom um|ch@edu  Mon May 24 13:11:30 2021
From: m|n@h@|| @end|ng |rom um|ch@edu (Greg Minshall)
Date: Mon, 24 May 2021 14:11:30 +0300
Subject: [Rd] 1954 from NA
In-Reply-To: Your message of "Mon, 24 May 2021 12:26:12 +0300."
 <CAJ=0CtCfRMJM6wq1ggkSv2BNfX+CGnW2N-SZAN8poOG3ev=nUA@mail.gmail.com>
Message-ID: <3443246.1621854690@apollo2.minshall.org>

Adrian,

> If it was only one column then your solution is neat. But with 5-600
> variables, each of which can contain multiple missing values, to
> double this number of variables just to describe NA values seems to me
> excessive.  Not to mention we should be able to quickly convert /
> import / export from one software package to another. This would imply
> maintaining some sort of metadata reference of which explanatory
> additional factor describes which original variable.

one thing *i* should keep in mind is the old saying: "The difference
between theory and practice is that in theory there is no difference,
but in practice, there is."

but, in theory:

if you have 500 columns of possibly-NA'd variables, you could have one
column of 500 "bits", where each bit has one of N values, N being the
number of explanations the corresponding column has for why the NA
exists.

i guess the CS'y thing that comes to my mind here is that one thing is
the *semantics* of what you are trying to convey, and the other is how
those semantics are *encoded* in whatever representation you are using.

cheers, Greg


From du@@@@dr|@n @end|ng |rom gm@||@com  Mon May 24 14:02:29 2021
From: du@@@@dr|@n @end|ng |rom gm@||@com (=?UTF-8?B?QWRyaWFuIER1yJlh?=)
Date: Mon, 24 May 2021 15:02:29 +0300
Subject: [Rd] 1954 from NA
In-Reply-To: <45cb76eb-40e9-f9e1-8dd3-c9042b4e529a@gmail.com>
References: <CAJ=0CtDvHor+DZsmagY07CEjayGGTy+srb8aZqutLFLrTamfJg@mail.gmail.com>
 <eede7930-8b70-a0a7-2c14-9b651878ecc6@gmail.com>
 <CAJ=0CtCprzMyNzjuhneBxyWG1X3g1KDQ+TJE3J6QuhVXG8wZpA@mail.gmail.com>
 <0aa3352e-62a0-a8b9-fa6a-a3a994ab8e55@gmail.com>
 <CAJ=0CtBJY8HcxzPX=5N65uXE410mQhnNDoG+dr7hYCtZhaTsQA@mail.gmail.com>
 <45cb76eb-40e9-f9e1-8dd3-c9042b4e529a@gmail.com>
Message-ID: <CAJ=0CtBr13cxOwKdD4HyAFCfwDhM3NVEfvarZigv+6TQBPjzDQ@mail.gmail.com>

On Mon, May 24, 2021 at 1:31 PM Tomas Kalibera <tomas.kalibera at gmail.com>
wrote:

> [...]
>
> For the reasons I explained, I would be against such a change. Keeping the
> data on the side, as also recommended by others on this list, would allow
> you for a reliable implementation. I don't want to support fragile package
> code building on unspecified R internals, and in this case particularly
> internals that themselves have not stood the test of time, so are at high
> risk of change.
>
I understand, and it makes sense.
We'll have to wait for the R internals to settle (this really is
surprising, I wonder how other software have solved this). In the meantime,
I will probably go ahead with NaNs.

Thank you again,
Adrian

	[[alternative HTML version deleted]]


From @|ex @end|ng |rom bed@t@dr|ven@com  Mon May 24 14:29:44 2021
From: @|ex @end|ng |rom bed@t@dr|ven@com (Bertram, Alexander)
Date: Mon, 24 May 2021 14:29:44 +0200
Subject: [Rd] 1954 from NA
In-Reply-To: <CAJ=0CtBr13cxOwKdD4HyAFCfwDhM3NVEfvarZigv+6TQBPjzDQ@mail.gmail.com>
References: <CAJ=0CtDvHor+DZsmagY07CEjayGGTy+srb8aZqutLFLrTamfJg@mail.gmail.com>
 <eede7930-8b70-a0a7-2c14-9b651878ecc6@gmail.com>
 <CAJ=0CtCprzMyNzjuhneBxyWG1X3g1KDQ+TJE3J6QuhVXG8wZpA@mail.gmail.com>
 <0aa3352e-62a0-a8b9-fa6a-a3a994ab8e55@gmail.com>
 <CAJ=0CtBJY8HcxzPX=5N65uXE410mQhnNDoG+dr7hYCtZhaTsQA@mail.gmail.com>
 <45cb76eb-40e9-f9e1-8dd3-c9042b4e529a@gmail.com>
 <CAJ=0CtBr13cxOwKdD4HyAFCfwDhM3NVEfvarZigv+6TQBPjzDQ@mail.gmail.com>
Message-ID: <CAOdV3zARBg=TDTDPZMuzWSOqi=Wi3yShRxrYTbAYM9njb4Bx0A@mail.gmail.com>

Dear Adrian,
I just wanted to pipe in and underscore Thomas' point: the payload bits of
IEEE 754 floating point values are no place to store data that you care
about or need to keep. That is not only related to the R APIs, but also how
processors handle floating point values and signaling and non-signaling
NaNs. It is very difficult to reason about when and under which
circumstances these bits are preserved. I spent a lot of time working on
Renjin's handling of these values and I can assure that any such scheme
will end in tears.

A far, far better option is to use R's attributes to store this kind of
metadata. This is exactly what this language feature is for. There is
already a standard 'levels' attribute that holds the labels of factors like
"Yes", "No" , "Refused", "Interviewer error'' etc. In the past, I've worked
on projects where we stored an additional attribute like "missingLevels"
that stores extra metadata on which levels should be used in which kind of
analysis. That way, you can preserve all the information, and then write a
utility function which automatically applies certain logic to a whole
dataframe just before passing the data to an analysis function. This is
also important because in surveys like this, different values should be
excluded at different times. For example, you might want to include all
responses in a data quality report, but exclude interviewer error and
refusals when conducting a PCA or fitting a model.

Best,
Alex

On Mon, May 24, 2021 at 2:03 PM Adrian Du?a <dusa.adrian at gmail.com> wrote:

> On Mon, May 24, 2021 at 1:31 PM Tomas Kalibera <tomas.kalibera at gmail.com>
> wrote:
>
> > [...]
> >
> > For the reasons I explained, I would be against such a change. Keeping
> the
> > data on the side, as also recommended by others on this list, would allow
> > you for a reliable implementation. I don't want to support fragile
> package
> > code building on unspecified R internals, and in this case particularly
> > internals that themselves have not stood the test of time, so are at high
> > risk of change.
> >
> I understand, and it makes sense.
> We'll have to wait for the R internals to settle (this really is
> surprising, I wonder how other software have solved this). In the meantime,
> I will probably go ahead with NaNs.
>
> Thank you again,
> Adrian
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


-- 
Alexander Bertram
Technical Director
*BeDataDriven BV*

Web: http://bedatadriven.com
Email: alex at bedatadriven.com
Tel. Nederlands: +31(0)647205388
Skype: akbertram

	[[alternative HTML version deleted]]


From du@@@@dr|@n @end|ng |rom un|buc@ro  Mon May 24 14:18:06 2021
From: du@@@@dr|@n @end|ng |rom un|buc@ro (=?UTF-8?B?QWRyaWFuIER1yJlh?=)
Date: Mon, 24 May 2021 15:18:06 +0300
Subject: [Rd] 1954 from NA
In-Reply-To: <3443246.1621854690@apollo2.minshall.org>
References: <CAJ=0CtCfRMJM6wq1ggkSv2BNfX+CGnW2N-SZAN8poOG3ev=nUA@mail.gmail.com>
 <3443246.1621854690@apollo2.minshall.org>
Message-ID: <CAJ=0CtDOuY+tVf+YxO0gdGY9XgaTv_Q4kMP2p_Z8CxrnHjT4-w@mail.gmail.com>

On Mon, May 24, 2021 at 2:11 PM Greg Minshall <minshall at umich.edu> wrote:

> [...]
> if you have 500 columns of possibly-NA'd variables, you could have one
> column of 500 "bits", where each bit has one of N values, N being the
> number of explanations the corresponding column has for why the NA
> exists.
>

The mere thought of implementing something like that gives me shivers. Not
to mention such a solution should also be robust when subsetting,
splitting, column and row binding, etc. and everything can be lost if the
user deletes that particular column without realising its importance.

Social science datasets are much more alive and complex than one might
first think: there are multi-wave studies with tens of countries, and
aggregating such data is already a complex process to add even more
complexity on top of that.

As undocumented as they may be, or even subject to change, I think the R
internals are much more reliable that this.

Best wishes,
Adrian

-- 
Adrian Dusa
University of Bucharest
Romanian Social Data Archive
Soseaua Panduri nr. 90-92
050663 Bucharest sector 5
Romania
https://adriandusa.eu

	[[alternative HTML version deleted]]


From du@@@@dr|@n @end|ng |rom gm@||@com  Mon May 24 15:09:21 2021
From: du@@@@dr|@n @end|ng |rom gm@||@com (=?UTF-8?B?QWRyaWFuIER1yJlh?=)
Date: Mon, 24 May 2021 16:09:21 +0300
Subject: [Rd] 1954 from NA
In-Reply-To: <CAOdV3zARBg=TDTDPZMuzWSOqi=Wi3yShRxrYTbAYM9njb4Bx0A@mail.gmail.com>
References: <CAJ=0CtDvHor+DZsmagY07CEjayGGTy+srb8aZqutLFLrTamfJg@mail.gmail.com>
 <eede7930-8b70-a0a7-2c14-9b651878ecc6@gmail.com>
 <CAJ=0CtCprzMyNzjuhneBxyWG1X3g1KDQ+TJE3J6QuhVXG8wZpA@mail.gmail.com>
 <0aa3352e-62a0-a8b9-fa6a-a3a994ab8e55@gmail.com>
 <CAJ=0CtBJY8HcxzPX=5N65uXE410mQhnNDoG+dr7hYCtZhaTsQA@mail.gmail.com>
 <45cb76eb-40e9-f9e1-8dd3-c9042b4e529a@gmail.com>
 <CAJ=0CtBr13cxOwKdD4HyAFCfwDhM3NVEfvarZigv+6TQBPjzDQ@mail.gmail.com>
 <CAOdV3zARBg=TDTDPZMuzWSOqi=Wi3yShRxrYTbAYM9njb4Bx0A@mail.gmail.com>
Message-ID: <CAJ=0CtBjqY629fxfUfapS-3ccO1XRMh5HAa1WPOW=rMuYCbijg@mail.gmail.com>

Dear Alex,

Thanks for piping in, I am learning with each new message.
The problem is clear, the solution escapes me though. I've already tried
the attributes route: it is going to triple the data size: along with the
additional (logical) variable that specifies which level is missing, one
also needs to store an index such that sorting the data would still
maintain the correct information.

One also needs to think about subsetting (subset the attributes as well),
splitting (the same), aggregating multiple datasets (even more attention),
creating custom vectors out of multiple variables... complexity quickly
grows towards infinity.

R factors are nice indeed, but:
- there are numerical variables which can hold multiple missing values (for
instance income)
- factors convert the original questionnaire values: if a missing value was
coded 999, turning that into a factor would convert that value into
something else

I really, and wholeheartedly, do appreciate all advice: but please be
assured that I have been thinking about this for more than 10 years and
still haven't found a satisfactory solution.

Which makes it even more intriguing, since other software like SAS or Stata
have solved this for decades: what is their implementation, and how come
they don't seem to be affected by the new M1 architecture?
When package "haven" introduced the tagged NA values I said: ah-haa... so
that is how it's done... only to learn that implementation is just as
fragile as the R internals.

There really should be a robust solution for this seemingly mundane
problem, but apparently is far from mundane...

Best wishes,
Adrian


On Mon, May 24, 2021 at 3:29 PM Bertram, Alexander <alex at bedatadriven.com>
wrote:

> Dear Adrian,
> I just wanted to pipe in and underscore Thomas' point: the payload bits of
> IEEE 754 floating point values are no place to store data that you care
> about or need to keep. That is not only related to the R APIs, but also how
> processors handle floating point values and signaling and non-signaling
> NaNs. It is very difficult to reason about when and under which
> circumstances these bits are preserved. I spent a lot of time working on
> Renjin's handling of these values and I can assure that any such scheme
> will end in tears.
>
> A far, far better option is to use R's attributes to store this kind of
> metadata. This is exactly what this language feature is for. There is
> already a standard 'levels' attribute that holds the labels of factors like
> "Yes", "No" , "Refused", "Interviewer error'' etc. In the past, I've worked
> on projects where we stored an additional attribute like "missingLevels"
> that stores extra metadata on which levels should be used in which kind of
> analysis. That way, you can preserve all the information, and then write a
> utility function which automatically applies certain logic to a whole
> dataframe just before passing the data to an analysis function. This is
> also important because in surveys like this, different values should be
> excluded at different times. For example, you might want to include all
> responses in a data quality report, but exclude interviewer error and
> refusals when conducting a PCA or fitting a model.
>
> Best,
> Alex
>
> On Mon, May 24, 2021 at 2:03 PM Adrian Du?a <dusa.adrian at gmail.com> wrote:
>
>> On Mon, May 24, 2021 at 1:31 PM Tomas Kalibera <tomas.kalibera at gmail.com>
>> wrote:
>>
>> > [...]
>> >
>> > For the reasons I explained, I would be against such a change. Keeping
>> the
>> > data on the side, as also recommended by others on this list, would
>> allow
>> > you for a reliable implementation. I don't want to support fragile
>> package
>> > code building on unspecified R internals, and in this case particularly
>> > internals that themselves have not stood the test of time, so are at
>> high
>> > risk of change.
>> >
>> I understand, and it makes sense.
>> We'll have to wait for the R internals to settle (this really is
>> surprising, I wonder how other software have solved this). In the
>> meantime,
>> I will probably go ahead with NaNs.
>>
>> Thank you again,
>> Adrian
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>
>
> --
> Alexander Bertram
> Technical Director
> *BeDataDriven BV*
>
> Web: http://bedatadriven.com
> Email: alex at bedatadriven.com
> Tel. Nederlands: +31(0)647205388
> Skype: akbertram
>

	[[alternative HTML version deleted]]


From iuke-tier@ey m@iii@g oii uiow@@edu  Mon May 24 15:14:56 2021
From: iuke-tier@ey m@iii@g oii uiow@@edu (iuke-tier@ey m@iii@g oii uiow@@edu)
Date: Mon, 24 May 2021 08:14:56 -0500 (CDT)
Subject: [Rd] [External] Re:  1954 from NA
In-Reply-To: <CAJ=0CtDOuY+tVf+YxO0gdGY9XgaTv_Q4kMP2p_Z8CxrnHjT4-w@mail.gmail.com>
References: <CAJ=0CtCfRMJM6wq1ggkSv2BNfX+CGnW2N-SZAN8poOG3ev=nUA@mail.gmail.com>
 <3443246.1621854690@apollo2.minshall.org>
 <CAJ=0CtDOuY+tVf+YxO0gdGY9XgaTv_Q4kMP2p_Z8CxrnHjT4-w@mail.gmail.com>
Message-ID: <alpine.DEB.2.21.2105240805580.3254@luke-Latitude-7480>

On Mon, 24 May 2021, Adrian Du?a wrote:

> On Mon, May 24, 2021 at 2:11 PM Greg Minshall <minshall at umich.edu> wrote:
>
>> [...]
>> if you have 500 columns of possibly-NA'd variables, you could have one
>> column of 500 "bits", where each bit has one of N values, N being the
>> number of explanations the corresponding column has for why the NA
>> exists.
>>

PLEASE DO NOT DO THIS!

It will not work reliably, as has been explained to you ad nauseam in
this thread.

If you distribute code that does this it will only lead to bug reports
on R that will waste R-core time.

As Alex explained, you can use attributes for this. If you need
operations to preserve attributes across subsetting you can define
subsetting methods that do that.

If you are dead set on doing something in C you can try to develop an
ALTREP class that provides augmented missing value information.

Best,

luke



>
> The mere thought of implementing something like that gives me shivers. Not
> to mention such a solution should also be robust when subsetting,
> splitting, column and row binding, etc. and everything can be lost if the
> user deletes that particular column without realising its importance.
>
> Social science datasets are much more alive and complex than one might
> first think: there are multi-wave studies with tens of countries, and
> aggregating such data is already a complex process to add even more
> complexity on top of that.
>
> As undocumented as they may be, or even subject to change, I think the R
> internals are much more reliable that this.
>
> Best wishes,
> Adrian
>
>

-- 
Luke Tierney
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu

From t@r@@@z@kh@rko @end|ng |rom uzh@ch  Mon May 24 15:18:47 2021
From: t@r@@@z@kh@rko @end|ng |rom uzh@ch (Taras Zakharko)
Date: Mon, 24 May 2021 15:18:47 +0200
Subject: [Rd] 1954 from NA
In-Reply-To: <CAJ=0CtBjqY629fxfUfapS-3ccO1XRMh5HAa1WPOW=rMuYCbijg@mail.gmail.com>
References: <CAJ=0CtDvHor+DZsmagY07CEjayGGTy+srb8aZqutLFLrTamfJg@mail.gmail.com>
 <eede7930-8b70-a0a7-2c14-9b651878ecc6@gmail.com>
 <CAJ=0CtCprzMyNzjuhneBxyWG1X3g1KDQ+TJE3J6QuhVXG8wZpA@mail.gmail.com>
 <0aa3352e-62a0-a8b9-fa6a-a3a994ab8e55@gmail.com>
 <CAJ=0CtBJY8HcxzPX=5N65uXE410mQhnNDoG+dr7hYCtZhaTsQA@mail.gmail.com>
 <45cb76eb-40e9-f9e1-8dd3-c9042b4e529a@gmail.com>
 <CAJ=0CtBr13cxOwKdD4HyAFCfwDhM3NVEfvarZigv+6TQBPjzDQ@mail.gmail.com>
 <CAOdV3zARBg=TDTDPZMuzWSOqi=Wi3yShRxrYTbAYM9njb4Bx0A@mail.gmail.com>
 <CAJ=0CtBjqY629fxfUfapS-3ccO1XRMh5HAa1WPOW=rMuYCbijg@mail.gmail.com>
Message-ID: <D2F37634-758C-4CD3-91A0-43172F074BD7@uzh.ch>

Hi Adrian, 

Have a look at vctrs package ? they have low-level primitives that might simplify your life a bit. I think you can get quite far by creating a custom type that stores NAs in an attribute and utilizes vctrs proxy functionality to preserve these attributes across different operations. Going that route will likely to give you a much more flexible and robust solution. 

Best, 

Taras

> On 24 May 2021, at 15:09, Adrian Du?a <dusa.adrian at gmail.com> wrote:
> 
> Dear Alex,
> 
> Thanks for piping in, I am learning with each new message.
> The problem is clear, the solution escapes me though. I've already tried
> the attributes route: it is going to triple the data size: along with the
> additional (logical) variable that specifies which level is missing, one
> also needs to store an index such that sorting the data would still
> maintain the correct information.
> 
> One also needs to think about subsetting (subset the attributes as well),
> splitting (the same), aggregating multiple datasets (even more attention),
> creating custom vectors out of multiple variables... complexity quickly
> grows towards infinity.
> 
> R factors are nice indeed, but:
> - there are numerical variables which can hold multiple missing values (for
> instance income)
> - factors convert the original questionnaire values: if a missing value was
> coded 999, turning that into a factor would convert that value into
> something else
> 
> I really, and wholeheartedly, do appreciate all advice: but please be
> assured that I have been thinking about this for more than 10 years and
> still haven't found a satisfactory solution.
> 
> Which makes it even more intriguing, since other software like SAS or Stata
> have solved this for decades: what is their implementation, and how come
> they don't seem to be affected by the new M1 architecture?
> When package "haven" introduced the tagged NA values I said: ah-haa... so
> that is how it's done... only to learn that implementation is just as
> fragile as the R internals.
> 
> There really should be a robust solution for this seemingly mundane
> problem, but apparently is far from mundane...
> 
> Best wishes,
> Adrian
> 
> 
> On Mon, May 24, 2021 at 3:29 PM Bertram, Alexander <alex at bedatadriven.com>
> wrote:
> 
>> Dear Adrian,
>> I just wanted to pipe in and underscore Thomas' point: the payload bits of
>> IEEE 754 floating point values are no place to store data that you care
>> about or need to keep. That is not only related to the R APIs, but also how
>> processors handle floating point values and signaling and non-signaling
>> NaNs. It is very difficult to reason about when and under which
>> circumstances these bits are preserved. I spent a lot of time working on
>> Renjin's handling of these values and I can assure that any such scheme
>> will end in tears.
>> 
>> A far, far better option is to use R's attributes to store this kind of
>> metadata. This is exactly what this language feature is for. There is
>> already a standard 'levels' attribute that holds the labels of factors like
>> "Yes", "No" , "Refused", "Interviewer error'' etc. In the past, I've worked
>> on projects where we stored an additional attribute like "missingLevels"
>> that stores extra metadata on which levels should be used in which kind of
>> analysis. That way, you can preserve all the information, and then write a
>> utility function which automatically applies certain logic to a whole
>> dataframe just before passing the data to an analysis function. This is
>> also important because in surveys like this, different values should be
>> excluded at different times. For example, you might want to include all
>> responses in a data quality report, but exclude interviewer error and
>> refusals when conducting a PCA or fitting a model.
>> 
>> Best,
>> Alex
>> 
>> On Mon, May 24, 2021 at 2:03 PM Adrian Du?a <dusa.adrian at gmail.com> wrote:
>> 
>>> On Mon, May 24, 2021 at 1:31 PM Tomas Kalibera <tomas.kalibera at gmail.com>
>>> wrote:
>>> 
>>>> [...]
>>>> 
>>>> For the reasons I explained, I would be against such a change. Keeping
>>> the
>>>> data on the side, as also recommended by others on this list, would
>>> allow
>>>> you for a reliable implementation. I don't want to support fragile
>>> package
>>>> code building on unspecified R internals, and in this case particularly
>>>> internals that themselves have not stood the test of time, so are at
>>> high
>>>> risk of change.
>>>> 
>>> I understand, and it makes sense.
>>> We'll have to wait for the R internals to settle (this really is
>>> surprising, I wonder how other software have solved this). In the
>>> meantime,
>>> I will probably go ahead with NaNs.
>>> 
>>> Thank you again,
>>> Adrian
>>> 
>>>        [[alternative HTML version deleted]]
>>> 
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>> 
>> 
>> 
>> --
>> Alexander Bertram
>> Technical Director
>> *BeDataDriven BV*
>> 
>> Web: http://bedatadriven.com
>> Email: alex at bedatadriven.com
>> Tel. Nederlands: +31(0)647205388
>> Skype: akbertram
>> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From @|ex @end|ng |rom bed@t@dr|ven@com  Mon May 24 15:34:22 2021
From: @|ex @end|ng |rom bed@t@dr|ven@com (Bertram, Alexander)
Date: Mon, 24 May 2021 15:34:22 +0200
Subject: [Rd] 1954 from NA
In-Reply-To: <D2F37634-758C-4CD3-91A0-43172F074BD7@uzh.ch>
References: <CAJ=0CtDvHor+DZsmagY07CEjayGGTy+srb8aZqutLFLrTamfJg@mail.gmail.com>
 <eede7930-8b70-a0a7-2c14-9b651878ecc6@gmail.com>
 <CAJ=0CtCprzMyNzjuhneBxyWG1X3g1KDQ+TJE3J6QuhVXG8wZpA@mail.gmail.com>
 <0aa3352e-62a0-a8b9-fa6a-a3a994ab8e55@gmail.com>
 <CAJ=0CtBJY8HcxzPX=5N65uXE410mQhnNDoG+dr7hYCtZhaTsQA@mail.gmail.com>
 <45cb76eb-40e9-f9e1-8dd3-c9042b4e529a@gmail.com>
 <CAJ=0CtBr13cxOwKdD4HyAFCfwDhM3NVEfvarZigv+6TQBPjzDQ@mail.gmail.com>
 <CAOdV3zARBg=TDTDPZMuzWSOqi=Wi3yShRxrYTbAYM9njb4Bx0A@mail.gmail.com>
 <CAJ=0CtBjqY629fxfUfapS-3ccO1XRMh5HAa1WPOW=rMuYCbijg@mail.gmail.com>
 <D2F37634-758C-4CD3-91A0-43172F074BD7@uzh.ch>
Message-ID: <CAOdV3zChZa=sYuLqaumGK6PHd8dFmuy7g2MsX4PscRDubpjFfw@mail.gmail.com>

Dear Adrian,
SPSS and other packages handle this problem in a very similar way to what I
described: they store additional metadata for each variable. You can see
this in the way that SPSS organizes it's file format: each "variable" has
additional metadata that indicate how specific values of the variable,
encoded as an integer or a floating point should be handled in analysis.
Before you actually run a crosstab in SPSS, the metadata is (presumably)
applied to the raw data to arrive at an in memory buffer on which the
actual model is fitted, etc.

The 20 line solution in R looks like this:


df <- data.frame(q1 = c(1, 10, 50, 999), q2 = c("Yes", "No", "Don't know",
"Interviewer napping"), stringsAsFactors = FALSE)
attr(df$q1, 'missing') <- 999
attr(df$q2, 'missing') <- c("Don't know", "Interviewer napping")

excludeMissing <- function(df) {
  for(q in names(df)) {
    v <- df[[q]]
    mv <- attr(v, 'missing')
    if(!is.null(mv)) {
      df[[q]] <- ifelse(v %in% mv, NA, v)
    }
  }
  df
}

table(excludeMissing(df))

If you want to preserve the missing attribute when subsetting the vectors
then you will have to take the example further by adding a class and
`[.withMissing` functions. This might bring the whole project to a few
hundred lines, but the rules that apply here are well defined and well
understood, giving you a proper basis on which to build. And perhaps the
vctrs package might make this even simpler, take a look.

Best,
Alex

On Mon, May 24, 2021 at 3:20 PM Taras Zakharko <taras.zakharko at uzh.ch>
wrote:

> Hi Adrian,
>
> Have a look at vctrs package ? they have low-level primitives that might
> simplify your life a bit. I think you can get quite far by creating a
> custom type that stores NAs in an attribute and utilizes vctrs proxy
> functionality to preserve these attributes across different operations.
> Going that route will likely to give you a much more flexible and robust
> solution.
>
> Best,
>
> Taras
>
> > On 24 May 2021, at 15:09, Adrian Du?a <dusa.adrian at gmail.com> wrote:
> >
> > Dear Alex,
> >
> > Thanks for piping in, I am learning with each new message.
> > The problem is clear, the solution escapes me though. I've already tried
> > the attributes route: it is going to triple the data size: along with the
> > additional (logical) variable that specifies which level is missing, one
> > also needs to store an index such that sorting the data would still
> > maintain the correct information.
> >
> > One also needs to think about subsetting (subset the attributes as well),
> > splitting (the same), aggregating multiple datasets (even more
> attention),
> > creating custom vectors out of multiple variables... complexity quickly
> > grows towards infinity.
> >
> > R factors are nice indeed, but:
> > - there are numerical variables which can hold multiple missing values
> (for
> > instance income)
> > - factors convert the original questionnaire values: if a missing value
> was
> > coded 999, turning that into a factor would convert that value into
> > something else
> >
> > I really, and wholeheartedly, do appreciate all advice: but please be
> > assured that I have been thinking about this for more than 10 years and
> > still haven't found a satisfactory solution.
> >
> > Which makes it even more intriguing, since other software like SAS or
> Stata
> > have solved this for decades: what is their implementation, and how come
> > they don't seem to be affected by the new M1 architecture?
> > When package "haven" introduced the tagged NA values I said: ah-haa... so
> > that is how it's done... only to learn that implementation is just as
> > fragile as the R internals.
> >
> > There really should be a robust solution for this seemingly mundane
> > problem, but apparently is far from mundane...
> >
> > Best wishes,
> > Adrian
> >
> >
> > On Mon, May 24, 2021 at 3:29 PM Bertram, Alexander <
> alex at bedatadriven.com>
> > wrote:
> >
> >> Dear Adrian,
> >> I just wanted to pipe in and underscore Thomas' point: the payload bits
> of
> >> IEEE 754 floating point values are no place to store data that you care
> >> about or need to keep. That is not only related to the R APIs, but also
> how
> >> processors handle floating point values and signaling and non-signaling
> >> NaNs. It is very difficult to reason about when and under which
> >> circumstances these bits are preserved. I spent a lot of time working on
> >> Renjin's handling of these values and I can assure that any such scheme
> >> will end in tears.
> >>
> >> A far, far better option is to use R's attributes to store this kind of
> >> metadata. This is exactly what this language feature is for. There is
> >> already a standard 'levels' attribute that holds the labels of factors
> like
> >> "Yes", "No" , "Refused", "Interviewer error'' etc. In the past, I've
> worked
> >> on projects where we stored an additional attribute like "missingLevels"
> >> that stores extra metadata on which levels should be used in which kind
> of
> >> analysis. That way, you can preserve all the information, and then
> write a
> >> utility function which automatically applies certain logic to a whole
> >> dataframe just before passing the data to an analysis function. This is
> >> also important because in surveys like this, different values should be
> >> excluded at different times. For example, you might want to include all
> >> responses in a data quality report, but exclude interviewer error and
> >> refusals when conducting a PCA or fitting a model.
> >>
> >> Best,
> >> Alex
> >>
> >> On Mon, May 24, 2021 at 2:03 PM Adrian Du?a <dusa.adrian at gmail.com>
> wrote:
> >>
> >>> On Mon, May 24, 2021 at 1:31 PM Tomas Kalibera <
> tomas.kalibera at gmail.com>
> >>> wrote:
> >>>
> >>>> [...]
> >>>>
> >>>> For the reasons I explained, I would be against such a change. Keeping
> >>> the
> >>>> data on the side, as also recommended by others on this list, would
> >>> allow
> >>>> you for a reliable implementation. I don't want to support fragile
> >>> package
> >>>> code building on unspecified R internals, and in this case
> particularly
> >>>> internals that themselves have not stood the test of time, so are at
> >>> high
> >>>> risk of change.
> >>>>
> >>> I understand, and it makes sense.
> >>> We'll have to wait for the R internals to settle (this really is
> >>> surprising, I wonder how other software have solved this). In the
> >>> meantime,
> >>> I will probably go ahead with NaNs.
> >>>
> >>> Thank you again,
> >>> Adrian
> >>>
> >>>        [[alternative HTML version deleted]]
> >>>
> >>> ______________________________________________
> >>> R-devel at r-project.org mailing list
> >>> https://stat.ethz.ch/mailman/listinfo/r-devel
> >>>
> >>
> >>
> >> --
> >> Alexander Bertram
> >> Technical Director
> >> *BeDataDriven BV*
> >>
> >> Web: http://bedatadriven.com
> >> Email: alex at bedatadriven.com
> >> Tel. Nederlands: +31(0)647205388
> >> Skype: akbertram
> >>
> >
> >       [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


-- 
Alexander Bertram
Technical Director
*BeDataDriven BV*

Web: http://bedatadriven.com
Email: alex at bedatadriven.com
Tel. Nederlands: +31(0)647205388
Skype: akbertram

	[[alternative HTML version deleted]]


From du@@@@dr|@n @end|ng |rom gm@||@com  Mon May 24 16:30:14 2021
From: du@@@@dr|@n @end|ng |rom gm@||@com (=?UTF-8?B?QWRyaWFuIER1yJlh?=)
Date: Mon, 24 May 2021 17:30:14 +0300
Subject: [Rd] 1954 from NA
In-Reply-To: <D2F37634-758C-4CD3-91A0-43172F074BD7@uzh.ch>
References: <CAJ=0CtDvHor+DZsmagY07CEjayGGTy+srb8aZqutLFLrTamfJg@mail.gmail.com>
 <eede7930-8b70-a0a7-2c14-9b651878ecc6@gmail.com>
 <CAJ=0CtCprzMyNzjuhneBxyWG1X3g1KDQ+TJE3J6QuhVXG8wZpA@mail.gmail.com>
 <0aa3352e-62a0-a8b9-fa6a-a3a994ab8e55@gmail.com>
 <CAJ=0CtBJY8HcxzPX=5N65uXE410mQhnNDoG+dr7hYCtZhaTsQA@mail.gmail.com>
 <45cb76eb-40e9-f9e1-8dd3-c9042b4e529a@gmail.com>
 <CAJ=0CtBr13cxOwKdD4HyAFCfwDhM3NVEfvarZigv+6TQBPjzDQ@mail.gmail.com>
 <CAOdV3zARBg=TDTDPZMuzWSOqi=Wi3yShRxrYTbAYM9njb4Bx0A@mail.gmail.com>
 <CAJ=0CtBjqY629fxfUfapS-3ccO1XRMh5HAa1WPOW=rMuYCbijg@mail.gmail.com>
 <D2F37634-758C-4CD3-91A0-43172F074BD7@uzh.ch>
Message-ID: <CAJ=0CtBFNR_=fCbg49B6-fdmprMf6p5gicP0Vq8M_ryqkAfkyQ@mail.gmail.com>

Hi Taras,

On Mon, May 24, 2021 at 4:20 PM Taras Zakharko <taras.zakharko at uzh.ch>
wrote:

> Hi Adrian,
>
> Have a look at vctrs package ? they have low-level primitives that might
> simplify your life a bit. I think you can get quite far by creating a
> custom type that stores NAs in an attribute and utilizes vctrs proxy
> functionality to preserve these attributes across different operations.
> Going that route will likely to give you a much more flexible and robust
> solution.
>

Yes I am well aware of the primitives from package vctrs, since package
haven itself uses the vctrs_vctr class.
They're doing a very interesting work, albeit not a solution for this
particular problem.

A.

	[[alternative HTML version deleted]]


From m|n@h@|| @end|ng |rom um|ch@edu  Mon May 24 16:32:35 2021
From: m|n@h@|| @end|ng |rom um|ch@edu (Greg Minshall)
Date: Mon, 24 May 2021 17:32:35 +0300
Subject: [Rd] [External] Re:  1954 from NA
In-Reply-To: Your message of "Mon, 24 May 2021 08:14:56 -0500."
 <alpine.DEB.2.21.2105240805580.3254@luke-Latitude-7480>
Message-ID: <3569883.1621866755@apollo2.minshall.org>

luke,

> PLEASE DO NOT DO THIS!

very happy to withdraw my offered alternative!

cheers, Greg


From du@@@@dr|@n @end|ng |rom gm@||@com  Mon May 24 16:47:06 2021
From: du@@@@dr|@n @end|ng |rom gm@||@com (=?UTF-8?B?QWRyaWFuIER1yJlh?=)
Date: Mon, 24 May 2021 17:47:06 +0300
Subject: [Rd] 1954 from NA
In-Reply-To: <CAOdV3zChZa=sYuLqaumGK6PHd8dFmuy7g2MsX4PscRDubpjFfw@mail.gmail.com>
References: <CAJ=0CtDvHor+DZsmagY07CEjayGGTy+srb8aZqutLFLrTamfJg@mail.gmail.com>
 <eede7930-8b70-a0a7-2c14-9b651878ecc6@gmail.com>
 <CAJ=0CtCprzMyNzjuhneBxyWG1X3g1KDQ+TJE3J6QuhVXG8wZpA@mail.gmail.com>
 <0aa3352e-62a0-a8b9-fa6a-a3a994ab8e55@gmail.com>
 <CAJ=0CtBJY8HcxzPX=5N65uXE410mQhnNDoG+dr7hYCtZhaTsQA@mail.gmail.com>
 <45cb76eb-40e9-f9e1-8dd3-c9042b4e529a@gmail.com>
 <CAJ=0CtBr13cxOwKdD4HyAFCfwDhM3NVEfvarZigv+6TQBPjzDQ@mail.gmail.com>
 <CAOdV3zARBg=TDTDPZMuzWSOqi=Wi3yShRxrYTbAYM9njb4Bx0A@mail.gmail.com>
 <CAJ=0CtBjqY629fxfUfapS-3ccO1XRMh5HAa1WPOW=rMuYCbijg@mail.gmail.com>
 <D2F37634-758C-4CD3-91A0-43172F074BD7@uzh.ch>
 <CAOdV3zChZa=sYuLqaumGK6PHd8dFmuy7g2MsX4PscRDubpjFfw@mail.gmail.com>
Message-ID: <CAJ=0CtC8s+8LjE3HDCsHpk8rw99R5tJKqLS2obg3mm=-PZMdNA@mail.gmail.com>

On Mon, May 24, 2021 at 4:40 PM Bertram, Alexander via R-devel <
r-devel at r-project.org> wrote:

> Dear Adrian,
> SPSS and other packages handle this problem in a very similar way to what I
> described: they store additional metadata for each variable. You can see
> this in the way that SPSS organizes it's file format: each "variable" has
> additional metadata that indicate how specific values of the variable,
> encoded as an integer or a floating point should be handled in analysis.
> Before you actually run a crosstab in SPSS, the metadata is (presumably)
> applied to the raw data to arrive at an in memory buffer on which the
> actual model is fitted, etc.
>

As far as I am aware, SAS and Stata use "very high" and "very low" values
to signal a missing value. Basically, the same solution using a different
sign bit (not creating attributes metadata, though).

Something similar to the IEEE-754 representation for the NaN:
0x7ff0000000000000

only using some other "high" word:
0x7fe0000000000000

If I understand this correctly, compilers are likely to mess around with
the payload from the 0x7ff0... stuff, which endangers even the most basic R
structure like a real NA.
Perhaps using a different high word such as 0x7fe would be stable, since
compilers won't confuse it with a NaN. And then any payload would be "safe"
for any specific purpose.

Not sure how SPSS manage its internals, but if they do it that way they
manage it in a standard procedural way. Now, since R's NA payload is at
risk, and if your solution is "good" for specific social science missing
data, would you recommend R creators to adopt it for a regular NA...?

We're looking for a general purpose solution that would create as little
additional work as possible for the end users. Your solution is already
implemented in the package "labelled" with the function user_na_to_na()
before doing any statistical analysis. That still requires users to pay
attention to details which the software should take care of automatically.

Best,
Adrian

The 20 line solution in R looks like this:
>
>
> df <- data.frame(q1 = c(1, 10, 50, 999), q2 = c("Yes", "No", "Don't know",
> "Interviewer napping"), stringsAsFactors = FALSE)
> attr(df$q1, 'missing') <- 999
> attr(df$q2, 'missing') <- c("Don't know", "Interviewer napping")
>
> excludeMissing <- function(df) {
>   for(q in names(df)) {
>     v <- df[[q]]
>     mv <- attr(v, 'missing')
>     if(!is.null(mv)) {
>       df[[q]] <- ifelse(v %in% mv, NA, v)
>     }
>   }
>   df
> }
>
> table(excludeMissing(df))
>
> If you want to preserve the missing attribute when subsetting the vectors
> then you will have to take the example further by adding a class and
> `[.withMissing` functions. This might bring the whole project to a few
> hundred lines, but the rules that apply here are well defined and well
> understood, giving you a proper basis on which to build. And perhaps the
> vctrs package might make this even simpler, take a look.
>
> Best,
> Alex
>
>

	[[alternative HTML version deleted]]


From g@bembecker @end|ng |rom gm@||@com  Mon May 24 16:47:28 2021
From: g@bembecker @end|ng |rom gm@||@com (Gabriel Becker)
Date: Mon, 24 May 2021 07:47:28 -0700
Subject: [Rd] 1954 from NA
In-Reply-To: <CAJ=0CtBFNR_=fCbg49B6-fdmprMf6p5gicP0Vq8M_ryqkAfkyQ@mail.gmail.com>
References: <CAJ=0CtDvHor+DZsmagY07CEjayGGTy+srb8aZqutLFLrTamfJg@mail.gmail.com>
 <eede7930-8b70-a0a7-2c14-9b651878ecc6@gmail.com>
 <CAJ=0CtCprzMyNzjuhneBxyWG1X3g1KDQ+TJE3J6QuhVXG8wZpA@mail.gmail.com>
 <0aa3352e-62a0-a8b9-fa6a-a3a994ab8e55@gmail.com>
 <CAJ=0CtBJY8HcxzPX=5N65uXE410mQhnNDoG+dr7hYCtZhaTsQA@mail.gmail.com>
 <45cb76eb-40e9-f9e1-8dd3-c9042b4e529a@gmail.com>
 <CAJ=0CtBr13cxOwKdD4HyAFCfwDhM3NVEfvarZigv+6TQBPjzDQ@mail.gmail.com>
 <CAOdV3zARBg=TDTDPZMuzWSOqi=Wi3yShRxrYTbAYM9njb4Bx0A@mail.gmail.com>
 <CAJ=0CtBjqY629fxfUfapS-3ccO1XRMh5HAa1WPOW=rMuYCbijg@mail.gmail.com>
 <D2F37634-758C-4CD3-91A0-43172F074BD7@uzh.ch>
 <CAJ=0CtBFNR_=fCbg49B6-fdmprMf6p5gicP0Vq8M_ryqkAfkyQ@mail.gmail.com>
Message-ID: <CAD4oTHHFKoZdP6NJ6+qcwYohgOdKxivdbM=vHA_ZydRsaLe_og@mail.gmail.com>

Hi Adrian,

I had the same thought as Luke. It is possible that you can develop an
ALTREP that carries around the tagging information you're looking for in a
way that is more persistent (in some cases) than R-level attributes and
more hidden than additional user-visible columns.

The downsides to this, of course, is that you'll in some sense be doing the
same "extra vector for each vector you want tagged NA-s within" under the
hood, and that only custom machinery you write will recognize things as
something other than bog-standard NAs/NaNs.  You'll also have some problems
with the fact that data in ALTREPs isn't currently modifiable without
losing ALTREPness. That said, ALTREPs are allowed to carry around arbitrary
persistent information with them, so from that perspective making an ALTREP
that carries around a "meaning of my NAs" vector of tags in its metadata
would be pretty straightforward.

Best,
~G

On Mon, May 24, 2021 at 7:30 AM Adrian Du?a <dusa.adrian at gmail.com> wrote:

> Hi Taras,
>
> On Mon, May 24, 2021 at 4:20 PM Taras Zakharko <taras.zakharko at uzh.ch>
> wrote:
>
> > Hi Adrian,
> >
> > Have a look at vctrs package ? they have low-level primitives that might
> > simplify your life a bit. I think you can get quite far by creating a
> > custom type that stores NAs in an attribute and utilizes vctrs proxy
> > functionality to preserve these attributes across different operations.
> > Going that route will likely to give you a much more flexible and robust
> > solution.
> >
>
> Yes I am well aware of the primitives from package vctrs, since package
> haven itself uses the vctrs_vctr class.
> They're doing a very interesting work, albeit not a solution for this
> particular problem.
>
> A.
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From du@@@@dr|@n @end|ng |rom gm@||@com  Mon May 24 17:15:46 2021
From: du@@@@dr|@n @end|ng |rom gm@||@com (=?UTF-8?B?QWRyaWFuIER1yJlh?=)
Date: Mon, 24 May 2021 18:15:46 +0300
Subject: [Rd] 1954 from NA
In-Reply-To: <CAD4oTHHFKoZdP6NJ6+qcwYohgOdKxivdbM=vHA_ZydRsaLe_og@mail.gmail.com>
References: <CAJ=0CtDvHor+DZsmagY07CEjayGGTy+srb8aZqutLFLrTamfJg@mail.gmail.com>
 <eede7930-8b70-a0a7-2c14-9b651878ecc6@gmail.com>
 <CAJ=0CtCprzMyNzjuhneBxyWG1X3g1KDQ+TJE3J6QuhVXG8wZpA@mail.gmail.com>
 <0aa3352e-62a0-a8b9-fa6a-a3a994ab8e55@gmail.com>
 <CAJ=0CtBJY8HcxzPX=5N65uXE410mQhnNDoG+dr7hYCtZhaTsQA@mail.gmail.com>
 <45cb76eb-40e9-f9e1-8dd3-c9042b4e529a@gmail.com>
 <CAJ=0CtBr13cxOwKdD4HyAFCfwDhM3NVEfvarZigv+6TQBPjzDQ@mail.gmail.com>
 <CAOdV3zARBg=TDTDPZMuzWSOqi=Wi3yShRxrYTbAYM9njb4Bx0A@mail.gmail.com>
 <CAJ=0CtBjqY629fxfUfapS-3ccO1XRMh5HAa1WPOW=rMuYCbijg@mail.gmail.com>
 <D2F37634-758C-4CD3-91A0-43172F074BD7@uzh.ch>
 <CAJ=0CtBFNR_=fCbg49B6-fdmprMf6p5gicP0Vq8M_ryqkAfkyQ@mail.gmail.com>
 <CAD4oTHHFKoZdP6NJ6+qcwYohgOdKxivdbM=vHA_ZydRsaLe_og@mail.gmail.com>
Message-ID: <CAJ=0CtDohu34qVJGB5TUkVK1gErw8CEwXgenVypBLcKeOjBZ_g@mail.gmail.com>

On Mon, May 24, 2021 at 5:47 PM Gabriel Becker <gabembecker at gmail.com>
wrote:

> Hi Adrian,
>
> I had the same thought as Luke. It is possible that you can develop an
> ALTREP that carries around the tagging information you're looking for in a
> way that is more persistent (in some cases) than R-level attributes and
> more hidden than additional user-visible columns.
>
> The downsides to this, of course, is that you'll in some sense be doing
> the same "extra vector for each vector you want tagged NA-s within" under
> the hood, and that only custom machinery you write will recognize things as
> something other than bog-standard NAs/NaNs.  You'll also have some problems
> with the fact that data in ALTREPs isn't currently modifiable without
> losing ALTREPness. That said, ALTREPs are allowed to carry around arbitrary
> persistent information with them, so from that perspective making an ALTREP
> that carries around a "meaning of my NAs" vector of tags in its metadata
> would be pretty straightforward.
>

Oh... now that is extremely interesting.
It is the first time I came across the ALTREP concept, so I need to study
the way it works before saying anything, but definitely something to
consider.

Thanks so much for the pointer,
Adrian

	[[alternative HTML version deleted]]


From g@bembecker @end|ng |rom gm@||@com  Tue May 25 00:23:58 2021
From: g@bembecker @end|ng |rom gm@||@com (Gabriel Becker)
Date: Mon, 24 May 2021 15:23:58 -0700
Subject: [Rd] 1954 from NA
In-Reply-To: <CACrL82V8RJpfnoyXrk_Lnrdz3jUOhPiXrTrpT3Xg50O4cHwL9w@mail.gmail.com>
References: <CAJ=0CtDvHor+DZsmagY07CEjayGGTy+srb8aZqutLFLrTamfJg@mail.gmail.com>
 <eede7930-8b70-a0a7-2c14-9b651878ecc6@gmail.com>
 <CAJ=0CtCprzMyNzjuhneBxyWG1X3g1KDQ+TJE3J6QuhVXG8wZpA@mail.gmail.com>
 <0aa3352e-62a0-a8b9-fa6a-a3a994ab8e55@gmail.com>
 <CAJ=0CtBJY8HcxzPX=5N65uXE410mQhnNDoG+dr7hYCtZhaTsQA@mail.gmail.com>
 <45cb76eb-40e9-f9e1-8dd3-c9042b4e529a@gmail.com>
 <CAJ=0CtBr13cxOwKdD4HyAFCfwDhM3NVEfvarZigv+6TQBPjzDQ@mail.gmail.com>
 <CAOdV3zARBg=TDTDPZMuzWSOqi=Wi3yShRxrYTbAYM9njb4Bx0A@mail.gmail.com>
 <CAJ=0CtBjqY629fxfUfapS-3ccO1XRMh5HAa1WPOW=rMuYCbijg@mail.gmail.com>
 <D2F37634-758C-4CD3-91A0-43172F074BD7@uzh.ch>
 <CAJ=0CtBFNR_=fCbg49B6-fdmprMf6p5gicP0Vq8M_ryqkAfkyQ@mail.gmail.com>
 <CAD4oTHHFKoZdP6NJ6+qcwYohgOdKxivdbM=vHA_ZydRsaLe_og@mail.gmail.com>
 <CAJ=0CtDohu34qVJGB5TUkVK1gErw8CEwXgenVypBLcKeOjBZ_g@mail.gmail.com>
 <CACrL82V8RJpfnoyXrk_Lnrdz3jUOhPiXrTrpT3Xg50O4cHwL9w@mail.gmail.com>
Message-ID: <CAD4oTHEZH7BkRc1SXdCopfXMYz2SyWHnrYBy+25uaJthBHWMmw@mail.gmail.com>

Hi All,

So there is a not particularly active, but closely curated (ie everything
on there should be good in terms of principled examples) github
organization of ALTREP examples: https://github.com/ALTREP-examples.
Currently there are two examples by Luke (including a package version of
the memory map ALTREP he wrote) and one by me.

To elaborate a bit more it looks like you could have read-only vectors with
tagged NAs, because despite my incorrect recollection, It looks like
Extract_subset IS hooked up, so subsetting an ALTREP can, depending on the
altrep class, give you another ALTREP.

They would effectively be subsettable but not mutable, though,
because setting elements in an ALTREP vector still wipes its altrepness.
This is unfortunate but an intentional design decision that itself
currently appears immutable,if you'll excuse the pun, last I heard.

I understand that that is a relatively sizable caveat, but ce la vie

Assuming that things would be useful with that caveat I can try to put a
proof of concept example into that organization that could works as the
starting board for a deeper collaboration soon. I think I have in my head a
way to approach it.

~G

On Mon, May 24, 2021 at 3:00 PM Nicholas Tierney <nicholas.tierney at gmail.com>
wrote:

> Hi all,
>
> When first hearing about ALTREP I've wondered how it might be able to be
> used to store special missing value information - how can we learn more
> about implementing ALTREP classes? The idea of carrying around a "meaning
> of my NAs" vector, as Gabe said, would be very interesting!
>
> I've done a bit on creating "special missing values", as done in SPSS,
> SAS, and STATA, here:
> http://naniar.njtierney.com/articles/special-missing-values.html  (Note
> this approach requires carrying a duplicated dataframe of missing data
> around with the data - which I argue makes it easier to reason with, at the
> cost of storage. However this is just my approach, and there are others out
> there).
>
> Best,
>
> Nick
>
> On Tue, 25 May 2021 at 01:16, Adrian Du?a <dusa.adrian at gmail.com> wrote:
>
>> On Mon, May 24, 2021 at 5:47 PM Gabriel Becker <gabembecker at gmail.com>
>> wrote:
>>
>> > Hi Adrian,
>> >
>> > I had the same thought as Luke. It is possible that you can develop an
>> > ALTREP that carries around the tagging information you're looking for
>> in a
>> > way that is more persistent (in some cases) than R-level attributes and
>> > more hidden than additional user-visible columns.
>> >
>> > The downsides to this, of course, is that you'll in some sense be doing
>> > the same "extra vector for each vector you want tagged NA-s within"
>> under
>> > the hood, and that only custom machinery you write will recognize
>> things as
>> > something other than bog-standard NAs/NaNs.  You'll also have some
>> problems
>> > with the fact that data in ALTREPs isn't currently modifiable without
>> > losing ALTREPness. That said, ALTREPs are allowed to carry around
>> arbitrary
>> > persistent information with them, so from that perspective making an
>> ALTREP
>> > that carries around a "meaning of my NAs" vector of tags in its metadata
>> > would be pretty straightforward.
>> >
>>
>> Oh... now that is extremely interesting.
>> It is the first time I came across the ALTREP concept, so I need to study
>> the way it works before saying anything, but definitely something to
>> consider.
>>
>> Thanks so much for the pointer,
>> Adrian
>>
>>         [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>

	[[alternative HTML version deleted]]


From n|cho|@@@t|erney @end|ng |rom gm@||@com  Tue May 25 00:00:16 2021
From: n|cho|@@@t|erney @end|ng |rom gm@||@com (Nicholas Tierney)
Date: Tue, 25 May 2021 08:00:16 +1000
Subject: [Rd] 1954 from NA
In-Reply-To: <CAJ=0CtDohu34qVJGB5TUkVK1gErw8CEwXgenVypBLcKeOjBZ_g@mail.gmail.com>
References: <CAJ=0CtDvHor+DZsmagY07CEjayGGTy+srb8aZqutLFLrTamfJg@mail.gmail.com>
 <eede7930-8b70-a0a7-2c14-9b651878ecc6@gmail.com>
 <CAJ=0CtCprzMyNzjuhneBxyWG1X3g1KDQ+TJE3J6QuhVXG8wZpA@mail.gmail.com>
 <0aa3352e-62a0-a8b9-fa6a-a3a994ab8e55@gmail.com>
 <CAJ=0CtBJY8HcxzPX=5N65uXE410mQhnNDoG+dr7hYCtZhaTsQA@mail.gmail.com>
 <45cb76eb-40e9-f9e1-8dd3-c9042b4e529a@gmail.com>
 <CAJ=0CtBr13cxOwKdD4HyAFCfwDhM3NVEfvarZigv+6TQBPjzDQ@mail.gmail.com>
 <CAOdV3zARBg=TDTDPZMuzWSOqi=Wi3yShRxrYTbAYM9njb4Bx0A@mail.gmail.com>
 <CAJ=0CtBjqY629fxfUfapS-3ccO1XRMh5HAa1WPOW=rMuYCbijg@mail.gmail.com>
 <D2F37634-758C-4CD3-91A0-43172F074BD7@uzh.ch>
 <CAJ=0CtBFNR_=fCbg49B6-fdmprMf6p5gicP0Vq8M_ryqkAfkyQ@mail.gmail.com>
 <CAD4oTHHFKoZdP6NJ6+qcwYohgOdKxivdbM=vHA_ZydRsaLe_og@mail.gmail.com>
 <CAJ=0CtDohu34qVJGB5TUkVK1gErw8CEwXgenVypBLcKeOjBZ_g@mail.gmail.com>
Message-ID: <CACrL82V8RJpfnoyXrk_Lnrdz3jUOhPiXrTrpT3Xg50O4cHwL9w@mail.gmail.com>

Hi all,

When first hearing about ALTREP I've wondered how it might be able to be
used to store special missing value information - how can we learn more
about implementing ALTREP classes? The idea of carrying around a "meaning
of my NAs" vector, as Gabe said, would be very interesting!

I've done a bit on creating "special missing values", as done in SPSS, SAS,
and STATA, here:
http://naniar.njtierney.com/articles/special-missing-values.html  (Note
this approach requires carrying a duplicated dataframe of missing data
around with the data - which I argue makes it easier to reason with, at the
cost of storage. However this is just my approach, and there are others out
there).

Best,

Nick

On Tue, 25 May 2021 at 01:16, Adrian Du?a <dusa.adrian at gmail.com> wrote:

> On Mon, May 24, 2021 at 5:47 PM Gabriel Becker <gabembecker at gmail.com>
> wrote:
>
> > Hi Adrian,
> >
> > I had the same thought as Luke. It is possible that you can develop an
> > ALTREP that carries around the tagging information you're looking for in
> a
> > way that is more persistent (in some cases) than R-level attributes and
> > more hidden than additional user-visible columns.
> >
> > The downsides to this, of course, is that you'll in some sense be doing
> > the same "extra vector for each vector you want tagged NA-s within" under
> > the hood, and that only custom machinery you write will recognize things
> as
> > something other than bog-standard NAs/NaNs.  You'll also have some
> problems
> > with the fact that data in ALTREPs isn't currently modifiable without
> > losing ALTREPness. That said, ALTREPs are allowed to carry around
> arbitrary
> > persistent information with them, so from that perspective making an
> ALTREP
> > that carries around a "meaning of my NAs" vector of tags in its metadata
> > would be pretty straightforward.
> >
>
> Oh... now that is extremely interesting.
> It is the first time I came across the ALTREP concept, so I need to study
> the way it works before saying anything, but definitely something to
> consider.
>
> Thanks so much for the pointer,
> Adrian
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From j@ke@e|m@tedt @end|ng |rom gm@||@com  Tue May 25 01:35:31 2021
From: j@ke@e|m@tedt @end|ng |rom gm@||@com (Jake Elmstedt)
Date: Mon, 24 May 2021 16:35:31 -0700
Subject: [Rd] Locking of base environment in R 4.1.0 breaks simple
 assignment of .First() (etc) from Rprofile.site
Message-ID: <CAPTrVR5wKThxggiPXhEoiFrZ6Vr3UM7vVhG5C3x4NLfTnMBV4g@mail.gmail.com>

Commits 80162 and 80163 lock the base environment and namespace during
startup, leading to an error when attempting to directly assign anything
from within Rprofile.site. While this is intentional and good, the help
file has not been updated to reflect this change.

Startup.Rd ( Description, paragraph 3) reads,

> ...This code is sourced into the base package. Users need to be careful
not to unintentionally overwrite objects in base, and it is normally
advisable to use local if code needs to be executed: see the examples.

Since the base environment and namespace are locked as of 4.1.0, I
recommend this be edited to something to the effect of:

> Prior to R 4.1.0, this code is sourced into the base package. Users need
to be careful not to unintentionally overwrite objects in base, and it is
normally advisable to use local if code needs to be executed: see the
examples. As of R 4.1.0 the base environment and namespace are locked and
any attempted direct assignment will fail with an error, and none of the
subsequent commands will be invoked. Users migrating to R 4.1.0 and above
can edit assignments made in Rprofile.site from the form `x <- value` to
`assign("x", value, envir = globalenv())`. Common uses of this may include
the binding of functions `.First` and `.Last`.

Description paragraph 8,

> "A function .First (and .Last) can be defined in appropriate ?.Rprofile?
or ?Rprofile.site? files..."

which might be misleading to users not well-versed in the intricacies of
the R startup process. Should this be edited to indicate variable
assignment from Rprofile.site must be done using assign() to bind them to
globalenv()? Something to the effect of,

> A function .First (and .Last) can be defined in appropriate ?.Rprofile?
or ?Rprofile.site? files (note: as of R 4.1.0, assignments from
Rprofile.site must target the global environment, e.g. use
`assign(".First", function(){}, envir = globalenv())` rather than `.First
<- function(){}`) or have been saved in ?.RData?.

I also recommend adding to Examples, under the Example of Rprofile.site
section, something to the effect of,

# Setting .First and .Last from within Rprofile.site requires
# assignment into the global environment for R versions
# 4.1.0 and later and is a best practice for earlier versions.
assign(".First", function() cat("\n   Welcome to R!\n\n"), envir =
globalenv())
assign(".Last", function() cat("\n   Goodbye!\n\n"), envir = globalenv())

	[[alternative HTML version deleted]]


From @v|gro@@ @end|ng |rom ver|zon@net  Tue May 25 04:51:20 2021
From: @v|gro@@ @end|ng |rom ver|zon@net (Avi Gross)
Date: Mon, 24 May 2021 22:51:20 -0400
Subject: [Rd] FW:  1954 from NA
References: <CAJ=0CtDvHor+DZsmagY07CEjayGGTy+srb8aZqutLFLrTamfJg@mail.gmail.com>
 <eede7930-8b70-a0a7-2c14-9b651878ecc6@gmail.com>
 <CAJ=0CtCprzMyNzjuhneBxyWG1X3g1KDQ+TJE3J6QuhVXG8wZpA@mail.gmail.com>
 <008a01d75008$d6612490$83236db0$@verizon.net>
 <CAJ=0CtCfRMJM6wq1ggkSv2BNfX+CGnW2N-SZAN8poOG3ev=nUA@mail.gmail.com> 
Message-ID: <013e01d75110$d77f4dd0$867de970$@verizon.net>

 

Adrian,

 

Agreed. To do what you said hundreds of columns of data by doubling it is indeed a pain just to get what you want. There are straightforward ways especially if you use tidyverse packages rather than base R. Just a warning, this message is a tad long for anyone not interested to skip.

 

But there is a caution about trying to use a feature nobody wanted changed until you came along. R has all kinds of dependencies on existing ways of looking at an NA value such as asking is.na(SOMETHING) or the many function like a mean where they handle mean(SOMETHING, na.rm=TRUE) or the way ggplot graphs skip items that are NA and so on. Any solution you come up with to enlarge the kinds of NA may break some of that and then you will have no right to complain.

 

What does your data look like? I mean for example if all the data in a column is small integers say under a thousand, you can pick some number like 10,000 and store some NA categories as 10,000 + 1 then 10,000 + 2 and so on. THEN you have to be careful, as noted above, to remove all such values in other contexts by either making a copy where all numbers above 10,000 are changed to an NA for the duration or you take subsets of the data that exclude those.

 

Floating point can also be done that way or by using a negative number or other tricks.

 

Character DATA obviously can have reserved words that will not happen in the rest of the DATA such as NA*NA:1 and NA*NA:2 or whatever makes sense to you. Ideally this can be something you can remove all at once with something like perhaps a regular expression when needed. If you use a factor to store such a field, as if often a good idea, there are ways to force the indexes of your NA-like fields to be whatever you want, such as the first 10 or even last 10, perhaps letting you play games when they need to be hidden or removed or refactored into a plain NA. It adds complexity and may break in unexpected ways.

 

And, I shudder to say this, more modern usage allows you to change normal vectors into list variables as columns. So you can replace a single column (or as many as you want) by a tuple column where the first part may be your data including just NA when needed and the second item would  be something else like a more specific reason for any items where the first is NA. Heck, you can add a series of Boolean entries in the list where the second to the last each encode TRUE if it has a particular excuse and you can even have multiple excuses (or none) for an entry. I repeat, I shudder, simply because many other normally used R functions are not expecting list columns and you may need to call them indirectly with something that extracts only the part needed first.

 

R does have some inconsistencies in how it handles some things such as name tags associated with parts of a vector. Some functions preserve the attributes used this way and others do not.  But if you want to emulate the same tricks normally used in making factors and matrices or giving column names, you can do something like this that might work. My EXAMPLE below makes a vector of a dozen sequential numbers as the VALUE and hides an attribute with month names to sort of match: It then changes every third to be NA:

 

temp <- 1:12

attr(temp, "Month") <- month.abb

temp[3] <- temp[6] <- temp[9] <- temp[12] <- NA

 

The current value of temp may look like this:

 

> temp

[1]  1  2 NA  4  5 NA  7  8 NA 10 11 NA

attr(,"Month")

[1] "Jan" "Feb" "Mar" "Apr" "May" "Jun" "Jul" "Aug" "Sep" "Oct" "Nov" "Dec"

 

So it has months attached as PLACEHOLDERS and four different NA values. To see an NA value?s REASON, the two have the same index so:

 

> attr(temp, "Month")[is.na(temp)]

[1] "Mar" "Jun" "Sep" "Dec"

 

The above asked to see what text is associated with each NA. You can use many techniques like the above to find out why a particular item is an NA. If you want to know why the sixth item is NA, with R using index of 6 as it starts with 1:

 

> attr(temp, "Month")[6]

[1] "Jun"

 

And it can work both ways. If I now were to change the above to say store an NA in the Months variable (renamed by you to Reason or something) except for other entries saying ?RanOutOfTime?,  ?DidNotUnderstandQuestion? and so on, you could search the attribute first and get the index numbers of which questions matched and other such things.

 

There may well be a well-reasoned package as just described and perhaps some that do not use as much space. The above very rough implementation just hides a second vector attached loosely tied to the first vector in a way that may be invisible to most other functionality. But it can easily have problems as so many things make  new vectors and remove your change. Consider just doubling the odd vector I created:

 

> temp2 <- c(temp, temp)

> temp2

[1]  1  2 NA  4  5 NA  7  8 NA 10 11 NA  1  2 NA  4  5 NA  7  8 NA 10 11 NA

 

The annotation is gone!

 

Now if you do something a tad more normal like re-use the names() feature, maybe you can preserve it in more cases:

 

temp <- 1:12

names(temp) <- month.abb

temp[3] <- temp[6] <- temp[9] <- temp[12] <- NA

 

> temp

[1]  1  2 NA  4  5 NA  7  8 NA 10 11 NA

attr(,"Month")

[1] "Jan" "Feb" "Mar" "Apr" "May" "Jun" "Jul" "Aug" "Sep" "Oct" "Nov" "Dec"

 

> temp

Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec 

1   2  NA   4   5  NA   7   8  NA  10  11  NA

 

Now NAMES used this way can be preserved sometimes. For example some functions have arguments like this:

 

> temp2 <- c(temp, temp, use.names=TRUE)

> temp2

Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec 

1   2  NA   4   5  NA   7   8  NA  10  11  NA   1   2  NA   4   5  NA   7   8  NA  10  11  NA

 

So, it may well be you can play such games with your input but doing that for hundreds of columns may be a tad of work that can be automated easily enough if all the columns are similar, such as in repeats of data in a time series. As noted, R functions that read in DATA expect all items in a column to be of the same underlying type or an NA. If your data has text giving a REASON, and you know exactly what reasons are allowed with any remaining values to be left as is, you might do something like this in pseudo code.

 

Say column_orig looks like this: 5, 1, bad, 2, worse, 1, 2, 5, bad, 6, worse, missing, 2

 

Your stuff may be read in as CHARACTER and look like:

 

> column_orig

[1] "5"       "1"       "bad"     "2"       "worse"   "1"       "2"       "5"       "bad"    

[10] "6"       "worse"   "missing" "2"

 

So, you can process the above with something like an ifelse() to make a temporary version VERY carefully as ifelse does not preserve name attributes!

 

> names.temp <- ifelse(column_orig %in% c("bad", "worse", "missing"), column_orig, NA)

> column_orig <- ifelse(column_orig %in% c("bad", "worse", "missing"), NA, column_orig)

> column_orig <- as.numeric(column_orig)

> names(column_orig) <- names.temp

 

> column_orig

<NA>    <NA>     bad    <NA>   worse    <NA>    <NA>    <NA>     bad    <NA>   worse missing    <NA> 

  5       1      NA       2      NA       1       2       5      NA       6      NA      NA       2

 

(the above may not show up formatted right in the email but shows the names on the first line and the data on the second. Wherever the data is NA, the reason is in the name.

 

Again, I am just playing with your specified need and pointing out ways R may partially support them but probably far from ideal as you are trying to do something it probably was never designed for. I suspect the philosophy behind using a tibble instead of a data.frame may preserve your meta-info better.

 

But if all you want is to know the reason for a missing observation while using little space, there may be other ways to consider such as making a sparse matrix from the original data if missing values are rare enough. Sure, it might have 600 columns and umpteen rows, but you can store a small integer or even a byte in each entry and perhaps skip any row that has nothing missing. If you later need the info and the data has not been scrambled such as by removing rows or columns or sorting, you can easily find it. Or, if you simply add one more column with some form of unique sequence number or ID and maintain it, you can always index back to find what you want, WITHOUT all the warnings mentioned above.

 

If memory is a huge concern, consider ways you can massage your original data to conserve what you need then save THAT to a file on disk and remove the extra space use for garbage collection. When and IF you ever need that info at some later date, the form you chose can be read back in. But you need to be careful as such meta-info is lost unless you use a method that conserves it. Do not save it as a CSV file, for example, but as something R uses and can read back in the same way.

 

Or, you can try to make your own twists on changing how NA works and take lots of risks as it is not doing something published and guaranteed. 

 

I think I can now politely bow out of this topic and wish you luck with whatever you choose. It may even be using something other than R!

 

From: Adrian Du?a <dusa.adrian at unibuc.ro <mailto:dusa.adrian at unibuc.ro> > 
Sent: Monday, May 24, 2021 5:26 AM
To: Avi Gross <avigross at verizon.net <mailto:avigross at verizon.net> >
Cc: r-devel <r-devel at r-project.org <mailto:r-devel at r-project.org> >
Subject: Re: [Rd] 1954 from NA

 

Hmm...

If it was only one column then your solution is neat. But with 5-600 variables, each of which can contain multiple missing values, to double this number of variables just to describe NA values seems to me excessive.

Not to mention we should be able to quickly convert / import / export from one software package to another. This would imply maintaining some sort of metadata reference of which explanatory additional factor describes which original variable.

 

All of this strikes me as a lot of hassle compared to storing some information within a tagged NA value... I just need a little bit more bits to play with.

 

Best wishes,

Adrian

 

On Sun, May 23, 2021 at 10:21 PM Avi Gross via R-devel <r-devel at r-project.org <mailto:r-devel at r-project.org> > wrote:

Arguably, R was not developed to satisfy some needs in the way intended.

When I have had to work with datasets from some of the social sciences I have had to adapt to subtleties in how they did things with software like SPSS in which an NA was done using an out of bounds marker like 999 or "." or even a blank cell. The problem is that R has a concept where data such as integers or floating point numbers is not stored as text normally but in their own formats and a vector by definition can only contain ONE data type. So the various forms of NA as well as Nan and Inf had to be grafted on to be considered VALID to share the same storage area as if they sort of were an integer or floating point number or text or whatever.

It does strike me as possible to simply have a column that is something like a factor that can contain as many NA excuses as you wish such as "NOT ANSWERED" to "CANNOT READ THE SQUIGLE" to "NOT SURE" to "WILL BE FILLED IN LATER" to "I DON'T SPEAK ENGLISH AND CANNOT ANSWER STUPID QUESTIONS". This additional column would presumably only have content when the other column has an NA. Your queries and other changes would work on something like a data.frame where both such columns coexisted.

Note reading in data with multiple NA reasons may take extra work. If your errors codes are text, it will all become text. If the errors are 999 and 998 and 997, it may all be treated as numeric and you may not want to convert all such codes to an NA immediately. Rather, you would use the first vector/column to make the second vector and THEN replace everything that should be an NA with an actual NA and reparse the entire vector to become properly numeric unless you like working with text and will convert to numbers as needed on the fly.

Now this form of annotation may not be pleasing but I suggest that an implementation that does allow annotation may use up space too. Of course, if your NA values are rare and space is only used then, you might save space. But if you could make a factor column and have it use the smallest int it can get as a basis, it may be a way to save on space.

People who have done work with R, especially those using the tidyverse, are quite used to using one column to explain another. So if you are asked to say tabulate what percent of missing values are due to reasons A/B/C then the added columns works fine for that calculation too.

 

-- 

Adrian Dusa
University of Bucharest
Romanian Social Data Archive
Soseaua Panduri nr. 90-92
050663 Bucharest sector 5
Romania

https://adriandusa.eu


	[[alternative HTML version deleted]]


From @v|gro@@ @end|ng |rom ver|zon@net  Tue May 25 04:51:53 2021
From: @v|gro@@ @end|ng |rom ver|zon@net (Avi Gross)
Date: Mon, 24 May 2021 22:51:53 -0400
Subject: [Rd] 1954 from NA
References: <CAJ=0CtCfRMJM6wq1ggkSv2BNfX+CGnW2N-SZAN8poOG3ev=nUA@mail.gmail.com>
 <3443246.1621854690@apollo2.minshall.org>
 <CAJ=0CtDOuY+tVf+YxO0gdGY9XgaTv_Q4kMP2p_Z8CxrnHjT4-w@mail.gmail.com> 
Message-ID: <014b01d75110$eba143e0$c2e3cba0$@verizon.net>

Adrian,

 

This is an aside. I note in many machine-learning algorithms they actually do something along the lines being discussed. They may take an item like a paragraph of words or an email message  and add thousands of columns with each one being a Boolean specifying if a particular word is in or not in that item. They may then run an analysis trying to heuristically match known SPAM items so as to be able to predict if new items might be SPAM. Some may even have a column for words taken two or more at a time such as ?must? followed by ?have? or ?Your?, ?last?, ?chance? resulting> column_orig

<NA>    <NA>     bad    <NA>   worse    <NA>    <NA>    <NA>     bad    <NA>   worse missing    <NA> 

  5       1      NA       2      NA       1       2       5      NA       6      NA      NA       2  in even more columns. The software than does the analysis can work on remarkably large such collections including in some cases taking multiple approaches at the same problem and choosing among them in some way.

 

In your case, yes, adding lots of columns seems like added work. But in data science, often the easiest way to do some complex things is to loop over selected existing columns and create multiple sets of additional columns that simplify later calculations by just using these values rather than some multi-line complex condition. I have as an example run statistical analyses where I have a Boolean column if the analysis failed (as in I caught it using try() or else it would kill my process) and another if I was told it did not converge properly and yet another column if it failed some post-tests. It simplified some queries that excluded rows where any one of the above was TRUE. I also stored columns for metrics like RMSEA and chi-squared values, sometimes dozens. And for each of the above, I actually had a set of columns for various models such as linear versus quadratic and more. Worse, as the analysis continued, more derived columns were added as various measures of the above results were compared to each other so the different models could be compared as in how often each was better. Careful choices of naming conventions and nice features of the tidyverse made it fairly simple to operate on many columns in the same way fairly easily such as all columns whose names start with a string or end with ?

 

And, yes, for some efficiency, I often made a narrower version of the above with just the fields I needed and was careful not to remove what I might need later.

 

So it can be done and fairly trivially if you know what you are doing. If the names of all your original columns that behave this way look like *.orig and others look different, you can ask for a function to be applied to just those that produces another set with the same prefixes but named *.converted and yet another called *.annotation and so on. You may want to remove the originals to save space but you get the idea. The fact there are six hundred means little with such a design as the above can be done in probably a dozen lines of code to all of them at once.

 

For me, the above is way less complex than what you want to do and can have benefits. For example, if you make a graph of points from my larger tibble/data.frame using ggplot(), you can do things like specify what color to use for a point using a variable that contains the reason the data was missing (albeit that assumes the missing part is not what is being graphed) or add text giving the reason just above each such point. Your method of faking multiple things YOU claim are an NA may not make it doable in the above example.

 

From: Adrian Du?a <dusa.adrian at unibuc.ro <mailto:dusa.adrian at unibuc.ro> > 
Sent: Monday, May 24, 2021 8:18 AM
To: Greg Minshall <minshall at umich.edu <mailto:minshall at umich.edu> >
Cc: Avi Gross <avigross at verizon.net <mailto:avigross at verizon.net> >; r-devel <r-devel at r-project.org <mailto:r-devel at r-project.org> >
Subject: Re: [Rd] 1954 from NA

 

On Mon, May 24, 2021 at 2:11 PM Greg Minshall <minshall at umich.edu <mailto:minshall at umich.edu> > wrote:

[...]
if you have 500 columns of possibly-NA'd variables, you could have one
column of 500 "bits", where each bit has one of N values, N being the
number of explanations the corresponding column has for why the NA
exists.

 

The mere thought of implementing something like that gives me shivers. Not to mention such a solution should also be robust when subsetting, splitting, column and row binding, etc. and everything can be lost if the user deletes that particular column without realising its importance.

 

Social science datasets are much more alive and complex than one might first think: there are multi-wave studies with tens of countries, and aggregating such data is already a complex process to add even more complexity on top of that.

 

As undocumented as they may be, or even subject to change, I think the R internals are much more reliable that this.

 

Best wishes,

Adrian

 

-- 

Adrian Dusa
University of Bucharest
Romanian Social Data Archive
Soseaua Panduri nr. 90-92
050663 Bucharest sector 5
Romania

https://adriandusa.eu


	[[alternative HTML version deleted]]


From @v|gro@@ @end|ng |rom ver|zon@net  Tue May 25 06:05:13 2021
From: @v|gro@@ @end|ng |rom ver|zon@net (Avi Gross)
Date: Tue, 25 May 2021 00:05:13 -0400
Subject: [Rd] [External] Re:  1954 from NA
In-Reply-To: <alpine.DEB.2.21.2105240805580.3254@luke-Latitude-7480>
References: <CAJ=0CtCfRMJM6wq1ggkSv2BNfX+CGnW2N-SZAN8poOG3ev=nUA@mail.gmail.com>
 <3443246.1621854690@apollo2.minshall.org>
 <CAJ=0CtDOuY+tVf+YxO0gdGY9XgaTv_Q4kMP2p_Z8CxrnHjT4-w@mail.gmail.com>
 <alpine.DEB.2.21.2105240805580.3254@luke-Latitude-7480>
Message-ID: <016e01d7511b$29c3bea0$7d4b3be0$@verizon.net>

I was thinking about how one does things in a language that is properly object-oriented versus R that makes various half-assed attempts at being such.

Clearly in some such languages you can make an object that is a wrapper that allows you to save an item that is the main payload as well as anything else you want. You might need a way to convince everything else to allow you to make things like lists and vectors and other collections of the objects and perhaps automatically unbox them for many purposes. As an example in a language like Python, you might provide methods so that adding A and B actually gets the value out of A and/or B and adds them properly.  But there may be too many edge cases to handle and some software may not pay attention to what you want including some libraries written in other languages.

I mention Python for the odd reason that it is now possible to combine Python and R in the same program and sort of switch back and forth between data representations. This may provide some openings for preserving and accessing metadata when needed.

Realistically, if R was being designed from scratch TODAY, many things might be done differently. But I recall it being developed at Bell Labs for purposes where it was sort of revolutionary at the time (back when it was S) and designed to do things in a vectorized way and probably primarily for the kinds of scientific and mathematical operations where a single NA (of several types depending on the data) was enough when augmented by a few things like a Nan and Inf and -Inf. I doubt they seriously saw a need for an unlimited number of NA that were all the same AND also all different that they felt had to be built-in. As noted, had they had a reason to make it fully object-oriented too and made the base types such as integer into full-fledged objects with room for additional metadata, then things may be different. I note I have seen languages which have both a data type called integer as lower case and Integer as upper case. One of them is regularly boxed and unboxed automagically when used in a context that needs the other. As far as efficiency goes, this invisibly adds many steps. So do languages that sometimes take a variable that is a pointer and invisibly reference it to provide the underlying field rather than make you do extra typing and so on.

So is there any reason only an NA should have such meta-data? Why not have reasons associated with Inf stating it was an Inf because you asked for one or the result of a calculation such as dividing by Zero (albeit maybe that might be a NaN) and so on. Maybe I could annotate integers with whether they are prime or even  versus odd  or a factor of 144 or anything else I can imagine. But at some point, the overhead from allowing all this can become substantial. I was amused at how python allows a function to be annotated including by itself since it is an object. So it can store such metadata perhaps in an attached dictionary so a complex costly calculation can have the results cached and when you ask for the same thing in the same session, it checks if it has done it and just returns the result in linear time. But after a while, how many cached results can there be?

-----Original Message-----
From: R-devel <r-devel-bounces at r-project.org> On Behalf Of luke-tierney at uiowa.edu
Sent: Monday, May 24, 2021 9:15 AM
To: Adrian Du?a <dusa.adrian at unibuc.ro>
Cc: Greg Minshall <minshall at umich.edu>; r-devel <r-devel at r-project.org>
Subject: Re: [Rd] [External] Re: 1954 from NA

On Mon, 24 May 2021, Adrian Du?a wrote:

> On Mon, May 24, 2021 at 2:11 PM Greg Minshall <minshall at umich.edu> wrote:
>
>> [...]
>> if you have 500 columns of possibly-NA'd variables, you could have 
>> one column of 500 "bits", where each bit has one of N values, N being 
>> the number of explanations the corresponding column has for why the 
>> NA exists.
>>

PLEASE DO NOT DO THIS!

It will not work reliably, as has been explained to you ad nauseam in this thread.

If you distribute code that does this it will only lead to bug reports on R that will waste R-core time.

As Alex explained, you can use attributes for this. If you need operations to preserve attributes across subsetting you can define subsetting methods that do that.

If you are dead set on doing something in C you can try to develop an ALTREP class that provides augmented missing value information.

Best,

luke



>
> The mere thought of implementing something like that gives me shivers. 
> Not to mention such a solution should also be robust when subsetting, 
> splitting, column and row binding, etc. and everything can be lost if 
> the user deletes that particular column without realising its importance.
>
> Social science datasets are much more alive and complex than one might 
> first think: there are multi-wave studies with tens of countries, and 
> aggregating such data is already a complex process to add even more 
> complexity on top of that.
>
> As undocumented as they may be, or even subject to change, I think the 
> R internals are much more reliable that this.
>
> Best wishes,
> Adrian
>
>

--
Luke Tierney
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu
______________________________________________
R-devel at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-devel


From du@@@@dr|@n @end|ng |rom un|buc@ro  Tue May 25 08:16:35 2021
From: du@@@@dr|@n @end|ng |rom un|buc@ro (=?UTF-8?B?QWRyaWFuIER1yJlh?=)
Date: Tue, 25 May 2021 09:16:35 +0300
Subject: [Rd] [External] Re: 1954 from NA
In-Reply-To: <016e01d7511b$29c3bea0$7d4b3be0$@verizon.net>
References: <CAJ=0CtCfRMJM6wq1ggkSv2BNfX+CGnW2N-SZAN8poOG3ev=nUA@mail.gmail.com>
 <3443246.1621854690@apollo2.minshall.org>
 <CAJ=0CtDOuY+tVf+YxO0gdGY9XgaTv_Q4kMP2p_Z8CxrnHjT4-w@mail.gmail.com>
 <alpine.DEB.2.21.2105240805580.3254@luke-Latitude-7480>
 <016e01d7511b$29c3bea0$7d4b3be0$@verizon.net>
Message-ID: <CAJ=0CtDC4jEfa-9qqDhixqSbvh1528wD5QpObQuwqxRs1EfiEw@mail.gmail.com>

Dear Avi,

Thank you so much for the extended messages, I read them carefully.
While partially offering a solution (I've already been there), it creates
additional work for the user, and some of that is unnecessary.

What I am trying to achieve is best described in this draft vignette:

devtools::install_github("dusadrian/mixed")
vignette("mixed")

Once a value is declared to be missing, the user should not do anything
else about it. Despite being present, the value should automatically be
treated as missing by the software. That is the way it's done in all major
statistical packages like SAS, Stata and even SPSS.

My end goal is to make R attractive for my faculty peers (and beyond),
almost all of whom are massively using SPSS and sometimes Stata. But in
order to convince them to (finally) make the switch, I need to provide
similar functionality, not additional work.

Re. your first part of the message, I am definitely not trying to change
the R internals. The NA will still be NA, exactly as currently defined.
My initial proposal was based on the observation that the 1954 payload was
stored as an unsigned int (thus occupying 32 bits) when it is obvious it
doesn't need more than 16. That was the only proposed modification, and
everything else stays the same.

I now learned, thanks to all contributors in this list, that building
something around that payload is risky because we do not know exactly what
the compilers will do. One possible solution that I can think of, while
(still) maintaining the current functionality around the NA, is to use a
different high word for the NA that would not trigger compilation issues.
But I have absolutely no idea what that implies for the other inner
workings of R.

I very much trust the R core will eventually find a robust solution,
they've solved much more complicated problems than this. I just hope the
current thread will push the idea of tagged NAs on the table, for when they
will discuss this.

Once that will be solved, and despite the current advice discouraging this
route, I believe tagging NAs is a valuable idea that should not be
discarded.
After all, the NA is nothing but a tagged NaN.

All the best,
Adrian


On Tue, May 25, 2021 at 7:05 AM Avi Gross via R-devel <r-devel at r-project.org>
wrote:

> I was thinking about how one does things in a language that is properly
> object-oriented versus R that makes various half-assed attempts at being
> such.
>
> Clearly in some such languages you can make an object that is a wrapper
> that allows you to save an item that is the main payload as well as
> anything else you want. You might need a way to convince everything else to
> allow you to make things like lists and vectors and other collections of
> the objects and perhaps automatically unbox them for many purposes. As an
> example in a language like Python, you might provide methods so that adding
> A and B actually gets the value out of A and/or B and adds them properly.
> But there may be too many edge cases to handle and some software may not
> pay attention to what you want including some libraries written in other
> languages.
>
> I mention Python for the odd reason that it is now possible to combine
> Python and R in the same program and sort of switch back and forth between
> data representations. This may provide some openings for preserving and
> accessing metadata when needed.
>
> Realistically, if R was being designed from scratch TODAY, many things
> might be done differently. But I recall it being developed at Bell Labs for
> purposes where it was sort of revolutionary at the time (back when it was
> S) and designed to do things in a vectorized way and probably primarily for
> the kinds of scientific and mathematical operations where a single NA (of
> several types depending on the data) was enough when augmented by a few
> things like a Nan and Inf and -Inf. I doubt they seriously saw a need for
> an unlimited number of NA that were all the same AND also all different
> that they felt had to be built-in. As noted, had they had a reason to make
> it fully object-oriented too and made the base types such as integer into
> full-fledged objects with room for additional metadata, then things may be
> different. I note I have seen languages which have both a data type called
> integer as lower case and Integer as upper case. One of them is regularly
> boxed and unboxed automagically when used in a context that needs the
> other. As far as efficiency goes, this invisibly adds many steps. So do
> languages that sometimes take a variable that is a pointer and invisibly
> reference it to provide the underlying field rather than make you do extra
> typing and so on.
>
> So is there any reason only an NA should have such meta-data? Why not have
> reasons associated with Inf stating it was an Inf because you asked for one
> or the result of a calculation such as dividing by Zero (albeit maybe that
> might be a NaN) and so on. Maybe I could annotate integers with whether
> they are prime or even  versus odd  or a factor of 144 or anything else I
> can imagine. But at some point, the overhead from allowing all this can
> become substantial. I was amused at how python allows a function to be
> annotated including by itself since it is an object. So it can store such
> metadata perhaps in an attached dictionary so a complex costly calculation
> can have the results cached and when you ask for the same thing in the same
> session, it checks if it has done it and just returns the result in linear
> time. But after a while, how many cached results can there be?
>
> -----Original Message-----
> From: R-devel <r-devel-bounces at r-project.org> On Behalf Of
> luke-tierney at uiowa.edu
> Sent: Monday, May 24, 2021 9:15 AM
> To: Adrian Du?a <dusa.adrian at unibuc.ro>
> Cc: Greg Minshall <minshall at umich.edu>; r-devel <r-devel at r-project.org>
> Subject: Re: [Rd] [External] Re: 1954 from NA
>
> On Mon, 24 May 2021, Adrian Du?a wrote:
>
> > On Mon, May 24, 2021 at 2:11 PM Greg Minshall <minshall at umich.edu>
> wrote:
> >
> >> [...]
> >> if you have 500 columns of possibly-NA'd variables, you could have
> >> one column of 500 "bits", where each bit has one of N values, N being
> >> the number of explanations the corresponding column has for why the
> >> NA exists.
> >>
>
> PLEASE DO NOT DO THIS!
>
> It will not work reliably, as has been explained to you ad nauseam in this
> thread.
>
> If you distribute code that does this it will only lead to bug reports on
> R that will waste R-core time.
>
> As Alex explained, you can use attributes for this. If you need operations
> to preserve attributes across subsetting you can define subsetting methods
> that do that.
>
> If you are dead set on doing something in C you can try to develop an
> ALTREP class that provides augmented missing value information.
>
> Best,
>
> luke
>
>
>
> >
> > The mere thought of implementing something like that gives me shivers.
> > Not to mention such a solution should also be robust when subsetting,
> > splitting, column and row binding, etc. and everything can be lost if
> > the user deletes that particular column without realising its importance.
> >
> > Social science datasets are much more alive and complex than one might
> > first think: there are multi-wave studies with tens of countries, and
> > aggregating such data is already a complex process to add even more
> > complexity on top of that.
> >
> > As undocumented as they may be, or even subject to change, I think the
> > R internals are much more reliable that this.
> >
> > Best wishes,
> > Adrian
> >
> >
>
> --
> Luke Tierney
> Ralph E. Wareham Professor of Mathematical Sciences
> University of Iowa                  Phone:             319-335-3386
> Department of Statistics and        Fax:               319-335-3017
>     Actuarial Science
> 241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
> Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


-- 
Adrian Dusa
University of Bucharest
Romanian Social Data Archive
Soseaua Panduri nr. 90-92
050663 Bucharest sector 5
Romania
https://adriandusa.eu

	[[alternative HTML version deleted]]


From iuke-tier@ey m@iii@g oii uiow@@edu  Tue May 25 15:14:38 2021
From: iuke-tier@ey m@iii@g oii uiow@@edu (iuke-tier@ey m@iii@g oii uiow@@edu)
Date: Tue, 25 May 2021 08:14:38 -0500 (CDT)
Subject: [Rd] [External] Re: 1954 from NA
In-Reply-To: <CAJ=0CtDC4jEfa-9qqDhixqSbvh1528wD5QpObQuwqxRs1EfiEw@mail.gmail.com>
References: <CAJ=0CtCfRMJM6wq1ggkSv2BNfX+CGnW2N-SZAN8poOG3ev=nUA@mail.gmail.com>
 <3443246.1621854690@apollo2.minshall.org>
 <CAJ=0CtDOuY+tVf+YxO0gdGY9XgaTv_Q4kMP2p_Z8CxrnHjT4-w@mail.gmail.com>
 <alpine.DEB.2.21.2105240805580.3254@luke-Latitude-7480>
 <016e01d7511b$29c3bea0$7d4b3be0$@verizon.net>
 <CAJ=0CtDC4jEfa-9qqDhixqSbvh1528wD5QpObQuwqxRs1EfiEw@mail.gmail.com>
Message-ID: <alpine.DEB.2.21.2105250810550.3254@luke-Latitude-7480>

On Tue, 25 May 2021, Adrian Du?a wrote:

> Dear Avi,
>
> Thank you so much for the extended messages, I read them carefully.
> While partially offering a solution (I've already been there), it creates
> additional work for the user, and some of that is unnecessary.
>
> What I am trying to achieve is best described in this draft vignette:
>
> devtools::install_github("dusadrian/mixed")
> vignette("mixed")
>
> Once a value is declared to be missing, the user should not do anything
> else about it. Despite being present, the value should automatically be
> treated as missing by the software. That is the way it's done in all major
> statistical packages like SAS, Stata and even SPSS.
>
> My end goal is to make R attractive for my faculty peers (and beyond),
> almost all of whom are massively using SPSS and sometimes Stata. But in
> order to convince them to (finally) make the switch, I need to provide
> similar functionality, not additional work.
>
> Re. your first part of the message, I am definitely not trying to change
> the R internals. The NA will still be NA, exactly as currently defined.
> My initial proposal was based on the observation that the 1954 payload was
> stored as an unsigned int (thus occupying 32 bits) when it is obvious it
> doesn't need more than 16. That was the only proposed modification, and
> everything else stays the same.
>
> I now learned, thanks to all contributors in this list, that building
> something around that payload is risky because we do not know exactly what
> the compilers will do. One possible solution that I can think of, while
> (still) maintaining the current functionality around the NA, is to use a
> different high word for the NA that would not trigger compilation issues.
> But I have absolutely no idea what that implies for the other inner
> workings of R.
>
> I very much trust the R core will eventually find a robust solution,
> they've solved much more complicated problems than this. I just hope the
> current thread will push the idea of tagged NAs on the table, for when they
> will discuss this.
>
> Once that will be solved, and despite the current advice discouraging this
> route, I believe tagging NAs is a valuable idea that should not be
> discarded.

Yes, it should be discarded.

You can of course do what you like in code you keep to yourself. But
please do not distribute code that does this. via CRAN or any other
means. It will only create problems for those maintaining R.

> After all, the NA is nothing but a tagged NaN.

And we are now paying a price for what was, in hindsight, an
unfortunate decision.

Best,

luke

> All the best,
> Adrian
>
>
> On Tue, May 25, 2021 at 7:05 AM Avi Gross via R-devel <r-devel at r-project.org>
> wrote:
>
>> I was thinking about how one does things in a language that is properly
>> object-oriented versus R that makes various half-assed attempts at being
>> such.
>>
>> Clearly in some such languages you can make an object that is a wrapper
>> that allows you to save an item that is the main payload as well as
>> anything else you want. You might need a way to convince everything else to
>> allow you to make things like lists and vectors and other collections of
>> the objects and perhaps automatically unbox them for many purposes. As an
>> example in a language like Python, you might provide methods so that adding
>> A and B actually gets the value out of A and/or B and adds them properly.
>> But there may be too many edge cases to handle and some software may not
>> pay attention to what you want including some libraries written in other
>> languages.
>>
>> I mention Python for the odd reason that it is now possible to combine
>> Python and R in the same program and sort of switch back and forth between
>> data representations. This may provide some openings for preserving and
>> accessing metadata when needed.
>>
>> Realistically, if R was being designed from scratch TODAY, many things
>> might be done differently. But I recall it being developed at Bell Labs for
>> purposes where it was sort of revolutionary at the time (back when it was
>> S) and designed to do things in a vectorized way and probably primarily for
>> the kinds of scientific and mathematical operations where a single NA (of
>> several types depending on the data) was enough when augmented by a few
>> things like a Nan and Inf and -Inf. I doubt they seriously saw a need for
>> an unlimited number of NA that were all the same AND also all different
>> that they felt had to be built-in. As noted, had they had a reason to make
>> it fully object-oriented too and made the base types such as integer into
>> full-fledged objects with room for additional metadata, then things may be
>> different. I note I have seen languages which have both a data type called
>> integer as lower case and Integer as upper case. One of them is regularly
>> boxed and unboxed automagically when used in a context that needs the
>> other. As far as efficiency goes, this invisibly adds many steps. So do
>> languages that sometimes take a variable that is a pointer and invisibly
>> reference it to provide the underlying field rather than make you do extra
>> typing and so on.
>>
>> So is there any reason only an NA should have such meta-data? Why not have
>> reasons associated with Inf stating it was an Inf because you asked for one
>> or the result of a calculation such as dividing by Zero (albeit maybe that
>> might be a NaN) and so on. Maybe I could annotate integers with whether
>> they are prime or even  versus odd  or a factor of 144 or anything else I
>> can imagine. But at some point, the overhead from allowing all this can
>> become substantial. I was amused at how python allows a function to be
>> annotated including by itself since it is an object. So it can store such
>> metadata perhaps in an attached dictionary so a complex costly calculation
>> can have the results cached and when you ask for the same thing in the same
>> session, it checks if it has done it and just returns the result in linear
>> time. But after a while, how many cached results can there be?
>>
>> -----Original Message-----
>> From: R-devel <r-devel-bounces at r-project.org> On Behalf Of
>> luke-tierney at uiowa.edu
>> Sent: Monday, May 24, 2021 9:15 AM
>> To: Adrian Du?a <dusa.adrian at unibuc.ro>
>> Cc: Greg Minshall <minshall at umich.edu>; r-devel <r-devel at r-project.org>
>> Subject: Re: [Rd] [External] Re: 1954 from NA
>>
>> On Mon, 24 May 2021, Adrian Du?a wrote:
>>
>>> On Mon, May 24, 2021 at 2:11 PM Greg Minshall <minshall at umich.edu>
>> wrote:
>>>
>>>> [...]
>>>> if you have 500 columns of possibly-NA'd variables, you could have
>>>> one column of 500 "bits", where each bit has one of N values, N being
>>>> the number of explanations the corresponding column has for why the
>>>> NA exists.
>>>>
>>
>> PLEASE DO NOT DO THIS!
>>
>> It will not work reliably, as has been explained to you ad nauseam in this
>> thread.
>>
>> If you distribute code that does this it will only lead to bug reports on
>> R that will waste R-core time.
>>
>> As Alex explained, you can use attributes for this. If you need operations
>> to preserve attributes across subsetting you can define subsetting methods
>> that do that.
>>
>> If you are dead set on doing something in C you can try to develop an
>> ALTREP class that provides augmented missing value information.
>>
>> Best,
>>
>> luke
>>
>>
>>
>>>
>>> The mere thought of implementing something like that gives me shivers.
>>> Not to mention such a solution should also be robust when subsetting,
>>> splitting, column and row binding, etc. and everything can be lost if
>>> the user deletes that particular column without realising its importance.
>>>
>>> Social science datasets are much more alive and complex than one might
>>> first think: there are multi-wave studies with tens of countries, and
>>> aggregating such data is already a complex process to add even more
>>> complexity on top of that.
>>>
>>> As undocumented as they may be, or even subject to change, I think the
>>> R internals are much more reliable that this.
>>>
>>> Best wishes,
>>> Adrian
>>>
>>>
>>
>> --
>> Luke Tierney
>> Ralph E. Wareham Professor of Mathematical Sciences
>> University of Iowa                  Phone:             319-335-3386
>> Department of Statistics and        Fax:               319-335-3017
>>     Actuarial Science
>> 241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
>> Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>
>
>

-- 
Luke Tierney
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu

From du@@@@dr|@n @end|ng |rom gm@||@com  Tue May 25 17:12:56 2021
From: du@@@@dr|@n @end|ng |rom gm@||@com (=?UTF-8?B?QWRyaWFuIER1yJlh?=)
Date: Tue, 25 May 2021 18:12:56 +0300
Subject: [Rd] [External] Re: 1954 from NA
In-Reply-To: <alpine.DEB.2.21.2105250810550.3254@luke-Latitude-7480>
References: <CAJ=0CtCfRMJM6wq1ggkSv2BNfX+CGnW2N-SZAN8poOG3ev=nUA@mail.gmail.com>
 <3443246.1621854690@apollo2.minshall.org>
 <CAJ=0CtDOuY+tVf+YxO0gdGY9XgaTv_Q4kMP2p_Z8CxrnHjT4-w@mail.gmail.com>
 <alpine.DEB.2.21.2105240805580.3254@luke-Latitude-7480>
 <016e01d7511b$29c3bea0$7d4b3be0$@verizon.net>
 <CAJ=0CtDC4jEfa-9qqDhixqSbvh1528wD5QpObQuwqxRs1EfiEw@mail.gmail.com>
 <alpine.DEB.2.21.2105250810550.3254@luke-Latitude-7480>
Message-ID: <CAJ=0CtC7HSofWhct6=Wgx31ZBTzzRmbxUZbZvZYp-HYc6pMMXQ@mail.gmail.com>

On Tue, May 25, 2021 at 4:14 PM <luke-tierney at uiowa.edu> wrote:

> [...]
>
> Yes, it should be discarded.
>
> You can of course do what you like in code you keep to yourself. But
> please do not distribute code that does this. via CRAN or any other
> means. It will only create problems for those maintaining R.
>
> > After all, the NA is nothing but a tagged NaN.
>
> And we are now paying a price for what was, in hindsight, an
> unfortunate decision.
>

I (only now) understand that. That code is based on the R sources and (mind
you) an almost identical one from package haven.

Regardless, it was not the code I was trying to show, but the vignette: the
end result, the functionality of the software.
That is, automatically treat declared missing values as NAs, without users
being required to explicitly deal with attributes.

Now that I think about it, there might be a way to do this without tagging
NAs, so back to square one.

Best wishes,
Adrian

	[[alternative HTML version deleted]]


From jke@ne @end|ng |rom gm@||@com  Tue May 25 20:45:35 2021
From: jke@ne @end|ng |rom gm@||@com (Jonathan Keane)
Date: Tue, 25 May 2021 13:45:35 -0500
Subject: [Rd] Should all.equal.POSIXt respect check.attributes?
Message-ID: <CAE+qdJaaL0v6nwDUEm8z7gAho5-NdvyuZ7nwZX2FEAMSPLMDsg@mail.gmail.com>

Hello,

Since bugzilla #17277
(https://bugs.r-project.org/bugzilla/show_bug.cgi?id=17277) was
resolved all.equal.POSIXt now compares timezone attributes. Comment 4
(https://bugs.r-project.org/bugzilla/show_bug.cgi?id=17277#c4) in that
ticket makes reference that both arguments check.tz (which appears to
have actually been implemented as check.tzone) and check.attributes
should disable this checking. However looking at the implementation
(and behavior with a devel version of R) I'm   finding that
check.attributes is not disabling the timezone checks.

Should the more general check.attributes disable this check (as well
as being able to specifically disable only timezone checks with
check.tzone)?

-Jon


From @v|gro@@ @end|ng |rom ver|zon@net  Tue May 25 21:01:59 2021
From: @v|gro@@ @end|ng |rom ver|zon@net (Avi Gross)
Date: Tue, 25 May 2021 15:01:59 -0400
Subject: [Rd] [External] Re: 1954 from NA
In-Reply-To: <CAJ=0CtDC4jEfa-9qqDhixqSbvh1528wD5QpObQuwqxRs1EfiEw@mail.gmail.com>
References: <CAJ=0CtCfRMJM6wq1ggkSv2BNfX+CGnW2N-SZAN8poOG3ev=nUA@mail.gmail.com>
 <3443246.1621854690@apollo2.minshall.org>
 <CAJ=0CtDOuY+tVf+YxO0gdGY9XgaTv_Q4kMP2p_Z8CxrnHjT4-w@mail.gmail.com>
 <alpine.DEB.2.21.2105240805580.3254@luke-Latitude-7480>
 <016e01d7511b$29c3bea0$7d4b3be0$@verizon.net>
 <CAJ=0CtDC4jEfa-9qqDhixqSbvh1528wD5QpObQuwqxRs1EfiEw@mail.gmail.com>
Message-ID: <01b201d75198$70f8a060$52e9e120$@verizon.net>

That helps get more understanding of what you want to do, Adrian. Getting anyone to switch is always a challenge but changing R enough to tempt them may be a bigger challenge. His is an old story. I was the first adopter for C++ in my area and at first had to have my code be built with an all C project making me reinvent some wheels so the same ?make? system knew how to build the two compatibly and link them. Of course, they all eventually had to join me in a later release but I had moved forward by then.

 

I have changed (or more accurately added) lots of languages in my life and continue to do so. The biggest challenge is not to just adapt and use it similarly to the previous ones already mastered but to understand WHY someone designed the language this way and what kind of idioms are common and useful even if that means a new way of thinking. But, of course, any ?older? language has evolved and often drifted in multiple directions. Many now borrow heavily from others even when the philosophy is different and often the results are not pretty. Making major changes in R might have serious impacts on existing programs including just by making them fail as they run out of memory.

 

If you look at R, there is plenty you can do in base R, sometimes by standing on your head. Yet you see package after package coming along that offers not just new things but sometimes a reworking and even remodeling of old things. R has a base graphics system I now rarely use and another called lattice I have no reason to use again because I can do so much quite easily in ggplot. Similarly, the evolving tidyverse group of packages approaches things from an interesting direction to the point where many people mainly use it and not base R. So if they were to teach a class in how to gather your data and analyze it and draw pretty pictures, the students might walk away thinking they had learned R but actually have learned these packages.

 

Your scenario seems related to a common scenario of how we can have values that signal beyond some range in an out-of-band manner. Years ago we had functions in languages like C that would return a -1 on failure when only non-negative results were otherwise possible. That can work fine but fails in cases when any possible value in the range can be returned. We have languages that deal with this kind of thing using error handling constructs like exceptions.  Sometimes you bundle up multiple items into a structure and return that with one element of the structure holding some kind of return status and another holding the payload. A variation on this theme, as in languages like GO is to have function that return multiple values with one of them containing nil on success and an error structure on failure.

 

The situation we have here that seems to be of concern to you is that you would like each item in a structure to have attributes that are recognized and propagated as it is being processed. Older languages tended not to even have a concept so basic types simply existed and two instances of the number 5 might even be the same underlying one or two strings with the same contents and so on. You could of course play the game of making a struct, as mentioned above, but then you needed your own code to do all the handling as nothing else knew it contained multiple items and which ones had which purpose.

 

R did add generalized attributes and some are fairly well integrated or at least partially. ?Names? were discussed as not being easy to keep around. Factors used their own tagging method that seems to work fairly well but probably not everywhere. But what you want may be more general and not built on similar foundations.

 

I look at languages like Python that are arguably more object-oriented now than R is and in some ways can be extended better, albeit not in others. If I wanted to create an object to hold the number 5 and I add methods to the object that allow it to participate in various ways with other objects using the hidden payload but also sometimes using the hidden payload, then I might pair it with the string ?five? but also with dozens of other strings for the word representing 5 in many languages. So I might have it act like a number in numerical situations and like text when someone is using it in writing a novel in any of many languages.

 

You seem to want to have the original text visible that gives a reason something is missing (or something like that) but have the software TREAT it like it is missing in calculations. In effect, you want is.na() to be a bit more like is.numeric() or is.character() and care more about the TYPE of what is being stored. An item may contain a 999 and yet not be seen as a number but as an NA. The problem I see is that you also may want the item to be a string like ?DELETED? and yet include it in the vector that R insists can only hold integers. R does have a built-in data structure called a list that indeed allows that. You can easily store data as a list of lists rather than a list of vectors and many other structures. Some of those structures might handle your needs BUT may only work properly if you build your own packages as with  the tidyverse and break as soon as any other functions encountered them!

 

But then you would arguably no longer be in R but in your own universe based on R.

 

I have written much code that does things a bit sideways. For example, I might have a treelike structure in which you do some form of search till you encounter a leaf node and return that value to be used in a calculation. To perform a calculation using multiple trees such as taking an average, you always use find_value(tree) and never hand over the tree itself. As I think I pointed out earlier, you can do things like that in many places and hand over a variation of your data. In the ggplot example, you might have:

 

ggplot(data=mydata, aes(x=abs(col1), y=convert_string_to_numeric(col2)) ?

 

Ggplot would not use the original data in plotting but the view it is asked to use. The function I made up above would know what values are some form of NA and convert all others like ?12.3? to numeric form. BUT it would not act as simply or smoothly as when your data is already in the format everyone else uses.

 

So how does R know what something is? Presumably there is some overhead associated with a vector or some table that records the type. A list presumably depends on each internal item to have such a type. So maybe what you want is for each item in a vector to have a type where one type is some for of NA. But as noted, R does often not give a damn about an NA and happily uses it to create more nonsense. The mean of a bunch of numbers that includes one or more copies of things like NA (or NaN or inf) can pollute them all. Generally R is not designed to give a darn. When people complain, they may get mean to add an na.rm=TRUE or remove them some way before asking for a mean or perhaps reset them to something like zero.

 

So if you want to leave your variables in place with assorted meanings but a tag saying they are to be treated as NA, much in R might have to change. Your suggested approach though is not yet clear but might mean doing something analogous to using extra bits and hoping nobody will notice.

 

So, the solution is both blindingly obvious and even more blindingly stupid. Use complex numbers! All normal content shall be stored as numbers like 5.3+0i and any variant on NA shall be stored as something like 0+3i where 3 means an NA of type 3.

 

OK, humor aside, since the social sciences do not tend to even know what complex numbers are, this should provide another dimension to hide lots of meaningless info. Heck, you could convert  message like ?LATE? into some numeric form. Assuming an English centered world (which I do not!) you could store it with L replaced by 12 and A by 01 and so on so the imaginary component might look like 0+12011905i and easily decoded back into LATE when needed. Again, not a serious proposal. The storage probably would be twice the size of a numeric albeit you can extract the real part when needed for normal calculations and the imaginary part when you want to know about NA type or whatever. 

 

What R really is missing is quaternions and octonions which are the only two other variations on complex numbers that are possible and are sort of complex numbers on steroids with either three or seven distinct square roots of minus-one  so they allow storage along additional axes in other dimensions.

 

Yes, I am sure someone wrote a package for that! LOL!

 

Ah, here is one: https://cran.r-project.org/web/packages/onion/onion.pdf

 

I will end by saying my experience is that enticing people to do something new is just a start. After they start, you often get lots of complaints and requests for help and even requests to help them move back! Unless you make some popular package everyone runs to, NOBODY else will be able to help them on some things. The reality is that some of the more common tasks these people do are sometimes already optimized for them and often do not make them know more. I have had to use these systems and for some common tasks they are easy. Dialog boxes can pop up and let you checks off various options and off you go. No need to learn lots of programming details like the names of various functions that do a Tukey test and what arguments they need and what errors might have to be handled and so on. I know SPSS often produces LOTS of output including many things you do not wat and then lets you remove parts you don?t need or even know what they mean. Sure, R can have similar functionality but often you are expected to sort of stitch various parts together as well as ADD your own bits. I love that and value being able to be creative. In my experience, most normal people just want to get the job done and be fairly certain others accept the results ad then do other activities they are better suited for, or at least think they are.

 

There are intermediates I have used where I let them do various kinds of processing on SPSS and save the result in some format I can read into R for additional processing. The latter may not be stuff that requires keeping track of multiple NA equivalents. Of course if you want to save the results and move them back, that is  a challenge. Hybrid approaches may tempt them to try something and maybe later do more and more and move over.

 

From: Adrian Du?a <dusa.adrian at unibuc.ro> 
Sent: Tuesday, May 25, 2021 2:17 AM
To: Avi Gross <avigross at verizon.net>
Cc: r-devel <r-devel at r-project.org>
Subject: Re: [Rd] [External] Re: 1954 from NA

 

Dear Avi,

 

Thank you so much for the extended messages, I read them carefully.

While partially offering a solution (I've already been there), it creates additional work for the user, and some of that is unnecessary.

 

What I am trying to achieve is best described in this draft vignette:

 

devtools::install_github("dusadrian/mixed")

vignette("mixed")

 

Once a value is declared to be missing, the user should not do anything else about it. Despite being present, the value should automatically be treated as missing by the software. That is the way it's done in all major statistical packages like SAS, Stata and even SPSS.

 

My end goal is to make R attractive for my faculty peers (and beyond), almost all of whom are massively using SPSS and sometimes Stata. But in order to convince them to (finally) make the switch, I need to provide similar functionality, not additional work.

 

Re. your first part of the message, I am definitely not trying to change the R internals. The NA will still be NA, exactly as currently defined.

My initial proposal was based on the observation that the 1954 payload was stored as an unsigned int (thus occupying 32 bits) when it is obvious it doesn't need more than 16. That was the only proposed modification, and everything else stays the same.

 

I now learned, thanks to all contributors in this list, that building something around that payload is risky because we do not know exactly what the compilers will do. One possible solution that I can think of, while (still) maintaining the current functionality around the NA, is to use a different high word for the NA that would not trigger compilation issues. But I have absolutely no idea what that implies for the other inner workings of R.

 

I very much trust the R core will eventually find a robust solution, they've solved much more complicated problems than this. I just hope the current thread will push the idea of tagged NAs on the table, for when they will discuss this.

 

Once that will be solved, and despite the current advice discouraging this route, I believe tagging NAs is a valuable idea that should not be discarded.

After all, the NA is nothing but a tagged NaN.

 

All the best,

Adrian

 

 

On Tue, May 25, 2021 at 7:05 AM Avi Gross via R-devel <r-devel at r-project.org <mailto:r-devel at r-project.org> > wrote:

I was thinking about how one does things in a language that is properly object-oriented versus R that makes various half-assed attempts at being such.

Clearly in some such languages you can make an object that is a wrapper that allows you to save an item that is the main payload as well as anything else you want. You might need a way to convince everything else to allow you to make things like lists and vectors and other collections of the objects and perhaps automatically unbox them for many purposes. As an example in a language like Python, you might provide methods so that adding A and B actually gets the value out of A and/or B and adds them properly.  But there may be too many edge cases to handle and some software may not pay attention to what you want including some libraries written in other languages.

I mention Python for the odd reason that it is now possible to combine Python and R in the same program and sort of switch back and forth between data representations. This may provide some openings for preserving and accessing metadata when needed.

Realistically, if R was being designed from scratch TODAY, many things might be done differently. But I recall it being developed at Bell Labs for purposes where it was sort of revolutionary at the time (back when it was S) and designed to do things in a vectorized way and probably primarily for the kinds of scientific and mathematical operations where a single NA (of several types depending on the data) was enough when augmented by a few things like a Nan and Inf and -Inf. I doubt they seriously saw a need for an unlimited number of NA that were all the same AND also all different that they felt had to be built-in. As noted, had they had a reason to make it fully object-oriented too and made the base types such as integer into full-fledged objects with room for additional metadata, then things may be different. I note I have seen languages which have both a data type called integer as lower case and Integer as upper case. One of them is regularly boxed and unboxed automagically when used in a context that needs the other. As far as efficiency goes, this invisibly adds many steps. So do languages that sometimes take a variable that is a pointer and invisibly reference it to provide the underlying field rather than make you do extra typing and so on.

So is there any reason only an NA should have such meta-data? Why not have reasons associated with Inf stating it was an Inf because you asked for one or the result of a calculation such as dividing by Zero (albeit maybe that might be a NaN) and so on. Maybe I could annotate integers with whether they are prime or even  versus odd  or a factor of 144 or anything else I can imagine. But at some point, the overhead from allowing all this can become substantial. I was amused at how python allows a function to be annotated including by itself since it is an object. So it can store such metadata perhaps in an attached dictionary so a complex costly calculation can have the results cached and when you ask for the same thing in the same session, it checks if it has done it and just returns the result in linear time. But after a while, how many cached results can there be?

-----Original Message-----
From: R-devel <r-devel-bounces at r-project.org <mailto:r-devel-bounces at r-project.org> > On Behalf Of luke-tierney at uiowa.edu <mailto:luke-tierney at uiowa.edu> 
Sent: Monday, May 24, 2021 9:15 AM
To: Adrian Du?a <dusa.adrian at unibuc.ro <mailto:dusa.adrian at unibuc.ro> >
Cc: Greg Minshall <minshall at umich.edu <mailto:minshall at umich.edu> >; r-devel <r-devel at r-project.org <mailto:r-devel at r-project.org> >
Subject: Re: [Rd] [External] Re: 1954 from NA

On Mon, 24 May 2021, Adrian Du?a wrote:

> On Mon, May 24, 2021 at 2:11 PM Greg Minshall <minshall at umich.edu <mailto:minshall at umich.edu> > wrote:
>
>> [...]
>> if you have 500 columns of possibly-NA'd variables, you could have 
>> one column of 500 "bits", where each bit has one of N values, N being 
>> the number of explanations the corresponding column has for why the 
>> NA exists.
>>

PLEASE DO NOT DO THIS!

It will not work reliably, as has been explained to you ad nauseam in this thread.

If you distribute code that does this it will only lead to bug reports on R that will waste R-core time.

As Alex explained, you can use attributes for this. If you need operations to preserve attributes across subsetting you can define subsetting methods that do that.

If you are dead set on doing something in C you can try to develop an ALTREP class that provides augmented missing value information.

Best,

luke



>
> The mere thought of implementing something like that gives me shivers. 
> Not to mention such a solution should also be robust when subsetting, 
> splitting, column and row binding, etc. and everything can be lost if 
> the user deletes that particular column without realising its importance.
>
> Social science datasets are much more alive and complex than one might 
> first think: there are multi-wave studies with tens of countries, and 
> aggregating such data is already a complex process to add even more 
> complexity on top of that.
>
> As undocumented as they may be, or even subject to change, I think the 
> R internals are much more reliable that this.
>
> Best wishes,
> Adrian
>
>

--
Luke Tierney
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu <mailto:luke-tierney at uiowa.edu> 
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu
______________________________________________
R-devel at r-project.org <mailto:R-devel at r-project.org>  mailing list
https://stat.ethz.ch/mailman/listinfo/r-devel

______________________________________________
R-devel at r-project.org <mailto:R-devel at r-project.org>  mailing list
https://stat.ethz.ch/mailman/listinfo/r-devel






-- 

Adrian Dusa
University of Bucharest
Romanian Social Data Archive
Soseaua Panduri nr. 90-92
050663 Bucharest sector 5
Romania

https://adriandusa.eu


	[[alternative HTML version deleted]]


From du@@@@dr|@n @end|ng |rom gm@||@com  Wed May 26 01:01:49 2021
From: du@@@@dr|@n @end|ng |rom gm@||@com (=?UTF-8?B?QWRyaWFuIER1yJlh?=)
Date: Wed, 26 May 2021 02:01:49 +0300
Subject: [Rd] [External] Re: 1954 from NA
In-Reply-To: <01b201d75198$70f8a060$52e9e120$@verizon.net>
References: <CAJ=0CtCfRMJM6wq1ggkSv2BNfX+CGnW2N-SZAN8poOG3ev=nUA@mail.gmail.com>
 <3443246.1621854690@apollo2.minshall.org>
 <CAJ=0CtDOuY+tVf+YxO0gdGY9XgaTv_Q4kMP2p_Z8CxrnHjT4-w@mail.gmail.com>
 <alpine.DEB.2.21.2105240805580.3254@luke-Latitude-7480>
 <016e01d7511b$29c3bea0$7d4b3be0$@verizon.net>
 <CAJ=0CtDC4jEfa-9qqDhixqSbvh1528wD5QpObQuwqxRs1EfiEw@mail.gmail.com>
 <01b201d75198$70f8a060$52e9e120$@verizon.net>
Message-ID: <CAJ=0CtBofw9JoCUAxDGueYHZs5NghJD6A7ND97nfpvm_0n6fMg@mail.gmail.com>

Dear Avi,

That was quite a lengthy email...
What you write makes sense of course. I try hard not to deviate from the
base R, and thought my solution does just that but apparently no such luck.

I suspect, however, that something will have to eventually change: since
one of the R building blocks (such as an NA) is questioned by compilers, it
is serious enough to attract attention from the R core and maintainers.
And if that happens, my fingers are crossed the solution would allow users
to declare existing values as missing.

The importance of that, for the social sciences, cannot be stressed enough.

Best wishes, thanks once again to everyone,
Adrian

On Tue, May 25, 2021 at 10:03 PM Avi Gross via R-devel <
r-devel at r-project.org> wrote:

> That helps get more understanding of what you want to do, Adrian. Getting
> anyone to switch is always a challenge but changing R enough to tempt them
> may be a bigger challenge. His is an old story. I was the first adopter for
> C++ in my area and at first had to have my code be built with an all C
> project making me reinvent some wheels so the same ?make? system knew how
> to build the two compatibly and link them. Of course, they all eventually
> had to join me in a later release but I had moved forward by then.
>
>
>
> I have changed (or more accurately added) lots of languages in my life and
> continue to do so. The biggest challenge is not to just adapt and use it
> similarly to the previous ones already mastered but to understand WHY
> someone designed the language this way and what kind of idioms are common
> and useful even if that means a new way of thinking. But, of course, any
> ?older? language has evolved and often drifted in multiple directions. Many
> now borrow heavily from others even when the philosophy is different and
> often the results are not pretty. Making major changes in R might have
> serious impacts on existing programs including just by making them fail as
> they run out of memory.
>
>
>
> If you look at R, there is plenty you can do in base R, sometimes by
> standing on your head. Yet you see package after package coming along that
> offers not just new things but sometimes a reworking and even remodeling of
> old things. R has a base graphics system I now rarely use and another
> called lattice I have no reason to use again because I can do so much quite
> easily in ggplot. Similarly, the evolving tidyverse group of packages
> approaches things from an interesting direction to the point where many
> people mainly use it and not base R. So if they were to teach a class in
> how to gather your data and analyze it and draw pretty pictures, the
> students might walk away thinking they had learned R but actually have
> learned these packages.
>
>
>
> Your scenario seems related to a common scenario of how we can have values
> that signal beyond some range in an out-of-band manner. Years ago we had
> functions in languages like C that would return a -1 on failure when only
> non-negative results were otherwise possible. That can work fine but fails
> in cases when any possible value in the range can be returned. We have
> languages that deal with this kind of thing using error handling constructs
> like exceptions.  Sometimes you bundle up multiple items into a structure
> and return that with one element of the structure holding some kind of
> return status and another holding the payload. A variation on this theme,
> as in languages like GO is to have function that return multiple values
> with one of them containing nil on success and an error structure on
> failure.
>
>
>
> The situation we have here that seems to be of concern to you is that you
> would like each item in a structure to have attributes that are recognized
> and propagated as it is being processed. Older languages tended not to even
> have a concept so basic types simply existed and two instances of the
> number 5 might even be the same underlying one or two strings with the same
> contents and so on. You could of course play the game of making a struct,
> as mentioned above, but then you needed your own code to do all the
> handling as nothing else knew it contained multiple items and which ones
> had which purpose.
>
>
>
> R did add generalized attributes and some are fairly well integrated or at
> least partially. ?Names? were discussed as not being easy to keep around.
> Factors used their own tagging method that seems to work fairly well but
> probably not everywhere. But what you want may be more general and not
> built on similar foundations.
>
>
>
> I look at languages like Python that are arguably more object-oriented now
> than R is and in some ways can be extended better, albeit not in others. If
> I wanted to create an object to hold the number 5 and I add methods to the
> object that allow it to participate in various ways with other objects
> using the hidden payload but also sometimes using the hidden payload, then
> I might pair it with the string ?five? but also with dozens of other
> strings for the word representing 5 in many languages. So I might have it
> act like a number in numerical situations and like text when someone is
> using it in writing a novel in any of many languages.
>
>
>
> You seem to want to have the original text visible that gives a reason
> something is missing (or something like that) but have the software TREAT
> it like it is missing in calculations. In effect, you want is.na() to be
> a bit more like is.numeric() or is.character() and care more about the TYPE
> of what is being stored. An item may contain a 999 and yet not be seen as a
> number but as an NA. The problem I see is that you also may want the item
> to be a string like ?DELETED? and yet include it in the vector that R
> insists can only hold integers. R does have a built-in data structure
> called a list that indeed allows that. You can easily store data as a list
> of lists rather than a list of vectors and many other structures. Some of
> those structures might handle your needs BUT may only work properly if you
> build your own packages as with  the tidyverse and break as soon as any
> other functions encountered them!
>
>
>
> But then you would arguably no longer be in R but in your own universe
> based on R.
>
>
>
> I have written much code that does things a bit sideways. For example, I
> might have a treelike structure in which you do some form of search till
> you encounter a leaf node and return that value to be used in a
> calculation. To perform a calculation using multiple trees such as taking
> an average, you always use find_value(tree) and never hand over the tree
> itself. As I think I pointed out earlier, you can do things like that in
> many places and hand over a variation of your data. In the ggplot example,
> you might have:
>
>
>
> ggplot(data=mydata, aes(x=abs(col1), y=convert_string_to_numeric(col2)) ?
>
>
>
> Ggplot would not use the original data in plotting but the view it is
> asked to use. The function I made up above would know what values are some
> form of NA and convert all others like ?12.3? to numeric form. BUT it would
> not act as simply or smoothly as when your data is already in the format
> everyone else uses.
>
>
>
> So how does R know what something is? Presumably there is some overhead
> associated with a vector or some table that records the type. A list
> presumably depends on each internal item to have such a type. So maybe what
> you want is for each item in a vector to have a type where one type is some
> for of NA. But as noted, R does often not give a damn about an NA and
> happily uses it to create more nonsense. The mean of a bunch of numbers
> that includes one or more copies of things like NA (or NaN or inf) can
> pollute them all. Generally R is not designed to give a darn. When people
> complain, they may get mean to add an na.rm=TRUE or remove them some way
> before asking for a mean or perhaps reset them to something like zero.
>
>
>
> So if you want to leave your variables in place with assorted meanings but
> a tag saying they are to be treated as NA, much in R might have to change.
> Your suggested approach though is not yet clear but might mean doing
> something analogous to using extra bits and hoping nobody will notice.
>
>
>
> So, the solution is both blindingly obvious and even more blindingly
> stupid. Use complex numbers! All normal content shall be stored as numbers
> like 5.3+0i and any variant on NA shall be stored as something like 0+3i
> where 3 means an NA of type 3.
>
>
>
> OK, humor aside, since the social sciences do not tend to even know what
> complex numbers are, this should provide another dimension to hide lots of
> meaningless info. Heck, you could convert  message like ?LATE? into some
> numeric form. Assuming an English centered world (which I do not!) you
> could store it with L replaced by 12 and A by 01 and so on so the imaginary
> component might look like 0+12011905i and easily decoded back into LATE
> when needed. Again, not a serious proposal. The storage probably would be
> twice the size of a numeric albeit you can extract the real part when
> needed for normal calculations and the imaginary part when you want to know
> about NA type or whatever.
>
>
>
> What R really is missing is quaternions and octonions which are the only
> two other variations on complex numbers that are possible and are sort of
> complex numbers on steroids with either three or seven distinct square
> roots of minus-one  so they allow storage along additional axes in other
> dimensions.
>
>
>
> Yes, I am sure someone wrote a package for that! LOL!
>
>
>
> Ah, here is one: https://cran.r-project.org/web/packages/onion/onion.pdf
>
>
>
> I will end by saying my experience is that enticing people to do something
> new is just a start. After they start, you often get lots of complaints and
> requests for help and even requests to help them move back! Unless you make
> some popular package everyone runs to, NOBODY else will be able to help
> them on some things. The reality is that some of the more common tasks
> these people do are sometimes already optimized for them and often do not
> make them know more. I have had to use these systems and for some common
> tasks they are easy. Dialog boxes can pop up and let you checks off various
> options and off you go. No need to learn lots of programming details like
> the names of various functions that do a Tukey test and what arguments they
> need and what errors might have to be handled and so on. I know SPSS often
> produces LOTS of output including many things you do not wat and then lets
> you remove parts you don?t need or even know what they mean. Sure, R can
> have similar functionality but often you are expected to sort of stitch
> various parts together as well as ADD your own bits. I love that and value
> being able to be creative. In my experience, most normal people just want
> to get the job done and be fairly certain others accept the results ad then
> do other activities they are better suited for, or at least think they are.
>
>
>
> There are intermediates I have used where I let them do various kinds of
> processing on SPSS and save the result in some format I can read into R for
> additional processing. The latter may not be stuff that requires keeping
> track of multiple NA equivalents. Of course if you want to save the results
> and move them back, that is  a challenge. Hybrid approaches may tempt them
> to try something and maybe later do more and more and move over.
>

	[[alternative HTML version deleted]]


From murdoch@dunc@n @end|ng |rom gm@||@com  Wed May 26 01:27:02 2021
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Tue, 25 May 2021 19:27:02 -0400
Subject: [Rd] [External] Re: 1954 from NA
In-Reply-To: <CAJ=0CtBofw9JoCUAxDGueYHZs5NghJD6A7ND97nfpvm_0n6fMg@mail.gmail.com>
References: <CAJ=0CtCfRMJM6wq1ggkSv2BNfX+CGnW2N-SZAN8poOG3ev=nUA@mail.gmail.com>
 <3443246.1621854690@apollo2.minshall.org>
 <CAJ=0CtDOuY+tVf+YxO0gdGY9XgaTv_Q4kMP2p_Z8CxrnHjT4-w@mail.gmail.com>
 <alpine.DEB.2.21.2105240805580.3254@luke-Latitude-7480>
 <016e01d7511b$29c3bea0$7d4b3be0$@verizon.net>
 <CAJ=0CtDC4jEfa-9qqDhixqSbvh1528wD5QpObQuwqxRs1EfiEw@mail.gmail.com>
 <01b201d75198$70f8a060$52e9e120$@verizon.net>
 <CAJ=0CtBofw9JoCUAxDGueYHZs5NghJD6A7ND97nfpvm_0n6fMg@mail.gmail.com>
Message-ID: <7303ff84-bdf1-cc76-e9e2-e625e80a24ca@gmail.com>

You've already been told how to solve this:  just add attributes to the 
objects. Use the standard NA to indicate that there is some kind of 
missingness, and the attribute to describe exactly what it is.  Stick a 
class on those objects and define methods so that subsetting and 
arithmetic preserves the extra info you've added. If you do some 
operation that turns those NAs into NaNs, big deal:  the attribute will 
still be there, and is.na(NaN) still returns TRUE.

Base R doesn't need anything else.

You complained that users shouldn't need to know about attributes, and 
they won't:  you, as the author of the package that does this, will 
handle all those details.  Working in your subject area you know all the 
different kinds of NAs that people care about, and how they code them in 
input data, so you can make it all totally transparent.  If you do it 
well, someone in some other subject area with a completely different set 
of kinds of missingness will be able to adapt your code to their use.

I imagine this has all been done in one of the thousands of packages on 
CRAN, but if it hasn't been done well enough for you, do it better.

Duncan Murdoch

On 25/05/2021 7:01 p.m., Adrian Du?a wrote:
> Dear Avi,
> 
> That was quite a lengthy email...
> What you write makes sense of course. I try hard not to deviate from the
> base R, and thought my solution does just that but apparently no such luck.
> 
> I suspect, however, that something will have to eventually change: since
> one of the R building blocks (such as an NA) is questioned by compilers, it
> is serious enough to attract attention from the R core and maintainers.
> And if that happens, my fingers are crossed the solution would allow users
> to declare existing values as missing.
> 
> The importance of that, for the social sciences, cannot be stressed enough.
> 
> Best wishes, thanks once again to everyone,
> Adrian
> 
> On Tue, May 25, 2021 at 10:03 PM Avi Gross via R-devel <
> r-devel at r-project.org> wrote:
> 
>> That helps get more understanding of what you want to do, Adrian. Getting
>> anyone to switch is always a challenge but changing R enough to tempt them
>> may be a bigger challenge. His is an old story. I was the first adopter for
>> C++ in my area and at first had to have my code be built with an all C
>> project making me reinvent some wheels so the same ?make? system knew how
>> to build the two compatibly and link them. Of course, they all eventually
>> had to join me in a later release but I had moved forward by then.
>>
>>
>>
>> I have changed (or more accurately added) lots of languages in my life and
>> continue to do so. The biggest challenge is not to just adapt and use it
>> similarly to the previous ones already mastered but to understand WHY
>> someone designed the language this way and what kind of idioms are common
>> and useful even if that means a new way of thinking. But, of course, any
>> ?older? language has evolved and often drifted in multiple directions. Many
>> now borrow heavily from others even when the philosophy is different and
>> often the results are not pretty. Making major changes in R might have
>> serious impacts on existing programs including just by making them fail as
>> they run out of memory.
>>
>>
>>
>> If you look at R, there is plenty you can do in base R, sometimes by
>> standing on your head. Yet you see package after package coming along that
>> offers not just new things but sometimes a reworking and even remodeling of
>> old things. R has a base graphics system I now rarely use and another
>> called lattice I have no reason to use again because I can do so much quite
>> easily in ggplot. Similarly, the evolving tidyverse group of packages
>> approaches things from an interesting direction to the point where many
>> people mainly use it and not base R. So if they were to teach a class in
>> how to gather your data and analyze it and draw pretty pictures, the
>> students might walk away thinking they had learned R but actually have
>> learned these packages.
>>
>>
>>
>> Your scenario seems related to a common scenario of how we can have values
>> that signal beyond some range in an out-of-band manner. Years ago we had
>> functions in languages like C that would return a -1 on failure when only
>> non-negative results were otherwise possible. That can work fine but fails
>> in cases when any possible value in the range can be returned. We have
>> languages that deal with this kind of thing using error handling constructs
>> like exceptions.  Sometimes you bundle up multiple items into a structure
>> and return that with one element of the structure holding some kind of
>> return status and another holding the payload. A variation on this theme,
>> as in languages like GO is to have function that return multiple values
>> with one of them containing nil on success and an error structure on
>> failure.
>>
>>
>>
>> The situation we have here that seems to be of concern to you is that you
>> would like each item in a structure to have attributes that are recognized
>> and propagated as it is being processed. Older languages tended not to even
>> have a concept so basic types simply existed and two instances of the
>> number 5 might even be the same underlying one or two strings with the same
>> contents and so on. You could of course play the game of making a struct,
>> as mentioned above, but then you needed your own code to do all the
>> handling as nothing else knew it contained multiple items and which ones
>> had which purpose.
>>
>>
>>
>> R did add generalized attributes and some are fairly well integrated or at
>> least partially. ?Names? were discussed as not being easy to keep around.
>> Factors used their own tagging method that seems to work fairly well but
>> probably not everywhere. But what you want may be more general and not
>> built on similar foundations.
>>
>>
>>
>> I look at languages like Python that are arguably more object-oriented now
>> than R is and in some ways can be extended better, albeit not in others. If
>> I wanted to create an object to hold the number 5 and I add methods to the
>> object that allow it to participate in various ways with other objects
>> using the hidden payload but also sometimes using the hidden payload, then
>> I might pair it with the string ?five? but also with dozens of other
>> strings for the word representing 5 in many languages. So I might have it
>> act like a number in numerical situations and like text when someone is
>> using it in writing a novel in any of many languages.
>>
>>
>>
>> You seem to want to have the original text visible that gives a reason
>> something is missing (or something like that) but have the software TREAT
>> it like it is missing in calculations. In effect, you want is.na() to be
>> a bit more like is.numeric() or is.character() and care more about the TYPE
>> of what is being stored. An item may contain a 999 and yet not be seen as a
>> number but as an NA. The problem I see is that you also may want the item
>> to be a string like ?DELETED? and yet include it in the vector that R
>> insists can only hold integers. R does have a built-in data structure
>> called a list that indeed allows that. You can easily store data as a list
>> of lists rather than a list of vectors and many other structures. Some of
>> those structures might handle your needs BUT may only work properly if you
>> build your own packages as with  the tidyverse and break as soon as any
>> other functions encountered them!
>>
>>
>>
>> But then you would arguably no longer be in R but in your own universe
>> based on R.
>>
>>
>>
>> I have written much code that does things a bit sideways. For example, I
>> might have a treelike structure in which you do some form of search till
>> you encounter a leaf node and return that value to be used in a
>> calculation. To perform a calculation using multiple trees such as taking
>> an average, you always use find_value(tree) and never hand over the tree
>> itself. As I think I pointed out earlier, you can do things like that in
>> many places and hand over a variation of your data. In the ggplot example,
>> you might have:
>>
>>
>>
>> ggplot(data=mydata, aes(x=abs(col1), y=convert_string_to_numeric(col2)) ?
>>
>>
>>
>> Ggplot would not use the original data in plotting but the view it is
>> asked to use. The function I made up above would know what values are some
>> form of NA and convert all others like ?12.3? to numeric form. BUT it would
>> not act as simply or smoothly as when your data is already in the format
>> everyone else uses.
>>
>>
>>
>> So how does R know what something is? Presumably there is some overhead
>> associated with a vector or some table that records the type. A list
>> presumably depends on each internal item to have such a type. So maybe what
>> you want is for each item in a vector to have a type where one type is some
>> for of NA. But as noted, R does often not give a damn about an NA and
>> happily uses it to create more nonsense. The mean of a bunch of numbers
>> that includes one or more copies of things like NA (or NaN or inf) can
>> pollute them all. Generally R is not designed to give a darn. When people
>> complain, they may get mean to add an na.rm=TRUE or remove them some way
>> before asking for a mean or perhaps reset them to something like zero.
>>
>>
>>
>> So if you want to leave your variables in place with assorted meanings but
>> a tag saying they are to be treated as NA, much in R might have to change.
>> Your suggested approach though is not yet clear but might mean doing
>> something analogous to using extra bits and hoping nobody will notice.
>>
>>
>>
>> So, the solution is both blindingly obvious and even more blindingly
>> stupid. Use complex numbers! All normal content shall be stored as numbers
>> like 5.3+0i and any variant on NA shall be stored as something like 0+3i
>> where 3 means an NA of type 3.
>>
>>
>>
>> OK, humor aside, since the social sciences do not tend to even know what
>> complex numbers are, this should provide another dimension to hide lots of
>> meaningless info. Heck, you could convert  message like ?LATE? into some
>> numeric form. Assuming an English centered world (which I do not!) you
>> could store it with L replaced by 12 and A by 01 and so on so the imaginary
>> component might look like 0+12011905i and easily decoded back into LATE
>> when needed. Again, not a serious proposal. The storage probably would be
>> twice the size of a numeric albeit you can extract the real part when
>> needed for normal calculations and the imaginary part when you want to know
>> about NA type or whatever.
>>
>>
>>
>> What R really is missing is quaternions and octonions which are the only
>> two other variations on complex numbers that are possible and are sort of
>> complex numbers on steroids with either three or seven distinct square
>> roots of minus-one  so they allow storage along additional axes in other
>> dimensions.
>>
>>
>>
>> Yes, I am sure someone wrote a package for that! LOL!
>>
>>
>>
>> Ah, here is one: https://cran.r-project.org/web/packages/onion/onion.pdf
>>
>>
>>
>> I will end by saying my experience is that enticing people to do something
>> new is just a start. After they start, you often get lots of complaints and
>> requests for help and even requests to help them move back! Unless you make
>> some popular package everyone runs to, NOBODY else will be able to help
>> them on some things. The reality is that some of the more common tasks
>> these people do are sometimes already optimized for them and often do not
>> make them know more. I have had to use these systems and for some common
>> tasks they are easy. Dialog boxes can pop up and let you checks off various
>> options and off you go. No need to learn lots of programming details like
>> the names of various functions that do a Tukey test and what arguments they
>> need and what errors might have to be handled and so on. I know SPSS often
>> produces LOTS of output including many things you do not wat and then lets
>> you remove parts you don?t need or even know what they mean. Sure, R can
>> have similar functionality but often you are expected to sort of stitch
>> various parts together as well as ADD your own bits. I love that and value
>> being able to be creative. In my experience, most normal people just want
>> to get the job done and be fairly certain others accept the results ad then
>> do other activities they are better suited for, or at least think they are.
>>
>>
>>
>> There are intermediates I have used where I let them do various kinds of
>> processing on SPSS and save the result in some format I can read into R for
>> additional processing. The latter may not be stuff that requires keeping
>> track of multiple NA equivalents. Of course if you want to save the results
>> and move them back, that is  a challenge. Hybrid approaches may tempt them
>> to try something and maybe later do more and more and move over.
>>
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From greg @end|ng |rom w@rne@@net  Wed May 26 03:13:03 2021
From: greg @end|ng |rom w@rne@@net (Gregory Warnes)
Date: Tue, 25 May 2021 21:13:03 -0400
Subject: [Rd] [External] Re: 1954 from NA
In-Reply-To: <01b201d75198$70f8a060$52e9e120$@verizon.net>
References: <01b201d75198$70f8a060$52e9e120$@verizon.net>
Message-ID: <CF512D1E-69AD-4CEE-B85F-78F13BD06B10@warnes.net>

As a side note, for floating point values, the IEEE 754 standard provides for a large set of NaN values, making it possible to have multiple types of NAs for floating point values...

Sent from my iPad

> On May 25, 2021, at 3:03 PM, Avi Gross via R-devel <r-devel at r-project.org> wrote:
> 
> ?That helps get more understanding of what you want to do, Adrian. Getting anyone to switch is always a challenge but changing R enough to tempt them may be a bigger challenge. His is an old story. I was the first adopter for C++ in my area and at first had to have my code be built with an all C project making me reinvent some wheels so the same ?make? system knew how to build the two compatibly and link them. Of course, they all eventually had to join me in a later release but I had moved forward by then.
> 
> 
> 
> I have changed (or more accurately added) lots of languages in my life and continue to do so. The biggest challenge is not to just adapt and use it similarly to the previous ones already mastered but to understand WHY someone designed the language this way and what kind of idioms are common and useful even if that means a new way of thinking. But, of course, any ?older? language has evolved and often drifted in multiple directions. Many now borrow heavily from others even when the philosophy is different and often the results are not pretty. Making major changes in R might have serious impacts on existing programs including just by making them fail as they run out of memory.
> 
> 
> 
> If you look at R, there is plenty you can do in base R, sometimes by standing on your head. Yet you see package after package coming along that offers not just new things but sometimes a reworking and even remodeling of old things. R has a base graphics system I now rarely use and another called lattice I have no reason to use again because I can do so much quite easily in ggplot. Similarly, the evolving tidyverse group of packages approaches things from an interesting direction to the point where many people mainly use it and not base R. So if they were to teach a class in how to gather your data and analyze it and draw pretty pictures, the students might walk away thinking they had learned R but actually have learned these packages.
> 
> 
> 
> Your scenario seems related to a common scenario of how we can have values that signal beyond some range in an out-of-band manner. Years ago we had functions in languages like C that would return a -1 on failure when only non-negative results were otherwise possible. That can work fine but fails in cases when any possible value in the range can be returned. We have languages that deal with this kind of thing using error handling constructs like exceptions.  Sometimes you bundle up multiple items into a structure and return that with one element of the structure holding some kind of return status and another holding the payload. A variation on this theme, as in languages like GO is to have function that return multiple values with one of them containing nil on success and an error structure on failure.
> 
> 
> 
> The situation we have here that seems to be of concern to you is that you would like each item in a structure to have attributes that are recognized and propagated as it is being processed. Older languages tended not to even have a concept so basic types simply existed and two instances of the number 5 might even be the same underlying one or two strings with the same contents and so on. You could of course play the game of making a struct, as mentioned above, but then you needed your own code to do all the handling as nothing else knew it contained multiple items and which ones had which purpose.
> 
> 
> 
> R did add generalized attributes and some are fairly well integrated or at least partially. ?Names? were discussed as not being easy to keep around. Factors used their own tagging method that seems to work fairly well but probably not everywhere. But what you want may be more general and not built on similar foundations.
> 
> 
> 
> I look at languages like Python that are arguably more object-oriented now than R is and in some ways can be extended better, albeit not in others. If I wanted to create an object to hold the number 5 and I add methods to the object that allow it to participate in various ways with other objects using the hidden payload but also sometimes using the hidden payload, then I might pair it with the string ?five? but also with dozens of other strings for the word representing 5 in many languages. So I might have it act like a number in numerical situations and like text when someone is using it in writing a novel in any of many languages.
> 
> 
> 
> You seem to want to have the original text visible that gives a reason something is missing (or something like that) but have the software TREAT it like it is missing in calculations. In effect, you want is.na() to be a bit more like is.numeric() or is.character() and care more about the TYPE of what is being stored. An item may contain a 999 and yet not be seen as a number but as an NA. The problem I see is that you also may want the item to be a string like ?DELETED? and yet include it in the vector that R insists can only hold integers. R does have a built-in data structure called a list that indeed allows that. You can easily store data as a list of lists rather than a list of vectors and many other structures. Some of those structures might handle your needs BUT may only work properly if you build your own packages as with  the tidyverse and break as soon as any other functions encountered them!
> 
> 
> 
> But then you would arguably no longer be in R but in your own universe based on R.
> 
> 
> 
> I have written much code that does things a bit sideways. For example, I might have a treelike structure in which you do some form of search till you encounter a leaf node and return that value to be used in a calculation. To perform a calculation using multiple trees such as taking an average, you always use find_value(tree) and never hand over the tree itself. As I think I pointed out earlier, you can do things like that in many places and hand over a variation of your data. In the ggplot example, you might have:
> 
> 
> 
> ggplot(data=mydata, aes(x=abs(col1), y=convert_string_to_numeric(col2)) ?
> 
> 
> 
> Ggplot would not use the original data in plotting but the view it is asked to use. The function I made up above would know what values are some form of NA and convert all others like ?12.3? to numeric form. BUT it would not act as simply or smoothly as when your data is already in the format everyone else uses.
> 
> 
> 
> So how does R know what something is? Presumably there is some overhead associated with a vector or some table that records the type. A list presumably depends on each internal item to have such a type. So maybe what you want is for each item in a vector to have a type where one type is some for of NA. But as noted, R does often not give a damn about an NA and happily uses it to create more nonsense. The mean of a bunch of numbers that includes one or more copies of things like NA (or NaN or inf) can pollute them all. Generally R is not designed to give a darn. When people complain, they may get mean to add an na.rm=TRUE or remove them some way before asking for a mean or perhaps reset them to something like zero.
> 
> 
> 
> So if you want to leave your variables in place with assorted meanings but a tag saying they are to be treated as NA, much in R might have to change. Your suggested approach though is not yet clear but might mean doing something analogous to using extra bits and hoping nobody will notice.
> 
> 
> 
> So, the solution is both blindingly obvious and even more blindingly stupid. Use complex numbers! All normal content shall be stored as numbers like 5.3+0i and any variant on NA shall be stored as something like 0+3i where 3 means an NA of type 3.
> 
> 
> 
> OK, humor aside, since the social sciences do not tend to even know what complex numbers are, this should provide another dimension to hide lots of meaningless info. Heck, you could convert  message like ?LATE? into some numeric form. Assuming an English centered world (which I do not!) you could store it with L replaced by 12 and A by 01 and so on so the imaginary component might look like 0+12011905i and easily decoded back into LATE when needed. Again, not a serious proposal. The storage probably would be twice the size of a numeric albeit you can extract the real part when needed for normal calculations and the imaginary part when you want to know about NA type or whatever. 
> 
> 
> 
> What R really is missing is quaternions and octonions which are the only two other variations on complex numbers that are possible and are sort of complex numbers on steroids with either three or seven distinct square roots of minus-one  so they allow storage along additional axes in other dimensions.
> 
> 
> 
> Yes, I am sure someone wrote a package for that! LOL!
> 
> 
> 
> Ah, here is one: https://cran.r-project.org/web/packages/onion/onion.pdf
> 
> 
> 
> I will end by saying my experience is that enticing people to do something new is just a start. After they start, you often get lots of complaints and requests for help and even requests to help them move back! Unless you make some popular package everyone runs to, NOBODY else will be able to help them on some things. The reality is that some of the more common tasks these people do are sometimes already optimized for them and often do not make them know more. I have had to use these systems and for some common tasks they are easy. Dialog boxes can pop up and let you checks off various options and off you go. No need to learn lots of programming details like the names of various functions that do a Tukey test and what arguments they need and what errors might have to be handled and so on. I know SPSS often produces LOTS of output including many things you do not wat and then lets you remove parts you don?t need or even know what they mean. Sure, R can have similar functionality but often you are expected to sort of stitch various parts together as well as ADD your own bits. I love that and value being able to be creative. In my experience, most normal people just want to get the job done and be fairly certain others accept the results ad then do other activities they are better suited for, or at least think they are.
> 
> 
> 
> There are intermediates I have used where I let them do various kinds of processing on SPSS and save the result in some format I can read into R for additional processing. The latter may not be stuff that requires keeping track of multiple NA equivalents. Of course if you want to save the results and move them back, that is  a challenge. Hybrid approaches may tempt them to try something and maybe later do more and more and move over.
> 
> 
> 
> From: Adrian Du?a <dusa.adrian at unibuc.ro> 
> Sent: Tuesday, May 25, 2021 2:17 AM
> To: Avi Gross <avigross at verizon.net>
> Cc: r-devel <r-devel at r-project.org>
> Subject: Re: [Rd] [External] Re: 1954 from NA
> 
> 
> 
> Dear Avi,
> 
> 
> 
> Thank you so much for the extended messages, I read them carefully.
> 
> While partially offering a solution (I've already been there), it creates additional work for the user, and some of that is unnecessary.
> 
> 
> 
> What I am trying to achieve is best described in this draft vignette:
> 
> 
> 
> devtools::install_github("dusadrian/mixed")
> 
> vignette("mixed")
> 
> 
> 
> Once a value is declared to be missing, the user should not do anything else about it. Despite being present, the value should automatically be treated as missing by the software. That is the way it's done in all major statistical packages like SAS, Stata and even SPSS.
> 
> 
> 
> My end goal is to make R attractive for my faculty peers (and beyond), almost all of whom are massively using SPSS and sometimes Stata. But in order to convince them to (finally) make the switch, I need to provide similar functionality, not additional work.
> 
> 
> 
> Re. your first part of the message, I am definitely not trying to change the R internals. The NA will still be NA, exactly as currently defined.
> 
> My initial proposal was based on the observation that the 1954 payload was stored as an unsigned int (thus occupying 32 bits) when it is obvious it doesn't need more than 16. That was the only proposed modification, and everything else stays the same.
> 
> 
> 
> I now learned, thanks to all contributors in this list, that building something around that payload is risky because we do not know exactly what the compilers will do. One possible solution that I can think of, while (still) maintaining the current functionality around the NA, is to use a different high word for the NA that would not trigger compilation issues. But I have absolutely no idea what that implies for the other inner workings of R.
> 
> 
> 
> I very much trust the R core will eventually find a robust solution, they've solved much more complicated problems than this. I just hope the current thread will push the idea of tagged NAs on the table, for when they will discuss this.
> 
> 
> 
> Once that will be solved, and despite the current advice discouraging this route, I believe tagging NAs is a valuable idea that should not be discarded.
> 
> After all, the NA is nothing but a tagged NaN.
> 
> 
> 
> All the best,
> 
> Adrian
> 
> 
> 
> 
> 
> On Tue, May 25, 2021 at 7:05 AM Avi Gross via R-devel <r-devel at r-project.org <mailto:r-devel at r-project.org> > wrote:
> 
> I was thinking about how one does things in a language that is properly object-oriented versus R that makes various half-assed attempts at being such.
> 
> Clearly in some such languages you can make an object that is a wrapper that allows you to save an item that is the main payload as well as anything else you want. You might need a way to convince everything else to allow you to make things like lists and vectors and other collections of the objects and perhaps automatically unbox them for many purposes. As an example in a language like Python, you might provide methods so that adding A and B actually gets the value out of A and/or B and adds them properly.  But there may be too many edge cases to handle and some software may not pay attention to what you want including some libraries written in other languages.
> 
> I mention Python for the odd reason that it is now possible to combine Python and R in the same program and sort of switch back and forth between data representations. This may provide some openings for preserving and accessing metadata when needed.
> 
> Realistically, if R was being designed from scratch TODAY, many things might be done differently. But I recall it being developed at Bell Labs for purposes where it was sort of revolutionary at the time (back when it was S) and designed to do things in a vectorized way and probably primarily for the kinds of scientific and mathematical operations where a single NA (of several types depending on the data) was enough when augmented by a few things like a Nan and Inf and -Inf. I doubt they seriously saw a need for an unlimited number of NA that were all the same AND also all different that they felt had to be built-in. As noted, had they had a reason to make it fully object-oriented too and made the base types such as integer into full-fledged objects with room for additional metadata, then things may be different. I note I have seen languages which have both a data type called integer as lower case and Integer as upper case. One of them is regularly boxed and unboxed automagically when used in a context that needs the other. As far as efficiency goes, this invisibly adds many steps. So do languages that sometimes take a variable that is a pointer and invisibly reference it to provide the underlying field rather than make you do extra typing and so on.
> 
> So is there any reason only an NA should have such meta-data? Why not have reasons associated with Inf stating it was an Inf because you asked for one or the result of a calculation such as dividing by Zero (albeit maybe that might be a NaN) and so on. Maybe I could annotate integers with whether they are prime or even  versus odd  or a factor of 144 or anything else I can imagine. But at some point, the overhead from allowing all this can become substantial. I was amused at how python allows a function to be annotated including by itself since it is an object. So it can store such metadata perhaps in an attached dictionary so a complex costly calculation can have the results cached and when you ask for the same thing in the same session, it checks if it has done it and just returns the result in linear time. But after a while, how many cached results can there be?
> 
> -----Original Message-----
> From: R-devel <r-devel-bounces at r-project.org <mailto:r-devel-bounces at r-project.org> > On Behalf Of luke-tierney at uiowa.edu <mailto:luke-tierney at uiowa.edu> 
> Sent: Monday, May 24, 2021 9:15 AM
> To: Adrian Du?a <dusa.adrian at unibuc.ro <mailto:dusa.adrian at unibuc.ro> >
> Cc: Greg Minshall <minshall at umich.edu <mailto:minshall at umich.edu> >; r-devel <r-devel at r-project.org <mailto:r-devel at r-project.org> >
> Subject: Re: [Rd] [External] Re: 1954 from NA
> 
>> On Mon, 24 May 2021, Adrian Du?a wrote:
>> 
>>> On Mon, May 24, 2021 at 2:11 PM Greg Minshall <minshall at umich.edu <mailto:minshall at umich.edu> > wrote:
>>> 
>>> [...]
>>> if you have 500 columns of possibly-NA'd variables, you could have 
>>> one column of 500 "bits", where each bit has one of N values, N being 
>>> the number of explanations the corresponding column has for why the 
>>> NA exists.
>>> 
> 
> PLEASE DO NOT DO THIS!
> 
> It will not work reliably, as has been explained to you ad nauseam in this thread.
> 
> If you distribute code that does this it will only lead to bug reports on R that will waste R-core time.
> 
> As Alex explained, you can use attributes for this. If you need operations to preserve attributes across subsetting you can define subsetting methods that do that.
> 
> If you are dead set on doing something in C you can try to develop an ALTREP class that provides augmented missing value information.
> 
> Best,
> 
> luke
> 
> 
> 
>> 
>> The mere thought of implementing something like that gives me shivers. 
>> Not to mention such a solution should also be robust when subsetting, 
>> splitting, column and row binding, etc. and everything can be lost if 
>> the user deletes that particular column without realising its importance.
>> 
>> Social science datasets are much more alive and complex than one might 
>> first think: there are multi-wave studies with tens of countries, and 
>> aggregating such data is already a complex process to add even more 
>> complexity on top of that.
>> 
>> As undocumented as they may be, or even subject to change, I think the 
>> R internals are much more reliable that this.
>> 
>> Best wishes,
>> Adrian
>> 
>> 
> 
> --
> Luke Tierney
> Ralph E. Wareham Professor of Mathematical Sciences
> University of Iowa                  Phone:             319-335-3386
> Department of Statistics and        Fax:               319-335-3017
>    Actuarial Science
> 241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu <mailto:luke-tierney at uiowa.edu> 
> Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu
> ______________________________________________
> R-devel at r-project.org <mailto:R-devel at r-project.org>  mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 
> ______________________________________________
> R-devel at r-project.org <mailto:R-devel at r-project.org>  mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 
> 
> 
> 
> 
> 
> -- 
> 
> Adrian Dusa
> University of Bucharest
> Romanian Social Data Archive
> Soseaua Panduri nr. 90-92
> 050663 Bucharest sector 5
> Romania
> 
> https://adriandusa.eu
> 
> 
>    [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From @v|gro@@ @end|ng |rom ver|zon@net  Wed May 26 04:03:42 2021
From: @v|gro@@ @end|ng |rom ver|zon@net (Avi Gross)
Date: Tue, 25 May 2021 22:03:42 -0400
Subject: [Rd] [External] Re: 1954 from NA
In-Reply-To: <CF512D1E-69AD-4CEE-B85F-78F13BD06B10@warnes.net>
References: <01b201d75198$70f8a060$52e9e120$@verizon.net>
 <CF512D1E-69AD-4CEE-B85F-78F13BD06B10@warnes.net>
Message-ID: <007301d751d3$5a479930$0ed6cb90$@verizon.net>

Greg,

I am curious what they suggest you use multiple NaN values for. Or, is it simply like how text messages on your phone started because standard size packets were bigger than what some uses required so they piggy-backed messages on the "empty" space.

If by NaN you include the various flavors of NA such as NA_logical_ and NA_complex_ I have sometimes wondered if they are slightly different bitstreams or all the same but interpreted by programs as being the right kind for their context. Sounds like maybe they are different and there is one for pretty much each basic type except perhaps raw.

But if you add more, in that case, will it be seen as the right NA for the environment it is in? Heck, if R adds yet another basic type (like a quaternion) or a nibble, could they use the same bits you took without asking for your application?

It does sound like some suggest you use a method with existing abilities and tightly control that all functions used to manipulate the data will behave and preserve those attributes. I am not so sure the clients using it will obey. I have seen plenty of people say use some tidyverse functions for various purposes then use something more base-R like complete.cases() or rbind() that may, but also may not, preserve what they want. And once lost, ...

Now, of course, you could write wrapper functions that will take the data, copy the attributes, allow whatever changes, and carefully put them back before returning. This may not be trivial though if you want to do something like delete lots of rows as you might need to first identify what rows will be kept, then adjust the vector of attributes accordingly before returning it. Sorting is another such annoyance. Many things do conversions such as making copies or converting a copy to a factor, that may mess things up. If it has already been done and people have experience, great. If not, good luck.

-----Original Message-----
From: Gregory Warnes <greg at warnes.net> 
Sent: Tuesday, May 25, 2021 9:13 PM
To: Avi Gross <avigross at verizon.net>
Cc: r-devel <R-devel at r-project.org>
Subject: Re: [Rd] [External] Re: 1954 from NA

As a side note, for floating point values, the IEEE 754 standard provides for a large set of NaN values, making it possible to have multiple types of NAs for floating point values...


From du@@@@dr|@n @end|ng |rom gm@||@com  Wed May 26 16:22:36 2021
From: du@@@@dr|@n @end|ng |rom gm@||@com (=?UTF-8?B?QWRyaWFuIER1yJlh?=)
Date: Wed, 26 May 2021 17:22:36 +0300
Subject: [Rd] [External] Re: 1954 from NA
In-Reply-To: <7303ff84-bdf1-cc76-e9e2-e625e80a24ca@gmail.com>
References: <CAJ=0CtCfRMJM6wq1ggkSv2BNfX+CGnW2N-SZAN8poOG3ev=nUA@mail.gmail.com>
 <3443246.1621854690@apollo2.minshall.org>
 <CAJ=0CtDOuY+tVf+YxO0gdGY9XgaTv_Q4kMP2p_Z8CxrnHjT4-w@mail.gmail.com>
 <alpine.DEB.2.21.2105240805580.3254@luke-Latitude-7480>
 <016e01d7511b$29c3bea0$7d4b3be0$@verizon.net>
 <CAJ=0CtDC4jEfa-9qqDhixqSbvh1528wD5QpObQuwqxRs1EfiEw@mail.gmail.com>
 <01b201d75198$70f8a060$52e9e120$@verizon.net>
 <CAJ=0CtBofw9JoCUAxDGueYHZs5NghJD6A7ND97nfpvm_0n6fMg@mail.gmail.com>
 <7303ff84-bdf1-cc76-e9e2-e625e80a24ca@gmail.com>
Message-ID: <CAJ=0CtDbZQHC1Z4gLHpkVmh1vfuShYHGqgZg4dvUorhjh79rdA@mail.gmail.com>

Dear Duncan,

On Wed, May 26, 2021 at 2:27 AM Duncan Murdoch <murdoch.duncan at gmail.com>
wrote:

> You've already been told how to solve this:  just add attributes to the
> objects. Use the standard NA to indicate that there is some kind of
> missingness, and the attribute to describe exactly what it is.  Stick a
> class on those objects and define methods so that subsetting and
> arithmetic preserves the extra info you've added. If you do some
> operation that turns those NAs into NaNs, big deal:  the attribute will
> still be there, and is.na(NaN) still returns TRUE.
>

I've already tried the attributes way, it is not so easy.
In the best case scenario, it unnecessarily triples the size of the data,
but perhaps this is the only way forward.



> Base R doesn't need anything else.
>
> You complained that users shouldn't need to know about attributes, and
> they won't:  you, as the author of the package that does this, will
> handle all those details.  Working in your subject area you know all the
> different kinds of NAs that people care about, and how they code them in
> input data, so you can make it all totally transparent.  If you do it
> well, someone in some other subject area with a completely different set
> of kinds of missingness will be able to adapt your code to their use.
>

But that is the whole point: the package author does not define possible
NAs (the possibilities are infinite), users do that.
The package should only provide a simple method to achieve that.


I imagine this has all been done in one of the thousands of packages on
> CRAN, but if it hasn't been done well enough for you, do it better.
>

If it were, I would have found it by now...

Best wishes,
Adrian

	[[alternative HTML version deleted]]


From du@@@@dr|@n @end|ng |rom gm@||@com  Wed May 26 16:29:31 2021
From: du@@@@dr|@n @end|ng |rom gm@||@com (=?UTF-8?B?QWRyaWFuIER1yJlh?=)
Date: Wed, 26 May 2021 17:29:31 +0300
Subject: [Rd] [External] Re: 1954 from NA
In-Reply-To: <CF512D1E-69AD-4CEE-B85F-78F13BD06B10@warnes.net>
References: <01b201d75198$70f8a060$52e9e120$@verizon.net>
 <CF512D1E-69AD-4CEE-B85F-78F13BD06B10@warnes.net>
Message-ID: <CAJ=0CtB+-1kNjVbg0rF1w1WT6efHp4+GkmLPjDHXgY-dECe6Lg@mail.gmail.com>

On Wed, May 26, 2021 at 4:13 AM Gregory Warnes <greg at warnes.net> wrote:

> As a side note, for floating point values, the IEEE 754 standard provides
> for a large set of NaN values, making it possible to have multiple types of
> NAs for floating point values...
>

That is interesting, but how does one use different NaN values from within
R?
Tagging such values wa already signaled as a dead end, is there another way?

	[[alternative HTML version deleted]]


From murdoch@dunc@n @end|ng |rom gm@||@com  Wed May 26 17:43:16 2021
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Wed, 26 May 2021 11:43:16 -0400
Subject: [Rd] [External] Re: 1954 from NA
In-Reply-To: <CAJ=0CtDbZQHC1Z4gLHpkVmh1vfuShYHGqgZg4dvUorhjh79rdA@mail.gmail.com>
References: <CAJ=0CtCfRMJM6wq1ggkSv2BNfX+CGnW2N-SZAN8poOG3ev=nUA@mail.gmail.com>
 <3443246.1621854690@apollo2.minshall.org>
 <CAJ=0CtDOuY+tVf+YxO0gdGY9XgaTv_Q4kMP2p_Z8CxrnHjT4-w@mail.gmail.com>
 <alpine.DEB.2.21.2105240805580.3254@luke-Latitude-7480>
 <016e01d7511b$29c3bea0$7d4b3be0$@verizon.net>
 <CAJ=0CtDC4jEfa-9qqDhixqSbvh1528wD5QpObQuwqxRs1EfiEw@mail.gmail.com>
 <01b201d75198$70f8a060$52e9e120$@verizon.net>
 <CAJ=0CtBofw9JoCUAxDGueYHZs5NghJD6A7ND97nfpvm_0n6fMg@mail.gmail.com>
 <7303ff84-bdf1-cc76-e9e2-e625e80a24ca@gmail.com>
 <CAJ=0CtDbZQHC1Z4gLHpkVmh1vfuShYHGqgZg4dvUorhjh79rdA@mail.gmail.com>
Message-ID: <76bbec7f-4802-8903-9b3c-c215e4863ab4@gmail.com>

On 26/05/2021 10:22 a.m., Adrian Du?a wrote:
> Dear?Duncan,
> 
> On Wed, May 26, 2021 at 2:27 AM Duncan Murdoch <murdoch.duncan at gmail.com 
> <mailto:murdoch.duncan at gmail.com>> wrote:
> 
>     You've already been told how to solve this:? just add attributes to the
>     objects. Use the standard NA to indicate that there is some kind of
>     missingness, and the attribute to describe exactly what it is.? Stick a
>     class on those objects and define methods so that subsetting and
>     arithmetic preserves the extra info you've added. If you do some
>     operation that turns those NAs into NaNs, big deal:? the attribute will
>     still be there, and is.na <http://is.na>(NaN) still returns TRUE.
> 
> 
> I've already tried the attributes way, it is not so easy.

If you have specific operations that are needed but that you can't get 
to work, post the issue here.

> In the best case scenario, it unnecessarily triples the size of the 
> data, but perhaps this is the only way forward.

I don't see how it could triple the size.  Surely an integer has enough 
values to cover all possible kinds of missingness.  So on integer or 
factor data you'd double the size, on real or character data you'd 
increase it by 50%.  (This is assuming you're on a 64 bit platform with 
32 bit integers and 64 bit reals and pointers.)

Here's a tiny implementation to show what I'm talking about:

asMultiMissing <- function(x) {
   if (isMultiMissing(x))
     return(x)
   missingKind <- ifelse(is.na(x), 1, 0)
   structure(x,
             missingKind = missingKind,
             class = c("MultiMissing", class(x)))
}

isMultiMissing <- function(x)
   inherits(x, "MultiMissing")

missingKind <- function(x) {
   if (isMultiMissing(x))
     attr(x, "missingKind")
   else
     ifelse(is.na(x), 1, 0)
}

`missingKind<-` <- function(x, value) {
   class(x) <- setdiff(class(x), "MultiMissing")
   x[value != 0] <- NA
   x <- asMultiMissing(x)
   attr(x, "missingKind") <- value
   x
}

`[.MultiMissing` <- function(x, i, ...) {
   missings <- missingKind(x)
   x <- NextMethod()
   missings <- missings[i]
   missingKind(x) <- missings
   x
}

print.MultiMissing <- function(x, ...) {
   vals <- as.character(x)
   if (!is.character(x) || inherits(x, "noquote"))
     print(noquote(vals))
   else
     print(vals)
}

`[<-.MultiMissing` <- function(x, i, value, ...) {
   missings <- missingKind(x)
   class(x) <- setdiff(class(x), "MultiMissing")
   x[i] <- value
   missings[i] <- missingKind(value)
   missingKind(x) <- missings
   x
}

as.character.MultiMissing <- function(x, ...) {
   missings <- missingKind(x)
   result <- NextMethod()
   ifelse(missings != 0,
          paste0("NA.", missings), result)

}

This is incomplete.  It doesn't do printing very well, and it doesn't 
handle the case of assigning a MultiMissing value to a regular vector at 
all.  (I think you'd need an S4 implementation if you want to support 
that.)  But it does the basics:

 > x <- 1:10
 > missingKind(x)[4] <- 23
 > x
  [1] 1     2     3     NA.23 5     6     7     8     9
[10] 10
 > is.na(x)
  [1] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE
[10] FALSE
 > missingKind(x)
  [1]  0  0  0 23  0  0  0  0  0  0
 >

Duncan Murdoch

> 
>     Base R doesn't need anything else.
> 
>     You complained that users shouldn't need to know about attributes, and
>     they won't:? you, as the author of the package that does this, will
>     handle all those details.? Working in your subject area you know all
>     the
>     different kinds of NAs that people care about, and how they code
>     them in
>     input data, so you can make it all totally transparent.? If you do it
>     well, someone in some other subject area with a completely different
>     set
>     of kinds of missingness will be able to adapt your code to their use.
> 
> 
> But that is the whole point: the package author does not define possible 
> NAs (the possibilities are infinite), users do that.
> The package should only provide a simple method to achieve that.
> 
> 
>     I imagine this has all been done in one of the thousands of packages on
>     CRAN, but if it hasn't been done well enough for you, do it better.
> 
> 
> If it were, I would have found it by now...
> 
> Best wishes,
> Adrian


From murdoch@dunc@n @end|ng |rom gm@||@com  Wed May 26 18:05:45 2021
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Wed, 26 May 2021 12:05:45 -0400
Subject: [Rd] [External] Re: 1954 from NA
In-Reply-To: <76bbec7f-4802-8903-9b3c-c215e4863ab4@gmail.com>
References: <CAJ=0CtCfRMJM6wq1ggkSv2BNfX+CGnW2N-SZAN8poOG3ev=nUA@mail.gmail.com>
 <3443246.1621854690@apollo2.minshall.org>
 <CAJ=0CtDOuY+tVf+YxO0gdGY9XgaTv_Q4kMP2p_Z8CxrnHjT4-w@mail.gmail.com>
 <alpine.DEB.2.21.2105240805580.3254@luke-Latitude-7480>
 <016e01d7511b$29c3bea0$7d4b3be0$@verizon.net>
 <CAJ=0CtDC4jEfa-9qqDhixqSbvh1528wD5QpObQuwqxRs1EfiEw@mail.gmail.com>
 <01b201d75198$70f8a060$52e9e120$@verizon.net>
 <CAJ=0CtBofw9JoCUAxDGueYHZs5NghJD6A7ND97nfpvm_0n6fMg@mail.gmail.com>
 <7303ff84-bdf1-cc76-e9e2-e625e80a24ca@gmail.com>
 <CAJ=0CtDbZQHC1Z4gLHpkVmh1vfuShYHGqgZg4dvUorhjh79rdA@mail.gmail.com>
 <76bbec7f-4802-8903-9b3c-c215e4863ab4@gmail.com>
Message-ID: <1328c364-264a-ddc2-1e56-ee2b3fc6b732@gmail.com>

After 5 minutes more thought:

- code non-missing as missingKind = NA, not 0, so that missingKind could 
be a character vector, or missingKind = 0 could be supported.

- print methods should return the main argument, so mine should be

print.MultiMissing <- function(x, ...) {
   vals <- as.character(x)
   if (!is.character(x) || inherits(x, "noquote"))
     print(noquote(vals))
   else
     print(vals)
   invisible(x)
}

This still needs a lot of improvement to be a good print method, but 
I'll leave that to you.

Duncan Murdoch

On 26/05/2021 11:43 a.m., Duncan Murdoch wrote:
> On 26/05/2021 10:22 a.m., Adrian Du?a wrote:
>> Dear?Duncan,
>>
>> On Wed, May 26, 2021 at 2:27 AM Duncan Murdoch <murdoch.duncan at gmail.com
>> <mailto:murdoch.duncan at gmail.com>> wrote:
>>
>>      You've already been told how to solve this:? just add attributes to the
>>      objects. Use the standard NA to indicate that there is some kind of
>>      missingness, and the attribute to describe exactly what it is.? Stick a
>>      class on those objects and define methods so that subsetting and
>>      arithmetic preserves the extra info you've added. If you do some
>>      operation that turns those NAs into NaNs, big deal:? the attribute will
>>      still be there, and is.na <http://is.na>(NaN) still returns TRUE.
>>
>>
>> I've already tried the attributes way, it is not so easy.
> 
> If you have specific operations that are needed but that you can't get
> to work, post the issue here.
> 
>> In the best case scenario, it unnecessarily triples the size of the
>> data, but perhaps this is the only way forward.
> 
> I don't see how it could triple the size.  Surely an integer has enough
> values to cover all possible kinds of missingness.  So on integer or
> factor data you'd double the size, on real or character data you'd
> increase it by 50%.  (This is assuming you're on a 64 bit platform with
> 32 bit integers and 64 bit reals and pointers.)
> 
> Here's a tiny implementation to show what I'm talking about:
> 
> asMultiMissing <- function(x) {
>     if (isMultiMissing(x))
>       return(x)
>     missingKind <- ifelse(is.na(x), 1, 0)
>     structure(x,
>               missingKind = missingKind,
>               class = c("MultiMissing", class(x)))
> }
> 
> isMultiMissing <- function(x)
>     inherits(x, "MultiMissing")
> 
> missingKind <- function(x) {
>     if (isMultiMissing(x))
>       attr(x, "missingKind")
>     else
>       ifelse(is.na(x), 1, 0)
> }
> 
> `missingKind<-` <- function(x, value) {
>     class(x) <- setdiff(class(x), "MultiMissing")
>     x[value != 0] <- NA
>     x <- asMultiMissing(x)
>     attr(x, "missingKind") <- value
>     x
> }
> 
> `[.MultiMissing` <- function(x, i, ...) {
>     missings <- missingKind(x)
>     x <- NextMethod()
>     missings <- missings[i]
>     missingKind(x) <- missings
>     x
> }
> 
> print.MultiMissing <- function(x, ...) {
>     vals <- as.character(x)
>     if (!is.character(x) || inherits(x, "noquote"))
>       print(noquote(vals))
>     else
>       print(vals)
> }
> 
> `[<-.MultiMissing` <- function(x, i, value, ...) {
>     missings <- missingKind(x)
>     class(x) <- setdiff(class(x), "MultiMissing")
>     x[i] <- value
>     missings[i] <- missingKind(value)
>     missingKind(x) <- missings
>     x
> }
> 
> as.character.MultiMissing <- function(x, ...) {
>     missings <- missingKind(x)
>     result <- NextMethod()
>     ifelse(missings != 0,
>            paste0("NA.", missings), result)
> 
> }
> 
> This is incomplete.  It doesn't do printing very well, and it doesn't
> handle the case of assigning a MultiMissing value to a regular vector at
> all.  (I think you'd need an S4 implementation if you want to support
> that.)  But it does the basics:
> 
>   > x <- 1:10
>   > missingKind(x)[4] <- 23
>   > x
>    [1] 1     2     3     NA.23 5     6     7     8     9
> [10] 10
>   > is.na(x)
>    [1] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE
> [10] FALSE
>   > missingKind(x)
>    [1]  0  0  0 23  0  0  0  0  0  0
>   >
> 
> Duncan Murdoch
> 
>>
>>      Base R doesn't need anything else.
>>
>>      You complained that users shouldn't need to know about attributes, and
>>      they won't:? you, as the author of the package that does this, will
>>      handle all those details.? Working in your subject area you know all
>>      the
>>      different kinds of NAs that people care about, and how they code
>>      them in
>>      input data, so you can make it all totally transparent.? If you do it
>>      well, someone in some other subject area with a completely different
>>      set
>>      of kinds of missingness will be able to adapt your code to their use.
>>
>>
>> But that is the whole point: the package author does not define possible
>> NAs (the possibilities are infinite), users do that.
>> The package should only provide a simple method to achieve that.
>>
>>
>>      I imagine this has all been done in one of the thousands of packages on
>>      CRAN, but if it hasn't been done well enough for you, do it better.
>>
>>
>> If it were, I would have found it by now...
>>
>> Best wishes,
>> Adrian
>


From pro|jcn@@h @end|ng |rom gm@||@com  Wed May 26 18:26:01 2021
From: pro|jcn@@h @end|ng |rom gm@||@com (J C Nash)
Date: Wed, 26 May 2021 12:26:01 -0400
Subject: [Rd] GSoC project "Improvement to nls()"
Message-ID: <c7a8fbb5-b0a4-0b66-cd17-b275635dd1ae@gmail.com>

This message is to let R developers know that the project in the Subject is now
a Google Summer of Code project.

Our aim in this project is to find simplifications and corrections to the nls()
code, which has become heavily patched. Moreover, it has some deficiencies in that
there is no Marquardt stabilization and it is likely the jacobian (called gradient
in R) computations are less than ideal. On the other hand, it has a lot of features
and capabilities.

A correction I proposed to avoid the "small residual" issue (when models are nearly
perfectly fittable) is now in R-devel. Using a new nls.control parameter one can avoid
failure, but the default value of 0 leaves legacy behaviour. We hope to be able to use
similar approaches so existing nls() example output is unaltered.

It is likely we will only partially meet our goals:
- to document and possibly simplify the existing code
- to correct some minor issues in documentation or function
- to find a graceful way to incorporate a Marquardt stabilization into the
  Gauss-Newton iteration
- to document, evaluate, and possibly improve the jacobian computation

all within the context that any changes impose minimal nuisance for R workers.

Some of these efforts overlap the nlsr, minpack.lm and likely other packages,
and may suggest improvements there also.

There is a gitlab repository established at https://gitlab.com/nashjc/improvenls.
We welcome interest and participation off list except for bugs in the current R
function(s).

John Nash
University of Ottawa


From du@@@@dr|@n @end|ng |rom gm@||@com  Wed May 26 19:08:42 2021
From: du@@@@dr|@n @end|ng |rom gm@||@com (=?UTF-8?B?QWRyaWFuIER1yJlh?=)
Date: Wed, 26 May 2021 20:08:42 +0300
Subject: [Rd] [External] Re: 1954 from NA
In-Reply-To: <76bbec7f-4802-8903-9b3c-c215e4863ab4@gmail.com>
References: <CAJ=0CtCfRMJM6wq1ggkSv2BNfX+CGnW2N-SZAN8poOG3ev=nUA@mail.gmail.com>
 <3443246.1621854690@apollo2.minshall.org>
 <CAJ=0CtDOuY+tVf+YxO0gdGY9XgaTv_Q4kMP2p_Z8CxrnHjT4-w@mail.gmail.com>
 <alpine.DEB.2.21.2105240805580.3254@luke-Latitude-7480>
 <016e01d7511b$29c3bea0$7d4b3be0$@verizon.net>
 <CAJ=0CtDC4jEfa-9qqDhixqSbvh1528wD5QpObQuwqxRs1EfiEw@mail.gmail.com>
 <01b201d75198$70f8a060$52e9e120$@verizon.net>
 <CAJ=0CtBofw9JoCUAxDGueYHZs5NghJD6A7ND97nfpvm_0n6fMg@mail.gmail.com>
 <7303ff84-bdf1-cc76-e9e2-e625e80a24ca@gmail.com>
 <CAJ=0CtDbZQHC1Z4gLHpkVmh1vfuShYHGqgZg4dvUorhjh79rdA@mail.gmail.com>
 <76bbec7f-4802-8903-9b3c-c215e4863ab4@gmail.com>
Message-ID: <CAJ=0CtAXif58tKMF5V=9-DseFiev1COMjAU9FpDYK9iew8TExQ@mail.gmail.com>

On Wed, May 26, 2021 at 6:43 PM Duncan Murdoch <murdoch.duncan at gmail.com>
wrote:

> [...]
> > In the best case scenario, it unnecessarily triples the size of the
> > data, but perhaps this is the only way forward.
>
> I don't see how it could triple the size.  Surely an integer has enough
> values to cover all possible kinds of missingness.  So on integer or
> factor data you'd double the size, on real or character data you'd
> increase it by 50%.  (This is assuming you're on a 64 bit platform with
> 32 bit integers and 64 bit reals and pointers.)


Apologies, that was supposed to be double the size not triple, 99% of the
survey data are integers.
But I suppose that is alright, space doesn't seem to be a problem.

Thank you very much for the examples, they do seem to cover the basics
indeed.
(that is what I meant when I wrote there might be a way without tagging
NAs).

Will take it from there, best wishes,
Adrian

	[[alternative HTML version deleted]]


From du@@@@dr|@n @end|ng |rom gm@||@com  Wed May 26 19:09:50 2021
From: du@@@@dr|@n @end|ng |rom gm@||@com (=?UTF-8?B?QWRyaWFuIER1yJlh?=)
Date: Wed, 26 May 2021 20:09:50 +0300
Subject: [Rd] [External] Re: 1954 from NA
In-Reply-To: <1328c364-264a-ddc2-1e56-ee2b3fc6b732@gmail.com>
References: <CAJ=0CtCfRMJM6wq1ggkSv2BNfX+CGnW2N-SZAN8poOG3ev=nUA@mail.gmail.com>
 <3443246.1621854690@apollo2.minshall.org>
 <CAJ=0CtDOuY+tVf+YxO0gdGY9XgaTv_Q4kMP2p_Z8CxrnHjT4-w@mail.gmail.com>
 <alpine.DEB.2.21.2105240805580.3254@luke-Latitude-7480>
 <016e01d7511b$29c3bea0$7d4b3be0$@verizon.net>
 <CAJ=0CtDC4jEfa-9qqDhixqSbvh1528wD5QpObQuwqxRs1EfiEw@mail.gmail.com>
 <01b201d75198$70f8a060$52e9e120$@verizon.net>
 <CAJ=0CtBofw9JoCUAxDGueYHZs5NghJD6A7ND97nfpvm_0n6fMg@mail.gmail.com>
 <7303ff84-bdf1-cc76-e9e2-e625e80a24ca@gmail.com>
 <CAJ=0CtDbZQHC1Z4gLHpkVmh1vfuShYHGqgZg4dvUorhjh79rdA@mail.gmail.com>
 <76bbec7f-4802-8903-9b3c-c215e4863ab4@gmail.com>
 <1328c364-264a-ddc2-1e56-ee2b3fc6b732@gmail.com>
Message-ID: <CAJ=0CtAhJOwV-UjMBO-KTH2CeqGQvBLiEkRksYhYHVcuVv0CsQ@mail.gmail.com>

Yes, that is even better.
Best,
Adrian

On Wed, May 26, 2021 at 7:05 PM Duncan Murdoch <murdoch.duncan at gmail.com>
wrote:

> After 5 minutes more thought:
>
> - code non-missing as missingKind = NA, not 0, so that missingKind could
> be a character vector, or missingKind = 0 could be supported.
>
> - print methods should return the main argument, so mine should be
>
> print.MultiMissing <- function(x, ...) {
>    vals <- as.character(x)
>    if (!is.character(x) || inherits(x, "noquote"))
>      print(noquote(vals))
>    else
>      print(vals)
>    invisible(x)
> }
>
> This still needs a lot of improvement to be a good print method, but
> I'll leave that to you.
>
> Duncan Murdoch
>
> On 26/05/2021 11:43 a.m., Duncan Murdoch wrote:
> > On 26/05/2021 10:22 a.m., Adrian Du?a wrote:
> >> Dear Duncan,
> >>
> >> On Wed, May 26, 2021 at 2:27 AM Duncan Murdoch <
> murdoch.duncan at gmail.com
> >> <mailto:murdoch.duncan at gmail.com>> wrote:
> >>
> >>      You've already been told how to solve this:  just add attributes
> to the
> >>      objects. Use the standard NA to indicate that there is some kind of
> >>      missingness, and the attribute to describe exactly what it is.
> Stick a
> >>      class on those objects and define methods so that subsetting and
> >>      arithmetic preserves the extra info you've added. If you do some
> >>      operation that turns those NAs into NaNs, big deal:  the attribute
> will
> >>      still be there, and is.na <http://is.na>(NaN) still returns TRUE.
> >>
> >>
> >> I've already tried the attributes way, it is not so easy.
> >
> > If you have specific operations that are needed but that you can't get
> > to work, post the issue here.
> >
> >> In the best case scenario, it unnecessarily triples the size of the
> >> data, but perhaps this is the only way forward.
> >
> > I don't see how it could triple the size.  Surely an integer has enough
> > values to cover all possible kinds of missingness.  So on integer or
> > factor data you'd double the size, on real or character data you'd
> > increase it by 50%.  (This is assuming you're on a 64 bit platform with
> > 32 bit integers and 64 bit reals and pointers.)
> >
> > Here's a tiny implementation to show what I'm talking about:
> >
> > asMultiMissing <- function(x) {
> >     if (isMultiMissing(x))
> >       return(x)
> >     missingKind <- ifelse(is.na(x), 1, 0)
> >     structure(x,
> >               missingKind = missingKind,
> >               class = c("MultiMissing", class(x)))
> > }
> >
> > isMultiMissing <- function(x)
> >     inherits(x, "MultiMissing")
> >
> > missingKind <- function(x) {
> >     if (isMultiMissing(x))
> >       attr(x, "missingKind")
> >     else
> >       ifelse(is.na(x), 1, 0)
> > }
> >
> > `missingKind<-` <- function(x, value) {
> >     class(x) <- setdiff(class(x), "MultiMissing")
> >     x[value != 0] <- NA
> >     x <- asMultiMissing(x)
> >     attr(x, "missingKind") <- value
> >     x
> > }
> >
> > `[.MultiMissing` <- function(x, i, ...) {
> >     missings <- missingKind(x)
> >     x <- NextMethod()
> >     missings <- missings[i]
> >     missingKind(x) <- missings
> >     x
> > }
> >
> > print.MultiMissing <- function(x, ...) {
> >     vals <- as.character(x)
> >     if (!is.character(x) || inherits(x, "noquote"))
> >       print(noquote(vals))
> >     else
> >       print(vals)
> > }
> >
> > `[<-.MultiMissing` <- function(x, i, value, ...) {
> >     missings <- missingKind(x)
> >     class(x) <- setdiff(class(x), "MultiMissing")
> >     x[i] <- value
> >     missings[i] <- missingKind(value)
> >     missingKind(x) <- missings
> >     x
> > }
> >
> > as.character.MultiMissing <- function(x, ...) {
> >     missings <- missingKind(x)
> >     result <- NextMethod()
> >     ifelse(missings != 0,
> >            paste0("NA.", missings), result)
> >
> > }
> >
> > This is incomplete.  It doesn't do printing very well, and it doesn't
> > handle the case of assigning a MultiMissing value to a regular vector at
> > all.  (I think you'd need an S4 implementation if you want to support
> > that.)  But it does the basics:
> >
> >   > x <- 1:10
> >   > missingKind(x)[4] <- 23
> >   > x
> >    [1] 1     2     3     NA.23 5     6     7     8     9
> > [10] 10
> >   > is.na(x)
> >    [1] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE
> > [10] FALSE
> >   > missingKind(x)
> >    [1]  0  0  0 23  0  0  0  0  0  0
> >   >
> >
> > Duncan Murdoch
> >
> >>
> >>      Base R doesn't need anything else.
> >>
> >>      You complained that users shouldn't need to know about attributes,
> and
> >>      they won't:  you, as the author of the package that does this, will
> >>      handle all those details.  Working in your subject area you know
> all
> >>      the
> >>      different kinds of NAs that people care about, and how they code
> >>      them in
> >>      input data, so you can make it all totally transparent.  If you do
> it
> >>      well, someone in some other subject area with a completely
> different
> >>      set
> >>      of kinds of missingness will be able to adapt your code to their
> use.
> >>
> >>
> >> But that is the whole point: the package author does not define possible
> >> NAs (the possibilities are infinite), users do that.
> >> The package should only provide a simple method to achieve that.
> >>
> >>
> >>      I imagine this has all been done in one of the thousands of
> packages on
> >>      CRAN, but if it hasn't been done well enough for you, do it better.
> >>
> >>
> >> If it were, I would have found it by now...
> >>
> >> Best wishes,
> >> Adrian
> >
>
>

	[[alternative HTML version deleted]]


From net|kj@ @end|ng |rom gm@||@com  Thu May 27 20:34:52 2021
From: net|kj@ @end|ng |rom gm@||@com (=?UTF-8?Q?Jan_Net=C3=ADk?=)
Date: Thu, 27 May 2021 20:34:52 +0200
Subject: [Rd] =?utf-8?q?Feature_request_=E2=80=93_math_in_HTML_help?=
Message-ID: <CA+6hu7fs_2UUfiMbE=8ywx17Hk+RC84o-MfXA3Xf7iziVpz0uA@mail.gmail.com>

Hi all,

long I have been thinking about proper rendering of math in the HTML form
of R documentation. As you know, you can write \eqn{} in your .Rd files and
this is nicely rendered into the PDF Reference manual of the package with
the aid of TeX. However, that is not the case in the aforementioned HTML
version that is used the most in my experience (using RStudio or help
function in your console). I think R is the best language for statisticians
and other data-driven fields, where formal definitions of key concepts are
necessary and widely used in the documentation, unfortunately quite
unusable for more complicated equations.

Recently I have stumbled upon an interesting approach to this issue, see
https://cran.r-project.org/package=mathjaxr, but it seems to me as some
weird kind of monkey patching. All packages should be able to benefit from
proper math rendering without any dependencies, in my opinion. I think it
should not be much of a problem utilizing mathjax or other similar library
to enable that. Note we already know what supposed to be math in .Rd (and
we parse and process it in a special way in the PDF routine), the thing is
to render it, not typeset in italics, as it is the case nowadays.

I would be happy to hear any opinion of yours!

Best,

Jan Netik

	[[alternative HTML version deleted]]


From murdoch@dunc@n @end|ng |rom gm@||@com  Thu May 27 22:15:29 2021
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Thu, 27 May 2021 16:15:29 -0400
Subject: [Rd] =?utf-8?q?Feature_request_=E2=80=93_math_in_HTML_help?=
In-Reply-To: <CA+6hu7fs_2UUfiMbE=8ywx17Hk+RC84o-MfXA3Xf7iziVpz0uA@mail.gmail.com>
References: <CA+6hu7fs_2UUfiMbE=8ywx17Hk+RC84o-MfXA3Xf7iziVpz0uA@mail.gmail.com>
Message-ID: <7e79c9f0-d9b8-a8c5-bf2c-1a7748443369@gmail.com>

On 27/05/2021 2:34 p.m., Jan Net?k wrote:
> Hi all,
> 
> long I have been thinking about proper rendering of math in the HTML form
> of R documentation. As you know, you can write \eqn{} in your .Rd files and
> this is nicely rendered into the PDF Reference manual of the package with
> the aid of TeX. However, that is not the case in the aforementioned HTML
> version that is used the most in my experience (using RStudio or help
> function in your console). I think R is the best language for statisticians
> and other data-driven fields, where formal definitions of key concepts are
> necessary and widely used in the documentation, unfortunately quite
> unusable for more complicated equations.
> 
> Recently I have stumbled upon an interesting approach to this issue, see
> https://cran.r-project.org/package=mathjaxr, but it seems to me as some
> weird kind of monkey patching. All packages should be able to benefit from
> proper math rendering without any dependencies, in my opinion. I think it
> should not be much of a problem utilizing mathjax or other similar library
> to enable that. Note we already know what supposed to be math in .Rd (and
> we parse and process it in a special way in the PDF routine), the thing is
> to render it, not typeset in italics, as it is the case nowadays.
> 
> I would be happy to hear any opinion of yours!

You are being very rude.

The problem has been solved by mathjaxr.  If you don't like their 
approach, don't ask someone else (i.e. R Core) to come up with a 
different solution, do it yourself.

I hope my opinion makes you happy!

Duncan Murdoch


From net|kj@ @end|ng |rom gm@||@com  Thu May 27 22:54:10 2021
From: net|kj@ @end|ng |rom gm@||@com (=?UTF-8?Q?Jan_Net=C3=ADk?=)
Date: Thu, 27 May 2021 22:54:10 +0200
Subject: [Rd] =?utf-8?q?Feature_request_=E2=80=93_math_in_HTML_help?=
In-Reply-To: <7e79c9f0-d9b8-a8c5-bf2c-1a7748443369@gmail.com>
References: <CA+6hu7fs_2UUfiMbE=8ywx17Hk+RC84o-MfXA3Xf7iziVpz0uA@mail.gmail.com>
 <7e79c9f0-d9b8-a8c5-bf2c-1a7748443369@gmail.com>
Message-ID: <CA+6hu7dm+D=ba0=RWdHv5T4eCoHUS8G_chvcG=beQAw4SENFqg@mail.gmail.com>

I am so sorry, my post sounded quite the opposite of what I intended! I
used the term "monkey patch" as a technical one (see
https://en.m.wikipedia.org/wiki/Monkey_patch) and I said "weird" meaning
mathjaxr is something fulfilling the concept, but only figuratively.

Actually I gave mathjaxr as a good (the best I know, in fact) example of an
approach how to solve the issue. However, no package can achieve nicely
formatted/rendered math in every single R documentation files. I thought
this could be solved globally, directly in R, as it is the case of PDF
Reference manual. I propose (in line with mathjaxr authors) that mathjax is
pretty good candidate for it.

Once more I am sorry for the ambiguity in my language.

Best,
Jan Netik

Dne ?t 27. 5. 2021 22:15 u?ivatel Duncan Murdoch <murdoch.duncan at gmail.com>
napsal:

> On 27/05/2021 2:34 p.m., Jan Net?k wrote:
> > Hi all,
> >
> > long I have been thinking about proper rendering of math in the HTML form
> > of R documentation. As you know, you can write \eqn{} in your .Rd files
> and
> > this is nicely rendered into the PDF Reference manual of the package with
> > the aid of TeX. However, that is not the case in the aforementioned HTML
> > version that is used the most in my experience (using RStudio or help
> > function in your console). I think R is the best language for
> statisticians
> > and other data-driven fields, where formal definitions of key concepts
> are
> > necessary and widely used in the documentation, unfortunately quite
> > unusable for more complicated equations.
> >
> > Recently I have stumbled upon an interesting approach to this issue, see
> > https://cran.r-project.org/package=mathjaxr, but it seems to me as some
> > weird kind of monkey patching. All packages should be able to benefit
> from
> > proper math rendering without any dependencies, in my opinion. I think it
> > should not be much of a problem utilizing mathjax or other similar
> library
> > to enable that. Note we already know what supposed to be math in .Rd (and
> > we parse and process it in a special way in the PDF routine), the thing
> is
> > to render it, not typeset in italics, as it is the case nowadays.
> >
> > I would be happy to hear any opinion of yours!
>
> You are being very rude.
>
> The problem has been solved by mathjaxr.  If you don't like their
> approach, don't ask someone else (i.e. R Core) to come up with a
> different solution, do it yourself.
>
> I hope my opinion makes you happy!
>
> Duncan Murdoch
>

	[[alternative HTML version deleted]]


From murdoch@dunc@n @end|ng |rom gm@||@com  Thu May 27 23:41:28 2021
From: murdoch@dunc@n @end|ng |rom gm@||@com (Duncan Murdoch)
Date: Thu, 27 May 2021 17:41:28 -0400
Subject: [Rd] =?utf-8?q?Feature_request_=E2=80=93_math_in_HTML_help?=
In-Reply-To: <CA+6hu7dm+D=ba0=RWdHv5T4eCoHUS8G_chvcG=beQAw4SENFqg@mail.gmail.com>
References: <CA+6hu7fs_2UUfiMbE=8ywx17Hk+RC84o-MfXA3Xf7iziVpz0uA@mail.gmail.com>
 <7e79c9f0-d9b8-a8c5-bf2c-1a7748443369@gmail.com>
 <CA+6hu7dm+D=ba0=RWdHv5T4eCoHUS8G_chvcG=beQAw4SENFqg@mail.gmail.com>
Message-ID: <5e9af627-985b-52ac-1811-965800788e4e@gmail.com>

On 27/05/2021 4:54 p.m., Jan Net?k wrote:
> I am so sorry, my post sounded quite the opposite of what I intended! I 
> used the term "monkey patch" as a technical one (see 
> https://en.m.wikipedia.org/wiki/Monkey_patch 
> <https://en.m.wikipedia.org/wiki/Monkey_patch>) and I said "weird" 
> meaning mathjaxr is something fulfilling the concept, but only figuratively.
> 
> Actually I gave mathjaxr as a good (the best I know, in fact) example of 
> an approach how to solve the issue. However, no package can achieve 
> nicely formatted/rendered math in every single R documentation files. I 
> thought this could be solved globally, directly in R, as it is the case 
> of PDF Reference manual. I propose (in line with mathjaxr authors) that 
> mathjax is pretty good candidate for it.

Now I don't understand your point at all.  If you like mathjax, what's 
wrong with mathjaxr?

Just guessing, but perhaps you think that base R should incorporate 
mathjax, so package authors don't need to use mathjaxr.  That's an 
example of a common suggestion, of the form "Package X does a great job 
at Y.  Why doesn't R incorporate it?"  The answer is usually "Because 
package X is doing a great job at it.  If R incorporated it, it would 
add to the R Core workload with no advantage."  R Core should do things 
that can't be done in packages (and things they like to do, let's give 
them that).  If something has been done in a package and you don't like 
the way they did it, then do it better in a different package.

Duncan Murdoch


From @|mon@urb@nek @end|ng |rom R-project@org  Fri May 28 05:57:00 2021
From: @|mon@urb@nek @end|ng |rom R-project@org (Simon Urbanek)
Date: Fri, 28 May 2021 15:57:00 +1200
Subject: [Rd] =?utf-8?q?Feature_request_=E2=80=93_math_in_HTML_help?=
In-Reply-To: <CA+6hu7fs_2UUfiMbE=8ywx17Hk+RC84o-MfXA3Xf7iziVpz0uA@mail.gmail.com>
References: <CA+6hu7fs_2UUfiMbE=8ywx17Hk+RC84o-MfXA3Xf7iziVpz0uA@mail.gmail.com>
Message-ID: <8CF3B063-2D97-4A9F-8DD4-C92292D6F123@R-project.org>


Just to clarify the confusion, let me rephrase that. I see this as a request to add support for converting \eqn{} and friends to MathJax in R html documentation (both in core packages as well as all contributed packages). That sounds like a reasonable request to me, but I would not volunteer to implement it so I don't know how difficult that would be. Obviously, that has to happen in core R since it is part of the package building process.

The other part of the e-mail was unfortunately just conflating different things such as alternative ways of creating MathJax output in packages which is already possible and a solved problem as Duncan pointed out. You can write R documentation which uses MathJax in your package, but you cannot render existing R documentation with MathJax (currently, if I understand that correctly). I hope this clarifies things a bit.

Cheers,
Simon



> On 28/05/2021, at 6:34 AM, Jan Net?k <netikja at gmail.com> wrote:
> 
> Hi all,
> 
> long I have been thinking about proper rendering of math in the HTML form
> of R documentation. As you know, you can write \eqn{} in your .Rd files and
> this is nicely rendered into the PDF Reference manual of the package with
> the aid of TeX. However, that is not the case in the aforementioned HTML
> version that is used the most in my experience (using RStudio or help
> function in your console). I think R is the best language for statisticians
> and other data-driven fields, where formal definitions of key concepts are
> necessary and widely used in the documentation, unfortunately quite
> unusable for more complicated equations.
> 
> Recently I have stumbled upon an interesting approach to this issue, see
> https://cran.r-project.org/package=mathjaxr, but it seems to me as some
> weird kind of monkey patching. All packages should be able to benefit from
> proper math rendering without any dependencies, in my opinion. I think it
> should not be much of a problem utilizing mathjax or other similar library
> to enable that. Note we already know what supposed to be math in .Rd (and
> we parse and process it in a special way in the PDF routine), the thing is
> to render it, not typeset in italics, as it is the case nowadays.
> 
> I would be happy to hear any opinion of yours!
> 
> Best,
> 
> Jan Netik
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 


From c@@rd|@g@bor @end|ng |rom gm@||@com  Fri May 28 10:53:42 2021
From: c@@rd|@g@bor @end|ng |rom gm@||@com (=?UTF-8?B?R8OhYm9yIENzw6FyZGk=?=)
Date: Fri, 28 May 2021 10:53:42 +0200
Subject: [Rd] Possible ALTREP bug
Message-ID: <CABtg=K==9KEfe6+_nLnQA=H73NSr8x7==fwZe-k=4=TSu6hanA@mail.gmail.com>

I have found some weird SEXP corruption behavior with ALTREP, which
could be a bug. (Or I could be doing something wrong.)

I have an integer ALTREP vector that calls back to R from the Elt
method. When this vector is indexed in a lapply(), its first element
gets corrupted. Sometimes it's just a type change to logical, but
sometimes the corruption causes a crash.

I saw this on macOS from R 3.5.3 to 4.2.0. I created a small package
that demonstrates this: https://github.com/gaborcsardi/redfish

The R callback in this package calls `loadNamespace("Matrix")`, but
the same crash happens for other packages as well, and sometimes it
also happens if I don't load any packages at all. (But that example
was much more complicated, so I went with the package loading.)

It is somewhat random, and sometimes turning off the JIT avoids the
crash, but not always.

Hopefully I am just doing something wrong in the ALTREP code (see
https://github.com/gaborcsardi/redfish/blob/main/src/test.c), and it
is not actually a bug.

Thanks,
Gabor


From iuke-tier@ey m@iii@g oii uiow@@edu  Fri May 28 15:41:28 2021
From: iuke-tier@ey m@iii@g oii uiow@@edu (iuke-tier@ey m@iii@g oii uiow@@edu)
Date: Fri, 28 May 2021 08:41:28 -0500 (CDT)
Subject: [Rd] [External]  Possible ALTREP bug
In-Reply-To: <CABtg=K==9KEfe6+_nLnQA=H73NSr8x7==fwZe-k=4=TSu6hanA@mail.gmail.com>
References: <CABtg=K==9KEfe6+_nLnQA=H73NSr8x7==fwZe-k=4=TSu6hanA@mail.gmail.com>
Message-ID: <alpine.DEB.2.21.2105280819020.3254@luke-Latitude-7480>

integer and real Elt methods are not expected to allocate. You would
have to suspend GC to be able to do that. This currently can't be done
from package code.

Best,

luke

On Fri, 28 May 2021, G?bor Cs?rdi wrote:

> I have found some weird SEXP corruption behavior with ALTREP, which
> could be a bug. (Or I could be doing something wrong.)
>
> I have an integer ALTREP vector that calls back to R from the Elt
> method. When this vector is indexed in a lapply(), its first element
> gets corrupted. Sometimes it's just a type change to logical, but
> sometimes the corruption causes a crash.
>
> I saw this on macOS from R 3.5.3 to 4.2.0. I created a small package
> that demonstrates this: https://github.com/gaborcsardi/redfish
>
> The R callback in this package calls `loadNamespace("Matrix")`, but
> the same crash happens for other packages as well, and sometimes it
> also happens if I don't load any packages at all. (But that example
> was much more complicated, so I went with the package loading.)
>
> It is somewhat random, and sometimes turning off the JIT avoids the
> crash, but not always.
>
> Hopefully I am just doing something wrong in the ALTREP code (see
> https://github.com/gaborcsardi/redfish/blob/main/src/test.c), and it
> is not actually a bug.
>
> Thanks,
> Gabor
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

-- 
Luke Tierney
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu

From @zwj|08 @end|ng |rom gm@||@com  Fri May 28 15:53:31 2021
From: @zwj|08 @end|ng |rom gm@||@com (Jiefei Wang)
Date: Fri, 28 May 2021 21:53:31 +0800
Subject: [Rd] Possible ALTREP bug
In-Reply-To: <CABtg=K==9KEfe6+_nLnQA=H73NSr8x7==fwZe-k=4=TSu6hanA@mail.gmail.com>
References: <CABtg=K==9KEfe6+_nLnQA=H73NSr8x7==fwZe-k=4=TSu6hanA@mail.gmail.com>
Message-ID: <CAGiFhPMapwnV+S9pGOFZt-BZnemrV1Kst9i=KYMkMwMQPT6Rsw@mail.gmail.com>

Hi Gabor,

Calling back to an R function from the ALTREP function is not safe.
There has been a heated discussion on why you should not do it and
that the main reason that we do not have any R level ALTREP API. If
you are interested in it you can find it from this issue:
https://github.com/Bioconductor/Contributions/issues/1222

Best,
Jiefei

On Fri, May 28, 2021 at 4:54 PM G?bor Cs?rdi <csardi.gabor at gmail.com> wrote:
>
> I have found some weird SEXP corruption behavior with ALTREP, which
> could be a bug. (Or I could be doing something wrong.)
>
> I have an integer ALTREP vector that calls back to R from the Elt
> method. When this vector is indexed in a lapply(), its first element
> gets corrupted. Sometimes it's just a type change to logical, but
> sometimes the corruption causes a crash.
>
> I saw this on macOS from R 3.5.3 to 4.2.0. I created a small package
> that demonstrates this: https://github.com/gaborcsardi/redfish
>
> The R callback in this package calls `loadNamespace("Matrix")`, but
> the same crash happens for other packages as well, and sometimes it
> also happens if I don't load any packages at all. (But that example
> was much more complicated, so I went with the package loading.)
>
> It is somewhat random, and sometimes turning off the JIT avoids the
> crash, but not always.
>
> Hopefully I am just doing something wrong in the ALTREP code (see
> https://github.com/gaborcsardi/redfish/blob/main/src/test.c), and it
> is not actually a bug.
>
> Thanks,
> Gabor
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From c@@rd|@g@bor @end|ng |rom gm@||@com  Fri May 28 16:01:48 2021
From: c@@rd|@g@bor @end|ng |rom gm@||@com (=?UTF-8?B?R8OhYm9yIENzw6FyZGk=?=)
Date: Fri, 28 May 2021 16:01:48 +0200
Subject: [Rd] [External]  Possible ALTREP bug
In-Reply-To: <alpine.DEB.2.21.2105280819020.3254@luke-Latitude-7480>
References: <CABtg=K==9KEfe6+_nLnQA=H73NSr8x7==fwZe-k=4=TSu6hanA@mail.gmail.com>
 <alpine.DEB.2.21.2105280819020.3254@luke-Latitude-7480>
Message-ID: <CABtg=Km_HBvVdueWJzrtPsCrdysd3roQBKQgNzq21MRLsZ_TkQ@mail.gmail.com>

Thank you Luke, that makes a lot of sense,
Gabor

On Fri, May 28, 2021 at 3:41 PM <luke-tierney at uiowa.edu> wrote:
>
> integer and real Elt methods are not expected to allocate. You would
> have to suspend GC to be able to do that. This currently can't be done
> from package code.
>
> Best,
>
> luke
>
> On Fri, 28 May 2021, G?bor Cs?rdi wrote:
>
> > I have found some weird SEXP corruption behavior with ALTREP, which
> > could be a bug. (Or I could be doing something wrong.)
> >
> > I have an integer ALTREP vector that calls back to R from the Elt
> > method. When this vector is indexed in a lapply(), its first element
> > gets corrupted. Sometimes it's just a type change to logical, but
> > sometimes the corruption causes a crash.
> >
> > I saw this on macOS from R 3.5.3 to 4.2.0. I created a small package
> > that demonstrates this: https://github.com/gaborcsardi/redfish
> >
> > The R callback in this package calls `loadNamespace("Matrix")`, but
> > the same crash happens for other packages as well, and sometimes it
> > also happens if I don't load any packages at all. (But that example
> > was much more complicated, so I went with the package loading.)
> >
> > It is somewhat random, and sometimes turning off the JIT avoids the
> > crash, but not always.
> >
> > Hopefully I am just doing something wrong in the ALTREP code (see
> > https://github.com/gaborcsardi/redfish/blob/main/src/test.c), and it
> > is not actually a bug.
> >
> > Thanks,
> > Gabor
> >
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
> >
>
> --
> Luke Tierney
> Ralph E. Wareham Professor of Mathematical Sciences
> University of Iowa                  Phone:             319-335-3386
> Department of Statistics and        Fax:               319-335-3017
>     Actuarial Science
> 241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
> Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu


From j@me@@|@he@ter @end|ng |rom gm@||@com  Fri May 28 17:29:55 2021
From: j@me@@|@he@ter @end|ng |rom gm@||@com (Jim Hester)
Date: Fri, 28 May 2021 11:29:55 -0400
Subject: [Rd] [External] Possible ALTREP bug
In-Reply-To: <alpine.DEB.2.21.2105280819020.3254@luke-Latitude-7480>
References: <CABtg=K==9KEfe6+_nLnQA=H73NSr8x7==fwZe-k=4=TSu6hanA@mail.gmail.com>
 <alpine.DEB.2.21.2105280819020.3254@luke-Latitude-7480>
Message-ID: <CAD6tx94epfFGWb9AmGsq3QdF3S=gdRPO9oLRETQ4UaPN0Ja6bA@mail.gmail.com>

>From reading the discussion on the Bioconductor issue tracker it seems like
the reason the GC is not suspended for the non-string ALTREP Elt methods is
primarily due to performance concerns.

If this is the case perhaps an additional flag could be added to the
`R_set_altrep_*()` functions so ALTREP authors could indicate if GC should
be halted when that particular method is called for that particular ALTREP
class.

This would avoid the performance hit (other than a boolean check) for the
standard case when no allocations are expected, but allow authors to
indicate that R should pause GC if needed for methods in their class.

On Fri, May 28, 2021 at 9:42 AM <luke-tierney at uiowa.edu> wrote:

> integer and real Elt methods are not expected to allocate. You would
> have to suspend GC to be able to do that. This currently can't be done
> from package code.
>
> Best,
>
> luke
>
> On Fri, 28 May 2021, G?bor Cs?rdi wrote:
>
> > I have found some weird SEXP corruption behavior with ALTREP, which
> > could be a bug. (Or I could be doing something wrong.)
> >
> > I have an integer ALTREP vector that calls back to R from the Elt
> > method. When this vector is indexed in a lapply(), its first element
> > gets corrupted. Sometimes it's just a type change to logical, but
> > sometimes the corruption causes a crash.
> >
> > I saw this on macOS from R 3.5.3 to 4.2.0. I created a small package
> > that demonstrates this: https://github.com/gaborcsardi/redfish
> >
> > The R callback in this package calls `loadNamespace("Matrix")`, but
> > the same crash happens for other packages as well, and sometimes it
> > also happens if I don't load any packages at all. (But that example
> > was much more complicated, so I went with the package loading.)
> >
> > It is somewhat random, and sometimes turning off the JIT avoids the
> > crash, but not always.
> >
> > Hopefully I am just doing something wrong in the ALTREP code (see
> > https://github.com/gaborcsardi/redfish/blob/main/src/test.c), and it
> > is not actually a bug.
> >
> > Thanks,
> > Gabor
> >
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
> >
>
> --
> Luke Tierney
> Ralph E. Wareham Professor of Mathematical Sciences
> University of Iowa                  Phone:             319-335-3386
> Department of Statistics and        Fax:               319-335-3017
>     Actuarial Science
> 241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
> Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From v|ncent@gou|et @end|ng |rom me@com  Fri May 28 18:44:18 2021
From: v|ncent@gou|et @end|ng |rom me@com (Vincent Goulet)
Date: Fri, 28 May 2021 12:44:18 -0400
Subject: [Rd] Typo in Writing R Extensions
Message-ID: <1A63F5C8-B4FF-4C20-BBEC-288845CA4857@me.com>

Hi,

Just noticed this: on line 15296 of the current (master) R-exts.texi (section 7 of the compiled document), one reads

	would do most likely do different things, to the justifiable 

Either one of the "do" is in extra.

Best,

Vincent Goulet
Universit? Laval

From g@bembecker @end|ng |rom gm@||@com  Fri May 28 19:13:17 2021
From: g@bembecker @end|ng |rom gm@||@com (Gabriel Becker)
Date: Fri, 28 May 2021 10:13:17 -0700
Subject: [Rd] [External] Possible ALTREP bug
In-Reply-To: <CAD6tx94epfFGWb9AmGsq3QdF3S=gdRPO9oLRETQ4UaPN0Ja6bA@mail.gmail.com>
References: <CABtg=K==9KEfe6+_nLnQA=H73NSr8x7==fwZe-k=4=TSu6hanA@mail.gmail.com>
 <alpine.DEB.2.21.2105280819020.3254@luke-Latitude-7480>
 <CAD6tx94epfFGWb9AmGsq3QdF3S=gdRPO9oLRETQ4UaPN0Ja6bA@mail.gmail.com>
Message-ID: <CAD4oTHHL2AQymy3M=jwek_hm8BJfkS_iXrNUNWQDtqNsvSXwXA@mail.gmail.com>

Hi Jim et al,

Just to hopefully add a bit to what Luke already answered, from what I am
recalling looking back at that bioconductor thread Elt methods are used in
places where there are hard implicit assumptions that no garbage collection
will occur (ie they are called on things that aren't PROTECTed), and beyond
that, in places where there are hard assumptions that no error (longjmp)
will occur. I could be wrong, but I don't know that suspending garbage
collection would protect from the second one. Ie it is possible that an
error *ever* being raised from R code that implements an elt method could
cause all hell to break loose.

Luke or Tomas Kalibera would know more.

I was disappointed that implementing ALTREPs in R code was not in the cards
(it was in my original proposal back in 2016 to the DSC) but I trust Luke
that there are important reasons we can't safely allow that.

Best,
~G

On Fri, May 28, 2021 at 8:31 AM Jim Hester <james.f.hester at gmail.com> wrote:

> From reading the discussion on the Bioconductor issue tracker it seems like
> the reason the GC is not suspended for the non-string ALTREP Elt methods is
> primarily due to performance concerns.
>
> If this is the case perhaps an additional flag could be added to the
> `R_set_altrep_*()` functions so ALTREP authors could indicate if GC should
> be halted when that particular method is called for that particular ALTREP
> class.
>
> This would avoid the performance hit (other than a boolean check) for the
> standard case when no allocations are expected, but allow authors to
> indicate that R should pause GC if needed for methods in their class.
>
> On Fri, May 28, 2021 at 9:42 AM <luke-tierney at uiowa.edu> wrote:
>
> > integer and real Elt methods are not expected to allocate. You would
> > have to suspend GC to be able to do that. This currently can't be done
> > from package code.
> >
> > Best,
> >
> > luke
> >
> > On Fri, 28 May 2021, G?bor Cs?rdi wrote:
> >
> > > I have found some weird SEXP corruption behavior with ALTREP, which
> > > could be a bug. (Or I could be doing something wrong.)
> > >
> > > I have an integer ALTREP vector that calls back to R from the Elt
> > > method. When this vector is indexed in a lapply(), its first element
> > > gets corrupted. Sometimes it's just a type change to logical, but
> > > sometimes the corruption causes a crash.
> > >
> > > I saw this on macOS from R 3.5.3 to 4.2.0. I created a small package
> > > that demonstrates this: https://github.com/gaborcsardi/redfish
> > >
> > > The R callback in this package calls `loadNamespace("Matrix")`, but
> > > the same crash happens for other packages as well, and sometimes it
> > > also happens if I don't load any packages at all. (But that example
> > > was much more complicated, so I went with the package loading.)
> > >
> > > It is somewhat random, and sometimes turning off the JIT avoids the
> > > crash, but not always.
> > >
> > > Hopefully I am just doing something wrong in the ALTREP code (see
> > > https://github.com/gaborcsardi/redfish/blob/main/src/test.c), and it
> > > is not actually a bug.
> > >
> > > Thanks,
> > > Gabor
> > >
> > > ______________________________________________
> > > R-devel at r-project.org mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-devel
> > >
> >
> > --
> > Luke Tierney
> > Ralph E. Wareham Professor of Mathematical Sciences
> > University of Iowa                  Phone:             319-335-3386
> > Department of Statistics and        Fax:               319-335-3017
> >     Actuarial Science
> > 241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
> > Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
> >
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

	[[alternative HTML version deleted]]


From iuke-tier@ey m@iii@g oii uiow@@edu  Fri May 28 23:15:33 2021
From: iuke-tier@ey m@iii@g oii uiow@@edu (iuke-tier@ey m@iii@g oii uiow@@edu)
Date: Fri, 28 May 2021 16:15:33 -0500 (CDT)
Subject: [Rd] [External] Possible ALTREP bug
In-Reply-To: <CAD4oTHHL2AQymy3M=jwek_hm8BJfkS_iXrNUNWQDtqNsvSXwXA@mail.gmail.com>
References: <CABtg=K==9KEfe6+_nLnQA=H73NSr8x7==fwZe-k=4=TSu6hanA@mail.gmail.com>
 <alpine.DEB.2.21.2105280819020.3254@luke-Latitude-7480>
 <CAD6tx94epfFGWb9AmGsq3QdF3S=gdRPO9oLRETQ4UaPN0Ja6bA@mail.gmail.com>
 <CAD4oTHHL2AQymy3M=jwek_hm8BJfkS_iXrNUNWQDtqNsvSXwXA@mail.gmail.com>
Message-ID: <alpine.DEB.2.21.2105281554190.3254@luke-Latitude-7480>

Since the INTEGER_ELT, REAL_ELT, etc, functions are fairly new it may
be possible to check that places where they are used allow for them to
allocate. I have fixed the one that got caught by Gabor's example, and
a rchk run might be able to pick up others if rchk knows these could
allocate. (I may also be forgetting other places where the _ELt
methods are used.)  Fixing all call sites for REAL, INTEGER, etc, was
never realistic so there GC has to be suspended during the method
call, and that is done in the dispatch mechanism.

The bigger problem is jumps from inside things that existing code
assumes will not do that. Catching those jumps is possible but
expensive; doing anything sensible if one is caught is really not
possible.

Best,

luke

On Fri, 28 May 2021, Gabriel Becker wrote:

> Hi Jim et al,
> Just to hopefully add a bit to what Luke already answered, from what I am
> recalling looking back at that bioconductor thread Elt methods are used in
> places where there are hard implicit assumptions that no garbage collection
> will occur (ie they are called on things that aren't PROTECTed), and beyond
> that, in places where there are hard assumptions that no error (longjmp)
> will occur. I could be wrong, but I don't know that suspending garbage
> collection would protect from the second one. Ie it is possible that an
> error *ever* being raised from R code that implements an elt method could
> cause all hell to break loose.
> 
> Luke or Tomas Kalibera would know more.
> 
> I was disappointed that implementing ALTREPs in R code was not in the cards
> (it was in my original proposal back in 2016 to the DSC) but I trust Luke
> that there are important reasons we can't safely allow that.
> 
> Best,
> ~G
> 
> On Fri, May 28, 2021 at 8:31 AM Jim Hester <james.f.hester at gmail.com> wrote:
>       From reading the discussion on the Bioconductor issue tracker it
>       seems like
>       the reason the GC is not suspended for the non-string ALTREP Elt
>       methods is
>       primarily due to performance concerns.
>
>       If this is the case perhaps an additional flag could be added to
>       the
>       `R_set_altrep_*()` functions so ALTREP authors could indicate if
>       GC should
>       be halted when that particular method is called for that
>       particular ALTREP
>       class.
>
>       This would avoid the performance hit (other than a boolean
>       check) for the
>       standard case when no allocations are expected, but allow
>       authors to
>       indicate that R should pause GC if needed for methods in their
>       class.
>
>       On Fri, May 28, 2021 at 9:42 AM <luke-tierney at uiowa.edu> wrote:
>
>       > integer and real Elt methods are not expected to allocate. You
>       would
>       > have to suspend GC to be able to do that. This currently can't
>       be done
>       > from package code.
>       >
>       > Best,
>       >
>       > luke
>       >
>       > On Fri, 28 May 2021, G?bor Cs?rdi wrote:
>       >
>       > > I have found some weird SEXP corruption behavior with
>       ALTREP, which
>       > > could be a bug. (Or I could be doing something wrong.)
>       > >
>       > > I have an integer ALTREP vector that calls back to R from
>       the Elt
>       > > method. When this vector is indexed in a lapply(), its first
>       element
>       > > gets corrupted. Sometimes it's just a type change to
>       logical, but
>       > > sometimes the corruption causes a crash.
>       > >
>       > > I saw this on macOS from R 3.5.3 to 4.2.0. I created a small
>       package
>       > > that demonstrates this:
>       https://github.com/gaborcsardi/redfish
>       > >
>       > > The R callback in this package calls
>       `loadNamespace("Matrix")`, but
>       > > the same crash happens for other packages as well, and
>       sometimes it
>       > > also happens if I don't load any packages at all. (But that
>       example
>       > > was much more complicated, so I went with the package
>       loading.)
>       > >
>       > > It is somewhat random, and sometimes turning off the JIT
>       avoids the
>       > > crash, but not always.
>       > >
>       > > Hopefully I am just doing something wrong in the ALTREP code
>       (see
>       > >
>       https://github.com/gaborcsardi/redfish/blob/main/src/test.c),
>       and it
>       > > is not actually a bug.
>       > >
>       > > Thanks,
>       > > Gabor
>       > >
>       > > ______________________________________________
>       > > R-devel at r-project.org mailing list
>       > > https://stat.ethz.ch/mailman/listinfo/r-devel
>       > >
>       >
>       > --
>       > Luke Tierney
>       > Ralph E. Wareham Professor of Mathematical Sciences
>       > University of Iowa? ? ? ? ? ? ? ? ? Phone:? ? ? ? ? ?
>       ?319-335-3386
>       > Department of Statistics and? ? ? ? Fax:? ? ? ? ? ? ?
>       ?319-335-3017
>       >? ? ?Actuarial Science
>       > 241 Schaeffer Hall? ? ? ? ? ? ? ? ? email:?
>       ?luke-tierney at uiowa.edu
>       > Iowa City, IA 52242? ? ? ? ? ? ? ? ?WWW:?
>       http://www.stat.uiowa.edu
>       > ______________________________________________
>       > R-devel at r-project.org mailing list
>       > https://stat.ethz.ch/mailman/listinfo/r-devel
>       >
>
>       ? ? ? ? [[alternative HTML version deleted]]
>
>       ______________________________________________
>       R-devel at r-project.org mailing list
>       https://stat.ethz.ch/mailman/listinfo/r-devel
> 
> 
>

-- 
Luke Tierney
Ralph E. Wareham Professor of Mathematical Sciences
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
    Actuarial Science
241 Schaeffer Hall                  email:   luke-tierney at uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu

From tom@@@k@||ber@ @end|ng |rom gm@||@com  Mon May 31 09:06:12 2021
From: tom@@@k@||ber@ @end|ng |rom gm@||@com (Tomas Kalibera)
Date: Mon, 31 May 2021 09:06:12 +0200
Subject: [Rd] Typo in Writing R Extensions
In-Reply-To: <1A63F5C8-B4FF-4C20-BBEC-288845CA4857@me.com>
References: <1A63F5C8-B4FF-4C20-BBEC-288845CA4857@me.com>
Message-ID: <ad10e29f-e135-0e9b-c348-571a8bba27b3@gmail.com>

Thanks, fixed now
Tomas

On 5/28/21 6:44 PM, Vincent Goulet via R-devel wrote:
> Hi,
>
> Just noticed this: on line 15296 of the current (master) R-exts.texi (section 7 of the compiled document), one reads
>
> 	would do most likely do different things, to the justifiable
>
> Either one of the "do" is in extra.
>
> Best,
>
> Vincent Goulet
> Universit? Laval
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


