From jeroenooms at gmail.com  Sat Mar  1 00:29:09 2014
From: jeroenooms at gmail.com (Jeroen Ooms)
Date: Fri, 28 Feb 2014 15:29:09 -0800
Subject: [Rd] How to use R script inside javascript?
In-Reply-To: <1393576944534-4685988.post@n4.nabble.com>
References: <1393576944534-4685988.post@n4.nabble.com>
Message-ID: <CABFfbXvduYGnbD12=u9z_pXH8QqzqmEEuSy6P3aTAUVdvPbWEw@mail.gmail.com>

On Fri, Feb 28, 2014 at 12:42 AM, jayesh.baviskar
<jayesh.baviskar at equestind.com> wrote:
>
> I want to use r script inside the webpage. I come to know that is it
> possible to embed R script with HTML.

This question is not appropriate for this mailing list.

That said, have a look at opencpu, and in particular the opencpu.js
javascript client library: https://www.opencpu.org/jslib.html. It
provides the most seamless integration of R and JavaScript available
today. The examples on jsfiddle should get you started:
http://jsfiddle.net/user/opencpu/fiddles/


From bbolker at gmail.com  Sat Mar  1 21:47:45 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Sat, 1 Mar 2014 15:47:45 -0500
Subject: [Rd] trivial typo in src/library/base/man/timezone.Rd
Message-ID: <53124771.7070000@gmail.com>

>From SVN r65092:

line 111: "Note that that the abbreviations have changed over the years"
(duplicated "that")

  Ben Bolker


From ripley at stats.ox.ac.uk  Sun Mar  2 10:11:45 2014
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 2 Mar 2014 09:11:45 +0000
Subject: [Rd] trivial typo in src/library/base/man/timezone.Rd
In-Reply-To: <53124771.7070000@gmail.com>
References: <53124771.7070000@gmail.com>
Message-ID: <5312F5D1.8040504@stats.ox.ac.uk>

On 01/03/2014 20:47, Ben Bolker wrote:
>>From SVN r65092:
>
> line 111: "Note that that the abbreviations have changed over the years"
> (duplicated "that")
>
>    Ben Bolker
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>
This is work in progress: see e.g. 
http://cran.r-project.org/bin/windows/base/rdevel.html .

Especially where there are multiple versions to be checked, it can take 
a few days to finish things like this, and that was already re-written 
in the working version.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From bbolker at gmail.com  Mon Mar  3 01:38:48 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Sun, 2 Mar 2014 19:38:48 -0500
Subject: [Rd] reference classes, LAZY_DUPLICATE_OK, and external pointers
Message-ID: <5313CF18.7090607@ufl.edu>

We (the lme4 authors) are having a problem with doing a proper deep
copy of a reference class object in recent versions of R-devel with
the LAZY_DUPLICATE_OK flag in src/main/bind.c enabled.

Apologies in advance for any improper terminology.

TL;DR Is there an elegant way to force non-lazy/deep copying in our
case? Is anyone else using reference classes with a field that is an
external pointer?

This is how copying of reference classes works in a normal
situation:

library(data.table) ## for address() function
setRefClass("defaultRC",fields="theta")
d1 <- new("defaultRC")
d1$theta <- 1
address(d1$theta)  ##  "0xbbbbb70"
d2 <- d1$copy()
address(d2$theta)  ## same as above
d2$theta <- 2
address(d2$theta)  ## now modified, by magic
d1$theta  ## unmodified

The extra complication in our case is that many of the objects within
our reference class are actually accessed via an external pointer,
which is initialized when necessary -- details are copied below for
those who want them, or you can see the code at
https://github.com/lme4/lme4

The problem is that this sneaky way of copying the object's contents
doesn't trigger R's (new) rules for recognizing that a non-lazy copy
should be made.

library(lme4)
fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
pp <- fm1 at pp
pp$theta ## [1] 0.96673279 0.01516906 0.23090960
address(pp$theta) ## something
pp$Ptr ## <pointer: ...>
xpp <- pp$copy() ## default is deep copy
xpp$Ptr  ## <pointer: (nil)>
address(xpp$theta)  ## same as above
xpp$setTheta(c(0,0,0)) ## referenced through Ptr field
xpp$Ptr  ## now set to non-nil
fm1 at pp$theta  ## changes to (0,0,0).  oops.

So apparently when the xpp$theta object is copied into the external
pointer, a reference/lazy copy is made.   (xpp$theta itself is
read-only, so I can't do the assignment that way)

I can hack around this in a very ugly way by doing a trivial
modification when assigning inside the copy method:

assign("theta",get("theta",envir=selfEnv)+0, envir=vEnv)

... but (a) this is very ugly and (b) it seems very unsafe --
as R gets smarter it should start to recognize trivial changes
like x+0 and x*1 and *not* copy in these cases ...

Method details:

## from R/AllClass.R, merPredD RC definition

ptr          = function() {
      'returns the external pointer, regenerating if necessary'
      if (length(theta)) {
            if (.Call(isNullExtPtr, Ptr)) initializePtr()
      }
      Ptr
},

## ditto

initializePtr = function() {
    Ptr <<- .Call(merPredDCreate, as(X, "matrix"), Lambdat,
                  LamtUt, Lind, RZX, Ut, Utr, V, VtV, Vtr,
                  Xwts, Zt, beta0, delb, delu, theta, u0)
 ...
}

merPredDCreate in turn just copies the relevant bits into a new C++
class object:

/* see src/external.cpp */

    SEXP merPredDCreate(SEXP Xs, SEXP Lambdat, SEXP LamtUt, SEXP Lind,
                        SEXP RZX, SEXP Ut, SEXP Utr, SEXP V, SEXP VtV,
                        SEXP Vtr, SEXP Xwts, SEXP Zt, SEXP beta0,
                        SEXP delb, SEXP delu, SEXP theta, SEXP u0) {
        BEGIN_RCPP;
        merPredD *ans = new merPredD(Xs, Lambdat, LamtUt, Lind, RZX,
Ut, Utr, V, VtV,
                                     Vtr, Xwts, Zt, beta0, delb, delu,
theta, u0);
        return wrap(XPtr<merPredD>(ans, true));
        END_RCPP;
    }


From simon.urbanek at r-project.org  Mon Mar  3 02:05:36 2014
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Sun, 2 Mar 2014 20:05:36 -0500
Subject: [Rd] reference classes, LAZY_DUPLICATE_OK, and external pointers
In-Reply-To: <5313CF18.7090607@ufl.edu>
References: <5313CF18.7090607@ufl.edu>
Message-ID: <197E962E-2774-4786-9542-F54FAC145BAA@r-project.org>

Ben,

On Mar 2, 2014, at 7:38 PM, Ben Bolker <bbolker at gmail.com> wrote:

> We (the lme4 authors) are having a problem with doing a proper deep
> copy of a reference class object in recent versions of R-devel with
> the LAZY_DUPLICATE_OK flag in src/main/bind.c enabled.
> 
> Apologies in advance for any improper terminology.
> 
> TL;DR Is there an elegant way to force non-lazy/deep copying in our
> case? Is anyone else using reference classes with a field that is an
> external pointer?
> 
> This is how copying of reference classes works in a normal
> situation:
> 
> library(data.table) ## for address() function
> setRefClass("defaultRC",fields="theta")
> d1 <- new("defaultRC")
> d1$theta <- 1
> address(d1$theta)  ##  "0xbbbbb70"
> d2 <- d1$copy()
> address(d2$theta)  ## same as above
> d2$theta <- 2
> address(d2$theta)  ## now modified, by magic
> d1$theta  ## unmodified
> 
> The extra complication in our case is that many of the objects within
> our reference class are actually accessed via an external pointer,
> which is initialized when necessary -- details are copied below for
> those who want them, or you can see the code at
> https://github.com/lme4/lme4
> 
> The problem is that this sneaky way of copying the object's contents
> doesn't trigger R's (new) rules for recognizing that a non-lazy copy
> should be made.
> 

This is not R's decision - AFAICS your code is incorrectly assuming that there is no other reference where there is no such guarantee. Your code that assigns into the external pointer has to make that decision - it's not R's to make since you are taking the full responsibility for external pointers by circumventing R's handing. External pointers had always had reference semantics. Note that this is not new - you had to inspect the NAMED bits and call duplicate() yourself to guarantee a copy even in previous R versions. It just so happened that bugs of not doing so were often masked by R being more conservative such that in some circumstanced there were enough references to function arguments that R would defensively create a new copy.

So, the same applies as it did before - if you store something that you want to be mutable in C/C++ you have to check the references and call duplicate() if you don't own the only reference.

Cheers,
Simon



> library(lme4)
> fm1 <- lmer(Reaction ~ Days + (Days|Subject), sleepstudy)
> pp <- fm1 at pp
> pp$theta ## [1] 0.96673279 0.01516906 0.23090960
> address(pp$theta) ## something
> pp$Ptr ## <pointer: ...>
> xpp <- pp$copy() ## default is deep copy
> xpp$Ptr  ## <pointer: (nil)>
> address(xpp$theta)  ## same as above
> xpp$setTheta(c(0,0,0)) ## referenced through Ptr field
> xpp$Ptr  ## now set to non-nil
> fm1 at pp$theta  ## changes to (0,0,0).  oops.
> 
> So apparently when the xpp$theta object is copied into the external
> pointer, a reference/lazy copy is made.   (xpp$theta itself is
> read-only, so I can't do the assignment that way)
> 
> I can hack around this in a very ugly way by doing a trivial
> modification when assigning inside the copy method:
> 
> assign("theta",get("theta",envir=selfEnv)+0, envir=vEnv)
> 
> ... but (a) this is very ugly and (b) it seems very unsafe --
> as R gets smarter it should start to recognize trivial changes
> like x+0 and x*1 and *not* copy in these cases ...
> 
> Method details:
> 
> ## from R/AllClass.R, merPredD RC definition
> 
> ptr          = function() {
>      'returns the external pointer, regenerating if necessary'
>      if (length(theta)) {
>            if (.Call(isNullExtPtr, Ptr)) initializePtr()
>      }
>      Ptr
> },
> 
> ## ditto
> 
> initializePtr = function() {
>    Ptr <<- .Call(merPredDCreate, as(X, "matrix"), Lambdat,
>                  LamtUt, Lind, RZX, Ut, Utr, V, VtV, Vtr,
>                  Xwts, Zt, beta0, delb, delu, theta, u0)
> ...
> }
> 
> merPredDCreate in turn just copies the relevant bits into a new C++
> class object:
> 
> /* see src/external.cpp */
> 
>    SEXP merPredDCreate(SEXP Xs, SEXP Lambdat, SEXP LamtUt, SEXP Lind,
>                        SEXP RZX, SEXP Ut, SEXP Utr, SEXP V, SEXP VtV,
>                        SEXP Vtr, SEXP Xwts, SEXP Zt, SEXP beta0,
>                        SEXP delb, SEXP delu, SEXP theta, SEXP u0) {
>        BEGIN_RCPP;
>        merPredD *ans = new merPredD(Xs, Lambdat, LamtUt, Lind, RZX,
> Ut, Utr, V, VtV,
>                                     Vtr, Xwts, Zt, beta0, delb, delu,
> theta, u0);
>        return wrap(XPtr<merPredD>(ans, true));
>        END_RCPP;
>    }
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 


From bbolker at gmail.com  Mon Mar  3 02:42:30 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Sun, 2 Mar 2014 20:42:30 -0500
Subject: [Rd] reference classes, LAZY_DUPLICATE_OK, and external pointers
In-Reply-To: <197E962E-2774-4786-9542-F54FAC145BAA@r-project.org>
References: <5313CF18.7090607@ufl.edu>
	<197E962E-2774-4786-9542-F54FAC145BAA@r-project.org>
Message-ID: <5313DE06.8020102@gmail.com>

On 14-03-02 08:05 PM, Simon Urbanek wrote:
> Ben,
> 
> On Mar 2, 2014, at 7:38 PM, Ben Bolker <bbolker at gmail.com> wrote:
> 
>> We (the lme4 authors) are having a problem with doing a proper
>> deep copy of a reference class object in recent versions of R-devel
>> with the LAZY_DUPLICATE_OK flag in src/main/bind.c enabled.
>> 
>> Apologies in advance for any improper terminology.
>> 
>> TL;DR Is there an elegant way to force non-lazy/deep copying in
>> our case? Is anyone else using reference classes with a field that
>> is an external pointer?
>> 
>> This is how copying of reference classes works in a normal 
>> situation:
>> 
>> library(data.table) ## for address() function 
>> setRefClass("defaultRC",fields="theta") d1 <- new("defaultRC") 
>> d1$theta <- 1 address(d1$theta)  ##  "0xbbbbb70" d2 <- d1$copy() 
>> address(d2$theta)  ## same as above d2$theta <- 2 address(d2$theta)
>> ## now modified, by magic d1$theta  ## unmodified
>> 
>> The extra complication in our case is that many of the objects
>> within our reference class are actually accessed via an external
>> pointer, which is initialized when necessary -- details are copied
>> below for those who want them, or you can see the code at 
>> https://github.com/lme4/lme4
>> 
>> The problem is that this sneaky way of copying the object's
>> contents doesn't trigger R's (new) rules for recognizing that a
>> non-lazy copy should be made.
>> 
> 
> This is not R's decision - AFAICS your code is incorrectly assuming
> that there is no other reference where there is no such guarantee.
> Your code that assigns into the external pointer has to make that
> decision - it's not R's to make since you are taking the full
> responsibility for external pointers by circumventing R's handing.
> External pointers had always had reference semantics. Note that this
> is not new - you had to inspect the NAMED bits and call duplicate()
> yourself to guarantee a copy even in previous R versions. It just so
> happened that bugs of not doing so were often masked by R being more
> conservative such that in some circumstanced there were enough
> references to function arguments that R would defensively create a
> new copy.
> 
> So, the same applies as it did before - if you store something that
> you want to be mutable in C/C++ you have to check the references and
> call duplicate() if you don't own the only reference.
> 
> Cheers, Simon

  Thanks, that's extremely useful.

  Ben
> 
> 
> 
>> library(lme4) fm1 <- lmer(Reaction ~ Days + (Days|Subject),
>> sleepstudy) pp <- fm1 at pp pp$theta ## [1] 0.96673279 0.01516906
>> 0.23090960 address(pp$theta) ## something pp$Ptr ## <pointer: ...> 
>> xpp <- pp$copy() ## default is deep copy xpp$Ptr  ## <pointer:
>> (nil)> address(xpp$theta)  ## same as above xpp$setTheta(c(0,0,0))
>> ## referenced through Ptr field xpp$Ptr  ## now set to non-nil 
>> fm1 at pp$theta  ## changes to (0,0,0).  oops.
>> 
>> So apparently when the xpp$theta object is copied into the
>> external pointer, a reference/lazy copy is made.   (xpp$theta
>> itself is read-only, so I can't do the assignment that way)
>> 
>> I can hack around this in a very ugly way by doing a trivial 
>> modification when assigning inside the copy method:
>> 
>> assign("theta",get("theta",envir=selfEnv)+0, envir=vEnv)
>> 
>> ... but (a) this is very ugly and (b) it seems very unsafe -- as R
>> gets smarter it should start to recognize trivial changes like x+0
>> and x*1 and *not* copy in these cases ...
>> 
>> Method details:
>> 
>> ## from R/AllClass.R, merPredD RC definition
>> 
>> ptr          = function() { 'returns the external pointer,
>> regenerating if necessary' if (length(theta)) { if
>> (.Call(isNullExtPtr, Ptr)) initializePtr() } Ptr },
>> 
>> ## ditto
>> 
>> initializePtr = function() { Ptr <<- .Call(merPredDCreate, as(X,
>> "matrix"), Lambdat, LamtUt, Lind, RZX, Ut, Utr, V, VtV, Vtr, Xwts,
>> Zt, beta0, delb, delu, theta, u0) ... }
>> 
>> merPredDCreate in turn just copies the relevant bits into a new
>> C++ class object:
>> 
>> /* see src/external.cpp */
>> 
>> SEXP merPredDCreate(SEXP Xs, SEXP Lambdat, SEXP LamtUt, SEXP Lind, 
>> SEXP RZX, SEXP Ut, SEXP Utr, SEXP V, SEXP VtV, SEXP Vtr, SEXP Xwts,
>> SEXP Zt, SEXP beta0, SEXP delb, SEXP delu, SEXP theta, SEXP u0) { 
>> BEGIN_RCPP; merPredD *ans = new merPredD(Xs, Lambdat, LamtUt, Lind,
>> RZX, Ut, Utr, V, VtV, Vtr, Xwts, Zt, beta0, delb, delu, theta,
>> u0); return wrap(XPtr<merPredD>(ans, true)); END_RCPP; }
>> 
>> ______________________________________________ 
>> R-devel at r-project.org mailing list 
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>> 
>


From Jens.Oehlschlaegel at truecluster.com  Sun Mar  2 18:37:59 2014
From: Jens.Oehlschlaegel at truecluster.com (=?ISO-8859-15?Q?Jens_Oehlschl=E4gel?=)
Date: Sun, 02 Mar 2014 18:37:59 +0100
Subject: [Rd] internal copying in R (soon to be released R-3.1.0
Message-ID: <53136C77.2010603@truecluster.com>

Dear core group,

Which operation in R guarantees to get a true copy of an atomic vector, 
not just a second symbol pointing to the same shared memory?

y <- x[]
#?

y <- x
y[1] <- y[1]
#?

Is there any function that returns its argument as a non-shared atomic 
but only copies if the argument was shared?

Given an atomic vector x, what is the best official way to find out 
whether other symbols share the vector RAM? Querying NAMED() < 2 doesn't 
work because .Call sets sxpinfo_struct.named to 2. It even sets it to 2 
if the argument to .Call was a never-named expression!?

 > named(1:3)
[1] 2

And it seems to set it permanently, pure read-access can trigger 
copy-on-modify:

 > x <- integer(1e8)
 > system.time(x[1]<-1L)
        User      System verstrichen
           0           0           0
 > system.time(x[1]<-2L)
        User      System verstrichen
           0           0           0

having called .Call now leads to an unnecessary copy on the next assignment

 > named(x)
[1] 2
 > system.time(x[1]<-3L)
        User      System verstrichen
        0.14        0.07        0.20
 > system.time(x[1]<-4L)
        User      System verstrichen
           0           0           0

this not only happens with user written functions doing read-access

 > is.unsorted(x)
[1] TRUE
 > system.time(x[1]<-5L)
        User      System verstrichen
        0.11        0.09        0.21

Why don't you simply give package authors read-access to 
sxpinfo_struct.named in .Call (without setting it to 2)? That would give 
us more control and also save some unnecessary copying. I guess once R 
switches to reference-counting preventive increasing in .Call could not 
be continued anyhow.

Kind regards


Jens Oehlschl?gel

P.S. please cc me in answers as I am not member of r-devel


P.P.S. function named() was tentatively defined as follows:

named <- function(x)
   .Call("R_bit_named", x, PACKAGE="bit")

SEXP R_bit_named(SEXP x){
   SEXP ret_;
   PROTECT( ret_ = allocVector(INTSXP,1) );
   INTEGER(ret_)[0] = NAMED(x);
   UNPROTECT(1);
   return ret_;
}


 > version
                _
platform       x86_64-w64-mingw32
arch           x86_64
os             mingw32
system         x86_64, mingw32
status         Under development (unstable)
major          3
minor          1.0
year           2014
month          02
day            28
svn rev        65091
language       R
version.string R Under development (unstable) (2014-02-28 r65091)
nickname       Unsuffered Consequences


From maechler at stat.math.ethz.ch  Mon Mar  3 13:48:39 2014
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 3 Mar 2014 13:48:39 +0100
Subject: [Rd] [PATCH] suggestions for R-lang manual
In-Reply-To: <CAE3=dmcKmmaQtSwx41i_PUfeC3o1z7rRTUWSBHfK64T-bpjbaQ@mail.gmail.com>
References: <CAE3=dmefQSwG-AELeZmZFHi90VaknXUtEgfsVQgxii7J0Z75cg@mail.gmail.com>
	<CAE3=dmcKmmaQtSwx41i_PUfeC3o1z7rRTUWSBHfK64T-bpjbaQ@mail.gmail.com>
Message-ID: <21268.31271.777129.441646@stat.math.ethz.ch>

>>>>> Scott Kostyshak <skostysh at princeton.edu>
>>>>>     on Thu, 27 Feb 2014 16:43:02 -0500 writes:

    > On Thu, Nov 21, 2013 at 1:17 AM, Scott Kostyshak <skostysh at princeton.edu> wrote:
    >> Attached is a patch with suggestions for the R-lang manual at r64277.
    >> 
    >> Below are a few comments (some are implemented in the patch):
    >> 
    >> In the section "Objects", there is a table introduced by "The
    >> following table describes the possible values returned by typeof". One
    >> of the results is "any". Can "any" be returned by "typeof()" ?

ANYSXP  is a valid internal type on the C level, and
src/main/util.c  will make  typeof(ob) return "any"
if you can get your hands at an R level object of that type.
I'd guess you can only get it currently by using .Call() and 
using your own C code, .. but at least that way it must be possible.

    >> Regarding the "Recycling rules" section,
    >> 
    >> -One exception is that when adding vectors to matrices, a warning is not
    >> -given if the lengths are incompatible.
    >> - at c Is that a bug?
    >> -
    >> 
    >> was this a bug that was fixed? 

I did not investigate in details, but yes, I vaguely remember we
had fixed that.  So indeed, it's fine you omitted the para in
your patch.

    >> I see the following behavior:
    >> 
    >>> myvec <- 1:3
    >>> mymat <- matrix(1:12, ncol=2)
    >>> myvec <- 1:5
    >>> myvec + mymat
    >> [,1] [,2]
    >> [1,]    2    9
    >> [2,]    4   11
    >> [3,]    6   13
    >> [4,]    8   15
    >> [5,]   10   12
    >> [6,]    7   14
    >> Warning message:
    >> In myvec + mymat :
    >> longer object length is not a multiple of shorter object length
    >>> 
    >> 
    >> Regarding
    >> 
    >> -The arguments in the call to the generic are rematched with the
    >> -arguments for the method using the standard argument matching mechanism.
    >> -The first argument, i.e.@: the object, will have been evaluated.
    >> -
    >> 
    >> this information is duplicated. See a few paragraphs up "When the
    >> method is invoked it is called..."

    >> Scott

Thank you, Scott.
Indeed, I've finally carefully looked at the patch, and applied
it - for R-devel, to become R 3.1.0 in April.

Martin


    >> --
    >> Scott Kostyshak
    >> Economics PhD Candidate
    >> Princeton University

    > The patch still applies cleanly (one offset) on r65090.

    > Best,
    > Scott


From simon.urbanek at r-project.org  Mon Mar  3 19:37:34 2014
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Mon, 3 Mar 2014 13:37:34 -0500
Subject: [Rd] internal copying in R (soon to be released R-3.1.0
In-Reply-To: <53136C77.2010603@truecluster.com>
References: <53136C77.2010603@truecluster.com>
Message-ID: <E4137A27-9188-4932-891E-7451ACEDC7C2@r-project.org>


On Mar 2, 2014, at 12:37 PM, Jens Oehlschl?gel <jens.oehlschlaegel at truecluster.com> wrote:

> Dear core group,
> 
> Which operation in R guarantees to get a true copy of an atomic vector, not just a second symbol pointing to the same shared memory?
> 

None, there is no concept of "shared" memory at R level. You seem to be mixing C level API specifics and the R language. In the former duplicate() creates a new copy.


> y <- x[]
> #?
> 
> y <- x
> y[1] <- y[1]
> #?
> 
> Is there any function that returns its argument as a non-shared atomic but only copies if the argument was shared?
> 
> Given an atomic vector x, what is the best official way to find out whether other symbols share the vector RAM? Querying NAMED() < 2 doesn't work because .Call sets sxpinfo_struct.named to 2. It even sets it to 2 if the argument to .Call was a never-named expression!?
> 
> > named(1:3)
> [1] 2
> 

Assuming that you are talking about the C API, please consider reading about the concepts involved. .Call() doesn't set named to 2 at all - it passes whatever object is passed so it is the C code's responsibility to handle incoming objects according to the desired semantics (see the previous post here). 


> And it seems to set it permanently, pure read-access can trigger copy-on-modify:
> 
> > x <- integer(1e8)
> > system.time(x[1]<-1L)
>       User      System verstrichen
>          0           0           0
> > system.time(x[1]<-2L)
>       User      System verstrichen
>          0           0           0
> 
> having called .Call now leads to an unnecessary copy on the next assignment
> 
> > named(x)
> [1] 2
> > system.time(x[1]<-3L)
>       User      System verstrichen
>       0.14        0.07        0.20
> > system.time(x[1]<-4L)
>       User      System verstrichen
>          0           0           0
> 
> this not only happens with user written functions doing read-access
> 
> > is.unsorted(x)
> [1] TRUE
> > system.time(x[1]<-5L)
>       User      System verstrichen
>       0.11        0.09        0.21
> 
> Why don't you simply give package authors read-access to sxpinfo_struct.named in .Call (without setting it to 2)? That would give us more control and also save some unnecessary copying.

Again, you're barking up the wrong tree - .Call() doesn't bump NAMED at all - it simply passes the object:

#include <Rinternals.h>
SEXP nam(SEXP x) { return ScalarInteger(NAMED(x)); }

> .Call("nam", 1+1)
[1] 0
> x=1+1
> .Call("nam", x)
[1] 1
> y=x
> .Call("nam", x)
[1] 2

Cheers,
Simon




> I guess once R switches to reference-counting preventive increasing in .Call could not be continued anyhow.
> 
> Kind regards
> 
> 
> Jens Oehlschl?gel
> 
> P.S. please cc me in answers as I am not member of r-devel
> 
> 
> P.P.S. function named() was tentatively defined as follows:
> 
> named <- function(x)
>  .Call("R_bit_named", x, PACKAGE="bit")
> 
> SEXP R_bit_named(SEXP x){
>  SEXP ret_;
>  PROTECT( ret_ = allocVector(INTSXP,1) );
>  INTEGER(ret_)[0] = NAMED(x);
>  UNPROTECT(1);
>  return ret_;
> }
> 
> 
> > version
>               _
> platform       x86_64-w64-mingw32
> arch           x86_64
> os             mingw32
> system         x86_64, mingw32
> status         Under development (unstable)
> major          3
> minor          1.0
> year           2014
> month          02
> day            28
> svn rev        65091
> language       R
> version.string R Under development (unstable) (2014-02-28 r65091)
> nickname       Unsuffered Consequences
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 


From cubranic at stat.ubc.ca  Mon Mar  3 20:09:18 2014
From: cubranic at stat.ubc.ca (Davor Cubranic)
Date: Mon, 3 Mar 2014 11:09:18 -0800
Subject: [Rd] Admin manual bug: packages installed on OS X
Message-ID: <F9DE5070-0EDD-401A-B60E-0B5171343733@stat.ubc.ca>

The administration manual for the current release of R says in section 4.2, Uninstalling under OS X:

"The installation consisted of three Apple packages: org.r-project.R.Leopard.fw.pkg, org.r-project.R.Leopard.GUI.pkg and org.r-project.x86_64.tcltk.x11?.

This is not the case with R 3.0.2 on OS X 10.9.2:

$ pkgutil --pkgs='.*r-project.*?
org.r-project.R.x86_64.fw.pkg
org.r-project.R.x86_64.GUI.pkg
org.r-project.x86_64.tcltk.x11

It looks like ?Leopard? should be replaced with ?x86_64?.

Davor

From simon.urbanek at r-project.org  Mon Mar  3 21:13:16 2014
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Mon, 3 Mar 2014 15:13:16 -0500
Subject: [Rd] Admin manual bug: packages installed on OS X
In-Reply-To: <F9DE5070-0EDD-401A-B60E-0B5171343733@stat.ubc.ca>
References: <F9DE5070-0EDD-401A-B60E-0B5171343733@stat.ubc.ca>
Message-ID: <A4B07467-E7A8-4AE8-BDD7-5462D90F2BC5@r-project.org>

Thanks, fixed,
Simon


On Mar 3, 2014, at 2:09 PM, Davor Cubranic <cubranic at stat.ubc.ca> wrote:

> The administration manual for the current release of R says in section 4.2, Uninstalling under OS X:
> 
> "The installation consisted of three Apple packages: org.r-project.R.Leopard.fw.pkg, org.r-project.R.Leopard.GUI.pkg and org.r-project.x86_64.tcltk.x11?.
> 
> This is not the case with R 3.0.2 on OS X 10.9.2:
> 
> $ pkgutil --pkgs='.*r-project.*?
> org.r-project.R.x86_64.fw.pkg
> org.r-project.R.x86_64.GUI.pkg
> org.r-project.x86_64.tcltk.x11
> 
> It looks like ?Leopard? should be replaced with ?x86_64?.
> 
> Davor
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 


From Jens.Oehlschlaegel at truecluster.com  Mon Mar  3 21:35:27 2014
From: Jens.Oehlschlaegel at truecluster.com (=?ISO-8859-1?Q?Jens_Oehlschl=E4gel?=)
Date: Mon, 03 Mar 2014 21:35:27 +0100
Subject: [Rd] internal copying in R (soon to be released R-3.1.0
In-Reply-To: <E4137A27-9188-4932-891E-7451ACEDC7C2@r-project.org>
References: <53136C77.2010603@truecluster.com>
	<E4137A27-9188-4932-891E-7451ACEDC7C2@r-project.org>
Message-ID: <5314E78F.4030804@truecluster.com>

Thanks for answering Simon,

 > None, there is no concept of "shared" memory at R level. You seem to 
be mixing C level API specifics and the R language. In the former 
duplicate() creates a new copy.

I take this as evidence that calling duplicate() is the only way to make 
sure I have a non-shared object.

 > Assuming that you are talking about the C API, please consider 
reading about the concepts involved. .Call() doesn't set named to 2 at 
all - it passes whatever object is passed so it is the C code's 
responsibility to handle incoming objects according to the desired 
semantics (see the previous post here).

Well, I did read, for example "Writing R Extensions" (Version 3.1.0 
Under development (2014-02-28)) chapter "5.9.10 Named objects and 
copying" which says "Currently all arguments to a .Call call will have 
NAMED set to 2, and so users must assume that they need to be duplicated 
before alteration." This is consistent with the observation of my test 
code: that NAMED() in .Call always returns 2. And that a .Call doing 
pure read access will trigger some delay most likely due to a full 
vector copy is a sign of .Call not only setting NAMED to 2 but also not 
resetting it once .Call terminates.

So what is needed to find NAMED(SEXP argument) < 2 during .Call?

Kind regards

Jens


From simon.urbanek at r-project.org  Mon Mar  3 21:50:37 2014
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Mon, 3 Mar 2014 15:50:37 -0500
Subject: [Rd] internal copying in R (soon to be released R-3.1.0
In-Reply-To: <5314E78F.4030804@truecluster.com>
References: <53136C77.2010603@truecluster.com>
	<E4137A27-9188-4932-891E-7451ACEDC7C2@r-project.org>
	<5314E78F.4030804@truecluster.com>
Message-ID: <F6070CCD-9C5B-4682-BE85-0F8DAD5F1C49@r-project.org>

Jens,

On Mar 3, 2014, at 3:35 PM, Jens Oehlschl?gel <jens.oehlschlaegel at truecluster.com> wrote:

> Thanks for answering Simon,
> 
> > None, there is no concept of "shared" memory at R level. You seem to be mixing C level API specifics and the R language. In the former duplicate() creates a new copy.
> 
> I take this as evidence that calling duplicate() is the only way to make sure I have a non-shared object.
> 

If NAMED > 0 then calling duplicate() is necessary to make sure you have a non-shared copy.


> > Assuming that you are talking about the C API, please consider reading about the concepts involved. .Call() doesn't set named to 2 at all - it passes whatever object is passed so it is the C code's responsibility to handle incoming objects according to the desired semantics (see the previous post here).
> 
> Well, I did read, for example "Writing R Extensions" (Version 3.1.0 Under development (2014-02-28)) chapter "5.9.10 Named objects and copying" which says "Currently all arguments to a .Call call will have NAMED set to 2, and so users must assume that they need to be duplicated before alteration."

Matthew pointed out that line and I cannot shed more light on it, since it's not true - at least not currently.


> This is consistent with the observation of my test code: that NAMED() in .Call always returns 2.

It is not - you're not testing .Call() - your'e testing the assignments in frames which cause additional bumps of NAMED. If you actually test .Call() you'll see what I have reported - .Call() itself does NOT affect NAMED.


> And that a .Call doing pure read access will trigger some delay most likely due to a full vector copy is a sign of .Call not only setting NAMED to 2 but also not resetting it once .Call terminates.
> 

Again, as I said earlier, you're on the wrong track here - .Call() doesn't touch it - it is left to the C code. Note that NAMED cannot be decremented (unless you use a ref counting version of R) once it reaches 2 since that means "two or more" so. The only time where you can decrement it is if you are the owner that set it from 0 to 1.

Cheers,
Simon


> So what is needed to find NAMED(SEXP argument) < 2 during .Call?
> 
> Kind regards
> 
> Jens
> 


From r.turner at auckland.ac.nz  Tue Mar  4 02:47:18 2014
From: r.turner at auckland.ac.nz (Rolf Turner)
Date: Tue, 04 Mar 2014 14:47:18 +1300
Subject: [Rd] Constructor/extractor.
Message-ID: <531530A6.8060307@auckland.ac.nz>


Preamble:
=========

In spatial point pattern analysis an important component of a point 
pattern object is the *observation window*.  In the spatstat package we 
have the function owin() to *construct* such a window.  It creates an 
object of class "owin" which can happily exist separately from any point 
pattern object (object of class "ppp").

However we have at the moment no convenient function to *extract* the 
observation component of a ppp object, nor have we a really convenient 
means of (re-) assigning the window component of such an object.  We can
extract the component via X$window or more suavely via as.owin(X) --- 
but the latter is rather counter-intuitive and lacks parallelism with 
other usage.  Re-assignment of the observation window component of X can
be done either via X$window <- W or X <- X[W].  These are not quite the 
same.  The former may leaves points which are outside of W hanging 
around. The latter does not suffer from this defect, and is more suave, 
but is not as intuitive as it might be.

I suggested that we could make the owin() function generic, make the 
current owin() creator function into owin.default(), and build an 
extractor function owin.ppp() (and an assignment function owin<-.ppp () 
to assign an observation window to a ppp object).

It was pointed out to me that this would make for an unorthodox syntax 
It is at the very least *unusual* if not unheard of for a function to be 
used both for *creation* of objects and for *extraction* of components 
from other objects.

Questions:
==========

My questions to R-devel are:

(1) Are there any other functions in R that serve this dual role of
constructor and extractor?

(2) Even if there are no such functions, is there anything intrinsically 
*wrong* with having a function possessing this somewhat schizophrenic 
nature?  Is it likely to cause confusion, induce syntactical mistakes, 
create errors, or produce wrong results?

Any thoughts, comments, insights or suggestions gratefully received.

cheers,

Rolf Turner


From friendly at yorku.ca  Tue Mar  4 16:50:07 2014
From: friendly at yorku.ca (Michael Friendly)
Date: Tue, 04 Mar 2014 10:50:07 -0500
Subject: [Rd] forking a CRAN project
Message-ID: <5315F62F.6080900@yorku.ca>

There is a CRAN package licensed just as "GPL", say, XX, I want to use 
in a book.
But I've needed to modify the package to make it do what I need for 
expository purposes.
The package author(s) are amenable to my modifications, but probably 
unlikely to
incorporate them into the CRAN version any time soon.

Am I allowed, under GPL, to create a new version of the package, say 
XX2, in a
public repository such as R-Forge or github?  I would, of course, 
maintain their
authorship, though perhaps take over the maintainer role.  For my 
purposes in
the book, I don't necessarily need to release my version to CRAN; just a 
public repo
a reader could download from.

-Michael

-- 
Michael Friendly     Email: friendly AT yorku DOT ca
Professor, Psychology Dept. & Chair, Quantitative Methods
York University      Voice: 416 736-2100 x66249 Fax: 416 736-5814
4700 Keele Street    Web:   http://www.datavis.ca
Toronto, ONT  M3J 1P3 CANADA


From b.rowlingson at lancaster.ac.uk  Tue Mar  4 17:08:24 2014
From: b.rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Tue, 4 Mar 2014 16:08:24 +0000
Subject: [Rd] Constructor/extractor.
In-Reply-To: <7f0b99dea7e442f7beaad57575f07606@EX-0-HT0.lancs.local>
References: <7f0b99dea7e442f7beaad57575f07606@EX-0-HT0.lancs.local>
Message-ID: <CANVKczNHE=fd4onLhdFQ=sON=EwFQHsdSQ9S=--cLRmTjS79FQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140304/fd7468cb/attachment.pl>

From pdalgd at gmail.com  Tue Mar  4 17:24:43 2014
From: pdalgd at gmail.com (peter dalgaard)
Date: Tue, 4 Mar 2014 17:24:43 +0100
Subject: [Rd] forking a CRAN project
In-Reply-To: <5315F62F.6080900@yorku.ca>
References: <5315F62F.6080900@yorku.ca>
Message-ID: <499FCB23-1B48-4055-BF30-03F8D1F7EA0C@gmail.com>

As usual: For legal advice, get a lawyer... However, yes, the GPL quite expressedly allows licencees to distribute modified versions, provided that the modified source code is made available and is redistributable under the GPL. 

It's common courtesy and possibly also required by law (the GPL itself doesn't say anything about it) to ensure that the original statements of authorship and copyright are retained. 

One thing to be slightly cautious about is to avoid inadvertent copyright transfer to your publisher of things that are also in the package. The back pages of ISwR (the book) has copies of the help pages for ISwR (the package), but "Reproduced with permission" so that Springer doesn't have the copyright for that part of the book. Not that they are likely to care, but better safe than sorry.

-Peter D


On 04 Mar 2014, at 16:50 , Michael Friendly <friendly at yorku.ca> wrote:

> There is a CRAN package licensed just as "GPL", say, XX, I want to use in a book.
> But I've needed to modify the package to make it do what I need for expository purposes.
> The package author(s) are amenable to my modifications, but probably unlikely to
> incorporate them into the CRAN version any time soon.
> 
> Am I allowed, under GPL, to create a new version of the package, say XX2, in a
> public repository such as R-Forge or github?  I would, of course, maintain their
> authorship, though perhaps take over the maintainer role.  For my purposes in
> the book, I don't necessarily need to release my version to CRAN; just a public repo
> a reader could download from.
> 
> -Michael
> 
> -- 
> Michael Friendly     Email: friendly AT yorku DOT ca
> Professor, Psychology Dept. & Chair, Quantitative Methods
> York University      Voice: 416 736-2100 x66249 Fax: 416 736-5814
> 4700 Keele Street    Web:   http://www.datavis.ca
> Toronto, ONT  M3J 1P3 CANADA
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
Peter Dalgaard, Professor
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From skostysh at princeton.edu  Tue Mar  4 22:45:22 2014
From: skostysh at princeton.edu (Scott Kostyshak)
Date: Tue, 4 Mar 2014 16:45:22 -0500
Subject: [Rd] [PATCH] suggestions for R-lang manual
In-Reply-To: <21268.31271.777129.441646@stat.math.ethz.ch>
References: <CAE3=dmefQSwG-AELeZmZFHi90VaknXUtEgfsVQgxii7J0Z75cg@mail.gmail.com>
	<CAE3=dmcKmmaQtSwx41i_PUfeC3o1z7rRTUWSBHfK64T-bpjbaQ@mail.gmail.com>
	<21268.31271.777129.441646@stat.math.ethz.ch>
Message-ID: <CAE3=dmcYGDvPhHpbLDWbXysCEYaJsrKNjU8mjXr0Oxm0iw1npA@mail.gmail.com>

On Mon, Mar 3, 2014 at 7:48 AM, Martin Maechler
<maechler at stat.math.ethz.ch> wrote:
>>>>>> Scott Kostyshak <skostysh at princeton.edu>
>>>>>>     on Thu, 27 Feb 2014 16:43:02 -0500 writes:
>
>     > On Thu, Nov 21, 2013 at 1:17 AM, Scott Kostyshak <skostysh at princeton.edu> wrote:
>     >> Attached is a patch with suggestions for the R-lang manual at r64277.
>     >>
>     >> Below are a few comments (some are implemented in the patch):
>     >>
>     >> In the section "Objects", there is a table introduced by "The
>     >> following table describes the possible values returned by typeof". One
>     >> of the results is "any". Can "any" be returned by "typeof()" ?
>
> ANYSXP  is a valid internal type on the C level, and
> src/main/util.c  will make  typeof(ob) return "any"
> if you can get your hands at an R level object of that type.
> I'd guess you can only get it currently by using .Call() and
> using your own C code, .. but at least that way it must be possible.

Interesting to know.

>     >> Regarding the "Recycling rules" section,
>     >>
>     >> -One exception is that when adding vectors to matrices, a warning is not
>     >> -given if the lengths are incompatible.
>     >> - at c Is that a bug?
>     >> -
>     >>
>     >> was this a bug that was fixed?
>
> I did not investigate in details, but yes, I vaguely remember we
> had fixed that.  So indeed, it's fine you omitted the para in
> your patch.
>
>     >> I see the following behavior:
>     >>
>     >>> myvec <- 1:3
>     >>> mymat <- matrix(1:12, ncol=2)
>     >>> myvec <- 1:5
>     >>> myvec + mymat
>     >> [,1] [,2]
>     >> [1,]    2    9
>     >> [2,]    4   11
>     >> [3,]    6   13
>     >> [4,]    8   15
>     >> [5,]   10   12
>     >> [6,]    7   14
>     >> Warning message:
>     >> In myvec + mymat :
>     >> longer object length is not a multiple of shorter object length
>     >>>
>     >>
>     >> Regarding
>     >>
>     >> -The arguments in the call to the generic are rematched with the
>     >> -arguments for the method using the standard argument matching mechanism.
>     >> -The first argument, i.e.@: the object, will have been evaluated.
>     >> -
>     >>
>     >> this information is duplicated. See a few paragraphs up "When the
>     >> method is invoked it is called..."
>
>     >> Scott
>
> Thank you, Scott.
> Indeed, I've finally carefully looked at the patch, and applied
> it - for R-devel, to become R 3.1.0 in April.

Thanks, Martin!

Scott


--
Scott Kostyshak
Economics PhD Candidate
Princeton University

> Martin
>
>
>     >> --
>     >> Scott Kostyshak
>     >> Economics PhD Candidate
>     >> Princeton University
>
>     > The patch still applies cleanly (one offset) on r65090.
>
>     > Best,
>     > Scott


From Zwang at connecticutchildrens.org  Tue Mar  4 16:21:12 2014
From: Zwang at connecticutchildrens.org (Wang, Zhu)
Date: Tue, 4 Mar 2014 15:21:12 +0000
Subject: [Rd] How to import S3 method
Message-ID: <DABBA49CC8C7AA4B959D2D0FEE496D721C4E1715@EXINFRCHMB1P.ccmc.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140304/d2cc5498/attachment.pl>

From aleemiubdr at gmail.com  Tue Mar  4 11:48:32 2014
From: aleemiubdr at gmail.com (Dr M Aleem)
Date: Tue, 4 Mar 2014 10:48:32 +0000
Subject: [Rd] [GSoC student proposal] Implementation of Modified Weibull-G
 Family Distribution in R
Message-ID: <CANtRVCC6vRb8iXaJHQodHqR3Oq2xyrOrcKGmo+=q5LLq7NWfJA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140304/c4a9c1a2/attachment.pl>

From michael.weylandt at gmail.com  Wed Mar  5 13:57:13 2014
From: michael.weylandt at gmail.com (Michael Weylandt)
Date: Wed, 5 Mar 2014 07:57:13 -0500
Subject: [Rd] How to import S3 method
In-Reply-To: <DABBA49CC8C7AA4B959D2D0FEE496D721C4E1715@EXINFRCHMB1P.ccmc.local>
References: <DABBA49CC8C7AA4B959D2D0FEE496D721C4E1715@EXINFRCHMB1P.ccmc.local>
Message-ID: <7F51B343-E1E8-4AA7-84CF-08621B06F27F@gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140305/d02dd2da/attachment.pl>

From karl.forner at gmail.com  Wed Mar  5 17:16:57 2014
From: karl.forner at gmail.com (Karl Forner)
Date: Wed, 5 Mar 2014 17:16:57 +0100
Subject: [Rd] [PATCH] Code coverage support proof of concept
Message-ID: <CAMd4_Af5uM9JQYEOKgEajmQvOaoWAgx9UiukjCrBy2PJKPDW8A@mail.gmail.com>

Hello,

I submit a patch for review that implements code coverage tracing in
the R interpreter.
It records the lines that are actually executed and their associated
frequency for which srcref information is available.

I perfectly understands that this patch will not make its way inside R
as it is, that they are many concerns of stability, compatibility,
maintenance and so on.
I would like to have the code reviewed, and proper guidance on how to
get this feature available at one point in R, in base R or as a
package or patch if other people are interested.

Usage
--------
Rcov_start()
# your code to trace here
res <- Rcov_stop()

res is currently a hashed env, with traced source filenames associated
with 2-columns matrices holding the line numbers and their
frequencies.


How it works
-----------------
I added a test in getSrcref(), that records the line numbers if code
coverage is started.
The overhead should be minimal since for a given file, subsequent
covered lines will be stored
in constant time. I use a hased env to store the occurrences by file.

I added two entry points in the utils package (Rcov_start() and Rcov_stop())


Example
-------------
* untar the latest R-devel and cd into it
* patch -p1 < rdev-cov-patch.txt
* ./configure [... ] && make && [sudo] make install
* install the devtools package
* run the following script using Rscript

library(methods)
library(devtools)
pkg  <- download.packages('testthat', '.', repos = "http://stat.ethz.ch/CRAN")
untar(pkg[1, 2])

Rcov_start()
test('testthat')
env <- Rcov_stop()

res <- lapply(ls(env), get, envir = env)
names(res) <- ls(env)
print(res)


This will hopefully output something like:
$`.../testthat/R/auto-test.r`
     [,1] [,2]
[1,]   33    1
[2,]   80    1

$`.../testthat/R/colour-text.r`
      [,1] [,2]
 [1,]   18    1
 [2,]   19  106
 [3,]   20  106
 [4,]   22  106
 [5,]   23  106
 [6,]   40    1
 [7,]   59    1
 [8,]   70    1
 [9,]   71  106
...


Karl Forner


Disclaimer
-------------
There are probably bugs  and ugly statements, but this is just a proof
of concept. This is untested and only run on a linux x86_64
-------------- next part --------------
diff -ruN R-devel/src/library/utils/man/Rcov_start.Rd R-devel-cov/src/library/utils/man/Rcov_start.Rd
--- R-devel/src/library/utils/man/Rcov_start.Rd	1970-01-01 01:00:00.000000000 +0100
+++ R-devel-cov/src/library/utils/man/Rcov_start.Rd	2014-03-05 16:07:45.907596276 +0100
@@ -0,0 +1,26 @@
+% File src/library/utils/man/Rcov_start.Rd
+% Part of the R package, http://www.R-project.org
+% Copyright 1995-2010 R Core Team
+% Distributed under GPL 2 or later
+
+\name{Rcov_start}
+\alias{Rcov_start}
+\title{Start Code Coverage analysis of R's Execution}
+\description{
+  Start Code Coverage analysis of the execution of \R expressions.
+}
+\usage{
+Rcov_start(nb_lines = 10000L, growth_rate = 2)
+}
+\arguments{
+  \item{nb_lines}{
+    Initial max number of lines per source file. 
+  }
+  \item{growth_rate}{
+    growth factor of the line numbers vectors per filename. 
+    If a reached line number L is greater than  nb_lines, the vector will
+    be reallocated with provisional size of growth_rate * L. 
+  }
+}
+
+\keyword{utilities}
diff -ruN R-devel/src/library/utils/man/Rcov_stop.Rd R-devel-cov/src/library/utils/man/Rcov_stop.Rd
--- R-devel/src/library/utils/man/Rcov_stop.Rd	1970-01-01 01:00:00.000000000 +0100
+++ R-devel-cov/src/library/utils/man/Rcov_stop.Rd	2014-03-03 16:14:25.883440716 +0100
@@ -0,0 +1,20 @@
+% File src/library/utils/man/Rcov_stop.Rd
+% Part of the R package, http://www.R-project.org
+% Copyright 1995-2010 R Core Team
+% Distributed under GPL 2 or later
+
+\name{Rcov_stop}
+\alias{Rcov_stop}
+\title{Start Code Coverage analysis of R's Execution}
+\description{
+  Start Code Coverage analysis of the execution of \R expressions.
+}
+\usage{
+Rcov_stop()
+}
+
+\value{
+  a named list of integer vectors holding occurrences counts (line number, frequency)
+  , named after the covered source file names. 
+}
+\keyword{utilities}
diff -ruN R-devel/src/library/utils/NAMESPACE R-devel-cov/src/library/utils/NAMESPACE
--- R-devel/src/library/utils/NAMESPACE	2013-09-10 03:04:59.000000000 +0200
+++ R-devel-cov/src/library/utils/NAMESPACE	2014-03-03 16:18:48.407430952 +0100
@@ -1,7 +1,7 @@
 # Refer to all C routines by their name prefixed by C_
 useDynLib(utils, .registration = TRUE, .fixes = "C_")
 
-export("?", .DollarNames, CRAN.packages, Rprof, Rprofmem, RShowDoc,
+export("?", .DollarNames, CRAN.packages, Rcov_start, Rcov_stop, Rprof, Rprofmem, RShowDoc,
        RSiteSearch, URLdecode, URLencode, View, adist, alarm, apropos,
        aregexec, argsAnywhere, assignInMyNamespace, assignInNamespace,
        as.roman, as.person, as.personList, as.relistable, aspell,
diff -ruN R-devel/src/library/utils/R/Rcov.R R-devel-cov/src/library/utils/R/Rcov.R
--- R-devel/src/library/utils/R/Rcov.R	1970-01-01 01:00:00.000000000 +0100
+++ R-devel-cov/src/library/utils/R/Rcov.R	2014-03-03 16:08:45.739453368 +0100
@@ -0,0 +1,27 @@
+#  File src/library/utils/R/Rcov.R
+#  Part of the R package, http://www.R-project.org
+#
+#  Copyright (C) 1995-2013 The R Core Team
+#
+#  This program is free software; you can redistribute it and/or modify
+#  it under the terms of the GNU General Public License as published by
+#  the Free Software Foundation; either version 2 of the License, or
+#  (at your option) any later version.
+#
+#  This program is distributed in the hope that it will be useful,
+#  but WITHOUT ANY WARRANTY; without even the implied warranty of
+#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+#  GNU General Public License for more details.
+#
+#  A copy of the GNU General Public License is available at
+#  http://www.r-project.org/Licenses/
+
+Rcov_start <- function(nb_lines = 10000L, growth_rate = 2)
+{
+    invisible(.External(C_Rcov_start, nb_lines, growth_rate))
+}
+
+
+Rcov_stop <- function() {
+	invisible(.External(C_Rcov_stop))
+}
diff -ruN R-devel/src/library/utils/src/init.c R-devel-cov/src/library/utils/src/init.c
--- R-devel/src/library/utils/src/init.c	2014-01-08 18:06:33.000000000 +0100
+++ R-devel-cov/src/library/utils/src/init.c	2014-03-03 16:16:00.667437192 +0100
@@ -74,6 +74,8 @@
 static const R_ExternalMethodDef ExtEntries[] = {
     EXTDEF(download, 5),
     EXTDEF(unzip, 7),
+    EXTDEF(Rcov_start, 2),
+    EXTDEF(Rcov_stop, 0),
     EXTDEF(Rprof, 8),
     EXTDEF(Rprofmem, 3),
 
diff -ruN R-devel/src/library/utils/src/utils.c R-devel-cov/src/library/utils/src/utils.c
--- R-devel/src/library/utils/src/utils.c	2012-10-01 17:52:17.000000000 +0200
+++ R-devel-cov/src/library/utils/src/utils.c	2014-03-03 16:24:06.259419131 +0100
@@ -27,6 +27,22 @@
 #include "utils.h"
 
 /* from src/main/eval.c */
+void do_Rcov_start(int nb_lines, double growth_rate);
+SEXP do_Rcov_stop(void);
+
+SEXP Rcov_start(SEXP args)
+{
+    do_Rcov_start(asInteger(CADR(args)), asReal(CADDR(args)));
+    return R_NilValue;		/* -Wall */
+}
+
+
+SEXP Rcov_stop(void)
+{
+    return do_Rcov_stop();
+}
+
+/* from src/main/eval.c */
 SEXP do_Rprof(SEXP args);
 
 SEXP Rprof(SEXP args)
diff -ruN R-devel/src/library/utils/src/utils.h R-devel-cov/src/library/utils/src/utils.h
--- R-devel/src/library/utils/src/utils.h	2014-01-06 03:04:59.000000000 +0100
+++ R-devel-cov/src/library/utils/src/utils.h	2014-03-03 16:23:54.355419573 +0100
@@ -26,6 +26,8 @@
 
 SEXP objectSize(SEXP s);
 SEXP unzip(SEXP args);
+SEXP Rcov_start(SEXP args);
+SEXP Rcov_stop(void);
 SEXP Rprof(SEXP args);
 SEXP Rprofmem(SEXP args);
 
diff -ruN R-devel/src/main/eval.c R-devel-cov/src/main/eval.c
--- R-devel/src/main/eval.c	2014-02-21 03:03:36.000000000 +0100
+++ R-devel-cov/src/main/eval.c	2014-03-05 16:32:22.427541357 +0100
@@ -37,6 +37,201 @@
 
 static SEXP bcEval(SEXP, SEXP, Rboolean);
 
+
+static int R_Code_Coverage = 0;
+#define R_CODE_COVERAGE
+#ifdef  R_CODE_COVERAGE
+
+/* A Simple mechanism for implementing code coverage.
+  When code coverage is enables (via the R_Code_Coverage global var),
+  each call to the getSrcref() function will record the current srcref filename and line
+  number.
+  The code coverage support is controlled by the R_CODE_COVERAGE preprocessor define.
+
+  The actual implementation consists for the moment in intercepting getSrcref() calls,
+  then calling the record_code_coverage() function.
+  The code coverage tracing is activating by calling the do_Rcov() (Rcov from R) function.
+
+  Karl Forner
+ */
+
+/*   global variable: hit lines freqs: a HashedEnv by filename */
+static SEXP R_Cov_freqs_hash = NULL;
+
+/* create a new non-sparsed vector of line frequencies at least of length size.
+ * Depending on the do_Rcov_start params nb_lines and growth_rate,
+ * it will allocate an actual size of either nb_lines or size * growth_rate
+ */
+static SEXP cov_new_lines_vector(int size) {
+	SEXP sexp, lines;
+	int nb_lines, i;
+	int *tab;
+	double growth_rate;
+
+	sexp = findVarInFrame(R_Cov_freqs_hash, install(".nb_lines"));
+	nb_lines = INTEGER(sexp)[0];
+	if (size > nb_lines) {
+		sexp = findVarInFrame(R_Cov_freqs_hash, install(".growth_rate"));
+		growth_rate = REAL(sexp)[0];
+		size = (int)(size * growth_rate);
+	} else {
+		size = nb_lines;
+	}
+
+	PROTECT(lines = allocVector(INTSXP, size));
+	tab = INTEGER(lines);
+	for (i = 0; i < size; ++i)
+		tab[i] = 0;
+	UNPROTECT(1);
+	return lines;
+}
+
+/* store a new line occurrence in R_Cov_freqs_hash for filename */
+static void cov_store_new_line(const char* filename, int line) {
+	SEXP lines, lines2;
+	int len, i, *t1, *t2;
+
+	lines = findVarInFrame(R_Cov_freqs_hash, install(filename));
+	if (lines == R_UnboundValue) { /* new file */
+		lines = cov_new_lines_vector(line + 1);
+		defineVar(install(filename), lines, R_Cov_freqs_hash);
+	}
+	if (length(lines) <= line) {
+		/* lines vector too short */
+		PROTECT(lines2 = cov_new_lines_vector(line + 1)); /* should allocate (line+1)*growth_rate */
+		len = length(lines);
+		i = 0;
+		t1 = INTEGER(lines);
+		t2 = INTEGER(lines2);
+		for (i = 0; i < len; ++i)
+			lines2[i] = lines[i];
+		defineVar(install(filename), lines2, R_Cov_freqs_hash);
+		lines = lines2;
+		UNPROTECT(1);
+	}
+
+	INTEGER(lines)[line]++;
+}
+
+/* maybe store a new srcref in R_Cov_freqs_hash */
+static void record_code_coverage(SEXP srcref)
+{
+	if (srcref && !isNull(srcref)) {
+		int fnum, line = asInteger(srcref);
+
+		SEXP srcfile = getAttrib(srcref, R_SrcfileSymbol);
+		const char *filename;
+
+		if (!srcfile || TYPEOF(srcfile) != ENVSXP) return;
+		srcfile = findVar(install("filename"), srcfile);
+		if (TYPEOF(srcfile) != STRSXP || !length(srcfile)) return;
+
+		filename = CHAR(STRING_ELT(srcfile, 0));
+		cov_store_new_line(filename, line);
+	}
+}
+
+
+/* This initiates the code coverage tracing.
+ * nb_lines is the initial size of frequencies vectors per file.
+ * If a line number L is encountered s.t L >=nb_lines, the vector will be extended
+ * to L * growth_rate
+ */
+void do_Rcov_start(int nb_lines, double growth_rate)
+{
+	SEXP sexp;
+
+	if (growth_rate < 1.1)
+		growth_rate = 1.1;
+
+	if (R_Code_Coverage) return;
+	R_Code_Coverage = 1;
+	if (R_Cov_freqs_hash != NULL)
+		R_ReleaseObject(R_Cov_freqs_hash);
+
+	/* put the params nb_lines and growth_rate as hidden vars of the hashed env */
+	R_Cov_freqs_hash = R_NewHashedEnv(R_NilValue, ScalarInteger(0));
+	PROTECT(sexp = ScalarInteger(nb_lines));
+	defineVar(install(".nb_lines"), sexp, R_Cov_freqs_hash);
+
+	PROTECT(sexp = ScalarReal(growth_rate));
+	defineVar(install(".growth_rate"), sexp, R_Cov_freqs_hash);
+
+	R_PreserveObject(R_Cov_freqs_hash);
+}
+
+/* Ends the code coverage tracing.
+ * and returns an environment with symbols named after the covered source files and values
+ * matrices of dim n*2, which first column is the line number and the second the nb of occurrences
+ */
+SEXP do_Rcov_stop(void)
+{
+	SEXP names, lines, mat, key, res;
+	int n, i, j, k, nb_lines, non_empty_lines, *tab, *m;
+
+	/* stop the code covered tracing */
+	R_Code_Coverage = 0;
+
+	/* convert frequencies by line to matrix N*2 of lines, freq */
+	PROTECT(names = R_lsInternal(R_Cov_freqs_hash, FALSE));
+	n = length(names);
+
+	for (i = 0; i < n; ++i) {
+		key = install(CHAR(STRING_ELT(names, i)));
+		lines = findVarInFrame(R_Cov_freqs_hash, key);
+
+		tab = INTEGER(lines);
+		nb_lines = length(lines);
+		non_empty_lines = 0;
+		for (j = 0; j < nb_lines; ++j)
+			if (tab[j])
+				++non_empty_lines;
+
+		PROTECT(mat = allocMatrix(INTSXP, non_empty_lines, 2));
+		m = INTEGER(mat);
+		k = 0;
+		for (j = 0; j < nb_lines; ++j) {
+			if (tab[j]) {
+				m[k] = j;
+				m[k + non_empty_lines] = tab[j];
+				++k;
+			}
+		}
+
+		defineVar(key, mat, R_Cov_freqs_hash);
+		UNPROTECT(1); /* mat */
+	}
+	UNPROTECT(1); /* names */
+
+	res = R_Cov_freqs_hash;
+	R_ReleaseObject(R_Cov_freqs_hash);
+	R_Cov_freqs_hash = NULL;
+
+    return res;
+}
+
+
+#else /* not R_CODE_COVERAGE */
+
+void do_Rcov_start(int nb_lines, int growth_rate)
+{
+    error(_("do_Rcov_start: R code coverage is not available on this system"));
+    return R_NilValue;		/* -Wall */
+}
+
+SEXP do_Rcov_stop()
+{
+    error(_("do_Rcov_stop: R code coverage is not available on this system"));
+	R_Code_Coverage = 0;
+}
+
+
+#endif
+
+
+
+
+
 /* BC_PROILFING needs to be defined here and in registration.c */
 /*#define BC_PROFILING*/
 #ifdef BC_PROFILING
@@ -851,10 +1046,17 @@
 	&& length(srcrefs) > ind
 	&& !isNull(result = VECTOR_ELT(srcrefs, ind))
 	&& TYPEOF(result) == INTSXP
-	&& length(result) >= 6)
-	return result;
-    else
-	return R_NilValue;
+	&& length(result) >= 6) {
+
+#ifdef R_CODE_COVERAGE
+    	if (R_Code_Coverage) record_code_coverage(result);
+#endif
+
+    } else {
+    	result = R_NilValue;
+    }
+
+    return result;
 }
 
 SEXP applyClosure(SEXP call, SEXP op, SEXP arglist, SEXP rho, SEXP suppliedenv)

From mtmorgan at fhcrc.org  Wed Mar  5 17:38:56 2014
From: mtmorgan at fhcrc.org (Martin Morgan)
Date: Wed, 05 Mar 2014 08:38:56 -0800
Subject: [Rd] Sweave provides a misleading error when vignette engine not
 fully specified
Message-ID: <53175320.8060304@fhcrc.org>

Trying to Stangle / Sweave a file

$ cat vignette.Rnw
%\VignetteEngine{knitr}
\documentclass{article}
\begin{document}
\end{document}

results in a misleading error message:

~/tmp$ R CMD Stangle vignette.Rnw
Error: Vignette engine package not specified
Execution halted

when what is missing is the full specification knitr::knitr; 'vignette engine 
package and function not specified' ? Also it's somehow unfortunate that the 
vignette builds when in a package/vignettes directory, but not as a stand-alone 
document.

Also for what its worth Sweave'ing still fails to produce graphics output

https://stat.ethz.ch/pipermail/r-devel/2014-February/068414.html

$ R --version|head -n 3
R Under development (unstable) (2014-03-05 r65124) -- "Unsuffered Consequences"
Copyright (C) 2014 The R Foundation for Statistical Computing
Platform: x86_64-unknown-linux-gnu (64-bit)

Martin
-- 
Computational Biology / Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N.
PO Box 19024 Seattle, WA 98109

Location: Arnold Building M1 B861
Phone: (206) 667-2793


From murdoch.duncan at gmail.com  Wed Mar  5 20:31:00 2014
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Wed, 05 Mar 2014 14:31:00 -0500
Subject: [Rd] Sweave provides a misleading error when vignette engine
 not fully specified
In-Reply-To: <53175320.8060304@fhcrc.org>
References: <53175320.8060304@fhcrc.org>
Message-ID: <53177B74.4090207@gmail.com>

On 05/03/2014 11:38 AM, Martin Morgan wrote:
> Trying to Stangle / Sweave a file
>
> $ cat vignette.Rnw
> %\VignetteEngine{knitr}
> \documentclass{article}
> \begin{document}
> \end{document}
>
> results in a misleading error message:
>
> ~/tmp$ R CMD Stangle vignette.Rnw
> Error: Vignette engine package not specified
> Execution halted

I don't see that as misleading.  The code can't figure out where the 
knitr engine comes from, and it's telling you that.
>
> when what is missing is the full specification knitr::knitr; 'vignette engine
> package and function not specified' ? Also it's somehow unfortunate that the
> vignette builds when in a package/vignettes directory, but not as a stand-alone
> document.
A package can declare the vignette building package in its DESCRIPTION 
file.  If it's a standalone Sweave document, it needs to be declared in 
the document itself.  (It's also possible to declare it on the Stangle 
command line, but you'll still need the fully qualified name, e.g.

R CMD Stangle --engine=knitr::knitr vignette.Rnw

It might be possible to set up R to automatically load knitr first (e.g. 
by putting it in your list of R_DEFAULT_PACKAGES); I haven't tried 
that.  But somehow you need to tell R where the knitr engine lives.


> Also for what its worth Sweave'ing still fails to produce graphics output

Now that sounds unrelated.

Duncan Murdoch
>
> https://stat.ethz.ch/pipermail/r-devel/2014-February/068414.html
>
> $ R --version|head -n 3
> R Under development (unstable) (2014-03-05 r65124) -- "Unsuffered Consequences"
> Copyright (C) 2014 The R Foundation for Statistical Computing
> Platform: x86_64-unknown-linux-gnu (64-bit)
>
> Martin


From therneau at mayo.edu  Wed Mar  5 22:52:32 2014
From: therneau at mayo.edu (Therneau, Terry M., Ph.D.)
Date: Wed, 05 Mar 2014 15:52:32 -0600
Subject: [Rd] forking a CRAN project
In-Reply-To: <mailman.19.1394017205.14480.r-devel@r-project.org>
References: <mailman.19.1394017205.14480.r-devel@r-project.org>
Message-ID: <6e55ab$8fqval@ironport10.mayo.edu>

Micheal,
   Keep in mind that the author of the package can give you a copy under any conditions 
that they choose, they are not bound by the GPL.  Simply send you code from their own 
source which is "upstream" of the GPL copy. (The copy out on CRAN propogates strictly via 
GPL however).  You could for instance start a new package de novo.
   This may or may not make sense in your case, but I wanted to point out the flexibility.

   A version of rpart has twice been bundled into commercial products via this route, 
where the user didn't want to be under the GPL.  None in the last 8-10 years; as Brian 
Ripley's and my contributions to rpart are now far too comingled to define a non-gpl code 
base.  I'll also note that in both those cases my employer took the quite defensible 
position that contributions to a for profit company are not free of charge.

On 03/05/2014 05:00 AM, r-devel-request at r-project.org wrote:
> There is a CRAN package licensed just as "GPL", say, XX, I want to use
> in a book.
> But I've needed to modify the package to make it do what I need for
> expository purposes.
> The package author(s) are amenable to my modifications, but probably
> unlikely to
> incorporate them into the CRAN version any time soon.
>
> Am I allowed, under GPL, to create a new version of the package, say
> XX2, in a
> public repository such as R-Forge or github?  I would, of course,
> maintain their
> authorship, though perhaps take over the maintainer role.  For my
> purposes in
> the book, I don't necessarily need to release my version to CRAN; just a
> public repo
> a reader could download from.
>
> -Michael


From hpages at fhcrc.org  Thu Mar  6 00:18:45 2014
From: hpages at fhcrc.org (=?ISO-8859-1?Q?Herv=E9_Pag=E8s?=)
Date: Wed, 05 Mar 2014 15:18:45 -0800
Subject: [Rd] Constructor/extractor.
In-Reply-To: <CANVKczNHE=fd4onLhdFQ=sON=EwFQHsdSQ9S=--cLRmTjS79FQ@mail.gmail.com>
References: <7f0b99dea7e442f7beaad57575f07606@EX-0-HT0.lancs.local>
	<CANVKczNHE=fd4onLhdFQ=sON=EwFQHsdSQ9S=--cLRmTjS79FQ@mail.gmail.com>
Message-ID: <5317B0D5.9010707@fhcrc.org>

Hi Rolf, Barry,

On 03/04/2014 08:08 AM, Barry Rowlingson wrote:
> On Tue, Mar 4, 2014 at 1:47 AM, Rolf Turner <r.turner at auckland.ac.nz> wrote:
>
>>
>>
>> Questions:
>> ==========
>>
>>
>> (2) Even if there are no such functions, is there anything intrinsically
>> *wrong* with having a function possessing this somewhat schizophrenic
>> nature?  Is it likely to cause confusion, induce syntactical mistakes,
>> create errors, or produce wrong results?
>>
>> Any thoughts, comments, insights or suggestions gratefully received.
>>
>
>
>   I don't see why you can't conceptually think of w = owin(some_ppp_object)
> as an owin "Constructor" rather than an "Accessor". Its "constructing" (and
> returning) an owin from an object, it just happens to be as simple as
> getting a component from that object.
>
>   The raster package has the 'extent' function - you can create an extent
> with extent(xmin,xmax,,,etc), get the extent of a raster with extent(r), or
> set the extent of a raster with extent(r1) <- extent(r2), so I don't see
> any problem with:
>
> w1 = owin(poly=...) # construct polygon owin
>
> ppp1 = ppp(x,y,window=w1)
> ppp2 = ppp(x,y, window=owin(ppp1)) # construct window from ppp object
>
> owin(ppp1) = owin(ppp2)  # give ppp1 the owin of ppp2
>
> That all reads pretty nicely. As long as owin(...) returns an observation
> window and owin<-(...) assigns an observation window to an object that can
> validly have one, I don't think you can go wrong. I've probably already
> tried to do "owin(ppp1)=..." a few times...

I agree with Barry. Once you realize there is no clear line between
constructor and extractor (conceptually every function can actually
be thought of as a constructor), then re-using the same function name
(via methods of a generic function) doesn't sound too bad. An example
of this design is the strand() generic in Bioconductor: can be used
to get or set the strand factor of an object containing such a
component (e.g. a GRanges object), and also for constructing a
strand factor from different kinds of input (character, logical,
integer vector, etc...)

Cheers,
H.

>
> Barry
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fhcrc.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From nilsson.henric at gmail.com  Thu Mar  6 11:17:36 2014
From: nilsson.henric at gmail.com (Henric Winell)
Date: Thu, 06 Mar 2014 11:17:36 +0100
Subject: [Rd] 'parallel' package changes '.Random.seed'
Message-ID: <53184B40.2050604@gmail.com>

Hi,

I've implemented parallelization in one of my packages using the 
'parallel' package -- many thanks for providing it!

In my package I'm importing 'parallel' and so added it to the 
DESCRIPTION file's 'Import:' tag and also added a 
'importFrom("parallel", ...)' statement in the NAMESPACE file.

Parallelization works nicely, but my package no longer passes any parts 
of its (unparallelized) checks that depends on random number generation, 
e.g., the simulated data in the check suite are no longer the same as 
before parallelization was added.  This seems to be due to 'parallel' 
changing '.Random.seed' when loading its name space:

 > set.seed(1)
 > rs1 <- .Random.seed
 > rnorm(1)
[1] -0.6264538
 > set.seed(1)
 > rs2 <- .Random.seed
 > identical(rs1, rs2)
[1] TRUE
 > loadNamespace("parallel")
<environment: namespace:parallel>
 > rs3 <- .Random.seed
 > identical(rs1, rs3)
[1] FALSE
 > rnorm(1)
[1] -0.3262334
 > set.seed(1)
 > rs4 <- .Random.seed
 > identical(rs1, rs4)
[1] TRUE

I've taken a look at the 'parallel' source code, and in a few places a 
call to 'runif(1)' is issued.  So, what effectively seems to happen when 
'parallel' is loaded is

 > set.seed(1)
 > runif(1)
[1] 0.2655087
 > rnorm(1)
[1] -0.3262334

which reproduces the above.  But is this really necessary?  And more 
importantly (at least to me):  Can it somehow be avoided?

The current state of affairs is a bit unfortunate, since it implies that 
a user just by loading the new parallelized version of my package can no 
longer reproduce any subsequent results depending on random number 
generation (unless a call to 'set.seed' was issued *after* attaching my 
package).

I'd be most grateful for any help that you're able to provide here. 
Many thanks!

Kind regards,
Henric Winell


> sessionInfo()
R Under development (unstable) (2014-01-26 r64897)
Platform: x86_64-redhat-linux-gnu (64-bit)

locale:
  [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
  [3] LC_TIME=sv_SE.UTF-8        LC_COLLATE=en_US.UTF-8
  [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8
  [7] LC_PAPER=en_US.UTF-8       LC_NAME=C
  [9] LC_ADDRESS=C               LC_TELEPHONE=C
[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base

loaded via a namespace (and not attached):
[1] compiler_3.1.0 parallel_3.1.0 tools_3.1.0


From nilsson.henric at gmail.com  Thu Mar  6 12:54:25 2014
From: nilsson.henric at gmail.com (Henric Winell)
Date: Thu, 06 Mar 2014 12:54:25 +0100
Subject: [Rd] 'parallel' package changes '.Random.seed'
In-Reply-To: <53184B40.2050604@gmail.com>
References: <53184B40.2050604@gmail.com>
Message-ID: <531861F1.404@gmail.com>

Comments below.

On 2014-03-06 11:17, Henric Winell wrote:
> Hi,
>
> I've implemented parallelization in one of my packages using the
> 'parallel' package -- many thanks for providing it!
>
> In my package I'm importing 'parallel' and so added it to the
> DESCRIPTION file's 'Import:' tag and also added a
> 'importFrom("parallel", ...)' statement in the NAMESPACE file.
>
> Parallelization works nicely, but my package no longer passes any parts
> of its (unparallelized) checks that depends on random number generation,
> e.g., the simulated data in the check suite are no longer the same as
> before parallelization was added.  This seems to be due to 'parallel'
> changing '.Random.seed' when loading its name space:
>
>  > set.seed(1)
>  > rs1 <- .Random.seed
>  > rnorm(1)
> [1] -0.6264538
>  > set.seed(1)
>  > rs2 <- .Random.seed
>  > identical(rs1, rs2)
> [1] TRUE
>  > loadNamespace("parallel")
> <environment: namespace:parallel>
>  > rs3 <- .Random.seed
>  > identical(rs1, rs3)
> [1] FALSE
>  > rnorm(1)
> [1] -0.3262334
>  > set.seed(1)
>  > rs4 <- .Random.seed
>  > identical(rs1, rs4)
> [1] TRUE
>
> I've taken a look at the 'parallel' source code, and in a few places a
> call to 'runif(1)' is issued.  So, what effectively seems to happen when
> 'parallel' is loaded is
>
>  > set.seed(1)
>  > runif(1)
> [1] 0.2655087
>  > rnorm(1)
> [1] -0.3262334

Some digging reveals that this is due to no port number for the socket 
connection being set by default, in which case 'parallel' picks a random 
port in the 11000-11999 range using 'runif(1L)'.  So, by setting 
R_PARALLEL_PORT the '.Random.seed' object is no longer touched:

 > Sys.setenv(R_PARALLEL_PORT = 11500)
 > set.seed(1)
 > rs1 <- .Random.seed
 > loadNamespace("parallel")
<environment: namespace:parallel>
 > rs2 <- .Random.seed
 > identical(rs1, rs2)
[1] TRUE

This is handled in the 'initDefaultClusterOptions' function in 'snow.R', 
where line 88 has

port <- 11000 + 1000 * ((stats::runif(1L) + unclass(Sys.time())/300)%%1)

It seems to me that we can tread more carefully here.  I've attached a 
trivial patch that

1. Checks if '.Random.seed' exists
2. If TRUE:  a) save '.Random.seed'
              b) make the call above
              c) reset '.Random.seed' to its state in a)
    If FALSE: a) make the call above
              b) remove '.Random.seed'

In due course I hope someone is interested enough to review it.


Henric Winell



>
> which reproduces the above.  But is this really necessary?  And more
> importantly (at least to me):  Can it somehow be avoided?
>
> The current state of affairs is a bit unfortunate, since it implies that
> a user just by loading the new parallelized version of my package can no
> longer reproduce any subsequent results depending on random number
> generation (unless a call to 'set.seed' was issued *after* attaching my
> package).
>
> I'd be most grateful for any help that you're able to provide here. Many
> thanks!
>
> Kind regards,
> Henric Winell
>
>
>> sessionInfo()
> R Under development (unstable) (2014-01-26 r64897)
> Platform: x86_64-redhat-linux-gnu (64-bit)
>
> locale:
>   [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
>   [3] LC_TIME=sv_SE.UTF-8        LC_COLLATE=en_US.UTF-8
>   [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8
>   [7] LC_PAPER=en_US.UTF-8       LC_NAME=C
>   [9] LC_ADDRESS=C               LC_TELEPHONE=C
> [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> loaded via a namespace (and not attached):
> [1] compiler_3.1.0 parallel_3.1.0 tools_3.1.0
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

-------------- next part --------------
A non-text attachment was scrubbed...
Name: snow.R.patch
Type: text/x-patch
Size: 1138 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140306/e83ca0fc/attachment.bin>

From ripley at stats.ox.ac.uk  Thu Mar  6 13:29:38 2014
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 06 Mar 2014 12:29:38 +0000
Subject: [Rd] 'parallel' package changes '.Random.seed'
In-Reply-To: <53184B40.2050604@gmail.com>
References: <53184B40.2050604@gmail.com>
Message-ID: <53186A32.80600@stats.ox.ac.uk>

On 06/03/2014 10:17, Henric Winell wrote:
> Hi,
>
> I've implemented parallelization in one of my packages using the
> 'parallel' package -- many thanks for providing it!
>
> In my package I'm importing 'parallel' and so added it to the
> DESCRIPTION file's 'Import:' tag and also added a
> 'importFrom("parallel", ...)' statement in the NAMESPACE file.
>
> Parallelization works nicely, but my package no longer passes any parts
> of its (unparallelized) checks that depends on random number generation,
> e.g., the simulated data in the check suite are no longer the same as
> before parallelization was added.  This seems to be due to 'parallel'
> changing '.Random.seed' when loading its name space:
>
>  > set.seed(1)
>  > rs1 <- .Random.seed
>  > rnorm(1)
> [1] -0.6264538
>  > set.seed(1)
>  > rs2 <- .Random.seed
>  > identical(rs1, rs2)
> [1] TRUE
>  > loadNamespace("parallel")
> <environment: namespace:parallel>
>  > rs3 <- .Random.seed
>  > identical(rs1, rs3)
> [1] FALSE
>  > rnorm(1)
> [1] -0.3262334
>  > set.seed(1)
>  > rs4 <- .Random.seed
>  > identical(rs1, rs4)
> [1] TRUE
>
> I've taken a look at the 'parallel' source code, and in a few places a
> call to 'runif(1)' is issued.  So, what effectively seems to happen when
> 'parallel' is loaded is
>
>  > set.seed(1)
>  > runif(1)
> [1] 0.2655087
>  > rnorm(1)
> [1] -0.3262334
>
> which reproduces the above.  But is this really necessary?

Yes, in the places it is used.  Two are to do with setting up parallel 
streams when called, and the other is only called if R_PARALLEL_PORT is 
unset.

So set R_PARALLEL_PORT.

But your presumptions are wrong: R is perfectly entitled to use its 
random number generator, as is other code running in the R interpreter. 
  Once your call returns you cannot expect the session state to remain 
unchanged.


And more
> importantly (at least to me):  Can it somehow be avoided?
>
> The current state of affairs is a bit unfortunate, since it implies that
> a user just by loading the new parallelized version of my package can no
> longer reproduce any subsequent results depending on random number
> generation (unless a call to 'set.seed' was issued *after* attaching my
> package).
>
> I'd be most grateful for any help that you're able to provide here. Many
> thanks!
>
> Kind regards,
> Henric Winell
>
>
>> sessionInfo()
> R Under development (unstable) (2014-01-26 r64897)

See what the posting guide says about updating before posting ....

> Platform: x86_64-redhat-linux-gnu (64-bit)
>
> locale:
>   [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
>   [3] LC_TIME=sv_SE.UTF-8        LC_COLLATE=en_US.UTF-8
>   [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8
>   [7] LC_PAPER=en_US.UTF-8       LC_NAME=C
>   [9] LC_ADDRESS=C               LC_TELEPHONE=C
> [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base
>
> loaded via a namespace (and not attached):
> [1] compiler_3.1.0 parallel_3.1.0 tools_3.1.0
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From bbolker at gmail.com  Thu Mar  6 14:52:19 2014
From: bbolker at gmail.com (Ben Bolker)
Date: Thu, 6 Mar 2014 13:52:19 +0000
Subject: [Rd] [GSoC student proposal] Implementation of Modified
	Weibull-G Family Distribution in R
References: <CANtRVCC6vRb8iXaJHQodHqR3Oq2xyrOrcKGmo+=q5LLq7NWfJA@mail.gmail.com>
Message-ID: <loom.20140306T144915-721@post.gmane.org>

Dr M Aleem <aleemiubdr <at> gmail.com> writes:

> 
> Dear Brian and R team,
> 
> I am Dr. Aleem and I want to participate as a student in GSoC 2014 in the
> development of R. I have extensive experience in Statistical research and
> development and I am a student of advanced statistics applications. I
> strongly believe that the inclusion of Modified Weibull-G Family
> Distribution in R will significanlty help the reseasrch community in doing
> reliability/survival analyses. I have expertise in these areas and I would
> like to be part of open source development and implement it. May I ask you
> for potential mentor(s) who can be my supervisor for GSoC 2014.
>  The details
> of project is following:
> 

  [snip snip snip]

  I'm not commenting one way or the other on the value of the project
or its suitability for GSoC, but: at least as presented, I can't see
any reason at all that this project can't be done as a package
(just like the other 3000+ packages on CRAN that implement valuable
added functionality).  Also, therefore, no need to get R-Core to
sign off on the project (unless that is needed for GSoC purposes).
Asking around for a potential mentor on the R lists seems reasonable,
although I don't know whether R-devel or R-help would be a better
venue.

 good luck,
  Ben Bolker


From therneau at mayo.edu  Thu Mar  6 19:19:58 2014
From: therneau at mayo.edu (Therneau, Terry M., Ph.D.)
Date: Thu, 06 Mar 2014 12:19:58 -0600
Subject: [Rd] makepredictcall
Message-ID: <6e55ab$8g0ru8@ironport10.mayo.edu>

An issue came up with the rms package today that makepredictcall would solve, and I was 
going to suggest it to the author.  But looking in the help documents I couldn't find any 
reference to it.  There is a manual page, but it does not give much aid in creating code 
for a new transformation function. Did I miss something?

If not, I'd be willing to draft a paragraph about that which could be added to the 
extensions document.  I figured it out, somehow, for the pspline function of the survival 
package.  Submit such draft to ____?  The naresid function would be another useful addition.

Terry Therneau


From sannandi at umail.iu.edu  Thu Mar  6 19:47:06 2014
From: sannandi at umail.iu.edu (Sandip Nandi)
Date: Thu, 6 Mar 2014 10:47:06 -0800
Subject: [Rd] Create dataframe in C from table and return to R
Message-ID: <CAGSjAUC-w9zt5fL0i=R6z6yFyKBT8Dv9bbGsfG7TwBp_GLE9sw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140306/5e84ad02/attachment.pl>

From friendly at yorku.ca  Thu Mar  6 20:39:11 2014
From: friendly at yorku.ca (Michael Friendly)
Date: Thu, 06 Mar 2014 14:39:11 -0500
Subject: [Rd] version numbers for CRAN submissions that give warnings/notes
Message-ID: <5318CEDF.4050605@yorku.ca>

It often happens that I submit a new revision of a package, say 
mypkg-1.0-10, from R-Forge
to CRAN after running R CMD check locally and looking at the log files 
on R-Forge.
But R-Forge has the devel checks disabled, and I get an email from CRAN 
pointing out
some new warning or note I'm asked to correct.

OK, I correct this and commit a new rev to R-Forge.  But, is it still 
required to bump the
version number to mypkg-1.0-11 before resubmitting to CRAN, even though 
mypkg-1.0-10
did not make it there?

To do so means also modifying the DESCRIPTION, NEWS and
mypkg-package.Rd files even for a minor warning or note.

-- 
Michael Friendly     Email: friendly AT yorku DOT ca
Professor, Psychology Dept. & Chair, Quantitative Methods
York University      Voice: 416 736-2100 x66249 Fax: 416 736-5814
4700 Keele Street    Web:   http://www.datavis.ca
Toronto, ONT  M3J 1P3 CANADA


From murdoch.duncan at gmail.com  Thu Mar  6 21:10:08 2014
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Thu, 06 Mar 2014 15:10:08 -0500
Subject: [Rd] Create dataframe in C from table and return to R
In-Reply-To: <CAGSjAUC-w9zt5fL0i=R6z6yFyKBT8Dv9bbGsfG7TwBp_GLE9sw@mail.gmail.com>
References: <CAGSjAUC-w9zt5fL0i=R6z6yFyKBT8Dv9bbGsfG7TwBp_GLE9sw@mail.gmail.com>
Message-ID: <5318D620.2000806@gmail.com>

On 06/03/2014 1:47 PM, Sandip Nandi wrote:
> Hi ,
>
> I am trying to create a dataframe in C and sebd it back to R.  Can anyone
> point me to the part of the source code where it is doing ,  let me explain
> the problem I am having .
>
> --------------------------------------------------------------------
> My simple implementation is like this
>
> SEXP formDF() {
>
> SEXP dfm ,df , dfint , dfStr,lsnm;
> char *ab[3] = {"aa","vv","gy"};
> int sn[3] ={99,89,12};
> char *listnames[2] = {"int","string"};
> int i;
>
>
> PROTECT(df = allocVector(VECSXP,2));
> PROTECT(dfint = allocVector(INTSXP,3));
> PROTECT(dfStr = allocVector(STRSXP,3));
> PROTECT(lsnm = allocVector(STRSXP,2));
>
> SET_STRING_ELT(lsnm,0,mkChar("int"));
> SET_STRING_ELT(lsnm,1,mkChar("string"));
>
> for ( i = 0 ; i < 3; i++ ) {
> SET_STRING_ELT(dfStr,i,mkChar(ab[i]));
> INTEGER(dfint)[i] = sn[i];
> }
> SET_VECTOR_ELT(df,0,dfint);
> SET_VECTOR_ELT(df,1,dfStr);
> setAttrib(df,R_NamesSymbol,lsnm);
> //PROTECT(dfm=LCONS(dfm,list3(dfm,R_MissingArg,mkFalse())));
>
> UNPROTECT(4);
>
> dfm = PROTECT(lang2(install("data.frame"),df));
> SEXP res = PROTECT(eval(dfm,R_GlobalEnv));
>
> UNPROTECT(2)
>
> }
>
> ------------------------------------------------------------------------
> It works fine but i want it the other way
>
> the output is
> print(result)
>    int string
> 1  99     aa
> 2  89     vv
> 3  12     gy
>
>
> I want it in transposed . like
>
> dft <- as.data.frame(t(result))
>
> *Can I do the transpose it from C itself ? Which part of code I should look
> a*t .
>
> What My objective ?
>
> *Reading  rows of a table and create a dataframe out of it .  R is embedded
> in database so cannot call the odbc .  Need to implement that part .
> Database gives me API only to get a whole row at once .*

What you are asking for isn't a normal dataframe.  Dataframe columns are 
vectors all of a type.  You want the first row to be a string, the 
second row to be an integer.  You can't do that with simple atomic 
columns, and you probably don't want to mess with the alternative (which 
is to have your columns be lists), because no user will know how to deal 
with that.

Duncan Murdoch


From ripley at stats.ox.ac.uk  Thu Mar  6 21:11:11 2014
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 06 Mar 2014 20:11:11 +0000
Subject: [Rd] makepredictcall
In-Reply-To: <6e55ab$8g0ru8@ironport10.mayo.edu>
References: <6e55ab$8g0ru8@ironport10.mayo.edu>
Message-ID: <5318D65F.2000005@stats.ox.ac.uk>

See the developer site, e.g. 
http://developer.r-project.org/model-fitting-functions.txt .

That is where specialized info is (and this is specialized).

On 06/03/2014 18:19, Therneau, Terry M., Ph.D. wrote:
> An issue came up with the rms package today that makepredictcall would
> solve, and I was going to suggest it to the author.  But looking in the
> help documents I couldn't find any reference to it.  There is a manual
> page, but it does not give much aid in creating code for a new
> transformation function. Did I miss something?
>
> If not, I'd be willing to draft a paragraph about that which could be
> added to the extensions document.  I figured it out, somehow, for the
> pspline function of the survival package.  Submit such draft to ____?
> The naresid function would be another useful addition.
>
> Terry Therneau
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From murdoch.duncan at gmail.com  Thu Mar  6 21:22:48 2014
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Thu, 06 Mar 2014 15:22:48 -0500
Subject: [Rd] version numbers for CRAN submissions that give
	warnings/notes
In-Reply-To: <5318CEDF.4050605@yorku.ca>
References: <5318CEDF.4050605@yorku.ca>
Message-ID: <5318D918.6090501@gmail.com>

On 06/03/2014 2:39 PM, Michael Friendly wrote:
> It often happens that I submit a new revision of a package, say
> mypkg-1.0-10, from R-Forge
> to CRAN after running R CMD check locally and looking at the log files
> on R-Forge.
> But R-Forge has the devel checks disabled, and I get an email from CRAN
> pointing out
> some new warning or note I'm asked to correct.
>
> OK, I correct this and commit a new rev to R-Forge.  But, is it still
> required to bump the
> version number to mypkg-1.0-11 before resubmitting to CRAN, even though
> mypkg-1.0-10
> did not make it there?
>
> To do so means also modifying the DESCRIPTION, NEWS and
> mypkg-package.Rd files even for a minor warning or note.
>

That sounds like a question about CRAN policy, so I think you'll need to 
write to cran at r-project.org for an answer.  But I would assume it could 
cause confusion if you submitted another identically named tarball, and 
I'd recommend bumping the version number.

Duncan Murdoch


From ripley at stats.ox.ac.uk  Thu Mar  6 21:51:12 2014
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 06 Mar 2014 20:51:12 +0000
Subject: [Rd] version numbers for CRAN submissions that give
	warnings/notes
In-Reply-To: <5318D918.6090501@gmail.com>
References: <5318CEDF.4050605@yorku.ca> <5318D918.6090501@gmail.com>
Message-ID: <5318DFC0.2000804@stats.ox.ac.uk>

On 06/03/2014 20:22, Duncan Murdoch wrote:
> On 06/03/2014 2:39 PM, Michael Friendly wrote:
>> It often happens that I submit a new revision of a package, say
>> mypkg-1.0-10, from R-Forge
>> to CRAN after running R CMD check locally and looking at the log files
>> on R-Forge.
>> But R-Forge has the devel checks disabled, and I get an email from CRAN
>> pointing out
>> some new warning or note I'm asked to correct.

So do as the CRAN policies ask, and check with R-devel locally (or on 
winbuilder).  CRAN does not run R-Forge and suggestions should be made 
to its management.

>>
>> OK, I correct this and commit a new rev to R-Forge.  But, is it still
>> required to bump the
>> version number to mypkg-1.0-11 before resubmitting to CRAN, even though
>> mypkg-1.0-10
>> did not make it there?
>>
>> To do so means also modifying the DESCRIPTION, NEWS and
>> mypkg-package.Rd files even for a minor warning or note.

Not really: much more courteous to follow the policies and get it right 
in the first place.

> That sounds like a question about CRAN policy, so I think you'll need to
> write to cran at r-project.org for an answer.  But I would assume it could
> cause confusion if you submitted another identically named tarball, and
> I'd recommend bumping the version number.

That's the correct advice.  CRAN does not in general currently insist 
that each submission has a new number, but enough maintainers get 
confused that it is recommended (and for maintainers with a track record 
of confusion, insisted on).


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From saptarshi.guha at gmail.com  Thu Mar  6 23:32:29 2014
From: saptarshi.guha at gmail.com (Saptarshi Guha)
Date: Thu, 6 Mar 2014 14:32:29 -0800
Subject: [Rd] A question about multiple(?) out of order ReleaseObject
Message-ID: <CAJDot1rPt-cirTfmMB_essu3XUjk9WXuHrWZ0PfhL8JD+pcHmg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140306/59543a84/attachment.pl>

From gmbecker at ucdavis.edu  Thu Mar  6 23:48:37 2014
From: gmbecker at ucdavis.edu (Gabriel Becker)
Date: Thu, 6 Mar 2014 14:48:37 -0800
Subject: [Rd] A question about multiple(?) out of order ReleaseObject
In-Reply-To: <CAJDot1rPt-cirTfmMB_essu3XUjk9WXuHrWZ0PfhL8JD+pcHmg@mail.gmail.com>
References: <CAJDot1rPt-cirTfmMB_essu3XUjk9WXuHrWZ0PfhL8JD+pcHmg@mail.gmail.com>
Message-ID: <CADwqtCO=ub+=cQTxh-RB4FM4giFo1dG23DF4hWdY+1EBmd-zJA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140306/cbfbabb1/attachment.pl>

From saptarshi.guha at gmail.com  Fri Mar  7 00:56:19 2014
From: saptarshi.guha at gmail.com (Saptarshi Guha)
Date: Thu, 6 Mar 2014 15:56:19 -0800
Subject: [Rd] A question about multiple(?) out of order ReleaseObject
In-Reply-To: <CADwqtCO=ub+=cQTxh-RB4FM4giFo1dG23DF4hWdY+1EBmd-zJA@mail.gmail.com>
References: <CAJDot1rPt-cirTfmMB_essu3XUjk9WXuHrWZ0PfhL8JD+pcHmg@mail.gmail.com>
	<CADwqtCO=ub+=cQTxh-RB4FM4giFo1dG23DF4hWdY+1EBmd-zJA@mail.gmail.com>
Message-ID: <CAJDot1oRtue0dAS5bMqW-BUBLArqDVNNbdB7BFSrgXfw3_zQ5w@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140306/eb778ae3/attachment.pl>

From saptarshi.guha at gmail.com  Fri Mar  7 01:30:46 2014
From: saptarshi.guha at gmail.com (Saptarshi Guha)
Date: Thu, 6 Mar 2014 16:30:46 -0800
Subject: [Rd] Repost: (apologies for HTML post) A question about multiple(?)
 out of order ReleaseObject
Message-ID: <CAJDot1o5kMgwhKFBQGOnrip4r6AJbz3TktUymG8c3Pd56+maEQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140306/45d5442e/attachment.pl>

From saptarshi.guha at gmail.com  Fri Mar  7 01:32:34 2014
From: saptarshi.guha at gmail.com (Saptarshi Guha)
Date: Thu, 6 Mar 2014 16:32:34 -0800
Subject: [Rd] Many apologies: last post: A question about multiple(?) out of
	order ReleaseObject
Message-ID: <CAJDot1qHTFC2bR7QgOoXSCYW7Z3yAoLjUMNJqSx6kd0Z4_Au9A@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140306/1ce8de1e/attachment.pl>

From gmbecker at ucdavis.edu  Fri Mar  7 03:20:59 2014
From: gmbecker at ucdavis.edu (Gabriel Becker)
Date: Thu, 6 Mar 2014 18:20:59 -0800
Subject: [Rd] A question about multiple(?) out of order ReleaseObject
In-Reply-To: <CAJDot1oRtue0dAS5bMqW-BUBLArqDVNNbdB7BFSrgXfw3_zQ5w@mail.gmail.com>
References: <CAJDot1rPt-cirTfmMB_essu3XUjk9WXuHrWZ0PfhL8JD+pcHmg@mail.gmail.com>
	<CADwqtCO=ub+=cQTxh-RB4FM4giFo1dG23DF4hWdY+1EBmd-zJA@mail.gmail.com>
	<CAJDot1oRtue0dAS5bMqW-BUBLArqDVNNbdB7BFSrgXfw3_zQ5w@mail.gmail.com>
Message-ID: <CADwqtCMbG68yc36Yvm5LKxTLh_a_G_+mf9=Ew9Xu0XXRVD60Vg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140306/2951cb42/attachment.pl>

From simon.urbanek at r-project.org  Fri Mar  7 04:05:35 2014
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Thu, 6 Mar 2014 22:05:35 -0500
Subject: [Rd] A question about multiple(?) out of order ReleaseObject
In-Reply-To: <CAJDot1rPt-cirTfmMB_essu3XUjk9WXuHrWZ0PfhL8JD+pcHmg@mail.gmail.com>
References: <CAJDot1rPt-cirTfmMB_essu3XUjk9WXuHrWZ0PfhL8JD+pcHmg@mail.gmail.com>
Message-ID: <9AB92085-293D-4008-9840-52AC93BBB78F@r-project.org>

On Mar 6, 2014, at 5:32 PM, Saptarshi Guha <saptarshi.guha at gmail.com> wrote:

> Hello,
> 
> This  is a question that probably reveals my lack of understanding.
> In a C function (call it cfunc), i created a SEXP, called S, and then
> called R_PreserveObject on S.
> 
> I returned the SEXP to the calling R function (call it rfunc). Note, I
> didn't call
> R_ReleaseObject on S.
> 
> v <- .Call("cfunc")
> 
> So, are the following  statements correct
> 
> 1.  S is 'doubly' protected from the GC by  being associated both with 'v'
> and because it has been added to the precious list (via a call to
> R_PreserveObject without ReleaseObject being called)
> 

yes


> 2. I have another C function called cfunc2. In cfunc2, I call
> R_ReleaseObject on S.  S , however, is still protected from the GC, because
> it is associated with 'v'
> 

yes (assuming the binding to v still exists at that point). Note, however, that is such a case you R_PreserveObject() is pointless since you don't need to protect it on exit (that's in fact the convention - return results are never protected).


> Is (1) and (2) correct?
>  
> I have not used R_protect/unprotect, because if I return from cfunc without the equivalent number of unprotects, i get 'unbalanced stack' warnings. I'd rather not have to worry about that because i intend to balance it later.
> 

Normally, you should not keep the result of a function protected since it means you *have* to guarantee the unprotect at a later point. That is in general impossible to guarantee unless you have another object that is holding a reference that will be cleared by an explicitly registered finalizer. So unless that is the case, you are creating an explicit leak = bad. If you don't have as stack-like design, you can always use explicitly managed object for the lifetime (personally, I prefer that) since all chained objects are protected by design, or use REPROTECT.

Cheers,
Simon


> Regards
> Saptarshi
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 


From saptarshi.guha at gmail.com  Fri Mar  7 07:09:59 2014
From: saptarshi.guha at gmail.com (Saptarshi Guha)
Date: Thu, 6 Mar 2014 22:09:59 -0800
Subject: [Rd] A question about multiple(?) out of order ReleaseObject
In-Reply-To: <9AB92085-293D-4008-9840-52AC93BBB78F@r-project.org>
References: <CAJDot1rPt-cirTfmMB_essu3XUjk9WXuHrWZ0PfhL8JD+pcHmg@mail.gmail.com>
	<9AB92085-293D-4008-9840-52AC93BBB78F@r-project.org>
Message-ID: <CAJDot1qBqzeE=9UtJZ1Ljro=5JMR+Y=9SDmg6tbAL7G7mP9s3Q@mail.gmail.com>

Hello Simon,

Thanks much for the replies. It makes sense now and I'm on the right
track. I'll explain why this
might happen.

I've written a package caller rterra [1] which essentially allows the
R user to write extensions in
Lua. The extensions are JIT compiled via LuaJIT. For deterministic
performance the user can write
their extension in Terra.

To make memory management easier for the extension writer, i wanted auto memory
management. Consider the following code sample which calls an
extension written in Lua to compute a sum
by traversing some JSON

1. In R, the extension, is called as

l <- terra("computeCountsByDayAndCountry", values, c("2013-01-01","2014-12-31"))

2. In Lua, the code looks like

function computeCountsByDayAndCountry(jsonstr,dateRange)
   local x,y = R.Robj(jsonstr),R.Robj(dateRange)
   local dstart,dend = ffi.string(y[0]),ffi.string(y[1])
   local f =  R.Robj{type='str', length =#x}
   R.autoProtect(f) -- line 'A'
   for index = 0, #x-1 do
      local ok,jc  = pcall(cjson.decode,ffi.string(x[index][0]))
      if ok then
     f[index] = cjson.encode( _computeSearchCountsCountry(jc,dstart,dend))
      else
     f[index] = cjson.encode({})
      end
   end
   return f
end

When this returns, the SEXP contained in 'f' is associated with 'l' (in step 1)

LuaJIT has garbage collection. At http://luajit.org/ext_ffi_api.html,
when 'f' above (line 'A') get
garbage collected, a finalizer is run. The call to autoProtect does

1. calls PReserveObject on f.sexp
2. sets the finalizer to call ReleaseObject.

When the user calls an extension written in Lua again, 'f' will get
garbage collected by Lua
(assuming it has no references in Lua).

And yes, there will be a mem leak, *iff* the user *never* calls an
extension written in Lua again,
otherwise no.

And of course, the user can not call autoProtect, but can manage it
themselves, i.e. using R.protect
and R.unprotect.

Thanks for the insight

http://people.mozilla.org/~sguha/blog/2013/08/01/rterra_first_post.html

Cheers
Saptarshi

On Thu, Mar 6, 2014 at 7:05 PM, Simon Urbanek
<simon.urbanek at r-project.org> wrote:
> On Mar 6, 2014, at 5:32 PM, Saptarshi Guha <saptarshi.guha at gmail.com> wrote:
>
>> Hello,
>>
>> This  is a question that probably reveals my lack of understanding.
>> In a C function (call it cfunc), i created a SEXP, called S, and then
>> called R_PreserveObject on S.
>>
>> I returned the SEXP to the calling R function (call it rfunc). Note, I
>> didn't call
>> R_ReleaseObject on S.
>>
>> v <- .Call("cfunc")
>>
>> So, are the following  statements correct
>>
>> 1.  S is 'doubly' protected from the GC by  being associated both with 'v'
>> and because it has been added to the precious list (via a call to
>> R_PreserveObject without ReleaseObject being called)
>>
>
> yes
>
>
>> 2. I have another C function called cfunc2. In cfunc2, I call
>> R_ReleaseObject on S.  S , however, is still protected from the GC, because
>> it is associated with 'v'
>>
>
> yes (assuming the binding to v still exists at that point). Note, however, that is such a case you R_PreserveObject() is pointless since you don't need to protect it on exit (that's in fact the convention - return results are never protected).
>
>
>> Is (1) and (2) correct?
>>
>> I have not used R_protect/unprotect, because if I return from cfunc without the equivalent number of unprotects, i get 'unbalanced stack' warnings. I'd rather not have to worry about that because i intend to balance it later.
>>
>
> Normally, you should not keep the result of a function protected since it means you *have* to guarantee the unprotect at a later point. That is in general impossible to guarantee unless you have another object that is holding a reference that will be cleared by an explicitly registered finalizer. So unless that is the case, you are creating an explicit leak = bad. If you don't have as stack-like design, you can always use explicitly managed object for the lifetime (personally, I prefer that) since all chained objects are protected by design, or use REPROTECT.
>
> Cheers,
> Simon
>
>
>> Regards
>> Saptarshi
>>
>>       [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>


From nilsson.henric at gmail.com  Fri Mar  7 11:21:19 2014
From: nilsson.henric at gmail.com (Henric Winell)
Date: Fri, 07 Mar 2014 11:21:19 +0100
Subject: [Rd] 'parallel' package changes '.Random.seed'
Message-ID: <53199D9F.5000304@gmail.com>

Dear Prof Ripley,

Thank you for your kind reply.  Please find my comments below.

> On 2014-03-06 13:29, Prof Brian Ripley wrote:
>
>> On 06/03/2014 10:17, Henric Winell wrote:
>> Hi,
>>
>> I've implemented parallelization in one of my packages using the
>> 'parallel' package -- many thanks for providing it!
>>
>> In my package I'm importing 'parallel' and so added it to the
>> DESCRIPTION file's 'Import:' tag and also added a
>> 'importFrom("parallel", ...)' statement in the NAMESPACE file.
>>
>> Parallelization works nicely, but my package no longer passes any
>> parts of its (unparallelized) checks that depends on random number
>> generation, e.g., the simulated data in the check suite are no
>> longer the same as before parallelization was added.  This seems to
>> be due to 'parallel' changing '.Random.seed' when loading its name
>> space:
>>
>> > set.seed(1)
>> > rs1 <- .Random.seed
>> > rnorm(1)
>> [1] -0.6264538
>> > set.seed(1)
>> > rs2 <- .Random.seed
>> > identical(rs1, rs2)
>> [1] TRUE
>> > loadNamespace("parallel")
>> <environment: namespace:parallel>
>> > rs3 <- .Random.seed
>> > identical(rs1, rs3)
>> [1] FALSE
>> > rnorm(1)
>> [1] -0.3262334
>> > set.seed(1)
>> > rs4 <- .Random.seed
>> > identical(rs1, rs4)
>> [1] TRUE
>>
>> I've taken a look at the 'parallel' source code, and in a few places
>> a call to 'runif(1)' is issued.  So, what effectively seems to happen
>> when 'parallel' is loaded is
>>
>> > set.seed(1)
>> > runif(1)
>> [1] 0.2655087
>> > rnorm(1)
>> [1] -0.3262334
>>
>> which reproduces the above.  But is this really necessary?
>
> Yes, in the places it is used.

I apologize for not expressing myself more clearly here.  I do not 
dispute whether it is necessary to call runif(1L) or not.

What I meant to ask was: Is it really necessary for 'parallel' to change 
'.Random.seed' when its namespace is loaded?

> Two are to do with setting up parallel streams when called,

Yes, and are not relevant here.

> and the other is only called if R_PARALLEL_PORT is unset.

But can't that action, i.e., choosing a random port in the 11000:11999 
range, be implemented so that it doesn't change '.Random.seed' when the 
namespace is loaded?

> So set R_PARALLEL_PORT.

Thanks for the suggesting it.  This works nicely as shown in my earlier 
follow-up post, and makes the package pass its own test.

The downside is that this will require the same intervention from any 
other user relying on random number generation.

> But your presumptions are wrong: R is perfectly entitled to use its
> random number generator, as is other code running in the R
> interpreter.  Once your call returns you cannot expect the session
> state to remain unchanged.

You're completely right, of course.

But I believe that it may lead to surprising behaviour for some 
unsuspecting users (me included), and should be avoided if possible. 
But maybe you're surprised, that I'm surprised?

I've looked at your implementation in the 'boot' package, where 
'parallel' is not explicitly imported and thus '.Random.seed' is 
untouched after loading the 'boot' namespace.  Is it preferable to not 
import 'parallel' and access the relevant 'parallel' functions using the 
'::' operator as you did there?  Please advise.

>> And more importantly (at least to me):  Can it somehow be avoided?
>>
>> The current state of affairs is a bit unfortunate, since it implies
>> that a user just by loading the new parallelized version of my
>> package can no longer reproduce any subsequent results depending on
>> random number generation (unless a call to 'set.seed' was issued
>> *after* attaching my package).
>>
>> I'd be most grateful for any help that you're able to provide here.
>> Many thanks!
>>
>> Kind regards,
>> Henric Winell
>>
>>
>> > sessionInfo()
>> R Under development (unstable) (2014-01-26 r64897)
>
> See what the posting guide says about updating before posting ....

Thanks for reminding me -- an update is long overdue.

Before posting I checked the SVN repository and found that the relevant 
code in 'parallel' was the same as in the version I used.


Henric Winell



>
>> Platform: x86_64-redhat-linux-gnu (64-bit)
>>
>> locale:
>>   [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
>>   [3] LC_TIME=sv_SE.UTF-8        LC_COLLATE=en_US.UTF-8
>>   [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8
>>   [7] LC_PAPER=en_US.UTF-8       LC_NAME=C
>>   [9] LC_ADDRESS=C               LC_TELEPHONE=C
>> [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C
>>
>> attached base packages:
>> [1] stats     graphics  grDevices utils     datasets  methods   base
>>
>> loaded via a namespace (and not attached):
>> [1] compiler_3.1.0 parallel_3.1.0 tools_3.1.0
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From therneau at mayo.edu  Fri Mar  7 15:28:10 2014
From: therneau at mayo.edu (Therneau, Terry M., Ph.D.)
Date: Fri, 07 Mar 2014 08:28:10 -0600
Subject: [Rd] R makepredictcall
In-Reply-To: <mailman.19.1394190006.22315.r-devel@r-project.org>
References: <mailman.19.1394190006.22315.r-devel@r-project.org>
Message-ID: <6e55ab$8g64it@ironport10.mayo.edu>

That site, and that document in particular, had nothing to add on this particular topic.

So on to question 2.  I think the material is useful.  If I write it up will that be 
welcome/tolerated/ignored addition to the R docs?


On 03/07/2014 05:00 AM, r-devel-request at r-project.org wrote:
> See the developer site, e.g.
> http://developer.r-project.org/model-fitting-functions.txt  .
>
> That is where specialized info is (and this is specialized).
>
> On 06/03/2014 18:19, Therneau, Terry M., Ph.D. wrote:
>> >An issue came up with the rms package today that makepredictcall would
>> >solve, and I was going to suggest it to the author.  But looking in the
>> >help documents I couldn't find any reference to it.  There is a manual
>> >page, but it does not give much aid in creating code for a new
>> >transformation function. Did I miss something?
>> >
>> >If not, I'd be willing to draft a paragraph about that which could be
>> >added to the extensions document.  I figured it out, somehow, for the
>> >pspline function of the survival package.  Submit such draft to ____?
>> >The naresid function would be another useful addition.
>> >
>> >Terry Therneau
>> >
>> >______________________________________________
>> >


From sachko.honda at mountainpacificgroup.com  Fri Mar  7 18:17:04 2014
From: sachko.honda at mountainpacificgroup.com (Sachko Honda)
Date: Fri, 7 Mar 2014 11:17:04 -0600
Subject: [Rd] Regarding: stat.math.ethz.ch mailing list memberships reminder
Message-ID: <42143135B5270A45996065C2EFFC2B59116D9AC4@MBX39.exg5.exghost.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140307/54c13691/attachment.pl>

From josh.m.ulrich at gmail.com  Fri Mar  7 18:19:52 2014
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Fri, 7 Mar 2014 11:19:52 -0600
Subject: [Rd] Regarding: stat.math.ethz.ch mailing list memberships
	reminder
In-Reply-To: <42143135B5270A45996065C2EFFC2B59116D9AC4@MBX39.exg5.exghost.com>
References: <42143135B5270A45996065C2EFFC2B59116D9AC4@MBX39.exg5.exghost.com>
Message-ID: <CAPPM_gSt==VUxj6+jUpGehvEH-vjYkjU2E0EjnCK3j2iZpviuA@mail.gmail.com>

Please read the instructions/warnings before subscribing.

"You may enter a privacy password below. This provides only mild
security, but should prevent others from messing with your
subscription. Do not use a valuable password as it will occasionally
be emailed back to you in cleartext."
--
Joshua Ulrich  |  about.me/joshuaulrich
FOSS Trading  |  www.fosstrading.com


On Fri, Mar 7, 2014 at 11:17 AM, Sachko Honda
<sachko.honda at mountainpacificgroup.com> wrote:
> Please never ever send the password in clear text, never!!!
>
> Sachko Honda
>
> Phone: 425.284.7200
> Fax: 425.284.7201
> Sachko.Honda at MountainPacificGroup.com
> Mountain Pacific Group
> 11820 Northup Way, Suite E210
> Bellevue, WA 98005-1926
>
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From karl.forner at gmail.com  Fri Mar  7 19:09:42 2014
From: karl.forner at gmail.com (Karl Forner)
Date: Fri, 7 Mar 2014 19:09:42 +0100
Subject: [Rd] [PATCH] Code coverage support proof of concept
In-Reply-To: <CAMd4_Af5uM9JQYEOKgEajmQvOaoWAgx9UiukjCrBy2PJKPDW8A@mail.gmail.com>
References: <CAMd4_Af5uM9JQYEOKgEajmQvOaoWAgx9UiukjCrBy2PJKPDW8A@mail.gmail.com>
Message-ID: <CAMd4_AdgTQH+KfpWbKS0Ze8oMM6-cb+cR2ZU+uj_DvuC+Z=14Q@mail.gmail.com>

Here's an updated version of the patch that fixes a stack imbalance bug.
N.B: the patch seems to work fine with R-3.0.2 too.

On Wed, Mar 5, 2014 at 5:16 PM, Karl Forner <karl.forner at gmail.com> wrote:
> Hello,
>
> I submit a patch for review that implements code coverage tracing in
> the R interpreter.
> It records the lines that are actually executed and their associated
> frequency for which srcref information is available.
>
> I perfectly understands that this patch will not make its way inside R
> as it is, that they are many concerns of stability, compatibility,
> maintenance and so on.
> I would like to have the code reviewed, and proper guidance on how to
> get this feature available at one point in R, in base R or as a
> package or patch if other people are interested.
>
> Usage
> --------
> Rcov_start()
> # your code to trace here
> res <- Rcov_stop()
>
> res is currently a hashed env, with traced source filenames associated
> with 2-columns matrices holding the line numbers and their
> frequencies.
>
>
> How it works
> -----------------
> I added a test in getSrcref(), that records the line numbers if code
> coverage is started.
> The overhead should be minimal since for a given file, subsequent
> covered lines will be stored
> in constant time. I use a hased env to store the occurrences by file.
>
> I added two entry points in the utils package (Rcov_start() and Rcov_stop())
>
>
> Example
> -------------
> * untar the latest R-devel and cd into it
> * patch -p1 < rdev-cov-patch.txt
> * ./configure [... ] && make && [sudo] make install
> * install the devtools package
> * run the following script using Rscript
>
> library(methods)
> library(devtools)
> pkg  <- download.packages('testthat', '.', repos = "http://stat.ethz.ch/CRAN")
> untar(pkg[1, 2])
>
> Rcov_start()
> test('testthat')
> env <- Rcov_stop()
>
> res <- lapply(ls(env), get, envir = env)
> names(res) <- ls(env)
> print(res)
>
>
> This will hopefully output something like:
> $`.../testthat/R/auto-test.r`
>      [,1] [,2]
> [1,]   33    1
> [2,]   80    1
>
> $`.../testthat/R/colour-text.r`
>       [,1] [,2]
>  [1,]   18    1
>  [2,]   19  106
>  [3,]   20  106
>  [4,]   22  106
>  [5,]   23  106
>  [6,]   40    1
>  [7,]   59    1
>  [8,]   70    1
>  [9,]   71  106
> ...
>
>
> Karl Forner
>
>
> Disclaimer
> -------------
> There are probably bugs  and ugly statements, but this is just a proof
> of concept. This is untested and only run on a linux x86_64
-------------- next part --------------
diff -urN -x '.*' R-devel/src/library/utils/man/Rcov_start.Rd R-develcov/src/library/utils/man/Rcov_start.Rd
--- R-devel/src/library/utils/man/Rcov_start.Rd	1970-01-01 01:00:00.000000000 +0100
+++ R-develcov/src/library/utils/man/Rcov_start.Rd	2014-03-07 18:41:33.117646470 +0100
@@ -0,0 +1,26 @@
+% File src/library/utils/man/Rcov_start.Rd
+% Part of the R package, http://www.R-project.org
+% Copyright 1995-2010 R Core Team
+% Distributed under GPL 2 or later
+
+\name{Rcov_start}
+\alias{Rcov_start}
+\title{Start Code Coverage analysis of R's Execution}
+\description{
+  Start Code Coverage analysis of the execution of \R expressions.
+}
+\usage{
+Rcov_start(nb_lines = 10000L, growth_rate = 2)
+}
+\arguments{
+  \item{nb_lines}{
+    Initial max number of lines per source file. 
+  }
+  \item{growth_rate}{
+    growth factor of the line numbers vectors per filename. 
+    If a reached line number L is greater than  nb_lines, the vector will
+    be reallocated with provisional size of growth_rate * L. 
+  }
+}
+
+\keyword{utilities}
diff -urN -x '.*' R-devel/src/library/utils/man/Rcov_stop.Rd R-develcov/src/library/utils/man/Rcov_stop.Rd
--- R-devel/src/library/utils/man/Rcov_stop.Rd	1970-01-01 01:00:00.000000000 +0100
+++ R-develcov/src/library/utils/man/Rcov_stop.Rd	2014-03-07 18:41:33.117646470 +0100
@@ -0,0 +1,20 @@
+% File src/library/utils/man/Rcov_stop.Rd
+% Part of the R package, http://www.R-project.org
+% Copyright 1995-2010 R Core Team
+% Distributed under GPL 2 or later
+
+\name{Rcov_stop}
+\alias{Rcov_stop}
+\title{Start Code Coverage analysis of R's Execution}
+\description{
+  Start Code Coverage analysis of the execution of \R expressions.
+}
+\usage{
+Rcov_stop()
+}
+
+\value{
+  a named list of integer vectors holding occurrences counts (line number, frequency)
+  , named after the covered source file names. 
+}
+\keyword{utilities}
diff -urN -x '.*' R-devel/src/library/utils/NAMESPACE R-develcov/src/library/utils/NAMESPACE
--- R-devel/src/library/utils/NAMESPACE	2013-09-10 03:04:59.000000000 +0200
+++ R-develcov/src/library/utils/NAMESPACE	2014-03-07 18:41:33.121646470 +0100
@@ -1,7 +1,7 @@
 # Refer to all C routines by their name prefixed by C_
 useDynLib(utils, .registration = TRUE, .fixes = "C_")
 
-export("?", .DollarNames, CRAN.packages, Rprof, Rprofmem, RShowDoc,
+export("?", .DollarNames, CRAN.packages, Rcov_start, Rcov_stop, Rprof, Rprofmem, RShowDoc,
        RSiteSearch, URLdecode, URLencode, View, adist, alarm, apropos,
        aregexec, argsAnywhere, assignInMyNamespace, assignInNamespace,
        as.roman, as.person, as.personList, as.relistable, aspell,
diff -urN -x '.*' R-devel/src/library/utils/R/Rcov.R R-develcov/src/library/utils/R/Rcov.R
--- R-devel/src/library/utils/R/Rcov.R	1970-01-01 01:00:00.000000000 +0100
+++ R-develcov/src/library/utils/R/Rcov.R	2014-03-07 18:41:33.121646470 +0100
@@ -0,0 +1,27 @@
+#  File src/library/utils/R/Rcov.R
+#  Part of the R package, http://www.R-project.org
+#
+#  Copyright (C) 1995-2013 The R Core Team
+#
+#  This program is free software; you can redistribute it and/or modify
+#  it under the terms of the GNU General Public License as published by
+#  the Free Software Foundation; either version 2 of the License, or
+#  (at your option) any later version.
+#
+#  This program is distributed in the hope that it will be useful,
+#  but WITHOUT ANY WARRANTY; without even the implied warranty of
+#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+#  GNU General Public License for more details.
+#
+#  A copy of the GNU General Public License is available at
+#  http://www.r-project.org/Licenses/
+
+Rcov_start <- function(nb_lines = 10000L, growth_rate = 2)
+{
+    invisible(.External(C_Rcov_start, nb_lines, growth_rate))
+}
+
+
+Rcov_stop <- function() {
+	invisible(.External(C_Rcov_stop))
+}
diff -urN -x '.*' R-devel/src/library/utils/src/init.c R-develcov/src/library/utils/src/init.c
--- R-devel/src/library/utils/src/init.c	2014-01-08 18:06:33.000000000 +0100
+++ R-develcov/src/library/utils/src/init.c	2014-03-07 18:41:33.129646469 +0100
@@ -74,6 +74,8 @@
 static const R_ExternalMethodDef ExtEntries[] = {
     EXTDEF(download, 5),
     EXTDEF(unzip, 7),
+    EXTDEF(Rcov_start, 2),
+    EXTDEF(Rcov_stop, 0),
     EXTDEF(Rprof, 8),
     EXTDEF(Rprofmem, 3),
 
diff -urN -x '.*' R-devel/src/library/utils/src/utils.c R-develcov/src/library/utils/src/utils.c
--- R-devel/src/library/utils/src/utils.c	2012-10-01 17:52:17.000000000 +0200
+++ R-develcov/src/library/utils/src/utils.c	2014-03-07 18:41:33.129646469 +0100
@@ -27,6 +27,22 @@
 #include "utils.h"
 
 /* from src/main/eval.c */
+void do_Rcov_start(int nb_lines, double growth_rate);
+SEXP do_Rcov_stop(void);
+
+SEXP Rcov_start(SEXP args)
+{
+    do_Rcov_start(asInteger(CADR(args)), asReal(CADDR(args)));
+    return R_NilValue;		/* -Wall */
+}
+
+
+SEXP Rcov_stop(void)
+{
+    return do_Rcov_stop();
+}
+
+/* from src/main/eval.c */
 SEXP do_Rprof(SEXP args);
 
 SEXP Rprof(SEXP args)
diff -urN -x '.*' R-devel/src/library/utils/src/utils.h R-develcov/src/library/utils/src/utils.h
--- R-devel/src/library/utils/src/utils.h	2014-01-06 03:04:59.000000000 +0100
+++ R-develcov/src/library/utils/src/utils.h	2014-03-07 18:41:33.129646469 +0100
@@ -26,6 +26,8 @@
 
 SEXP objectSize(SEXP s);
 SEXP unzip(SEXP args);
+SEXP Rcov_start(SEXP args);
+SEXP Rcov_stop(void);
 SEXP Rprof(SEXP args);
 SEXP Rprofmem(SEXP args);
 
diff -urN -x '.*' R-devel/src/main/eval.c R-develcov/src/main/eval.c
--- R-devel/src/main/eval.c	2014-02-21 03:03:36.000000000 +0100
+++ R-develcov/src/main/eval.c	2014-03-07 18:41:33.133646469 +0100
@@ -37,6 +37,202 @@
 
 static SEXP bcEval(SEXP, SEXP, Rboolean);
 
+
+static int R_Code_Coverage = 0;
+#define R_CODE_COVERAGE
+#ifdef  R_CODE_COVERAGE
+
+/* A Simple mechanism for implementing code coverage.
+  When code coverage is enables (via the R_Code_Coverage global var),
+  each call to the getSrcref() function will record the current srcref filename and line
+  number.
+  The code coverage support is controlled by the R_CODE_COVERAGE preprocessor define.
+
+  The actual implementation consists for the moment in intercepting getSrcref() calls,
+  then calling the record_code_coverage() function.
+  The code coverage tracing is activating by calling the do_Rcov() (Rcov from R) function.
+
+  Karl Forner
+ */
+
+/*   global variable: hit lines freqs: a HashedEnv by filename */
+static SEXP R_Cov_freqs_hash = NULL;
+
+/* create a new non-sparsed vector of line frequencies at least of length size.
+ * Depending on the do_Rcov_start params nb_lines and growth_rate,
+ * it will allocate an actual size of either nb_lines or size * growth_rate
+ */
+static SEXP cov_new_lines_vector(int size) {
+	SEXP sexp, lines;
+	int nb_lines, i;
+	int *tab;
+	double growth_rate;
+
+	sexp = findVarInFrame(R_Cov_freqs_hash, install(".nb_lines"));
+	nb_lines = INTEGER(sexp)[0];
+	if (size > nb_lines) {
+		sexp = findVarInFrame(R_Cov_freqs_hash, install(".growth_rate"));
+		growth_rate = REAL(sexp)[0];
+		size = (int)(size * growth_rate);
+	} else {
+		size = nb_lines;
+	}
+
+	PROTECT(lines = allocVector(INTSXP, size));
+	tab = INTEGER(lines);
+	for (i = 0; i < size; ++i)
+		tab[i] = 0;
+	UNPROTECT(1);
+	return lines;
+}
+
+/* store a new line occurrence in R_Cov_freqs_hash for filename */
+static void cov_store_new_line(const char* filename, int line) {
+	SEXP lines, lines2;
+	int len, i, *t1, *t2;
+
+	lines = findVarInFrame(R_Cov_freqs_hash, install(filename));
+	if (lines == R_UnboundValue) { /* new file */
+		lines = cov_new_lines_vector(line + 1);
+		defineVar(install(filename), lines, R_Cov_freqs_hash);
+	}
+	if (length(lines) <= line) {
+		/* lines vector too short */
+		PROTECT(lines2 = cov_new_lines_vector(line + 1)); /* should allocate (line+1)*growth_rate */
+		len = length(lines);
+		i = 0;
+		t1 = INTEGER(lines);
+		t2 = INTEGER(lines2);
+		for (i = 0; i < len; ++i)
+			lines2[i] = lines[i];
+		defineVar(install(filename), lines2, R_Cov_freqs_hash);
+		lines = lines2;
+		UNPROTECT(1);
+	}
+
+	INTEGER(lines)[line]++;
+}
+
+/* maybe store a new srcref in R_Cov_freqs_hash */
+static void record_code_coverage(SEXP srcref)
+{
+	if (srcref && !isNull(srcref)) {
+		int fnum, line = asInteger(srcref);
+
+		SEXP srcfile = getAttrib(srcref, R_SrcfileSymbol);
+		const char *filename;
+
+		if (!srcfile || TYPEOF(srcfile) != ENVSXP) return;
+		srcfile = findVar(install("filename"), srcfile);
+		if (TYPEOF(srcfile) != STRSXP || !length(srcfile)) return;
+
+		filename = CHAR(STRING_ELT(srcfile, 0));
+		cov_store_new_line(filename, line);
+	}
+}
+
+
+/* This initiates the code coverage tracing.
+ * nb_lines is the initial size of frequencies vectors per file.
+ * If a line number L is encountered s.t L >=nb_lines, the vector will be extended
+ * to L * growth_rate
+ */
+void do_Rcov_start(int nb_lines, double growth_rate)
+{
+	SEXP sexp;
+
+	if (growth_rate < 1.1)
+		growth_rate = 1.1;
+
+	if (R_Code_Coverage) return;
+	R_Code_Coverage = 1;
+	if (R_Cov_freqs_hash != NULL)
+		R_ReleaseObject(R_Cov_freqs_hash);
+
+	/* put the params nb_lines and growth_rate as hidden vars of the hashed env */
+	R_Cov_freqs_hash = R_NewHashedEnv(R_NilValue, ScalarInteger(0));
+	R_PreserveObject(R_Cov_freqs_hash);
+	PROTECT(sexp = ScalarInteger(nb_lines));
+	defineVar(install(".nb_lines"), sexp, R_Cov_freqs_hash);
+
+	PROTECT(sexp = ScalarReal(growth_rate));
+	defineVar(install(".growth_rate"), sexp, R_Cov_freqs_hash);
+
+	UNPROTECT(2);
+}
+
+/* Ends the code coverage tracing.
+ * and returns an environment with symbols named after the covered source files and values
+ * matrices of dim n*2, which first column is the line number and the second the nb of occurrences
+ */
+SEXP do_Rcov_stop(void)
+{
+	SEXP names, lines, mat, key, res;
+	int n, i, j, k, nb_lines, non_empty_lines, *tab, *m;
+
+	/* stop the code covered tracing */
+	R_Code_Coverage = 0;
+
+	/* convert frequencies by line to matrix N*2 of lines, freq */
+	PROTECT(names = R_lsInternal(R_Cov_freqs_hash, FALSE));
+	n = length(names);
+
+	for (i = 0; i < n; ++i) {
+		key = install(CHAR(STRING_ELT(names, i)));
+		lines = findVarInFrame(R_Cov_freqs_hash, key);
+
+		tab = INTEGER(lines);
+		nb_lines = length(lines);
+		non_empty_lines = 0;
+		for (j = 0; j < nb_lines; ++j)
+			if (tab[j])
+				++non_empty_lines;
+
+		PROTECT(mat = allocMatrix(INTSXP, non_empty_lines, 2));
+		m = INTEGER(mat);
+		k = 0;
+		for (j = 0; j < nb_lines; ++j) {
+			if (tab[j]) {
+				m[k] = j;
+				m[k + non_empty_lines] = tab[j];
+				++k;
+			}
+		}
+
+		defineVar(key, mat, R_Cov_freqs_hash);
+		UNPROTECT(1); /* mat */
+	}
+	UNPROTECT(1); /* names */
+
+	res = R_Cov_freqs_hash;
+	R_ReleaseObject(R_Cov_freqs_hash);
+	R_Cov_freqs_hash = NULL;
+
+    return res;
+}
+
+
+#else /* not R_CODE_COVERAGE */
+
+void do_Rcov_start(int nb_lines, int growth_rate)
+{
+    error(_("do_Rcov_start: R code coverage is not available on this system"));
+    return R_NilValue;		/* -Wall */
+}
+
+SEXP do_Rcov_stop()
+{
+    error(_("do_Rcov_stop: R code coverage is not available on this system"));
+	R_Code_Coverage = 0;
+}
+
+
+#endif
+
+
+
+
+
 /* BC_PROILFING needs to be defined here and in registration.c */
 /*#define BC_PROFILING*/
 #ifdef BC_PROFILING
@@ -851,10 +1047,17 @@
 	&& length(srcrefs) > ind
 	&& !isNull(result = VECTOR_ELT(srcrefs, ind))
 	&& TYPEOF(result) == INTSXP
-	&& length(result) >= 6)
-	return result;
-    else
-	return R_NilValue;
+	&& length(result) >= 6) {
+
+#ifdef R_CODE_COVERAGE
+    	if (R_Code_Coverage) record_code_coverage(result);
+#endif
+
+    } else {
+    	result = R_NilValue;
+    }
+
+    return result;
 }
 
 SEXP applyClosure(SEXP call, SEXP op, SEXP arglist, SEXP rho, SEXP suppliedenv)

From S.Ellison at LGCGroup.com  Sun Mar  9 02:04:32 2014
From: S.Ellison at LGCGroup.com (S Ellison)
Date: Sun, 9 Mar 2014 01:04:32 +0000
Subject: [Rd] Maintainer NOTE in R CMD Check
In-Reply-To: <42143135B5270A45996065C2EFFC2B59116D9AC4@MBX39.exg5.exghost.com>
References: <42143135B5270A45996065C2EFFC2B59116D9AC4@MBX39.exg5.exghost.com>
Message-ID: <A4E5A0B016B8CB41A485FC629B633CED5BB21CCE07@GOLD.corp.lgc-group.com>

Using R 3.0.3 and Rtools 31, I now see a Note (reproduced on my R-forge checks) of the form

* checking CRAN incoming feasibility ... NOTE
Maintainer: 'firstname A B lastname '

where A and B are middle initials.

I can change to a 'firstname lastname' form or 'INITS lastname' form and that removes the above Note*, but I then get a Note warning of maintainer change.

Is either Note going to get in the way of CRAN submission? (And if one of them will, which one?)

S Ellison

*A minor aside: I couldn't find any documented reason for that, or indeed any restriction on the format of a maintaner's name other than 'name first email second in <>'; perhaps I missed something there?

*******************************************************************
This email and any attachments are confidential. Any use...{{dropped:8}}


From mdsumner at gmail.com  Sun Mar  9 06:29:41 2014
From: mdsumner at gmail.com (Michael Sumner)
Date: Sun, 9 Mar 2014 16:29:41 +1100
Subject: [Rd] Maintainer NOTE in R CMD Check
In-Reply-To: <A4E5A0B016B8CB41A485FC629B633CED5BB21CCE07@GOLD.corp.lgc-group.com>
References: <42143135B5270A45996065C2EFFC2B59116D9AC4@MBX39.exg5.exghost.com>
	<A4E5A0B016B8CB41A485FC629B633CED5BB21CCE07@GOLD.corp.lgc-group.com>
Message-ID: <CAAcGz98wCg0b6DkmdjDFHBuOuB=3bUFqdksZncUuwXYgBC24YA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140309/7bf44b82/attachment.pl>

From ligges at statistik.tu-dortmund.de  Sun Mar  9 12:23:45 2014
From: ligges at statistik.tu-dortmund.de (Uwe Ligges)
Date: Sun, 09 Mar 2014 12:23:45 +0100
Subject: [Rd] Maintainer NOTE in R CMD Check
In-Reply-To: <CAAcGz98wCg0b6DkmdjDFHBuOuB=3bUFqdksZncUuwXYgBC24YA@mail.gmail.com>
References: <42143135B5270A45996065C2EFFC2B59116D9AC4@MBX39.exg5.exghost.com>	<A4E5A0B016B8CB41A485FC629B633CED5BB21CCE07@GOLD.corp.lgc-group.com>
	<CAAcGz98wCg0b6DkmdjDFHBuOuB=3bUFqdksZncUuwXYgBC24YA@mail.gmail.com>
Message-ID: <531C4F41.7080602@statistik.tu-dortmund.de>



On 09.03.2014 06:29, Michael Sumner wrote:
> I believe it's just a flag to notify that the author string has changed.
> You might need to send a confirmation email that yes you did mean to
> change, it is just a double check for both ends. I changed my email for a
> package once for some reason and made the confirmation.
>
> Cheers, Mike
> On 9 Mar 2014 12:06, "S Ellison" <S.Ellison at lgcgroup.com> wrote:
>
>> Using R 3.0.3 and Rtools 31, I now see a Note (reproduced on my R-forge
>> checks) of the form
>>
>> * checking CRAN incoming feasibility ... NOTE
>> Maintainer: 'firstname A B lastname '

This is just a Note that reminds CRAN maintainers to check that the 
submission comes actually from his maintainer and not anybody else.

Best,
Uwe Ligges


>>
>> where A and B are middle initials.
>>
>> I can change to a 'firstname lastname' form or 'INITS lastname' form and
>> that removes the above Note*, but I then get a Note warning of maintainer
>> change.
>>
>> Is either Note going to get in the way of CRAN submission? (And if one of
>> them will, which one?)
>>
>> S Ellison
>>
>> *A minor aside: I couldn't find any documented reason for that, or indeed
>> any restriction on the format of a maintaner's name other than 'name first
>> email second in <>'; perhaps I missed something there?
>>
>> *******************************************************************
>> This email and any attachments are confidential. Any u...{{dropped:10}}
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From htl10 at users.sourceforge.net  Thu Mar  6 20:32:51 2014
From: htl10 at users.sourceforge.net (Hin-Tak Leung)
Date: Thu, 06 Mar 2014 19:32:51 +0000
Subject: [Rd] 2 bugs in R's grDevices, and fixes.
In-Reply-To: <52CAC16A.3000509@users.sourceforge.net>
References: <52CAC16A.3000509@users.sourceforge.net>
Message-ID: <5318CD63.3070005@users.sourceforge.net>

Since R 3.0.3 is just around the corner and the problem is still there, here is 
a repost from around the new year.

Hin-Tak Leung wrote:
> Just before the holiday, I asked the freetype developers what is the context
> of these two comments about freetype in the code of R's grDevices:
>
> ======= R/src/library/grDevices/src/cairo/cairoFns.c around line 720 =====
>          /* some FreeType versions have broken index support,
>             fall back to index 0 */
>          if (!FT_New_Face(ft_library,
>                   (const char *) file, index, &face) ||
>              (index && !FT_New_Face(ft_library,
>                         (const char *) file, 0, &face))) {
>              FcFontSetDestroy (fs);
>
> #ifdef __APPLE__
>              /* FreeType is broken on OS X in that face index
>                 is often wrong (unfortunately even for Helvetica!)
>                 - we try to find the best match through enumeration.
>                 And italic and bold are swapped */
>              if (style == 2) style = 1; else if (style == 1) style = 2;
> ==========================================================================
>
> It turned out that the R developers are accumulating a few mistakes,
> which over the years gradually "somewhat" cancel each others out, at
> the expenses of other applications misbehaving on Mac OS X.
>
> Freetype is shipped with, and part of Mac OS X.
> The bottom line is that there are two ways of building freetype on Mac OS X,
> one against the Carbon API (i.e. Mac OS Classic), and the other against
> the POSIX API(i.e. modern unix systems). Apple have always shipped the
> Carbon build.
>
> The "problem", or "broken" issues experienced by R users are because
> the official binary of R has a private copy of freetype, and also
> that is built differently from Apple's. The two copies both read and write
> the public fontconfig's cache for fonts' metadata, with unpredictable
> results to any application that uses fonts - which basically means every
> GUI-based application.
>
> The two builds differ by the "--with-old-mac-fonts" option, which
> "allow Mac resource-based fonts to be used". They differ also on treatments
> of dfonts (an intermediate font format from the transition of
> Classic to X). Helvetica, Times, Courier, Geneva, Monaco are still currently
> shipped as dfonts on Mac OS X.
>
> Here are the two correct ways of doing things:
>
> 1. put either build in /usr/local, and use just one of it dynamically,
>     for everything.
>
> 2. if a private copy of freetype needs to be bundled, and that this
>     private copy interacts with the public font cache, it has to be
>     a Carbon build.
>
> The freetype developers also kindly offer a 3rd option
> - e2d2b1544f24413fa62e0c845184b429eb227e9d -
> made on 27th Dec, to make the POSIX build matches the behavior of
> the Carbon build on caching dfonts. For those unfortunate people
> who alternate between using different builds on a daily or even hourly
> basis, forced upon them by application binary builders.
>
> Somewhat related, I have finally gotten round to make two small bundles,
> which replace the small cairo.dll/cairo.so' in the official
> windows or Mac R binaries, to fix quite a few problems with them,
> the first of which was reported almost a year ago. Just move the two
> small files in the official binaries aside and replace them by these:
>

http://sourceforge.net/projects/outmodedbonsai/files/R/

> R-2.15.3-library-grDevices-libs-winCairo.zip
> R.framework-Versions-2.15-Resources-library-grDevices-libs-cairo.tgz
>
> The windows dlls had been tested and verified to fix the original
> problem. BTW, it is quite a challenge to try to plot
> R graphs with non-latin texts using English windows. (also posted).
> I'll post the test results for Mac OS X when I get round to it. Those
> should be easier to do.
>
> YMMV. The move/rename can always be reverted if things don't work out.
>
> The code mentioned above at the begining should be removed (and was
> removed before building the bundle). It penalizes some mac R users who
> build R himself/herself to end up with a broken build.


From john.laing at gmail.com  Thu Mar  6 16:56:55 2014
From: john.laing at gmail.com (John Laing)
Date: Thu, 6 Mar 2014 10:56:55 -0500
Subject: [Rd] Bundling vendor library inside a package
Message-ID: <CAA3Wa=uC9-bJ2M8Fyo0ia_gSaBZgNiYaiW4=0pHC0RYLrttaFg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140306/119e1b10/attachment.pl>

From Mike.Beddo at dataventures.com  Thu Mar  6 15:34:57 2014
From: Mike.Beddo at dataventures.com (Mike Beddo)
Date: Thu, 6 Mar 2014 14:34:57 +0000
Subject: [Rd] AIX 7.1 and R build problems
Message-ID: <AA01911644A2F84FB4571C323FC45DD5252DDBF9@SNICKERS.dataventures.local>

Has anyone managed to build R-3.0.2 from source on AIX 7.1 using gcc 4.2.0. The configure script finishes with:

...
checking whether wctrans exists and is declared... no 
checking whether iswblank exists and is declared... no 
checking whether wctype exists and is declared... no 
checking whether iswctype exists and is declared... no
configure: error: Support for MBCS locales is required.

Scanning through the config.log the configure script seems to be happy with the C99 compliance at hand. I have tried reading NEWS, README, and R-Help archives but I can't get past this. I'm all Google'd out. Is my compiler too old?

_____________________________________________________________________
Michael Beddo
Senior Scientist

Data Ventures, Inc.
1475 Central Ave. Suite 230  |  Los Alamos, NM 87544 tel  505.695.2132 http://www.dataventures.com  |  "Advanced - Effective - Actionable - Proven. Analytics"


From edd at debian.org  Sun Mar  9 15:36:24 2014
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 9 Mar 2014 09:36:24 -0500
Subject: [Rd] Bundling vendor library inside a package
In-Reply-To: <CAA3Wa=uC9-bJ2M8Fyo0ia_gSaBZgNiYaiW4=0pHC0RYLrttaFg@mail.gmail.com>
References: <CAA3Wa=uC9-bJ2M8Fyo0ia_gSaBZgNiYaiW4=0pHC0RYLrttaFg@mail.gmail.com>
Message-ID: <21276.31848.47700.563674@max.nulle.part>


On 6 March 2014 at 10:56, John Laing wrote:
| If I have a vendor library (say libvendor.so) that I want my package to
| link to, what is the "right way" to bundle that inside my package? My ideal
| outcome would be that the package can be installed from tarball without
| having to worry about installing anything to /usr/lib or tweaking some
| systemwide variables. This isn't for public distribution so I'm not worried
| about CRAN restrictions or licensing issues. I'm running R 3.0.2 on Linux.
| 
| I've put the library in the inst directory of my package, and provided
| PKG_LIBS=-L../inst -lvendor. That works for compilation but then I need to
| provide LD_LIBRARY_PATH for R to find the library at runtime.

As it is a local package, you could use the 'rpath' directive to encode the
path to you vendor library in the shared library object which your package
creates.  

>From http://tldp.org/HOWTO/Program-Library-HOWTO/shared-libraries.html

   One link option you might use is ld's ``rpath'' option, which specifies
   the runtime library search path of that particular program being
   compiled. From gcc, you can invoke the rpath option by specifying it this
   way: 
      -Wl,-rpath,$(DEFAULT_LIB_INSTALL_PATH)
   If you use this option when building the library client program, you don't
   need to bother with LD_LIBRARY_PATH (described next) other than to ensure
   it's not conflicting, or using other techniques to hide the library. 

It work as long as you never change the underlying path. Which is a fine
assumption as long you use the same R installation path -- which is quite
likely. 

Dirk

-- 
Dirk Eddelbuettel | edd at debian.org | http://dirk.eddelbuettel.com


From h.wickham at gmail.com  Mon Mar 10 16:36:41 2014
From: h.wickham at gmail.com (Hadley Wickham)
Date: Mon, 10 Mar 2014 10:36:41 -0500
Subject: [Rd] Maintainer NOTE in R CMD Check
In-Reply-To: <531C4F41.7080602@statistik.tu-dortmund.de>
References: <42143135B5270A45996065C2EFFC2B59116D9AC4@MBX39.exg5.exghost.com>
	<A4E5A0B016B8CB41A485FC629B633CED5BB21CCE07@GOLD.corp.lgc-group.com>
	<CAAcGz98wCg0b6DkmdjDFHBuOuB=3bUFqdksZncUuwXYgBC24YA@mail.gmail.com>
	<531C4F41.7080602@statistik.tu-dortmund.de>
Message-ID: <CABdHhvEKc64WUotD460kbGf1UkFBxrBKHcc2nC+xdfhixL+V4A@mail.gmail.com>

>>> * checking CRAN incoming feasibility ... NOTE
>>> Maintainer: 'firstname A B lastname '
>
>
> This is just a Note that reminds CRAN maintainers to check that the
> submission comes actually from his maintainer and not anybody else.

How does CRAN do that?  Seems like a challenging problem given an
anonymous web form upload.

Hadley

-- 
http://had.co.nz/


From csardi.gabor at gmail.com  Mon Mar 10 16:42:41 2014
From: csardi.gabor at gmail.com (=?ISO-8859-1?B?R+Fib3IgQ3PhcmRp?=)
Date: Mon, 10 Mar 2014 11:42:41 -0400
Subject: [Rd] Maintainer NOTE in R CMD Check
In-Reply-To: <CABdHhvEKc64WUotD460kbGf1UkFBxrBKHcc2nC+xdfhixL+V4A@mail.gmail.com>
References: <42143135B5270A45996065C2EFFC2B59116D9AC4@MBX39.exg5.exghost.com>
	<A4E5A0B016B8CB41A485FC629B633CED5BB21CCE07@GOLD.corp.lgc-group.com>
	<CAAcGz98wCg0b6DkmdjDFHBuOuB=3bUFqdksZncUuwXYgBC24YA@mail.gmail.com>
	<531C4F41.7080602@statistik.tu-dortmund.de>
	<CABdHhvEKc64WUotD460kbGf1UkFBxrBKHcc2nC+xdfhixL+V4A@mail.gmail.com>
Message-ID: <CABtg=KkaQ68FB+8dR88XOy2-1q7T+wKySCtt22nKhQMOFbTSMg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140310/33c52281/attachment.pl>

From Philippe.GROSJEAN at umons.ac.be  Mon Mar 10 16:53:56 2014
From: Philippe.GROSJEAN at umons.ac.be (Philippe GROSJEAN)
Date: Mon, 10 Mar 2014 15:53:56 +0000
Subject: [Rd] Maintainer NOTE in R CMD Check
In-Reply-To: <CABtg=KkaQ68FB+8dR88XOy2-1q7T+wKySCtt22nKhQMOFbTSMg@mail.gmail.com>
References: <42143135B5270A45996065C2EFFC2B59116D9AC4@MBX39.exg5.exghost.com>
	<A4E5A0B016B8CB41A485FC629B633CED5BB21CCE07@GOLD.corp.lgc-group.com>
	<CAAcGz98wCg0b6DkmdjDFHBuOuB=3bUFqdksZncUuwXYgBC24YA@mail.gmail.com>
	<531C4F41.7080602@statistik.tu-dortmund.de>
	<CABdHhvEKc64WUotD460kbGf1UkFBxrBKHcc2nC+xdfhixL+V4A@mail.gmail.com>
	<CABtg=KkaQ68FB+8dR88XOy2-1q7T+wKySCtt22nKhQMOFbTSMg@mail.gmail.com>
Message-ID: <765086C7-4D36-4E0E-B117-0C9A0C4B1E94@umons.ac.be>

Actually, you have to send an email at CRAN at r-project.org with the title "submission packageXXX versionYYY" send from the same email address that the one in the maintainer field.

Philippe


On 10 Mar 2014, at 16:42, G?bor Cs?rdi <csardi.gabor at gmail.com> wrote:

> On Mon, Mar 10, 2014 at 11:36 AM, Hadley Wickham <h.wickham at gmail.com>wrote:
> 
>>>>> * checking CRAN incoming feasibility ... NOTE
>>>>> Maintainer: 'firstname A B lastname '
>>> 
>>> 
>>> This is just a Note that reminds CRAN maintainers to check that the
>>> submission comes actually from his maintainer and not anybody else.
>> 
>> How does CRAN do that?  Seems like a challenging problem given an
>> anonymous web form upload.
>> 
> 
> According to my memories, they send an email to the maintainer's email
> address to confirm the upload. If the email address has changed, they send
> an email to both the new and old addresses.
> 
> If the old address does not exist any more, then it is tricky indeed, and I
> am not sure what you can do in this case.
> 
> Gabor
> 
> 
>> 
>> Hadley
>> 
>> --
>> http://had.co.nz/
>> 
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 


From edzer.pebesma at uni-muenster.de  Mon Mar 10 17:35:17 2014
From: edzer.pebesma at uni-muenster.de (Edzer Pebesma)
Date: Mon, 10 Mar 2014 17:35:17 +0100
Subject: [Rd] Maintainer NOTE in R CMD Check
In-Reply-To: <CABtg=KkaQ68FB+8dR88XOy2-1q7T+wKySCtt22nKhQMOFbTSMg@mail.gmail.com>
References: <42143135B5270A45996065C2EFFC2B59116D9AC4@MBX39.exg5.exghost.com>	<A4E5A0B016B8CB41A485FC629B633CED5BB21CCE07@GOLD.corp.lgc-group.com>	<CAAcGz98wCg0b6DkmdjDFHBuOuB=3bUFqdksZncUuwXYgBC24YA@mail.gmail.com>	<531C4F41.7080602@statistik.tu-dortmund.de>	<CABdHhvEKc64WUotD460kbGf1UkFBxrBKHcc2nC+xdfhixL+V4A@mail.gmail.com>
	<CABtg=KkaQ68FB+8dR88XOy2-1q7T+wKySCtt22nKhQMOFbTSMg@mail.gmail.com>
Message-ID: <531DE9C5.6080708@uni-muenster.de>



On 03/10/2014 04:42 PM, G?bor Cs?rdi wrote:
> On Mon, Mar 10, 2014 at 11:36 AM, Hadley Wickham <h.wickham at gmail.com>wrote:
> 
>>>>> * checking CRAN incoming feasibility ... NOTE
>>>>> Maintainer: 'firstname A B lastname '
>>>
>>>
>>> This is just a Note that reminds CRAN maintainers to check that the
>>> submission comes actually from his maintainer and not anybody else.
>>
>> How does CRAN do that?  Seems like a challenging problem given an
>> anonymous web form upload.
>>
> 
> According to my memories, they send an email to the maintainer's email
> address to confirm the upload. If the email address has changed, they send
> an email to both the new and old addresses.
> 
> If the old address does not exist any more, then it is tricky indeed, and I
> am not sure what you can do in this case.
> 
> Gabor

Besides the effort it would involve, are there reasons why CRAN does not
use a stronger package author identification mechanism, like web of
trust (which debian uses) or public key infrastructure, and require
packages to be signed?


> 
> 
>>
>> Hadley
>>
>> --
>> http://had.co.nz/
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 

-- 
Edzer Pebesma
Institute for Geoinformatics (ifgi), University of M?nster
Heisenbergstra?e 2, 48149 M?nster, Germany. Phone: +49 251
83 33081 http://ifgi.uni-muenster.de GPG key ID 0xAC227795

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 490 bytes
Desc: OpenPGP digital signature
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140310/87571f6a/attachment.bin>

From peter.meilstrup at gmail.com  Tue Mar 11 10:53:32 2014
From: peter.meilstrup at gmail.com (Peter Meilstrup)
Date: Tue, 11 Mar 2014 02:53:32 -0700
Subject: [Rd] ddFindVar
Message-ID: <CAJoaRhZryW8N5t+VszZZ2iWnctqiADJOKy4mgdeGLeO0Oy47EQ@mail.gmail.com>

Hi all,

Is there any particular reason Rf_ddfindVar is not exposed in Rinternals.h?

Peter


From ross at biostat.ucsf.edu  Wed Mar 12 22:34:15 2014
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Wed, 12 Mar 2014 14:34:15 -0700
Subject: [Rd] 2 versions of same library loaded
Message-ID: <1394660055.25697.92.camel@localhost>

Can anyone help me understand how I got 2 versions of the same library
loaded, how to prevent it, and what the consequences are?  Running under
Debian GNU/Linux squeeze.

lsof and /proc/xxx/map both show 2 copies of several libraries loaded:
/home/ross/install/lib/libmpi.so.1.3.0
/home/ross/install/lib/libopen-pal.so.6.1.0
/home/ross/install/lib/libopen-rte.so.7.0.0
/home/ross/Rlib-3.0.1/Rmpi/libs/Rmpi.so
/usr/lib/openmpi/lib/libmpi.so.0.0.2
/usr/lib/openmpi/lib/libopen-pal.so.0.0.0
/usr/lib/openmpi/lib/libopen-rte.so.0.0.0
/usr/lib/R/lib/libR.so


The system has the old version of MPI installed under /usr/lib.  I built
a personal, newer copy in my directory, and then rebuilt Rmpi (an R
package) against it.  ldd on the personal Rmpi.so and libmpi.so shows
all references to mpi libraries on personal paths.

R was installed from a debian package, and presumably compiled without
having MPI around.  Before running I set LD_LIBRARY_PATH to
~/install/lib, and then stuffed the same path at the start of
LD_LIBRARY_PATH using Sys.setenv in my profile because R seems to
prepend some libraries to that path when it starts (I'm curious about
that too).  I also prepended ~/install/bin to my path, though I'm not
sure that's relevant.

Does R use ld.so or some other mechanism for loading libraries?

Can I assume the highest version number of a library will be preferred?
http://cran.r-project.org/doc/manuals/r-devel/R-exts.html#index-Dynamic-loading says "If a shared object/DLL is loaded more than once the most recent version is used."  I'm not sure if "most recent" means the one loaded most recently by the program (I don't know which that is) or "highest version number."

Why is /usr/lib/openmpi being looked at in the first place?

How can I stop the madness?  Some folks on the openmpi list have
indicated I need to rebuild R, telling it where my MPI is, but that
seems an awfully big hammer for the problem.

Thanks.
Ross Boylan


From simon.urbanek at r-project.org  Thu Mar 13 03:50:52 2014
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Wed, 12 Mar 2014 22:50:52 -0400
Subject: [Rd] 2 versions of same library loaded
In-Reply-To: <1394660055.25697.92.camel@localhost>
References: <1394660055.25697.92.camel@localhost>
Message-ID: <E236A681-5D73-478E-8FB2-3B04837B00DA@r-project.org>

Ross,

On Mar 12, 2014, at 5:34 PM, Ross Boylan <ross at biostat.ucsf.edu> wrote:

> Can anyone help me understand how I got 2 versions of the same library
> loaded, how to prevent it, and what the consequences are?  Running under
> Debian GNU/Linux squeeze.
> 
> lsof and /proc/xxx/map both show 2 copies of several libraries loaded:
> /home/ross/install/lib/libmpi.so.1.3.0
> /home/ross/install/lib/libopen-pal.so.6.1.0
> /home/ross/install/lib/libopen-rte.so.7.0.0
> /home/ross/Rlib-3.0.1/Rmpi/libs/Rmpi.so
> /usr/lib/openmpi/lib/libmpi.so.0.0.2
> /usr/lib/openmpi/lib/libopen-pal.so.0.0.0
> /usr/lib/openmpi/lib/libopen-rte.so.0.0.0
> /usr/lib/R/lib/libR.so
> 
> 
> The system has the old version of MPI installed under /usr/lib.  I built
> a personal, newer copy in my directory, and then rebuilt Rmpi (an R
> package) against it.  ldd on the personal Rmpi.so and libmpi.so shows
> all references to mpi libraries on personal paths.
> 
> R was installed from a debian package, and presumably compiled without
> having MPI around.  Before running I set LD_LIBRARY_PATH to
> ~/install/lib, and then stuffed the same path at the start of
> LD_LIBRARY_PATH using Sys.setenv in my profile because R seems to
> prepend some libraries to that path when it starts (I'm curious about
> that too).  I also prepended ~/install/bin to my path, though I'm not
> sure that's relevant.
> 
> Does R use ld.so or some other mechanism for loading libraries?
> 

R uses dlopen to load package libraries - it is essentially identical to using ld.so for dependencies.


> Can I assume the highest version number of a library will be preferred?

No.


> http://cran.r-project.org/doc/manuals/r-devel/R-exts.html#index-Dynamic-loading says "If a shared object/DLL is loaded more than once the most recent version is used."  I'm not sure if "most recent" means the one loaded most recently by the program (I don't know which that is) or "highest version number."
> 

The former - whichever you load last wins. Note, however, that this refers to explicitly loaded objects since they are loaded into a flat namespace so a load will overwrite all symbols that get loaded.


> Why is /usr/lib/openmpi being looked at in the first place?
> 

You'll have to consult your system. The search path (assuming rpath is not involved) is governed by LD_LIBRARY_PATH and /etc/ld.so.conf*. Note that LD_LIBRARY_PATH is consulted at the time of the resolution (when the library is looked up), so you may be changing it too late. Also note that you have to expand ~ in the path (it's not a valid path, it's a shell expansion feature).

R's massaging of the LD_LIBRARY_PATH is typically done in $R_HOME/etc/ldpaths so you may want to check it and/or adjust it as needed. Normally (in stock R), it only prepends its own libraries and Java so it should not be causing any issues, but you may want to check in case Debian scripts add anything else.


> How can I stop the madness?  Some folks on the openmpi list have
> indicated I need to rebuild R, telling it where my MPI is, but that
> seems an awfully big hammer for the problem.
> 

I would check LD_LIBRARY_PATH and also check at which point are those old libraries loaded to find where they are referenced.

Cheers,
Simon


> Thanks.
> Ross Boylan
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 


From ross at biostat.ucsf.edu  Thu Mar 13 06:00:56 2014
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Wed, 12 Mar 2014 22:00:56 -0700
Subject: [Rd] 2 versions of same library loaded
In-Reply-To: <E236A681-5D73-478E-8FB2-3B04837B00DA@r-project.org>
References: <1394660055.25697.92.camel@localhost>
	<E236A681-5D73-478E-8FB2-3B04837B00DA@r-project.org>
Message-ID: <1394686856.15874.16.camel@localhost>

Comments/questions interspersed below.
On Wed, 2014-03-12 at 22:50 -0400, Simon Urbanek wrote:
> Ross,
> 
> On Mar 12, 2014, at 5:34 PM, Ross Boylan <ross at biostat.ucsf.edu> wrote:
> 
> > Can anyone help me understand how I got 2 versions of the same library
> > loaded, how to prevent it, and what the consequences are?  Running under
> > Debian GNU/Linux squeeze.
> > 
> > lsof and /proc/xxx/map both show 2 copies of several libraries loaded:
> > /home/ross/install/lib/libmpi.so.1.3.0
> > /home/ross/install/lib/libopen-pal.so.6.1.0
> > /home/ross/install/lib/libopen-rte.so.7.0.0
> > /home/ross/Rlib-3.0.1/Rmpi/libs/Rmpi.so
> > /usr/lib/openmpi/lib/libmpi.so.0.0.2
> > /usr/lib/openmpi/lib/libopen-pal.so.0.0.0
> > /usr/lib/openmpi/lib/libopen-rte.so.0.0.0
> > /usr/lib/R/lib/libR.so
> > 
> > 
> > The system has the old version of MPI installed under /usr/lib.  I built
> > a personal, newer copy in my directory, and then rebuilt Rmpi (an R
> > package) against it.  ldd on the personal Rmpi.so and libmpi.so shows
> > all references to mpi libraries on personal paths.
> > 
> > R was installed from a debian package, and presumably compiled without
> > having MPI around.  Before running I set LD_LIBRARY_PATH to
> > ~/install/lib, and then stuffed the same path at the start of
> > LD_LIBRARY_PATH using Sys.setenv in my profile because R seems to
> > prepend some libraries to that path when it starts (I'm curious about
> > that too).  I also prepended ~/install/bin to my path, though I'm not
> > sure that's relevant.
> > 
> > Does R use ld.so or some other mechanism for loading libraries?
> > 
> 
> R uses dlopen to load package libraries - it is essentially identical to using ld.so for dependencies.
> 
> 
> > Can I assume the highest version number of a library will be preferred?
> 
> No.
> 
Bummer.  The fact that Rmpi is not crashing suggests to me it's using
the right version of the mpi libraries (it does produce lots of errors
if I run it without setting LD_LIBRARY_PATH so only the system mpi libs
are in play), but it would be nice to be certain.  Or the 2 versions
could be combined in a big mess.
> 
> > http://cran.r-project.org/doc/manuals/r-devel/R-exts.html#index-Dynamic-loading says "If a shared object/DLL is loaded more than once the most recent version is used."  I'm not sure if "most recent" means the one loaded most recently by the program (I don't know which that is) or "highest version number."
> > 
> 
> The former - whichever you load last wins. Note, however, that this refers to explicitly loaded objects since they are loaded into a flat namespace so a load will overwrite all symbols that get loaded.
It might be good to clarify that in the manual.

If I understand the term, the mpi libraries are loaded implicitly; that
is, Rmpi.so is loaded explicitly, and then it pulls in dependencies.
What are the rules in that case? 

> 
> 
> > Why is /usr/lib/openmpi being looked at in the first place?
> > 
> 
> You'll have to consult your system. The search path (assuming rpath is not involved) is governed by LD_LIBRARY_PATH and /etc/ld.so.conf*. Note that LD_LIBRARY_PATH is consulted at the time of the resolution (when the library is looked up), so you may be changing it too late. Also note that you have to expand ~ in the path (it's not a valid path, it's a shell expansion feature).
> 
I just used the ~ as a shortcut; the shell expanded it and the full path
ended up in the variable.

I assume the loader checks LD_LIBRARY_PATH first; once it finds the mpi
libraries there I don't know why it keeps looking.

I'm not sure I follow the part about too late, but is it this?: all the
R's launched under MPI have the MPI library loaded automatically.   If
that happens before my profile is read, reseting LD_LIBRARY_PATH will be
irrelevant.  I don't know whether the profile or Rmpi load happens
first.

The reseting is just a reordering, and since the  other elements in
LD_LIBRARY_PATH don't have any mpi libraries I don't think the order
matters.


> R's massaging of the LD_LIBRARY_PATH is typically done in $R_HOME/etc/ldpaths so you may want to check it and/or adjust it as needed. Normally (in stock R), it only prepends its own libraries and Java so it should not be causing any issues, but you may want to check in case Debian scripts add anything else.
> 
The extra paths are limited as you describe, and so are probably no
threat for loading the wrong MPI library
(/usr/lib64/R/lib:/usr/lib/jvm/java-6-openjdk/jre/lib/amd64/server).
> 
> > How can I stop the madness?  Some folks on the openmpi list have
> > indicated I need to rebuild R, telling it where my MPI is, but that
> > seems an awfully big hammer for the problem.
> > 
> 
> I would check LD_LIBRARY_PATH and also check at which point are those old libraries loaded to find where they are referenced.
How do I tell the point at which the old libraries are loaded?  I assume
it happens implicitly when Rmpi is loaded, but I don't know which of the
2 versions of the libraries is loaded first, and I don't know how to
tell.

Thanks for your help.
Ross Boylan


From lubinbin220 at gmail.com  Thu Mar 13 15:24:46 2014
From: lubinbin220 at gmail.com (binbin lu)
Date: Thu, 13 Mar 2014 14:24:46 +0000
Subject: [Rd] Fwd: C code and DLL file in the package
In-Reply-To: <CANuoaEEqK2WoxVj8-OHQfxkHGvBqmx3N6=OdjfBvVudZKS9ipA@mail.gmail.com>
References: <CANuoaEEqK2WoxVj8-OHQfxkHGvBqmx3N6=OdjfBvVudZKS9ipA@mail.gmail.com>
Message-ID: <CANuoaEGsw4WNM2kJyj7JyKqMM+UmfevmFzNRGkpmPzHdnhtSKg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140313/9311b330/attachment.pl>

From murdoch.duncan at gmail.com  Thu Mar 13 15:57:14 2014
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Thu, 13 Mar 2014 10:57:14 -0400
Subject: [Rd] Fwd: C code and DLL file in the package
In-Reply-To: <CANuoaEGsw4WNM2kJyj7JyKqMM+UmfevmFzNRGkpmPzHdnhtSKg@mail.gmail.com>
References: <CANuoaEEqK2WoxVj8-OHQfxkHGvBqmx3N6=OdjfBvVudZKS9ipA@mail.gmail.com>
	<CANuoaEGsw4WNM2kJyj7JyKqMM+UmfevmFzNRGkpmPzHdnhtSKg@mail.gmail.com>
Message-ID: <5321C74A.7030500@gmail.com>

On 13/03/2014 10:24 AM, binbin lu wrote:
> Hi,
>
> I am trying to compile a package, in which C code is used.
>
> When I was checking the package with the dll file under the sub-folder
> "SRC", a WARNING msg was returned:
> "Source packages should not contain undeclared executable files."
>
> When I exclued it from the folder, I got an ERROR msg:
>
> Error in library.dynam(lib, package, package.lib) :
>    DLL 'Cs2g' not found: maybe not installed for this architecture?
>
>
> Anyone knows how to solve this problem? Sorry for this naive question but I
> am really confused. Many thanks.

Put the C code for the DLL in "src" (not "SRC").  R will compile it for 
the platform where your package is being installed.

Duncan Murdoch


From edd at debian.org  Thu Mar 13 15:58:43 2014
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 13 Mar 2014 09:58:43 -0500
Subject: [Rd] Fwd: C code and DLL file in the package
In-Reply-To: <CANuoaEGsw4WNM2kJyj7JyKqMM+UmfevmFzNRGkpmPzHdnhtSKg@mail.gmail.com>
References: <CANuoaEEqK2WoxVj8-OHQfxkHGvBqmx3N6=OdjfBvVudZKS9ipA@mail.gmail.com>
	<CANuoaEGsw4WNM2kJyj7JyKqMM+UmfevmFzNRGkpmPzHdnhtSKg@mail.gmail.com>
Message-ID: <21281.51107.473292.800346@max.nulle.part>


Binbin,

On 13 March 2014 at 14:24, binbin lu wrote:
| I am trying to compile a package, in which C code is used.
| 
| When I was checking the package with the dll file under the sub-folder
| "SRC", a WARNING msg was returned:
| "Source packages should not contain undeclared executable files."
| 
| When I exclued it from the folder, I got an ERROR msg:
| 
| Error in library.dynam(lib, package, package.lib) :
|   DLL 'Cs2g' not found: maybe not installed for this architecture?
| 
| 
| Anyone knows how to solve this problem? Sorry for this naive question but I
| am really confused. Many thanks.

I see two cases here:

Case 1:  You intend to upload this package to CRAN

         --> Don't include the dll in the first place. See 'Writing R
             Extensions' for details.

Case 2:  You only plan to do use this locally.

         --> Learn more about package builds. See 'Writing R Extensions' for
             details, as well as 'R CMD INSTALL --help' and pay attention to
             single architecture builds.

             The Visual Studio tool chain is not officially supported by R,
             so you are unlikely to get specific help. You mix this, but it 
             is an advanced topic, and it is somewhat outside the supported
             range of endeavours with R, you are on your own.

             Google for 'mixing MinGW gcc and Visual Studio' and other topics.

Good luck,  Dirk

-- 
Dirk Eddelbuettel | edd at debian.org | http://dirk.eddelbuettel.com


From wdunlap at tibco.com  Thu Mar 13 16:21:26 2014
From: wdunlap at tibco.com (William Dunlap)
Date: Thu, 13 Mar 2014 15:21:26 +0000
Subject: [Rd] 2 versions of same library loaded
In-Reply-To: <1394686856.15874.16.camel@localhost>
References: <1394660055.25697.92.camel@localhost>
	<E236A681-5D73-478E-8FB2-3B04837B00DA@r-project.org>
	<1394686856.15874.16.camel@localhost>
Message-ID: <E66794E69CFDE04D9A70842786030B933FA936DE@PA-MBX01.na.tibco.com>

Have you tried using the environment variable LD_DEBUG to
see what the dynamic linker is doing?  E.g.,

  env LD_DEBUG=files R
or
  % setenv RHOME `R RHOME`
  % R CMD env LD_DEBUG=files $RHOME/bin/exec/R
      2322:
      2322:     file=libR.so [0];  needed by /opt/sw/R/R-3.0.2.atlas1/lib/R/bin/exec/R [0]
      2322:     file=libR.so [0];  generating link map
      2322:       dynamic: 0x00007f484e2da0b8  base: 0x00007f484de30000   size: 0x00000000005a0c60
      2322:         entry: 0x00007f484de59e60  phdr: 0x00007f484de30040  phnum:                  7
      2322:
      2322:
      2322:     file=libpthread.so.0 [0];  needed by /opt/sw/R/R-3.0.2.atlas1/lib/R/bin/exec/R [0]
      2322:     file=libpthread.so.0 [0];  generating link map
      2322:       dynamic: 0x00007f484de2ad88  base: 0x00007f484dc13000   size: 0x000000000021c438
      2322:         entry: 0x00007f484dc19c50  phdr: 0x00007f484dc13040  phnum:                  9
      2322:
      2322:
      2322:     file=libc.so.6 [0];  needed by /opt/sw/R/R-3.0.2.atlas1/lib/R/bin/exec/R [0]
      2322:     file=libc.so.6 [0];  generating link map
      2322:       dynamic: 0x00007f484dc0bb40  base: 0x00007f484d870000   size: 0x00000000003a2368
      2322:         entry: 0x00007f484d891420  phdr: 0x00007f484d870040  phnum:                 10
      ...
      > library(lme4)
      ... lots more messages about shared objects being loaded.

See 'man ld.so' for more information; run 'env LD_DEBUG=help echo foo' for the available options.

> > > Before running I set LD_LIBRARY_PATH to
> > > ~/install/lib, and then stuffed the same path at the start of
> > > LD_LIBRARY_PATH using Sys.setenv in my profile
It has been a long time since I worried about these things, but I thought that changing
LD_LIBRARY_PATH after a process starts has no effect on the dynamic linking done for
the current process - ld.so only looks at LD_LIBRARY_PATH when ls.so starts.

Bill Dunlap
TIBCO Software
wdunlap tibco.com


> -----Original Message-----
> From: r-devel-bounces at r-project.org [mailto:r-devel-bounces at r-project.org] On Behalf
> Of Ross Boylan
> Sent: Wednesday, March 12, 2014 10:01 PM
> To: Simon Urbanek
> Cc: r-devel at r-project.org
> Subject: Re: [Rd] 2 versions of same library loaded
> 
> Comments/questions interspersed below.
> On Wed, 2014-03-12 at 22:50 -0400, Simon Urbanek wrote:
> > Ross,
> >
> > On Mar 12, 2014, at 5:34 PM, Ross Boylan <ross at biostat.ucsf.edu> wrote:
> >
> > > Can anyone help me understand how I got 2 versions of the same library
> > > loaded, how to prevent it, and what the consequences are?  Running under
> > > Debian GNU/Linux squeeze.
> > >
> > > lsof and /proc/xxx/map both show 2 copies of several libraries loaded:
> > > /home/ross/install/lib/libmpi.so.1.3.0
> > > /home/ross/install/lib/libopen-pal.so.6.1.0
> > > /home/ross/install/lib/libopen-rte.so.7.0.0
> > > /home/ross/Rlib-3.0.1/Rmpi/libs/Rmpi.so
> > > /usr/lib/openmpi/lib/libmpi.so.0.0.2
> > > /usr/lib/openmpi/lib/libopen-pal.so.0.0.0
> > > /usr/lib/openmpi/lib/libopen-rte.so.0.0.0
> > > /usr/lib/R/lib/libR.so
> > >
> > >
> > > The system has the old version of MPI installed under /usr/lib.  I built
> > > a personal, newer copy in my directory, and then rebuilt Rmpi (an R
> > > package) against it.  ldd on the personal Rmpi.so and libmpi.so shows
> > > all references to mpi libraries on personal paths.
> > >
> > > R was installed from a debian package, and presumably compiled without
> > > having MPI around.  Before running I set LD_LIBRARY_PATH to
> > > ~/install/lib, and then stuffed the same path at the start of
> > > LD_LIBRARY_PATH using Sys.setenv in my profile because R seems to
> > > prepend some libraries to that path when it starts (I'm curious about
> > > that too).  I also prepended ~/install/bin to my path, though I'm not
> > > sure that's relevant.
> > >
> > > Does R use ld.so or some other mechanism for loading libraries?
> > >
> >
> > R uses dlopen to load package libraries - it is essentially identical to using ld.so for
> dependencies.
> >
> >
> > > Can I assume the highest version number of a library will be preferred?
> >
> > No.
> >
> Bummer.  The fact that Rmpi is not crashing suggests to me it's using
> the right version of the mpi libraries (it does produce lots of errors
> if I run it without setting LD_LIBRARY_PATH so only the system mpi libs
> are in play), but it would be nice to be certain.  Or the 2 versions
> could be combined in a big mess.
> >
> > > http://cran.r-project.org/doc/manuals/r-devel/R-exts.html#index-Dynamic-loading
> says "If a shared object/DLL is loaded more than once the most recent version is used."
> I'm not sure if "most recent" means the one loaded most recently by the program (I don't
> know which that is) or "highest version number."
> > >
> >
> > The former - whichever you load last wins. Note, however, that this refers to explicitly
> loaded objects since they are loaded into a flat namespace so a load will overwrite all
> symbols that get loaded.
> It might be good to clarify that in the manual.
> 
> If I understand the term, the mpi libraries are loaded implicitly; that
> is, Rmpi.so is loaded explicitly, and then it pulls in dependencies.
> What are the rules in that case?
> 
> >
> >
> > > Why is /usr/lib/openmpi being looked at in the first place?
> > >
> >
> > You'll have to consult your system. The search path (assuming rpath is not involved) is
> governed by LD_LIBRARY_PATH and /etc/ld.so.conf*. Note that LD_LIBRARY_PATH is
> consulted at the time of the resolution (when the library is looked up), so you may be
> changing it too late. Also note that you have to expand ~ in the path (it's not a valid path,
> it's a shell expansion feature).
> >
> I just used the ~ as a shortcut; the shell expanded it and the full path
> ended up in the variable.
> 
> I assume the loader checks LD_LIBRARY_PATH first; once it finds the mpi
> libraries there I don't know why it keeps looking.
> 
> I'm not sure I follow the part about too late, but is it this?: all the
> R's launched under MPI have the MPI library loaded automatically.   If
> that happens before my profile is read, reseting LD_LIBRARY_PATH will be
> irrelevant.  I don't know whether the profile or Rmpi load happens
> first.
> 
> The reseting is just a reordering, and since the  other elements in
> LD_LIBRARY_PATH don't have any mpi libraries I don't think the order
> matters.
> 
> 
> > R's massaging of the LD_LIBRARY_PATH is typically done in $R_HOME/etc/ldpaths so
> you may want to check it and/or adjust it as needed. Normally (in stock R), it only
> prepends its own libraries and Java so it should not be causing any issues, but you may
> want to check in case Debian scripts add anything else.
> >
> The extra paths are limited as you describe, and so are probably no
> threat for loading the wrong MPI library
> (/usr/lib64/R/lib:/usr/lib/jvm/java-6-openjdk/jre/lib/amd64/server).
> >
> > > How can I stop the madness?  Some folks on the openmpi list have
> > > indicated I need to rebuild R, telling it where my MPI is, but that
> > > seems an awfully big hammer for the problem.
> > >
> >
> > I would check LD_LIBRARY_PATH and also check at which point are those old libraries
> loaded to find where they are referenced.
> How do I tell the point at which the old libraries are loaded?  I assume
> it happens implicitly when Rmpi is loaded, but I don't know which of the
> 2 versions of the libraries is loaded first, and I don't know how to
> tell.
> 
> Thanks for your help.
> Ross Boylan
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From ross at biostat.ucsf.edu  Thu Mar 13 18:46:31 2014
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Thu, 13 Mar 2014 10:46:31 -0700
Subject: [Rd] 2 versions of same library loaded
In-Reply-To: <E66794E69CFDE04D9A70842786030B933FA936DE@PA-MBX01.na.tibco.com>
References: <1394660055.25697.92.camel@localhost>
	<E236A681-5D73-478E-8FB2-3B04837B00DA@r-project.org>
	<1394686856.15874.16.camel@localhost>
	<E66794E69CFDE04D9A70842786030B933FA936DE@PA-MBX01.na.tibco.com>
Message-ID: <1394732791.15874.53.camel@localhost>

On Thu, 2014-03-13 at 15:21 +0000, William Dunlap wrote:
> Have you tried using the environment variable LD_DEBUG to
> see what the dynamic linker is doing?  E.g.,
> 
>   env LD_DEBUG=files R
> or
That was very helpful.  I'm still having trouble determining exactly
which file is getting loaded, because the trace sometimes does not
include the complete path.  However, judging by calls to init I can see

1. My premise that R had no references to mpi was incorrect.  The logs
show      
24312:     file=libmpi.so.1 [0];  needed by /home/ross/Rlib-3.0.1/Rmpi/libs/Rmpi.so [0]
24312:     find library=libmpi.so.1 [0]; searching
24312:      search path=/usr/lib64/R/lib:/home/ross/install/lib            (LD_LIBRARY_PATH)
24312:       trying file=/usr/lib64/R/lib/libmpi.so.1
24312:       trying file=/home/ross/install/lib/libmpi.so.1

I used 
   env LD_DEBUG='libs files versions' R 
to try to get more clues about what was going on.

2.  There are calls to init for 
calling init: /home/ross/install/lib/libmpi.so.1
calling init: /usr/lib/libmpi.so.0
in that order.

mpi.so.0 is there because
     24312:     file=libmpi.so.0 [0];  needed by /home/ross/Rlib-3.0.1/Rmpi/libs/Rmpi.so [0]
     24312:     find library=libmpi.so.0 [0]; searching
     24312:      search path=/usr/lib64/R/lib:/home/ross/install/lib            (LD_LIBRARY_PATH)
     24312:       trying file=/usr/lib64/R/lib/libmpi.so.0
     24312:       trying file=/home/ross/install/lib/libmpi.so.0
     24312:      search cache=/etc/ld.so.cache
     24312:       trying file=/usr/lib/libmpi.so.0

It seems very odd that the same Rmpi.so is requiring both the old and new libmpi.so (compare to the first 
trace in in point 1).  There is this code in Rmpi.c:
    if (!dlopen("libmpi.so.0", RTLD_GLOBAL | RTLD_LAZY)
        && !dlopen("libmpi.so", RTLD_GLOBAL | RTLD_LAZY)){


So I'm still not sure what it's using, or if there is some mishmash of the 2.

It might be relevant that 
     24312:     calling init: /home/ross/Rlib-3.0.1/Rmpi/libs/Rmpi.so
     24312:
     24312:     opening file=/home/ross/Rlib-3.0.1/Rmpi/libs/Rmpi.so [0]; direct_opencount=1
     24312:
     24312:     /home/ross/Rlib-3.0.1/Rmpi/libs/Rmpi.so: error: symbol lookup error: undefined symbol: R_init_Rmpi (fatal)
     24312:     /home/ross/Rlib-3.0.1/Rmpi/libs/Rmpi.so: error: symbol lookup error: undefined symbol: R_init_Rmpi (fatal)

ldd on Rmpi.so has no mention of mpi.so.0:
$ ldd /home/ross/Rlib-3.0.1/Rmpi/libs/Rmpi.so
        linux-vdso.so.1 =>  (0x00007fff69135000)
        libmpi.so.1 => /home/ross/install/lib/libmpi.so.1 (0x00007f899b579000)
        libR.so => /usr/lib/libR.so (0x00007f899af8c000)
        libc.so.6 => /lib/libc.so.6 (0x00007f899ac29000)
        libibverbs.so.1 => /usr/lib/libibverbs.so.1 (0x00007f899aa1d000)
        libpthread.so.0 => /lib/libpthread.so.0 (0x00007f899a801000)
        libopen-rte.so.7 => /home/ross/install/lib/libopen-rte.so.7 (0x00007f899a518000)
        libtorque.so.2 => /usr/lib/libtorque.so.2 (0x00007f899a208000)
        libopen-pal.so.6 => /home/ross/install/lib/libopen-pal.so.6 (0x00007f8999f34000)
        libdl.so.2 => /lib/libdl.so.2 (0x00007f8999d2f000)
        libnuma.so.1 => /usr/lib/libnuma.so.1 (0x00007f8999b27000)
        libpciaccess.so.0 => /usr/lib/libpciaccess.so.0 (0x00007f899991f000)
        librt.so.1 => /lib/librt.so.1 (0x00007f8999716000)
        libnsl.so.1 => /lib/libnsl.so.1 (0x00007f89994fe000)
        libutil.so.1 => /lib/libutil.so.1 (0x00007f89992fb000)
        libm.so.6 => /lib/libm.so.6 (0x00007f8999078000)
        libblas.so.3gf => /usr/lib/libblas.so.3gf (0x00007f8998b58000)
        libgfortran.so.3 => /usr/lib/libgfortran.so.3 (0x00007f899886c000)
        libreadline.so.6 => /lib/libreadline.so.6 (0x00007f8998627000)
        libgomp.so.1 => /usr/lib/libgomp.so.1 (0x00007f899841a000)
        /lib64/ld-linux-x86-64.so.2 (0x00007f899bc6e000)
        libz.so.1 => /usr/lib/libz.so.1 (0x00007f8998202000)
        libgcc_s.so.1 => /lib/libgcc_s.so.1 (0x00007f8997fec000)
        libncurses.so.5 => /lib/libncurses.so.5 (0x00007f8997da6000)


From ross at biostat.ucsf.edu  Thu Mar 13 19:12:38 2014
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Thu, 13 Mar 2014 11:12:38 -0700
Subject: [Rd] 2 versions of same library loaded
In-Reply-To: <1394732791.15874.53.camel@localhost>
References: <1394660055.25697.92.camel@localhost>
	<E236A681-5D73-478E-8FB2-3B04837B00DA@r-project.org>
	<1394686856.15874.16.camel@localhost>
	<E66794E69CFDE04D9A70842786030B933FA936DE@PA-MBX01.na.tibco.com>
	<1394732791.15874.53.camel@localhost>
Message-ID: <1394734358.15874.66.camel@localhost>

On Thu, 2014-03-13 at 10:46 -0700, Ross Boylan wrote:
> 1. My premise that R had no references to mpi was incorrect.  The logs
> show      
> 24312:     file=libmpi.so.1 [0];  needed
> by /home/ross/Rlib-3.0.1/Rmpi/libs/Rmpi.so [0]
> 24312:     find library=libmpi.so.1 [0]; searching
> 24312:      search path=/usr/lib64/R/lib:/home/ross/install/lib
> (LD_LIBRARY_PATH)
> 24312:       trying file=/usr/lib64/R/lib/libmpi.so.1
> 24312:       trying file=/home/ross/install/lib/libmpi.so.1

Except there is no file /usr/lib64/R/lib/libmpi.so.1

>


From ross at biostat.ucsf.edu  Thu Mar 13 19:40:08 2014
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Thu, 13 Mar 2014 11:40:08 -0700
Subject: [Rd] 2 versions of same library loaded
In-Reply-To: <1394732791.15874.53.camel@localhost>
References: <1394660055.25697.92.camel@localhost>
	<E236A681-5D73-478E-8FB2-3B04837B00DA@r-project.org>
	<1394686856.15874.16.camel@localhost>
	<E66794E69CFDE04D9A70842786030B933FA936DE@PA-MBX01.na.tibco.com>
	<1394732791.15874.53.camel@localhost>
Message-ID: <1394736008.15874.70.camel@localhost>

On Thu, 2014-03-13 at 10:46 -0700, Ross Boylan wrote:
> 
> It seems very odd that the same Rmpi.so is requiring both the old and
> new libmpi.so (compare to the first 
> trace in in point 1).  There is this code in Rmpi.c:
>     if (!dlopen("libmpi.so.0", RTLD_GLOBAL | RTLD_LAZY)
>         && !dlopen("libmpi.so", RTLD_GLOBAL | RTLD_LAZY)){
> 
> 
> So I'm still not sure what it's using, or if there is some mishmash of
> the 2. 

There is an explanation for the explicit load in the Changelog:
2007-10-24, version 0.5-5:

dlopen has been used to load libmpi.so explicitly. This is mainly useful
for Rmpi under OpenMPI where one might see many error messages:
mca: base: component_find: unable to open osc pt2pt: file not found
(ignored) if libmpi.so is not loaded with RTLD_GLOBAL flag.
http://www.stats.uwo.ca/faculty/yu/Rmpi/changelogs.htm

There is another interesting note about openmpi:
It looks like that the option --disable-dlopen is not necessary to
install Open MPI 1.6, at least on Debian. This might be R's .onLoad
correctly loading dynamic libraries and Open MPI is not required to be
compiled with static libraries enabled.


From smckinney at bccrc.ca  Thu Mar 13 21:08:28 2014
From: smckinney at bccrc.ca (Steven McKinney)
Date: Thu, 13 Mar 2014 13:08:28 -0700
Subject: [Rd] R-3.0.3 tar file not on Sources page
Message-ID: <DCE81E14EB74504B971DAD4D2DB0356B0CB926E30A@crcmail4.BCCRC.CA>

Checking CRAN today, I see that on the Sources page

   http://cran.r-project.org/sources.html

that R-3.0.2.tar.gz is still there.  R-3.0.3.tar.gz is available from the
Older releases are available >>here<<. hyperlink to 

   http://cran.r-project.org/src/base/
 

Is this the right place to report this, for an update to the Sources page?




Steven McKinney

Statistician
Molecular Oncology and Breast Cancer Program
British Columbia Cancer Research Centre


From lubinbin220 at gmail.com  Fri Mar 14 04:12:08 2014
From: lubinbin220 at gmail.com (binbin lu)
Date: Fri, 14 Mar 2014 03:12:08 +0000
Subject: [Rd] Fwd: C code and DLL file in the package
In-Reply-To: <21281.51107.473292.800346@max.nulle.part>
References: <CANuoaEEqK2WoxVj8-OHQfxkHGvBqmx3N6=OdjfBvVudZKS9ipA@mail.gmail.com>
	<CANuoaEGsw4WNM2kJyj7JyKqMM+UmfevmFzNRGkpmPzHdnhtSKg@mail.gmail.com>
	<21281.51107.473292.800346@max.nulle.part>
Message-ID: <CANuoaEHT9DJ0qLLxLLMcyMAhvnMRhTwmS3xHmXxpiZyBwhEHQg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140314/52a1ae89/attachment.pl>

From pdalgd at gmail.com  Fri Mar 14 09:49:23 2014
From: pdalgd at gmail.com (peter dalgaard)
Date: Fri, 14 Mar 2014 09:49:23 +0100
Subject: [Rd] R-3.0.3 tar file not on Sources page
In-Reply-To: <DCE81E14EB74504B971DAD4D2DB0356B0CB926E30A@crcmail4.BCCRC.CA>
References: <DCE81E14EB74504B971DAD4D2DB0356B0CB926E30A@crcmail4.BCCRC.CA>
Message-ID: <817E1259-FFCF-4138-8623-1A371624C253@gmail.com>

On 13 Mar 2014, at 21:08 , Steven McKinney <smckinney at bccrc.ca> wrote:

> Checking CRAN today, I see that on the Sources page
> 
>   http://cran.r-project.org/sources.html
> 
> that R-3.0.2.tar.gz is still there.  R-3.0.3.tar.gz is available from the
> Older releases are available >>here<<. hyperlink to 
> 
>   http://cran.r-project.org/src/base/
> 
> Is this the right place to report this, for an update to the Sources page?

It's a CRAN issue, so cran at r-project.org reaches the right people more efficiently. 

The front page of CRAN has the right information, so I guess that the Sources page got overlooked.

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From kirill.mueller at ivt.baug.ethz.ch  Fri Mar 14 13:09:10 2014
From: kirill.mueller at ivt.baug.ethz.ch (=?ISO-8859-1?Q?Kirill_M=FCller?=)
Date: Fri, 14 Mar 2014 13:09:10 +0100
Subject: [Rd] Detect a terminated pipe
Message-ID: <5322F166.30706@ivt.baug.ethz.ch>

Hi

Is there a way to detect that the process that corresponds to a pipe has 
ended? On my system (Ubuntu 13.04), I see

 > p <- pipe("true", "w"); Sys.sleep(1); system("ps -elf | grep true | 
grep -v grep"); isOpen(p)
[1] TRUE

The "true" process has long ended (as the filtered ps system call emits 
no output), still R believes that the pipe p is open.

Thanks for your input.


Best regards

Kirill


From simon.urbanek at r-project.org  Fri Mar 14 15:54:10 2014
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Fri, 14 Mar 2014 10:54:10 -0400
Subject: [Rd] Detect a terminated pipe
In-Reply-To: <5322F166.30706@ivt.baug.ethz.ch>
References: <5322F166.30706@ivt.baug.ethz.ch>
Message-ID: <034131C4-F4C4-44C6-BF23-9F8892DD9138@r-project.org>

On Mar 14, 2014, at 8:09 AM, Kirill M?ller <kirill.mueller at ivt.baug.ethz.ch> wrote:

> Hi
> 
> Is there a way to detect that the process that corresponds to a pipe has ended? On my system (Ubuntu 13.04), I see
> 
> > p <- pipe("true", "w"); Sys.sleep(1); system("ps -elf | grep true | grep -v grep"); isOpen(p)
> [1] TRUE
> 
> The "true" process has long ended (as the filtered ps system call emits no output), still R believes that the pipe p is open.
> 

As far as R is concerned, the connection is open. In addition, pipes exist even without the process - you can close one end of a pipe and it will still exist (that?s what makes pipes useful, actually, because you can choose to close arbitrary combination of the R/W ends). Detecting that the other end of the pipe has closed is generally done by sending/receiving data to/from the end of interest - i.e. reading from a pipe that has closed the write end on the other side will yield 0 bytes read. Writing to a pipe that has closed the read end on the other side will yield SIGPIPE error (note that for text connections you have to call flush() to send the buffer):

> p=pipe("true","r")
> readLines(p)
character(0)
> close(p)

> p=pipe("true","w")
> writeLines("", p)
> flush(p)
Error in flush.connection(p) : ignoring SIGPIPE signal
> close(p)

Cheers,
Simon




> Thanks for your input.
> 
> 
> Best regards
> 
> Kirill
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 


From kirill.mueller at ivt.baug.ethz.ch  Fri Mar 14 16:03:31 2014
From: kirill.mueller at ivt.baug.ethz.ch (=?windows-1252?Q?Kirill_M=FCller?=)
Date: Fri, 14 Mar 2014 16:03:31 +0100
Subject: [Rd] Detect a terminated pipe
In-Reply-To: <034131C4-F4C4-44C6-BF23-9F8892DD9138@r-project.org>
References: <5322F166.30706@ivt.baug.ethz.ch>
	<034131C4-F4C4-44C6-BF23-9F8892DD9138@r-project.org>
Message-ID: <53231A43.8000603@ivt.baug.ethz.ch>

On 03/14/2014 03:54 PM, Simon Urbanek wrote:
> As far as R is concerned, the connection is open. In addition, pipes exist even without the process - you can close one end of a pipe and it will still exist (that?s what makes pipes useful, actually, because you can choose to close arbitrary combination of the R/W ends). Detecting that the other end of the pipe has closed is generally done by sending/receiving data to/from the end of interest - i.e. reading from a pipe that has closed the write end on the other side will yield 0 bytes read. Writing to a pipe that has closed the read end on the other side will yield SIGPIPE error (note that for text connections you have to call flush() to send the buffer):
>
>> >p=pipe("true","r")
>> >readLines(p)
> character(0)
>> >close(p)
>> >p=pipe("true","w")
>> >writeLines("", p)
>> >flush(p)
> Error in flush.connection(p) : ignoring SIGPIPE signal
>> >close(p)
Thanks for your reply. I tried this in an R console and received the 
error, just like you described. Unfortunately, the error is not thrown 
when trying the same in RStudio. Any ideas?


Cheers

Kirill


From simon.urbanek at r-project.org  Fri Mar 14 16:12:37 2014
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Fri, 14 Mar 2014 11:12:37 -0400
Subject: [Rd] Detect a terminated pipe
In-Reply-To: <53231A43.8000603@ivt.baug.ethz.ch>
References: <5322F166.30706@ivt.baug.ethz.ch>
	<034131C4-F4C4-44C6-BF23-9F8892DD9138@r-project.org>
	<53231A43.8000603@ivt.baug.ethz.ch>
Message-ID: <E96B5372-1E25-4D2E-9733-66F8BAF0FC2F@r-project.org>

On Mar 14, 2014, at 11:03 AM, Kirill M?ller <kirill.mueller at ivt.baug.ethz.ch> wrote:

> On 03/14/2014 03:54 PM, Simon Urbanek wrote:
>> As far as R is concerned, the connection is open. In addition, pipes exist even without the process - you can close one end of a pipe and it will still exist (that?s what makes pipes useful, actually, because you can choose to close arbitrary combination of the R/W ends). Detecting that the other end of the pipe has closed is generally done by sending/receiving data to/from the end of interest - i.e. reading from a pipe that has closed the write end on the other side will yield 0 bytes read. Writing to a pipe that has closed the read end on the other side will yield SIGPIPE error (note that for text connections you have to call flush() to send the buffer):
>> 
>>> >p=pipe("true","r")
>>> >readLines(p)
>> character(0)
>>> >close(p)
>>> >p=pipe("true","w")
>>> >writeLines("", p)
>>> >flush(p)
>> Error in flush.connection(p) : ignoring SIGPIPE signal
>>> >close(p)
> Thanks for your reply. I tried this in an R console and received the error, just like you described. Unfortunately, the error is not thrown when trying the same in RStudio. Any ideas?
> 

RStudio is not R, so it?s possible that they catch signals and fail to distinguish their use from R?s eating the signal before R can get it. I would suggest filing a bug report with RStudio.

Cheers,
Simon


From skyebend at skyeome.net  Fri Mar 14 17:28:01 2014
From: skyebend at skyeome.net (Skye Bender-deMoll)
Date: Fri, 14 Mar 2014 09:28:01 -0700
Subject: [Rd] Problems building package vignette: Sweave requires multiple
 passes to build document.
Message-ID: <53232E11.4050402@skyeome.net>

Dear R-devel,


Question:

     How can I get Sweave to recognize that the \thebibliography section 
is already created and generate the vignette pdf with a single pass? Or 
is there a way to let R CMD build know that Sweave needs to be run twice?

     If (1) is not possible, any suggestions how to auto-generate a 
crude generic bibtex .bib file from my existing document?

Background:

I maintain serveral R packages that include package vignettes. The 
vignettes include the reference citations inline as\bibitem lines inside 
a thebiblilography section. For some reason, this requires running 
Sweave twice on the document (the first pass is presumably preformatting 
the citation information?) to produce a pdf. This seems to be the source 
of problems when included in R's preferred vignette location 
(/vignettes/myPackage.Rnw) because the R package builder just runs 
Sweave once during the check process, so no pdf is produced and warnings 
are generated like

checking package vignettes in ???inst/doc??? ... WARNING
  Package vignette without corresponding PDF/HTML

In the past, it has been possible for me to pre-build the pdf, and 
include it in the /inst/doc/ directory. But it seems the upcoming 
versions of R may prohibit this.

It appears that it is possible to avoid all this by including a bibtex 
.bib file along with the vignette instead of doing the citations inline. 
If the .bib files exists, it seems to generate with a single pass. But 
reformatting all of the citations for each document into bibtex will be 
a huge pain. Including a blank \bibliography{} command above my 
\thebibliography environment also allows the document to be build in a 
single pass, but generates a warning about duplicate biliography and 
writes the section header twice (not surprisingly).


Note: I've cross-posted this question at:
http://stackoverflow.com/questions/22237675/how-to-get-sweave-to-recognize-thebibliography-environment-and-build-r-package

thanks for your help,
  best,
  -skye


From murdoch.duncan at gmail.com  Fri Mar 14 17:41:31 2014
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Fri, 14 Mar 2014 12:41:31 -0400
Subject: [Rd] Problems building package vignette: Sweave requires
 multiple passes to build document.
In-Reply-To: <53232E11.4050402@skyeome.net>
References: <53232E11.4050402@skyeome.net>
Message-ID: <5323313B.6090901@gmail.com>

On 14/03/2014 12:28 PM, Skye Bender-deMoll wrote:
> Dear R-devel,
>
>
> Question:
>
>       How can I get Sweave to recognize that the \thebibliography section
> is already created and generate the vignette pdf with a single pass? Or
> is there a way to let R CMD build know that Sweave needs to be run twice?

I think the problem isn't Sweave:  it ignores LaTeX code.  The problem 
is the following step that converts the .tex output into a .pdf, which 
is handled by tools::texi2pdf (which runs tools::texi2dvi).  That 
function makes an effort to determine how many LaTeX passes are 
required, but it appears you are fooling it somehow.

Take a look at the source to tools::texi2dvi, and see if you can spot 
what's going wrong in your case.  There are a number of different paths 
through that code that depend on your particular setup, so we can't do 
that.  If you don't succeed in this, then post a simplified sample 
package and someone else will be able to see if they have the same 
problem as you, and perhaps fix it.

Duncan Murdoch

>
>       If (1) is not possible, any suggestions how to auto-generate a
> crude generic bibtex .bib file from my existing document?
>
> Background:
>
> I maintain serveral R packages that include package vignettes. The
> vignettes include the reference citations inline as\bibitem lines inside
> a thebiblilography section. For some reason, this requires running
> Sweave twice on the document (the first pass is presumably preformatting
> the citation information?) to produce a pdf. This seems to be the source
> of problems when included in R's preferred vignette location
> (/vignettes/myPackage.Rnw) because the R package builder just runs
> Sweave once during the check process, so no pdf is produced and warnings
> are generated like
>
> checking package vignettes in ???inst/doc??? ... WARNING
>    Package vignette without corresponding PDF/HTML
>
> In the past, it has been possible for me to pre-build the pdf, and
> include it in the /inst/doc/ directory. But it seems the upcoming
> versions of R may prohibit this.
>
> It appears that it is possible to avoid all this by including a bibtex
> .bib file along with the vignette instead of doing the citations inline.
> If the .bib files exists, it seems to generate with a single pass. But
> reformatting all of the citations for each document into bibtex will be
> a huge pain. Including a blank \bibliography{} command above my
> \thebibliography environment also allows the document to be build in a
> single pass, but generates a warning about duplicate biliography and
> writes the section header twice (not surprisingly).
>
>
> Note: I've cross-posted this question at:
> http://stackoverflow.com/questions/22237675/how-to-get-sweave-to-recognize-thebibliography-environment-and-build-r-package
>
> thanks for your help,
>    best,
>    -skye
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From j.hadfield at ed.ac.uk  Fri Mar 14 18:56:25 2014
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Fri, 14 Mar 2014 17:56:25 +0000
Subject: [Rd] Conditional jump or move depends on uninitialised value(s)
Message-ID: <20140314175625.11267aj8gcymx4co@www.staffmail.ed.ac.uk>

Hi,

I'm using valgrind to check over some C/C++ code for an R library. I'm  
getting the report (see below), but can't track down the uninitialised  
value(s). I tried using  --track-origins=yes in valgrind which gives:

==28258==  Uninitialised value was created by a stack allocation
==28258==    at 0xEE33D98: ??? (in /usr/lib64/R/lib/libRlapack.so)

I presume the problem is an uninitialised value being used in my code,  
rather than in libRlapack, but there is a better way of tracking down  
where it is?

Cheers,

Jarrod

> R -d "valgrind --tool=memcheck --leak-check=full --track-origins=yes"

==28258== Conditional jump or move depends on uninitialised value(s)
==28258==    at 0xEE87208: dstemr_ (in /usr/lib64/R/lib/libRlapack.so)
==28258==    by 0xEE7F39B: dsyevr_ (in /usr/lib64/R/lib/libRlapack.so)
==28258==    by 0x15B23BD9: ??? (in /usr/lib64/R/modules/lapack.so)
==28258==    by 0x15B28397: ??? (in /usr/lib64/R/modules/lapack.so)
==28258==    by 0x35CA4BFEAC: ??? (in /usr/lib64/R/lib/libR.so)
==28258==    by 0x35CA4C8A1F: Rf_eval (in /usr/lib64/R/lib/libR.so)
==28258==    by 0x35CA4BB310: Rf_applyClosure (in /usr/lib64/R/lib/libR.so)
==28258==    by 0x35CA4C58F9: ??? (in /usr/lib64/R/lib/libR.so)
==28258==    by 0x35CA4C8A1F: Rf_eval (in /usr/lib64/R/lib/libR.so)
==28258==    by 0x35CA4BB310: Rf_applyClosure (in /usr/lib64/R/lib/libR.so)
==28258==    by 0x35CA4C8ACC: Rf_eval (in /usr/lib64/R/lib/libR.so)
==28258==    by 0x35CA4CAEE7: ??? (in /usr/lib64/R/lib/libR.so)
==28258==  Uninitialised value was created by a stack allocation
==28258==    at 0xEE33D98: ??? (in /usr/lib64/R/lib/libRlapack.so)


-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From ripley at stats.ox.ac.uk  Fri Mar 14 20:00:52 2014
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 14 Mar 2014 19:00:52 +0000
Subject: [Rd] Conditional jump or move depends on uninitialised value(s)
In-Reply-To: <20140314175625.11267aj8gcymx4co@www.staffmail.ed.ac.uk>
References: <20140314175625.11267aj8gcymx4co@www.staffmail.ed.ac.uk>
Message-ID: <532351E4.7080006@stats.ox.ac.uk>

On 14/03/2014 17:56, Jarrod Hadfield wrote:
> Hi,
>
> I'm using valgrind to check over some C/C++ code for an R library. I'm

R package?

> getting the report (see below), but can't track down the uninitialised
> value(s). I tried using  --track-origins=yes in valgrind which gives:
>
> ==28258==  Uninitialised value was created by a stack allocation
> ==28258==    at 0xEE33D98: ??? (in /usr/lib64/R/lib/libRlapack.so)
>
> I presume the problem is an uninitialised value being used in my code,
> rather than in libRlapack, but there is a better way of tracking down
> where it is?

You need debug symbols.  Maybe available for your unstated platform, or 
build a version of R with an internal LAPACK.

I doubt this is you: 'Writing R Extensions' has

'Note that memory access errors may be seen with LAPACK, BLAS and 
Java-using packages: some at least of these seem to be intentional, and 
some are related to passing characters to Fortran.'

Also, it is entirely possible that this is a non-current version of 
LAPACK: quite a few things like this have been plugged recently.  For 
example, Fedora currently has 3.4.2 and 3.5.0 has been out for a lot 
longer than Fedora 20.

>
> Cheers,
>
> Jarrod
>
>> R -d "valgrind --tool=memcheck --leak-check=full --track-origins=yes"
>
> ==28258== Conditional jump or move depends on uninitialised value(s)
> ==28258==    at 0xEE87208: dstemr_ (in /usr/lib64/R/lib/libRlapack.so)
> ==28258==    by 0xEE7F39B: dsyevr_ (in /usr/lib64/R/lib/libRlapack.so)
> ==28258==    by 0x15B23BD9: ??? (in /usr/lib64/R/modules/lapack.so)
> ==28258==    by 0x15B28397: ??? (in /usr/lib64/R/modules/lapack.so)
> ==28258==    by 0x35CA4BFEAC: ??? (in /usr/lib64/R/lib/libR.so)
> ==28258==    by 0x35CA4C8A1F: Rf_eval (in /usr/lib64/R/lib/libR.so)
> ==28258==    by 0x35CA4BB310: Rf_applyClosure (in /usr/lib64/R/lib/libR.so)
> ==28258==    by 0x35CA4C58F9: ??? (in /usr/lib64/R/lib/libR.so)
> ==28258==    by 0x35CA4C8A1F: Rf_eval (in /usr/lib64/R/lib/libR.so)
> ==28258==    by 0x35CA4BB310: Rf_applyClosure (in /usr/lib64/R/lib/libR.so)
> ==28258==    by 0x35CA4C8ACC: Rf_eval (in /usr/lib64/R/lib/libR.so)
> ==28258==    by 0x35CA4CAEE7: ??? (in /usr/lib64/R/lib/libR.so)
> ==28258==  Uninitialised value was created by a stack allocation
> ==28258==    at 0xEE33D98: ??? (in /usr/lib64/R/lib/libRlapack.so)
>
>


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From sams.james at gmail.com  Sat Mar 15 18:53:39 2014
From: sams.james at gmail.com (James Sams)
Date: Sat, 15 Mar 2014 12:53:39 -0500
Subject: [Rd] allocation error and high CPU usage from kworker and
 migration: memory fragmentation?
Message-ID: <532493A3.1030809@gmail.com>

Hi,

I'm new to this list (and R), but my impression is that this question is 
more appropriate here than R-help. I hope that is right.

I'm having several issues with the performance of an R script. 
Occasionally it crashes with the well-known 'Error: cannot allocate 
vector of size X' (this past time it was 4.8 Gb). When it doesn't crash, 
CPU usage frequently drops quite low (often to 0) with high migration/X 
usage. Adding the 'last CPU used' field to top indicates that the R 
process is hopping from core to core quite frequently. Using taskset to 
set an affinity to one core results in CPU usage more typically in the 
40-60% range with no migration/X usage. But the core starts sharing time 
with a kworker task. renice'ing doesn't seem to change anything. If I 
had to guess, I would think that the kworker task is from R trying to 
re-arrange things in memory to make space for my large objects.

2 machines:
   - 128 and 256 GiB RAM,
   - dual processor Xeons (16 cores + hyperthreading, 32 total 'cores'),
   - Ubuntu 13.10 and 13.04 (both 64 bit),
   - R 3.0.2,
   - data.table 1.8.11 (svn r1129).*

Data: We have main fact tables stored in about 1000 R data files that 
range up to 3 GiB in size on disk; so up to like 50 GiB in RAM.

Questions:
   - Why is R skipping around cores so much? I've never seen that happen 
before with other R scripts or with other statistical software. Is it 
something I'm doing?
   - When I set the affinity of R to one core, why is there so much 
kworker activity? It seems obvious that it is the R script generating 
this kworker activity on the same core. I'm guessing this is R trying to 
recover from memory fragmentation?
   - I suspect a lot of my problem is from the merges. If I did that in 
one line, would this help at all?
     move <- merge(merge(move, upc, by=c('upc')), parent, by=c('store', 
'year'))
     * other strategies to improve merge performance?
   - If this is a memory fragmentation issue, is there a way to get 
lapply to allocate not just pointers to the data.tables that will be 
allocated, but to (over)allocate the data.tables themselves. The final 
list should be about 1000 data.tables long with each data.table no 
larger than 6000x4.

I've used data.table in a similar strategy to build lists like this 
before without issue from the same data. I'm not sure what is different 
about this code compared to my other code. Perhaps the merging?

The gist of the R code is pretty basic (modified for simplicity). The 
action is all happening in the reduction_function and lapply. I keep 
reassigning to move to try to indicate to R that it can gc the previous 
object referenced by move.

library(data.table)
library(lubridate)
# imports several data.tables, total 730 MiB
load(UPC) # provides PL_flag data.table
load(STORES) # and parent data.table
timevar = 'month'
by=c('retailer', 'month')
save.dir='/tmp/R_cache'
each.parent <- rbindlist(lapply(sort(list.files(MOVEMENT, full.names=T),
                                     reduction_function, upc=PL_flag,
                                     parent=parent, timevar=timevar, by=by))

reduction_function <- function(filename, upc, parent, timevar, by, 
save.dir=NA) {
     load(filename) # imports move a potentially large data.table 
(memory size 10 MiB-50 GiB)
     move[, c(timevar, 'year') := list(floor_date(week_end, unit=timevar),
                                       year(week_end))]
     move <- merge(move, upc, by=c('upc')) # adds is_PL column, a boolean
     move <- merge(move, parent, by=c('store', 'year') # adds parent 
column, an integer
     setkeyv(move, by)
     # this reduces move to a data.table with at most 6000 rows, but 
always 4 columns
     move <- move[, list(revenue=sum(price*units), 
revenue_PL=sum(price*units*is_PL)),
                keyby=by]
     move[, category := gsub(search, replace, filename)]
     return(move)
}

-- 
James Sams
sams.james at gmail.com


From freecnpro at gmail.com  Sun Mar 16 11:36:05 2014
From: freecnpro at gmail.com (Bill Wang)
Date: Sun, 16 Mar 2014 18:36:05 +0800
Subject: [Rd] How to convert time_t to R date object
Message-ID: <CACY4ALiS73rxKVNwuW8W_0zsE35NcZh==JiSPUv1VZ+7E4P4Kg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140316/d25b9501/attachment.pl>

From edd at debian.org  Sun Mar 16 15:55:44 2014
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 16 Mar 2014 09:55:44 -0500
Subject: [Rd] How to convert time_t to R date object
In-Reply-To: <CACY4ALiS73rxKVNwuW8W_0zsE35NcZh==JiSPUv1VZ+7E4P4Kg@mail.gmail.com>
References: <CACY4ALiS73rxKVNwuW8W_0zsE35NcZh==JiSPUv1VZ+7E4P4Kg@mail.gmail.com>
Message-ID: <21285.47984.318336.174502@max.nulle.part>


On 16 March 2014 at 18:36, Bill Wang wrote:
| I am writing a R extensions, and I need pass time_t to R in C, but I don't
| know how to do.
| Can you give me some help? do not use double directly.

Just treat it as an int:

  R> library(Rcpp)
  R> cppFunction("Date time_t2date(time_t what) { return((int) what); }")
  R> time_t2date(0)
  [1] "1970-01-01"
  R> time_t2date( Sys.Date() )
  [1] "2014-03-16"
  R>


Here I use Rcpp to define the 'time_t2date' function on the fly. 

It takes the time_t and returns a Date type (which here is a C++ Date type
mapping to the R Date -- you can ignore that, but will have to write the
legwork yourself if you don't use Rcpp).  

As 'time_t' is 'long int' on my system, so I cast it to int. The rest is
automagic (thanks to RcpP).  

Notice that I also get today's date down and up correctly.

See 'Writing R Extensions' for the details at the C level. 

See the Rcpp documentation (and, if I may, my book) for details on Rcpp if
that interests you.

Dirk

-- 
Dirk Eddelbuettel | edd at debian.org | http://dirk.eddelbuettel.com


From ross at biostat.ucsf.edu  Mon Mar 17 00:51:16 2014
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Sun, 16 Mar 2014 16:51:16 -0700
Subject: [Rd] Does R ever move objecsts in memory?
Message-ID: <20140316235116.GC14057@markov.biostat.ucsf.edu>

R objects can disappear if they are garbage collected; can they move,
i.e., change their location in memory?

I don't see any indication this might happen in "Writing R Extensions"
or "R Internals".  But I'd like to be sure.

Context: Rmpi serializes objects in raw vectors for transmission by
mpi.  Some send operations (isend) return before transmission is
complete and so need the bits to remain untouched until transmission
completes.  If a preserve a reference to the raw vector in R code that
will prevent it from being garbage collected, but if it gets moved
that would invalidate the transfer.

I was just using the blocking sends to avoid this problem, but the
result is significant delays.

Thanks.
Ross Boylan


From murdoch.duncan at gmail.com  Mon Mar 17 01:31:01 2014
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Sun, 16 Mar 2014 20:31:01 -0400
Subject: [Rd] Does R ever move objecsts in memory?
In-Reply-To: <20140316235116.GC14057@markov.biostat.ucsf.edu>
References: <20140316235116.GC14057@markov.biostat.ucsf.edu>
Message-ID: <53264245.7030001@gmail.com>

On 14-03-16 7:51 PM, Ross Boylan wrote:
> R objects can disappear if they are garbage collected; can they move,
> i.e., change their location in memory?
>
> I don't see any indication this might happen in "Writing R Extensions"
> or "R Internals".  But I'd like to be sure.
>
> Context: Rmpi serializes objects in raw vectors for transmission by
> mpi.  Some send operations (isend) return before transmission is
> complete and so need the bits to remain untouched until transmission
> completes.  If a preserve a reference to the raw vector in R code that
> will prevent it from being garbage collected, but if it gets moved
> that would invalidate the transfer.
>
> I was just using the blocking sends to avoid this problem, but the
> result is significant delays.
>

If the object is unchanged, it won't move.  If you modify it (e.g. 
assign a new value to one element of a vector), it may be duplicated and 
the result after the change will be at a new location.

Duncan Murdoch


From freecnpro at gmail.com  Mon Mar 17 05:54:10 2014
From: freecnpro at gmail.com (Bill Wang)
Date: Mon, 17 Mar 2014 12:54:10 +0800
Subject: [Rd] How to convert time_t to R date object
In-Reply-To: <21285.47984.318336.174502@max.nulle.part>
References: <CACY4ALiS73rxKVNwuW8W_0zsE35NcZh==JiSPUv1VZ+7E4P4Kg@mail.gmail.com>
	<21285.47984.318336.174502@max.nulle.part>
Message-ID: <CACY4ALhbhtvNR7CNj7efEeSgXbhJRCH325OOW=0AhrbDETsoLQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140317/b7dd9c93/attachment.pl>

From sannandi at umail.iu.edu  Mon Mar 17 06:24:26 2014
From: sannandi at umail.iu.edu (Sandip Nandi)
Date: Sun, 16 Mar 2014 22:24:26 -0700
Subject: [Rd] How to convert time_t to R date object
In-Reply-To: <CACY4ALhbhtvNR7CNj7efEeSgXbhJRCH325OOW=0AhrbDETsoLQ@mail.gmail.com>
References: <CACY4ALiS73rxKVNwuW8W_0zsE35NcZh==JiSPUv1VZ+7E4P4Kg@mail.gmail.com>
	<21285.47984.318336.174502@max.nulle.part>
	<CACY4ALhbhtvNR7CNj7efEeSgXbhJRCH325OOW=0AhrbDETsoLQ@mail.gmail.com>
Message-ID: <CAGSjAUBnh9qS2A2P7Fd0GLzSQngcWEu2KS6VPxNnkgUTbn0HVg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140316/319fb941/attachment.pl>

From kirill.mueller at ivt.baug.ethz.ch  Mon Mar 17 10:13:37 2014
From: kirill.mueller at ivt.baug.ethz.ch (=?ISO-8859-1?Q?Kirill_M=FCller?=)
Date: Mon, 17 Mar 2014 10:13:37 +0100
Subject: [Rd] Deep copy of factor levels?
Message-ID: <5326BCC1.9020809@ivt.baug.ethz.ch>

Hi


It seems that selecting an element of a factor will copy its levels 
(Ubuntu 13.04, R 3.0.2). Below is the output of a script that creates a 
factor with 10000 elements and then calls as.list() on it. The new 
object seems to use more than 700 MB, and inspection of the levels of 
the individual elements of the list suggest that they are distinct objects.

Perhaps some performance gain could be achieved by copying the levels 
"by reference", but I don't know R internals well enough to see if it's 
possible. Is there a particular reason for creating a full copy of the 
factor levels?

This has come up when looking at the performance of rbind.fill (in the 
plyr package) with factors: https://github.com/hadley/plyr/issues/206 .


Best regards

Kirill



 > gc()
           used (Mb) gc trigger  (Mb)  max used   (Mb)
Ncells  325977 17.5    1074393  57.4  10049951  536.8
Vcells 4617168 35.3   87439742 667.2 204862160 1563.0
 > system.time(x <- factor(seq_len(1e4)))
    user  system elapsed
   0.008   0.000   0.007
 > system.time(xx <- as.list(x))
    user  system elapsed
   4.263   0.000   4.322
 > gc()
             used  (Mb) gc trigger  (Mb)  max used   (Mb)
Ncells    385991  20.7    1074393  57.4  10049951  536.8
Vcells 104672187 798.6  112367694 857.3 204862160 1563.0
 > .Internal(inspect(levels(xx[[1]])))
@387f620 16 STRSXP g1c7 [MARK,NAM(2)] (len=10000, tl=0)
   @144da4e8 09 CHARSXP g1c1 [MARK,gp=0x60] [ASCII] [cached] "1"
   @144da518 09 CHARSXP g1c1 [MARK,gp=0x60] [ASCII] [cached] "2"
   @27d1298 09 CHARSXP g1c1 [MARK,gp=0x60] [ASCII] [cached] "3"
   @144da548 09 CHARSXP g1c1 [MARK,gp=0x60] [ASCII] [cached] "4"
   @144da578 09 CHARSXP g1c1 [MARK,gp=0x60] [ASCII] [cached] "5"
   ...
 > .Internal(inspect(levels(xx[[2]])))
@1b38cb90 16 STRSXP g1c7 [MARK,NAM(2)] (len=10000, tl=0)
   @144da4e8 09 CHARSXP g1c1 [MARK,gp=0x60] [ASCII] [cached] "1"
   @144da518 09 CHARSXP g1c1 [MARK,gp=0x60] [ASCII] [cached] "2"
   @27d1298 09 CHARSXP g1c1 [MARK,gp=0x60] [ASCII] [cached] "3"
   @144da548 09 CHARSXP g1c1 [MARK,gp=0x60] [ASCII] [cached] "4"
   @144da578 09 CHARSXP g1c1 [MARK,gp=0x60] [ASCII] [cached] "5"
   ...


From ripley at stats.ox.ac.uk  Mon Mar 17 10:30:10 2014
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 17 Mar 2014 09:30:10 +0000
Subject: [Rd] Deep copy of factor levels?
In-Reply-To: <5326BCC1.9020809@ivt.baug.ethz.ch>
References: <5326BCC1.9020809@ivt.baug.ethz.ch>
Message-ID: <5326C0A2.4000806@stats.ox.ac.uk>

Do use current R (3.1.0 alpha at present: 3.0.2 is obsolete) before 
reporting such things.  I think you will see that this has changed ....

On 17/03/2014 09:13, Kirill M?ller wrote:
> Hi
>
>
> It seems that selecting an element of a factor will copy its levels
> (Ubuntu 13.04, R 3.0.2). Below is the output of a script that creates a
> factor with 10000 elements and then calls as.list() on it. The new
> object seems to use more than 700 MB, and inspection of the levels of
> the individual elements of the list suggest that they are distinct objects.
>
> Perhaps some performance gain could be achieved by copying the levels
> "by reference", but I don't know R internals well enough to see if it's
> possible. Is there a particular reason for creating a full copy of the
> factor levels?
>
> This has come up when looking at the performance of rbind.fill (in the
> plyr package) with factors: https://github.com/hadley/plyr/issues/206 .
>
>
> Best regards
>
> Kirill
>
>
>
>  > gc()
>            used (Mb) gc trigger  (Mb)  max used   (Mb)
> Ncells  325977 17.5    1074393  57.4  10049951  536.8
> Vcells 4617168 35.3   87439742 667.2 204862160 1563.0
>  > system.time(x <- factor(seq_len(1e4)))
>     user  system elapsed
>    0.008   0.000   0.007
>  > system.time(xx <- as.list(x))
>     user  system elapsed
>    4.263   0.000   4.322
>  > gc()
>              used  (Mb) gc trigger  (Mb)  max used   (Mb)
> Ncells    385991  20.7    1074393  57.4  10049951  536.8
> Vcells 104672187 798.6  112367694 857.3 204862160 1563.0
>  > .Internal(inspect(levels(xx[[1]])))
> @387f620 16 STRSXP g1c7 [MARK,NAM(2)] (len=10000, tl=0)
>    @144da4e8 09 CHARSXP g1c1 [MARK,gp=0x60] [ASCII] [cached] "1"
>    @144da518 09 CHARSXP g1c1 [MARK,gp=0x60] [ASCII] [cached] "2"
>    @27d1298 09 CHARSXP g1c1 [MARK,gp=0x60] [ASCII] [cached] "3"
>    @144da548 09 CHARSXP g1c1 [MARK,gp=0x60] [ASCII] [cached] "4"
>    @144da578 09 CHARSXP g1c1 [MARK,gp=0x60] [ASCII] [cached] "5"
>    ...
>  > .Internal(inspect(levels(xx[[2]])))
> @1b38cb90 16 STRSXP g1c7 [MARK,NAM(2)] (len=10000, tl=0)
>    @144da4e8 09 CHARSXP g1c1 [MARK,gp=0x60] [ASCII] [cached] "1"
>    @144da518 09 CHARSXP g1c1 [MARK,gp=0x60] [ASCII] [cached] "2"
>    @27d1298 09 CHARSXP g1c1 [MARK,gp=0x60] [ASCII] [cached] "3"
>    @144da548 09 CHARSXP g1c1 [MARK,gp=0x60] [ASCII] [cached] "4"
>    @144da578 09 CHARSXP g1c1 [MARK,gp=0x60] [ASCII] [cached] "5"
>    ...
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From edd at debian.org  Mon Mar 17 12:39:57 2014
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon, 17 Mar 2014 06:39:57 -0500
Subject: [Rd] How to convert time_t to R date object
In-Reply-To: <CACY4ALhbhtvNR7CNj7efEeSgXbhJRCH325OOW=0AhrbDETsoLQ@mail.gmail.com>
References: <CACY4ALiS73rxKVNwuW8W_0zsE35NcZh==JiSPUv1VZ+7E4P4Kg@mail.gmail.com>
	<21285.47984.318336.174502@max.nulle.part>
	<CACY4ALhbhtvNR7CNj7efEeSgXbhJRCH325OOW=0AhrbDETsoLQ@mail.gmail.com>
Message-ID: <21286.57101.842877.461798@max.nulle.part>


Bill,

On 17 March 2014 at 12:54, Bill Wang wrote:
| Thanks for your reply, I neede convert time_t to R type in C code, can not use
| Rcpp. Maybe Rcpp source code could help me.

Start by reading 'Writing R Extensions' and figure out how to send an int
back and forth.  Then cast between int and time_t.  Then set the class of the
int variable to Date type.

Dirk

-- 
Dirk Eddelbuettel | edd at debian.org | http://dirk.eddelbuettel.com


From edd at debian.org  Mon Mar 17 12:40:54 2014
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon, 17 Mar 2014 06:40:54 -0500
Subject: [Rd] How to convert time_t to R date object
In-Reply-To: <CAGSjAUBnh9qS2A2P7Fd0GLzSQngcWEu2KS6VPxNnkgUTbn0HVg@mail.gmail.com>
References: <CACY4ALiS73rxKVNwuW8W_0zsE35NcZh==JiSPUv1VZ+7E4P4Kg@mail.gmail.com>
	<21285.47984.318336.174502@max.nulle.part>
	<CACY4ALhbhtvNR7CNj7efEeSgXbhJRCH325OOW=0AhrbDETsoLQ@mail.gmail.com>
	<CAGSjAUBnh9qS2A2P7Fd0GLzSQngcWEu2KS6VPxNnkgUTbn0HVg@mail.gmail.com>
Message-ID: <21286.57158.699773.522284@max.nulle.part>


On 16 March 2014 at 22:24, Sandip Nandi wrote:
| Hi Bill ,
| 
| The following C code ?may help you , time_t is typedef to long int?
| 
| SEXP ?getTime() {
| 
| ? time_t current_time;
| ? ? char* c_time_string;
| ? ? current_time = time(NULL);
| ? ? c_time_string = ctime(&current_time);
| 
| ? ?return mkString(c_time_String); // or if you want to return as int vector
| return scalarInt(current_time);
| }
| 
| If you feel anything wrong , I will be very happy to know .?

That is a) time and not date, and b) formatted as a string.

Dirk

 
| Thanks,
| Sandip
| 
| 
| On Sun, Mar 16, 2014 at 9:54 PM, Bill Wang <freecnpro at gmail.com> wrote:
| 
|     Hi Dirk,
| 
|     Thanks for your reply, I neede convert time_t to R type in C code, can not
|     use Rcpp. Maybe Rcpp source code could help me.
| 
|     Cheers,
|     Bill
| 
| 
|     2014-03-16 22:55 GMT+08:00 Dirk Eddelbuettel <edd at debian.org>:
| 
|     >
|     > On 16 March 2014 at 18:36, Bill Wang wrote:
|     > | I am writing a R extensions, and I need pass time_t to R in C, but I
|     > don't
|     > | know how to do.
|     > | Can you give me some help? do not use double directly.
|     >
|     > Just treat it as an int:
|     >
|     > ? R> library(Rcpp)
|     > ? R> cppFunction("Date time_t2date(time_t what) { return((int) what); }
|     ")
|     > ? R> time_t2date(0)
|     > ? [1] "1970-01-01"
|     > ? R> time_t2date( Sys.Date() )
|     > ? [1] "2014-03-16"
|     > ? R>
|     >
|     >
|     > Here I use Rcpp to define the 'time_t2date' function on the fly.
|     >
|     > It takes the time_t and returns a Date type (which here is a C++ Date
|     type
|     > mapping to the R Date -- you can ignore that, but will have to write the
|     > legwork yourself if you don't use Rcpp).
|     >
|     > As 'time_t' is 'long int' on my system, so I cast it to int. The rest is
|     > automagic (thanks to RcpP).
|     >
|     > Notice that I also get today's date down and up correctly.
|     >
|     > See 'Writing R Extensions' for the details at the C level.
|     >
|     > See the Rcpp documentation (and, if I may, my book) for details on Rcpp
|     if
|     > that interests you.
|     >
|     > Dirk
|     >
|     > --
|     > Dirk Eddelbuettel | edd at debian.org | http://dirk.eddelbuettel.com
|     >
| 
| 
| 
|     --
| 
|     *Travel | Programming*
|     *http://freecnpro.net* <http://freecnpro.net>
| 
|     ? ? ? ? [[alternative HTML version deleted]]
| 
|     ______________________________________________
|     R-devel at r-project.org mailing list
|     https://stat.ethz.ch/mailman/listinfo/r-devel
| 
| 

-- 
Dirk Eddelbuettel | edd at debian.org | http://dirk.eddelbuettel.com


From freecnpro at gmail.com  Mon Mar 17 13:41:55 2014
From: freecnpro at gmail.com (Bill Wang)
Date: Mon, 17 Mar 2014 20:41:55 +0800
Subject: [Rd] How to convert time_t to R date object
In-Reply-To: <CAGSjAUBnh9qS2A2P7Fd0GLzSQngcWEu2KS6VPxNnkgUTbn0HVg@mail.gmail.com>
References: <CACY4ALiS73rxKVNwuW8W_0zsE35NcZh==JiSPUv1VZ+7E4P4Kg@mail.gmail.com>
	<21285.47984.318336.174502@max.nulle.part>
	<CACY4ALhbhtvNR7CNj7efEeSgXbhJRCH325OOW=0AhrbDETsoLQ@mail.gmail.com>
	<CAGSjAUBnh9qS2A2P7Fd0GLzSQngcWEu2KS6VPxNnkgUTbn0HVg@mail.gmail.com>
Message-ID: <CACY4ALhoaS2yD-ZV-c7sjcGQdYJ8JM_ZjftFPM0vAgbiSMdRvw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140317/5406691d/attachment.pl>

From ripley at stats.ox.ac.uk  Mon Mar 17 14:11:53 2014
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 17 Mar 2014 13:11:53 +0000
Subject: [Rd] How to convert time_t to R date object
In-Reply-To: <21286.57101.842877.461798@max.nulle.part>
References: <CACY4ALiS73rxKVNwuW8W_0zsE35NcZh==JiSPUv1VZ+7E4P4Kg@mail.gmail.com>	<21285.47984.318336.174502@max.nulle.part>	<CACY4ALhbhtvNR7CNj7efEeSgXbhJRCH325OOW=0AhrbDETsoLQ@mail.gmail.com>
	<21286.57101.842877.461798@max.nulle.part>
Message-ID: <5326F499.6030105@stats.ox.ac.uk>

On 17/03/2014 11:39, Dirk Eddelbuettel wrote:
>
> Bill,
>
> On 17 March 2014 at 12:54, Bill Wang wrote:
> | Thanks for your reply, I neede convert time_t to R type in C code, can not use
> | Rcpp. Maybe Rcpp source code could help me.
>
> Start by reading 'Writing R Extensions' and figure out how to send an int
> back and forth.  Then cast between int and time_t.  Then set the class of the
> int variable to Date type.

But note that time_t is not 'int' on many modern systems: almost all 
64-bit ones and on some 32-bit ones.

I guess this is actually meant to be a date-time object, hence class 
POSIXct.  Class POSIXct is based on doubles, so return a double and add 
the classes in the R wrapper.  If you want class Date, divide by 86400 
and return an integer.

That does assume that the system is POSIX-compliant and so ignores leap 
seconds.   I have not encountered one that is not for many years, but 
allegedly there are locales which do count leap seconds on some 
Unix-alikes.  C code to adjust that is in src/main/datetime.c in the R 
sources.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From axel.urbiz at gmail.com  Mon Mar 17 18:17:05 2014
From: axel.urbiz at gmail.com (Axel Urbiz)
Date: Mon, 17 Mar 2014 13:17:05 -0400
Subject: [Rd] R-devel 3.2.0 MAC OS
Message-ID: <CAAyVsXLVFH4TnNC3pv_cFbWuJYJ9cYBd2Eq8dGh+Vvq+UaLMTw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140317/3c400060/attachment.pl>

From j.hadfield at ed.ac.uk  Mon Mar 17 18:26:12 2014
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Mon, 17 Mar 2014 17:26:12 +0000
Subject: [Rd] valgrind and C++
Message-ID: <20140317172612.14976udr7chlzlcs@www.staffmail.ed.ac.uk>

Hi,

I am sorry if this is perceived as a C++ question rather than an R  
question. After uploading an R library to CRAN (MCMCglmm) the C++ code  
failed to pass the memory checks.  The errors come in pairs like:

Mismatched free() / delete / delete []
at 0x4A077E6: free (vg_replace_malloc.c:446)
by 0x144FA28E: MCMCglmm (MCMCglmm.cc:2184)


Address 0x129850c0 is 0 bytes inside a block of size 4 alloc'd
at 0x4A07CE4: operator new[](unsigned long) (vg_replace_malloc.c:363)
by 0x144F12B7: MCMCglmm (MCMCglmm.cc:99)

which is associated with lines allocating and freeing memory (nG is an  
integer):

int *keep = new int [nG];

and

delete [] keep;

To me this looks fine, and on my machine (Scientific Linux 6.4) using  
gcc 4.4.7-3 and valgrind 1:3.8.1-3.2 I get no such errors. Its not  
clear to me which flavour of Linux or compiler the CRAN team used,  
although from MCMCglmm-Ex.Rout I can see the same version of valgrind  
was used. Any insight would be very welcome.

Kind Regards,

Jarrod








-- 
The University of Edinburgh is a charitable body, registered in
Scotland, with registration number SC005336.


From simon.urbanek at r-project.org  Mon Mar 17 19:45:36 2014
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Mon, 17 Mar 2014 14:45:36 -0400
Subject: [Rd] R-devel 3.2.0 MAC OS
In-Reply-To: <CAAyVsXLVFH4TnNC3pv_cFbWuJYJ9cYBd2Eq8dGh+Vvq+UaLMTw@mail.gmail.com>
References: <CAAyVsXLVFH4TnNC3pv_cFbWuJYJ9cYBd2Eq8dGh+Vvq+UaLMTw@mail.gmail.com>
Message-ID: <B994CA38-6FB7-4683-939E-38D0F0066D58@r-project.org>

On Mar 17, 2014, at 1:17 PM, Axel Urbiz <axel.urbiz at gmail.com> wrote:

> Hello,
> 
> I'm trying to build a package to submit to CRAN using R-devel 3.2.0 MAC OS
> from http://r.research.att.com/.
> 
> As my package has dependencies, where should I get the contributed packages
> for this R-devel version? I found the package sources here
> http://r.research.att.com/src/contrib/3.2.0/, but I'm looking for the
> binaries?
> 

Please use R-SIG-Mac, and there are currently no R-devel binaries in the light of the 3.1.0 release since they have just been branched and resources are scarce to build the full set. We typically don't provide package binaries for R-devel until later when it starts diverging.

Cheers,
Simon


From mtalbert at usgs.gov  Mon Mar 17 21:30:21 2014
From: mtalbert at usgs.gov (Marian Talbert)
Date: Mon, 17 Mar 2014 13:30:21 -0700 (PDT)
Subject: [Rd] CRAN rejects package because of write statement in Fortran
In-Reply-To: <1389213790688-4683287.post@n4.nabble.com>
References: <1389213790688-4683287.post@n4.nabble.com>
Message-ID: <1395088221378-4686997.post@n4.nabble.com>

So the best solution I've had just yet for this problem is to convert numeric
to character in C and bypass the Fortran "Write" in that way.  Unfortunately
my C is a bit rusty and I'm having trouble getting this to work.  I image in
the C it should look something like:
void F77_SUB(converti)(char *ch_Result, int
i_Int){*ch_Result=sprintf("%d",*a); }

and in the Fortran something like 
call converti(ch_Result,i_Int)

but this is clearly not right any help would be greatly appreciated. 




--
View this message in context: http://r.789695.n4.nabble.com/CRAN-rejects-package-because-of-write-statement-in-Fortran-tp4683287p4686997.html
Sent from the R devel mailing list archive at Nabble.com.


From tlumley at uw.edu  Tue Mar 18 17:29:07 2014
From: tlumley at uw.edu (Thomas Lumley)
Date: Tue, 18 Mar 2014 11:29:07 -0500
Subject: [Rd] internal copying in R (soon to be released R-3.1.0
In-Reply-To: <F6070CCD-9C5B-4682-BE85-0F8DAD5F1C49@r-project.org>
References: <53136C77.2010603@truecluster.com>
	<E4137A27-9188-4932-891E-7451ACEDC7C2@r-project.org>
	<5314E78F.4030804@truecluster.com>
	<F6070CCD-9C5B-4682-BE85-0F8DAD5F1C49@r-project.org>
Message-ID: <CAJ55+dLa38ROnmCBKXbg34L01aLjSt8dg8mv7PMdYA0dd3eJCA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140318/9c2eb559/attachment.pl>

From plummerm at iarc.fr  Tue Mar 18 17:53:55 2014
From: plummerm at iarc.fr (Martyn Plummer)
Date: Tue, 18 Mar 2014 16:53:55 +0000
Subject: [Rd] valgrind and C++
In-Reply-To: <20140317172612.14976udr7chlzlcs@www.staffmail.ed.ac.uk>
References: <20140317172612.14976udr7chlzlcs@www.staffmail.ed.ac.uk>
Message-ID: <1395161635.15618.4.camel@braque.iarc.fr>

I think the server that runs the valgrind checks is still running the
old version of your package (2.17) not the new one (2.18). Wait for an
update.

Martyn

On Mon, 2014-03-17 at 17:26 +0000, Jarrod Hadfield wrote:
> Hi,
> 
> I am sorry if this is perceived as a C++ question rather than an R  
> question. After uploading an R library to CRAN (MCMCglmm) the C++ code  
> failed to pass the memory checks.  The errors come in pairs like:
> 
> Mismatched free() / delete / delete []
> at 0x4A077E6: free (vg_replace_malloc.c:446)
> by 0x144FA28E: MCMCglmm (MCMCglmm.cc:2184)
> 
> 
> Address 0x129850c0 is 0 bytes inside a block of size 4 alloc'd
> at 0x4A07CE4: operator new[](unsigned long) (vg_replace_malloc.c:363)
> by 0x144F12B7: MCMCglmm (MCMCglmm.cc:99)
> 
> which is associated with lines allocating and freeing memory (nG is an  
> integer):
> 
> int *keep = new int [nG];
> 
> and
> 
> delete [] keep;
> 
> To me this looks fine, and on my machine (Scientific Linux 6.4) using  
> gcc 4.4.7-3 and valgrind 1:3.8.1-3.2 I get no such errors. Its not  
> clear to me which flavour of Linux or compiler the CRAN team used,  
> although from MCMCglmm-Ex.Rout I can see the same version of valgrind  
> was used. Any insight would be very welcome.
> 
> Kind Regards,
> 
> Jarrod
> 
> 
> 
> 
> 
> 
> 
> 

-----------------------------------------------------------------------
This message and its attachments are strictly confidenti...{{dropped:8}}


From j.hadfield at ed.ac.uk  Tue Mar 18 18:26:47 2014
From: j.hadfield at ed.ac.uk (Jarrod Hadfield)
Date: Tue, 18 Mar 2014 17:26:47 +0000
Subject: [Rd] valgrind and C++
In-Reply-To: <1395161635.15618.4.camel@braque.iarc.fr>
References: <20140317172612.14976udr7chlzlcs@www.staffmail.ed.ac.uk>
	<1395161635.15618.4.camel@braque.iarc.fr>
Message-ID: <20140318172647.1500618e886bcwco@www.staffmail.ed.ac.uk>

Hi Martyn,

Thanks for the email - you are right. I thought I was getting the  
valgrind checks on my submission, not the version on CRAN.

Kind Regards,

Jarrod



Quoting Martyn Plummer <plummerm at iarc.fr> on Tue, 18 Mar 2014 16:53:55 +0000:

> I think the server that runs the valgrind checks is still running the
> old version of your package (2.17) not the new one (2.18). Wait for an
> update.
>
> Martyn
>
> On Mon, 2014-03-17 at 17:26 +0000, Jarrod Hadfield wrote:
>> Hi,
>>
>> I am sorry if this is perceived as a C++ question rather than an R
>> question. After uploading an R library to CRAN (MCMCglmm) the C++ code
>> failed to pass the memory checks.  The errors come in pairs like:
>>
>> Mismatched free() / delete / delete []
>> at 0x4A077E6: free (vg_replace_malloc.c:446)
>> by 0x144FA28E: MCMCglmm (MCMCglmm.cc:2184)
>>
>>
>> Address 0x129850c0 is 0 bytes inside a block of size 4 alloc'd
>> at 0x4A07CE4: operator new[](unsigned long) (vg_replace_malloc.c:363)
>> by 0x144F12B7: MCMCglmm (MCMCglmm.cc:99)
>>
>> which is associated with lines allocating and freeing memory (nG is an
>> integer):
>>
>> int *keep = new int [nG];
>>
>> and
>>
>> delete [] keep;
>>
>> To me this looks fine, and on my machine (Scientific Linux 6.4) using
>> gcc 4.4.7-3 and valgrind 1:3.8.1-3.2 I get no such errors. Its not
>> clear to me which flavour of Linux or compiler the CRAN team used,
>> although from MCMCglmm-Ex.Rout I can see the same version of valgrind
>> was used. Any insight would be very welcome.
>>
>> Kind Regards,
>>
>> Jarrod
>>
>>
>>
>>
>>
>>
>>
>>
>
> -----------------------------------------------------------------------
> This message and its attachments are strictly confiden...{{dropped:17}}


From jeroen.ooms at stat.ucla.edu  Tue Mar 18 21:24:46 2014
From: jeroen.ooms at stat.ucla.edu (Jeroen Ooms)
Date: Tue, 18 Mar 2014 13:24:46 -0700
Subject: [Rd] [RFC] A case for freezing CRAN
Message-ID: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>

This came up again recently with an irreproducible paper. Below an
attempt to make a case for extending the r-devel/r-release cycle to
CRAN packages. These suggestions are not in any way intended as
criticism on anyone or the status quo.

The proposal described in [1] is to freeze a snapshot of CRAN along
with every release of R. In this design, updates for contributed
packages treated the same as updates for base packages in the sense
that they are only published to the r-devel branch of CRAN and do not
affect users of "released" versions of R. Thereby all users, stacks
and applications using a particular version of R will by default be
using the identical version of each CRAN package. The bioconductor
project uses similar policies.

This system has several important advantages:

## Reproducibility

Currently r/sweave/knitr scripts are unstable because of ambiguity
introduced by constantly changing cran packages. This causes scripts
to break or change behavior when upstream packages are updated, which
makes reproducing old results extremely difficult.

A common counter-argument is that script authors should document
package versions used in the script using sessionInfo(). However even
if authors would manually do this, reconstructing the author's
environment from this information is cumbersome and often nearly
impossible, because binary packages might no longer be available,
dependency conflicts, etc. See [1] for a worked example. In practice,
the current system causes many results or documents generated with R
no to be reproducible, sometimes already after a few months.

In a system where contributed packages inherit the r-base release
cycle, scripts will behave the same across users/systems/time within a
given version of R. This severely reduces ambiguity of R behavior, and
has the potential of making reproducibility a natural part of the
language, rather than a tedious exercise.

## Repository Management

Just like scripts suffer from upstream changes, so do packages
depending on other packages. A particular package that has been
developed and tested against the current version of a particular
dependency is not guaranteed to work against *any future version* of
that dependency. Therefore, packages inevitably break over time as
their dependencies are updated.

One recent example is the Rcpp 0.11 release, which required all
reverse dependencies to be rebuild/modified. This updated caused some
serious disruption on our production servers. Initially we refrained
from updating Rcpp on these servers to prevent currently installed
packages depending on Rcpp to stop working. However soon after the
Rcpp 0.11 release, many other cran packages started to require Rcpp >=
0.11, and our users started complaining about not being able to
install those packages. This resulted in the impossible situation
where currently installed packages would not work with the new Rcpp,
but newly installed packages would not work with the old Rcpp.

Current CRAN policies blame this problem on package authors. However
as is explained in [1], this policy does not solve anything, is
unsustainable with growing repository size, and sets completely the
wrong incentives for contributing code. Progress comes with breaking
changes, and the system should be able to accommodate this. Much of
the trouble could have been prevented by a system that does not push
bleeding edge updates straight to end-users, but has a devel branch
where conflicts are resolved before publishing them in the next
r-release.

## Reliability

Another example, this time on a very small scale. We recently
discovered that R code plotting medal counts from the Sochi Olympics
generated different results for users on OSX than it did on
Linux/Windows. After some debugging, we narrowed it down to the XML
package. The application used the following code to scrape results
from the Sochi website:

XML::readHTMLTable("http://www.sochi2014.com/en/speed-skating", which=2, skip=1)

This code was developed and tested on mac, but results in a different
winner on windows/linux. This happens because the current version of
the XML package on CRAN is 3.98, but the latest mac binary is 3.95.
Apparently this new version of XML introduces a tiny change that
causes html-table-headers to become colnames, rather than a row in the
matrix, resulting in different medal counts.

This example illustrates that we should never assume package versions
to be interchangeable. Any small bugfix release can have side effects
altering results. It is impossible to protect code against such
upstream changes using CMD check or unit testing. All R scripts and
packages are really only developed and tested for a single version of
their dependencies. Assuming anything else makes results
untrustworthy, and code unreliable.

## Summary

Extending the r-release cycle to CRAN seems like a solution that would
be easy to implement. Package updates simply only get pushed to the
r-devel branches of cran, rather than r-release and r-release-old.
This separates development from production/use in a way that is common
sense in most open source communities. Benefits for R include:

- Regular R users (statisticians, researchers, students, teachers) can
share their homemade scripts/documents/packages and rely on them to
work and produce the same results within a given version of R, without
manual efforts to manage package versions.

- Package authors can publish breaking changes to the devel branch
without causing major disruption or affecting users and/or
maintainers. Authors of depending packages have a timeframe to sync
their package with upstream changes before the next release.

- CRAN maintainers can focus quality control and testing efforts on
the devel branch around the time of the code freeze. No need for
crisis management when a package update introduces some severe
breaking changes. Users of released versions are unaffected.


[1] http://journal.r-project.org/archive/2013-1/ooms.pdf


From tbfowler4 at gmail.com  Tue Mar 18 21:43:16 2014
From: tbfowler4 at gmail.com (Thell Fowler)
Date: Tue, 18 Mar 2014 15:43:16 -0500
Subject: [Rd] Writing R Extensions: clarification/modification request
Message-ID: <CAAJPTXjWCsJgF6-YLXAQ6TA2V5xa5dyhqgb1=jp3V2if0WBJZQ@mail.gmail.com>

Hello R-core developers,

The upcoming release of R-3.1.0 is exciting, especially the support
structure changes for c++11!

But, just the other day I encountered an issue with the package
creation tools within Rcpp and Dirk referred me to 'Writing R
Extensions' [footnote 13] along with the comment that `.hpp` file
extensions are "verboten" on CRAN and then proceeded to find packages
with `.hpp` files in CRAN.

Hopefully you can provide some clarification as to the reasoning of
current stance on this as the documentation doesn't provide any
direction. Research indicates that the footnote may no longer reflect
the current state of R or CRAN and that it may be time to _remove the
statement from the documentation rather than punting and editing to
reflect `c++11`_.

Knowing that the time to manage the upcoming release schedule and the
changes to CRAN is more than likely keeping you to busy to research a
seemingly insignificant edge case I apologize for the length of this
message. Hopefully my time in gathering this supporting information
will save yours.

__Research Notes__

The footnote pertains to section 1.1.5 [Package-subdirectories]
paragraph 8 where it states

>  We recommend using .h for headers, also for C++ or Fortran 9x include files.
>  footnote: Using .hpp is not guaranteed to be portable.

This statement and accompanying footnote are modifications of the
original by Kurt Hornick in rev at 40010 on Nov. 26 2006.

>  We recommend using .h for headers, also for C++
> footnote: Using .hpp, although somewhat popular, is not guaranteed to be portable.

It is completely understandable that this stance was taken, at that
time, since tool-chains hadn't considered `.hpp` as a standard
extension for c++ (none of the then released GCC g++ compilers (3.4.6,
4.0.3, 4.1.1) made any official distinction for it).  As the original
commit indicates though, the it was common. So common in fact that a
[hpp-patch] was filed in 2004 to address this.

The `.hpp` extension became a first-class c++ extension when the
hpp-patch was _finally_ applied in Aug. 2007 and so for GCC >= 4.2.2
it was standard. LLVM had a [clang patch] in their code base to
recognize `.hpp` for its frontend options in 2009, so LLVM >= 2.7,
meaning that releases of XCode >= 3.2.6 can consider `.hpp` an
officially supported extension.  XCode 3.2.6 goes back to OS X 10.6.

According to R Administration and Installation [Installing R under OS X]:

>  ..., then download the file R-3.2.0.pkg and install it. This runs on OS X 10.6 and later (Snow Leopard, Lion, Mountain Lion, Mavericks, ...); it is a 64-bit ('x86_64') build which should run on all Macs from mid-2008 on.

The above would still hold true for all Macs with _updated_ XCode utilities.

---

Obviously GCC, LLVM, and XCode do not represent the extent of the R
user base but hopefully the fact that these tools and many others (
linkers, autoconf tools, etc...) have recognized, as you are doing
with R-3.1.0 and c++11 support, that times have changed and the people
have set the groundwork for best-practice and standards.


-- 
Sincerely,
Thell

[footnote 13]: http://cran.r-project.org/doc/manuals/r-devel/R-exts.html#FOOT13
[Package-subdirectories]:
http://cran.r-project.org/doc/manuals/r-devel/R-exts.html#Package-subdirectories
[rev at 40010]: https://github.com/wch/r-source/commit/cc84426f5b665fb0cba9945c4726348307f36831
[hpp-patch]: http://gcc.gnu.org/bugzilla/show_bug.cgi?id=13676
[clang patch]: https://github.com/llvm-mirror/clang/commit/2059d992cf87caaa6e1790afacdfdf6a26eb57e1
[Installing R under OS X]:
http://r.research.att.com/man/R-admin.html#Installing-R-under-OS-X


From f.harrell at Vanderbilt.Edu  Wed Mar 19 13:26:35 2014
From: f.harrell at Vanderbilt.Edu (Frank Harrell)
Date: Wed, 19 Mar 2014 07:26:35 -0500
Subject: [Rd] [RFC] A case for freezing CRAN
Message-ID: <53298CFB.8060406@vanderbilt.edu>

To me it boils down to one simple question: is an update to a package on 
CRAN more likely to (1) fix a bug, (2) introduce a bug or downward 
incompatibility, or (3) add a new feature or fix a compatibility problem 
without introducing a bug?  I think the probability of (1) | (3) is much 
greater than the probability of (2), hence the current approach 
maximizes user benefit.

Frank
-- 
Frank E Harrell Jr Professor and Chairman      School of Medicine
                    Department of Biostatistics Vanderbilt University


From josh.m.ulrich at gmail.com  Wed Mar 19 13:52:07 2014
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Wed, 19 Mar 2014 07:52:07 -0500
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>
References: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>
Message-ID: <CAPPM_gS1Ey2UyyBSCqecT=NzWXKtREBoXmqNBussR8Hv1Zok3g@mail.gmail.com>

On Tue, Mar 18, 2014 at 3:24 PM, Jeroen Ooms <jeroen.ooms at stat.ucla.edu> wrote:
<snip>
> ## Summary
>
> Extending the r-release cycle to CRAN seems like a solution that would
> be easy to implement. Package updates simply only get pushed to the
> r-devel branches of cran, rather than r-release and r-release-old.
> This separates development from production/use in a way that is common
> sense in most open source communities. Benefits for R include:
>
Nothing is ever as simple as it seems (especially from the perspective
of one who won't be doing the work).

There is nothing preventing you (or anyone else) from creating
repositories that do what you suggest.  Create a CRAN mirror (or more
than one) that only include the package versions you think they
should.  Then have your production servers use it (them) instead of
CRAN.

Better yet, make those repositories public.  If many people like your
idea, they will use your new repositories instead of CRAN.  There is
no reason to impose this change on all world-wide CRAN users.

Best,
--
Joshua Ulrich  |  about.me/joshuaulrich
FOSS Trading  |  www.fosstrading.com


From murdoch.duncan at gmail.com  Wed Mar 19 13:52:36 2014
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Wed, 19 Mar 2014 08:52:36 -0400
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>
References: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>
Message-ID: <53299314.6080608@gmail.com>

I don't see why CRAN needs to be involved in this effort at all.  A 
third party could take snapshots of CRAN at R release dates, and make 
those available to package users in a separate repository.  It is not 
hard to set a different repository than CRAN as the default location 
from which to obtain packages.

The only objection I can see to this is that it requires extra work by 
the third party, rather than extra work by the CRAN team. I don't think 
the total amount of work required is much different.  I'm very 
unsympathetic to proposals to dump work on others.

Duncan Murdoch

On 18/03/2014 4:24 PM, Jeroen Ooms wrote:
> This came up again recently with an irreproducible paper. Below an
> attempt to make a case for extending the r-devel/r-release cycle to
> CRAN packages. These suggestions are not in any way intended as
> criticism on anyone or the status quo.
>
> The proposal described in [1] is to freeze a snapshot of CRAN along
> with every release of R. In this design, updates for contributed
> packages treated the same as updates for base packages in the sense
> that they are only published to the r-devel branch of CRAN and do not
> affect users of "released" versions of R. Thereby all users, stacks
> and applications using a particular version of R will by default be
> using the identical version of each CRAN package. The bioconductor
> project uses similar policies.
>
> This system has several important advantages:
>
> ## Reproducibility
>
> Currently r/sweave/knitr scripts are unstable because of ambiguity
> introduced by constantly changing cran packages. This causes scripts
> to break or change behavior when upstream packages are updated, which
> makes reproducing old results extremely difficult.
>
> A common counter-argument is that script authors should document
> package versions used in the script using sessionInfo(). However even
> if authors would manually do this, reconstructing the author's
> environment from this information is cumbersome and often nearly
> impossible, because binary packages might no longer be available,
> dependency conflicts, etc. See [1] for a worked example. In practice,
> the current system causes many results or documents generated with R
> no to be reproducible, sometimes already after a few months.
>
> In a system where contributed packages inherit the r-base release
> cycle, scripts will behave the same across users/systems/time within a
> given version of R. This severely reduces ambiguity of R behavior, and
> has the potential of making reproducibility a natural part of the
> language, rather than a tedious exercise.
>
> ## Repository Management
>
> Just like scripts suffer from upstream changes, so do packages
> depending on other packages. A particular package that has been
> developed and tested against the current version of a particular
> dependency is not guaranteed to work against *any future version* of
> that dependency. Therefore, packages inevitably break over time as
> their dependencies are updated.
>
> One recent example is the Rcpp 0.11 release, which required all
> reverse dependencies to be rebuild/modified. This updated caused some
> serious disruption on our production servers. Initially we refrained
> from updating Rcpp on these servers to prevent currently installed
> packages depending on Rcpp to stop working. However soon after the
> Rcpp 0.11 release, many other cran packages started to require Rcpp >=
> 0.11, and our users started complaining about not being able to
> install those packages. This resulted in the impossible situation
> where currently installed packages would not work with the new Rcpp,
> but newly installed packages would not work with the old Rcpp.
>
> Current CRAN policies blame this problem on package authors. However
> as is explained in [1], this policy does not solve anything, is
> unsustainable with growing repository size, and sets completely the
> wrong incentives for contributing code. Progress comes with breaking
> changes, and the system should be able to accommodate this. Much of
> the trouble could have been prevented by a system that does not push
> bleeding edge updates straight to end-users, but has a devel branch
> where conflicts are resolved before publishing them in the next
> r-release.
>
> ## Reliability
>
> Another example, this time on a very small scale. We recently
> discovered that R code plotting medal counts from the Sochi Olympics
> generated different results for users on OSX than it did on
> Linux/Windows. After some debugging, we narrowed it down to the XML
> package. The application used the following code to scrape results
> from the Sochi website:
>
> XML::readHTMLTable("http://www.sochi2014.com/en/speed-skating", which=2, skip=1)
>
> This code was developed and tested on mac, but results in a different
> winner on windows/linux. This happens because the current version of
> the XML package on CRAN is 3.98, but the latest mac binary is 3.95.
> Apparently this new version of XML introduces a tiny change that
> causes html-table-headers to become colnames, rather than a row in the
> matrix, resulting in different medal counts.
>
> This example illustrates that we should never assume package versions
> to be interchangeable. Any small bugfix release can have side effects
> altering results. It is impossible to protect code against such
> upstream changes using CMD check or unit testing. All R scripts and
> packages are really only developed and tested for a single version of
> their dependencies. Assuming anything else makes results
> untrustworthy, and code unreliable.
>
> ## Summary
>
> Extending the r-release cycle to CRAN seems like a solution that would
> be easy to implement. Package updates simply only get pushed to the
> r-devel branches of cran, rather than r-release and r-release-old.
> This separates development from production/use in a way that is common
> sense in most open source communities. Benefits for R include:
>
> - Regular R users (statisticians, researchers, students, teachers) can
> share their homemade scripts/documents/packages and rely on them to
> work and produce the same results within a given version of R, without
> manual efforts to manage package versions.
>
> - Package authors can publish breaking changes to the devel branch
> without causing major disruption or affecting users and/or
> maintainers. Authors of depending packages have a timeframe to sync
> their package with upstream changes before the next release.
>
> - CRAN maintainers can focus quality control and testing efforts on
> the devel branch around the time of the code freeze. No need for
> crisis management when a package update introduces some severe
> breaking changes. Users of released versions are unaffected.
>
>
> [1] http://journal.r-project.org/archive/2013-1/ooms.pdf
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From kasperdanielhansen at gmail.com  Wed Mar 19 15:00:23 2014
From: kasperdanielhansen at gmail.com (Kasper Daniel Hansen)
Date: Wed, 19 Mar 2014 10:00:23 -0400
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <CAPPM_gS1Ey2UyyBSCqecT=NzWXKtREBoXmqNBussR8Hv1Zok3g@mail.gmail.com>
References: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>
	<CAPPM_gS1Ey2UyyBSCqecT=NzWXKtREBoXmqNBussR8Hv1Zok3g@mail.gmail.com>
Message-ID: <CAC2h7utmzLPR-cT1pDpnH7gvE0Fzjy5JO363sqBj-f621t22MQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140319/e15acf29/attachment.pl>

From edd at debian.org  Wed Mar 19 15:01:17 2014
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 19 Mar 2014 09:01:17 -0500
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <CAPPM_gS1Ey2UyyBSCqecT=NzWXKtREBoXmqNBussR8Hv1Zok3g@mail.gmail.com>
References: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>
	<CAPPM_gS1Ey2UyyBSCqecT=NzWXKtREBoXmqNBussR8Hv1Zok3g@mail.gmail.com>
Message-ID: <21289.41773.701608.905419@max.nulle.part>


Piling on:

On 19 March 2014 at 07:52, Joshua Ulrich wrote:
| There is nothing preventing you (or anyone else) from creating
| repositories that do what you suggest.  Create a CRAN mirror (or more
| than one) that only include the package versions you think they
| should.  Then have your production servers use it (them) instead of
| CRAN.
| 
| Better yet, make those repositories public.  If many people like your
| idea, they will use your new repositories instead of CRAN.  There is
| no reason to impose this change on all world-wide CRAN users.

On 19 March 2014 at 08:52, Duncan Murdoch wrote:
| I don't see why CRAN needs to be involved in this effort at all.  A 
| third party could take snapshots of CRAN at R release dates, and make 
| those available to package users in a separate repository.  It is not 
| hard to set a different repository than CRAN as the default location 
| from which to obtain packages.
| 
| The only objection I can see to this is that it requires extra work by 
| the third party, rather than extra work by the CRAN team. I don't think 
| the total amount of work required is much different.  I'm very 
| unsympathetic to proposals to dump work on others.


And to a first approximation some of those efforts already exist:

  -- 200+ r-cran-* packages in Debian proper

  -- 2000+ r-cran-* packages in Michael's c2d4u (via launchpad)

  -- 5000+ r-cran-* packages in Don's debian-r.debian.net

The only difference here is that Jeroen wants to organize source packages.
But that is just a matter of stacking them in directory trees and calling

    setwd("/path/to/root/of/your/repo/version")
    tools::write_PACKAGES(".", type="source")'

to create PACKAGES and PACKAGES.gz.

Dirk

-- 
Dirk Eddelbuettel | edd at debian.org | http://dirk.eddelbuettel.com


From h.wickham at gmail.com  Wed Mar 19 15:17:47 2014
From: h.wickham at gmail.com (Hadley Wickham)
Date: Wed, 19 Mar 2014 09:17:47 -0500
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <CAC2h7utmzLPR-cT1pDpnH7gvE0Fzjy5JO363sqBj-f621t22MQ@mail.gmail.com>
References: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>
	<CAPPM_gS1Ey2UyyBSCqecT=NzWXKtREBoXmqNBussR8Hv1Zok3g@mail.gmail.com>
	<CAC2h7utmzLPR-cT1pDpnH7gvE0Fzjy5JO363sqBj-f621t22MQ@mail.gmail.com>
Message-ID: <CABdHhvHyKqu4mSn6Ziw=GgOR8pQOXnGRXW4fZr8H0z41Gkpo4g@mail.gmail.com>

> What would be more useful in terms of reproducibility is the capability of
> installing a specific version of a package from a repository using
> install.packages(), which would require archiving older versions in a
> coordinated fashion. I know CRAN archives old versions, but I am not aware
> if we can programmatically query the repository about this.

See devtools::install_version().

The main caveat is that you also need to be able to build the package,
and ensure you have dependencies that work with that version.

Hadley


-- 
http://had.co.nz/


From geoffjentry at hexdump.org  Wed Mar 19 16:22:22 2014
From: geoffjentry at hexdump.org (Geoff Jentry)
Date: Wed, 19 Mar 2014 08:22:22 -0700 (PDT)
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>
References: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>
Message-ID: <alpine.DEB.2.00.1403190818200.31617@cardinals.dreamhost.com>

> using the identical version of each CRAN package. The bioconductor
> project uses similar policies.

While I agree that this can be an issue, I don't think it is fair to 
compare CRAN to BioC. Unless things have changed, the latter has a more 
rigorous barrier to entry which includes buy in of various ideals (e.g. 
interoperability w/ other BioC packages, making use of BioC constructs, 
the official release cycle). All of that requires extra management 
overhead (read: human effort) which considering that CRAN isn't exactly 
swimming in spare cycles seems unlikely to happen.

It seems like one could set up a curated CRAN-a-like quite easily, 
advertise the heck out of it and let the "market" decide. That is, IMO, 
the beauty of open source.

-J


From jeroen.ooms at stat.ucla.edu  Wed Mar 19 18:59:31 2014
From: jeroen.ooms at stat.ucla.edu (Jeroen Ooms)
Date: Wed, 19 Mar 2014 10:59:31 -0700
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <53299314.6080608@gmail.com>
References: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>
	<53299314.6080608@gmail.com>
Message-ID: <CABFfbXuTvefJ0Nmc25JJEwhmUQge+K1Fi=oaTwoO_jfK75mUZg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140319/d91421a8/attachment.pl>

From spencer.graves at structuremonitoring.com  Wed Mar 19 19:36:10 2014
From: spencer.graves at structuremonitoring.com (Spencer Graves)
Date: Wed, 19 Mar 2014 11:36:10 -0700
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <CABFfbXuTvefJ0Nmc25JJEwhmUQge+K1Fi=oaTwoO_jfK75mUZg@mail.gmail.com>
References: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>	<53299314.6080608@gmail.com>
	<CABFfbXuTvefJ0Nmc25JJEwhmUQge+K1Fi=oaTwoO_jfK75mUZg@mail.gmail.com>
Message-ID: <5329E39A.7060805@structuremonitoring.com>

       What about having this purpose met with something like an 
expansion of R-Forge?  We could have packages submitted to R-Forge 
rather than CRAN, and people who wanted the latest could get it from 
R-Forge.  If changes I make on R-Forge break a reverse dependency, 
emails explaining the problem are sent to both me and the maintainer for 
the package I broke.


       The budget for R-Forge would almost certainly need to be 
increased:  They currently disable many of the tests they once ran.


       Regarding budget, the R Project would get more donations if they 
asked for them and made it easier to contribute.  I've tried multiple 
times without success to find a way to donate.  I didn't try hard, but 
it shouldn't be hard ;-)  (And donations should be accepted in US 
dollars and Euros -- and maybe other currencies.) There should be a 
procedure whereby anyone could receive a pro forma invoice, which they 
can pay or ignore as they choose.  I mention this, because many grants 
could cover a reasonable fee provided they have an invoice.


       Spencer Graves


On 3/19/2014 10:59 AM, Jeroen Ooms wrote:
> On Wed, Mar 19, 2014 at 5:52 AM, Duncan Murdoch <murdoch.duncan at gmail.com>wrote:
>
>> I don't see why CRAN needs to be involved in this effort at all.  A third
>> party could take snapshots of CRAN at R release dates, and make those
>> available to package users in a separate repository.  It is not hard to set
>> a different repository than CRAN as the default location from which to
>> obtain packages.
>>
> I am happy to see many people giving this some thought and engage in the
> discussion.
>
> Several have suggested that staging & freezing can be simply done by a
> third party. This solution and its limitations is also described in the
> paper [1] in the section titled "R: downstream staging and repackaging".
>
> If this would solve the problem without affecting CRAN, we would have been
> done this obviously. In fact, as described in the paper and pointed out by
> some people, initiatives such as Debian or Revolution Enterprise already
> include a frozen library of R packages. Also companies like Google maintain
> their own internal repository with packages that are used throughout the
> company.
>
> The problem with this approach is that when you using some 3rd party
> package snapshot, your r/sweave scripts will still only be
> reliable/reproducible for other users of that specific snapshot. E.g. for
> the examples above, a script that is written in R 3.0 by a Debian user is
> not guaranteed to work on R 3.0 in Google, or R 3.0 on some other 3rd party
> cran snapshot. Hence this solution merely redefines the problem from "this
> script depends on pkgA 1.1 and pkgB 0.2.3" to "this script depends on
> repository foo 2.0". And given that most users would still be pulling
> packages straight from CRAN, it would still be terribly difficult to
> reproduce a 5 year old sweave script from e.g. JSS.
>
> For this reason I believe the only effective place to organize this staging
> is all the way upstream, on CRAN. Imagine a world where your r/sweave
> script would be reliable/reproducible, out of the box, on any system, any
> platform in any company using on R 3.0. No need to investigate which
> specific packages or cran snapshot the author was using at the time of
> writing the script, and trying to reconstruct such libraries for each
> script you want to reproduce. No ambiguity about which package versions are
> used by R 3.0. However for better or worse, I think this could only be
> accomplished with a cran release cycle (i.e. "universal snapshots")
> accompanying the already existing r releases.
>
>
>
>> The only objection I can see to this is that it requires extra work by the
>> third party, rather than extra work by the CRAN team. I don't think the
>> total amount of work required is much different.  I'm very unsympathetic to
>> proposals to dump work on others.
>
> I am merely trying to discuss a technical issue in an attempt to improve
> reliability of our software and reproducibility of papers created with R.
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From josh.m.ulrich at gmail.com  Wed Mar 19 19:50:58 2014
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Wed, 19 Mar 2014 13:50:58 -0500
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <CABFfbXuTvefJ0Nmc25JJEwhmUQge+K1Fi=oaTwoO_jfK75mUZg@mail.gmail.com>
References: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>
	<53299314.6080608@gmail.com>
	<CABFfbXuTvefJ0Nmc25JJEwhmUQge+K1Fi=oaTwoO_jfK75mUZg@mail.gmail.com>
Message-ID: <CAPPM_gRP4xKEE1bFiR8Aaeo1XOX0kpu4p=EyOm2z+6fkVkJaag@mail.gmail.com>

On Wed, Mar 19, 2014 at 12:59 PM, Jeroen Ooms <jeroen.ooms at stat.ucla.edu> wrote:
> On Wed, Mar 19, 2014 at 5:52 AM, Duncan Murdoch <murdoch.duncan at gmail.com>wrote:
>
>> I don't see why CRAN needs to be involved in this effort at all.  A third
>> party could take snapshots of CRAN at R release dates, and make those
>> available to package users in a separate repository.  It is not hard to set
>> a different repository than CRAN as the default location from which to
>> obtain packages.
>>
>
> I am happy to see many people giving this some thought and engage in the
> discussion.
>
> Several have suggested that staging & freezing can be simply done by a
> third party. This solution and its limitations is also described in the
> paper [1] in the section titled "R: downstream staging and repackaging".
>
> If this would solve the problem without affecting CRAN, we would have been
> done this obviously. In fact, as described in the paper and pointed out by
> some people, initiatives such as Debian or Revolution Enterprise already
> include a frozen library of R packages. Also companies like Google maintain
> their own internal repository with packages that are used throughout the
> company.
>
The suggested solution is not described in the referenced article.  It
was not suggested that it be the operating system's responsibility to
distribute snapshots, nor was it suggested to create binary
repositories for specific operating systems, nor was it suggested to
freeze only a subset of CRAN packages.

> The problem with this approach is that when you using some 3rd party
> package snapshot, your r/sweave scripts will still only be
> reliable/reproducible for other users of that specific snapshot. E.g. for
> the examples above, a script that is written in R 3.0 by a Debian user is
> not guaranteed to work on R 3.0 in Google, or R 3.0 on some other 3rd party
> cran snapshot. Hence this solution merely redefines the problem from "this
> script depends on pkgA 1.1 and pkgB 0.2.3" to "this script depends on
> repository foo 2.0". And given that most users would still be pulling
> packages straight from CRAN, it would still be terribly difficult to
> reproduce a 5 year old sweave script from e.g. JSS.
>
This can be solved by the third party making the repository public.

> For this reason I believe the only effective place to organize this staging
> is all the way upstream, on CRAN. Imagine a world where your r/sweave
> script would be reliable/reproducible, out of the box, on any system, any
> platform in any company using on R 3.0. No need to investigate which
> specific packages or cran snapshot the author was using at the time of
> writing the script, and trying to reconstruct such libraries for each
> script you want to reproduce. No ambiguity about which package versions are
> used by R 3.0. However for better or worse, I think this could only be
> accomplished with a cran release cycle (i.e. "universal snapshots")
> accompanying the already existing r releases.
>
This could be done by a public third-party repository, independent of
CRAN.  However, you would need to find a way to actively _prevent_
people from installing newer versions of packages with the stable R
releases.

--
Joshua Ulrich  |  about.me/joshuaulrich
FOSS Trading  |  www.fosstrading.com


From cboettig at gmail.com  Wed Mar 19 19:58:10 2014
From: cboettig at gmail.com (Carl Boettiger)
Date: Wed, 19 Mar 2014 11:58:10 -0700
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <5329E39A.7060805@structuremonitoring.com>
References: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>
	<53299314.6080608@gmail.com>
	<CABFfbXuTvefJ0Nmc25JJEwhmUQge+K1Fi=oaTwoO_jfK75mUZg@mail.gmail.com>
	<5329E39A.7060805@structuremonitoring.com>
Message-ID: <CAN_1p9w2Yyw3rV20EAeVTgGJtsDpXiQ1Jo1OF+k_p42tR8q2NA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140319/b48aca42/attachment.pl>

From jeroen.ooms at stat.ucla.edu  Wed Mar 19 20:09:46 2014
From: jeroen.ooms at stat.ucla.edu (Jeroen Ooms)
Date: Wed, 19 Mar 2014 12:09:46 -0700
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <CAC2h7utmzLPR-cT1pDpnH7gvE0Fzjy5JO363sqBj-f621t22MQ@mail.gmail.com>
References: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>
	<CAPPM_gS1Ey2UyyBSCqecT=NzWXKtREBoXmqNBussR8Hv1Zok3g@mail.gmail.com>
	<CAC2h7utmzLPR-cT1pDpnH7gvE0Fzjy5JO363sqBj-f621t22MQ@mail.gmail.com>
Message-ID: <CABFfbXvYVLOLN1HUJthwsoSUrhBafvueiP_wmgYswc58BfkjTA@mail.gmail.com>

On Wed, Mar 19, 2014 at 7:00 AM, Kasper Daniel Hansen
<kasperdanielhansen at gmail.com> wrote:
> Our experience in Bioconductor is that this is a pretty hard problem.
>
> What the OP presumably wants is some guarantee that all packages on CRAN work well together.

Obviously we can not guarantee that all packages on CRAN work
together. But what we can do is prevent problems that are introduced
by version ambiguity. If author develops and tests a script/package
with dependency Rcpp 0.10.6, the best chance of making that script or
package work for other users is using Rcpp 0.10.6.

This especially holds if there is a big time difference between the
author creating the pkg/script and someone using it. In practice most
Sweave/knitr scripts used for generating papers and articles can not
be reproduced after a while because the dependency packages have
changed in the mean time. These problem can largely be mitigated with
a release cycle.

I am not arguing that anyone should put manual effort into testing
that packages work together. On the contrary: a system that separates
development from released branches prevents you from having to
continuously test all reverse dependencies for every package update.

My argument is simply that many problems introduced by version
ambiguity can be prevented if we can unite the entire R community
around using a single version of each CRAN package for every specific
release of R. Similar to how linux distributions use a single version
of each software package in a particular release of the distribution.


From hpages at fhcrc.org  Wed Mar 19 22:00:23 2014
From: hpages at fhcrc.org (=?UTF-8?B?SGVydsOpIFBhZ8Oocw==?=)
Date: Wed, 19 Mar 2014 14:00:23 -0700
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <CAC2h7utmzLPR-cT1pDpnH7gvE0Fzjy5JO363sqBj-f621t22MQ@mail.gmail.com>
References: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>	<CAPPM_gS1Ey2UyyBSCqecT=NzWXKtREBoXmqNBussR8Hv1Zok3g@mail.gmail.com>
	<CAC2h7utmzLPR-cT1pDpnH7gvE0Fzjy5JO363sqBj-f621t22MQ@mail.gmail.com>
Message-ID: <532A0567.9010400@fhcrc.org>

Hi,

On 03/19/2014 07:00 AM, Kasper Daniel Hansen wrote:
> Our experience in Bioconductor is that this is a pretty hard problem.

What's hard and requires a substantial amount of human resources is to
run our build system (set up the build machines, keep up with changes
in R, babysit the builds, assist developers with build issues, etc...)

But *freezing* the CRAN packages for each version of R is *very* easy
to do. The CRAN maintainers already do it for the binary packages.
What could be the reason for not doing it for source packages too?
Maybe in prehistoric times there was this belief that a source package
was aimed to remain compatible with all versions of R, present and
future, but that dream is dead and gone...

Right now the layout of the CRAN package repo is:

   ??? src
   ?   ??? contrib
   ??? bin
       ??? windows
       ?   ??? contrib
       ?       ? ...
       ?       ? 3.0
       ?       ? 3.1
       ?       ? ...
       ??? macosx
           ??? contrib
               ? ...
               ? 3.0
               ? 3.1
               ? ...

when it could be:

   ??? 3.0
   ?   ??? src
   ?   ?   ??? contrib
   ?   ??? bin
   ?       ??? windows
   ?       ?   ??? contrib
   ?       ??? macosx
   ?           ??? contrib
   ??? 3.1
   ?   ??? src
   ?   ?   ??? contrib
   ?   ??? bin
   ?       ??? windows
   ?       ?   ??? contrib
   ?       ??? macosx
   ?           ??? contrib
   ??? ...

That is: the split by version is done at the top, not at the bottom.

It doesn't use more disk space than the current layout (you can just
throw the src/contrib/Archive/ folder away, there is no more need
for it).

install.packages() and family would need to be modified a little bit
to work with this new layout. And that's all!

The never ending changes in Mac OS X binary formats can be handled
in a cleaner way i.e. no more symlinks under bin/macosx to keep
backward compatibility with different binary formats and with old
versions of install.packages().

Then in 10 years from now, you can reproduce an analysis that you
did today with R-3.0. Because when you'll install R-3.0 and the
packages required for this analysis, you'll end up with exactly
the same packages as today.

Cheers,
H.

>
> What the OP presumably wants is some guarantee that all packages on CRAN
> work well together.  A good example is when Rcpp was updated, it broke
> other packages (quick note: The Rcpp developers do a incredible amount of
> work to deal with this; it is almost impossible to not have a few days of
> chaos).  Ensuring this is not a trivial task, and it requires some buy-in
> both from the "repository" and from the developers.
>
> For Bioconductor it is even harder as the dependency graph of Bioconductor
> is much more involved than the one for CRAN, where most packages depends
> only on a few other packages.  This is why we need to do this for Bioc.
>
> Based on my experience with CRAN I am not sure I see a need for a
> coordinated release (or rather, I can sympathize with the need, but I don't
> think the effort is worth it).
>
> What would be more useful in terms of reproducibility is the capability of
> installing a specific version of a package from a repository using
> install.packages(), which would require archiving older versions in a
> coordinated fashion. I know CRAN archives old versions, but I am not aware
> if we can programmatically query the repository about this.
>
> Best,
> Kasper
>
>
> On Wed, Mar 19, 2014 at 8:52 AM, Joshua Ulrich <josh.m.ulrich at gmail.com>wrote:
>
>> On Tue, Mar 18, 2014 at 3:24 PM, Jeroen Ooms <jeroen.ooms at stat.ucla.edu>
>> wrote:
>> <snip>
>>> ## Summary
>>>
>>> Extending the r-release cycle to CRAN seems like a solution that would
>>> be easy to implement. Package updates simply only get pushed to the
>>> r-devel branches of cran, rather than r-release and r-release-old.
>>> This separates development from production/use in a way that is common
>>> sense in most open source communities. Benefits for R include:
>>>
>> Nothing is ever as simple as it seems (especially from the perspective
>> of one who won't be doing the work).
>>
>> There is nothing preventing you (or anyone else) from creating
>> repositories that do what you suggest.  Create a CRAN mirror (or more
>> than one) that only include the package versions you think they
>> should.  Then have your production servers use it (them) instead of
>> CRAN.
>>
>> Better yet, make those repositories public.  If many people like your
>> idea, they will use your new repositories instead of CRAN.  There is
>> no reason to impose this change on all world-wide CRAN users.
>>
>> Best,
>> --
>> Joshua Ulrich  |  about.me/joshuaulrich
>> FOSS Trading  |  www.fosstrading.com
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fhcrc.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From jeroen.ooms at stat.ucla.edu  Wed Mar 19 22:28:39 2014
From: jeroen.ooms at stat.ucla.edu (Jeroen Ooms)
Date: Wed, 19 Mar 2014 14:28:39 -0700
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <CAPPM_gRP4xKEE1bFiR8Aaeo1XOX0kpu4p=EyOm2z+6fkVkJaag@mail.gmail.com>
References: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>
	<53299314.6080608@gmail.com>
	<CABFfbXuTvefJ0Nmc25JJEwhmUQge+K1Fi=oaTwoO_jfK75mUZg@mail.gmail.com>
	<CAPPM_gRP4xKEE1bFiR8Aaeo1XOX0kpu4p=EyOm2z+6fkVkJaag@mail.gmail.com>
Message-ID: <CABFfbXtmFyoyaRSU6LyWrUzyXvvkSV14yD3vc0c2fzRcHbtNzQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140319/53d3c3bb/attachment.pl>

From cgenolin at u-paris10.fr  Wed Mar 19 22:58:44 2014
From: cgenolin at u-paris10.fr (Christophe Genolini)
Date: Wed, 19 Mar 2014 22:58:44 +0100
Subject: [Rd] Memcheck: Invalid  read of size 4
Message-ID: <532A1314.8040206@u-paris10.fr>

Hi the list,

One of my package has a memory issue that I do not manage to understand. The Memtest notes is here:
<http://www.stats.ox.ac.uk/pub/bdr/memtests/valgrind/kml-Ex.Rout>

Here is the message that I get from Memtest

--- 8< ----------------
  ~ Fast KmL ~
==27283== Invalid read of size 4
==27283==    at 0x10C5DF28: kml1 (kml.c:183)
...
==27283==    by 0x10C5DE4F: kml1 (kml.c:151)
...
==27283==    at 0x10C5DF90: kml1 (kml.c:198)
--- 8< ----------------


Here is the function kml1 from the file kml.c (I add some comments to tag the lines 151, 183 and 198)

--- 8< ----------------
void kml1(double *traj, int *nbInd, int *nbTime, int *nbClusters, int *maxIt, int *clusterAffectation1, int *convergenceTime){

     int i=0,iter=0;
     int *clusterAffectation2=malloc(*nbInd * sizeof(int));                      // lines 151
     double *trajMean=malloc(*nbClusters * *nbTime * sizeof(double));

     for(i = 0; i < *nbClusters * *nbTime; i++){trajMean[i] = 0.0;};
     for(i = 0; i < *nbInd; i++){clusterAffectation2[i] = 0;};

     for(iter = 0; iter < *maxIt; iter+=2){
	calculMean(traj,nbInd,nbTime,clusterAffectation1,nbClusters,trajMean);
        	affecteIndiv(traj,nbInd,nbTime,trajMean,nbClusters,clusterAffectation2);

	i = 0;
	while(clusterAffectation1[i]==clusterAffectation2[i] && i <*nbInd){i++;}; // lines 183
	if(i == *nbInd){
	    *convergenceTime = iter + 1;
	    break;
	}else{};

	calculMean(traj,nbInd,nbTime,clusterAffectation2,nbClusters,trajMean);
	affecteIndiv(traj,nbInd,nbTime,trajMean,nbClusters,clusterAffectation1);

	i = 0;
	while(clusterAffectation1[i]==clusterAffectation2[i] && i<*nbInd){i++;}; // lines 198
	if(i == *nbInd){
	    *convergenceTime = iter + 2;
       	    break;
	}else{};
     }
}
--- 8< ----------------

Do you know what is wrong in my C code?
Thanks

Christophe

-- 
Christophe Genolini
Ma?tre de conf?rences en bio-statistique
Universit? Paris Ouest Nanterre La D?fense
INSERM UMR 1027


From josh.m.ulrich at gmail.com  Wed Mar 19 22:59:53 2014
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Wed, 19 Mar 2014 16:59:53 -0500
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <CABFfbXtmFyoyaRSU6LyWrUzyXvvkSV14yD3vc0c2fzRcHbtNzQ@mail.gmail.com>
References: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>
	<53299314.6080608@gmail.com>
	<CABFfbXuTvefJ0Nmc25JJEwhmUQge+K1Fi=oaTwoO_jfK75mUZg@mail.gmail.com>
	<CAPPM_gRP4xKEE1bFiR8Aaeo1XOX0kpu4p=EyOm2z+6fkVkJaag@mail.gmail.com>
	<CABFfbXtmFyoyaRSU6LyWrUzyXvvkSV14yD3vc0c2fzRcHbtNzQ@mail.gmail.com>
Message-ID: <CAPPM_gR7pP7eq2Gg3nT4osSw1S69hvEW=yCx3-Q7T1V0fb19Vg@mail.gmail.com>

On Wed, Mar 19, 2014 at 4:28 PM, Jeroen Ooms <jeroen.ooms at stat.ucla.edu> wrote:
> On Wed, Mar 19, 2014 at 11:50 AM, Joshua Ulrich <josh.m.ulrich at gmail.com>
> wrote:
>>
>> The suggested solution is not described in the referenced article.  It
>> was not suggested that it be the operating system's responsibility to
>> distribute snapshots, nor was it suggested to create binary
>> repositories for specific operating systems, nor was it suggested to
>> freeze only a subset of CRAN packages.
>
>
> IMO this is an implementation detail. If we could all agree on a particular
> set of cran packages to be used with a certain release of R, then it doesn't
> matter how the 'snapshotting' gets implemented. It could be a separate
> repository, or a directory on cran with symbolic links, or a page somewhere
> with hyperlinks to the respective source packages. Or you can put all
> packages in a big zip file, or include it in your OS distribution. You can
> even distribute your entire repo on cdroms (debian style!) or do all of the
> above.
>
> The hard problem is not implementation. The hard part is that for
> reproducibility to work, we need community wide conventions on which
> versions of cran packages are used by a particular release of R. Local
> downstream solutions are impractical, because this results in
> scripts/packages that only work within your niche using this particular
> snapshot. I expect that requiring every script be executed in the context of
> dependencies from some particular third party repository will make
> reproducibility even less common. Therefore I am trying to make a case for a
> solution that would naturally improve reliability/reproducibility of R code
> without any effort by the end-user.
>
So implementation isn't a problem.  The problem is that you need a way
to force people not to be able to use different package versions than
what existed at the time of each R release.  I said this in my
previous email, but you removed and did not address it: "However, you
would need to find a way to actively _prevent_ people from installing
newer versions of packages with the stable R releases."  Frankly, I
would stop using CRAN if this policy were adopted.

I suggest you go build this yourself.  You have all the code available
on CRAN, and the dates at which each package was published.  If others
who care about reproducible research find what you've built useful,
you will create the very community you want.  And you won't have to
force one single person to change their workflow.

Best,
--
Joshua Ulrich  |  about.me/joshuaulrich
FOSS Trading  |  www.fosstrading.com


From dtenenba at fhcrc.org  Wed Mar 19 23:11:53 2014
From: dtenenba at fhcrc.org (Dan Tenenbaum)
Date: Wed, 19 Mar 2014 15:11:53 -0700 (PDT)
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <CAPPM_gR7pP7eq2Gg3nT4osSw1S69hvEW=yCx3-Q7T1V0fb19Vg@mail.gmail.com>
Message-ID: <1155117970.958856.1395267113693.JavaMail.root@fhcrc.org>



----- Original Message -----
> From: "Joshua Ulrich" <josh.m.ulrich at gmail.com>
> To: "Jeroen Ooms" <jeroen.ooms at stat.ucla.edu>
> Cc: "r-devel" <r-devel at r-project.org>
> Sent: Wednesday, March 19, 2014 2:59:53 PM
> Subject: Re: [Rd] [RFC] A case for freezing CRAN
> 
> On Wed, Mar 19, 2014 at 4:28 PM, Jeroen Ooms
> <jeroen.ooms at stat.ucla.edu> wrote:
> > On Wed, Mar 19, 2014 at 11:50 AM, Joshua Ulrich
> > <josh.m.ulrich at gmail.com>
> > wrote:
> >>
> >> The suggested solution is not described in the referenced article.
> >>  It
> >> was not suggested that it be the operating system's responsibility
> >> to
> >> distribute snapshots, nor was it suggested to create binary
> >> repositories for specific operating systems, nor was it suggested
> >> to
> >> freeze only a subset of CRAN packages.
> >
> >
> > IMO this is an implementation detail. If we could all agree on a
> > particular
> > set of cran packages to be used with a certain release of R, then
> > it doesn't
> > matter how the 'snapshotting' gets implemented. It could be a
> > separate
> > repository, or a directory on cran with symbolic links, or a page
> > somewhere
> > with hyperlinks to the respective source packages. Or you can put
> > all
> > packages in a big zip file, or include it in your OS distribution.
> > You can
> > even distribute your entire repo on cdroms (debian style!) or do
> > all of the
> > above.
> >
> > The hard problem is not implementation. The hard part is that for
> > reproducibility to work, we need community wide conventions on
> > which
> > versions of cran packages are used by a particular release of R.
> > Local
> > downstream solutions are impractical, because this results in
> > scripts/packages that only work within your niche using this
> > particular
> > snapshot. I expect that requiring every script be executed in the
> > context of
> > dependencies from some particular third party repository will make
> > reproducibility even less common. Therefore I am trying to make a
> > case for a
> > solution that would naturally improve reliability/reproducibility
> > of R code
> > without any effort by the end-user.
> >
> So implementation isn't a problem.  The problem is that you need a
> way
> to force people not to be able to use different package versions than
> what existed at the time of each R release.  I said this in my
> previous email, but you removed and did not address it: "However, you
> would need to find a way to actively _prevent_ people from installing
> newer versions of packages with the stable R releases."  Frankly, I
> would stop using CRAN if this policy were adopted.
> 

I don't see how the proposal forces anyone to do anything. If you have an old version of R and you still want to install newer versions of packages, you can download them from their CRAN landing page. As I understand it, the proposal only addresses what packages would be installed **by default** for a given version of R.

People would be free to override those default settings (by downloading newer packages as described above) but they should then not expect to be able to reproduce an earlier analysis since they'll have the wrong package versions. If they don't care, that's fine (provided that no other problems arise, such as the newer package depending on a feature of R that doesn't exist in the version you're running).

Dan

> I suggest you go build this yourself.  You have all the code
> available
> on CRAN, and the dates at which each package was published.  If
> others
> who care about reproducible research find what you've built useful,
> you will create the very community you want.  And you won't have to
> force one single person to change their workflow.
> 
> Best,
> --
> Joshua Ulrich  |  about.me/joshuaulrich
> FOSS Trading  |  www.fosstrading.com
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From jeroen.ooms at stat.ucla.edu  Wed Mar 19 23:16:41 2014
From: jeroen.ooms at stat.ucla.edu (Jeroen Ooms)
Date: Wed, 19 Mar 2014 15:16:41 -0700
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <CAPPM_gR7pP7eq2Gg3nT4osSw1S69hvEW=yCx3-Q7T1V0fb19Vg@mail.gmail.com>
References: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>
	<53299314.6080608@gmail.com>
	<CABFfbXuTvefJ0Nmc25JJEwhmUQge+K1Fi=oaTwoO_jfK75mUZg@mail.gmail.com>
	<CAPPM_gRP4xKEE1bFiR8Aaeo1XOX0kpu4p=EyOm2z+6fkVkJaag@mail.gmail.com>
	<CABFfbXtmFyoyaRSU6LyWrUzyXvvkSV14yD3vc0c2fzRcHbtNzQ@mail.gmail.com>
	<CAPPM_gR7pP7eq2Gg3nT4osSw1S69hvEW=yCx3-Q7T1V0fb19Vg@mail.gmail.com>
Message-ID: <CABFfbXteYwBwE018xf9aa_z95h3k3OquT2_BQmrsL6n4W57tWA@mail.gmail.com>

On Wed, Mar 19, 2014 at 2:59 PM, Joshua Ulrich <josh.m.ulrich at gmail.com> wrote:
>
> So implementation isn't a problem.  The problem is that you need a way
> to force people not to be able to use different package versions than
> what existed at the time of each R release.  I said this in my
> previous email, but you removed and did not address it: "However, you
> would need to find a way to actively _prevent_ people from installing
> newer versions of packages with the stable R releases."  Frankly, I
> would stop using CRAN if this policy were adopted.

I am not proposing to "force" anything to anyone, those are your
words. Please read the proposal more carefully before derailing the
discussion. Below *verbatim* a section from the paper:

To fully make the transition to a staged CRAN, the default behavior of
the package manager must be modified to download packages from the
stable branch of the current version of R, rather than the latest
development release. As such, all users on a given version of R will
be using the same version of each CRAN package, regardless on when it
was installed. The user could still be given an option to try and
install the development version from the unstable branch, for example
by adding an additional parameter to install.packages named
devel=TRUE. However when installing an unstable package, it must be
flagged, and the user must be warned that this version is not properly
tested and might not be working as expected. Furthermore, when loading
this package a warning could be shown with the version number so that
it is also obvious from the output that results were produced using a
non-standard version of the contributed package. Finally, users that
would always like to use the very latest versions of all packages,
e.g. developers, could install the r-devel release of R. This version
contains the latest commits by R Core and downloads packages from the
devel branch on CRAN, but should not be used or in production or
reproducible research settings.


From pdalgd at gmail.com  Wed Mar 19 23:17:13 2014
From: pdalgd at gmail.com (peter dalgaard)
Date: Wed, 19 Mar 2014 23:17:13 +0100
Subject: [Rd] Memcheck: Invalid  read of size 4
In-Reply-To: <532A1314.8040206@u-paris10.fr>
References: <532A1314.8040206@u-paris10.fr>
Message-ID: <373A4631-6C57-493C-A347-0BAB256C507A@gmail.com>


On 19 Mar 2014, at 22:58 , Christophe Genolini <cgenolin at u-paris10.fr> wrote:

> Hi the list,
> 
> One of my package has a memory issue that I do not manage to understand. The Memtest notes is here:
> <http://www.stats.ox.ac.uk/pub/bdr/memtests/valgrind/kml-Ex.Rout>
> 
> Here is the message that I get from Memtest
> 
> --- 8< ----------------
> ~ Fast KmL ~
> ==27283== Invalid read of size 4
> ==27283==    at 0x10C5DF28: kml1 (kml.c:183)
> ...
> ==27283==    by 0x10C5DE4F: kml1 (kml.c:151)
> ...
> ==27283==    at 0x10C5DF90: kml1 (kml.c:198)
> --- 8< ----------------
> 
> 
> Here is the function kml1 from the file kml.c (I add some comments to tag the lines 151, 183 and 198)
> 
> --- 8< ----------------
> void kml1(double *traj, int *nbInd, int *nbTime, int *nbClusters, int *maxIt, int *clusterAffectation1, int *convergenceTime){
> 
>    int i=0,iter=0;
>    int *clusterAffectation2=malloc(*nbInd * sizeof(int));                      // lines 151
>    double *trajMean=malloc(*nbClusters * *nbTime * sizeof(double));
> 
>    for(i = 0; i < *nbClusters * *nbTime; i++){trajMean[i] = 0.0;};
>    for(i = 0; i < *nbInd; i++){clusterAffectation2[i] = 0;};
> 
>    for(iter = 0; iter < *maxIt; iter+=2){
> 	calculMean(traj,nbInd,nbTime,clusterAffectation1,nbClusters,trajMean);
>       	affecteIndiv(traj,nbInd,nbTime,trajMean,nbClusters,clusterAffectation2);
> 
> 	i = 0;
> 	while(clusterAffectation1[i]==clusterAffectation2[i] && i <*nbInd){i++;}; // lines 183
> 	if(i == *nbInd){
> 	    *convergenceTime = iter + 1;
> 	    break;
> 	}else{};
> 
> 	calculMean(traj,nbInd,nbTime,clusterAffectation2,nbClusters,trajMean);
> 	affecteIndiv(traj,nbInd,nbTime,trajMean,nbClusters,clusterAffectation1);
> 
> 	i = 0;
> 	while(clusterAffectation1[i]==clusterAffectation2[i] && i<*nbInd){i++;}; // lines 198
> 	if(i == *nbInd){
> 	    *convergenceTime = iter + 2;
>      	    break;
> 	}else{};
>    }
> }
> --- 8< ----------------
> 
> Do you know what is wrong in my C code?

Yes. You need to reverse operands of &&. Otherwise you'll be indexing with i==*nbind before finding that (i < *nbind) is false. 

> Thanks
> 
> Christophe
> 
> -- 
> Christophe Genolini
> Ma?tre de conf?rences en bio-statistique
> Universit? Paris Ouest Nanterre La D?fense
> INSERM UMR 1027
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From josh.m.ulrich at gmail.com  Wed Mar 19 23:42:41 2014
From: josh.m.ulrich at gmail.com (Joshua Ulrich)
Date: Wed, 19 Mar 2014 17:42:41 -0500
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <CABFfbXteYwBwE018xf9aa_z95h3k3OquT2_BQmrsL6n4W57tWA@mail.gmail.com>
References: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>
	<53299314.6080608@gmail.com>
	<CABFfbXuTvefJ0Nmc25JJEwhmUQge+K1Fi=oaTwoO_jfK75mUZg@mail.gmail.com>
	<CAPPM_gRP4xKEE1bFiR8Aaeo1XOX0kpu4p=EyOm2z+6fkVkJaag@mail.gmail.com>
	<CABFfbXtmFyoyaRSU6LyWrUzyXvvkSV14yD3vc0c2fzRcHbtNzQ@mail.gmail.com>
	<CAPPM_gR7pP7eq2Gg3nT4osSw1S69hvEW=yCx3-Q7T1V0fb19Vg@mail.gmail.com>
	<CABFfbXteYwBwE018xf9aa_z95h3k3OquT2_BQmrsL6n4W57tWA@mail.gmail.com>
Message-ID: <CAPPM_gR4kvVxk+sWqYGRxLyaPzw5bMr=kM==-o7sTKGpJy1iug@mail.gmail.com>

On Wed, Mar 19, 2014 at 5:16 PM, Jeroen Ooms <jeroen.ooms at stat.ucla.edu> wrote:
> On Wed, Mar 19, 2014 at 2:59 PM, Joshua Ulrich <josh.m.ulrich at gmail.com> wrote:
>>
>> So implementation isn't a problem.  The problem is that you need a way
>> to force people not to be able to use different package versions than
>> what existed at the time of each R release.  I said this in my
>> previous email, but you removed and did not address it: "However, you
>> would need to find a way to actively _prevent_ people from installing
>> newer versions of packages with the stable R releases."  Frankly, I
>> would stop using CRAN if this policy were adopted.
>
> I am not proposing to "force" anything to anyone, those are your
> words. Please read the proposal more carefully before derailing the
> discussion. Below *verbatim* a section from the paper:
>
<snip>

Yes "force" is too strong a word.  You want a barrier (however small)
to prevent people from installing newer (or older) versions of
packages than those that correspond to a given R release.

I still think you're going to have a very hard time convincing CRAN
maintainers to take up your cause, even if you were to build support
for it.  Especially because there's nothing stopping anyone else from
doing it.

--
Joshua Ulrich  |  about.me/joshuaulrich
FOSS Trading  |  www.fosstrading.com


From hpages at fhcrc.org  Wed Mar 19 23:57:27 2014
From: hpages at fhcrc.org (=?ISO-8859-1?Q?Herv=E9_Pag=E8s?=)
Date: Wed, 19 Mar 2014 15:57:27 -0700
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <CAPPM_gR7pP7eq2Gg3nT4osSw1S69hvEW=yCx3-Q7T1V0fb19Vg@mail.gmail.com>
References: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>	<53299314.6080608@gmail.com>	<CABFfbXuTvefJ0Nmc25JJEwhmUQge+K1Fi=oaTwoO_jfK75mUZg@mail.gmail.com>	<CAPPM_gRP4xKEE1bFiR8Aaeo1XOX0kpu4p=EyOm2z+6fkVkJaag@mail.gmail.com>	<CABFfbXtmFyoyaRSU6LyWrUzyXvvkSV14yD3vc0c2fzRcHbtNzQ@mail.gmail.com>
	<CAPPM_gR7pP7eq2Gg3nT4osSw1S69hvEW=yCx3-Q7T1V0fb19Vg@mail.gmail.com>
Message-ID: <532A20D7.2010905@fhcrc.org>



On 03/19/2014 02:59 PM, Joshua Ulrich wrote:
> On Wed, Mar 19, 2014 at 4:28 PM, Jeroen Ooms <jeroen.ooms at stat.ucla.edu> wrote:
>> On Wed, Mar 19, 2014 at 11:50 AM, Joshua Ulrich <josh.m.ulrich at gmail.com>
>> wrote:
>>>
>>> The suggested solution is not described in the referenced article.  It
>>> was not suggested that it be the operating system's responsibility to
>>> distribute snapshots, nor was it suggested to create binary
>>> repositories for specific operating systems, nor was it suggested to
>>> freeze only a subset of CRAN packages.
>>
>>
>> IMO this is an implementation detail. If we could all agree on a particular
>> set of cran packages to be used with a certain release of R, then it doesn't
>> matter how the 'snapshotting' gets implemented. It could be a separate
>> repository, or a directory on cran with symbolic links, or a page somewhere
>> with hyperlinks to the respective source packages. Or you can put all
>> packages in a big zip file, or include it in your OS distribution. You can
>> even distribute your entire repo on cdroms (debian style!) or do all of the
>> above.
>>
>> The hard problem is not implementation. The hard part is that for
>> reproducibility to work, we need community wide conventions on which
>> versions of cran packages are used by a particular release of R. Local
>> downstream solutions are impractical, because this results in
>> scripts/packages that only work within your niche using this particular
>> snapshot. I expect that requiring every script be executed in the context of
>> dependencies from some particular third party repository will make
>> reproducibility even less common. Therefore I am trying to make a case for a
>> solution that would naturally improve reliability/reproducibility of R code
>> without any effort by the end-user.
>>
> So implementation isn't a problem.  The problem is that you need a way
> to force people not to be able to use different package versions than
> what existed at the time of each R release.  I said this in my
> previous email, but you removed and did not address it: "However, you
> would need to find a way to actively _prevent_ people from installing
> newer versions of packages with the stable R releases."  Frankly, I
> would stop using CRAN if this policy were adopted.
>
> I suggest you go build this yourself.  You have all the code available
> on CRAN, and the dates at which each package was published.  If others
> who care about reproducible research find what you've built useful,
> you will create the very community you want.  And you won't have to
> force one single person to change their workflow.

Yeah we've already heard this "do it yourself" kind of answer. Not a
very productive one honestly.

Well actually that's what we've done for the Bioconductor repositories:
we freeze the BioC packages for each version of Bioconductor. But since
this freezing doesn't happen at the CRAN level, and many BioC packages
depend on CRAN packages, the freezing is only at the surface. Would be
much better if the freezing was all the way down to the bottom of the
sea. (Note that it is already if you install binary packages only.)

Yes it's technically possible to work around this by also hosting
frozen versions of CRAN, one per version of Bioconductor, and have
biocLite() (the tool BioC users use for installing packages) point to
these frozen versions of CRAN in order to get the correct dependencies
for any given version of BioC. However we don't do that because that
would mean extra costs for us in terms of storage space and bandwidth.
And also because we believe that it would be more effective and would
ultimately benefit the entire R community (and not just the BioC
community) if this problem was addressed upstream.

H.

>
> Best,
> --
> Joshua Ulrich  |  about.me/joshuaulrich
> FOSS Trading  |  www.fosstrading.com
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fhcrc.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From romain at r-enthusiasts.com  Thu Mar 20 01:13:03 2014
From: romain at r-enthusiasts.com (Romain Francois)
Date: Thu, 20 Mar 2014 01:13:03 +0100
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <532A20D7.2010905@fhcrc.org>
References: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>	<53299314.6080608@gmail.com>	<CABFfbXuTvefJ0Nmc25JJEwhmUQge+K1Fi=oaTwoO_jfK75mUZg@mail.gmail.com>	<CAPPM_gRP4xKEE1bFiR8Aaeo1XOX0kpu4p=EyOm2z+6fkVkJaag@mail.gmail.com>	<CABFfbXtmFyoyaRSU6LyWrUzyXvvkSV14yD3vc0c2fzRcHbtNzQ@mail.gmail.com>
	<CAPPM_gR7pP7eq2Gg3nT4osSw1S69hvEW=yCx3-Q7T1V0fb19Vg@mail.gmail.com>
	<532A20D7.2010905@fhcrc.org>
Message-ID: <ED1036A5-FCB2-4F8A-B635-72957B6E7FAA@r-enthusiasts.com>

Weighting in. FWIW, I find the proposal conceptually quite interesting. 

For package developers, it does not have to be a frustration to have to wait a new version of R to release their code. Anticipated frustration was my initial reaction. Thinking about this more, I think this could be changed into opportunity. 

Since the pattern here is to use Rcpp as an example of something causing compatibility headaches, and I have some responsibility there, maybe I can comment on this. I would find it extremely valuable if there was only one unique version of Rcpp for a given released version of R. 

Users would have to wait longer to have the new stuff, but one can argue that at least they get something that is more tested. 

Would it be helpful for authors of package that have lots of dependency to start having stricter depends declarations in their DESCRIPTION files, e.g. : 

Depends: R (== 3.1.0)

?

Romain


For example, personally I?m waiting for 3.1.0 for releasing Rcpp11 because I want to leverage some C++11 support that has been included in R. It has been frustrating to have to wait, but it does change the way I make changes to the codebase. Perhaps it is a good habit to take. And it does not need ? more work ? for others, just more discipline and self control from people implementing this pattern. 

also, declaring a strict dependency requirement against a released version of R perhaps could resume the drama of ? you were asked to test this against a very recent version of R-devel, and guess what a few hours ago I?ve just added a new test that makes your package non R CMD check worthy ?. So less work for CRAN maintainers then. 

Le 19 mars 2014 ? 23:57, Herv? Pag?s <hpages at fhcrc.org> a ?crit :

> 
> 
> On 03/19/2014 02:59 PM, Joshua Ulrich wrote:
>> On Wed, Mar 19, 2014 at 4:28 PM, Jeroen Ooms <jeroen.ooms at stat.ucla.edu> wrote:
>>> On Wed, Mar 19, 2014 at 11:50 AM, Joshua Ulrich <josh.m.ulrich at gmail.com>
>>> wrote:
>>>> 
>>>> The suggested solution is not described in the referenced article.  It
>>>> was not suggested that it be the operating system's responsibility to
>>>> distribute snapshots, nor was it suggested to create binary
>>>> repositories for specific operating systems, nor was it suggested to
>>>> freeze only a subset of CRAN packages.
>>> 
>>> 
>>> IMO this is an implementation detail. If we could all agree on a particular
>>> set of cran packages to be used with a certain release of R, then it doesn't
>>> matter how the 'snapshotting' gets implemented. It could be a separate
>>> repository, or a directory on cran with symbolic links, or a page somewhere
>>> with hyperlinks to the respective source packages. Or you can put all
>>> packages in a big zip file, or include it in your OS distribution. You can
>>> even distribute your entire repo on cdroms (debian style!) or do all of the
>>> above.
>>> 
>>> The hard problem is not implementation. The hard part is that for
>>> reproducibility to work, we need community wide conventions on which
>>> versions of cran packages are used by a particular release of R. Local
>>> downstream solutions are impractical, because this results in
>>> scripts/packages that only work within your niche using this particular
>>> snapshot. I expect that requiring every script be executed in the context of
>>> dependencies from some particular third party repository will make
>>> reproducibility even less common. Therefore I am trying to make a case for a
>>> solution that would naturally improve reliability/reproducibility of R code
>>> without any effort by the end-user.
>>> 
>> So implementation isn't a problem.  The problem is that you need a way
>> to force people not to be able to use different package versions than
>> what existed at the time of each R release.  I said this in my
>> previous email, but you removed and did not address it: "However, you
>> would need to find a way to actively _prevent_ people from installing
>> newer versions of packages with the stable R releases."  Frankly, I
>> would stop using CRAN if this policy were adopted.
>> 
>> I suggest you go build this yourself.  You have all the code available
>> on CRAN, and the dates at which each package was published.  If others
>> who care about reproducible research find what you've built useful,
>> you will create the very community you want.  And you won't have to
>> force one single person to change their workflow.
> 
> Yeah we've already heard this "do it yourself" kind of answer. Not a
> very productive one honestly.
> 
> Well actually that's what we've done for the Bioconductor repositories:
> we freeze the BioC packages for each version of Bioconductor. But since
> this freezing doesn't happen at the CRAN level, and many BioC packages
> depend on CRAN packages, the freezing is only at the surface. Would be
> much better if the freezing was all the way down to the bottom of the
> sea. (Note that it is already if you install binary packages only.)
> 
> Yes it's technically possible to work around this by also hosting
> frozen versions of CRAN, one per version of Bioconductor, and have
> biocLite() (the tool BioC users use for installing packages) point to
> these frozen versions of CRAN in order to get the correct dependencies
> for any given version of BioC. However we don't do that because that
> would mean extra costs for us in terms of storage space and bandwidth.
> And also because we believe that it would be more effective and would
> ultimately benefit the entire R community (and not just the BioC
> community) if this problem was addressed upstream.
> 
> H.
> 
>> 
>> Best,
>> --
>> Joshua Ulrich  |  about.me/joshuaulrich
>> FOSS Trading  |  www.fosstrading.com
>> 
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>> 
> 
> -- 
> Herv? Pag?s
> 
> Program in Computational Biology
> Division of Public Health Sciences
> Fred Hutchinson Cancer Research Center
> 1100 Fairview Ave. N, M1-B514
> P.O. Box 19024
> Seattle, WA 98109-1024
> 
> E-mail: hpages at fhcrc.org
> Phone:  (206) 667-5791
> Fax:    (206) 667-1319
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From jefferis at mrc-lmb.cam.ac.uk  Thu Mar 20 01:22:19 2014
From: jefferis at mrc-lmb.cam.ac.uk (Dr Gregory Jefferis)
Date: Thu, 20 Mar 2014 00:22:19 +0000
Subject: [Rd] possible bug: graphics::image seems to ignore
 getOption("preferRaster")
Message-ID: <37F174C5-0EEA-4584-B241-D336AEDD24E7@mrc-lmb.cam.ac.uk>

the details section of ?image says:

> If useRaster is not specified, raster images are used when the 
> getOption("preferRaster") is true, the grid is regular and either 
> dev.capabilities("raster") is "yes" or it is "non-missing" and there 
> are no missing values.

but in my experience this is never the case and 
getOption("preferRaster") is ignored. As far as I can see, the logic for 
checking this is in image is broken here:

   ras <- dev.capabilities("raster")
   if (identical(ras, "yes"))
     useRaster <- TRUE

because dev.capabilities("raster") returns a list like this (on my 
machine, R.version in footer)
> $rasterImage
> [1] "yes"

You can test this by doing:

   ras=structure(list(rasterImage = "yes"), .Names = "rasterImage")
   identical(ras,'yes') # returns FALSE

so the test would need to be something like:

   ras <- dev.capabilities("raster")[[1]]
   if (identical(ras, "yes"))
     useRaster <- TRUE

I can't find any relevant changes in R news
	
	http://stat.ethz.ch/R-manual/R-devel/doc/html/NEWS.html

This discussion

	https://www.mail-archive.com/r-devel at r-project.org/msg22811.html

suggests that Simon Urbanek may have added the useRaster option and 
looking at git blame on this mirror repo:

	https://github.com/wch/r-source/blame/c3ba5b0be36d3a1290e18fe189142c88f1e43236/src/library/graphics/R/image.R#L111-L120

suggests that Brian Ripley's svn commit 56949 was the last to touch 
these lines:

	https://github.com/wch/r-source/commit/b9012424f895bf681daf1b85255942547d495bcd

Thanks for any pointers if I am missing something!

Best wishes,

Greg Jefferis.


> R.version
                _
platform       x86_64-apple-darwin10.8.0
arch           x86_64
os             darwin10.8.0
system         x86_64, darwin10.8.0
status
major          3
minor          0.3
year           2014
month          03
day            06
svn rev        65126
language       R
version.string R version 3.0.3 (2014-03-06)
nickname       Warm Puppy

--
Gregory Jefferis, PhD
Division of Neurobiology
MRC Laboratory of Molecular Biology
Francis Crick Avenue
Cambridge Biomedical Campus
Cambridge, CB2 OQH, UK

http://www2.mrc-lmb.cam.ac.uk/group-leaders/h-to-m/g-jefferis
http://jefferislab.org
http://flybrain.stanford.edu


From ucfagls at gmail.com  Thu Mar 20 02:41:48 2014
From: ucfagls at gmail.com (Gavin Simpson)
Date: Wed, 19 Mar 2014 19:41:48 -0600
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <CAN_1p9w2Yyw3rV20EAeVTgGJtsDpXiQ1Jo1OF+k_p42tR8q2NA@mail.gmail.com>
References: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>
	<53299314.6080608@gmail.com>
	<CABFfbXuTvefJ0Nmc25JJEwhmUQge+K1Fi=oaTwoO_jfK75mUZg@mail.gmail.com>
	<5329E39A.7060805@structuremonitoring.com>
	<CAN_1p9w2Yyw3rV20EAeVTgGJtsDpXiQ1Jo1OF+k_p42tR8q2NA@mail.gmail.com>
Message-ID: <CAAHES9yOS6i4wTQp54UZ_-3wO7d_nCtmDjodH_zAbU_rHsz+9w@mail.gmail.com>

"What am I overlooking?"

That this is already available and possible in R today, but perhaps
not widely used. Developers do tend to only include a lower bound if
they include any bounds at all on package dependencies.

As I mentioned elsewhere, R packages often aren't "built" against
other R packages and often developers may have a range of versions
being tested against, some of which may not be on CRAN yet.

Technically, all packages on CRAN would need to have a dependency cap
on R-devel, but as that is a moving target until it is released I
don't see in practice how enforcing an upper limit on the R dependency
would  work. The way CRAN works, you can't just set a dependency on R
== 3.0.x say. (As far as I understand CRAN's policies.)

For packages it is quite trivial for the developers to manually add
the required info for the upperbound, less so the lower bound, but you
could just pick a known working version. An upper range on the
dependencies could be stated as whatever version is current on CRAN.
But then what happens? Unbeknownst to you, a few days after you
release to CRAN your package foo with stated dependency on bar >= 1.2,
bar <= 1.8, the developer of bar releases bar v 2.0 and your package
no longer passes checks, CRAN gets in touch and you have to resubmit
another version. This could be desirable in terms of helping
contribute to reproducibility exercises, but incurs more effort on the
CRAN maintainers and package maintainers. Now, this might be an issue
because of the desire on CRAN's behalf to have some elements of human
intervention in the submission process, but you either work with CRAN
or do your own thing.

As Bioconductor have shown (for example) it is possible, if people
want to put in time and effort and have a community buy into an ethos,
to achieve staged releases etc.

G

On 19 March 2014 12:58, Carl Boettiger <cboettig at gmail.com> wrote:
> Dear list,
>
> I'm curious what people would think of a more modest proposal at this time:
>
> State the version of the dependencies used by the package authors when the
> package was built.
>
> Eventually CRAN could enforce such a statement be present in the
> description. We encourage users to declare the version of the packages they
> use in publications, so why not have the same expectation of developers?
>  This would help address the problem of archived packages that Jeroen
> raises, as it is currently it is impossible to reliably install archived
> packages because their dependencies have since been updated and are no
> longer compatible.  (Even if it passes checks and installs, we have no way
> of knowing if the upstream changes have introduced a bug).  This
> information would be relatively straight forward to capture, shouldn't
> change the way anyone currently uses CRAN, and should address a major pain
> point anyone trying to install archived versions from CRAN has probably
> encountered.  What am I overlooking?
>
> Carl
>
>
> On Wed, Mar 19, 2014 at 11:36 AM, Spencer Graves <
> spencer.graves at structuremonitoring.com> wrote:
>
>>       What about having this purpose met with something like an expansion
>> of R-Forge?  We could have packages submitted to R-Forge rather than CRAN,
>> and people who wanted the latest could get it from R-Forge.  If changes I
>> make on R-Forge break a reverse dependency, emails explaining the problem
>> are sent to both me and the maintainer for the package I broke.
>>
>>
>>       The budget for R-Forge would almost certainly need to be increased:
>>  They currently disable many of the tests they once ran.
>>
>>
>>       Regarding budget, the R Project would get more donations if they
>> asked for them and made it easier to contribute.  I've tried multiple times
>> without success to find a way to donate.  I didn't try hard, but it
>> shouldn't be hard ;-)  (And donations should be accepted in US dollars and
>> Euros -- and maybe other currencies.) There should be a procedure whereby
>> anyone could receive a pro forma invoice, which they can pay or ignore as
>> they choose.  I mention this, because many grants could cover a reasonable
>> fee provided they have an invoice.
>>
>>
>>       Spencer Graves
>>
>>
>> On 3/19/2014 10:59 AM, Jeroen Ooms wrote:
>>
>>> On Wed, Mar 19, 2014 at 5:52 AM, Duncan Murdoch <murdoch.duncan at gmail.com
>>> >wrote:
>>>
>>>  I don't see why CRAN needs to be involved in this effort at all.  A third
>>>> party could take snapshots of CRAN at R release dates, and make those
>>>> available to package users in a separate repository.  It is not hard to
>>>> set
>>>> a different repository than CRAN as the default location from which to
>>>> obtain packages.
>>>>
>>>>  I am happy to see many people giving this some thought and engage in the
>>> discussion.
>>>
>>> Several have suggested that staging & freezing can be simply done by a
>>> third party. This solution and its limitations is also described in the
>>> paper [1] in the section titled "R: downstream staging and repackaging".
>>>
>>> If this would solve the problem without affecting CRAN, we would have been
>>> done this obviously. In fact, as described in the paper and pointed out by
>>> some people, initiatives such as Debian or Revolution Enterprise already
>>> include a frozen library of R packages. Also companies like Google
>>> maintain
>>> their own internal repository with packages that are used throughout the
>>> company.
>>>
>>> The problem with this approach is that when you using some 3rd party
>>> package snapshot, your r/sweave scripts will still only be
>>> reliable/reproducible for other users of that specific snapshot. E.g. for
>>> the examples above, a script that is written in R 3.0 by a Debian user is
>>> not guaranteed to work on R 3.0 in Google, or R 3.0 on some other 3rd
>>> party
>>> cran snapshot. Hence this solution merely redefines the problem from "this
>>> script depends on pkgA 1.1 and pkgB 0.2.3" to "this script depends on
>>> repository foo 2.0". And given that most users would still be pulling
>>> packages straight from CRAN, it would still be terribly difficult to
>>> reproduce a 5 year old sweave script from e.g. JSS.
>>>
>>> For this reason I believe the only effective place to organize this
>>> staging
>>> is all the way upstream, on CRAN. Imagine a world where your r/sweave
>>> script would be reliable/reproducible, out of the box, on any system, any
>>> platform in any company using on R 3.0. No need to investigate which
>>> specific packages or cran snapshot the author was using at the time of
>>> writing the script, and trying to reconstruct such libraries for each
>>> script you want to reproduce. No ambiguity about which package versions
>>> are
>>> used by R 3.0. However for better or worse, I think this could only be
>>> accomplished with a cran release cycle (i.e. "universal snapshots")
>>> accompanying the already existing r releases.
>>>
>>>
>>>
>>>  The only objection I can see to this is that it requires extra work by
>>>> the
>>>> third party, rather than extra work by the CRAN team. I don't think the
>>>> total amount of work required is much different.  I'm very unsympathetic
>>>> to
>>>> proposals to dump work on others.
>>>>
>>>
>>> I am merely trying to discuss a technical issue in an attempt to improve
>>> reliability of our software and reproducibility of papers created with R.
>>>
>>>         [[alternative HTML version deleted]]
>>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>
>
>
> --
> Carl Boettiger
> UC Santa Cruz
> http://carlboettiger.info/
>
>         [[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel



-- 
Gavin Simpson, PhD


From ucfagls at gmail.com  Thu Mar 20 02:54:57 2014
From: ucfagls at gmail.com (Gavin Simpson)
Date: Wed, 19 Mar 2014 19:54:57 -0600
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <532A20D7.2010905@fhcrc.org>
References: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>
	<53299314.6080608@gmail.com>
	<CABFfbXuTvefJ0Nmc25JJEwhmUQge+K1Fi=oaTwoO_jfK75mUZg@mail.gmail.com>
	<CAPPM_gRP4xKEE1bFiR8Aaeo1XOX0kpu4p=EyOm2z+6fkVkJaag@mail.gmail.com>
	<CABFfbXtmFyoyaRSU6LyWrUzyXvvkSV14yD3vc0c2fzRcHbtNzQ@mail.gmail.com>
	<CAPPM_gR7pP7eq2Gg3nT4osSw1S69hvEW=yCx3-Q7T1V0fb19Vg@mail.gmail.com>
	<532A20D7.2010905@fhcrc.org>
Message-ID: <CAAHES9wRkuDUENMftHYOPh7v2ZfPDQURceEFitNH-JCFcf4ZSw@mail.gmail.com>

Given that R is (has) moved to a 12 month release cycle, I don't want
to either i) wait a year to get new packages (or allow users to use
new versions of my packages), or ii) have to run R-devel just to use
new packages. (or be on R-testing for that matter).

People then will start finding ways around these limitations and then
we're back to square one of having people use a set of R packages and
R versions that could potentially be all over the place.

As a package developer, it is pretty easy to say I've tested my
package works with these other packages and their versions, and set
DESCRIPTION to reflect only those versions as allowed (or a range as a
package matures and the maintainer has tested against more versions of
the dependencies). CRAN may well not like this if your package no
longer builds/checks on their system but then you have a choice to
make; stick to your reproducibility guns & forsake CRAN in favour of
something else (github, one's own repo), or relent and meet CRANs
requirements.

On 19 March 2014 16:57, Herv? Pag?s <hpages at fhcrc.org> wrote:
>
>
> On 03/19/2014 02:59 PM, Joshua Ulrich wrote:
>>
>> On Wed, Mar 19, 2014 at 4:28 PM, Jeroen Ooms <jeroen.ooms at stat.ucla.edu>
>> wrote:
>>>
>>> On Wed, Mar 19, 2014 at 11:50 AM, Joshua Ulrich <josh.m.ulrich at gmail.com>
>>> wrote:
>>>>
>>>>
>>>> The suggested solution is not described in the referenced article.  It
>>>> was not suggested that it be the operating system's responsibility to
>>>> distribute snapshots, nor was it suggested to create binary
>>>> repositories for specific operating systems, nor was it suggested to
>>>> freeze only a subset of CRAN packages.
>>>
>>>
>>>
>>> IMO this is an implementation detail. If we could all agree on a
>>> particular
>>> set of cran packages to be used with a certain release of R, then it
>>> doesn't
>>> matter how the 'snapshotting' gets implemented. It could be a separate
>>> repository, or a directory on cran with symbolic links, or a page
>>> somewhere
>>> with hyperlinks to the respective source packages. Or you can put all
>>> packages in a big zip file, or include it in your OS distribution. You
>>> can
>>> even distribute your entire repo on cdroms (debian style!) or do all of
>>> the
>>> above.
>>>
>>> The hard problem is not implementation. The hard part is that for
>>> reproducibility to work, we need community wide conventions on which
>>> versions of cran packages are used by a particular release of R. Local
>>> downstream solutions are impractical, because this results in
>>> scripts/packages that only work within your niche using this particular
>>> snapshot. I expect that requiring every script be executed in the context
>>> of
>>> dependencies from some particular third party repository will make
>>> reproducibility even less common. Therefore I am trying to make a case
>>> for a
>>> solution that would naturally improve reliability/reproducibility of R
>>> code
>>> without any effort by the end-user.
>>>
>> So implementation isn't a problem.  The problem is that you need a way
>> to force people not to be able to use different package versions than
>> what existed at the time of each R release.  I said this in my
>> previous email, but you removed and did not address it: "However, you
>> would need to find a way to actively _prevent_ people from installing
>> newer versions of packages with the stable R releases."  Frankly, I
>> would stop using CRAN if this policy were adopted.
>>
>> I suggest you go build this yourself.  You have all the code available
>> on CRAN, and the dates at which each package was published.  If others
>> who care about reproducible research find what you've built useful,
>> you will create the very community you want.  And you won't have to
>> force one single person to change their workflow.
>
>
> Yeah we've already heard this "do it yourself" kind of answer. Not a
> very productive one honestly.
>
> Well actually that's what we've done for the Bioconductor repositories:
> we freeze the BioC packages for each version of Bioconductor. But since
> this freezing doesn't happen at the CRAN level, and many BioC packages
> depend on CRAN packages, the freezing is only at the surface. Would be
> much better if the freezing was all the way down to the bottom of the
> sea. (Note that it is already if you install binary packages only.)
>
> Yes it's technically possible to work around this by also hosting
> frozen versions of CRAN, one per version of Bioconductor, and have
> biocLite() (the tool BioC users use for installing packages) point to
> these frozen versions of CRAN in order to get the correct dependencies
> for any given version of BioC. However we don't do that because that
> would mean extra costs for us in terms of storage space and bandwidth.
> And also because we believe that it would be more effective and would
> ultimately benefit the entire R community (and not just the BioC
> community) if this problem was addressed upstream.
>
>
> H.
>
>>
>> Best,
>> --
>> Joshua Ulrich  |  about.me/joshuaulrich
>> FOSS Trading  |  www.fosstrading.com
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>
> --
> Herv? Pag?s
>
> Program in Computational Biology
> Division of Public Health Sciences
> Fred Hutchinson Cancer Research Center
> 1100 Fairview Ave. N, M1-B514
> P.O. Box 19024
> Seattle, WA 98109-1024
>
> E-mail: hpages at fhcrc.org
> Phone:  (206) 667-5791
> Fax:    (206) 667-1319
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel



-- 
Gavin Simpson, PhD


From michael.weylandt at gmail.com  Thu Mar 20 02:55:18 2014
From: michael.weylandt at gmail.com (Michael Weylandt)
Date: Wed, 19 Mar 2014 21:55:18 -0400
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <CAPPM_gR4kvVxk+sWqYGRxLyaPzw5bMr=kM==-o7sTKGpJy1iug@mail.gmail.com>
References: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>
	<53299314.6080608@gmail.com>
	<CABFfbXuTvefJ0Nmc25JJEwhmUQge+K1Fi=oaTwoO_jfK75mUZg@mail.gmail.com>
	<CAPPM_gRP4xKEE1bFiR8Aaeo1XOX0kpu4p=EyOm2z+6fkVkJaag@mail.gmail.com>
	<CABFfbXtmFyoyaRSU6LyWrUzyXvvkSV14yD3vc0c2fzRcHbtNzQ@mail.gmail.com>
	<CAPPM_gR7pP7eq2Gg3nT4osSw1S69hvEW=yCx3-Q7T1V0fb19Vg@mail.gmail.com>
	<CABFfbXteYwBwE018xf9aa_z95h3k3OquT2_BQmrsL6n4W57tWA@mail.gmail.com>
	<CAPPM_gR4kvVxk+sWqYGRxLyaPzw5bMr=kM==-o7sTKGpJy1iug@mail.gmail.com>
Message-ID: <A5F1114B-D870-478F-B11F-2EB7419F6BCE@gmail.com>



On Mar 19, 2014, at 18:42, Joshua Ulrich <josh.m.ulrich at gmail.com> wrote:

> On Wed, Mar 19, 2014 at 5:16 PM, Jeroen Ooms <jeroen.ooms at stat.ucla.edu> wrote:
>> On Wed, Mar 19, 2014 at 2:59 PM, Joshua Ulrich <josh.m.ulrich at gmail.com> wrote:
>>> 
>>> So implementation isn't a problem.  The problem is that you need a way
>>> to force people not to be able to use different package versions than
>>> what existed at the time of each R release.  I said this in my
>>> previous email, but you removed and did not address it: "However, you
>>> would need to find a way to actively _prevent_ people from installing
>>> newer versions of packages with the stable R releases."  Frankly, I
>>> would stop using CRAN if this policy were adopted.
>> 
>> I am not proposing to "force" anything to anyone, those are your
>> words. Please read the proposal more carefully before derailing the
>> discussion. Below *verbatim* a section from the paper:
> <snip>
> 
> Yes "force" is too strong a word.  You want a barrier (however small)
> to prevent people from installing newer (or older) versions of
> packages than those that correspond to a given R release.


Jeroen,

Reading this thread again, is it a fair summary of your position to say "reproducibility by default is more important than giving users access to the newest bug fixes and features by default?" It's certainly arguable, but I'm not sure I'm convinced: I'd imagine that the ratio of new work being done vs reproductions is rather high and the current setup optimizes for that already. 

What I'm trying to figure out is why the standard "install the following list of package versions" isn't good enough in your eyes? Is it the lack of CRAN provided binaries or the fact that the user has to proactively set up their environment to replicate that of published results?

In your XML example, it seems the problem was that the reproducer didn't check that the same package versions as the reproducee and instead assumed that 'latest' would be the same. Annoying yes, but easy to solve. 

Michael


From ucfagls at gmail.com  Thu Mar 20 03:17:18 2014
From: ucfagls at gmail.com (Gavin Simpson)
Date: Wed, 19 Mar 2014 20:17:18 -0600
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <A5F1114B-D870-478F-B11F-2EB7419F6BCE@gmail.com>
References: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>
	<53299314.6080608@gmail.com>
	<CABFfbXuTvefJ0Nmc25JJEwhmUQge+K1Fi=oaTwoO_jfK75mUZg@mail.gmail.com>
	<CAPPM_gRP4xKEE1bFiR8Aaeo1XOX0kpu4p=EyOm2z+6fkVkJaag@mail.gmail.com>
	<CABFfbXtmFyoyaRSU6LyWrUzyXvvkSV14yD3vc0c2fzRcHbtNzQ@mail.gmail.com>
	<CAPPM_gR7pP7eq2Gg3nT4osSw1S69hvEW=yCx3-Q7T1V0fb19Vg@mail.gmail.com>
	<CABFfbXteYwBwE018xf9aa_z95h3k3OquT2_BQmrsL6n4W57tWA@mail.gmail.com>
	<CAPPM_gR4kvVxk+sWqYGRxLyaPzw5bMr=kM==-o7sTKGpJy1iug@mail.gmail.com>
	<A5F1114B-D870-478F-B11F-2EB7419F6BCE@gmail.com>
Message-ID: <CAAHES9ys5z4T4VcOdXjXX+9MczFVxK0XLwfT-eMPY4xs6aQZCA@mail.gmail.com>

Michael,

I think the issue is that Jeroen wants to take that responsibility out
of the hands of the person trying to reproduce a work. If it used R
3.0.x and packages A, B and C then it would be trivial to to install
that version of R and then pull down the stable versions of A B and C
for that version of R. At the moment, one might note the packages used
and even their versions, but what about the versions of the packages
that the used packages rely upon & so on? What if developers don't
state know working versions of dependencies?

The problem is how the heck do you know which versions of packages are
needed if developers don't record these dependencies in sufficient
detail? The suggested solution is to freeze CRAN at intervals
alongside R releases. Then you'd know what the stable versions were.

Or we could just get package developers to be more thorough in
documenting dependencies. Or R CMD check could refuse to pass if a
package is listed as a dependency but with no version qualifiers. Or
have R CMD build add an upper bound (from the current, at build-time
version of dependencies on CRAN) if the package developer didn't
include and upper bound. Or... The first is unliekly to happen
consistently, and no-one wants *more* checks and hoops to jump through
:-)

To my mind it is incumbent upon those wanting reproducibility to build
the tools to enable users to reproduce works. When you write a paper
or release a tool, you will have tested it with a specific set of
packages. It is relatively easy to work out what those versions are
(there are tools in R for this). What is required is an automated way
to record that info in an agreed upon way in an approved
file/location, and have a tool that facilitates setting up a package
library sufficient with which to reproduce a work. That approval
doesn't need to come from CRAN or R Core - we can store anything in
./inst.

Reproducibility is a very important part of doing "science", but not
everyone using CRAN is doing that. Why force everyone to march to the
reproducibility drum? I would place the onus elsewhere to make this
work.

Gavin
A scientist, very much interested in reproducibility of my work and others.

On 19 March 2014 19:55, Michael Weylandt <michael.weylandt at gmail.com> wrote:
>
>
> On Mar 19, 2014, at 18:42, Joshua Ulrich <josh.m.ulrich at gmail.com> wrote:
>
>> On Wed, Mar 19, 2014 at 5:16 PM, Jeroen Ooms <jeroen.ooms at stat.ucla.edu> wrote:
>>> On Wed, Mar 19, 2014 at 2:59 PM, Joshua Ulrich <josh.m.ulrich at gmail.com> wrote:
>>>>
>>>> So implementation isn't a problem.  The problem is that you need a way
>>>> to force people not to be able to use different package versions than
>>>> what existed at the time of each R release.  I said this in my
>>>> previous email, but you removed and did not address it: "However, you
>>>> would need to find a way to actively _prevent_ people from installing
>>>> newer versions of packages with the stable R releases."  Frankly, I
>>>> would stop using CRAN if this policy were adopted.
>>>
>>> I am not proposing to "force" anything to anyone, those are your
>>> words. Please read the proposal more carefully before derailing the
>>> discussion. Below *verbatim* a section from the paper:
>> <snip>
>>
>> Yes "force" is too strong a word.  You want a barrier (however small)
>> to prevent people from installing newer (or older) versions of
>> packages than those that correspond to a given R release.
>
>
> Jeroen,
>
> Reading this thread again, is it a fair summary of your position to say "reproducibility by default is more important than giving users access to the newest bug fixes and features by default?" It's certainly arguable, but I'm not sure I'm convinced: I'd imagine that the ratio of new work being done vs reproductions is rather high and the current setup optimizes for that already.
>
> What I'm trying to figure out is why the standard "install the following list of package versions" isn't good enough in your eyes? Is it the lack of CRAN provided binaries or the fact that the user has to proactively set up their environment to replicate that of published results?
>
> In your XML example, it seems the problem was that the reproducer didn't check that the same package versions as the reproducee and instead assumed that 'latest' would be the same. Annoying yes, but easy to solve.
>
> Michael
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel



-- 
Gavin Simpson, PhD


From ross at biostat.ucsf.edu  Thu Mar 20 03:22:23 2014
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Wed, 19 Mar 2014 19:22:23 -0700
Subject: [Rd] modifying data in a package
Message-ID: <1395282143.1752.111.camel@localhost>

I've tweaked Rmpi and want to have some variables that hold data in the
package.  One of the R files starts
mpi.isend.obj <- vector("list", 500) #mpi.request.maxsize())                                                                          
mpi.isend.inuse <- rep(FALSE, 500) #mpi.request.maxsize())    

and then functions update those variables with <<-.  When run:
  Error in mpi.isend.obj[[i]] <<- .force.type(x, type) :                                                                                
  cannot change value of locked binding for 'mpi.isend.obj'

I'm writing to ask the proper way to accomplish this objective (getting
a variable I can update in package namespace--or at least somewhere
useful and hidden from the outside).

I think the problem is that the package namespace is locked.  So how do
I achieve the same effect?
http://www.r-bloggers.com/package-wide-variablescache-in-r-packages/
recommends creating an environment and then updating it.  Is that the
preferred route?  (It seems odd that the list should be locked but the
environment would be manipulable.  I know environments are special.)

The comments indicate that 500 "should" be mpi.request.maxsize().  That
doesn't work because mpi.request.maxsize calls a C function, and there
is an error that the function isn't loaded.  I guess the R code is
evaluated before the C libraries are loaded. The packages zzz.R starts
.onLoad <- function (lib, pkg) {
    library.dynam("Rmpi", pkg, lib)

So would moving the code into .onLoad after that work?  In that case,
how do I get the environment into the  proper scope?  Would
 .onLoad <- function (lib, pkg) {
    library.dynam("Rmpi", pkg, lib)
    assign("mpi.globals", new.env(), environment(mpi.isend))
    assign("mpi.isend.obj", vector("list", mpi.request.maxsize(),
mpi.globals)
work?

mpi.isend is a function in Rmpi.  But I'd guess the first assign will
fail because the environment is locked.

Thanks.
Ross Boylan


From michael.weylandt at gmail.com  Thu Mar 20 03:44:14 2014
From: michael.weylandt at gmail.com (Michael Weylandt)
Date: Wed, 19 Mar 2014 22:44:14 -0400
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <CAAHES9ys5z4T4VcOdXjXX+9MczFVxK0XLwfT-eMPY4xs6aQZCA@mail.gmail.com>
References: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>
	<53299314.6080608@gmail.com>
	<CABFfbXuTvefJ0Nmc25JJEwhmUQge+K1Fi=oaTwoO_jfK75mUZg@mail.gmail.com>
	<CAPPM_gRP4xKEE1bFiR8Aaeo1XOX0kpu4p=EyOm2z+6fkVkJaag@mail.gmail.com>
	<CABFfbXtmFyoyaRSU6LyWrUzyXvvkSV14yD3vc0c2fzRcHbtNzQ@mail.gmail.com>
	<CAPPM_gR7pP7eq2Gg3nT4osSw1S69hvEW=yCx3-Q7T1V0fb19Vg@mail.gmail.com>
	<CABFfbXteYwBwE018xf9aa_z95h3k3OquT2_BQmrsL6n4W57tWA@mail.gmail.com>
	<CAPPM_gR4kvVxk+sWqYGRxLyaPzw5bMr=kM==-o7sTKGpJy1iug@mail.gmail.com>
	<A5F1114B-D870-478F-B11F-2EB7419F6BCE@gmail.com>
	<CAAHES9ys5z4T4VcOdXjXX+9MczFVxK0XLwfT-eMPY4xs6aQZCA@mail.gmail.com>
Message-ID: <29BEAB7B-B96F-4F93-AE56-B7541F0270D7@gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140319/3ea8a3fd/attachment.pl>

From jeroen.ooms at stat.ucla.edu  Thu Mar 20 03:45:02 2014
From: jeroen.ooms at stat.ucla.edu (Jeroen Ooms)
Date: Wed, 19 Mar 2014 19:45:02 -0700
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <A5F1114B-D870-478F-B11F-2EB7419F6BCE@gmail.com>
References: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>
	<53299314.6080608@gmail.com>
	<CABFfbXuTvefJ0Nmc25JJEwhmUQge+K1Fi=oaTwoO_jfK75mUZg@mail.gmail.com>
	<CAPPM_gRP4xKEE1bFiR8Aaeo1XOX0kpu4p=EyOm2z+6fkVkJaag@mail.gmail.com>
	<CABFfbXtmFyoyaRSU6LyWrUzyXvvkSV14yD3vc0c2fzRcHbtNzQ@mail.gmail.com>
	<CAPPM_gR7pP7eq2Gg3nT4osSw1S69hvEW=yCx3-Q7T1V0fb19Vg@mail.gmail.com>
	<CABFfbXteYwBwE018xf9aa_z95h3k3OquT2_BQmrsL6n4W57tWA@mail.gmail.com>
	<CAPPM_gR4kvVxk+sWqYGRxLyaPzw5bMr=kM==-o7sTKGpJy1iug@mail.gmail.com>
	<A5F1114B-D870-478F-B11F-2EB7419F6BCE@gmail.com>
Message-ID: <CABFfbXvDmbGihx1o2+dW1U3ska68RoGJr2PwQj++ySJJjrCkhA@mail.gmail.com>

On Wed, Mar 19, 2014 at 6:55 PM, Michael Weylandt
<michael.weylandt at gmail.com> wrote:
> Reading this thread again, is it a fair summary of your position to say "reproducibility by default is more important than giving users access to the newest bug fixes and features by default?" It's certainly arguable, but I'm not sure I'm convinced: I'd imagine that the ratio of new work being done vs reproductions is rather high and the current setup optimizes for that already.

I think that separating development from released branches can give us
both reliability/reproducibility (stable branch) as well as new
features (unstable branch). The user gets to pick (and you can pick
both!). The same is true for r-base: when using a 'released' version
you get 'stable' base packages that are up to 12 months old. If you
want to have the latest stuff you download a nightly build of r-devel.
For regular users and reproducible research it is recommended to use
the stable branch. However if you are a developer (e.g. package
author) you might want to develop/test/check your work with the latest
r-devel.

I think that extending the R release cycle to CRAN would result both
in more stable released versions of R, as well as more freedom for
package authors to implement rigorous change in the unstable branch.
When writing a script that is part of a production pipeline, or sweave
paper that should be reproducible 10 years from now, or a book on
using R, you use stable version of R, which is guaranteed to behave
the same over time. However when developing packages that should be
compatible with the upcoming release of R, you use r-devel which has
the latest versions of other CRAN and base packages.


> What I'm trying to figure out is why the standard "install the following list of package versions" isn't good enough in your eyes?

Almost nobody does this because it is cumbersome and impractical. We
can do so much better than this. Note that in order to install old
packages you also need to investigate which versions of dependencies
of those packages were used. On win/osx, users need to manually build
those packages which can be a pain. All in all it makes reproducible
research difficult and expensive and error prone. At the end of the
day most published results obtain with R just won't be reproducible.

Also I believe that keeping it simple is essential for solutions to be
practical. If every script has to be run inside an environment with
custom libraries, it takes away much of its power. Running a bash or
python script in Linux is so easy and reliable that entire
distributions are based on it. I don't understand why we make our
lives so difficult in R.

In my estimation, a system where stable versions of R pull packages
from a stable branch of CRAN will naturally resolve the majority of
the reproducibility and reliability problems with R. And in contrast
to what some people here are suggesting it does not introduce any
limitations. If you want to get the latest stuff, you either grab a
copy of r-devel, or just enable the testing branch and off you go.
Debian 'testing' works in a similar way, see
http://www.debian.org/devel/testing.


From michael.weylandt at gmail.com  Thu Mar 20 04:16:32 2014
From: michael.weylandt at gmail.com (Michael Weylandt)
Date: Wed, 19 Mar 2014 23:16:32 -0400
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <CABFfbXvDmbGihx1o2+dW1U3ska68RoGJr2PwQj++ySJJjrCkhA@mail.gmail.com>
References: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>
	<53299314.6080608@gmail.com>
	<CABFfbXuTvefJ0Nmc25JJEwhmUQge+K1Fi=oaTwoO_jfK75mUZg@mail.gmail.com>
	<CAPPM_gRP4xKEE1bFiR8Aaeo1XOX0kpu4p=EyOm2z+6fkVkJaag@mail.gmail.com>
	<CABFfbXtmFyoyaRSU6LyWrUzyXvvkSV14yD3vc0c2fzRcHbtNzQ@mail.gmail.com>
	<CAPPM_gR7pP7eq2Gg3nT4osSw1S69hvEW=yCx3-Q7T1V0fb19Vg@mail.gmail.com>
	<CABFfbXteYwBwE018xf9aa_z95h3k3OquT2_BQmrsL6n4W57tWA@mail.gmail.com>
	<CAPPM_gR4kvVxk+sWqYGRxLyaPzw5bMr=kM==-o7sTKGpJy1iug@mail.gmail.com>
	<A5F1114B-D870-478F-B11F-2EB7419F6BCE@gmail.com>
	<CABFfbXvDmbGihx1o2+dW1U3ska68RoGJr2PwQj++ySJJjrCkhA@mail.gmail.com>
Message-ID: <EA4F3F44-DFD4-4928-92EE-9BDD2ED61490@gmail.com>

On Mar 19, 2014, at 22:45, Jeroen Ooms <jeroen.ooms at stat.ucla.edu> wrote:

> On Wed, Mar 19, 2014 at 6:55 PM, Michael Weylandt
> <michael.weylandt at gmail.com> wrote:
>> Reading this thread again, is it a fair summary of your position to say "reproducibility by default is more important than giving users access to the newest bug fixes and features by default?" It's certainly arguable, but I'm not sure I'm convinced: I'd imagine that the ratio of new work being done vs reproductions is rather high and the current setup optimizes for that already.
> 
> I think that separating development from released branches can give us
> both reliability/reproducibility (stable branch) as well as new
> features (unstable branch). The user gets to pick (and you can pick
> both!). The same is true for r-base: when using a 'released' version
> you get 'stable' base packages that are up to 12 months old. If you
> want to have the latest stuff you download a nightly build of r-devel.
> For regular users and reproducible research it is recommended to use
> the stable branch. However if you are a developer (e.g. package
> author) you might want to develop/test/check your work with the latest
> r-devel.

I think where you are getting push back (e.g., Frank Harrell and Josh Ulrich) is from saying that 'stable' is the right branch for 'regular users.' And I tend to agree: I think most folks need features and bug fixes more than they need to reproduce a particular paper with no effort on their end. 

> 
> I think that extending the R release cycle to CRAN would result both
> in more stable released versions of R, as well as more freedom for
> package authors to implement rigorous change in the unstable branch.

Not sure what exactly you mean by this sentence. 

> When writing a script that is part of a production pipeline, or sweave
> paper that should be reproducible 10 years from now, or a book on
> using R, you use stable version of R, which is guaranteed to behave
> the same over time.

Only if you never upgrade anything... But that's the case already, isn't it?


> However when developing packages that should be
> compatible with the upcoming release of R, you use r-devel which has
> the latest versions of other CRAN and base packages.
> 
> 
>> What I'm trying to figure out is why the standard "install the following list of package versions" isn't good enough in your eyes?
> 
> Almost nobody does this because it is cumbersome and impractical. We
> can do so much better than this. Note that in order to install old
> packages you also need to investigate which versions of dependencies
> of those packages were used. On win/osx, users need to manually build
> those packages which can be a pain. All in all it makes reproducible
> research difficult and expensive and error prone. At the end of the
> day most published results obtain with R just won't be reproducible.

So you want CRAN to host old binaries ad infinitum? I think that's entirely reasonable/doable if (big if) storage and network are free. 

> 
> Also I believe that keeping it simple is essential for solutions to be
> practical. If every script has to be run inside an environment with
> custom libraries, it takes away much of its power. Running a bash or
> python script in Linux is so easy and reliable that entire
> distributions are based on it. I don't understand why we make our
> lives so difficult in R.

Because for Debian style (stop the world on release) distro, there are no upgrades within a release. And that's only halfway reasonable because of Debian's shockingly good QA. 

It's certainly not true for, e.g., Arch. 

I've been looking at python incompatibilities across different RHEL versions lately. There's simply no way to get around explicit version pinning (either by release number or date, but when you have many moving pieces, picking a set of release numbers is much easier than finding a single day when they all happened to work together) if it has to work exactly as it used to. 

> 
> In my estimation, a system where stable versions of R pull packages
> from a stable branch of CRAN will naturally resolve the majority of
> the reproducibility and reliability problems with R.

And what everyone else is saying is "if you want to reproduce results made with old software,  download and use the old software." Both can me made to work -- it's just a matter of pros and cons of different defaults. 


> And in contrast
> to what some people here are suggesting it does not introduce any
> limitations. If you want to get the latest stuff, you either grab a
> copy of r-devel, or just enable the testing branch and off you go.
> Debian 'testing' works in a similar way, see
> http://www.debian.org/devel/testing.


From kmillar at google.com  Thu Mar 20 05:30:32 2014
From: kmillar at google.com (Karl Millar)
Date: Wed, 19 Mar 2014 21:30:32 -0700
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <CABFfbXvDmbGihx1o2+dW1U3ska68RoGJr2PwQj++ySJJjrCkhA@mail.gmail.com>
References: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>
	<53299314.6080608@gmail.com>
	<CABFfbXuTvefJ0Nmc25JJEwhmUQge+K1Fi=oaTwoO_jfK75mUZg@mail.gmail.com>
	<CAPPM_gRP4xKEE1bFiR8Aaeo1XOX0kpu4p=EyOm2z+6fkVkJaag@mail.gmail.com>
	<CABFfbXtmFyoyaRSU6LyWrUzyXvvkSV14yD3vc0c2fzRcHbtNzQ@mail.gmail.com>
	<CAPPM_gR7pP7eq2Gg3nT4osSw1S69hvEW=yCx3-Q7T1V0fb19Vg@mail.gmail.com>
	<CABFfbXteYwBwE018xf9aa_z95h3k3OquT2_BQmrsL6n4W57tWA@mail.gmail.com>
	<CAPPM_gR4kvVxk+sWqYGRxLyaPzw5bMr=kM==-o7sTKGpJy1iug@mail.gmail.com>
	<A5F1114B-D870-478F-B11F-2EB7419F6BCE@gmail.com>
	<CABFfbXvDmbGihx1o2+dW1U3ska68RoGJr2PwQj++ySJJjrCkhA@mail.gmail.com>
Message-ID: <CABz6aZdpBC_6LX+bbbn+igrFVBrs0i_pD3NY2_YK89WVuWW6xg@mail.gmail.com>

I think what you really want here is the ability to easily identify
and sync to CRAN snapshots.

The easy way to do this is setup a CRAN mirror, but back it up with
version control, so that it's easy to reproduce the exact state of
CRAN at any given point in time.  CRAN's not particularly large and
doesn't churn a whole lot, so most version control systems should be
able to handle that without difficulty.

Using svn, mod_dav_svn and (maybe) mod_rewrite, you could setup the
server so that e.g.:
   http://my.cran.mirror/repos/2013-01-01/
is a mirror of how CRAN looked at midnight 2013-01-01.

Users can then set their repository to that URL, and will have a
stable snapshot to work with, and can have all their packages built
with that snapshot if they like.  For reproducibility purposes, all
users need to do is to agree on the same date to use.  For publication
purposes, the date of the snapshot should be sufficient.

We'd need a version of update.packages() that force-syncs all the
packages to the version in the repository, even if they're downgrades,
but otherwise it ought to be fairly straight-forward.

FWIW, we do something similar internally at Google.  All the packages
that a user has installed come from the same source control revision,
where we know that all the package versions are mutually compatible.
It saves a lot of headaches, and users can rollback to any previous
point in time easily if they run into problems.


On Wed, Mar 19, 2014 at 7:45 PM, Jeroen Ooms <jeroen.ooms at stat.ucla.edu> wrote:
> On Wed, Mar 19, 2014 at 6:55 PM, Michael Weylandt
> <michael.weylandt at gmail.com> wrote:
>> Reading this thread again, is it a fair summary of your position to say "reproducibility by default is more important than giving users access to the newest bug fixes and features by default?" It's certainly arguable, but I'm not sure I'm convinced: I'd imagine that the ratio of new work being done vs reproductions is rather high and the current setup optimizes for that already.
>
> I think that separating development from released branches can give us
> both reliability/reproducibility (stable branch) as well as new
> features (unstable branch). The user gets to pick (and you can pick
> both!). The same is true for r-base: when using a 'released' version
> you get 'stable' base packages that are up to 12 months old. If you
> want to have the latest stuff you download a nightly build of r-devel.
> For regular users and reproducible research it is recommended to use
> the stable branch. However if you are a developer (e.g. package
> author) you might want to develop/test/check your work with the latest
> r-devel.
>
> I think that extending the R release cycle to CRAN would result both
> in more stable released versions of R, as well as more freedom for
> package authors to implement rigorous change in the unstable branch.
> When writing a script that is part of a production pipeline, or sweave
> paper that should be reproducible 10 years from now, or a book on
> using R, you use stable version of R, which is guaranteed to behave
> the same over time. However when developing packages that should be
> compatible with the upcoming release of R, you use r-devel which has
> the latest versions of other CRAN and base packages.
>
>
>> What I'm trying to figure out is why the standard "install the following list of package versions" isn't good enough in your eyes?
>
> Almost nobody does this because it is cumbersome and impractical. We
> can do so much better than this. Note that in order to install old
> packages you also need to investigate which versions of dependencies
> of those packages were used. On win/osx, users need to manually build
> those packages which can be a pain. All in all it makes reproducible
> research difficult and expensive and error prone. At the end of the
> day most published results obtain with R just won't be reproducible.
>
> Also I believe that keeping it simple is essential for solutions to be
> practical. If every script has to be run inside an environment with
> custom libraries, it takes away much of its power. Running a bash or
> python script in Linux is so easy and reliable that entire
> distributions are based on it. I don't understand why we make our
> lives so difficult in R.
>
> In my estimation, a system where stable versions of R pull packages
> from a stable branch of CRAN will naturally resolve the majority of
> the reproducibility and reliability problems with R. And in contrast
> to what some people here are suggesting it does not introduce any
> limitations. If you want to get the latest stuff, you either grab a
> copy of r-devel, or just enable the testing branch and off you go.
> Debian 'testing' works in a similar way, see
> http://www.debian.org/devel/testing.
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From dwinsemius at comcast.net  Thu Mar 20 07:03:32 2014
From: dwinsemius at comcast.net (David Winsemius)
Date: Wed, 19 Mar 2014 23:03:32 -0700
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <CABFfbXvDmbGihx1o2+dW1U3ska68RoGJr2PwQj++ySJJjrCkhA@mail.gmail.com>
References: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>
	<53299314.6080608@gmail.com>
	<CABFfbXuTvefJ0Nmc25JJEwhmUQge+K1Fi=oaTwoO_jfK75mUZg@mail.gmail.com>
	<CAPPM_gRP4xKEE1bFiR8Aaeo1XOX0kpu4p=EyOm2z+6fkVkJaag@mail.gmail.com>
	<CABFfbXtmFyoyaRSU6LyWrUzyXvvkSV14yD3vc0c2fzRcHbtNzQ@mail.gmail.com>
	<CAPPM_gR7pP7eq2Gg3nT4osSw1S69hvEW=yCx3-Q7T1V0fb19Vg@mail.gmail.com>
	<CABFfbXteYwBwE018xf9aa_z95h3k3OquT2_BQmrsL6n4W57tWA@mail.gmail.com>
	<CAPPM_gR4kvVxk+sWqYGRxLyaPzw5bMr=kM==-o7sTKGpJy1iug@mail.gmail.com>
	<A5F1114B-D870-478F-B11F-2EB7419F6BCE@gmail.com>
	<CABFfbXvDmbGihx1o2+dW1U3ska68RoGJr2PwQj++ySJJjrCkhA@mail.gmail.com>
Message-ID: <AD67664B-CDAD-4051-9B64-9350C2C38F64@comcast.net>


On Mar 19, 2014, at 7:45 PM, Jeroen Ooms wrote:

> On Wed, Mar 19, 2014 at 6:55 PM, Michael Weylandt
> <michael.weylandt at gmail.com> wrote:
>> Reading this thread again, is it a fair summary of your position to say "reproducibility by default is more important than giving users access to the newest bug fixes and features by default?" It's certainly arguable, but I'm not sure I'm convinced: I'd imagine that the ratio of new work being done vs reproductions is rather high and the current setup optimizes for that already.
> 
> I think that separating development from released branches can give us
> both reliability/reproducibility (stable branch) as well as new
> features (unstable branch). The user gets to pick (and you can pick
> both!). The same is true for r-base: when using a 'released' version
> you get 'stable' base packages that are up to 12 months old. If you
> want to have the latest stuff you download a nightly build of r-devel.
> For regular users and reproducible research it is recommended to use
> the stable branch. However if you are a developer (e.g. package
> author) you might want to develop/test/check your work with the latest
> r-devel.
> 
> I think that extending the R release cycle to CRAN would result both
> in more stable released versions of R, as well as more freedom for
> package authors to implement rigorous change in the unstable branch.
> When writing a script that is part of a production pipeline, or sweave
> paper that should be reproducible 10 years from now, or a book on
> using R, you use stable version of R, which is guaranteed to behave
> the same over time. However when developing packages that should be
> compatible with the upcoming release of R, you use r-devel which has
> the latest versions of other CRAN and base packages.


As I remember ... The example demonstrating the need for this was an XML package that cause an extract from a website where the headers were misinterpreted as data in one version of pkg:XML and not in another. That seems fairly unconvincing. Data cleaning and validation is a basic task of data analysis. It also seems excessive to assert that it is the responsibility of CRAN to maintain a synced binary archive that will be available in ten years. Bug fixes would be inhibited for years.... not unlike SAS and Excel. What next? Perhaps al bugs should be labeled as features?  Surely this CRAN-of-the-future would be offering something that no other statistical package currently offers, nicht wahr?

Why not leave it to the authors to specify the packages which version numbers were used in their publications. The authors of the packages would get recognition and the dependencies would be recorded.

-- 
David.
> 
> 
>> What I'm trying to figure out is why the standard "install the following list of package versions" isn't good enough in your eyes?
> 
> Almost nobody does this because it is cumbersome and impractical. We
> can do so much better than this. Note that in order to install old
> packages you also need to investigate which versions of dependencies
> of those packages were used. On win/osx, users need to manually build
> those packages which can be a pain. All in all it makes reproducible
> research difficult and expensive and error prone. At the end of the
> day most published results obtain with R just won't be reproducible.
> 
> Also I believe that keeping it simple is essential for solutions to be
> practical. If every script has to be run inside an environment with
> custom libraries, it takes away much of its power. Running a bash or
> python script in Linux is so easy and reliable that entire
> distributions are based on it. I don't understand why we make our
> lives so difficult in R.
> 
> In my estimation, a system where stable versions of R pull packages
> from a stable branch of CRAN will naturally resolve the majority of
> the reproducibility and reliability problems with R. And in contrast
> to what some people here are suggesting it does not introduce any
> limitations. If you want to get the latest stuff, you either grab a
> copy of r-devel, or just enable the testing branch and off you go.
> Debian 'testing' works in a similar way, see
> http://www.debian.org/devel/testing.
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

David Winsemius
Alameda, CA, USA


From dtenenba at fhcrc.org  Thu Mar 20 07:15:56 2014
From: dtenenba at fhcrc.org (Dan Tenenbaum)
Date: Wed, 19 Mar 2014 23:15:56 -0700 (PDT)
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <AD67664B-CDAD-4051-9B64-9350C2C38F64@comcast.net>
Message-ID: <129902082.964468.1395296156508.JavaMail.root@fhcrc.org>



----- Original Message -----
> From: "David Winsemius" <dwinsemius at comcast.net>
> To: "Jeroen Ooms" <jeroen.ooms at stat.ucla.edu>
> Cc: "r-devel" <r-devel at r-project.org>
> Sent: Wednesday, March 19, 2014 11:03:32 PM
> Subject: Re: [Rd] [RFC] A case for freezing CRAN
> 
> 
> On Mar 19, 2014, at 7:45 PM, Jeroen Ooms wrote:
> 
> > On Wed, Mar 19, 2014 at 6:55 PM, Michael Weylandt
> > <michael.weylandt at gmail.com> wrote:
> >> Reading this thread again, is it a fair summary of your position
> >> to say "reproducibility by default is more important than giving
> >> users access to the newest bug fixes and features by default?"
> >> It's certainly arguable, but I'm not sure I'm convinced: I'd
> >> imagine that the ratio of new work being done vs reproductions is
> >> rather high and the current setup optimizes for that already.
> > 
> > I think that separating development from released branches can give
> > us
> > both reliability/reproducibility (stable branch) as well as new
> > features (unstable branch). The user gets to pick (and you can pick
> > both!). The same is true for r-base: when using a 'released'
> > version
> > you get 'stable' base packages that are up to 12 months old. If you
> > want to have the latest stuff you download a nightly build of
> > r-devel.
> > For regular users and reproducible research it is recommended to
> > use
> > the stable branch. However if you are a developer (e.g. package
> > author) you might want to develop/test/check your work with the
> > latest
> > r-devel.
> > 
> > I think that extending the R release cycle to CRAN would result
> > both
> > in more stable released versions of R, as well as more freedom for
> > package authors to implement rigorous change in the unstable
> > branch.
> > When writing a script that is part of a production pipeline, or
> > sweave
> > paper that should be reproducible 10 years from now, or a book on
> > using R, you use stable version of R, which is guaranteed to behave
> > the same over time. However when developing packages that should be
> > compatible with the upcoming release of R, you use r-devel which
> > has
> > the latest versions of other CRAN and base packages.
> 
> 
> As I remember ... The example demonstrating the need for this was an
> XML package that cause an extract from a website where the headers
> were misinterpreted as data in one version of pkg:XML and not in
> another. That seems fairly unconvincing. Data cleaning and
> validation is a basic task of data analysis. It also seems excessive
> to assert that it is the responsibility of CRAN to maintain a synced
> binary archive that will be available in ten years. 


CRAN already does this, the bin/windows/contrib directory has subdirectories going back to 1.7, with packages dated October 2004. I don't see why it is burdensome to continue to archive these. It would be nice if source versions had a similar archive.

Dan




> Bug fixes would
> be inhibited for years.... not unlike SAS and Excel. What next?
> Perhaps al bugs should be labeled as features?  Surely this
> CRAN-of-the-future would be offering something that no other
> statistical package currently offers, nicht wahr?
> 
> Why not leave it to the authors to specify the packages which version
> numbers were used in their publications. The authors of the packages
> would get recognition and the dependencies would be recorded.
> 
> --
> David.
> > 
> > 
> >> What I'm trying to figure out is why the standard "install the
> >> following list of package versions" isn't good enough in your
> >> eyes?
> > 
> > Almost nobody does this because it is cumbersome and impractical.
> > We
> > can do so much better than this. Note that in order to install old
> > packages you also need to investigate which versions of
> > dependencies
> > of those packages were used. On win/osx, users need to manually
> > build
> > those packages which can be a pain. All in all it makes
> > reproducible
> > research difficult and expensive and error prone. At the end of the
> > day most published results obtain with R just won't be
> > reproducible.
> > 
> > Also I believe that keeping it simple is essential for solutions to
> > be
> > practical. If every script has to be run inside an environment with
> > custom libraries, it takes away much of its power. Running a bash
> > or
> > python script in Linux is so easy and reliable that entire
> > distributions are based on it. I don't understand why we make our
> > lives so difficult in R.
> > 
> > In my estimation, a system where stable versions of R pull packages
> > from a stable branch of CRAN will naturally resolve the majority of
> > the reproducibility and reliability problems with R. And in
> > contrast
> > to what some people here are suggesting it does not introduce any
> > limitations. If you want to get the latest stuff, you either grab a
> > copy of r-devel, or just enable the testing branch and off you go.
> > Debian 'testing' works in a similar way, see
> > http://www.debian.org/devel/testing.
> > 
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
> 
> David Winsemius
> Alameda, CA, USA
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From ross at biostat.ucsf.edu  Thu Mar 20 07:45:10 2014
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Wed, 19 Mar 2014 23:45:10 -0700
Subject: [Rd] modifying data in a package [a solution]
In-Reply-To: <1395282143.1752.111.camel@localhost>
References: <1395282143.1752.111.camel@localhost>
Message-ID: <1395297910.1752.120.camel@localhost>

On Wed, 2014-03-19 at 19:22 -0700, Ross Boylan wrote:
> I've tweaked Rmpi and want to have some variables that hold data in the
> package.  One of the R files starts
> mpi.isend.obj <- vector("list", 500) #mpi.request.maxsize())                                                                          
> mpi.isend.inuse <- rep(FALSE, 500) #mpi.request.maxsize())    
> 
> and then functions update those variables with <<-.  When run:
>   Error in mpi.isend.obj[[i]] <<- .force.type(x, type) :                                                                                
>   cannot change value of locked binding for 'mpi.isend.obj'
> 
> I'm writing to ask the proper way to accomplish this objective (getting
> a variable I can update in package namespace--or at least somewhere
> useful and hidden from the outside).
> 
I've discovered one way to do it:
In one of the regular R files
mpi.global <- new.env()

Then at the end of .onLoad in zzz.R:
assign("mpi.isend.obj", vector("list", mpi.request.maxsize()),
mpi.global)
and similary for the logical vector mpi.isend.inuse

Access with functions like this:
## Next 2 functions have 3 modes                                                                                                      
##  foo()  returns foo from mpi.global                                                                                                
##  foo(request) returns foo[request] from mpi.global                                                                                 
##  foo(request, value) set foo[request] to value                                                                                     
mpi.isend.inuse <- function(request, value) {
    if (missing(request))
        return(get("mpi.isend.inuse", mpi.global))
    i <- request+1L
    parent.env(mpi.global) <- environment()
    if (missing(value))
        return(evalq(mpi.isend.inuse[i], mpi.global))
    return(evalq(mpi.isend.inuse[i] <- value, mpi.global))
}

# request, if present, must be a single value                                                                                         
mpi.isend.obj <- function(request, value){
    if (missing(request))
        return(get("mpi.isend.obj", mpi.global))
    i <- request+1L
    parent.env(mpi.global) <- environment()
    if (missing(value))
        return(evalq(mpi.isend.obj[[i]], mpi.global))
    return(evalq(mpi.isend.inuse[[i]] <- value, mpi.global))
}

This is pretty awkward; I'd love to know a better way.  Some of the
names probably should change too: mpi.isend.obj() sounds too much as if
it actually sends something, like mpi.isend.Robj().

Ross


From Rainer at krugs.de  Thu Mar 20 09:49:12 2014
From: Rainer at krugs.de (Rainer M Krug)
Date: Thu, 20 Mar 2014 09:49:12 +0100
Subject: [Rd] [RFC] A case for freezing CRAN
References: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>
	<53299314.6080608@gmail.com>
	<CABFfbXuTvefJ0Nmc25JJEwhmUQge+K1Fi=oaTwoO_jfK75mUZg@mail.gmail.com>
	<CAPPM_gRP4xKEE1bFiR8Aaeo1XOX0kpu4p=EyOm2z+6fkVkJaag@mail.gmail.com>
	<CABFfbXtmFyoyaRSU6LyWrUzyXvvkSV14yD3vc0c2fzRcHbtNzQ@mail.gmail.com>
	<CAPPM_gR7pP7eq2Gg3nT4osSw1S69hvEW=yCx3-Q7T1V0fb19Vg@mail.gmail.com>
	<CABFfbXteYwBwE018xf9aa_z95h3k3OquT2_BQmrsL6n4W57tWA@mail.gmail.com>
	<CAPPM_gR4kvVxk+sWqYGRxLyaPzw5bMr=kM==-o7sTKGpJy1iug@mail.gmail.com>
	<A5F1114B-D870-478F-B11F-2EB7419F6BCE@gmail.com>
	<CAAHES9ys5z4T4VcOdXjXX+9MczFVxK0XLwfT-eMPY4xs6aQZCA@mail.gmail.com>
	<29BEAB7B-B96F-4F93-AE56-B7541F0270D7@gmail.com>
Message-ID: <m2r45x9rw7.fsf@krugs.de>

Michael Weylandt <michael.weylandt at gmail.com> writes:

> On Mar 19, 2014, at 22:17, Gavin Simpson <ucfagls at gmail.com> wrote:
>
>> Michael,
>> 
>> I think the issue is that Jeroen wants to take that responsibility out
>> of the hands of the person trying to reproduce a work. If it used R
>> 3.0.x and packages A, B and C then it would be trivial to to install
>> that version of R and then pull down the stable versions of A B and C
>> for that version of R. At the moment, one might note the packages used
>> and even their versions, but what about the versions of the packages
>> that the used packages rely upon & so on? What if developers don't
>> state know working versions of dependencies?
>
> Doesn't sessionInfo() give all of this?
>
> If you want to be very worried about every last bit, I suppose it
> should also include options(), compiler flags, compiler version, BLAS
> details, etc.  (Good talk on the dregs of a floating point number and
> how hard it is to reproduce them across processors
> http://www.youtube.com/watch?v=GIlp4rubv8U)

In principle yes - but this calls specifically for a package which is
extracting the info and stores it into a human readable format, which
can then be used to re-install (automatically) all the versions for
(hopefully) reproducibility - because if there are external libraries
included, you HAVE problems.

>
>> 
>> The problem is how the heck do you know which versions of packages are
>> needed if developers don't record these dependencies in sufficient
>> detail? The suggested solution is to freeze CRAN at intervals
>> alongside R releases. Then you'd know what the stable versions were.
>
> Only if you knew which R release was used. 

Well - that would be easier to specify in a paper then the version infos
of all packages needed - and which ones of the installed ones are
actually needed? OK - the ones specified in library() calls. But wait -
there are dependencies, imports, ... That is a lot of digging - I wpul;d
not know how to do this out of my head, except by digging through the
DESCRIPTION files of the packages...

>
>> 
>> Or we could just get package developers to be more thorough in
>> documenting dependencies. Or R CMD check could refuse to pass if a
>> package is listed as a dependency but with no version qualifiers. Or
>> have R CMD build add an upper bound (from the current, at build-time
>> version of dependencies on CRAN) if the package developer didn't
>> include and upper bound. Or... The first is unliekly to happen
>> consistently, and no-one wants *more* checks and hoops to jump through
>> :-)
>> 
>> To my mind it is incumbent upon those wanting reproducibility to build
>> the tools to enable users to reproduce works.
>
> But the tools already allow it with minimal effort. If the author
> can't even include session info, how can we be sure the version of R
> is known. If we can't know which version of R, can we ever change R at
> all? Etc to absurdity.
>
> My (serious) point is that the tools are in place, but ramming them
> down folks' throats by intentionally keeping them on older versions by
> default is too much.
>
>> When you write a paper
>> or release a tool, you will have tested it with a specific set of
>> packages. It is relatively easy to work out what those versions are
>> (there are tools in R for this). What is required is an automated way
>> to record that info in an agreed upon way in an approved
>> file/location, and have a tool that facilitates setting up a package
>> library sufficient with which to reproduce a work. That approval
>> doesn't need to come from CRAN or R Core - we can store anything in
>> ./inst.
>
> I think the package version and published paper cases are different. 
>
> For the latter, the recipe is simple: if you want the same results,
> use the same software (as noted by sessionInfoPlus() or equiv)

Dependencies, imports, package versions, ... not that straight forward I
would say.

>
> For the former, I think you start straying into this NP complete problem: http://people.debian.org/~dburrows/model.pdf 
>
> Yes, a good config can (and should be recorded) but isn't that exactly what sessionInfo() gives?
>
>> 
>> Reproducibility is a very important part of doing "science", but not
>> everyone using CRAN is doing that. Why force everyone to march to the
>> reproducibility drum? I would place the onus elsewhere to make this
>> work.
>> 
>
> Agreed: reproducibility is the onus of the author, not the reader

Exactly - but also the authors of the software which is aimed at being
used in the context of reproducibility - the tools should be there to
make it easy!

My points are:

1) I think the snapshot idea of CRAN is a good idea which should be
followed
2) The snapshots should be incorporated at CRAN as I assume that CRAN
will be there longer then any third party repository.
3) the default for the user should *not* change, i.e. normal users will
always get the newest packages as it is now
4) If this can / will not be done because of workload, storage space,
... commands should be incorporated in a package (preferably which
becomes part of the core packages) to store snapshots of installed
package and R version information as a human readable text file, but
which can be parsed by a second command to re-create this setup.

Cheers, and thanks for this important discussion (could have been a GSoC
project?),

Rainer


>
>
>> Gavin
>> A scientist, very much interested in reproducibility of my work and others.
>
> Michael
> In finance, where we call it "Auditability" and care very much as well :-)
>
>
> 	[[alternative HTML version deleted]]
>

-- 
Rainer M. Krug
email: Rainer<at>krugs<dot>de
PGP: 0x0F52F982
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 494 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140320/0cb9bc6f/attachment.bin>

From Rainer at krugs.de  Thu Mar 20 10:04:25 2014
From: Rainer at krugs.de (Rainer M Krug)
Date: Thu, 20 Mar 2014 10:04:25 +0100
Subject: [Rd] [RFC] A case for freezing CRAN
References: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>
	<CAPPM_gS1Ey2UyyBSCqecT=NzWXKtREBoXmqNBussR8Hv1Zok3g@mail.gmail.com>
	<CAC2h7utmzLPR-cT1pDpnH7gvE0Fzjy5JO363sqBj-f621t22MQ@mail.gmail.com>
	<CABdHhvHyKqu4mSn6Ziw=GgOR8pQOXnGRXW4fZr8H0z41Gkpo4g@mail.gmail.com>
Message-ID: <m2mwgl9r6u.fsf@krugs.de>

Hadley Wickham <h.wickham at gmail.com> writes:

>> What would be more useful in terms of reproducibility is the capability of
>> installing a specific version of a package from a repository using
>> install.packages(), which would require archiving older versions in a
>> coordinated fashion. I know CRAN archives old versions, but I am not aware
>> if we can programmatically query the repository about this.
>
> See devtools::install_version().
>
> The main caveat is that you also need to be able to build the package,
> and ensure you have dependencies that work with that version.

The compiling will always be the problem when using older source
packages, whatever is done.

But for the dependencies: an automatic parsing of the dependencies
(DEPENDS, IMPORTS, ...) would help a lot. 

Together with a command which scans the installed package in the session
and stores them in a parsable human readable format so that all packages
(with the specified version) required can be installed with one command,
and I think the problem would be much closer to be solved.

Rainer

>
> Hadley

-- 
Rainer M. Krug
email: Rainer<at>krugs<dot>de
PGP: 0x0F52F982
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 494 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140320/f8b1e895/attachment.bin>

From leegi001 at umn.edu  Thu Mar 20 04:58:32 2014
From: leegi001 at umn.edu (Cathy Lee Gierke)
Date: Wed, 19 Mar 2014 22:58:32 -0500
Subject: [Rd] Time format in parameters
Message-ID: <CAOeg=_8Zp=69=zX09Yt0if4Rt=XeKBaBP_7r6=UVE7T9EfgZHQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140319/aa2d9cd3/attachment.pl>

From murdoch.duncan at gmail.com  Thu Mar 20 11:52:53 2014
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Thu, 20 Mar 2014 06:52:53 -0400
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <129902082.964468.1395296156508.JavaMail.root@fhcrc.org>
References: <129902082.964468.1395296156508.JavaMail.root@fhcrc.org>
Message-ID: <532AC885.1090305@gmail.com>

On 14-03-20 2:15 AM, Dan Tenenbaum wrote:
>
>
> ----- Original Message -----
>> From: "David Winsemius" <dwinsemius at comcast.net>
>> To: "Jeroen Ooms" <jeroen.ooms at stat.ucla.edu>
>> Cc: "r-devel" <r-devel at r-project.org>
>> Sent: Wednesday, March 19, 2014 11:03:32 PM
>> Subject: Re: [Rd] [RFC] A case for freezing CRAN
>>
>>
>> On Mar 19, 2014, at 7:45 PM, Jeroen Ooms wrote:
>>
>>> On Wed, Mar 19, 2014 at 6:55 PM, Michael Weylandt
>>> <michael.weylandt at gmail.com> wrote:
>>>> Reading this thread again, is it a fair summary of your position
>>>> to say "reproducibility by default is more important than giving
>>>> users access to the newest bug fixes and features by default?"
>>>> It's certainly arguable, but I'm not sure I'm convinced: I'd
>>>> imagine that the ratio of new work being done vs reproductions is
>>>> rather high and the current setup optimizes for that already.
>>>
>>> I think that separating development from released branches can give
>>> us
>>> both reliability/reproducibility (stable branch) as well as new
>>> features (unstable branch). The user gets to pick (and you can pick
>>> both!). The same is true for r-base: when using a 'released'
>>> version
>>> you get 'stable' base packages that are up to 12 months old. If you
>>> want to have the latest stuff you download a nightly build of
>>> r-devel.
>>> For regular users and reproducible research it is recommended to
>>> use
>>> the stable branch. However if you are a developer (e.g. package
>>> author) you might want to develop/test/check your work with the
>>> latest
>>> r-devel.
>>>
>>> I think that extending the R release cycle to CRAN would result
>>> both
>>> in more stable released versions of R, as well as more freedom for
>>> package authors to implement rigorous change in the unstable
>>> branch.
>>> When writing a script that is part of a production pipeline, or
>>> sweave
>>> paper that should be reproducible 10 years from now, or a book on
>>> using R, you use stable version of R, which is guaranteed to behave
>>> the same over time. However when developing packages that should be
>>> compatible with the upcoming release of R, you use r-devel which
>>> has
>>> the latest versions of other CRAN and base packages.
>>
>>
>> As I remember ... The example demonstrating the need for this was an
>> XML package that cause an extract from a website where the headers
>> were misinterpreted as data in one version of pkg:XML and not in
>> another. That seems fairly unconvincing. Data cleaning and
>> validation is a basic task of data analysis. It also seems excessive
>> to assert that it is the responsibility of CRAN to maintain a synced
>> binary archive that will be available in ten years.
>
>
> CRAN already does this, the bin/windows/contrib directory has subdirectories going back to 1.7, with packages dated October 2004. I don't see why it is burdensome to continue to archive these. It would be nice if source versions had a similar archive.

The bin/windows/contrib directories are updated every day for active R 
versions.  It's only when Uwe decides that a version is no longer worth 
active support that he stops doing updates, and it "freezes".  A 
consequence of this is that the snapshots preserved in those older 
directories are unlikely to match what someone who keeps up to date with 
R releases is using.  Their purpose is to make sure that those older 
versions aren't completely useless, but they aren't what Jeroen was 
asking for.

Karl Millar's suggestion seems like an ideal solution to this problem. 
Any CRAN mirror could implement it.  If someone sets this up and commits 
to maintaining it, I'd be happy to work on the necessary changes to the 
install.packages/update.packages code to allow people to use it from 
within R.

Duncan Murdoch


From roger.bivand at nhh.no  Thu Mar 20 12:37:51 2014
From: roger.bivand at nhh.no (Roger Bivand)
Date: Thu, 20 Mar 2014 11:37:51 +0000
Subject: [Rd] [RFC] A case for freezing CRAN
References: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>
	<53299314.6080608@gmail.com>
	<CABFfbXuTvefJ0Nmc25JJEwhmUQge+K1Fi=oaTwoO_jfK75mUZg@mail.gmail.com>
	<CAPPM_gRP4xKEE1bFiR8Aaeo1XOX0kpu4p=EyOm2z+6fkVkJaag@mail.gmail.com>
	<CABFfbXtmFyoyaRSU6LyWrUzyXvvkSV14yD3vc0c2fzRcHbtNzQ@mail.gmail.com>
	<CAPPM_gR7pP7eq2Gg3nT4osSw1S69hvEW=yCx3-Q7T1V0fb19Vg@mail.gmail.com>
	<CABFfbXteYwBwE018xf9aa_z95h3k3OquT2_BQmrsL6n4W57tWA@mail.gmail.com>
	<CAPPM_gR4kvVxk+sWqYGRxLyaPzw5bMr=kM==-o7sTKGpJy1iug@mail.gmail.com>
	<A5F1114B-D870-478F-B11F-2EB7419F6BCE@gmail.com>
	<CAAHES9ys5z4T4VcOdXjXX+9MczFVxK0XLwfT-eMPY4xs6aQZCA@mail.gmail.com>
Message-ID: <loom.20140320T122358-863@post.gmane.org>

Gavin Simpson <ucfagls <at> gmail.com> writes:

> 
...
> 
> 
> To my mind it is incumbent upon those wanting reproducibility to build
> the tools to enable users to reproduce works. When you write a paper
> or release a tool, you will have tested it with a specific set of
> packages. It is relatively easy to work out what those versions are
> (there are tools in R for this). What is required is an automated way
> to record that info in an agreed upon way in an approved
> file/location, and have a tool that facilitates setting up a package
> library sufficient with which to reproduce a work. That approval
> doesn't need to come from CRAN or R Core - we can store anything in
> ./inst.

Gavin,

Thanks for contributing useful insights. With reference to Jeroen's proposal
and the discussion so far, I can see where the problem lies, but the
proposed solutions are very invasive. What might offer a less invasive
resolution is through a robust and predictable schema for sessionInfo()
content, permitting ready parsing, so that (using Hadley's interjection) the
reproducer could reconstruct the original execution environment at least as
far as R and package versions are concerned.

In fact, I'd argue that the responsibility for securing reproducibility lies
with the originating author or organisation, so that work where
reproducibility is desired should include such a standardised record. 

There is an additional problem not addressed directly in this thread but
mentioned in some contributions, upstream of R. The further problem upstream
is actually in the external dependencies and compilers, beyond that in
hardware. So raising consciousness about the importance of being able to
query version information to enable reproducibility is important.

Next, encapsulating the information permitting its parsing would perhaps
enable the original execution environment to be reconstructed locally by
installing external dependencies, then R, then packages from source, using
the same versions of build train components if possible (and noting
mismatches if not). Maybe ressurect StatDataML in addition to RData
serialization of the version dependencies? Of course, current R and package
versions may provide reproducibility, but if they don't, one would use the
parseable record of the original development environment 

> 
> Reproducibility is a very important part of doing "science", but not
> everyone using CRAN is doing that. Why force everyone to march to the
> reproducibility drum? I would place the onus elsewhere to make this
> work.

Exactly.

Roger

> 
> Gavin
> A scientist, very much interested in reproducibility of my work and others.
> 
...
> >
> > ______________________________________________
> > R-devel <at> r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
>


From S.Ellison at lgcgroup.com  Thu Mar 20 13:14:14 2014
From: S.Ellison at lgcgroup.com (S Ellison)
Date: Thu, 20 Mar 2014 12:14:14 +0000
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <CABFfbXtmFyoyaRSU6LyWrUzyXvvkSV14yD3vc0c2fzRcHbtNzQ@mail.gmail.com>
References: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>
	<53299314.6080608@gmail.com>
	<CABFfbXuTvefJ0Nmc25JJEwhmUQge+K1Fi=oaTwoO_jfK75mUZg@mail.gmail.com>
	<CAPPM_gRP4xKEE1bFiR8Aaeo1XOX0kpu4p=EyOm2z+6fkVkJaag@mail.gmail.com>
	<CABFfbXtmFyoyaRSU6LyWrUzyXvvkSV14yD3vc0c2fzRcHbtNzQ@mail.gmail.com>
Message-ID: <A4E5A0B016B8CB41A485FC629B633CED5E8CE5B5D9@GOLD.corp.lgc-group.com>

>  If we could all agree on a particular set
> of cran packages to be used with a certain release of R, then it doesn't matter
> how the 'snapshotting' gets implemented.

This is pretty much the sticking point, though. I see no practical way of reaching that agreement without the kind of decision authority (and effort) that Linux distro maintainers put in to the internal consistency of each distribution.

CRAN doesn't try to do that; it's just a place to access packages offered by maintainers. 

As a package maintainer, I think support for critical version dependencies in the imports or dependency lists is a good idea that individual package maintainers could relatively easily manage, but I think freezing CRAN as a whole or adopting single release cycles for CRAN would be thoroughly impractical.

S Ellison





*******************************************************************
This email and any attachments are confidential. Any use...{{dropped:8}}


From therneau at mayo.edu  Thu Mar 20 13:19:41 2014
From: therneau at mayo.edu (Therneau, Terry M., Ph.D.)
Date: Thu, 20 Mar 2014 07:19:41 -0500
Subject: [Rd] The case for freezing CRAN
Message-ID: <3dfcdc$fuu13r@ironport9.mayo.edu>

There is a central assertion to this argument that I don't follow:

> At the end of the day most published results obtained with R just won't be reproducible.

This is a very strong assertion. What is the evidence for it?

  I write a lot of Sweave/knitr in house as a way of documenting complex analyses, and a 
glm() based logistic regression looks the same yesterday as it will tomorrow.

Terry Therneau


From michael.weylandt at gmail.com  Thu Mar 20 13:48:28 2014
From: michael.weylandt at gmail.com (Michael Weylandt)
Date: Thu, 20 Mar 2014 08:48:28 -0400
Subject: [Rd] The case for freezing CRAN
In-Reply-To: <3dfcdc$fuu13r@ironport9.mayo.edu>
References: <3dfcdc$fuu13r@ironport9.mayo.edu>
Message-ID: <F6B1E03C-BB62-4B7D-A56C-C94DC934D5FE@gmail.com>

On Mar 20, 2014, at 8:19, "Therneau, Terry M., Ph.D." <therneau at mayo.edu> wrote:

> There is a central assertion to this argument that I don't follow:
> 
>> At the end of the day most published results obtained with R just won't be reproducible.
> 
> This is a very strong assertion. What is the evidence for it?

If I've understood Jeroen correctly, his point might be alternatively phrased as "won't be reproducED" (i.e., end user difficulties, not software availability).

Michael


From therneau at mayo.edu  Thu Mar 20 14:00:26 2014
From: therneau at mayo.edu (Therneau, Terry M., Ph.D.)
Date: Thu, 20 Mar 2014 08:00:26 -0500
Subject: [Rd] The case for freezing CRAN
In-Reply-To: <F6B1E03C-BB62-4B7D-A56C-C94DC934D5FE@gmail.com>
References: <3dfcdc$fuu13r@ironport9.mayo.edu>
	<F6B1E03C-BB62-4B7D-A56C-C94DC934D5FE@gmail.com>
Message-ID: <6e55ab$8iijgo@ironport10.mayo.edu>



On 03/20/2014 07:48 AM, Michael Weylandt wrote:
> On Mar 20, 2014, at 8:19, "Therneau, Terry M., Ph.D." <therneau at mayo.edu> wrote:
>
>> There is a central assertion to this argument that I don't follow:
>>
>>> At the end of the day most published results obtained with R just won't be reproducible.
>>
>> This is a very strong assertion. What is the evidence for it?
>
> If I've understood Jeroen correctly, his point might be alternatively phrased as "won't be reproducED" (i.e., end user difficulties, not software availability).
>
> Michael
>

That was my point as well.  Of the 30+ Sweave documents that I've produced I can't think 
of one that will change its output with a new version of R.  My 0/30 estimate is at odds 
with the "nearly all" assertion.  Perhaps I only do dull things?

Terry T.


From kevin.r.coombes at gmail.com  Thu Mar 20 14:23:39 2014
From: kevin.r.coombes at gmail.com (Kevin Coombes)
Date: Thu, 20 Mar 2014 09:23:39 -0400
Subject: [Rd] The case for freezing CRAN
In-Reply-To: <6e55ab$8iijgo@ironport10.mayo.edu>
References: <3dfcdc$fuu13r@ironport9.mayo.edu>	<F6B1E03C-BB62-4B7D-A56C-C94DC934D5FE@gmail.com>
	<6e55ab$8iijgo@ironport10.mayo.edu>
Message-ID: <532AEBDB.3000101@gmail.com>


On 3/20/2014 9:00 AM, Therneau, Terry M., Ph.D. wrote:
>
>
> On 03/20/2014 07:48 AM, Michael Weylandt wrote:
>> On Mar 20, 2014, at 8:19, "Therneau, Terry M., Ph.D." 
>> <therneau at mayo.edu> wrote:
>>
>>> There is a central assertion to this argument that I don't follow:
>>>
>>>> At the end of the day most published results obtained with R just 
>>>> won't be reproducible.
>>>
>>> This is a very strong assertion. What is the evidence for it?
>>
>> If I've understood Jeroen correctly, his point might be alternatively 
>> phrased as "won't be reproducED" (i.e., end user difficulties, not 
>> software availability).
>>
>> Michael
>>
>
> That was my point as well.  Of the 30+ Sweave documents that I've 
> produced I can't think of one that will change its output with a new 
> version of R.  My 0/30 estimate is at odds with the "nearly all" 
> assertion.  Perhaps I only do dull things?
>
> Terry T.
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

The only concrete example that comes to mind from my own Sweave reports 
was actually caused by BioConductor and not CRAN. I had a set of 
analyses that used DNAcopy, and the results changed substantially with a 
new release of the package in which they changed the default values to 
the main function call.   As a result, I've taken to writing out more of 
the defaults that I previously just accepted.  There have been a few 
minor issues similar to this one (with changes to parts of the Mclust 
package ??). So my estimates are somewhat higher than 0/30 but are still 
a long way from "almost all".

Kevin


From edd at debian.org  Thu Mar 20 14:32:02 2014
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 20 Mar 2014 08:32:02 -0500
Subject: [Rd] The case for freezing CRAN
In-Reply-To: <3dfcdc$fuu13r@ironport9.mayo.edu>
References: <3dfcdc$fuu13r@ironport9.mayo.edu>
Message-ID: <21290.60882.865159.407363@max.nulle.part>


No attempt to summarize the thread, but a few highlighted points:

 o Karl's suggestion of versioned / dated access to the repo by adding a
   layer to webaccess is (as usual) nice.  It works on the 'supply' side. But
   Jeroen's problem is on the demand side.  Even when we know that an
   analysis was done on 20xx-yy-zz, and we reconstruct CRAN that day, it only
   gives us a 'ceiling' estimate of what was on the machine.  In production
   or lab environments, installations get stale.  Maybe packages were already
   a year old?  To me, this is an issue that needs to be addressed on the
   'demand' side of the user. But just writing out version numbers is not
   good enough.

 o Roger correctly notes that R scripts and packages are just one issue.
   Compilers, libraries and the OS matter.  To me, the natural approach these
   days would be to think of something based on Docker or Vagrant or (if you
   must, VirtualBox).  The newer alternatives make snapshotting very cheap
   (eg by using Linux LXC).  That approach reproduces a full environemnt as
   best as we can while still ignoring the hardware layer (and some readers
   may recall the infamous Pentium bug of two decades ago).

 o Reproduciblity will probably remain the responsibility of study
   authors. If an investigator on a mega-grant wants to (or needs to) freeze,
   they do have the tools now.  Requiring the need of a few to push work on
   those already overloaded (ie CRAN) and changing the workflow of everybody
   is a non-starter.

 o As Terry noted, Jeroen made some strong claims about exactly how flawed
   the existing system is and keeps coming back to the example of 'a JSS
   paper that cannot be re-run'.  I would really like to see empirics on
   this.  Studies of reproducibility appear to be publishable these days, so
   maybe some enterprising grad student wants to run with the idea of
   actually _testing_ this.  We maybe be above Terry's 0/30 and nearer to
   Kevin's 'low'/30.  But let's bring some data to the debate.

 o Overall, I would tend to think that our CRAN standards of releasing with
   tests, examples, and checks on every build and release already do a much
   better job of keeping things tidy and workable than in most if not all
   other related / similar open source projects. I would of course welcome
   contradictory examples.

Dirk
 
-- 
Dirk Eddelbuettel | edd at debian.org | http://dirk.eddelbuettel.com


From jari.oksanen at oulu.fi  Thu Mar 20 14:43:57 2014
From: jari.oksanen at oulu.fi (Jari Oksanen)
Date: Thu, 20 Mar 2014 13:43:57 +0000
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <A4E5A0B016B8CB41A485FC629B633CED5E8CE5B5D9@GOLD.corp.lgc-group.com>
References: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>
	<53299314.6080608@gmail.com>
	<CABFfbXuTvefJ0Nmc25JJEwhmUQge+K1Fi=oaTwoO_jfK75mUZg@mail.gmail.com>
	<CAPPM_gRP4xKEE1bFiR8Aaeo1XOX0kpu4p=EyOm2z+6fkVkJaag@mail.gmail.com>
	<CABFfbXtmFyoyaRSU6LyWrUzyXvvkSV14yD3vc0c2fzRcHbtNzQ@mail.gmail.com>
	<A4E5A0B016B8CB41A485FC629B633CED5E8CE5B5D9@GOLD.corp.lgc-group.com>
Message-ID: <231A3DD4-082D-454E-857E-4BF4005AD7A7@oulu.fi>


On 20/03/2014, at 14:14 PM, S Ellison wrote:

>> If we could all agree on a particular set
>> of cran packages to be used with a certain release of R, then it doesn't matter
>> how the 'snapshotting' gets implemented.
> 
> This is pretty much the sticking point, though. I see no practical way of reaching that agreement without the kind of decision authority (and effort) that Linux distro maintainers put in to the internal consistency of each distribution.
> 
> CRAN doesn't try to do that; it's just a place to access packages offered by maintainers. 
> 
> As a package maintainer, I think support for critical version dependencies in the imports or dependency lists is a good idea that individual package maintainers could relatively easily manage, but I think freezing CRAN as a whole or adopting single release cycles for CRAN would be thoroughly impractical.
> 

I have a feeling that this discussion has floated between two different arguments in favour of freezing: discontent with package authors who break their packages within R release cycle, and ability to reproduce old results. In the beginning the first argument was more prominent, but now the discussion has drifted to reproducing old results. 

I cannot see how freezing CRAN would help with package authors who do not separate development and CRAN release branches but introduce broken code, or code that breaks other packages. Freezing a broken snapshot would only mean that the situation cannot be cured before next R release, and then new breakage could be introduced. Result would be dysfunctional CRAN. I think that quite a few of the package updates are bug fixes and minor enhancements. Further, I do think that these should be "backported" to previous versions of R: users of previous version of R should also benefit from bug fixes. This also is the current CRAN policy and I think this is a good policy. Personally, I try to keep my packages in such a condition that they will also work in previous versions of R so that people do not need to upgrade R to have bug fixes in packages. 

The policy is the same with Linux maintainers: they do not just build a consistent release, but maintain the release by providing bug fixes. In Linux distributions, end of life equals freezing, or not providing new versions of software.

Another issue is reproducing old analyses. This is a valuable thing, and sessionInfo and ability to get certain versions of package certainly are steps forward. It looks that guaranteed reproduction is a hard task, though. For instance, R 2.14.2 is the oldest version of R that I can build out of the box in my Linux desktop. I have earlier built older, even much older, R versions, but something has happened in my OS that crashes the build process. To reproduce an old analysis, I also should install an older version of my OS,  then build old R and then get the old versions of packages. It is nice if the last step is made easier.

Cheers, Jari Oksanen


From cgenolin at u-paris10.fr  Thu Mar 20 16:56:34 2014
From: cgenolin at u-paris10.fr (Christophe Genolini)
Date: Thu, 20 Mar 2014 16:56:34 +0100
Subject: [Rd] Memcheck: Invalid  read of size 4
In-Reply-To: <373A4631-6C57-493C-A347-0BAB256C507A@gmail.com>
References: <532A1314.8040206@u-paris10.fr>
	<373A4631-6C57-493C-A347-0BAB256C507A@gmail.com>
Message-ID: <532B0FB2.30208@u-paris10.fr>

Thanks a lot. Your correction works just fine.

Any idea of what goes wrong for the line 151, which is

    int *clusterAffectation2=malloc(*nbInd * sizeof(int));                      // lines 151





> On 19 Mar 2014, at 22:58 , Christophe Genolini <cgenolin at u-paris10.fr> wrote:
>
>> Hi the list,
>>
>> One of my package has a memory issue that I do not manage to understand. The Memtest notes is here:
>> <http://www.stats.ox.ac.uk/pub/bdr/memtests/valgrind/kml-Ex.Rout>
>>
>> Here is the message that I get from Memtest
>>
>> --- 8< ----------------
>> ~ Fast KmL ~
>> ==27283== Invalid read of size 4
>> ==27283==    at 0x10C5DF28: kml1 (kml.c:183)
>> ...
>> ==27283==    by 0x10C5DE4F: kml1 (kml.c:151)
>> ...
>> ==27283==    at 0x10C5DF90: kml1 (kml.c:198)
>> --- 8< ----------------
>>
>>
>> Here is the function kml1 from the file kml.c (I add some comments to tag the lines 151, 183 and 198)
>>
>> --- 8< ----------------
>> void kml1(double *traj, int *nbInd, int *nbTime, int *nbClusters, int *maxIt, int *clusterAffectation1, int *convergenceTime){
>>
>>     int i=0,iter=0;
>>     int *clusterAffectation2=malloc(*nbInd * sizeof(int));                      // lines 151
>>     double *trajMean=malloc(*nbClusters * *nbTime * sizeof(double));
>>
>>     for(i = 0; i < *nbClusters * *nbTime; i++){trajMean[i] = 0.0;};
>>     for(i = 0; i < *nbInd; i++){clusterAffectation2[i] = 0;};
>>
>>     for(iter = 0; iter < *maxIt; iter+=2){
>> 	calculMean(traj,nbInd,nbTime,clusterAffectation1,nbClusters,trajMean);
>>        	affecteIndiv(traj,nbInd,nbTime,trajMean,nbClusters,clusterAffectation2);
>>
>> 	i = 0;
>> 	while(clusterAffectation1[i]==clusterAffectation2[i] && i <*nbInd){i++;}; // lines 183
>> 	if(i == *nbInd){
>> 	    *convergenceTime = iter + 1;
>> 	    break;
>> 	}else{};
>>
>> 	calculMean(traj,nbInd,nbTime,clusterAffectation2,nbClusters,trajMean);
>> 	affecteIndiv(traj,nbInd,nbTime,trajMean,nbClusters,clusterAffectation1);
>>
>> 	i = 0;
>> 	while(clusterAffectation1[i]==clusterAffectation2[i] && i<*nbInd){i++;}; // lines 198
>> 	if(i == *nbInd){
>> 	    *convergenceTime = iter + 2;
>>       	    break;
>> 	}else{};
>>     }
>> }
>> --- 8< ----------------
>>
>> Do you know what is wrong in my C code?
> Yes. You need to reverse operands of &&. Otherwise you'll be indexing with i==*nbind before finding that (i < *nbind) is false.
>
>> Thanks
>>
>> Christophe
>>
>> -- 
>> Christophe Genolini
>> Ma?tre de conf?rences en bio-statistique
>> Universit? Paris Ouest Nanterre La D?fense
>> INSERM UMR 1027
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel


-- 
Christophe Genolini
Ma?tre de conf?rences en bio-statistique
Universit? Paris Ouest Nanterre La D?fense
INSERM UMR 1027


From pdalgd at gmail.com  Thu Mar 20 18:01:18 2014
From: pdalgd at gmail.com (peter dalgaard)
Date: Thu, 20 Mar 2014 18:01:18 +0100
Subject: [Rd] Memcheck: Invalid  read of size 4
In-Reply-To: <532B0FB2.30208@u-paris10.fr>
References: <532A1314.8040206@u-paris10.fr>
	<373A4631-6C57-493C-A347-0BAB256C507A@gmail.com>
	<532B0FB2.30208@u-paris10.fr>
Message-ID: <81FA6CE4-5E61-4974-8D93-90B86D9764D9@gmail.com>


On 20 Mar 2014, at 16:56 , Christophe Genolini <cgenolin at u-paris10.fr> wrote:

> Thanks a lot. Your correction works just fine.
> 
> Any idea of what goes wrong for the line 151, which is
> 
>   int *clusterAffectation2=malloc(*nbInd * sizeof(int));                      // lines 151
> 

Nothing. It's just that memcheck marks the point of allocation for you: There's a discrepancy between what you allocate and what you access, but it can't really tell whether the allocation was too short or the access steps past the end.

-pd

> 
> 
> 
>> On 19 Mar 2014, at 22:58 , Christophe Genolini <cgenolin at u-paris10.fr> wrote:
>> 
>>> Hi the list,
>>> 
>>> One of my package has a memory issue that I do not manage to understand. The Memtest notes is here:
>>> <http://www.stats.ox.ac.uk/pub/bdr/memtests/valgrind/kml-Ex.Rout>
>>> 
>>> Here is the message that I get from Memtest
>>> 
>>> --- 8< ----------------
>>> ~ Fast KmL ~
>>> ==27283== Invalid read of size 4
>>> ==27283==    at 0x10C5DF28: kml1 (kml.c:183)
>>> ...
>>> ==27283==    by 0x10C5DE4F: kml1 (kml.c:151)
>>> ...
>>> ==27283==    at 0x10C5DF90: kml1 (kml.c:198)
>>> --- 8< ----------------
>>> 
>>> 
>>> Here is the function kml1 from the file kml.c (I add some comments to tag the lines 151, 183 and 198)
>>> 
>>> --- 8< ----------------
>>> void kml1(double *traj, int *nbInd, int *nbTime, int *nbClusters, int *maxIt, int *clusterAffectation1, int *convergenceTime){
>>> 
>>>    int i=0,iter=0;
>>>    int *clusterAffectation2=malloc(*nbInd * sizeof(int));                      // lines 151
>>>    double *trajMean=malloc(*nbClusters * *nbTime * sizeof(double));
>>> 
>>>    for(i = 0; i < *nbClusters * *nbTime; i++){trajMean[i] = 0.0;};
>>>    for(i = 0; i < *nbInd; i++){clusterAffectation2[i] = 0;};
>>> 
>>>    for(iter = 0; iter < *maxIt; iter+=2){
>>> 	calculMean(traj,nbInd,nbTime,clusterAffectation1,nbClusters,trajMean);
>>>       	affecteIndiv(traj,nbInd,nbTime,trajMean,nbClusters,clusterAffectation2);
>>> 
>>> 	i = 0;
>>> 	while(clusterAffectation1[i]==clusterAffectation2[i] && i <*nbInd){i++;}; // lines 183
>>> 	if(i == *nbInd){
>>> 	    *convergenceTime = iter + 1;
>>> 	    break;
>>> 	}else{};
>>> 
>>> 	calculMean(traj,nbInd,nbTime,clusterAffectation2,nbClusters,trajMean);
>>> 	affecteIndiv(traj,nbInd,nbTime,trajMean,nbClusters,clusterAffectation1);
>>> 
>>> 	i = 0;
>>> 	while(clusterAffectation1[i]==clusterAffectation2[i] && i<*nbInd){i++;}; // lines 198
>>> 	if(i == *nbInd){
>>> 	    *convergenceTime = iter + 2;
>>>      	    break;
>>> 	}else{};
>>>    }
>>> }
>>> --- 8< ----------------
>>> 
>>> Do you know what is wrong in my C code?
>> Yes. You need to reverse operands of &&. Otherwise you'll be indexing with i==*nbind before finding that (i < *nbind) is false.
>> 
>>> Thanks
>>> 
>>> Christophe
>>> 
>>> -- 
>>> Christophe Genolini
>>> Ma?tre de conf?rences en bio-statistique
>>> Universit? Paris Ouest Nanterre La D?fense
>>> INSERM UMR 1027
>>> 
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
> 
> 
> -- 
> Christophe Genolini
> Ma?tre de conf?rences en bio-statistique
> Universit? Paris Ouest Nanterre La D?fense
> INSERM UMR 1027

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From 538280 at gmail.com  Thu Mar 20 18:23:47 2014
From: 538280 at gmail.com (Greg Snow)
Date: Thu, 20 Mar 2014 11:23:47 -0600
Subject: [Rd] The case for freezing CRAN
In-Reply-To: <21290.60882.865159.407363@max.nulle.part>
References: <3dfcdc$fuu13r@ironport9.mayo.edu>
	<21290.60882.865159.407363@max.nulle.part>
Message-ID: <CAFEqCdy8opJ-Rix0uB_7CAOEWqaA01YuL5nN78YhnhExfgfuOA@mail.gmail.com>

On Thu, Mar 20, 2014 at 7:32 AM, Dirk Eddelbuettel <edd at debian.org> wrote:
[snip]

>      (and some readers
>    may recall the infamous Pentium bug of two decades ago).

It was a "Flaw" not a "Bug".  At least I remember the Intel people
making a big deal about that distinction.

But I do remember the time well, I was a biostatistics Ph.D. student
at the time and bought one of the flawed pentiums.  My attempts at
getting the chip replaced resulted in a major run around and each
person that I talked to would first try to explain that I really did
not need the fix because the only people likely to be affected were
large corporations and research scientists.  I will admit that I was
not a large corporation, but if a Ph.D. student in biostatistics is
not a research scientist, then I did not know what they defined one
as.  When I pointed this out they would usually then say that it still
would not matter, unless I did a few thousand floating point
operations I was unlikely to encounter one of the problematic
divisions.  I would then point out that some days I did over 10,000
floating point operations before breakfast (I had checked after the
1st person told me this and 10,000 was a low estimate of a lower bound
of one set of simulations) at which point they would admit that I had
a case and then send me to talk to someone else who would start the
process over.



[snip]
> --
> Dirk Eddelbuettel | edd at debian.org | http://dirk.eddelbuettel.com
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel



-- 
Gregory (Greg) L. Snow Ph.D.
538280 at gmail.com


From kmillar at google.com  Thu Mar 20 18:57:21 2014
From: kmillar at google.com (Karl Millar)
Date: Thu, 20 Mar 2014 10:57:21 -0700
Subject: [Rd] The case for freezing CRAN
In-Reply-To: <21290.60882.865159.407363@max.nulle.part>
References: <3dfcdc$fuu13r@ironport9.mayo.edu>
	<21290.60882.865159.407363@max.nulle.part>
Message-ID: <CABz6aZdhFS_nnGDx5CMhv2DjiXXv7tFXS7naguwnUr44y_rb2g@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140320/13e19c44/attachment.pl>

From cboettig at gmail.com  Thu Mar 20 18:59:08 2014
From: cboettig at gmail.com (Carl Boettiger)
Date: Thu, 20 Mar 2014 10:59:08 -0700
Subject: [Rd] The case for freezing CRAN
In-Reply-To: <CAFEqCdy8opJ-Rix0uB_7CAOEWqaA01YuL5nN78YhnhExfgfuOA@mail.gmail.com>
References: <3dfcdc$fuu13r@ironport9.mayo.edu>
	<21290.60882.865159.407363@max.nulle.part>
	<CAFEqCdy8opJ-Rix0uB_7CAOEWqaA01YuL5nN78YhnhExfgfuOA@mail.gmail.com>
Message-ID: <CAN_1p9xvpDUDxs6H8u1TXUbwGh2=hgMHPXbBsB8waP5Udc4BTg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140320/891e5b92/attachment.pl>

From marc_schwartz at me.com  Thu Mar 20 19:02:53 2014
From: marc_schwartz at me.com (Marc Schwartz)
Date: Thu, 20 Mar 2014 13:02:53 -0500
Subject: [Rd] The case for freezing CRAN
In-Reply-To: <CAFEqCdy8opJ-Rix0uB_7CAOEWqaA01YuL5nN78YhnhExfgfuOA@mail.gmail.com>
References: <3dfcdc$fuu13r@ironport9.mayo.edu>
	<21290.60882.865159.407363@max.nulle.part>
	<CAFEqCdy8opJ-Rix0uB_7CAOEWqaA01YuL5nN78YhnhExfgfuOA@mail.gmail.com>
Message-ID: <487E20B5-F47B-4840-9D3F-E357592BB05E@me.com>


On Mar 20, 2014, at 12:23 PM, Greg Snow <538280 at gmail.com> wrote:

> On Thu, Mar 20, 2014 at 7:32 AM, Dirk Eddelbuettel <edd at debian.org> wrote:
> [snip]
> 
>>     (and some readers
>>   may recall the infamous Pentium bug of two decades ago).
> 
> It was a "Flaw" not a "Bug".  At least I remember the Intel people
> making a big deal about that distinction.
> 
> But I do remember the time well, I was a biostatistics Ph.D. student
> at the time and bought one of the flawed pentiums.  My attempts at
> getting the chip replaced resulted in a major run around and each
> person that I talked to would first try to explain that I really did
> not need the fix because the only people likely to be affected were
> large corporations and research scientists.  I will admit that I was
> not a large corporation, but if a Ph.D. student in biostatistics is
> not a research scientist, then I did not know what they defined one
> as.  When I pointed this out they would usually then say that it still
> would not matter, unless I did a few thousand floating point
> operations I was unlikely to encounter one of the problematic
> divisions.  I would then point out that some days I did over 10,000
> floating point operations before breakfast (I had checked after the
> 1st person told me this and 10,000 was a low estimate of a lower bound
> of one set of simulations) at which point they would admit that I had
> a case and then send me to talk to someone else who would start the
> process over.


Further segue:

That (1994) was a watershed moment for Intel as a company. A time during which Intel's future was quite literally at stake. Intel's internal response to that debacle, which fundamentally altered their own perception of just who their customer was (the OEM's like IBM, COMPAQ and Dell versus the end users like us), took time to be realized, as the impact of increasingly negative PR took hold. It was also a good example of the impact of public perception (a flawed product) versus the realities of how infrequently the flaw would be observed in "typical" computing. "Perception is reality", as some would observe.

Intel ultimately spent somewhere in the neighborhood of $500 million (in 1994 U.S. dollars), as I recall, to implement a large scale Pentium chip replacement infrastructure targeted to end users. The "Intel Inside" marketing campaign was also an outgrowth of that time period.

Regards,

Marc Schwartz


> [snip]
>> --
>> Dirk Eddelbuettel | edd at debian.org | http://dirk.eddelbuettel.com
>> 
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
> 
> 
> 
> -- 
> Gregory (Greg) L. Snow Ph.D.
> 538280 at gmail.com
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From hpages at fhcrc.org  Thu Mar 20 20:14:18 2014
From: hpages at fhcrc.org (=?ISO-8859-1?Q?Herv=E9_Pag=E8s?=)
Date: Thu, 20 Mar 2014 12:14:18 -0700
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <532AC885.1090305@gmail.com>
References: <129902082.964468.1395296156508.JavaMail.root@fhcrc.org>
	<532AC885.1090305@gmail.com>
Message-ID: <532B3E0A.4030907@fhcrc.org>

On 03/20/2014 03:52 AM, Duncan Murdoch wrote:
> On 14-03-20 2:15 AM, Dan Tenenbaum wrote:
>>
>>
>> ----- Original Message -----
>>> From: "David Winsemius" <dwinsemius at comcast.net>
>>> To: "Jeroen Ooms" <jeroen.ooms at stat.ucla.edu>
>>> Cc: "r-devel" <r-devel at r-project.org>
>>> Sent: Wednesday, March 19, 2014 11:03:32 PM
>>> Subject: Re: [Rd] [RFC] A case for freezing CRAN
>>>
>>>
>>> On Mar 19, 2014, at 7:45 PM, Jeroen Ooms wrote:
>>>
>>>> On Wed, Mar 19, 2014 at 6:55 PM, Michael Weylandt
>>>> <michael.weylandt at gmail.com> wrote:
>>>>> Reading this thread again, is it a fair summary of your position
>>>>> to say "reproducibility by default is more important than giving
>>>>> users access to the newest bug fixes and features by default?"
>>>>> It's certainly arguable, but I'm not sure I'm convinced: I'd
>>>>> imagine that the ratio of new work being done vs reproductions is
>>>>> rather high and the current setup optimizes for that already.
>>>>
>>>> I think that separating development from released branches can give
>>>> us
>>>> both reliability/reproducibility (stable branch) as well as new
>>>> features (unstable branch). The user gets to pick (and you can pick
>>>> both!). The same is true for r-base: when using a 'released'
>>>> version
>>>> you get 'stable' base packages that are up to 12 months old. If you
>>>> want to have the latest stuff you download a nightly build of
>>>> r-devel.
>>>> For regular users and reproducible research it is recommended to
>>>> use
>>>> the stable branch. However if you are a developer (e.g. package
>>>> author) you might want to develop/test/check your work with the
>>>> latest
>>>> r-devel.
>>>>
>>>> I think that extending the R release cycle to CRAN would result
>>>> both
>>>> in more stable released versions of R, as well as more freedom for
>>>> package authors to implement rigorous change in the unstable
>>>> branch.
>>>> When writing a script that is part of a production pipeline, or
>>>> sweave
>>>> paper that should be reproducible 10 years from now, or a book on
>>>> using R, you use stable version of R, which is guaranteed to behave
>>>> the same over time. However when developing packages that should be
>>>> compatible with the upcoming release of R, you use r-devel which
>>>> has
>>>> the latest versions of other CRAN and base packages.
>>>
>>>
>>> As I remember ... The example demonstrating the need for this was an
>>> XML package that cause an extract from a website where the headers
>>> were misinterpreted as data in one version of pkg:XML and not in
>>> another. That seems fairly unconvincing. Data cleaning and
>>> validation is a basic task of data analysis. It also seems excessive
>>> to assert that it is the responsibility of CRAN to maintain a synced
>>> binary archive that will be available in ten years.
>>
>>
>> CRAN already does this, the bin/windows/contrib directory has
>> subdirectories going back to 1.7, with packages dated October 2004. I
>> don't see why it is burdensome to continue to archive these. It would
>> be nice if source versions had a similar archive.
>
> The bin/windows/contrib directories are updated every day for active R
> versions.  It's only when Uwe decides that a version is no longer worth
> active support that he stops doing updates, and it "freezes".  A
> consequence of this is that the snapshots preserved in those older
> directories are unlikely to match what someone who keeps up to date with
> R releases is using.  Their purpose is to make sure that those older
> versions aren't completely useless, but they aren't what Jeroen was
> asking for.

But it is almost completely useless from a reproducibility point of
view to get random package versions. For example if some people try
to use R-2.13.2 today to reproduce an analysis that was published
2 years ago, they'll get Matrix 1.0-4 on Windows, Matrix 1.0-3 on Mac,
and Matrix 1.1-2-2 on Unix. And none of them of course is what was used
by the authors of the paper (they used Matrix 1.0-1, which is what was
current when they ran their analysis).

A big improvement from a reproducibility point of view would be to
(a) have a clear cut for the freezes, (b) freeze the source
packages as well as the binary packages, and (c) freeze the same
versions of source and binaries. For example the freeze of
bin/windows/contrib/x.y, bin/macosx/contrib/x.y and contrib/x.y
could happen when the R-x.y series itself freezes (i.e. no more
minor versions planned for this series).

Cheers,
H.

>
> Karl Millar's suggestion seems like an ideal solution to this problem.
> Any CRAN mirror could implement it.  If someone sets this up and commits
> to maintaining it, I'd be happy to work on the necessary changes to the
> install.packages/update.packages code to allow people to use it from
> within R.
>
> Duncan Murdoch
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fhcrc.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From marc_schwartz at me.com  Thu Mar 20 21:24:48 2014
From: marc_schwartz at me.com (Marc Schwartz)
Date: Thu, 20 Mar 2014 15:24:48 -0500
Subject: [Rd] The case for freezing CRAN
In-Reply-To: <487E20B5-F47B-4840-9D3F-E357592BB05E@me.com>
References: <3dfcdc$fuu13r@ironport9.mayo.edu>
	<21290.60882.865159.407363@max.nulle.part>
	<CAFEqCdy8opJ-Rix0uB_7CAOEWqaA01YuL5nN78YhnhExfgfuOA@mail.gmail.com>
	<487E20B5-F47B-4840-9D3F-E357592BB05E@me.com>
Message-ID: <106161DC-B5C2-4A76-8520-EEF1D8FEA87F@me.com>


On Mar 20, 2014, at 1:02 PM, Marc Schwartz <marc_schwartz at me.com> wrote:

> 
> On Mar 20, 2014, at 12:23 PM, Greg Snow <538280 at gmail.com> wrote:
> 
>> On Thu, Mar 20, 2014 at 7:32 AM, Dirk Eddelbuettel <edd at debian.org> wrote:
>> [snip]
>> 
>>>    (and some readers
>>>  may recall the infamous Pentium bug of two decades ago).
>> 
>> It was a "Flaw" not a "Bug".  At least I remember the Intel people
>> making a big deal about that distinction.
>> 
>> But I do remember the time well, I was a biostatistics Ph.D. student
>> at the time and bought one of the flawed pentiums.  My attempts at
>> getting the chip replaced resulted in a major run around and each
>> person that I talked to would first try to explain that I really did
>> not need the fix because the only people likely to be affected were
>> large corporations and research scientists.  I will admit that I was
>> not a large corporation, but if a Ph.D. student in biostatistics is
>> not a research scientist, then I did not know what they defined one
>> as.  When I pointed this out they would usually then say that it still
>> would not matter, unless I did a few thousand floating point
>> operations I was unlikely to encounter one of the problematic
>> divisions.  I would then point out that some days I did over 10,000
>> floating point operations before breakfast (I had checked after the
>> 1st person told me this and 10,000 was a low estimate of a lower bound
>> of one set of simulations) at which point they would admit that I had
>> a case and then send me to talk to someone else who would start the
>> process over.
> 
> 
> Further segue:
> 
> That (1994) was a watershed moment for Intel as a company. A time during which Intel's future was quite literally at stake. Intel's internal response to that debacle, which fundamentally altered their own perception of just who their customer was (the OEM's like IBM, COMPAQ and Dell versus the end users like us), took time to be realized, as the impact of increasingly negative PR took hold. It was also a good example of the impact of public perception (a flawed product) versus the realities of how infrequently the flaw would be observed in "typical" computing. "Perception is reality", as some would observe.
> 
> Intel ultimately spent somewhere in the neighborhood of $500 million (in 1994 U.S. dollars), as I recall, to implement a large scale Pentium chip replacement infrastructure targeted to end users. The "Intel Inside" marketing campaign was also an outgrowth of that time period.
> 


Quick correction, thanks to Peter, on my assertion that the "Intel Inside" campaign arose from the 1994 Pentium issue. It actually started in 1991.

I had a faulty recollection from my long ago reading of Andy Grove's 1996 book, "Only The Paranoid Survive", that the slogan arose from Intel's reaction to the Pentium fiasco. It actually pre-dated that time frame by a few years.

Thanks Peter!

Regards,

Marc


From r.ted.byers at gmail.com  Thu Mar 20 21:28:22 2014
From: r.ted.byers at gmail.com (Ted Byers)
Date: Thu, 20 Mar 2014 16:28:22 -0400
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <532B3E0A.4030907@fhcrc.org>
References: <129902082.964468.1395296156508.JavaMail.root@fhcrc.org>
	<532AC885.1090305@gmail.com> <532B3E0A.4030907@fhcrc.org>
Message-ID: <CAOTG1hVzjPBhw8yZ5egftKf6dBXKzgUZ_xDBw7fMz+ASk10xTQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140320/79cdfd8a/attachment.pl>

From jeroen.ooms at stat.ucla.edu  Thu Mar 20 21:53:04 2014
From: jeroen.ooms at stat.ucla.edu (Jeroen Ooms)
Date: Thu, 20 Mar 2014 13:53:04 -0700
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <CAOTG1hVzjPBhw8yZ5egftKf6dBXKzgUZ_xDBw7fMz+ASk10xTQ@mail.gmail.com>
References: <129902082.964468.1395296156508.JavaMail.root@fhcrc.org>
	<532AC885.1090305@gmail.com> <532B3E0A.4030907@fhcrc.org>
	<CAOTG1hVzjPBhw8yZ5egftKf6dBXKzgUZ_xDBw7fMz+ASk10xTQ@mail.gmail.com>
Message-ID: <CABFfbXtZaW-2jZOkiqt+VTGbEkA=VfOgSi3gMWvtD1WerV_uUQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140320/3a69cf4a/attachment.pl>

From tim.triche at gmail.com  Thu Mar 20 22:11:17 2014
From: tim.triche at gmail.com (Tim Triche, Jr.)
Date: Thu, 20 Mar 2014 14:11:17 -0700
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <CAOTG1hVzjPBhw8yZ5egftKf6dBXKzgUZ_xDBw7fMz+ASk10xTQ@mail.gmail.com>
References: <129902082.964468.1395296156508.JavaMail.root@fhcrc.org>
	<532AC885.1090305@gmail.com> <532B3E0A.4030907@fhcrc.org>
	<CAOTG1hVzjPBhw8yZ5egftKf6dBXKzgUZ_xDBw7fMz+ASk10xTQ@mail.gmail.com>
Message-ID: <CAC+N9BXCA-a6_x7z=bWmsk_=w1mHZkpvK6Mt-qqtQ7nGW1XaMQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140320/31000478/attachment.pl>

From r.ted.byers at gmail.com  Thu Mar 20 22:13:26 2014
From: r.ted.byers at gmail.com (Ted Byers)
Date: Thu, 20 Mar 2014 17:13:26 -0400
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <CABFfbXtZaW-2jZOkiqt+VTGbEkA=VfOgSi3gMWvtD1WerV_uUQ@mail.gmail.com>
References: <129902082.964468.1395296156508.JavaMail.root@fhcrc.org>
	<532AC885.1090305@gmail.com> <532B3E0A.4030907@fhcrc.org>
	<CAOTG1hVzjPBhw8yZ5egftKf6dBXKzgUZ_xDBw7fMz+ASk10xTQ@mail.gmail.com>
	<CABFfbXtZaW-2jZOkiqt+VTGbEkA=VfOgSi3gMWvtD1WerV_uUQ@mail.gmail.com>
Message-ID: <CAOTG1hUhhnO70MQ+Mhmt-byGQx8arr0nAOPec2fs10iDJaDJgA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140320/d8042489/attachment.pl>

From r.ted.byers at gmail.com  Thu Mar 20 22:24:47 2014
From: r.ted.byers at gmail.com (Ted Byers)
Date: Thu, 20 Mar 2014 17:24:47 -0400
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <CAC+N9BXCA-a6_x7z=bWmsk_=w1mHZkpvK6Mt-qqtQ7nGW1XaMQ@mail.gmail.com>
References: <129902082.964468.1395296156508.JavaMail.root@fhcrc.org>
	<532AC885.1090305@gmail.com> <532B3E0A.4030907@fhcrc.org>
	<CAOTG1hVzjPBhw8yZ5egftKf6dBXKzgUZ_xDBw7fMz+ASk10xTQ@mail.gmail.com>
	<CAC+N9BXCA-a6_x7z=bWmsk_=w1mHZkpvK6Mt-qqtQ7nGW1XaMQ@mail.gmail.com>
Message-ID: <CAOTG1hV9x3fJ2JA=+ZgtU0SaoEotnoCbs4EmdygN523PmZXA7A@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140320/0bfc5673/attachment.pl>

From tim.triche at gmail.com  Thu Mar 20 22:27:00 2014
From: tim.triche at gmail.com (Tim Triche, Jr.)
Date: Thu, 20 Mar 2014 14:27:00 -0700
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <CAOTG1hUhhnO70MQ+Mhmt-byGQx8arr0nAOPec2fs10iDJaDJgA@mail.gmail.com>
References: <129902082.964468.1395296156508.JavaMail.root@fhcrc.org>
	<532AC885.1090305@gmail.com> <532B3E0A.4030907@fhcrc.org>
	<CAOTG1hVzjPBhw8yZ5egftKf6dBXKzgUZ_xDBw7fMz+ASk10xTQ@mail.gmail.com>
	<CABFfbXtZaW-2jZOkiqt+VTGbEkA=VfOgSi3gMWvtD1WerV_uUQ@mail.gmail.com>
	<CAOTG1hUhhnO70MQ+Mhmt-byGQx8arr0nAOPec2fs10iDJaDJgA@mail.gmail.com>
Message-ID: <CAC+N9BVDeD7SxjQfStsr0-sYva-+FfVM-N4ksfU8zaU3r1sJbg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140320/9e1f800d/attachment.pl>

From r.ted.byers at gmail.com  Thu Mar 20 22:55:52 2014
From: r.ted.byers at gmail.com (Ted Byers)
Date: Thu, 20 Mar 2014 17:55:52 -0400
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <CAC+N9BVDeD7SxjQfStsr0-sYva-+FfVM-N4ksfU8zaU3r1sJbg@mail.gmail.com>
References: <129902082.964468.1395296156508.JavaMail.root@fhcrc.org>
	<532AC885.1090305@gmail.com> <532B3E0A.4030907@fhcrc.org>
	<CAOTG1hVzjPBhw8yZ5egftKf6dBXKzgUZ_xDBw7fMz+ASk10xTQ@mail.gmail.com>
	<CABFfbXtZaW-2jZOkiqt+VTGbEkA=VfOgSi3gMWvtD1WerV_uUQ@mail.gmail.com>
	<CAOTG1hUhhnO70MQ+Mhmt-byGQx8arr0nAOPec2fs10iDJaDJgA@mail.gmail.com>
	<CAC+N9BVDeD7SxjQfStsr0-sYva-+FfVM-N4ksfU8zaU3r1sJbg@mail.gmail.com>
Message-ID: <CAOTG1hXFxihMnZUbaQ+psBJJqecBE_pDL8bZ-OSZf7csFTp=pQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140320/415c16a1/attachment.pl>

From hpages at fhcrc.org  Thu Mar 20 23:23:57 2014
From: hpages at fhcrc.org (=?ISO-8859-1?Q?Herv=E9_Pag=E8s?=)
Date: Thu, 20 Mar 2014 15:23:57 -0700
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <CAOTG1hVzjPBhw8yZ5egftKf6dBXKzgUZ_xDBw7fMz+ASk10xTQ@mail.gmail.com>
References: <129902082.964468.1395296156508.JavaMail.root@fhcrc.org>	<532AC885.1090305@gmail.com>	<532B3E0A.4030907@fhcrc.org>
	<CAOTG1hVzjPBhw8yZ5egftKf6dBXKzgUZ_xDBw7fMz+ASk10xTQ@mail.gmail.com>
Message-ID: <532B6A7D.3070800@fhcrc.org>



On 03/20/2014 01:28 PM, Ted Byers wrote:
> On Thu, Mar 20, 2014 at 3:14 PM, Herv? Pag?s <hpages at fhcrc.org
> <mailto:hpages at fhcrc.org>> wrote:
>
>     On 03/20/2014 03:52 AM, Duncan Murdoch wrote:
>
>         On 14-03-20 2:15 AM, Dan Tenenbaum wrote:
>
>
>
>             ----- Original Message -----
>
>                 From: "David Winsemius" <dwinsemius at comcast.net
>                 <mailto:dwinsemius at comcast.net>>
>                 To: "Jeroen Ooms" <jeroen.ooms at stat.ucla.edu
>                 <mailto:jeroen.ooms at stat.ucla.edu>>
>                 Cc: "r-devel" <r-devel at r-project.org
>                 <mailto:r-devel at r-project.org>>
>                 Sent: Wednesday, March 19, 2014 11:03:32 PM
>                 Subject: Re: [Rd] [RFC] A case for freezing CRAN
>
>
>                 On Mar 19, 2014, at 7:45 PM, Jeroen Ooms wrote:
>
>                     On Wed, Mar 19, 2014 at 6:55 PM, Michael Weylandt
>                     <michael.weylandt at gmail.com
>                     <mailto:michael.weylandt at gmail.com>> wrote:
>
>                         Reading this thread again, is it a fair summary
>                         of your position
>                         to say "reproducibility by default is more
>                         important than giving
>                         users access to the newest bug fixes and
>                         features by default?"
>                         It's certainly arguable, but I'm not sure I'm
>                         convinced: I'd
>                         imagine that the ratio of new work being done vs
>                         reproductions is
>                         rather high and the current setup optimizes for
>                         that already.
>
>
>                     I think that separating development from released
>                     branches can give
>                     us
>                     both reliability/reproducibility (stable branch) as
>                     well as new
>                     features (unstable branch). The user gets to pick
>                     (and you can pick
>                     both!). The same is true for r-base: when using a
>                     'released'
>                     version
>                     you get 'stable' base packages that are up to 12
>                     months old. If you
>                     want to have the latest stuff you download a nightly
>                     build of
>                     r-devel.
>                     For regular users and reproducible research it is
>                     recommended to
>                     use
>                     the stable branch. However if you are a developer
>                     (e.g. package
>                     author) you might want to develop/test/check your
>                     work with the
>                     latest
>                     r-devel.
>
>                     I think that extending the R release cycle to CRAN
>                     would result
>                     both
>                     in more stable released versions of R, as well as
>                     more freedom for
>                     package authors to implement rigorous change in the
>                     unstable
>                     branch.
>                     When writing a script that is part of a production
>                     pipeline, or
>                     sweave
>                     paper that should be reproducible 10 years from now,
>                     or a book on
>                     using R, you use stable version of R, which is
>                     guaranteed to behave
>                     the same over time. However when developing packages
>                     that should be
>                     compatible with the upcoming release of R, you use
>                     r-devel which
>                     has
>                     the latest versions of other CRAN and base packages.
>
>
>
>                 As I remember ... The example demonstrating the need for
>                 this was an
>                 XML package that cause an extract from a website where
>                 the headers
>                 were misinterpreted as data in one version of pkg:XML
>                 and not in
>                 another. That seems fairly unconvincing. Data cleaning and
>                 validation is a basic task of data analysis. It also
>                 seems excessive
>                 to assert that it is the responsibility of CRAN to
>                 maintain a synced
>                 binary archive that will be available in ten years.
>
>
>
>             CRAN already does this, the bin/windows/contrib directory has
>             subdirectories going back to 1.7, with packages dated
>             October 2004. I
>             don't see why it is burdensome to continue to archive these.
>             It would
>             be nice if source versions had a similar archive.
>
>
>         The bin/windows/contrib directories are updated every day for
>         active R
>         versions.  It's only when Uwe decides that a version is no
>         longer worth
>         active support that he stops doing updates, and it "freezes".  A
>         consequence of this is that the snapshots preserved in those older
>         directories are unlikely to match what someone who keeps up to
>         date with
>         R releases is using.  Their purpose is to make sure that those older
>         versions aren't completely useless, but they aren't what Jeroen was
>         asking for.
>
>
>     But it is almost completely useless from a reproducibility point of
>     view to get random package versions. For example if some people try
>     to use R-2.13.2 today to reproduce an analysis that was published
>     2 years ago, they'll get Matrix 1.0-4 on Windows, Matrix 1.0-3 on Mac,
>     and Matrix 1.1-2-2 on Unix. And none of them of course is what was used
>     by the authors of the paper (they used Matrix 1.0-1, which is what was
>     current when they ran their analysis).
>
> Initially this discussion brought back nightmares of DLL hell on
> Windows.  Those as ancient as I will remember that well.  But now, the
> focus seems to be on reproducibility, but with what strikes me as a
> seriously flawed notion of what reproducibility means.
>
> Herve Pages mentions the risk of irreproducibility across three minor
> revisions of version 1.0 of Matrix.

If you use R-2.13.2, you get Matrix 1.1-2-2 on Linux. AFAIK this is
the most recent version of Matrix, aimed to be compatible with the most
current version of R (i.e. R 3.0.3). However, it has never been tested
with R-2.13.2. I'm not saying that it should, that would be a big waste
of resources of course. All I'm saying it that it doesn't make sense to
serve by default a version that is known to be incompatible with the
version of R being used. It's very likely to not even install properly.

For the apparently small differences between the versions you get on
Windows and Mac, the Matrix package was just an example. With other
packages you get (again if you use R-2.13.2):

               src   win    mac
   abc         1.8   1.5    1.4
   ape       3.1-1 3.0-1    2.8
   BaSTA     1.9.3   1.1    1.0
   bcrm      0.4.3   0.2    0.1
   BMA    3.16.2.3  3.15 3.14.1
   Boruta    3.0.0   1.6    1.5
   ...

Are the differences big enough?

Also note that back in October 2011, people using R-2.13.2 would get
e.g. ape 2.7-3 on Linux, Windows and Mac. Wouldn't it make sense that
people using R-2.13.2 today get the same? Why would anybody use
R-2.13.2 today if it's not to run again some code that was written
and used two years ago to obtain some important results?

Cheers,
H.


> My gut reaction would be that if
> the results are not reproducible across such minor revisions of one
> library, they are probably just so much BS.  I am trained in
> mathematical ecology, with more than a couple decades of post-doc
> experience working with risk assessment in the private sector.  When I
> need to do an analysis, I will repeat it myself in multiple products, as
> well as C++ or FORTRAN code I have hand-crafted myself (and when I wrote
> number crunching code myself, I would do so in multiple programming
> languages - C++, Java, FORTRAN, applying rigorous QA procedures to each
> program/library I developed).  Back when I was a grad student, I would
> not even show the results to my supervisor, let alone try to publish
> them, unless the results were reproducible across ALL the tools I used.
> If there was a discrepancy, I would debug that before discussing them
> with anyone.  Surely, it is the responsibility of the journals' editors
> and reviewers to apply a similar practice.
>
> The concept of reproducibility used to this point in this discussion
> might be adequate from a programmers perspective (except in my lab), it
> is wholly inadequate from a scientist's perspective.  I maintain that if
> you have the original data, and repeat the analysis using the latest
> version of R and the available, relevant packages, the original results
> are probably due to a bug either in the R script or in R or the packages
> used IF the results obtained using the latest versions of these are not
> consistent with the originally reported results.  Therefore, of the
> concerns I see raised in this discussion, the principle one of concern
> is that of package developers who fail to pay sufficient attention to
> backwards compatibility: a new version ought not break any code that
> executes fine using previous versions.  That is not a trivial task, and
> may require contributors obtaining the assistance of a software
> engineer.  I am sure anyone in this list who programs in C++ knows how
> the ANSI committees handle change management.  Introduction of new
> features is something that is largely irrelevant for backwards
> compatibility (but there are exceptions), but features to be removed
> are handled by declaring them deprecated, and leaving them in that
> condition for years.  That tells anyone using the language that they
> ought to plan to adapt their code to work when the deprecated feature is
> finally removed.
>
> I am responsible for maintaining code (involving distributed computing)
> to which many companies integrate their systems, and I am careful to
> ensure that no change I make breaks their integration into my system,
> even though I often have to add new features.  And I don't add features
> lightly, and have yet to remove features.  When that eventually happens,
> the old feature will be deprecated, so that the other companies have
> plenty of time to adapt their integration code.  I do not know whether
> CRAN ought to have any responsibility for this sort of change
> management, or if they have assumed some responsibility for some of it,
> but I would argue that the package developers have the primary
> responsibility for doing this right.
>
> Just my $0.05 (the penny no longer exists in Canada)
>
> Cheers
>
> Ted
> R.E. (Ted) Byers, Ph.D., Ed.D.

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fhcrc.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From ligges at statistik.tu-dortmund.de  Thu Mar 20 23:29:46 2014
From: ligges at statistik.tu-dortmund.de (Uwe Ligges)
Date: Thu, 20 Mar 2014 23:29:46 +0100
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <532B6A7D.3070800@fhcrc.org>
References: <129902082.964468.1395296156508.JavaMail.root@fhcrc.org>	<532AC885.1090305@gmail.com>	<532B3E0A.4030907@fhcrc.org>	<CAOTG1hVzjPBhw8yZ5egftKf6dBXKzgUZ_xDBw7fMz+ASk10xTQ@mail.gmail.com>
	<532B6A7D.3070800@fhcrc.org>
Message-ID: <532B6BDA.8070006@statistik.tu-dortmund.de>



On 20.03.2014 23:23, Herv? Pag?s wrote:
>
>
> On 03/20/2014 01:28 PM, Ted Byers wrote:
>> On Thu, Mar 20, 2014 at 3:14 PM, Herv? Pag?s <hpages at fhcrc.org
>> <mailto:hpages at fhcrc.org>> wrote:
>>
>>     On 03/20/2014 03:52 AM, Duncan Murdoch wrote:
>>
>>         On 14-03-20 2:15 AM, Dan Tenenbaum wrote:
>>
>>
>>
>>             ----- Original Message -----
>>
>>                 From: "David Winsemius" <dwinsemius at comcast.net
>>                 <mailto:dwinsemius at comcast.net>>
>>                 To: "Jeroen Ooms" <jeroen.ooms at stat.ucla.edu
>>                 <mailto:jeroen.ooms at stat.ucla.edu>>
>>                 Cc: "r-devel" <r-devel at r-project.org
>>                 <mailto:r-devel at r-project.org>>
>>                 Sent: Wednesday, March 19, 2014 11:03:32 PM
>>                 Subject: Re: [Rd] [RFC] A case for freezing CRAN
>>
>>
>>                 On Mar 19, 2014, at 7:45 PM, Jeroen Ooms wrote:
>>
>>                     On Wed, Mar 19, 2014 at 6:55 PM, Michael Weylandt
>>                     <michael.weylandt at gmail.com
>>                     <mailto:michael.weylandt at gmail.com>> wrote:
>>
>>                         Reading this thread again, is it a fair summary
>>                         of your position
>>                         to say "reproducibility by default is more
>>                         important than giving
>>                         users access to the newest bug fixes and
>>                         features by default?"
>>                         It's certainly arguable, but I'm not sure I'm
>>                         convinced: I'd
>>                         imagine that the ratio of new work being done vs
>>                         reproductions is
>>                         rather high and the current setup optimizes for
>>                         that already.
>>
>>
>>                     I think that separating development from released
>>                     branches can give
>>                     us
>>                     both reliability/reproducibility (stable branch) as
>>                     well as new
>>                     features (unstable branch). The user gets to pick
>>                     (and you can pick
>>                     both!). The same is true for r-base: when using a
>>                     'released'
>>                     version
>>                     you get 'stable' base packages that are up to 12
>>                     months old. If you
>>                     want to have the latest stuff you download a nightly
>>                     build of
>>                     r-devel.
>>                     For regular users and reproducible research it is
>>                     recommended to
>>                     use
>>                     the stable branch. However if you are a developer
>>                     (e.g. package
>>                     author) you might want to develop/test/check your
>>                     work with the
>>                     latest
>>                     r-devel.
>>
>>                     I think that extending the R release cycle to CRAN
>>                     would result
>>                     both
>>                     in more stable released versions of R, as well as
>>                     more freedom for
>>                     package authors to implement rigorous change in the
>>                     unstable
>>                     branch.
>>                     When writing a script that is part of a production
>>                     pipeline, or
>>                     sweave
>>                     paper that should be reproducible 10 years from now,
>>                     or a book on
>>                     using R, you use stable version of R, which is
>>                     guaranteed to behave
>>                     the same over time. However when developing packages
>>                     that should be
>>                     compatible with the upcoming release of R, you use
>>                     r-devel which
>>                     has
>>                     the latest versions of other CRAN and base packages.
>>
>>
>>
>>                 As I remember ... The example demonstrating the need for
>>                 this was an
>>                 XML package that cause an extract from a website where
>>                 the headers
>>                 were misinterpreted as data in one version of pkg:XML
>>                 and not in
>>                 another. That seems fairly unconvincing. Data cleaning
>> and
>>                 validation is a basic task of data analysis. It also
>>                 seems excessive
>>                 to assert that it is the responsibility of CRAN to
>>                 maintain a synced
>>                 binary archive that will be available in ten years.
>>
>>
>>
>>             CRAN already does this, the bin/windows/contrib directory has
>>             subdirectories going back to 1.7, with packages dated
>>             October 2004. I
>>             don't see why it is burdensome to continue to archive these.
>>             It would
>>             be nice if source versions had a similar archive.
>>
>>
>>         The bin/windows/contrib directories are updated every day for
>>         active R
>>         versions.  It's only when Uwe decides that a version is no
>>         longer worth
>>         active support that he stops doing updates, and it "freezes".  A
>>         consequence of this is that the snapshots preserved in those
>> older
>>         directories are unlikely to match what someone who keeps up to
>>         date with
>>         R releases is using.  Their purpose is to make sure that those
>> older
>>         versions aren't completely useless, but they aren't what
>> Jeroen was
>>         asking for.
>>
>>
>>     But it is almost completely useless from a reproducibility point of
>>     view to get random package versions. For example if some people try
>>     to use R-2.13.2 today to reproduce an analysis that was published
>>     2 years ago, they'll get Matrix 1.0-4 on Windows, Matrix 1.0-3 on
>> Mac,
>>     and Matrix 1.1-2-2 on Unix.

Not true, since Matrix 1.1-2-2 has

Depends: 	R (? 2.15.2)


Best,
Uwe Ligges


  And none of them of course is what was
>> used
>>     by the authors of the paper (they used Matrix 1.0-1, which is what
>> was
>>     current when they ran their analysis).
>>
>> Initially this discussion brought back nightmares of DLL hell on
>> Windows.  Those as ancient as I will remember that well.  But now, the
>> focus seems to be on reproducibility, but with what strikes me as a
>> seriously flawed notion of what reproducibility means.
>>
>> Herve Pages mentions the risk of irreproducibility across three minor
>> revisions of version 1.0 of Matrix.
>
> If you use R-2.13.2, you get Matrix 1.1-2-2 on Linux. AFAIK this is
> the most recent version of Matrix, aimed to be compatible with the most
> current version of R (i.e. R 3.0.3). However, it has never been tested
> with R-2.13.2. I'm not saying that it should, that would be a big waste
> of resources of course. All I'm saying it that it doesn't make sense to
> serve by default a version that is known to be incompatible with the
> version of R being used. It's very likely to not even install properly.
>
> For the apparently small differences between the versions you get on
> Windows and Mac, the Matrix package was just an example. With other
> packages you get (again if you use R-2.13.2):
>
>                src   win    mac
>    abc         1.8   1.5    1.4
>    ape       3.1-1 3.0-1    2.8
>    BaSTA     1.9.3   1.1    1.0
>    bcrm      0.4.3   0.2    0.1
>    BMA    3.16.2.3  3.15 3.14.1
>    Boruta    3.0.0   1.6    1.5
>    ...
>
> Are the differences big enough?
>
> Also note that back in October 2011, people using R-2.13.2 would get
> e.g. ape 2.7-3 on Linux, Windows and Mac. Wouldn't it make sense that
> people using R-2.13.2 today get the same? Why would anybody use
> R-2.13.2 today if it's not to run again some code that was written
> and used two years ago to obtain some important results?
>
> Cheers,
> H.
>
>
>> My gut reaction would be that if
>> the results are not reproducible across such minor revisions of one
>> library, they are probably just so much BS.  I am trained in
>> mathematical ecology, with more than a couple decades of post-doc
>> experience working with risk assessment in the private sector.  When I
>> need to do an analysis, I will repeat it myself in multiple products, as
>> well as C++ or FORTRAN code I have hand-crafted myself (and when I wrote
>> number crunching code myself, I would do so in multiple programming
>> languages - C++, Java, FORTRAN, applying rigorous QA procedures to each
>> program/library I developed).  Back when I was a grad student, I would
>> not even show the results to my supervisor, let alone try to publish
>> them, unless the results were reproducible across ALL the tools I used.
>> If there was a discrepancy, I would debug that before discussing them
>> with anyone.  Surely, it is the responsibility of the journals' editors
>> and reviewers to apply a similar practice.
>>
>> The concept of reproducibility used to this point in this discussion
>> might be adequate from a programmers perspective (except in my lab), it
>> is wholly inadequate from a scientist's perspective.  I maintain that if
>> you have the original data, and repeat the analysis using the latest
>> version of R and the available, relevant packages, the original results
>> are probably due to a bug either in the R script or in R or the packages
>> used IF the results obtained using the latest versions of these are not
>> consistent with the originally reported results.  Therefore, of the
>> concerns I see raised in this discussion, the principle one of concern
>> is that of package developers who fail to pay sufficient attention to
>> backwards compatibility: a new version ought not break any code that
>> executes fine using previous versions.  That is not a trivial task, and
>> may require contributors obtaining the assistance of a software
>> engineer.  I am sure anyone in this list who programs in C++ knows how
>> the ANSI committees handle change management.  Introduction of new
>> features is something that is largely irrelevant for backwards
>> compatibility (but there are exceptions), but features to be removed
>> are handled by declaring them deprecated, and leaving them in that
>> condition for years.  That tells anyone using the language that they
>> ought to plan to adapt their code to work when the deprecated feature is
>> finally removed.
>>
>> I am responsible for maintaining code (involving distributed computing)
>> to which many companies integrate their systems, and I am careful to
>> ensure that no change I make breaks their integration into my system,
>> even though I often have to add new features.  And I don't add features
>> lightly, and have yet to remove features.  When that eventually happens,
>> the old feature will be deprecated, so that the other companies have
>> plenty of time to adapt their integration code.  I do not know whether
>> CRAN ought to have any responsibility for this sort of change
>> management, or if they have assumed some responsibility for some of it,
>> but I would argue that the package developers have the primary
>> responsibility for doing this right.
>>
>> Just my $0.05 (the penny no longer exists in Canada)
>>
>> Cheers
>>
>> Ted
>> R.E. (Ted) Byers, Ph.D., Ed.D.
>


From hpages at fhcrc.org  Fri Mar 21 00:28:15 2014
From: hpages at fhcrc.org (=?UTF-8?B?SGVydsOpIFBhZ8Oocw==?=)
Date: Thu, 20 Mar 2014 16:28:15 -0700
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <532B6BDA.8070006@statistik.tu-dortmund.de>
References: <129902082.964468.1395296156508.JavaMail.root@fhcrc.org>	<532AC885.1090305@gmail.com>	<532B3E0A.4030907@fhcrc.org>	<CAOTG1hVzjPBhw8yZ5egftKf6dBXKzgUZ_xDBw7fMz+ASk10xTQ@mail.gmail.com>
	<532B6A7D.3070800@fhcrc.org>
	<532B6BDA.8070006@statistik.tu-dortmund.de>
Message-ID: <532B798F.2040905@fhcrc.org>

On 03/20/2014 03:29 PM, Uwe Ligges wrote:
>
>
> On 20.03.2014 23:23, Herv? Pag?s wrote:
>>
>>
>> On 03/20/2014 01:28 PM, Ted Byers wrote:
>>> On Thu, Mar 20, 2014 at 3:14 PM, Herv? Pag?s <hpages at fhcrc.org
>>> <mailto:hpages at fhcrc.org>> wrote:
>>>
>>>     On 03/20/2014 03:52 AM, Duncan Murdoch wrote:
>>>
>>>         On 14-03-20 2:15 AM, Dan Tenenbaum wrote:
>>>
>>>
>>>
>>>             ----- Original Message -----
>>>
>>>                 From: "David Winsemius" <dwinsemius at comcast.net
>>>                 <mailto:dwinsemius at comcast.net>>
>>>                 To: "Jeroen Ooms" <jeroen.ooms at stat.ucla.edu
>>>                 <mailto:jeroen.ooms at stat.ucla.edu>>
>>>                 Cc: "r-devel" <r-devel at r-project.org
>>>                 <mailto:r-devel at r-project.org>>
>>>                 Sent: Wednesday, March 19, 2014 11:03:32 PM
>>>                 Subject: Re: [Rd] [RFC] A case for freezing CRAN
>>>
>>>
>>>                 On Mar 19, 2014, at 7:45 PM, Jeroen Ooms wrote:
>>>
>>>                     On Wed, Mar 19, 2014 at 6:55 PM, Michael Weylandt
>>>                     <michael.weylandt at gmail.com
>>>                     <mailto:michael.weylandt at gmail.com>> wrote:
>>>
>>>                         Reading this thread again, is it a fair summary
>>>                         of your position
>>>                         to say "reproducibility by default is more
>>>                         important than giving
>>>                         users access to the newest bug fixes and
>>>                         features by default?"
>>>                         It's certainly arguable, but I'm not sure I'm
>>>                         convinced: I'd
>>>                         imagine that the ratio of new work being done vs
>>>                         reproductions is
>>>                         rather high and the current setup optimizes for
>>>                         that already.
>>>
>>>
>>>                     I think that separating development from released
>>>                     branches can give
>>>                     us
>>>                     both reliability/reproducibility (stable branch) as
>>>                     well as new
>>>                     features (unstable branch). The user gets to pick
>>>                     (and you can pick
>>>                     both!). The same is true for r-base: when using a
>>>                     'released'
>>>                     version
>>>                     you get 'stable' base packages that are up to 12
>>>                     months old. If you
>>>                     want to have the latest stuff you download a nightly
>>>                     build of
>>>                     r-devel.
>>>                     For regular users and reproducible research it is
>>>                     recommended to
>>>                     use
>>>                     the stable branch. However if you are a developer
>>>                     (e.g. package
>>>                     author) you might want to develop/test/check your
>>>                     work with the
>>>                     latest
>>>                     r-devel.
>>>
>>>                     I think that extending the R release cycle to CRAN
>>>                     would result
>>>                     both
>>>                     in more stable released versions of R, as well as
>>>                     more freedom for
>>>                     package authors to implement rigorous change in the
>>>                     unstable
>>>                     branch.
>>>                     When writing a script that is part of a production
>>>                     pipeline, or
>>>                     sweave
>>>                     paper that should be reproducible 10 years from now,
>>>                     or a book on
>>>                     using R, you use stable version of R, which is
>>>                     guaranteed to behave
>>>                     the same over time. However when developing packages
>>>                     that should be
>>>                     compatible with the upcoming release of R, you use
>>>                     r-devel which
>>>                     has
>>>                     the latest versions of other CRAN and base packages.
>>>
>>>
>>>
>>>                 As I remember ... The example demonstrating the need for
>>>                 this was an
>>>                 XML package that cause an extract from a website where
>>>                 the headers
>>>                 were misinterpreted as data in one version of pkg:XML
>>>                 and not in
>>>                 another. That seems fairly unconvincing. Data cleaning
>>> and
>>>                 validation is a basic task of data analysis. It also
>>>                 seems excessive
>>>                 to assert that it is the responsibility of CRAN to
>>>                 maintain a synced
>>>                 binary archive that will be available in ten years.
>>>
>>>
>>>
>>>             CRAN already does this, the bin/windows/contrib directory
>>> has
>>>             subdirectories going back to 1.7, with packages dated
>>>             October 2004. I
>>>             don't see why it is burdensome to continue to archive these.
>>>             It would
>>>             be nice if source versions had a similar archive.
>>>
>>>
>>>         The bin/windows/contrib directories are updated every day for
>>>         active R
>>>         versions.  It's only when Uwe decides that a version is no
>>>         longer worth
>>>         active support that he stops doing updates, and it "freezes".  A
>>>         consequence of this is that the snapshots preserved in those
>>> older
>>>         directories are unlikely to match what someone who keeps up to
>>>         date with
>>>         R releases is using.  Their purpose is to make sure that those
>>> older
>>>         versions aren't completely useless, but they aren't what
>>> Jeroen was
>>>         asking for.
>>>
>>>
>>>     But it is almost completely useless from a reproducibility point of
>>>     view to get random package versions. For example if some people try
>>>     to use R-2.13.2 today to reproduce an analysis that was published
>>>     2 years ago, they'll get Matrix 1.0-4 on Windows, Matrix 1.0-3 on
>>> Mac,
>>>     and Matrix 1.1-2-2 on Unix.
>
> Not true, since Matrix 1.1-2-2 has
>
> Depends:     R (? 2.15.2)

OK. So that means Matrix is not available today for R-2.13.2 users on
Linux:

   > "Matrix" %in% rownames(available.packages()[ , ])
   [1] FALSE

However since Matrix is a recommended package, it's included in
the official R-2.13.2 source tarball so it gets installed when I
install R:

   > installed.packages()["Matrix", "Version", drop=FALSE]
          Version
   Matrix "0.9996875-3"

As I mentioned earlier, the Matrix package was just an example. In the
case of a non-recommended package, it will either be:
   - unavailable by default (if the source package was removed or if
     the package maintainer consciously used the R >= x.y.z feature,
     e.g. the ape package),
   - or available but incompatible (e.g. bcrm is broken with R-2.13.2
     on Linux),
   - or available and compatible, but with a very different version
     than the version that was available 2 years ago (e.g. BaSTA),
   - or available and at the exact same version as 2 years ago (bingo!)

This is a very painful experience for anybody trying to install and
use R-2.13.2 today to reproduce 2-year old results. Things could be
improved a lot with very little changes.

Cheers,
H.

   > sessionInfo()
   R version 2.13.2 (2011-09-30)
   Platform: x86_64-unknown-linux-gnu (64-bit)

   locale:
    [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
    [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
    [5] LC_MONETARY=C              LC_MESSAGES=en_US.UTF-8
    [7] LC_PAPER=en_US.UTF-8       LC_NAME=C
    [9] LC_ADDRESS=C               LC_TELEPHONE=C
   [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C

   attached base packages:
   [1] stats     graphics  grDevices utils     datasets  methods   base

   loaded via a namespace (and not attached):
   [1] tools_2.13.2

>
>
> Best,
> Uwe Ligges
>
>
>   And none of them of course is what was
>>> used
>>>     by the authors of the paper (they used Matrix 1.0-1, which is what
>>> was
>>>     current when they ran their analysis).
>>>
>>> Initially this discussion brought back nightmares of DLL hell on
>>> Windows.  Those as ancient as I will remember that well.  But now, the
>>> focus seems to be on reproducibility, but with what strikes me as a
>>> seriously flawed notion of what reproducibility means.
>>>
>>> Herve Pages mentions the risk of irreproducibility across three minor
>>> revisions of version 1.0 of Matrix.
>>
>> If you use R-2.13.2, you get Matrix 1.1-2-2 on Linux. AFAIK this is
>> the most recent version of Matrix, aimed to be compatible with the most
>> current version of R (i.e. R 3.0.3). However, it has never been tested
>> with R-2.13.2. I'm not saying that it should, that would be a big waste
>> of resources of course. All I'm saying it that it doesn't make sense to
>> serve by default a version that is known to be incompatible with the
>> version of R being used. It's very likely to not even install properly.
>>
>> For the apparently small differences between the versions you get on
>> Windows and Mac, the Matrix package was just an example. With other
>> packages you get (again if you use R-2.13.2):
>>
>>                src   win    mac
>>    abc         1.8   1.5    1.4
>>    ape       3.1-1 3.0-1    2.8
>>    BaSTA     1.9.3   1.1    1.0
>>    bcrm      0.4.3   0.2    0.1
>>    BMA    3.16.2.3  3.15 3.14.1
>>    Boruta    3.0.0   1.6    1.5
>>    ...
>>
>> Are the differences big enough?
>>
>> Also note that back in October 2011, people using R-2.13.2 would get
>> e.g. ape 2.7-3 on Linux, Windows and Mac. Wouldn't it make sense that
>> people using R-2.13.2 today get the same? Why would anybody use
>> R-2.13.2 today if it's not to run again some code that was written
>> and used two years ago to obtain some important results?
>>
>> Cheers,
>> H.
>>
>>
>>> My gut reaction would be that if
>>> the results are not reproducible across such minor revisions of one
>>> library, they are probably just so much BS.  I am trained in
>>> mathematical ecology, with more than a couple decades of post-doc
>>> experience working with risk assessment in the private sector.  When I
>>> need to do an analysis, I will repeat it myself in multiple products, as
>>> well as C++ or FORTRAN code I have hand-crafted myself (and when I wrote
>>> number crunching code myself, I would do so in multiple programming
>>> languages - C++, Java, FORTRAN, applying rigorous QA procedures to each
>>> program/library I developed).  Back when I was a grad student, I would
>>> not even show the results to my supervisor, let alone try to publish
>>> them, unless the results were reproducible across ALL the tools I used.
>>> If there was a discrepancy, I would debug that before discussing them
>>> with anyone.  Surely, it is the responsibility of the journals' editors
>>> and reviewers to apply a similar practice.
>>>
>>> The concept of reproducibility used to this point in this discussion
>>> might be adequate from a programmers perspective (except in my lab), it
>>> is wholly inadequate from a scientist's perspective.  I maintain that if
>>> you have the original data, and repeat the analysis using the latest
>>> version of R and the available, relevant packages, the original results
>>> are probably due to a bug either in the R script or in R or the packages
>>> used IF the results obtained using the latest versions of these are not
>>> consistent with the originally reported results.  Therefore, of the
>>> concerns I see raised in this discussion, the principle one of concern
>>> is that of package developers who fail to pay sufficient attention to
>>> backwards compatibility: a new version ought not break any code that
>>> executes fine using previous versions.  That is not a trivial task, and
>>> may require contributors obtaining the assistance of a software
>>> engineer.  I am sure anyone in this list who programs in C++ knows how
>>> the ANSI committees handle change management.  Introduction of new
>>> features is something that is largely irrelevant for backwards
>>> compatibility (but there are exceptions), but features to be removed
>>> are handled by declaring them deprecated, and leaving them in that
>>> condition for years.  That tells anyone using the language that they
>>> ought to plan to adapt their code to work when the deprecated feature is
>>> finally removed.
>>>
>>> I am responsible for maintaining code (involving distributed computing)
>>> to which many companies integrate their systems, and I am careful to
>>> ensure that no change I make breaks their integration into my system,
>>> even though I often have to add new features.  And I don't add features
>>> lightly, and have yet to remove features.  When that eventually happens,
>>> the old feature will be deprecated, so that the other companies have
>>> plenty of time to adapt their integration code.  I do not know whether
>>> CRAN ought to have any responsibility for this sort of change
>>> management, or if they have assumed some responsibility for some of it,
>>> but I would argue that the package developers have the primary
>>> responsibility for doing this right.
>>>
>>> Just my $0.05 (the penny no longer exists in Canada)
>>>
>>> Cheers
>>>
>>> Ted
>>> R.E. (Ted) Byers, Ph.D., Ed.D.
>>

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fhcrc.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From cgenolin at u-paris10.fr  Fri Mar 21 01:02:05 2014
From: cgenolin at u-paris10.fr (Christophe Genolini)
Date: Fri, 21 Mar 2014 01:02:05 +0100
Subject: [Rd] Memcheck: error in a switch using getGraphicsEvent
Message-ID: <532B817D.5070606@u-paris10.fr>

Hi the list,

One of my package has an (other) error detected by memtest that I do not manage to understand.
Here is the message that I get from Memtest

--- 8< ----------------
 > try(choice(cld1))
Error in switch(EXPR = choix, Up = { : EXPR must be a length 1 vector
--- 8< ----------------

The choice function does call the choiceChangeParam function, which is:

--- 8< ----------------
choiceChangeParam <- function(paramChoice){

     texte <- paste("     ~ Choice : menu    ~\n",sep="")

     choix <- getGraphicsEvent(texte,onKeybd=function(key){return(key)})
     switch(EXPR=choix,
            "Up"    = {
                if(xy[1]>1){
                    paramChoice['toDo'] <- "xy"
                    xy[2]<-1
                    xy[1]<-xy[1]-1
                    paramChoice['xy']<-xy
                }else{paramChoice['toDo'] <- ""}
            },
            "Down"  = {
                if(xy[1]<nrow(paramChoice['critMatrix'])){
                    paramChoice['toDo'] <- "xy"
                    xy[2]<-1
                    xy[1]<-xy[1]+1
                    paramChoice['xy']<-xy
            "d" = {
                paramChoice['toDo'] <- "changeCriterion"
                paramChoice['critRank'] <- (paramChoice['critRank']%%length(CRITERION_NAMES)) + 1
            },
            "c" = {
                paramChoice['toDo'] <- "order"
            },
            default={}

            )
     return(paramChoice)
}
--- 8< ----------------

choix is a character of lenght 1 since getGraphicsEvent return a character, am I wrong?

-- 
Christophe Genolini
Ma?tre de conf?rences en bio-statistique
Universit? Paris Ouest Nanterre La D?fense
INSERM UMR 1027


From murdoch.duncan at gmail.com  Fri Mar 21 01:45:28 2014
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Thu, 20 Mar 2014 20:45:28 -0400
Subject: [Rd] Memcheck: error in a switch using getGraphicsEvent
In-Reply-To: <532B817D.5070606@u-paris10.fr>
References: <532B817D.5070606@u-paris10.fr>
Message-ID: <532B8BA8.7070201@gmail.com>

On 2014-03-20, 8:02 PM, Christophe Genolini wrote:
> Hi the list,
>
> One of my package has an (other) error detected by memtest that I do not manage to understand.
> Here is the message that I get from Memtest
>
> --- 8< ----------------
>   > try(choice(cld1))
> Error in switch(EXPR = choix, Up = { : EXPR must be a length 1 vector
> --- 8< ----------------
>
> The choice function does call the choiceChangeParam function, which is:
>
> --- 8< ----------------
> choiceChangeParam <- function(paramChoice){
>
>       texte <- paste("     ~ Choice : menu    ~\n",sep="")
>
>       choix <- getGraphicsEvent(texte,onKeybd=function(key){return(key)})
>       switch(EXPR=choix,
>              "Up"    = {
>                  if(xy[1]>1){
>                      paramChoice['toDo'] <- "xy"
>                      xy[2]<-1
>                      xy[1]<-xy[1]-1
>                      paramChoice['xy']<-xy
>                  }else{paramChoice['toDo'] <- ""}
>              },
>              "Down"  = {
>                  if(xy[1]<nrow(paramChoice['critMatrix'])){
>                      paramChoice['toDo'] <- "xy"
>                      xy[2]<-1
>                      xy[1]<-xy[1]+1
>                      paramChoice['xy']<-xy
>              "d" = {
>                  paramChoice['toDo'] <- "changeCriterion"
>                  paramChoice['critRank'] <- (paramChoice['critRank']%%length(CRITERION_NAMES)) + 1
>              },
>              "c" = {
>                  paramChoice['toDo'] <- "order"
>              },
>              default={}
>
>              )
>       return(paramChoice)
> }
> --- 8< ----------------
>
> choix is a character of lenght 1 since getGraphicsEvent return a character, am I wrong?
>

It can also return NULL, but to be sure, why not print the value?

Duncan Murdoch


From csardi.gabor at gmail.com  Fri Mar 21 02:23:33 2014
From: csardi.gabor at gmail.com (=?ISO-8859-1?B?R+Fib3IgQ3PhcmRp?=)
Date: Thu, 20 Mar 2014 21:23:33 -0400
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <532B798F.2040905@fhcrc.org>
References: <129902082.964468.1395296156508.JavaMail.root@fhcrc.org>
	<532AC885.1090305@gmail.com> <532B3E0A.4030907@fhcrc.org>
	<CAOTG1hVzjPBhw8yZ5egftKf6dBXKzgUZ_xDBw7fMz+ASk10xTQ@mail.gmail.com>
	<532B6A7D.3070800@fhcrc.org>
	<532B6BDA.8070006@statistik.tu-dortmund.de>
	<532B798F.2040905@fhcrc.org>
Message-ID: <CABtg=KkDOtXFhq5LgS0ZGnhe1wyNwAUSDcPWB8czF8YLPPFhkA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140320/713e1bca/attachment.pl>

From wdunlap at tibco.com  Fri Mar 21 02:45:59 2014
From: wdunlap at tibco.com (William Dunlap)
Date: Fri, 21 Mar 2014 01:45:59 +0000
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <CABtg=KkDOtXFhq5LgS0ZGnhe1wyNwAUSDcPWB8czF8YLPPFhkA@mail.gmail.com>
References: <129902082.964468.1395296156508.JavaMail.root@fhcrc.org>
	<532AC885.1090305@gmail.com> <532B3E0A.4030907@fhcrc.org>
	<CAOTG1hVzjPBhw8yZ5egftKf6dBXKzgUZ_xDBw7fMz+ASk10xTQ@mail.gmail.com>
	<532B6A7D.3070800@fhcrc.org>	<532B6BDA.8070006@statistik.tu-dortmund.de>
	<532B798F.2040905@fhcrc.org>
	<CABtg=KkDOtXFhq5LgS0ZGnhe1wyNwAUSDcPWB8czF8YLPPFhkA@mail.gmail.com>
Message-ID: <E66794E69CFDE04D9A70842786030B933FA94EED@PA-MBX01.na.tibco.com>

> In particular, updating a package with many reverse dependencies is a
> frustrating process, for everybody. As a maintainer with ~150 reverse
> dependencies, I think not twice, but ten times if I really want to publish
> a new version on CRAN.

It might be easier if more of those packages came with good test suites.

Bill Dunlap
TIBCO Software
wdunlap tibco.com


> -----Original Message-----
> From: r-devel-bounces at r-project.org [mailto:r-devel-bounces at r-project.org] On Behalf
> Of G?bor Cs?rdi
> Sent: Thursday, March 20, 2014 6:24 PM
> To: r-devel
> Subject: Re: [Rd] [RFC] A case for freezing CRAN
> 
> Much of the discussion was about reproducibility so far. Let me emphasize
> another point from Jeroen's proposal.
> 
> This is hard to measure of course, but I think I can say that the existence
> and the quality of CRAN and its packages contributed immensely to the
> success of R and the success of people using R. Having one central, well
> controlled and tested package repository is a huge advantage for the users.
> (I know that there are other repositories, but they are either similarly
> well controlled and specialized (BioC), or less used.) It would be great to
> keep it like this.
> 
> I also think that the current CRAN policy is not ideal for further growth.
> In particular, updating a package with many reverse dependencies is a
> frustrating process, for everybody. As a maintainer with ~150 reverse
> dependencies, I think not twice, but ten times if I really want to publish
> a new version on CRAN. I cannot speak for other maintainers of course, but
> I have a feeling that I am not alone.
> 
> Tying CRAN packages to R releases would help, because then I would not have
> to worry about breaking packages in the stable version of CRAN, only in
> CRAN-devel.
> 
> Somebody mentioned that it is good not to do this because then users get
> bug fixes and new features earlier. Well, in my case, the opposite it true.
> As I am not updating, they actually get it (much) later. If it wasn't such
> a hassle, I would definitely update more often, about once a month. Now my
> goal is more like once a year.
> 
> Again, I cannot speak for others, but I believe the current policy does not
> help progress, and is not sustainable in the long run. It penalizes the
> maintainers of "more important" (= many rev. dependencies, that is, which
> probably also means many users) packages, and I fear they will slowly move
> away from CRAN. I don't think this is what anybody in the R community would
> want.
> 
> Best,
> Gabor
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From csardi.gabor at gmail.com  Fri Mar 21 03:15:59 2014
From: csardi.gabor at gmail.com (=?ISO-8859-1?B?R+Fib3IgQ3PhcmRp?=)
Date: Thu, 20 Mar 2014 22:15:59 -0400
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <E66794E69CFDE04D9A70842786030B933FA94EED@PA-MBX01.na.tibco.com>
References: <129902082.964468.1395296156508.JavaMail.root@fhcrc.org>
	<532AC885.1090305@gmail.com> <532B3E0A.4030907@fhcrc.org>
	<CAOTG1hVzjPBhw8yZ5egftKf6dBXKzgUZ_xDBw7fMz+ASk10xTQ@mail.gmail.com>
	<532B6A7D.3070800@fhcrc.org>
	<532B6BDA.8070006@statistik.tu-dortmund.de>
	<532B798F.2040905@fhcrc.org>
	<CABtg=KkDOtXFhq5LgS0ZGnhe1wyNwAUSDcPWB8czF8YLPPFhkA@mail.gmail.com>
	<E66794E69CFDE04D9A70842786030B933FA94EED@PA-MBX01.na.tibco.com>
Message-ID: <CABtg=K=HozOjgCFJVQ4=CF1L2seZi74-=8FuaYtVn+5y7v01cQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140320/39e404b2/attachment.pl>

From tim.triche at gmail.com  Fri Mar 21 03:19:52 2014
From: tim.triche at gmail.com (Tim Triche, Jr.)
Date: Thu, 20 Mar 2014 19:19:52 -0700
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <CABtg=K=HozOjgCFJVQ4=CF1L2seZi74-=8FuaYtVn+5y7v01cQ@mail.gmail.com>
References: <129902082.964468.1395296156508.JavaMail.root@fhcrc.org>
	<532AC885.1090305@gmail.com> <532B3E0A.4030907@fhcrc.org>
	<CAOTG1hVzjPBhw8yZ5egftKf6dBXKzgUZ_xDBw7fMz+ASk10xTQ@mail.gmail.com>
	<532B6A7D.3070800@fhcrc.org>
	<532B6BDA.8070006@statistik.tu-dortmund.de>
	<532B798F.2040905@fhcrc.org>
	<CABtg=KkDOtXFhq5LgS0ZGnhe1wyNwAUSDcPWB8czF8YLPPFhkA@mail.gmail.com>
	<E66794E69CFDE04D9A70842786030B933FA94EED@PA-MBX01.na.tibco.com>
	<CABtg=K=HozOjgCFJVQ4=CF1L2seZi74-=8FuaYtVn+5y7v01cQ@mail.gmail.com>
Message-ID: <52CA86D9-E784-48DC-ACFB-8E286F1D0BFF@gmail.com>

Heh, you just described BioC

--t

> On Mar 20, 2014, at 7:15 PM, G?bor Cs?rdi <csardi.gabor at gmail.com> wrote:
> 
> On Thu, Mar 20, 2014 at 9:45 PM, William Dunlap <wdunlap at tibco.com> wrote:
> 
>>> In particular, updating a package with many reverse dependencies is a
>>> frustrating process, for everybody. As a maintainer with ~150 reverse
>>> dependencies, I think not twice, but ten times if I really want to
>> publish
>>> a new version on CRAN.
>> 
>> It might be easier if more of those packages came with good test suites.
> 
> Test suites are great, but I don't think this would make my job easier.
> More tests means more potential breakage. The extreme of not having any
> examples and tests in these 150 packages would be the easiest for _me_,
> actually. Not for the users, though.....
> 
> What would really help is either fully versioned package dependencies
> (daydreaming here), or having a CRAN-devel repository, that changes and
> might break often, and a CRAN-stable that does not change (much).
> 
> Gabor
> 
> [...]
> 
>    [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From tim.triche at gmail.com  Fri Mar 21 03:20:44 2014
From: tim.triche at gmail.com (Tim Triche, Jr.)
Date: Thu, 20 Mar 2014 19:20:44 -0700
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <CABtg=K=HozOjgCFJVQ4=CF1L2seZi74-=8FuaYtVn+5y7v01cQ@mail.gmail.com>
References: <129902082.964468.1395296156508.JavaMail.root@fhcrc.org>
	<532AC885.1090305@gmail.com> <532B3E0A.4030907@fhcrc.org>
	<CAOTG1hVzjPBhw8yZ5egftKf6dBXKzgUZ_xDBw7fMz+ASk10xTQ@mail.gmail.com>
	<532B6A7D.3070800@fhcrc.org>
	<532B6BDA.8070006@statistik.tu-dortmund.de>
	<532B798F.2040905@fhcrc.org>
	<CABtg=KkDOtXFhq5LgS0ZGnhe1wyNwAUSDcPWB8czF8YLPPFhkA@mail.gmail.com>
	<E66794E69CFDE04D9A70842786030B933FA94EED@PA-MBX01.na.tibco.com>
	<CABtg=K=HozOjgCFJVQ4=CF1L2seZi74-=8FuaYtVn+5y7v01cQ@mail.gmail.com>
Message-ID: <7666D0E2-F675-4DC1-BE9B-670B6FDF52FF@gmail.com>

Except that tests (as vignettes) are mandatory for BioC. So if something blows up you hear about it right quick :-)

--t

> On Mar 20, 2014, at 7:15 PM, G?bor Cs?rdi <csardi.gabor at gmail.com> wrote:
> 
> On Thu, Mar 20, 2014 at 9:45 PM, William Dunlap <wdunlap at tibco.com> wrote:
> 
>>> In particular, updating a package with many reverse dependencies is a
>>> frustrating process, for everybody. As a maintainer with ~150 reverse
>>> dependencies, I think not twice, but ten times if I really want to
>> publish
>>> a new version on CRAN.
>> 
>> It might be easier if more of those packages came with good test suites.
> 
> Test suites are great, but I don't think this would make my job easier.
> More tests means more potential breakage. The extreme of not having any
> examples and tests in these 150 packages would be the easiest for _me_,
> actually. Not for the users, though.....
> 
> What would really help is either fully versioned package dependencies
> (daydreaming here), or having a CRAN-devel repository, that changes and
> might break often, and a CRAN-stable that does not change (much).
> 
> Gabor
> 
> [...]
> 
>    [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From dtenenba at fhcrc.org  Fri Mar 21 03:48:30 2014
From: dtenenba at fhcrc.org (Dan Tenenbaum)
Date: Thu, 20 Mar 2014 19:48:30 -0700 (PDT)
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <CABtg=KkDOtXFhq5LgS0ZGnhe1wyNwAUSDcPWB8czF8YLPPFhkA@mail.gmail.com>
Message-ID: <2010964152.996598.1395370110737.JavaMail.root@fhcrc.org>



----- Original Message -----
> From: "G?bor Cs?rdi" <csardi.gabor at gmail.com>
> To: "r-devel" <r-devel at r-project.org>
> Sent: Thursday, March 20, 2014 6:23:33 PM
> Subject: Re: [Rd] [RFC] A case for freezing CRAN
> 
> Much of the discussion was about reproducibility so far. Let me
> emphasize
> another point from Jeroen's proposal.
> 
> This is hard to measure of course, but I think I can say that the
> existence
> and the quality of CRAN and its packages contributed immensely to the
> success of R and the success of people using R. Having one central,
> well
> controlled and tested package repository is a huge advantage for the
> users.
> (I know that there are other repositories, but they are either
> similarly
> well controlled and specialized (BioC), or less used.) It would be
> great to
> keep it like this.
> 
> I also think that the current CRAN policy is not ideal for further
> growth.
> In particular, updating a package with many reverse dependencies is a
> frustrating process, for everybody. As a maintainer with ~150 reverse
> dependencies, I think not twice, but ten times if I really want to
> publish
> a new version on CRAN. I cannot speak for other maintainers of
> course, but
> I have a feeling that I am not alone.
> 
> Tying CRAN packages to R releases would help, because then I would
> not have
> to worry about breaking packages in the stable version of CRAN, only
> in
> CRAN-devel.
> 
> Somebody mentioned that it is good not to do this because then users
> get
> bug fixes and new features earlier. Well, in my case, the opposite it
> true.
> As I am not updating, they actually get it (much) later. If it wasn't
> such
> a hassle, I would definitely update more often, about once a month.
> Now my
> goal is more like once a year.
> 

These are good points. Not only do maintainers think twice (or more) before updating packages but it also seems that there are CRAN policies that discourage frequent updates. Whereas Bioconductor welcomes frequent updates because they usually fix problems and help us understand interoperability/dependency issues. Probably the main reason for this difference is the existence of a devel branch where breakage can happen and it's not the end of the world.





> Again, I cannot speak for others, but I believe the current policy
> does not
> help progress, and is not sustainable in the long run. It penalizes
> the
> maintainers of "more important" (= many rev. dependencies, that is,
> which
> probably also means many users) packages, and I fear they will slowly
> move
> away from CRAN. I don't think this is what anybody in the R community
> would
> want.
> 
> Best,
> Gabor
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 


From jari.oksanen at oulu.fi  Fri Mar 21 09:33:32 2014
From: jari.oksanen at oulu.fi (Jari Oksanen)
Date: Fri, 21 Mar 2014 08:33:32 +0000
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <532B798F.2040905@fhcrc.org>
References: <129902082.964468.1395296156508.JavaMail.root@fhcrc.org>
	<532AC885.1090305@gmail.com>	<532B3E0A.4030907@fhcrc.org>
	<CAOTG1hVzjPBhw8yZ5egftKf6dBXKzgUZ_xDBw7fMz+ASk10xTQ@mail.gmail.com>
	<532B6A7D.3070800@fhcrc.org>	<532B6BDA.8070006@statistik.tu-dortmund.de>
	<532B798F.2040905@fhcrc.org>
Message-ID: <DFC5E4AD-36F9-424A-BF4D-307ABF093A6F@oulu.fi>

Freezing CRAN solves no problem of reproducibility. If you know the sessionInfo() or the version of R, the packages used and their versions, you can reproduce that set up. If you do not know, then you cannot. You can try guess: source code of old release versions of R and old packages are in CRAN archive, and these files have dates. So you can collect a snapshot of R and packages for a given date. This is not an ideal solution, but it is the same level of reproducibility that you get with strictly frozen CRAN. CRAN is no the sole source of packages, and even with strictly frozen CRAN the users may have used packages from other source. I am sure that if CRAN would be frozen (but I assume it happens the same day hell freezes), people would increasingly often use other package sources than CRAN. The choice is easy if the alternatives are to wait for the next year for the bug fix release, or do the analysis now and use package versions in R-Forge or github. Then you could not assume that frozen CRAN packages were used.

CRAN policy is not made in this mailing list, and CRAN maintainers are so silent that it hurts ears. However, I hope they won't freeze CRAN. 

Strict reproduction seems to be harder than I first imagined: ./configure && make really failed for R 2.14.1 and older in my office desktop. To reproduce older analysis, I would also need to install older tool sets (I suspect gfortran and cairo libraries).

CRAN is one source of R packages, and certainly its policy does not suit all developers. There is no policy that suits all.  Frozen CRAN would suit some, but certainly would deter some others. 

There seems to a common sentiment here that the only reason anybody would use R older than 3.0.3 is to reproduce old results. My experience form the Real Life(?) is that many of us use computers that we do not own, but they are the property of our employer. This may mean that we are not allowed to install there any software or we have to pay, or the Department of project has to pay, to the computer administration for installing new versions of software (our case). This is often called security. Personally I avoid this by using Mac laptop and Linux desktop: these are not supported by the University computer administration and I can do what I please with these, but poor Windows users are stuck. Computer classes are also maintained by centralized computer administration. This January they had new R, but last year it was still two years old. However, users can install packages in their personal "folders" so that they can use current packages even with older R. Therefore I want to take care that the packages I maintain also run in older R. Therefore I also applaud the current CRAN policy where new versions of packages are "backported" to previous R release: Even if you are stuck with stale R, you need not be stuck with stale packages. Currently I cannot test with older R than 2.14.2, though, but I do that regularly and certainly before CRAN releases.  If somebody wants to prevent this, they can set their package to unnecessarily depend on the current version of R. I would regard this as antisocial, but nobody would ask what I think about this so it does not matter.

The development branch of my package is in R-Forge, and only bug fixes and (hopefully) non-breaking enhancements (isolated so that they do not influence other functions, safe so that API does not change or  format of the output does not change) are merged to the CRAN release branch. This policy was adopted because it fits the current CRAN policy, and probably would need to change if CRAN policy changes.

Cheers, Jari Oksanen

From Rainer at krugs.de  Fri Mar 21 09:40:10 2014
From: Rainer at krugs.de (Rainer M Krug)
Date: Fri, 21 Mar 2014 09:40:10 +0100
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>
	(Jeroen Ooms's message of "Tue, 18 Mar 2014 13:24:46 -0700")
References: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>
Message-ID: <m2ha6s2bdh.fsf@krugs.de>



This is a long and (mainly) interesting discussion, which is fanning out
in many different directions, and I think many are not that relevant to
the OP's suggestion. 

I see the advantages of having such a dynamic CRAN, but also of having a
more stable CRAN. I prefer CRAN as it is now, but ion many cases a more
stable CRAN might b an advantage. So having releases of CRAN might make
sense. But then there is the archiving issue of CRAN.

The suggestion was made to move the responsibility away from CRAN and
the R infrastructure to the user / researcher to guarantee that the
results can be re-run years later. It would be nice to have this build
in CRAN, but let's stick at the scenario that the user should care for
reproducability.

Leaving the issue of compilation out, a package which is creating a
custom installation of the R version which includes the source of the R
version used and the sources of the packages in a on Linux compilable
format, given that the relevant dependencies are installed, would be a
huge step forward. 

I know - compilation on Windows (and sometimes Mac) is a serious
problem), but to archive *all* binaries and to re-compile all older
versions of R and all packages would be an impossible task.

Apart from that - doing your analysis in a Virtual Machine and then
simply archiving this Virtual Machine, would also be an option, but only
for the more tech savy users.

In a nutshell: I think a package would be able to provide the solution
for a local archiving to make it possible to re-run the simulation with
the same tools at a later stage - although guarantees would not be
possible.

Cheers,

Rainer
-- 
Rainer M. Krug
email: Rainer<at>krugs<dot>de
PGP: 0x0F52F982


From Rainer at krugs.de  Fri Mar 21 09:49:31 2014
From: Rainer at krugs.de (Rainer M Krug)
Date: Fri, 21 Mar 2014 09:49:31 +0100
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <DFC5E4AD-36F9-424A-BF4D-307ABF093A6F@oulu.fi> (Jari Oksanen's
	message of "Fri, 21 Mar 2014 08:33:32 +0000")
References: <129902082.964468.1395296156508.JavaMail.root@fhcrc.org>
	<532AC885.1090305@gmail.com> <532B3E0A.4030907@fhcrc.org>
	<CAOTG1hVzjPBhw8yZ5egftKf6dBXKzgUZ_xDBw7fMz+ASk10xTQ@mail.gmail.com>
	<532B6A7D.3070800@fhcrc.org>
	<532B6BDA.8070006@statistik.tu-dortmund.de>
	<532B798F.2040905@fhcrc.org>
	<DFC5E4AD-36F9-424A-BF4D-307ABF093A6F@oulu.fi>
Message-ID: <m28us42axw.fsf@krugs.de>

Jari Oksanen <jari.oksanen at oulu.fi> writes:

> Freezing CRAN solves no problem of reproducibility. If you know the
> sessionInfo() or the version of R, the packages used and their
> versions, you can reproduce that set up. If you do not know, then you
> cannot. You can try guess: source code of old release versions of R
> and old packages are in CRAN archive, and these files have dates. So
> you can collect a snapshot of R and packages for a given date. This is
> not an ideal solution, but it is the same level of reproducibility
> that you get with strictly frozen CRAN. CRAN is no the sole source of
> packages, and even with strictly frozen CRAN the users may have used
> packages from other source. I am sure that if CRAN would be frozen
> (but I assume it happens the same day hell freezes), people would
> increasingly often use other package sources than CRAN. The choice is
> easy if the alternatives are to wait for the next year for the bug fix
> release, or do the analysis now and use package versions in R-Forge or
> github. Then you could not assume that frozen CRAN packages were used.

Agree completely here - the solution would be a package, which is
packaging the source (or even binaries?) of your local R setup including
R and packages used. The solution is local - not on a server.

>
> CRAN policy is not made in this mailing list, and CRAN maintainers are
> so silent that it hurts ears. 

+1

> However, I hope they won't freeze CRAN.

Yes and no - if they do, we need a devel branch which acts like the
current CRAN.

>
> Strict reproduction seems to be harder than I first imagined:
> ./configure && make really failed for R 2.14.1 and older in my office
> desktop. To reproduce older analysis, I would also need to install
> older tool sets (I suspect gfortran and cairo libraries).

Absolutely - let's not go there. And then there is also the hardware
issue.

>
> CRAN is one source of R packages, and certainly its policy does not
> suit all developers. There is no policy that suits all.  Frozen CRAN
> would suit some, but certainly would deter some others.
>
> There seems to a common sentiment here that the only reason anybody
> would use R older than 3.0.3 is to reproduce old results. My
> experience form the Real Life(?) is that many of us use computers that
> we do not own, but they are the property of our employer. This may
> mean that we are not allowed to install there any software or we have
> to pay, or the Department of project has to pay, to the computer
> administration for installing new versions of software (our
> case).  

> This is often called security. Personally I avoid this by using
> Mac laptop and Linux desktop: these are not supported by the
> University computer administration and I can do what I please with
> these, but poor Windows users are stuck. 

Nicely put.

> Computer classes are also
> maintained by centralized computer administration. This January they
> had new R, but last year it was still two years old. However, users
> can install packages in their personal "folders" so that they can use
> current packages even with older R. Therefore I want to take care that
> the packages I maintain also run in older R. Therefore I also applaud
> the current CRAN policy where new versions of packages are
> "backported" to previous R release: Even if you are stuck with stale
> R, you need not be stuck with stale packages. Currently I cannot test
> with older R than 2.14.2, though, but I do that regularly and
> certainly before CRAN releases.  If somebody wants to prevent this,
> they can set their package to unnecessarily depend on the current
> version of R. I would regard this as antisocial, but nobody would ask
> what I think about this so it does not matter.
>
> The development branch of my package is in R-Forge, and only bug fixes
> and (hopefully) non-breaking enhancements (isolated so that they do
> not influence other functions, safe so that API does not change or
> format of the output does not change) are merged to the CRAN release
> branch. This policy was adopted because it fits the current CRAN
> policy, and probably would need to change if CRAN policy changes.
>
> Cheers, Jari Oksanen

-- 
Rainer M. Krug
email: Rainer<at>krugs<dot>de
PGP: 0x0F52F982
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 494 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140321/89feccd6/attachment.bin>

From phgrosjean at sciviews.org  Fri Mar 21 10:06:27 2014
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Fri, 21 Mar 2014 10:06:27 +0100
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <CABFfbXtZaW-2jZOkiqt+VTGbEkA=VfOgSi3gMWvtD1WerV_uUQ@mail.gmail.com>
References: <129902082.964468.1395296156508.JavaMail.root@fhcrc.org>
	<532AC885.1090305@gmail.com> <532B3E0A.4030907@fhcrc.org>
	<CAOTG1hVzjPBhw8yZ5egftKf6dBXKzgUZ_xDBw7fMz+ASk10xTQ@mail.gmail.com>
	<CABFfbXtZaW-2jZOkiqt+VTGbEkA=VfOgSi3gMWvtD1WerV_uUQ@mail.gmail.com>
Message-ID: <A5D05CA8-E4B6-4FEE-902F-72D4B124F327@sciviews.org>

This is becoming an extremely long thread, and it is going in too many directions. However, I would like to mention here our ongoing five years projects ECOS project for the study of Open Source Ecosystems, among which, CRAN. You can find info here: http://informatique.umons.ac.be/genlog/projects/ecos/. We are in the second year now.

We are currently working on CRAN maintainability questions. See:

- Claes Maelick, Mens Tom, Grosjean Philippe, "On the maintainability of CRAN packages" in IEEE CSMR-WCRE 2014 Software Evolution Week, Antwerpen, Belgique, 2014 (2014)

- Mens Tom, Claes Maelick, Grosjean Philippe, Serebrenik Alexander, "Studying Evolving Software Ecosystems based on Ecological Models" in Mens Tom, Serebrenik Alexander, Cleve Anthony, "Evolving Software Systems" , Springer, Mens Tom, Serebrenik Alexander, Cleve Anthony, 978-3-642-45397-7 (2014)

Currently, we are building an Open Source system based on Virtualbox and Vagrant to recreate a virtual machine under Linux (Debian and Ubuntu considered for the moment) that would be as close as possible as a "simulated CRAN environment as it was at any given date". Our plans are to replay CRAN back in time and to instrumentize that platform to measure what we need for our ecological studies of CRAN.

The connection with this thread is the possibility to reuse this system for proposing something useful for reproducible research, that is, a reproducible platform, in the definition of reproducibility vs replicability Jeroen Ooms mentions. It would then be enough to record the date some R code was run on that platform (and perhaps whether it is 32 or 64 bit system) to be able to rebuild a similar software environment with all corresponding CRAN packages of the right version easily installable. In case something specific is required in addition to software proposed by default, Vagrant allows provisioning the Virtual machine in an easy way too? but then, the provisioning script must be provided too (not much a problem). Info required to rebuild the platform is shrunk down to a few kb Ascii text file. This is something easy to put together with your R code in, say, additional material of a publication. 

Please, keep in mind that many platform-specific features in R (graphic devices, string encoding, and many more) may be a problem too for reproducing published results. Hence, the idea to use a virtual box using only one OS, Linux, no matter if you work on Windows, or Mac OS X, or? Solaris (anyone there?).

PhG


On 20 Mar 2014, at 21:53, Jeroen Ooms <jeroen.ooms at stat.ucla.edu> wrote:

> On Thu, Mar 20, 2014 at 1:28 PM, Ted Byers <r.ted.byers at gmail.com> wrote:
>> 
>> Herve Pages mentions the risk of irreproducibility across three minor
>> revisions of version 1.0 of Matrix.  My gut reaction would be that if the
>> results are not reproducible across such minor revisions of one library,
>> they are probably just so much BS.
>> 
> 
> Perhaps this is just terminology, but what you refer to I would generally
> call 'replication'. Of course being able to replicate results with other
> data or other software is important to validate claims. But being able to
> reproduce how the original results were obtained is an important part of
> this process.
> 
> If someone is publishing results that I think are questionable and I cannot
> replicate them, I want to know exactly how those outcomes were obtained in
> the first place, so that I can 'debug' the problem. It's quite important to
> be able to trace back if incorrect results were a result of a bug,
> incompetence or fraud.
> 
> Let's take the example of the Reinhart and Rogoff case. The results
> obviously were not replicable, but without more information it was just the
> word of a grad students vs two Harvard professors. Only after reproducing
> the original analysis it was possible to point out the errors and proof
> that the original were incorrect.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 


From jari.oksanen at oulu.fi  Fri Mar 21 10:17:24 2014
From: jari.oksanen at oulu.fi (Jari Oksanen)
Date: Fri, 21 Mar 2014 09:17:24 +0000
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <m2ha6s2bdh.fsf@krugs.de>
References: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>
	<m2ha6s2bdh.fsf@krugs.de>
Message-ID: <05FA3CA2-56BC-436F-A30A-1D0E54EF27D5@oulu.fi>


On 21/03/2014, at 10:40 AM, Rainer M Krug wrote:

> 
> 
> This is a long and (mainly) interesting discussion, which is fanning out
> in many different directions, and I think many are not that relevant to
> the OP's suggestion. 
> 
> I see the advantages of having such a dynamic CRAN, but also of having a
> more stable CRAN. I prefer CRAN as it is now, but ion many cases a more
> stable CRAN might b an advantage. So having releases of CRAN might make
> sense. But then there is the archiving issue of CRAN.
> 
> The suggestion was made to move the responsibility away from CRAN and
> the R infrastructure to the user / researcher to guarantee that the
> results can be re-run years later. It would be nice to have this build
> in CRAN, but let's stick at the scenario that the user should care for
> reproducability.

There are two different problems that alternate in the discussion: reproducibility and breakage of CRAN dependencies. Frozen CRAN could make *approximate* reproducibility easier to achieve, but real reproducibility needs stricter solutions. Actual sessionInfo() is minimal information, but re-building a spitting image of old environment may still be demanding (but in many cases this does not matter). 

Another problem is that CRAN is so volatile that new versions of packages break other packages or old scripts. Here the main problem is how package developers work. Freezing CRAN would not change that: if package maintainers release breaking code, that would be frozen. I think that most packages do not make distinction between development and release branches, and CRAN policy won't change that. 

I can sympathize with package maintainers having 150 reverse dependencies. My main package only has ~50, and it is sure that I won't test them all with new release. I sometimes tried, but I could not even get all those built because they had other dependencies on packages that failed. Even those that I could test failed to detect problems (in one case all examples were \dontrun and passed nicely tests). I only wish that if people *really* depend on my package, they test it against R-Forge version and alert me before CRAN releases, but that is not very likely (I guess many dependencies are not *really* necessary, but only concern marginal features of the package, but CRAN forces to declare those). 

Still a few words about reproducibility of scripts: this can be hardly achieved with good coverage, because many scripts are so very ad hoc. When I edit and review manuscripts for journals, I very often get Sweave or knitr scripts that "just work", where "just" means "just so and so". Often they do not work at all, because they had some undeclared private functionalities or stray files in the author workspace that did not travel with the Sweave document. I think these -- published scientific papers -- are the main field where the code really should be reproducible, but they often are the hardest to reproduce. Nothing CRAN people do can help with sloppy code scientists write for publications. You know, they are scientists -- not engineers. 

Cheers, Jari Oksanen
> 
> Leaving the issue of compilation out, a package which is creating a
> custom installation of the R version which includes the source of the R
> version used and the sources of the packages in a on Linux compilable
> format, given that the relevant dependencies are installed, would be a
> huge step forward. 
> 
> I know - compilation on Windows (and sometimes Mac) is a serious
> problem), but to archive *all* binaries and to re-compile all older
> versions of R and all packages would be an impossible task.
> 
> Apart from that - doing your analysis in a Virtual Machine and then
> simply archiving this Virtual Machine, would also be an option, but only
> for the more tech savy users.
> 
> In a nutshell: I think a package would be able to provide the solution
> for a local archiving to make it possible to re-run the simulation with
> the same tools at a later stage - although guarantees would not be
> possible.
> 
> Cheers,
> 
> Rainer
> -- 
> Rainer M. Krug
> email: Rainer<at>krugs<dot>de
> PGP: 0x0F52F982
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From Rainer at krugs.de  Fri Mar 21 10:59:04 2014
From: Rainer at krugs.de (Rainer M Krug)
Date: Fri, 21 Mar 2014 10:59:04 +0100
Subject: [Rd] Docker versus Vagrant for reproducability - was: The case for
	freezing CRAN
In-Reply-To: <21290.60882.865159.407363@max.nulle.part> (Dirk Eddelbuettel's
	message of "Thu, 20 Mar 2014 08:32:02 -0500")
References: <3dfcdc$fuu13r@ironport9.mayo.edu>
	<21290.60882.865159.407363@max.nulle.part>
Message-ID: <m21txv3maf.fsf_-_@krugs.de>

Dirk Eddelbuettel <edd at debian.org> writes:

>  o Roger correctly notes that R scripts and packages are just one issue.
>    Compilers, libraries and the OS matter.  To me, the natural approach these
>    days would be to think of something based on Docker or Vagrant or (if you
>    must, VirtualBox).  The newer alternatives make snapshotting very cheap
>    (eg by using Linux LXC).  That approach reproduces a full environemnt as
>    best as we can while still ignoring the hardware layer (and some readers
>    may recall the infamous Pentium bug of two decades ago).

These two tools look very interesting - but I have, even after reading a
few discussions of their differences, no idea which one is better suited
to be used for what has been discussed here: Making it possible to run
the analysis later to reproduce results using the same versions used in
the initial analysis.

Am I right in saying:

- Vagrant uses VMs to emulate the hardware
- Docker does not

wherefore
- Vagrant is slower and requires more space
- Docker is faster and requires less space

Therefore, could one say that Vagrant is more "robust" in the long run?

How do they compare in relation to different platforms? Vagrant seems to
be platform agnostic, I can develop and run on Linux, Mac and Windows -
how does it work with Docker? 

I just followed [1] and setup Docker on OSX - loos promising - it also
uses an underlying VM. SO both should be equal in regards to
reproducability in the long run?

Please note: I see these questions in the light of this discussion of
reproducability and not in regards to deployment of applications what
the discussions on the web are.

Any comments, thoughts, remarks?

Rainer


Footnotes: 
[1]  http://docs.docker.io/en/latest/installation/mac/

-- 
Rainer M. Krug
email: Rainer<at>krugs<dot>de
PGP: 0x0F52F982
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 494 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140321/09332430/attachment.bin>

From Rainer at krugs.de  Fri Mar 21 11:08:46 2014
From: Rainer at krugs.de (Rainer M Krug)
Date: Fri, 21 Mar 2014 11:08:46 +0100
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <05FA3CA2-56BC-436F-A30A-1D0E54EF27D5@oulu.fi> (Jari Oksanen's
	message of "Fri, 21 Mar 2014 09:17:24 +0000")
References: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>
	<m2ha6s2bdh.fsf@krugs.de>
	<05FA3CA2-56BC-436F-A30A-1D0E54EF27D5@oulu.fi>
Message-ID: <m2wqfn279t.fsf@krugs.de>

Jari Oksanen <jari.oksanen at oulu.fi> writes:

> On 21/03/2014, at 10:40 AM, Rainer M Krug wrote:
>
>> 
>> 
>> This is a long and (mainly) interesting discussion, which is fanning out
>> in many different directions, and I think many are not that relevant to
>> the OP's suggestion. 
>> 
>> I see the advantages of having such a dynamic CRAN, but also of having a
>> more stable CRAN. I prefer CRAN as it is now, but ion many cases a more
>> stable CRAN might b an advantage. So having releases of CRAN might make
>> sense. But then there is the archiving issue of CRAN.
>> 
>> The suggestion was made to move the responsibility away from CRAN and
>> the R infrastructure to the user / researcher to guarantee that the
>> results can be re-run years later. It would be nice to have this build
>> in CRAN, but let's stick at the scenario that the user should care for
>> reproducability.
>
> There are two different problems that alternate in the discussion:
> reproducibility and breakage of CRAN dependencies. Frozen CRAN could
> make *approximate* reproducibility easier to achieve, but real
> reproducibility needs stricter solutions. Actual sessionInfo() is
> minimal information, but re-building a spitting image of old
> environment may still be demanding (but in many cases this does not
> matter).
>
> Another problem is that CRAN is so volatile that new versions of
> packages break other packages or old scripts. Here the main problem is
> how package developers work. Freezing CRAN would not change that: if
> package maintainers release breaking code, that would be frozen. I
> think that most packages do not make distinction between development
> and release branches, and CRAN policy won't change that.
>
> I can sympathize with package maintainers having 150 reverse
> dependencies. My main package only has ~50, and it is sure that I
> won't test them all with new release. I sometimes tried, but I could
> not even get all those built because they had other dependencies on
> packages that failed. Even those that I could test failed to detect
> problems (in one case all examples were \dontrun and passed nicely
> tests). I only wish that if people *really* depend on my package, they
> test it against R-Forge version and alert me before CRAN releases, but
> that is not very likely (I guess many dependencies are not *really*
> necessary, but only concern marginal features of the package, but CRAN
> forces to declare those).

Breakage of CRAN packages is a problem, to which I can not comment
much. I have no idea how this could be saved unless one introduces more
checks, which nobody wants. CRAN is a (more or less) open repository for
packages written by engineers / programmers but also scientists of other
fields - and that is the strength of CRAN - a central repository to find
packages which conform to a minimal standard and format. 

>
> Still a few words about reproducibility of scripts: this can be hardly
> achieved with good coverage, because many scripts are so very ad
> hoc. When I edit and review manuscripts for journals, I very often get
> Sweave or knitr scripts that "just work", where "just" means "just so
> and so". Often they do not work at all, because they had some
> undeclared private functionalities or stray files in the author
> workspace that did not travel with the Sweave document. 

One reason why I *always* start my R sessions --vanilla and ave a local
initialization script which I call manually. 

> I think these
> -- published scientific papers -- are the main field where the code
> really should be reproducible, but they often are the hardest to
> reproduce. 

And this is completely ouyt of the hands of R / CRAN / ... and in the
hand of Journals and Authors. But R could provide a framework to make
this more easy in form of a package which provides functions to make
this a one-command approach.

> Nothing CRAN people do can help with sloppy code scientists
> write for publications. You know, they are scientists -- not
> engineers.

Absolutely - and I am also a sloppy scientists - I put my code online,
but hope that not many people ask me later about it.

Cheers,

Rainer

>
> Cheers, Jari Oksanen
>> 
>> Leaving the issue of compilation out, a package which is creating a
>> custom installation of the R version which includes the source of the R
>> version used and the sources of the packages in a on Linux compilable
>> format, given that the relevant dependencies are installed, would be a
>> huge step forward. 
>> 
>> I know - compilation on Windows (and sometimes Mac) is a serious
>> problem), but to archive *all* binaries and to re-compile all older
>> versions of R and all packages would be an impossible task.
>> 
>> Apart from that - doing your analysis in a Virtual Machine and then
>> simply archiving this Virtual Machine, would also be an option, but only
>> for the more tech savy users.
>> 
>> In a nutshell: I think a package would be able to provide the solution
>> for a local archiving to make it possible to re-run the simulation with
>> the same tools at a later stage - although guarantees would not be
>> possible.
>> 
>> Cheers,
>> 
>> Rainer
>> -- 
>> Rainer M. Krug
>> email: Rainer<at>krugs<dot>de
>> PGP: 0x0F52F982
>> 
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>

-- 
Rainer M. Krug
email: Rainer<at>krugs<dot>de
PGP: 0x0F52F982
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 494 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140321/dc83421f/attachment.bin>

From cgenolin at u-paris10.fr  Fri Mar 21 11:47:37 2014
From: cgenolin at u-paris10.fr (Christophe Genolini)
Date: Fri, 21 Mar 2014 11:47:37 +0100
Subject: [Rd] Memcheck: error in a switch using getGraphicsEvent
In-Reply-To: <532B8BA8.7070201@gmail.com>
References: <532B817D.5070606@u-paris10.fr> <532B8BA8.7070201@gmail.com>
Message-ID: <532C18C9.1070405@u-paris10.fr>

Le 21/03/2014 01:45, Duncan Murdoch a ?crit :
> On 2014-03-20, 8:02 PM, Christophe Genolini wrote:
>> Hi the list,
>>
>> One of my package has an (other) error detected by memtest that I do not manage to understand.
>> Here is the message that I get from Memtest
>>
>> --- 8< ----------------
>>   > try(choice(cld1))
>> Error in switch(EXPR = choix, Up = { : EXPR must be a length 1 vector
>> --- 8< ----------------
>>
>> The choice function does call the choiceChangeParam function, which is:
>>
>> --- 8< ----------------
>> choiceChangeParam <- function(paramChoice){
>>
>>       texte <- paste("     ~ Choice : menu    ~\n",sep="")
>>
>>       choix <- getGraphicsEvent(texte,onKeybd=function(key){return(key)})
>>       switch(EXPR=choix,
>>              "Up"    = {
>>                  if(xy[1]>1){
>>                      paramChoice['toDo'] <- "xy"
>>                      xy[2]<-1
>>                      xy[1]<-xy[1]-1
>>                      paramChoice['xy']<-xy
>>                  }else{paramChoice['toDo'] <- ""}
>>              },
>>              "Down"  = {
>>                  if(xy[1]<nrow(paramChoice['critMatrix'])){
>>                      paramChoice['toDo'] <- "xy"
>>                      xy[2]<-1
>>                      xy[1]<-xy[1]+1
>>                      paramChoice['xy']<-xy
>>              "d" = {
>>                  paramChoice['toDo'] <- "changeCriterion"
>>                  paramChoice['critRank'] <- (paramChoice['critRank']%%length(CRITERION_NAMES)) + 1
>>              },
>>              "c" = {
>>                  paramChoice['toDo'] <- "order"
>>              },
>>              default={}
>>
>>              )
>>       return(paramChoice)
>> }
>> --- 8< ----------------
>>
>> choix is a character of lenght 1 since getGraphicsEvent return a character, am I wrong?
>>
>
> It can also return NULL, but to be sure, why not print the value?
>
> Duncan Murdoch
>
>
Thanks for your answer. According to your suggestion, I add the line "cat" between the two other lines:
--- 8< -----------------
     ....
     choix <- getGraphicsEvent(texte,onKeybd=function(key){return(key)})
     cat("Choix : ",choix," class :",class(choix)," length :",length(choix),"\n")
     switch(EXPR=choix,
     ....
--- 8< -----------------

I then make several attempts. In all the cases, the length of choix is 1:

--- 8< ------------------
     ~ Choice : menu ~
Choix :  Down  class : character  length : 1
      ~ Choice : menu ~
Choix :  Up  class : character  length : 1
      ~ Choice : menu ~
Choix :  e  class : character  length : 1
      ~ Choice : menu ~
Choix :  ,  class : character  length : 1
      ~ Choice : menu ~
Choix :     class : character  length : 1
      ~ Choice : menu ~
Choix :  9  class : character  length : 1
      ~ Choice : menu ~
Choix :  ctrl-I  class : character  length : 1
      ~ Choice : menu ~
Choix :  ?  class : character  length : 1
      ~ Choice : menu ~
Choix :  ctrl-J  class : character  length : 1
--- 8< ---------------------

-- 
Christophe Genolini
Ma?tre de conf?rences en bio-statistique
Universit? Paris Ouest Nanterre La D?fense
INSERM UMR 1027


From therneau at mayo.edu  Fri Mar 21 13:43:40 2014
From: therneau at mayo.edu (Therneau, Terry M., Ph.D.)
Date: Fri, 21 Mar 2014 07:43:40 -0500
Subject: [Rd] The case for freezing CRAN
Message-ID: <6e55ab$8imvjf@ironport10.mayo.edu>

This has been a fascinating discussion.

Carl Boettinger replied with a set of examples where the world is much more fragile than 
my examples.  That was useful.  It seems that people in my area (medical research and 
survival) are more careful with their packages (whew!).

Gabor Csardi discussed the problems with maintaining a package with lots of dependencies.
I maintain the survival package which currently has 246 reverse dependencies and take a 
slightly different view, which could be described as "the price of fame".  I feel a 
responsiblity to not break R.  I have automated scripts which download the latest copy of 
all 246, using the install-tests option, and run them all. Most updates have 1-3 issues.  
About 25% of the time it turns out to be a problem that I introduced, and in all the 
others I have found the other package authors to be responsive.  It is a nuisance, yes, 
but also worth it.  I've built the test scripts over several years, with help from several 
others; a place to share this information would be a useful addition.

This process also keeps me honest about any updates that are not backwards compatable.  
There is hardly a single option that is not used by some other package, somewhere.

Terry Therneau


From Philippe.GROSJEAN at umons.ac.be  Fri Mar 21 13:51:38 2014
From: Philippe.GROSJEAN at umons.ac.be (Philippe GROSJEAN)
Date: Fri, 21 Mar 2014 12:51:38 +0000
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <m2wqfn279t.fsf@krugs.de>
References: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>
	<m2ha6s2bdh.fsf@krugs.de>
	<05FA3CA2-56BC-436F-A30A-1D0E54EF27D5@oulu.fi>
	<m2wqfn279t.fsf@krugs.de>
Message-ID: <23E6A63A-B33C-48D9-929A-7C6A2CDDBDA3@umons.ac.be>


On 21 Mar 2014, at 11:08, Rainer M Krug <Rainer at krugs.de> wrote:

> Jari Oksanen <jari.oksanen at oulu.fi> writes:
> 
>> On 21/03/2014, at 10:40 AM, Rainer M Krug wrote:
>> 
>>> 
>>> 
>>> This is a long and (mainly) interesting discussion, which is fanning out
>>> in many different directions, and I think many are not that relevant to
>>> the OP's suggestion. 
>>> 
>>> I see the advantages of having such a dynamic CRAN, but also of having a
>>> more stable CRAN. I prefer CRAN as it is now, but ion many cases a more
>>> stable CRAN might b an advantage. So having releases of CRAN might make
>>> sense. But then there is the archiving issue of CRAN.
>>> 
>>> The suggestion was made to move the responsibility away from CRAN and
>>> the R infrastructure to the user / researcher to guarantee that the
>>> results can be re-run years later. It would be nice to have this build
>>> in CRAN, but let's stick at the scenario that the user should care for
>>> reproducability.
>> 
>> There are two different problems that alternate in the discussion:
>> reproducibility and breakage of CRAN dependencies. Frozen CRAN could
>> make *approximate* reproducibility easier to achieve, but real
>> reproducibility needs stricter solutions. Actual sessionInfo() is
>> minimal information, but re-building a spitting image of old
>> environment may still be demanding (but in many cases this does not
>> matter).
>> 
>> Another problem is that CRAN is so volatile that new versions of
>> packages break other packages or old scripts. Here the main problem is
>> how package developers work. Freezing CRAN would not change that: if
>> package maintainers release breaking code, that would be frozen. I
>> think that most packages do not make distinction between development
>> and release branches, and CRAN policy won't change that.
>> 
>> I can sympathize with package maintainers having 150 reverse
>> dependencies. My main package only has ~50, and it is sure that I
>> won't test them all with new release. I sometimes tried, but I could
>> not even get all those built because they had other dependencies on
>> packages that failed. Even those that I could test failed to detect
>> problems (in one case all examples were \dontrun and passed nicely
>> tests). I only wish that if people *really* depend on my package, they
>> test it against R-Forge version and alert me before CRAN releases, but
>> that is not very likely (I guess many dependencies are not *really*
>> necessary, but only concern marginal features of the package, but CRAN
>> forces to declare those).
> 
We work on these too. So far, for latest CRAN version, we have successfully installed 4999 packages among the 5321 CRAN package on our platform. Regarding conflicts in term of function names, around 2000 packages are clean, but the rest produce more than 11,000 pairs of conflicts (i.e., same function name in different packages). For dependency errors, look at the cited references earlier. It is strange that a large portion of R CMD check errors on CRAN occur and disappear *without any version update* of a package or any of its direct or indirect dependencies! That is, a fraction of errors or warnings seem to appear and disappear without any code update. We have traced back some of these to interaction with the net (e.g., example or vignette downloading data from a server and the server may be sometimes unavailable). So, yes, a complex and difficult topic.


> Breakage of CRAN packages is a problem, to which I can not comment
> much. I have no idea how this could be saved unless one introduces more
> checks, which nobody wants. CRAN is a (more or less) open repository for
> packages written by engineers / programmers but also scientists of other
> fields - and that is the strength of CRAN - a central repository to find
> packages which conform to a minimal standard and format. 
> 
>> 
>> Still a few words about reproducibility of scripts: this can be hardly
>> achieved with good coverage, because many scripts are so very ad
>> hoc. When I edit and review manuscripts for journals, I very often get
>> Sweave or knitr scripts that "just work", where "just" means "just so
>> and so". Often they do not work at all, because they had some
>> undeclared private functionalities or stray files in the author
>> workspace that did not travel with the Sweave document. 
> 
> One reason why I *always* start my R sessions --vanilla and ave a local
> initialization script which I call manually. 
> 
>> I think these
>> -- published scientific papers -- are the main field where the code
>> really should be reproducible, but they often are the hardest to
>> reproduce. 
> 
> And this is completely ouyt of the hands of R / CRAN / ... and in the
> hand of Journals and Authors. But R could provide a framework to make
> this more easy in form of a package which provides functions to make
> this a one-command approach.
> 
>> Nothing CRAN people do can help with sloppy code scientists
>> write for publications. You know, they are scientists -- not
>> engineers.
> 
This would be a first step. Then, people would have to learn how to use, say, Sweave, in order to ensure reproducibility. This begins to be enforced by journal editors or publishers (JSS, or Elsevier comes to mind).

Best,

Philippe


> Absolutely - and I am also a sloppy scientists - I put my code online,
> but hope that not many people ask me later about it.
> 
> Cheers,
> 
> Rainer
> 
>> 
>> Cheers, Jari Oksanen
>>> 
>>> Leaving the issue of compilation out, a package which is creating a
>>> custom installation of the R version which includes the source of the R
>>> version used and the sources of the packages in a on Linux compilable
>>> format, given that the relevant dependencies are installed, would be a
>>> huge step forward. 
>>> 
>>> I know - compilation on Windows (and sometimes Mac) is a serious
>>> problem), but to archive *all* binaries and to re-compile all older
>>> versions of R and all packages would be an impossible task.
>>> 
>>> Apart from that - doing your analysis in a Virtual Machine and then
>>> simply archiving this Virtual Machine, would also be an option, but only
>>> for the more tech savy users.
>>> 
>>> In a nutshell: I think a package would be able to provide the solution
>>> for a local archiving to make it possible to re-run the simulation with
>>> the same tools at a later stage - although guarantees would not be
>>> possible.
>>> 
>>> Cheers,
>>> 
>>> Rainer
>>> -- 
>>> Rainer M. Krug
>>> email: Rainer<at>krugs<dot>de
>>> PGP: 0x0F52F982
>>> 
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>> 
> 
> -- 
> Rainer M. Krug
> email: Rainer<at>krugs<dot>de
> PGP: 0x0F52F982
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From csardi.gabor at gmail.com  Fri Mar 21 14:19:46 2014
From: csardi.gabor at gmail.com (=?ISO-8859-1?B?R+Fib3IgQ3PhcmRp?=)
Date: Fri, 21 Mar 2014 09:19:46 -0400
Subject: [Rd] The case for freezing CRAN
In-Reply-To: <3dfcdc$fvj9p5@ironport9.mayo.edu>
References: <3dfcdc$fvj9p5@ironport9.mayo.edu>
Message-ID: <CABtg=Km2pmrdPirNm-_f1RU=WXSox_pA1r8qtJPpiju998sGGg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140321/242dfa2f/attachment.pl>

From edd at debian.org  Fri Mar 21 14:36:07 2014
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 21 Mar 2014 08:36:07 -0500
Subject: [Rd] The case for freezing CRAN
In-Reply-To: <6e55ab$8imvjf@ironport10.mayo.edu>
References: <6e55ab$8imvjf@ironport10.mayo.edu>
Message-ID: <21292.16455.871710.54980@max.nulle.part>


On 21 March 2014 at 07:43, Therneau, Terry M., Ph.D. wrote:
| This has been a fascinating discussion.

I am not so sure. Seems more like rehashing of old and known arguments, while
some folks try to push their work (Hi Jeroen :) onto already overloaded
others.  The only real thing I learned so far is that Philippe is busy
earning publication credits along the line 'damn, just go and test it'
suggestion I made (somewhat flippantly) in my last email.

| I maintain the survival package which currently has 246 reverse dependencies and take a 
| slightly different view, which could be described as "the price of fame".  I feel a 
| responsiblity to not break R.  I have automated scripts which download the latest copy of 
| all 246, using the install-tests option, and run them all. Most updates have 1-3 issues.  

Same here, but as a somewhat younger package Rcpp is so far "only" at 189 and
counting, with pretty decent growth.  My experience has been positive too,
and CRAN appears appreciative for us doing preemptive work and trying to be
careful about not introducing breaking changes.  I too see the latter part as
something we owe the users of our package: a "promise" not to mess with the
interface unless we absolutely must.   

| but also worth it.  I've built the test scripts over several years, with help from several 
| others; a place to share this information would be a useful addition.

I put my script on GitHub next to Rcpp itself, turns out that another thread
participant just a need for exactly that script yesterday.

Dirk

-- 
Dirk Eddelbuettel | edd at debian.org | http://dirk.eddelbuettel.com


From Philippe.GROSJEAN at umons.ac.be  Fri Mar 21 14:03:26 2014
From: Philippe.GROSJEAN at umons.ac.be (Philippe GROSJEAN)
Date: Fri, 21 Mar 2014 13:03:26 +0000
Subject: [Rd] Docker versus Vagrant for reproducability - was: The case
 for freezing CRAN
In-Reply-To: <m21txv3maf.fsf_-_@krugs.de>
References: <3dfcdc$fuu13r@ironport9.mayo.edu>
	<21290.60882.865159.407363@max.nulle.part> <m21txv3maf.fsf_-_@krugs.de>
Message-ID: <6C92022B-E12F-47CE-9143-D807F2D40278@umons.ac.be>


..............................................<?}))><........
 ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
 ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
 ) ) ) ) )   Mons University, Belgium
( ( ( ( (
..............................................................

On 21 Mar 2014, at 10:59, Rainer M Krug <Rainer at krugs.de> wrote:

> Dirk Eddelbuettel <edd at debian.org> writes:
> 
>> o Roger correctly notes that R scripts and packages are just one issue.
>>   Compilers, libraries and the OS matter.  To me, the natural approach these
>>   days would be to think of something based on Docker or Vagrant or (if you
>>   must, VirtualBox).  The newer alternatives make snapshotting very cheap
>>   (eg by using Linux LXC).  That approach reproduces a full environemnt as
>>   best as we can while still ignoring the hardware layer (and some readers
>>   may recall the infamous Pentium bug of two decades ago).
> 
> These two tools look very interesting - but I have, even after reading a
> few discussions of their differences, no idea which one is better suited
> to be used for what has been discussed here: Making it possible to run
> the analysis later to reproduce results using the same versions used in
> the initial analysis.
> 
> Am I right in saying:
> 
> - Vagrant uses VMs to emulate the hardware
> - Docker does not
> 
Yes.


> wherefore
> - Vagrant is slower and requires more space
> - Docker is faster and requires less space
> 
It depends. For instance, if you run R in VirtualBox under Windows, it may run faster depending on the code you run and, say, the Lapack library used. On Linux, you typically got R code run in the VM 2-3% slower than natively, but In a Windows host, most of my R code runs faster in the VM? But yes, you need more RAM.

With Vagrant, you do not need to keep you VM once you don't use it any more. Then, disk space is shrunk down to a few kB, corresponding to the Vagrant configuration file. I guess the same is true for Docker?

A big advantage of Vagrant + VirtualBox is that you got a very similar virtual hardware, no matter if your host system is Linux, Windows or Mac OS X. I see this as a good point for better reproducibility.


> Therefore, could one say that Vagrant is more "robust" in the long run?
> 
May be,? but it depends almost entirely how VirtualBox will support old VMs in the future!

PhG

> How do they compare in relation to different platforms? Vagrant seems to
> be platform agnostic, I can develop and run on Linux, Mac and Windows -
> how does it work with Docker? 
> 
> I just followed [1] and setup Docker on OSX - loos promising - it also
> uses an underlying VM. SO both should be equal in regards to
> reproducability in the long run?
> 
> Please note: I see these questions in the light of this discussion of
> reproducability and not in regards to deployment of applications what
> the discussions on the web are.
> 
> Any comments, thoughts, remarks?
> 
> Rainer
> 
> 
> Footnotes: 
> [1]  http://docs.docker.io/en/latest/installation/mac/
> 
> -- 
> Rainer M. Krug
> email: Rainer<at>krugs<dot>de
> PGP: 0x0F52F982
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From csardi.gabor at gmail.com  Fri Mar 21 16:01:08 2014
From: csardi.gabor at gmail.com (=?ISO-8859-1?B?R+Fib3IgQ3PhcmRp?=)
Date: Fri, 21 Mar 2014 11:01:08 -0400
Subject: [Rd] Docker versus Vagrant for reproducability - was: The case
 for freezing CRAN
In-Reply-To: <6C92022B-E12F-47CE-9143-D807F2D40278@umons.ac.be>
References: <3dfcdc$fuu13r@ironport9.mayo.edu>
	<21290.60882.865159.407363@max.nulle.part>
	<m21txv3maf.fsf_-_@krugs.de>
	<6C92022B-E12F-47CE-9143-D807F2D40278@umons.ac.be>
Message-ID: <CABtg=KkSaXCwRLcD30ANQfGgEtPby+A1E17W5XfnGExhHPktEg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140321/dff39167/attachment.pl>

From Rainer at krugs.de  Fri Mar 21 16:12:16 2014
From: Rainer at krugs.de (Rainer M Krug)
Date: Fri, 21 Mar 2014 16:12:16 +0100
Subject: [Rd] Docker versus Vagrant for reproducability - was: The case
	for freezing CRAN
In-Reply-To: <CABtg=KkSaXCwRLcD30ANQfGgEtPby+A1E17W5XfnGExhHPktEg@mail.gmail.com>
	(=?utf-8?Q?=22G=C3=A1bor_Cs=C3=A1rdi=22's?= message of "Fri, 21 Mar 2014
	11:01:08 -0400")
References: <3dfcdc$fuu13r@ironport9.mayo.edu>
	<21290.60882.865159.407363@max.nulle.part>
	<m21txv3maf.fsf_-_@krugs.de>
	<6C92022B-E12F-47CE-9143-D807F2D40278@umons.ac.be>
	<CABtg=KkSaXCwRLcD30ANQfGgEtPby+A1E17W5XfnGExhHPktEg@mail.gmail.com>
Message-ID: <m2lhw31t7z.fsf@krugs.de>

G?bor Cs?rdi <csardi.gabor at gmail.com> writes:

> You might want to look at packer as well, which can build virtual machines
> from an ISO, without any user intaraction. I successfully used it to build
> VMs with Linux, OSX and Windows. It can also create vagrant boxes. You can
> specify provisioners, e.g. to install R, or a set of R packages, etc. It is
> under heavy development, by the same team as vagrant.

I think I am getting lost in these - I looked ad Docker, and it looks
promising, but I actually didn't even manage to sh into the running
container. Is there somewhere an howto on how one can use these in R, to
the purpose discussed in this thread? If not, I really think this would
be needed. It is extremely difficult for me to translate what I want to
do into the deployment / management / development scenarios discussed in
the blogs I have found.

Cheers, 

(a confused)
Rainer


>
> Gabor
>
> On Fri, Mar 21, 2014 at 9:03 AM, Philippe GROSJEAN <
> Philippe.GROSJEAN at umons.ac.be> wrote:
>
>>
>> ..............................................<}))><........
>>  ) ) ) ) )
>> ( ( ( ( (    Prof. Philippe Grosjean
>>  ) ) ) ) )
>> ( ( ( ( (    Numerical Ecology of Aquatic Systems
>>  ) ) ) ) )   Mons University, Belgium
>> ( ( ( ( (
>> ..............................................................
>>
>> On 21 Mar 2014, at 10:59, Rainer M Krug <Rainer at krugs.de> wrote:
>>
>> > Dirk Eddelbuettel <edd at debian.org> writes:
>> >
>> >> o Roger correctly notes that R scripts and packages are just one issue.
>> >>   Compilers, libraries and the OS matter.  To me, the natural approach
>> these
>> >>   days would be to think of something based on Docker or Vagrant or (if
>> you
>> >>   must, VirtualBox).  The newer alternatives make snapshotting very
>> cheap
>> >>   (eg by using Linux LXC).  That approach reproduces a full environemnt
>> as
>> >>   best as we can while still ignoring the hardware layer (and some
>> readers
>> >>   may recall the infamous Pentium bug of two decades ago).
>> >
>> > These two tools look very interesting - but I have, even after reading a
>> > few discussions of their differences, no idea which one is better suited
>> > to be used for what has been discussed here: Making it possible to run
>> > the analysis later to reproduce results using the same versions used in
>> > the initial analysis.
>> >
>> > Am I right in saying:
>> >
>> > - Vagrant uses VMs to emulate the hardware
>> > - Docker does not
>> >
>> Yes.
>>
>>
>> > wherefore
>> > - Vagrant is slower and requires more space
>> > - Docker is faster and requires less space
>> >
>> It depends. For instance, if you run R in VirtualBox under Windows, it may
>> run faster depending on the code you run and, say, the Lapack library used.
>> On Linux, you typically got R code run in the VM 2-3% slower than natively,
>> but In a Windows host, most of my R code runs faster in the VM... But yes,
>> you need more RAM.
>>
>> With Vagrant, you do not need to keep you VM once you don't use it any
>> more. Then, disk space is shrunk down to a few kB, corresponding to the
>> Vagrant configuration file. I guess the same is true for Docker?
>>
>> A big advantage of Vagrant + VirtualBox is that you got a very similar
>> virtual hardware, no matter if your host system is Linux, Windows or Mac OS
>> X. I see this as a good point for better reproducibility.
>>
>>
>> > Therefore, could one say that Vagrant is more "robust" in the long run?
>> >
>> May be,... but it depends almost entirely how VirtualBox will support old
>> VMs in the future!
>>
>> PhG
>>
>> > How do they compare in relation to different platforms? Vagrant seems to
>> > be platform agnostic, I can develop and run on Linux, Mac and Windows -
>> > how does it work with Docker?
>> >
>> > I just followed [1] and setup Docker on OSX - loos promising - it also
>> > uses an underlying VM. SO both should be equal in regards to
>> > reproducability in the long run?
>> >
>> > Please note: I see these questions in the light of this discussion of
>> > reproducability and not in regards to deployment of applications what
>> > the discussions on the web are.
>> >
>> > Any comments, thoughts, remarks?
>> >
>> > Rainer
>> >
>> >
>> > Footnotes:
>> > [1]  http://docs.docker.io/en/latest/installation/mac/
>> >
>> > --
>> > Rainer M. Krug
>> > email: Rainer<at>krugs<dot>de
>> > PGP: 0x0F52F982
>> > ______________________________________________
>> > R-devel at r-project.org mailing list
>> > https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>
> 	[[alternative HTML version deleted]]
>

-- 
Rainer M. Krug
email: Rainer<at>krugs<dot>de
PGP: 0x0F52F982
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 494 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140321/64e36fe4/attachment.bin>

From karl.forner at gmail.com  Fri Mar 21 17:08:52 2014
From: karl.forner at gmail.com (Karl Forner)
Date: Fri, 21 Mar 2014 17:08:52 +0100
Subject: [Rd] Fwd:  [RFC] A case for freezing CRAN
In-Reply-To: <CAMd4_AeAVSc1tXk=YFTPKvB=eb4YF2rP63_8EW27tx=cjP0ttA@mail.gmail.com>
References: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>
	<CAMd4_AeAVSc1tXk=YFTPKvB=eb4YF2rP63_8EW27tx=cjP0ttA@mail.gmail.com>
Message-ID: <CAMd4_AdYyz8Uw5CDsN0Go7LVO1z735LqBYdqeS8AV0WP8y7Oeg@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140321/8e70d69c/attachment.pl>

From tshort.rlists at gmail.com  Fri Mar 21 17:16:12 2014
From: tshort.rlists at gmail.com (Tom Short)
Date: Fri, 21 Mar 2014 12:16:12 -0400
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>
References: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>
Message-ID: <CA+o9du1pe2F87d2menDkw3=NFsqWB7rF=HQsy-9yz+jh1R21ag@mail.gmail.com>

For me, the most important aspect is being able to reproduce my own
work. Some other tools offer interesting approaches to managing
packages:

* NPM -- The Node Package Manager for Node.js loads a local copy of
all packages and dependencies. This helps ensure reproducibility and
avoids dependency issues. Different projects in different directories
can then use different package versions.

* Julia -- Julia's package manager is based on git, so users should
have a local copy of all package versions they've used. Theoretically,
you could use separate git repos for different projects, and merge as
desired.

I've thought about putting my local R library into a git repository.
Then, I could clone that into a project directory and use
.libPaths(".Rlibrary")  in a .Rprofile file to set the library
directory to the clone. In addition to handling package versions, this
might be nice for installing packages that are rarely used (my library
directory tends to get cluttered if I start trying out packages).
Another addition could be a local script that starts a specific
version of R.

For now, I don't have much incentive to do this. For the packages that
I use, R's been pretty good to me with backwards compatibility.

I do like the idea of a CRAN mirror that's under version control.




On Tue, Mar 18, 2014 at 4:24 PM, Jeroen Ooms <jeroen.ooms at stat.ucla.edu> wrote:
> This came up again recently with an irreproducible paper. Below an
> attempt to make a case for extending the r-devel/r-release cycle to
> CRAN packages. These suggestions are not in any way intended as
> criticism on anyone or the status quo.
>
> The proposal described in [1] is to freeze a snapshot of CRAN along
> with every release of R. In this design, updates for contributed
> packages treated the same as updates for base packages in the sense
> that they are only published to the r-devel branch of CRAN and do not
> affect users of "released" versions of R. Thereby all users, stacks
> and applications using a particular version of R will by default be
> using the identical version of each CRAN package. The bioconductor
> project uses similar policies.
>
> This system has several important advantages:
>
> ## Reproducibility
>
> Currently r/sweave/knitr scripts are unstable because of ambiguity
> introduced by constantly changing cran packages. This causes scripts
> to break or change behavior when upstream packages are updated, which
> makes reproducing old results extremely difficult.
>
> A common counter-argument is that script authors should document
> package versions used in the script using sessionInfo(). However even
> if authors would manually do this, reconstructing the author's
> environment from this information is cumbersome and often nearly
> impossible, because binary packages might no longer be available,
> dependency conflicts, etc. See [1] for a worked example. In practice,
> the current system causes many results or documents generated with R
> no to be reproducible, sometimes already after a few months.
>
> In a system where contributed packages inherit the r-base release
> cycle, scripts will behave the same across users/systems/time within a
> given version of R. This severely reduces ambiguity of R behavior, and
> has the potential of making reproducibility a natural part of the
> language, rather than a tedious exercise.
>
> ## Repository Management
>
> Just like scripts suffer from upstream changes, so do packages
> depending on other packages. A particular package that has been
> developed and tested against the current version of a particular
> dependency is not guaranteed to work against *any future version* of
> that dependency. Therefore, packages inevitably break over time as
> their dependencies are updated.
>
> One recent example is the Rcpp 0.11 release, which required all
> reverse dependencies to be rebuild/modified. This updated caused some
> serious disruption on our production servers. Initially we refrained
> from updating Rcpp on these servers to prevent currently installed
> packages depending on Rcpp to stop working. However soon after the
> Rcpp 0.11 release, many other cran packages started to require Rcpp >=
> 0.11, and our users started complaining about not being able to
> install those packages. This resulted in the impossible situation
> where currently installed packages would not work with the new Rcpp,
> but newly installed packages would not work with the old Rcpp.
>
> Current CRAN policies blame this problem on package authors. However
> as is explained in [1], this policy does not solve anything, is
> unsustainable with growing repository size, and sets completely the
> wrong incentives for contributing code. Progress comes with breaking
> changes, and the system should be able to accommodate this. Much of
> the trouble could have been prevented by a system that does not push
> bleeding edge updates straight to end-users, but has a devel branch
> where conflicts are resolved before publishing them in the next
> r-release.
>
> ## Reliability
>
> Another example, this time on a very small scale. We recently
> discovered that R code plotting medal counts from the Sochi Olympics
> generated different results for users on OSX than it did on
> Linux/Windows. After some debugging, we narrowed it down to the XML
> package. The application used the following code to scrape results
> from the Sochi website:
>
> XML::readHTMLTable("http://www.sochi2014.com/en/speed-skating", which=2, skip=1)
>
> This code was developed and tested on mac, but results in a different
> winner on windows/linux. This happens because the current version of
> the XML package on CRAN is 3.98, but the latest mac binary is 3.95.
> Apparently this new version of XML introduces a tiny change that
> causes html-table-headers to become colnames, rather than a row in the
> matrix, resulting in different medal counts.
>
> This example illustrates that we should never assume package versions
> to be interchangeable. Any small bugfix release can have side effects
> altering results. It is impossible to protect code against such
> upstream changes using CMD check or unit testing. All R scripts and
> packages are really only developed and tested for a single version of
> their dependencies. Assuming anything else makes results
> untrustworthy, and code unreliable.
>
> ## Summary
>
> Extending the r-release cycle to CRAN seems like a solution that would
> be easy to implement. Package updates simply only get pushed to the
> r-devel branches of cran, rather than r-release and r-release-old.
> This separates development from production/use in a way that is common
> sense in most open source communities. Benefits for R include:
>
> - Regular R users (statisticians, researchers, students, teachers) can
> share their homemade scripts/documents/packages and rely on them to
> work and produce the same results within a given version of R, without
> manual efforts to manage package versions.
>
> - Package authors can publish breaking changes to the devel branch
> without causing major disruption or affecting users and/or
> maintainers. Authors of depending packages have a timeframe to sync
> their package with upstream changes before the next release.
>
> - CRAN maintainers can focus quality control and testing efforts on
> the devel branch around the time of the code freeze. No need for
> crisis management when a package update introduces some severe
> breaking changes. Users of released versions are unaffected.
>
>
> [1] http://journal.r-project.org/archive/2013-1/ooms.pdf
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From csardi.gabor at gmail.com  Fri Mar 21 17:16:31 2014
From: csardi.gabor at gmail.com (=?ISO-8859-1?B?R+Fib3IgQ3PhcmRp?=)
Date: Fri, 21 Mar 2014 12:16:31 -0400
Subject: [Rd] Fwd: [RFC] A case for freezing CRAN
In-Reply-To: <CAMd4_AdYyz8Uw5CDsN0Go7LVO1z735LqBYdqeS8AV0WP8y7Oeg@mail.gmail.com>
References: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>
	<CAMd4_AeAVSc1tXk=YFTPKvB=eb4YF2rP63_8EW27tx=cjP0ttA@mail.gmail.com>
	<CAMd4_AdYyz8Uw5CDsN0Go7LVO1z735LqBYdqeS8AV0WP8y7Oeg@mail.gmail.com>
Message-ID: <CABtg=KmFqdoqfidF8PnSysy4TREOhMgffB6rW1PqKh+_XDBvRQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140321/15fec0d7/attachment.pl>

From karl.forner at gmail.com  Fri Mar 21 17:40:50 2014
From: karl.forner at gmail.com (Karl Forner)
Date: Fri, 21 Mar 2014 17:40:50 +0100
Subject: [Rd] Fwd: [RFC] A case for freezing CRAN
In-Reply-To: <CABtg=KmFqdoqfidF8PnSysy4TREOhMgffB6rW1PqKh+_XDBvRQ@mail.gmail.com>
References: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>
	<CAMd4_AeAVSc1tXk=YFTPKvB=eb4YF2rP63_8EW27tx=cjP0ttA@mail.gmail.com>
	<CAMd4_AdYyz8Uw5CDsN0Go7LVO1z735LqBYdqeS8AV0WP8y7Oeg@mail.gmail.com>
	<CABtg=KmFqdoqfidF8PnSysy4TREOhMgffB6rW1PqKh+_XDBvRQ@mail.gmail.com>
Message-ID: <CAMd4_AcN4kHT9tgFO2N6tLnLa7O+uefaVTPa=hCEPqE-=O2Vnw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140321/606dc552/attachment.pl>

From csardi.gabor at gmail.com  Fri Mar 21 18:27:57 2014
From: csardi.gabor at gmail.com (=?ISO-8859-1?B?R+Fib3IgQ3PhcmRp?=)
Date: Fri, 21 Mar 2014 13:27:57 -0400
Subject: [Rd] Fwd: [RFC] A case for freezing CRAN
In-Reply-To: <CAMd4_AcN4kHT9tgFO2N6tLnLa7O+uefaVTPa=hCEPqE-=O2Vnw@mail.gmail.com>
References: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>
	<CAMd4_AeAVSc1tXk=YFTPKvB=eb4YF2rP63_8EW27tx=cjP0ttA@mail.gmail.com>
	<CAMd4_AdYyz8Uw5CDsN0Go7LVO1z735LqBYdqeS8AV0WP8y7Oeg@mail.gmail.com>
	<CABtg=KmFqdoqfidF8PnSysy4TREOhMgffB6rW1PqKh+_XDBvRQ@mail.gmail.com>
	<CAMd4_AcN4kHT9tgFO2N6tLnLa7O+uefaVTPa=hCEPqE-=O2Vnw@mail.gmail.com>
Message-ID: <CABtg=KnVfaf8PsTLsvhPXG-sPm7TU2458_=+8LznYdtM9LcCmA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140321/b687ac6b/attachment.pl>

From karl.forner at gmail.com  Fri Mar 21 18:38:34 2014
From: karl.forner at gmail.com (Karl Forner)
Date: Fri, 21 Mar 2014 18:38:34 +0100
Subject: [Rd] Fwd: [RFC] A case for freezing CRAN
In-Reply-To: <CABtg=KnVfaf8PsTLsvhPXG-sPm7TU2458_=+8LznYdtM9LcCmA@mail.gmail.com>
References: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>
	<CAMd4_AeAVSc1tXk=YFTPKvB=eb4YF2rP63_8EW27tx=cjP0ttA@mail.gmail.com>
	<CAMd4_AdYyz8Uw5CDsN0Go7LVO1z735LqBYdqeS8AV0WP8y7Oeg@mail.gmail.com>
	<CABtg=KmFqdoqfidF8PnSysy4TREOhMgffB6rW1PqKh+_XDBvRQ@mail.gmail.com>
	<CAMd4_AcN4kHT9tgFO2N6tLnLa7O+uefaVTPa=hCEPqE-=O2Vnw@mail.gmail.com>
	<CABtg=KnVfaf8PsTLsvhPXG-sPm7TU2458_=+8LznYdtM9LcCmA@mail.gmail.com>
Message-ID: <CAMd4_Aep8b1F337LgftGtq5b_gn6Pi33gVz7RUvdXV9j2_jBKw@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140321/2ecc741b/attachment.pl>

From csardi.gabor at gmail.com  Fri Mar 21 18:51:11 2014
From: csardi.gabor at gmail.com (=?ISO-8859-1?B?R+Fib3IgQ3PhcmRp?=)
Date: Fri, 21 Mar 2014 13:51:11 -0400
Subject: [Rd] Fwd: [RFC] A case for freezing CRAN
In-Reply-To: <CAMd4_Aep8b1F337LgftGtq5b_gn6Pi33gVz7RUvdXV9j2_jBKw@mail.gmail.com>
References: <CABFfbXt0PjHEw-4=VfeWeeosm_wOBH-fVsj85j9LMe81Lx0gWQ@mail.gmail.com>
	<CAMd4_AeAVSc1tXk=YFTPKvB=eb4YF2rP63_8EW27tx=cjP0ttA@mail.gmail.com>
	<CAMd4_AdYyz8Uw5CDsN0Go7LVO1z735LqBYdqeS8AV0WP8y7Oeg@mail.gmail.com>
	<CABtg=KmFqdoqfidF8PnSysy4TREOhMgffB6rW1PqKh+_XDBvRQ@mail.gmail.com>
	<CAMd4_AcN4kHT9tgFO2N6tLnLa7O+uefaVTPa=hCEPqE-=O2Vnw@mail.gmail.com>
	<CABtg=KnVfaf8PsTLsvhPXG-sPm7TU2458_=+8LznYdtM9LcCmA@mail.gmail.com>
	<CAMd4_Aep8b1F337LgftGtq5b_gn6Pi33gVz7RUvdXV9j2_jBKw@mail.gmail.com>
Message-ID: <CABtg=KkvAoX3qpJ1KODhLXXEX05n1eTp5xG6yFwJ0oUZ5k1ztA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140321/8d38710d/attachment.pl>

From paulgeeleher at gmail.com  Fri Mar 21 20:04:29 2014
From: paulgeeleher at gmail.com (Paul Geeleher)
Date: Fri, 21 Mar 2014 14:04:29 -0500
Subject: [Rd] "ERROR: installing package indices failed" message when
	building my package?
Message-ID: <CAKt-OfzKGBBvPmrA1vhwh-oqXevj=QJrVLUc8J=93oDH+o33Hw@mail.gmail.com>

Hey all,

I'm currently trying to build an R package and don't really know what
to make of the error message I'm getting. I couldn't find anything
helpful online so hopefully somebody here can give me a couple of
pointers. I see the following when I try to build:

s$ R CMD build --verbose r_package_files/
Warning: unknown option '--verbose'
* checking for file 'r_package_files/DESCRIPTION' ... OK
* preparing 'pRRophetic':
* checking DESCRIPTION meta-information ... OK
* installing the package to build vignettes
      -----------------------------------
* installing *source* package 'pRRophetic' ...
** R
** data
** inst
** preparing package for lazy loading
Warning: replacing previous import by 'genefilter::Anova' when loading
'pRRophetic'
** help
No man pages found in package  'pRRophetic'
*** installing help indices
** building package indices
Error in scan(file, what, nmax, sep, dec, quote, skip, nlines, na.strings,  :
  line 1 did not have 38 elements
ERROR: installing package indices failed



Thanks in advance for any help!

Paul.



-- 
Dr. Paul Geeleher, PhD
Section of Hematology-Oncology
Department of Medicine
The University of Chicago
900 E. 57th St.,
KCBD, Room 7144
Chicago, IL 60637
--
www.bioinformaticstutorials.com


From csardi.gabor at gmail.com  Fri Mar 21 20:21:08 2014
From: csardi.gabor at gmail.com (=?ISO-8859-1?B?R+Fib3IgQ3PhcmRp?=)
Date: Fri, 21 Mar 2014 15:21:08 -0400
Subject: [Rd] Docker versus Vagrant for reproducability - was: The case
 for freezing CRAN
In-Reply-To: <m2lhw31t7z.fsf@krugs.de>
References: <3dfcdc$fuu13r@ironport9.mayo.edu>
	<21290.60882.865159.407363@max.nulle.part>
	<m21txv3maf.fsf_-_@krugs.de>
	<6C92022B-E12F-47CE-9143-D807F2D40278@umons.ac.be>
	<CABtg=KkSaXCwRLcD30ANQfGgEtPby+A1E17W5XfnGExhHPktEg@mail.gmail.com>
	<m2lhw31t7z.fsf@krugs.de>
Message-ID: <CABtg=K=vr8uYcGCn3sTRRDqnnFYpQm_wmLUktopUG-Z0S9QzOQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140321/87f960b4/attachment.pl>

From Philippe.GROSJEAN at umons.ac.be  Sat Mar 22 08:58:45 2014
From: Philippe.GROSJEAN at umons.ac.be (Philippe GROSJEAN)
Date: Sat, 22 Mar 2014 07:58:45 +0000
Subject: [Rd] Docker versus Vagrant for reproducability - was: The case
 for freezing CRAN
In-Reply-To: <CABtg=K=vr8uYcGCn3sTRRDqnnFYpQm_wmLUktopUG-Z0S9QzOQ@mail.gmail.com>
References: <3dfcdc$fuu13r@ironport9.mayo.edu>
	<21290.60882.865159.407363@max.nulle.part> <m21txv3maf.fsf_-_@krugs.de>
	<6C92022B-E12F-47CE-9143-D807F2D40278@umons.ac.be>
	<CABtg=KkSaXCwRLcD30ANQfGgEtPby+A1E17W5XfnGExhHPktEg@mail.gmail.com>
	<m2lhw31t7z.fsf@krugs.de>
	<CABtg=K=vr8uYcGCn3sTRRDqnnFYpQm_wmLUktopUG-Z0S9QzOQ@mail.gmail.com>
Message-ID: <4529A5D9-1940-4C3C-BC60-A4E25F897873@umons.ac.be>

On 21 Mar 2014, at 20:21, G?bor Cs?rdi <csardi.gabor at gmail.com> wrote:

> On Fri, Mar 21, 2014 at 11:12 AM, Rainer M Krug <Rainer at krugs.de> wrote:
> 
>> G?bor Cs?rdi <csardi.gabor at gmail.com> writes:
>> 
>>> You might want to look at packer as well, which can build virtual
>> machines
>>> from an ISO, without any user intaraction. I successfully used it to
>> build
>>> VMs with Linux, OSX and Windows. It can also create vagrant boxes. You
>> can
>>> specify provisioners, e.g. to install R, or a set of R packages, etc. It
>> is
>>> under heavy development, by the same team as vagrant.
>> 
>> I think I am getting lost in these - I looked ad Docker, and it looks
>> promising, but I actually didn't even manage to sh into the running
>> container. Is there somewhere an howto on how one can use these in R, to
>> the purpose discussed in this thread? If not, I really think this would
>> be needed. It is extremely difficult for me to translate what I want to
>> do into the deployment / management / development scenarios discussed in
>> the blogs I have found.
>> 
> 
> I haven't tried Docker, so I cannot say anything about that. The purpose of
> vagrant and packer is slightly different, but there seems to be some
> overlap.
> 
> Packer helps you building a virtual machine from an ISO, automatically,
> without any human interaction. That's pretty much it. The result can be a
> VirtualBox, VMWare, etc. virtual machine, or even a vagrant box. I used it
> to build Ubuntu, OSX and Windows boxes, it works great if you have a
> working configuration. If you need to tweak a config to install additional
> software, etc. then it requires some experimenting and patience, because
> debugging is not that great.
> 
> Vagrant manages disposable virtual machines. I.e. it takes a vagrant box,
> which is essentially a VM and some extra configuration info, provisions it,
> which usually means installing software or setting up a development
> environment, and then manages it, so that you can ssh to it, or do whatever
> you want with it.
> 
> There are a number of boxes available, so if you want a minimal VM with
> Ubuntu32, it takes one command to create it from a public box, another one
> starting it, and a third one to ssh to it. It is literally a couple of
> minutes, downloading the box takes longest. If you have the box, then it is
> even quicker.
> 
> You can use packer and vagrant together. Packer creates the vagrant box,
> sets up a very minimal environment. Then you can use vagrant with this box.
> 
> In my opinion it is somewhat cumbersome to use this for everyday work,
> although good virtualization software definitely helps.
> 
> Gabor
> 
Additional info: you access R into the VM from within the host by ssh. You can enable x11 forwarding there and you also got GUI stuff. It works like a charm, but there are still some problems on my side when I try to disconnect and reconnect to the same R process. I can solve this with, say, screen. However, if any X11 window is displayed while I disconnect, R crashes immediately on reconnection.
Best,

PhG




> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From njs at pobox.com  Sat Mar 22 14:10:08 2014
From: njs at pobox.com (Nathaniel Smith)
Date: Sat, 22 Mar 2014 13:10:08 +0000
Subject: [Rd] Docker versus Vagrant for reproducability - was: The case
 for freezing CRAN
In-Reply-To: <4529A5D9-1940-4C3C-BC60-A4E25F897873@umons.ac.be>
References: <3dfcdc$fuu13r@ironport9.mayo.edu>
	<21290.60882.865159.407363@max.nulle.part>
	<m21txv3maf.fsf_-_@krugs.de>
	<6C92022B-E12F-47CE-9143-D807F2D40278@umons.ac.be>
	<CABtg=KkSaXCwRLcD30ANQfGgEtPby+A1E17W5XfnGExhHPktEg@mail.gmail.com>
	<m2lhw31t7z.fsf@krugs.de>
	<CABtg=K=vr8uYcGCn3sTRRDqnnFYpQm_wmLUktopUG-Z0S9QzOQ@mail.gmail.com>
	<4529A5D9-1940-4C3C-BC60-A4E25F897873@umons.ac.be>
Message-ID: <CAPJVwBkEAhS3SXzXK1Cacd+Zbv_6v9UZFDkc348Jv8yt7LYxNQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140322/c9bf6b76/attachment.pl>

From kirill.mueller at ivt.baug.ethz.ch  Sat Mar 22 17:39:07 2014
From: kirill.mueller at ivt.baug.ethz.ch (=?ISO-8859-1?Q?Kirill_M=FCller?=)
Date: Sat, 22 Mar 2014 17:39:07 +0100
Subject: [Rd] Docker versus Vagrant for reproducability - was: The case
 for freezing CRAN
In-Reply-To: <CAPJVwBkEAhS3SXzXK1Cacd+Zbv_6v9UZFDkc348Jv8yt7LYxNQ@mail.gmail.com>
References: <3dfcdc$fuu13r@ironport9.mayo.edu>	<21290.60882.865159.407363@max.nulle.part>	<m21txv3maf.fsf_-_@krugs.de>	<6C92022B-E12F-47CE-9143-D807F2D40278@umons.ac.be>	<CABtg=KkSaXCwRLcD30ANQfGgEtPby+A1E17W5XfnGExhHPktEg@mail.gmail.com>	<m2lhw31t7z.fsf@krugs.de>	<CABtg=K=vr8uYcGCn3sTRRDqnnFYpQm_wmLUktopUG-Z0S9QzOQ@mail.gmail.com>	<4529A5D9-1940-4C3C-BC60-A4E25F897873@umons.ac.be>
	<CAPJVwBkEAhS3SXzXK1Cacd+Zbv_6v9UZFDkc348Jv8yt7LYxNQ@mail.gmail.com>
Message-ID: <532DBCAB.7060708@ivt.baug.ethz.ch>


On 03/22/2014 02:10 PM, Nathaniel Smith wrote:
> On 22 Mar 2014 12:38, "Philippe GROSJEAN" <Philippe.GROSJEAN at umons.ac.be>
> wrote:
>> On 21 Mar 2014, at 20:21, G??bor Cs??rdi <csardi.gabor at gmail.com> wrote:
>>> In my opinion it is somewhat cumbersome to use this for everyday work,
>>> although good virtualization software definitely helps.
>>>
>>> Gabor
>>>
>> Additional info: you access R into the VM from within the host by ssh.
> You can enable x11 forwarding there and you also got GUI stuff. It works
> like a charm, but there are still some problems on my side when I try to
> disconnect and reconnect to the same R process. I can solve this with, say,
> screen. However, if any X11 window is displayed while I disconnect, R
> crashes immediately on reconnection.
>
> You might find the program 'xpra' useful. It's like screen, but for x11
> programs.
>
> -n
I second that. However, by default, xpra and GNU Screen are not aware of 
each other. To connect to xpra from within GNU Screen, you usually need 
to set the DISPLAY environment variable manually. I have described a 
solution that automates this, so that GUI applications "just work" from 
within GNU Screen and also survive a disconnect: 
http://krlmlr.github.io/integrating-xpra-with-screen/ .


-Kirill


From edd at debian.org  Sat Mar 22 18:17:11 2014
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sat, 22 Mar 2014 12:17:11 -0500
Subject: [Rd] Docker versus Vagrant for reproducability - was: The case
 for freezing CRAN
In-Reply-To: <CAPJVwBkEAhS3SXzXK1Cacd+Zbv_6v9UZFDkc348Jv8yt7LYxNQ@mail.gmail.com>
References: <3dfcdc$fuu13r@ironport9.mayo.edu>
	<21290.60882.865159.407363@max.nulle.part>
	<m21txv3maf.fsf_-_@krugs.de>
	<6C92022B-E12F-47CE-9143-D807F2D40278@umons.ac.be>
	<CABtg=KkSaXCwRLcD30ANQfGgEtPby+A1E17W5XfnGExhHPktEg@mail.gmail.com>
	<m2lhw31t7z.fsf@krugs.de>
	<CABtg=K=vr8uYcGCn3sTRRDqnnFYpQm_wmLUktopUG-Z0S9QzOQ@mail.gmail.com>
	<4529A5D9-1940-4C3C-BC60-A4E25F897873@umons.ac.be>
	<CAPJVwBkEAhS3SXzXK1Cacd+Zbv_6v9UZFDkc348Jv8yt7LYxNQ@mail.gmail.com>
Message-ID: <21293.50583.274048.1023@max.nulle.part>


On 22 March 2014 at 13:10, Nathaniel Smith wrote:
| You might find the program 'xpra' useful. It's like screen, but for x11
| programs.

There are also NXserver/NXclien/FreeNX which keep 'x11 / xdm sessions' and
can resume / reconnect when the client dies.  I find x2go quite useful at work.

Dirk

-- 
Dirk Eddelbuettel | edd at debian.org | http://dirk.eddelbuettel.com


From radford at cs.toronto.edu  Sat Mar 22 18:32:37 2014
From: radford at cs.toronto.edu (Radford Neal)
Date: Sat, 22 Mar 2014 13:32:37 -0400
Subject: [Rd] Varying results of package checks due to random seed
In-Reply-To: <mailman.23.1395486006.17262.r-devel@r-project.org>
References: <mailman.23.1395486006.17262.r-devel@r-project.org>
Message-ID: <20140322173237.GA22878@cs.toronto.edu>

> From: Philippe GROSJEAN <Philippe.GROSJEAN at umons.ac.be>
>
> ... for latest CRAN version, we have successfully installed 4999
> packages among the 5321 CRAN package on our platform. ... It is
> strange that a large portion of R CMD check errors on CRAN occur and
> disappear *without any version update* of a package or any of its
> direct or indirect dependencies! That is, a fraction of errors or
> warnings seem to appear and disappear without any code update. 

Some of these are likely the result of packages running tests using
random number generation without setting the random numbers seed, in
which case the seed is set based on the current time and process id,
with an obvious possibility of results varying from run to run.

In the current development version of pqR (in branch 19-mods, found at
https://github.com/radfordneal/pqR/tree/19-mods), I have implemented a
change so that if the R_SEED environment variable is set, the random
seed is initialized to its value, rather than from the time and
process id.  This was motivated by exactly this problem - I can now
just set R_SEED to something before running all the package checks.

   Radford Neal


From richierocks at gmail.com  Sun Mar 23 13:42:55 2014
From: richierocks at gmail.com (Richard Cotton)
Date: Sun, 23 Mar 2014 15:42:55 +0300
Subject: [Rd] A rep_each function
Message-ID: <CAPp_+=eXGWBdKR8pvHMFLgEFbAaN+tBB8CFFgW93c+stSJ3Nww@mail.gmail.com>

The rep function is very versatile, but that versatility comes at a
cost: it takes a bit of effort to learn (and remember) its syntax.
This is a problem, since rep is one of the first functions many
beginners will come across.  Of the three main uses of rep, two have
simpler alternatives.

rep(x, times = ) has rep.int
rep(x, length.out  = ) has rep_len

I think that a rep_each function would be a worthy addition for the
third use case

rep(x, each = )

(It might also be worth having rep_times as a synonym for rep.int.)

While this could go in a package, since one of its main benefits is to
help beginners, I feel it ought to go in base R.

Before I submit this as a feature request, I thought I'd check here to
see if there was any enthusiasm.  Does rep_len sound useful to you?

-- 
Regards,
Richie

Learning R <http://shop.oreilly.com/product/0636920028352.do>
4dpiecharts.com


From randy.cs.lai at gmail.com  Sun Mar 23 08:10:31 2014
From: randy.cs.lai at gmail.com (Randy Lai)
Date: Sun, 23 Mar 2014 00:10:31 -0700
Subject: [Rd] Makevars clean target not in action?
In-Reply-To: <20140322173237.GA22878@cs.toronto.edu>
References: <mailman.23.1395486006.17262.r-devel@r-project.org>
	<20140322173237.GA22878@cs.toronto.edu>
Message-ID: <8DA6B9BB-2FCA-4192-B0AE-285825B3D89B@gmail.com>

Hi, developers

When I run `R CMD INSTALL --clean my_package`, only the .o files and the .so file are removed, however, they were deleted not because of the clean target defined in Makevars.
The clean target was indeed not executed.

I have the following simple Marvars to show that clean target is not functioning.

Makevars
-----------------------------

all: foo $(SHLIB)

clean: 
	rm -f *.o $(SHLIB)
	rm -f b

foo:
	touch b


After I run ``R CMD INSTALL --clean my_package``, the file `b` is still there.

Any thought?

Best,
Randy

From bennet at umich.edu  Sun Mar 23 15:08:59 2014
From: bennet at umich.edu (Bennet Fauber)
Date: Sun, 23 Mar 2014 10:08:59 -0400
Subject: [Rd] Docker versus Vagrant for reproducability - was: The case
 for freezing CRAN
Message-ID: <CAB2ovosgAy4=nqWWD8L-EZ+gf9npfe1gHAEe7y=np1QtaL0d-A@mail.gmail.com>

Vagrant and packer look like the could be useful to developers,
especially when testing, as they make it relatively painless to
'reinstall' a machine to a (mostly) known state.  I find that, no
matter how hard I try, there always seem to be a few keystrokes that
don't make it into the changelog, and recreating from scratch is how I
find them.  Thanks for pointing them out.

I would weigh in on the side of those who are against 'freezing' CRAN.
 The notion of having an R package repository that is frozen is a fine
one, and I would probably support such a thing and find it useful, but
that's not, to my mind, what CRAN is.  CRAN is an open place for
people to put their packages where others can get at them, while they
are under active development and after.  I don't think it a good idea
to make it into something else.

'Freezing it' also seems a bit misguided, as that would freeze it,
warts and all.  Something along the lines of the RHEL or LTS idea,
where perhaps major.minor versions of things are essentially frozen,
but security and bug patches continue seems more right-minded.  Strict
replicability is best done with VM images, though I suppose one should
also put a 'This image packed on <DATE>', along with a list of
ingredients, on any VM image.  It probably should be the
responsibility of the researcher to look after their research, if it
isn't already, so the onus of freezing should be there, not at CRAN.

With respect to X, NX is quite nice, though I've had mixed success
with the older, open-source server not always seeming to work.  The
commercial one is free, but limited to two users.  I've used the
classic VNC (in several flavors, which depends on your linux distro)
with ssh tunnels, where you ssh to a text screen, tunneling the port
you want X over, then manually start an Xvnc session.  You can
disconnect from the VNC session and reconnect later and the session
will be running.  NX is essentially the same thing, except it handles
the details of the ssh tunnel for you.  With both, graphical
performance is much, much better than tunneling X directly.

-- bennet
=====
HPC Software Support
University of Michigan


From hpages at fhcrc.org  Mon Mar 24 06:37:38 2014
From: hpages at fhcrc.org (=?windows-1252?Q?Herv=E9_Pag=E8s?=)
Date: Sun, 23 Mar 2014 22:37:38 -0700
Subject: [Rd] A rep_each function
In-Reply-To: <CAPp_+=eXGWBdKR8pvHMFLgEFbAaN+tBB8CFFgW93c+stSJ3Nww@mail.gmail.com>
References: <CAPp_+=eXGWBdKR8pvHMFLgEFbAaN+tBB8CFFgW93c+stSJ3Nww@mail.gmail.com>
Message-ID: <532FC4A2.1020203@fhcrc.org>

Hi,

On 03/23/2014 05:42 AM, Richard Cotton wrote:
> The rep function is very versatile, but that versatility comes at a
> cost: it takes a bit of effort to learn (and remember) its syntax.
> This is a problem, since rep is one of the first functions many
> beginners will come across.  Of the three main uses of rep, two have
> simpler alternatives.
>
> rep(x, times = ) has rep.int
> rep(x, length.out  = ) has rep_len
>
> I think that a rep_each function would be a worthy addition for the
> third use case
>
> rep(x, each = )
>
> (It might also be worth having rep_times as a synonym for rep.int.)

I agree that rep_times() would be a much better name. Because rep.int()
looks like an S3 method for rep(), 'R CMD check' emits the following
NOTE on the BiocGenerics package (where rep.int() is turned into an
S4 generic):

   * checking Rd \usage sections ... NOTE
   S3 methods shown with full name in documentation object 'rep':
     ?rep.int?

   The \usage entries for S3 methods should use the \method markup and not
   their full name.
   See the chapter ?Writing R documentation files? in the ?Writing R
   Extensions? manual.

Cheers,
H.

>
> While this could go in a package, since one of its main benefits is to
> help beginners, I feel it ought to go in base R.
>
> Before I submit this as a feature request, I thought I'd check here to
> see if there was any enthusiasm.  Does rep_len sound useful to you?
>

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fhcrc.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From Rainer at krugs.de  Mon Mar 24 09:41:42 2014
From: Rainer at krugs.de (Rainer M Krug)
Date: Mon, 24 Mar 2014 09:41:42 +0100
Subject: [Rd] Docker versus Vagrant for reproducability - was: The case
	for freezing CRAN
In-Reply-To: <532DBCAB.7060708@ivt.baug.ethz.ch> ("Kirill \=\?utf-8\?Q\?M\?\=
	\=\?utf-8\?Q\?\=C3\=BCller\=22's\?\= message of
	"Sat, 22 Mar 2014 17:39:07 +0100")
References: <3dfcdc$fuu13r@ironport9.mayo.edu>
	<21290.60882.865159.407363@max.nulle.part>
	<m21txv3maf.fsf_-_@krugs.de>
	<6C92022B-E12F-47CE-9143-D807F2D40278@umons.ac.be>
	<CABtg=KkSaXCwRLcD30ANQfGgEtPby+A1E17W5XfnGExhHPktEg@mail.gmail.com>
	<m2lhw31t7z.fsf@krugs.de>
	<CABtg=K=vr8uYcGCn3sTRRDqnnFYpQm_wmLUktopUG-Z0S9QzOQ@mail.gmail.com>
	<4529A5D9-1940-4C3C-BC60-A4E25F897873@umons.ac.be>
	<CAPJVwBkEAhS3SXzXK1Cacd+Zbv_6v9UZFDkc348Jv8yt7LYxNQ@mail.gmail.com>
	<532DBCAB.7060708@ivt.baug.ethz.ch>
Message-ID: <m2r45sount.fsf@krugs.de>

Thanks everybody for their input - interesting suggestions and useful
information - thanks.

But I am still struggling to use this information. What I got so far:

1) I have decided to try docker [1]
2) Installed docker and boot2docker on a Mac via homebrew and it works
3) I found some Dockerfiles to create an image with R and ssh
4) The dockerfile runs and creates the image
5) I can interactively connect to the image by using bash and R is
running there
5) As I am using emacs /  ess, I want to use ssh do R stuff (other
suggestions welcome)

Problems:
1) I don't manage to connect to the running docker image following [2] -
I even managed to freeze my computer while trying.
2) Even if I could, I understand that the ssh port would be different each
time - not very nice. Is there a way of setting the port?

Questions:

1) Am I right in saying, that I have to use ssh to access the running
image, or is there a (faster?) alternative? I mean - I am working
locally and I don't need any encryption.

2) Would Vagrant make the process easier?

And finally:

I think it would be great if this information could be collected in a
wiki page, as I did not find anything about the usage scenario of docker
/ vagrant discussed here - I will certainly see that I blog about my
tries.

Cheers,

Rainer

Kirill M?ller <kirill.mueller at ivt.baug.ethz.ch> writes:

> On 03/22/2014 02:10 PM, Nathaniel Smith wrote:
>> On 22 Mar 2014 12:38, "Philippe GROSJEAN" <Philippe.GROSJEAN at umons.ac.be>
>> wrote:
>>> On 21 Mar 2014, at 20:21, G??bor Cs??rdi <csardi.gabor at gmail.com> wrote:
>>>> In my opinion it is somewhat cumbersome to use this for everyday work,
>>>> although good virtualization software definitely helps.
>>>>
>>>> Gabor
>>>>
>>> Additional info: you access R into the VM from within the host by ssh.
>> You can enable x11 forwarding there and you also got GUI stuff. It works
>> like a charm, but there are still some problems on my side when I try to
>> disconnect and reconnect to the same R process. I can solve this with, say,
>> screen. However, if any X11 window is displayed while I disconnect, R
>> crashes immediately on reconnection.
>>
>> You might find the program 'xpra' useful. It's like screen, but for x11
>> programs.
>>
>> -n
> I second that. However, by default, xpra and GNU Screen are not aware
> of each other. To connect to xpra from within GNU Screen, you usually
> need to set the DISPLAY environment variable manually. I have
> described a solution that automates this, so that GUI applications
> "just work" from within GNU Screen and also survive a disconnect:
> http://krlmlr.github.io/integrating-xpra-with-screen/ .
>
>
> -Kirill
>


Footnotes: 
[1]  https://www.docker.io

[2]  http://docs.docker.io/en/latest/examples/running_ssh_service/

-- 
Rainer M. Krug
email: Rainer<at>krugs<dot>de
PGP: 0x0F52F982
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 494 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140324/8a531819/attachment.bin>

From Rainer at krugs.de  Mon Mar 24 09:45:12 2014
From: Rainer at krugs.de (Rainer M Krug)
Date: Mon, 24 Mar 2014 09:45:12 +0100
Subject: [Rd] Docker versus Vagrant for reproducability - was: The case
	for freezing CRAN
In-Reply-To: <m2r45sount.fsf@krugs.de> (Rainer M. Krug's message of "Mon, 24
	Mar 2014 09:41:42 +0100")
References: <3dfcdc$fuu13r@ironport9.mayo.edu>
	<21290.60882.865159.407363@max.nulle.part>
	<m21txv3maf.fsf_-_@krugs.de>
	<6C92022B-E12F-47CE-9143-D807F2D40278@umons.ac.be>
	<CABtg=KkSaXCwRLcD30ANQfGgEtPby+A1E17W5XfnGExhHPktEg@mail.gmail.com>
	<m2lhw31t7z.fsf@krugs.de>
	<CABtg=K=vr8uYcGCn3sTRRDqnnFYpQm_wmLUktopUG-Z0S9QzOQ@mail.gmail.com>
	<4529A5D9-1940-4C3C-BC60-A4E25F897873@umons.ac.be>
	<CAPJVwBkEAhS3SXzXK1Cacd+Zbv_6v9UZFDkc348Jv8yt7LYxNQ@mail.gmail.com>
	<532DBCAB.7060708@ivt.baug.ethz.ch> <m2r45sount.fsf@krugs.de>
Message-ID: <m2lhw0ouhz.fsf@krugs.de>


Forgot: My Dockerfiloe is on github:

https://github.com/rkrug/R-docker

Rainer M Krug <Rainer at krugs.de> writes:

> Thanks everybody for their input - interesting suggestions and useful
> information - thanks.
>
> But I am still struggling to use this information. What I got so far:
>
> 1) I have decided to try docker [1]
> 2) Installed docker and boot2docker on a Mac via homebrew and it works
> 3) I found some Dockerfiles to create an image with R and ssh
> 4) The dockerfile runs and creates the image
> 5) I can interactively connect to the image by using bash and R is
> running there
> 5) As I am using emacs /  ess, I want to use ssh do R stuff (other
> suggestions welcome)
>
> Problems:
> 1) I don't manage to connect to the running docker image following [2] -
> I even managed to freeze my computer while trying.
> 2) Even if I could, I understand that the ssh port would be different each
> time - not very nice. Is there a way of setting the port?
>
> Questions:
>
> 1) Am I right in saying, that I have to use ssh to access the running
> image, or is there a (faster?) alternative? I mean - I am working
> locally and I don't need any encryption.
>
> 2) Would Vagrant make the process easier?
>
> And finally:
>
> I think it would be great if this information could be collected in a
> wiki page, as I did not find anything about the usage scenario of docker
> / vagrant discussed here - I will certainly see that I blog about my
> tries.
>
> Cheers,
>
> Rainer
>
> Kirill M?ller <kirill.mueller at ivt.baug.ethz.ch> writes:
>
>> On 03/22/2014 02:10 PM, Nathaniel Smith wrote:
>>> On 22 Mar 2014 12:38, "Philippe GROSJEAN" <Philippe.GROSJEAN at umons.ac.be>
>>> wrote:
>>>> On 21 Mar 2014, at 20:21, G??bor Cs??rdi <csardi.gabor at gmail.com> wrote:
>>>>> In my opinion it is somewhat cumbersome to use this for everyday work,
>>>>> although good virtualization software definitely helps.
>>>>>
>>>>> Gabor
>>>>>
>>>> Additional info: you access R into the VM from within the host by ssh.
>>> You can enable x11 forwarding there and you also got GUI stuff. It works
>>> like a charm, but there are still some problems on my side when I try to
>>> disconnect and reconnect to the same R process. I can solve this with, say,
>>> screen. However, if any X11 window is displayed while I disconnect, R
>>> crashes immediately on reconnection.
>>>
>>> You might find the program 'xpra' useful. It's like screen, but for x11
>>> programs.
>>>
>>> -n
>> I second that. However, by default, xpra and GNU Screen are not aware
>> of each other. To connect to xpra from within GNU Screen, you usually
>> need to set the DISPLAY environment variable manually. I have
>> described a solution that automates this, so that GUI applications
>> "just work" from within GNU Screen and also survive a disconnect:
>> http://krlmlr.github.io/integrating-xpra-with-screen/ .
>>
>>
>> -Kirill
>>
>
>
> Footnotes: 
> [1]  https://www.docker.io
>
> [2]  http://docs.docker.io/en/latest/examples/running_ssh_service/

-- 
Rainer M. Krug
email: Rainer<at>krugs<dot>de
PGP: 0x0F52F982
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 494 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140324/3feea779/attachment.bin>

From maechler at stat.math.ethz.ch  Mon Mar 24 11:28:54 2014
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 24 Mar 2014 11:28:54 +0100
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <532B6A7D.3070800@fhcrc.org>
References: <129902082.964468.1395296156508.JavaMail.root@fhcrc.org>
	<532AC885.1090305@gmail.com> <532B3E0A.4030907@fhcrc.org>
	<CAOTG1hVzjPBhw8yZ5egftKf6dBXKzgUZ_xDBw7fMz+ASk10xTQ@mail.gmail.com>
	<532B6A7D.3070800@fhcrc.org>
Message-ID: <21296.2278.195299.711734@stat.math.ethz.ch>

>>>>> Herv? Pag?s <hpages at fhcrc.org>
>>>>>     on Thu, 20 Mar 2014 15:23:57 -0700 writes:

    > On 03/20/2014 01:28 PM, Ted Byers wrote:
    >> On Thu, Mar 20, 2014 at 3:14 PM, Herv? Pag?s
    >> <hpages at fhcrc.org <mailto:hpages at fhcrc.org>> wrote:
    >> 
    >> On 03/20/2014 03:52 AM, Duncan Murdoch wrote:
    >> 
    >> On 14-03-20 2:15 AM, Dan Tenenbaum wrote:
    >> 
    >> 
    >> 
    >> ----- Original Message -----
    >> 
    >> From: "David Winsemius" <dwinsemius at comcast.net
    >> <mailto:dwinsemius at comcast.net>> To: "Jeroen Ooms"
    >> <jeroen.ooms at stat.ucla.edu
    >> <mailto:jeroen.ooms at stat.ucla.edu>> Cc: "r-devel"
    >> <r-devel at r-project.org <mailto:r-devel at r-project.org>>
    >> Sent: Wednesday, March 19, 2014 11:03:32 PM Subject: Re:
    >> [Rd] [RFC] A case for freezing CRAN
    >> 
    >> 
    >> On Mar 19, 2014, at 7:45 PM, Jeroen Ooms wrote:
    >> 
    >> On Wed, Mar 19, 2014 at 6:55 PM, Michael Weylandt
    >> <michael.weylandt at gmail.com
    >> <mailto:michael.weylandt at gmail.com>> wrote:
    >> 
    >> Reading this thread again, is it a fair summary of your
    >> position to say "reproducibility by default is more
    >> important than giving users access to the newest bug
    >> fixes and features by default?"  It's certainly arguable,
    >> but I'm not sure I'm convinced: I'd imagine that the
    >> ratio of new work being done vs reproductions is rather
    >> high and the current setup optimizes for that already.
    >> 
    >> 
    >> I think that separating development from released
    >> branches can give us both reliability/reproducibility
    >> (stable branch) as well as new features (unstable
    >> branch). The user gets to pick (and you can pick
    >> both!). The same is true for r-base: when using a
    >> 'released' version you get 'stable' base packages that
    >> are up to 12 months old. If you want to have the latest
    >> stuff you download a nightly build of r-devel.  For
    >> regular users and reproducible research it is recommended
    >> to use the stable branch. However if you are a developer
    >> (e.g. package author) you might want to
    >> develop/test/check your work with the latest r-devel.
    >> 
    >> I think that extending the R release cycle to CRAN would
    >> result both in more stable released versions of R, as
    >> well as more freedom for package authors to implement
    >> rigorous change in the unstable branch.  When writing a
    >> script that is part of a production pipeline, or sweave
    >> paper that should be reproducible 10 years from now, or a
    >> book on using R, you use stable version of R, which is
    >> guaranteed to behave the same over time. However when
    >> developing packages that should be compatible with the
    >> upcoming release of R, you use r-devel which has the
    >> latest versions of other CRAN and base packages.
    >> 
    >> 
    >> 
    >> As I remember ... The example demonstrating the need for
    >> this was an XML package that cause an extract from a
    >> website where the headers were misinterpreted as data in
    >> one version of pkg:XML and not in another. That seems
    >> fairly unconvincing. Data cleaning and validation is a
    >> basic task of data analysis. It also seems excessive to
    >> assert that it is the responsibility of CRAN to maintain
    >> a synced binary archive that will be available in ten
    >> years.
    >> 
    >> 
    >> 
    >> CRAN already does this, the bin/windows/contrib directory
    >> has subdirectories going back to 1.7, with packages dated
    >> October 2004. I don't see why it is burdensome to
    >> continue to archive these.  It would be nice if source
    >> versions had a similar archive.
    >> 
    >> 
    >> The bin/windows/contrib directories are updated every day
    >> for active R versions.  It's only when Uwe decides that a
    >> version is no longer worth active support that he stops
    >> doing updates, and it "freezes".  A consequence of this
    >> is that the snapshots preserved in those older
    >> directories are unlikely to match what someone who keeps
    >> up to date with R releases is using.  Their purpose is to
    >> make sure that those older versions aren't completely
    >> useless, but they aren't what Jeroen was asking for.
    >> 
    >> 
    >> But it is almost completely useless from a
    >> reproducibility point of view to get random package
    >> versions. For example if some people try to use R-2.13.2
    >> today to reproduce an analysis that was published 2 years
    >> ago, they'll get Matrix 1.0-4 on Windows, Matrix 1.0-3 on
    >> Mac, and Matrix 1.1-2-2 on Unix. And none of them of
    >> course is what was used by the authors of the paper (they
    >> used Matrix 1.0-1, which is what was current when they
    >> ran their analysis).
    >> 
    >> Initially this discussion brought back nightmares of DLL
    >> hell on Windows.  Those as ancient as I will remember
    >> that well.  But now, the focus seems to be on
    >> reproducibility, but with what strikes me as a seriously
    >> flawed notion of what reproducibility means.
    >> 
    >> Herve Pages mentions the risk of irreproducibility across
    >> three minor revisions of version 1.0 of Matrix.

    > If you use R-2.13.2, you get Matrix 1.1-2-2 on
    > Linux. 

No way!  Matrix 1.1-2-2 has  Depends: R (>= 2.15.2)


    > AFAIK this is the most recent version of Matrix,
    > aimed to be compatible with the most current version of R
    > (i.e. R 3.0.3). However, it has never been tested with R-2.13.2.

Exactly. And for this reason, I have adopted to keep
	 Depends: R (>= ...)
in Matrix and partly, in other packages I maintain.

Doing so does prevent users of old versions of R to get new
features, and even more importantly, get the latest (few, of
course ! ;-) bug-fixes for Matrix.

But apart from this short note.
I'm very sympathetic with optionally providing easier (not
"easy") ways of setting up old versions of R and packages,
where users can pretty quickly use the printed (unfortunately,
for now) output of sessionInfo(), to reinstall 
1) the version of R
2) an install.packages() call which tries (!) to get
   the corresponding packages (in their correct version) from
   CRAN (including ./Archive/ !).. 

similarly to what Duncan Murdoch has agreed to.

    > I'm not saying that it should, that would be a
    > big waste of resources of course. All I'm saying it that
    > it doesn't make sense to serve by default a version that
    > is known to be incompatible with the version of R being
    > used. It's very likely to not even install properly.

    [..............]

    > Also note that back in October 2011, people using R-2.13.2
    > would get e.g. ape 2.7-3 on Linux, Windows and
    > Mac. Wouldn't it make sense that people using R-2.13.2
    > today get the same? Why would anybody use R-2.13.2 today
    > if it's not to run again some code that was written and
    > used two years ago to obtain some important results?

I also tend to agree that it would be great if someone (Karl
Millar -> Google ?) would setup a good time-stamping system for
CRAN {and Bioconductor and Omegahat and ..?} packages.
Ideally that system would work by *using* the CRAN (and ..)
infrastructure.

    > Cheers, H.

I'm still unsure if I should agree with you (Herv?) that some
freezing / "data base of package timestamps" should
happen on-CRAN in addition.

Martin


From jon.clayden at gmail.com  Mon Mar 24 11:40:04 2014
From: jon.clayden at gmail.com (Jon Clayden)
Date: Mon, 24 Mar 2014 10:40:04 +0000
Subject: [Rd] Timezone warnings on package install in R-alpha
Message-ID: <CAM9CR=3cWvm1cmJNOrm3C_VckK6f=TV9tzyGJNo+OQHaut=0yQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140324/359a59a3/attachment.pl>

From maechler at stat.math.ethz.ch  Mon Mar 24 11:53:28 2014
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 24 Mar 2014 11:53:28 +0100
Subject: [Rd] "ERROR: installing package indices failed" message
	when	building my package?
In-Reply-To: <CAKt-OfzKGBBvPmrA1vhwh-oqXevj=QJrVLUc8J=93oDH+o33Hw@mail.gmail.com>
References: <CAKt-OfzKGBBvPmrA1vhwh-oqXevj=QJrVLUc8J=93oDH+o33Hw@mail.gmail.com>
Message-ID: <21296.3752.592995.240438@stat.math.ethz.ch>

>>>>> Paul Geeleher <paulgeeleher at gmail.com>
>>>>>     on Fri, 21 Mar 2014 14:04:29 -0500 writes:

    > Hey all, I'm currently trying to build an R package and
    > don't really know what to make of the error message I'm
    > getting. I couldn't find anything helpful online so
    > hopefully somebody here can give me a couple of
    > pointers. I see the following when I try to build:

    > s$ R CMD build --verbose r_package_files/
    > Warning: unknown option '--verbose'
    > * checking for file 'r_package_files/DESCRIPTION' ... OK
    > * preparing 'pRRophetic':
    > * checking DESCRIPTION meta-information ... OK
    > * installing the package to build vignettes
    > -----------------------------------
    > * installing *source* package 'pRRophetic' ...
    > ** R
    > ** data
    > ** inst
    > ** preparing package for lazy loading
    > Warning: replacing previous import by 'genefilter::Anova' when loading
    > 'pRRophetic'

    > ** help
    > No man pages found in package  'pRRophetic'

Well, this could be it.  It may well be that we (R authors) do
require some man pages, and I would not disagree with such a requirement.

Do provide at least a man/pRRophetic-package.Rd file
if nothing else.

You can use   promptPackage()   to create a prototype for such a
file easily.

Martin

    > *** installing help indices
    > ** building package indices
    > Error in scan(file, what, nmax, sep, dec, quote, skip, nlines, na.strings,  :
    > line 1 did not have 38 elements
    > ERROR: installing package indices failed



    > Thanks in advance for any help!

    > Paul.



    > -- 
    > Dr. Paul Geeleher, PhD
    > Section of Hematology-Oncology
    > Department of Medicine
    > The University of Chicago
    > 900 E. 57th St.,
    > KCBD, Room 7144
    > Chicago, IL 60637
    > --
    > www.bioinformaticstutorials.com

    > ______________________________________________
    > R-devel at r-project.org mailing list
    > https://stat.ethz.ch/mailman/listinfo/r-devel


From csardi.gabor at gmail.com  Mon Mar 24 14:43:11 2014
From: csardi.gabor at gmail.com (=?ISO-8859-1?B?R+Fib3IgQ3PhcmRp?=)
Date: Mon, 24 Mar 2014 09:43:11 -0400
Subject: [Rd] [RFC] A case for freezing CRAN
In-Reply-To: <21296.2278.195299.711734@stat.math.ethz.ch>
References: <129902082.964468.1395296156508.JavaMail.root@fhcrc.org>
	<532AC885.1090305@gmail.com> <532B3E0A.4030907@fhcrc.org>
	<CAOTG1hVzjPBhw8yZ5egftKf6dBXKzgUZ_xDBw7fMz+ASk10xTQ@mail.gmail.com>
	<532B6A7D.3070800@fhcrc.org>
	<21296.2278.195299.711734@stat.math.ethz.ch>
Message-ID: <CABtg=K=K41=WkjYByQE+uM+tOQUX-n0Q+jGOn1NZV6E9bzAf1Q@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140324/bba5ba95/attachment.pl>

From paulgeeleher at gmail.com  Mon Mar 24 15:37:57 2014
From: paulgeeleher at gmail.com (Paul Geeleher)
Date: Mon, 24 Mar 2014 09:37:57 -0500
Subject: [Rd] "ERROR: installing package indices failed" message when
 building my package?
In-Reply-To: <21296.3752.592995.240438@stat.math.ethz.ch>
References: <CAKt-OfzKGBBvPmrA1vhwh-oqXevj=QJrVLUc8J=93oDH+o33Hw@mail.gmail.com>
	<21296.3752.592995.240438@stat.math.ethz.ch>
Message-ID: <CAKt-OfyKXVP2CGgBc6ivFYLig3OC39zr7wtF_0akPLGTJY3RqQ@mail.gmail.com>

Hey Martin,

Thanks for your reply. I actually have documentation, but I pasted a
version of the output that was created when I removed the .Rd files
(as I suspected they may be causing the problem). Here's the output
when the man files are all included (its more or less the same):


$ R CMD build --verbose r_package_files/
Warning: unknown option '--verbose'
* checking for file 'r_package_files/DESCRIPTION' ... OK
* preparing 'pRRophetic':
* checking DESCRIPTION meta-information ... OK
* installing the package to build vignettes
      -----------------------------------
* installing *source* package 'pRRophetic' ...
** R
** data
** inst
** preparing package for lazy loading
Warning: replacing previous import by 'genefilter::Anova' when loading
'pRRophetic'
** help
*** installing help indices
** building package indices
Error in scan(file, what, nmax, sep, dec, quote, skip, nlines, na.strings,  :
  line 1 did not have 38 elements
ERROR: installing package indices failed
* removing '/tmp/Rtmp6cwCZH/Rinst2f8f69e1ee/pRRophetic'
      -----------------------------------
ERROR: package installation failed


Thanks again,

Paul.

On Mon, Mar 24, 2014 at 5:53 AM, Martin Maechler
<maechler at stat.math.ethz.ch> wrote:
>>>>>> Paul Geeleher <paulgeeleher at gmail.com>
>>>>>>     on Fri, 21 Mar 2014 14:04:29 -0500 writes:
>
>     > Hey all, I'm currently trying to build an R package and
>     > don't really know what to make of the error message I'm
>     > getting. I couldn't find anything helpful online so
>     > hopefully somebody here can give me a couple of
>     > pointers. I see the following when I try to build:
>
>     > s$ R CMD build --verbose r_package_files/
>     > Warning: unknown option '--verbose'
>     > * checking for file 'r_package_files/DESCRIPTION' ... OK
>     > * preparing 'pRRophetic':
>     > * checking DESCRIPTION meta-information ... OK
>     > * installing the package to build vignettes
>     > -----------------------------------
>     > * installing *source* package 'pRRophetic' ...
>     > ** R
>     > ** data
>     > ** inst
>     > ** preparing package for lazy loading
>     > Warning: replacing previous import by 'genefilter::Anova' when loading
>     > 'pRRophetic'
>
>     > ** help
>     > No man pages found in package  'pRRophetic'
>
> Well, this could be it.  It may well be that we (R authors) do
> require some man pages, and I would not disagree with such a requirement.
>
> Do provide at least a man/pRRophetic-package.Rd file
> if nothing else.
>
> You can use   promptPackage()   to create a prototype for such a
> file easily.
>
> Martin
>
>     > *** installing help indices
>     > ** building package indices
>     > Error in scan(file, what, nmax, sep, dec, quote, skip, nlines, na.strings,  :
>     > line 1 did not have 38 elements
>     > ERROR: installing package indices failed
>
>
>
>     > Thanks in advance for any help!
>
>     > Paul.
>
>
>
>     > --
>     > Dr. Paul Geeleher, PhD
>     > Section of Hematology-Oncology
>     > Department of Medicine
>     > The University of Chicago
>     > 900 E. 57th St.,
>     > KCBD, Room 7144
>     > Chicago, IL 60637
>     > --
>     > www.bioinformaticstutorials.com
>
>     > ______________________________________________
>     > R-devel at r-project.org mailing list
>     > https://stat.ethz.ch/mailman/listinfo/r-devel



-- 
Dr. Paul Geeleher, PhD
Section of Hematology-Oncology
Department of Medicine
The University of Chicago
900 E. 57th St.,
KCBD, Room 7144
Chicago, IL 60637
--
www.bioinformaticstutorials.com


From maechler at stat.math.ethz.ch  Mon Mar 24 15:49:55 2014
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 24 Mar 2014 15:49:55 +0100
Subject: [Rd] "ERROR: installing package indices failed" message when
 building my package?
In-Reply-To: <CAKt-OfyKXVP2CGgBc6ivFYLig3OC39zr7wtF_0akPLGTJY3RqQ@mail.gmail.com>
References: <CAKt-OfzKGBBvPmrA1vhwh-oqXevj=QJrVLUc8J=93oDH+o33Hw@mail.gmail.com>
	<21296.3752.592995.240438@stat.math.ethz.ch>
	<CAKt-OfyKXVP2CGgBc6ivFYLig3OC39zr7wtF_0akPLGTJY3RqQ@mail.gmail.com>
Message-ID: <21296.17939.287540.684827@stat.math.ethz.ch>

>>>>> Paul Geeleher <paulgeeleher at gmail.com>
>>>>>     on Mon, 24 Mar 2014 09:37:57 -0500 writes:

    > Hey Martin,
    > Thanks for your reply. I actually have documentation, but I pasted a
    > version of the output that was created when I removed the .Rd files
    > (as I suspected they may be causing the problem). Here's the output
    > when the man files are all included (its more or less the same):


    > $ R CMD build --verbose r_package_files/
    > Warning: unknown option '--verbose'
    > * checking for file 'r_package_files/DESCRIPTION' ... OK
    > * preparing 'pRRophetic':
    > * checking DESCRIPTION meta-information ... OK
    > * installing the package to build vignettes
    > -----------------------------------
    > * installing *source* package 'pRRophetic' ...
    > ** R
    > ** data
    > ** inst
    > ** preparing package for lazy loading
    > Warning: replacing previous import by 'genefilter::Anova' when loading
    > 'pRRophetic'
    > ** help
    > *** installing help indices
    > ** building package indices
    > Error in scan(file, what, nmax, sep, dec, quote, skip, nlines, na.strings,  :
    > line 1 did not have 38 elements

I see. Now try

    options(error=recover)
    tools:::.build_packages("<your_source_package_path>/pRRophetic")


    > ERROR: installing package indices failed
    > * removing '/tmp/Rtmp6cwCZH/Rinst2f8f69e1ee/pRRophetic'


From paulgeeleher at gmail.com  Mon Mar 24 15:58:52 2014
From: paulgeeleher at gmail.com (Paul Geeleher)
Date: Mon, 24 Mar 2014 09:58:52 -0500
Subject: [Rd] "ERROR: installing package indices failed" message when
 building my package?
In-Reply-To: <21296.17939.287540.684827@stat.math.ethz.ch>
References: <CAKt-OfzKGBBvPmrA1vhwh-oqXevj=QJrVLUc8J=93oDH+o33Hw@mail.gmail.com>
	<21296.3752.592995.240438@stat.math.ethz.ch>
	<CAKt-OfyKXVP2CGgBc6ivFYLig3OC39zr7wtF_0akPLGTJY3RqQ@mail.gmail.com>
	<21296.17939.287540.684827@stat.math.ethz.ch>
Message-ID: <CAKt-OfwjneoA7LyRX3g2N0FRx+euxcSqw=69ttTc+YWjo7Yo+Q@mail.gmail.com>

Hey Martin,

The output looks similar:


> options(error=recover)
> tools:::.build_packages("pRRophetic")
* checking for file 'pRRophetic/DESCRIPTION' ... OK
* preparing 'pRRophetic':
* checking DESCRIPTION meta-information ... OK
* installing the package to build vignettes
      -----------------------------------
* installing *source* package 'pRRophetic' ...
** R
** data
** inst
** preparing package for lazy loading
Warning: replacing previous import by 'genefilter::Anova' when loading
'pRRophetic'
** help
*** installing help indices
** building package indices
Error in scan(file, what, nmax, sep, dec, quote, skip, nlines, na.strings,  :
  line 1 did not have 38 elements
ERROR: installing package indices failed
* removing '/tmp/Rtmpe59RbM/Rinst54e22da5e85a/pRRophetic'
      -----------------------------------
ERROR: package installation failed



Thanks again,

Paul.


On Mon, Mar 24, 2014 at 9:49 AM, Martin Maechler
<maechler at stat.math.ethz.ch> wrote:
>>>>>> Paul Geeleher <paulgeeleher at gmail.com>
>>>>>>     on Mon, 24 Mar 2014 09:37:57 -0500 writes:
>
>     > Hey Martin,
>     > Thanks for your reply. I actually have documentation, but I pasted a
>     > version of the output that was created when I removed the .Rd files
>     > (as I suspected they may be causing the problem). Here's the output
>     > when the man files are all included (its more or less the same):
>
>
>     > $ R CMD build --verbose r_package_files/
>     > Warning: unknown option '--verbose'
>     > * checking for file 'r_package_files/DESCRIPTION' ... OK
>     > * preparing 'pRRophetic':
>     > * checking DESCRIPTION meta-information ... OK
>     > * installing the package to build vignettes
>     > -----------------------------------
>     > * installing *source* package 'pRRophetic' ...
>     > ** R
>     > ** data
>     > ** inst
>     > ** preparing package for lazy loading
>     > Warning: replacing previous import by 'genefilter::Anova' when loading
>     > 'pRRophetic'
>     > ** help
>     > *** installing help indices
>     > ** building package indices
>     > Error in scan(file, what, nmax, sep, dec, quote, skip, nlines, na.strings,  :
>     > line 1 did not have 38 elements
>
> I see. Now try
>
>     options(error=recover)
>     tools:::.build_packages("<your_source_package_path>/pRRophetic")
>
>
>     > ERROR: installing package indices failed
>     > * removing '/tmp/Rtmp6cwCZH/Rinst2f8f69e1ee/pRRophetic'
>



-- 
Dr. Paul Geeleher, PhD
Section of Hematology-Oncology
Department of Medicine
The University of Chicago
900 E. 57th St.,
KCBD, Room 7144
Chicago, IL 60637
--
www.bioinformaticstutorials.com


From paulgeeleher at gmail.com  Mon Mar 24 17:15:24 2014
From: paulgeeleher at gmail.com (Paul Geeleher)
Date: Mon, 24 Mar 2014 11:15:24 -0500
Subject: [Rd] "ERROR: installing package indices failed" message when
 building my package?
In-Reply-To: <CAKt-OfwjneoA7LyRX3g2N0FRx+euxcSqw=69ttTc+YWjo7Yo+Q@mail.gmail.com>
References: <CAKt-OfzKGBBvPmrA1vhwh-oqXevj=QJrVLUc8J=93oDH+o33Hw@mail.gmail.com>
	<21296.3752.592995.240438@stat.math.ethz.ch>
	<CAKt-OfyKXVP2CGgBc6ivFYLig3OC39zr7wtF_0akPLGTJY3RqQ@mail.gmail.com>
	<21296.17939.287540.684827@stat.math.ethz.ch>
	<CAKt-OfwjneoA7LyRX3g2N0FRx+euxcSqw=69ttTc+YWjo7Yo+Q@mail.gmail.com>
Message-ID: <CAKt-Ofwf-Y4EDcWxTVCRY7ThQTqSP_hYuRQGopp0iUhJaBVZQw@mail.gmail.com>

Hey Martin,

So by trial and error, I figured out that the reason for this was that
there was a text file in the "data/" directory that shouldn't have
been there. Thanks for your help!

Paul.

On Mon, Mar 24, 2014 at 9:58 AM, Paul Geeleher <paulgeeleher at gmail.com> wrote:
> Hey Martin,
>
> The output looks similar:
>
>
>> options(error=recover)
>> tools:::.build_packages("pRRophetic")
> * checking for file 'pRRophetic/DESCRIPTION' ... OK
> * preparing 'pRRophetic':
> * checking DESCRIPTION meta-information ... OK
> * installing the package to build vignettes
>       -----------------------------------
> * installing *source* package 'pRRophetic' ...
> ** R
> ** data
> ** inst
> ** preparing package for lazy loading
> Warning: replacing previous import by 'genefilter::Anova' when loading
> 'pRRophetic'
> ** help
> *** installing help indices
> ** building package indices
> Error in scan(file, what, nmax, sep, dec, quote, skip, nlines, na.strings,  :
>   line 1 did not have 38 elements
> ERROR: installing package indices failed
> * removing '/tmp/Rtmpe59RbM/Rinst54e22da5e85a/pRRophetic'
>       -----------------------------------
> ERROR: package installation failed
>
>
>
> Thanks again,
>
> Paul.
>
>
> On Mon, Mar 24, 2014 at 9:49 AM, Martin Maechler
> <maechler at stat.math.ethz.ch> wrote:
>>>>>>> Paul Geeleher <paulgeeleher at gmail.com>
>>>>>>>     on Mon, 24 Mar 2014 09:37:57 -0500 writes:
>>
>>     > Hey Martin,
>>     > Thanks for your reply. I actually have documentation, but I pasted a
>>     > version of the output that was created when I removed the .Rd files
>>     > (as I suspected they may be causing the problem). Here's the output
>>     > when the man files are all included (its more or less the same):
>>
>>
>>     > $ R CMD build --verbose r_package_files/
>>     > Warning: unknown option '--verbose'
>>     > * checking for file 'r_package_files/DESCRIPTION' ... OK
>>     > * preparing 'pRRophetic':
>>     > * checking DESCRIPTION meta-information ... OK
>>     > * installing the package to build vignettes
>>     > -----------------------------------
>>     > * installing *source* package 'pRRophetic' ...
>>     > ** R
>>     > ** data
>>     > ** inst
>>     > ** preparing package for lazy loading
>>     > Warning: replacing previous import by 'genefilter::Anova' when loading
>>     > 'pRRophetic'
>>     > ** help
>>     > *** installing help indices
>>     > ** building package indices
>>     > Error in scan(file, what, nmax, sep, dec, quote, skip, nlines, na.strings,  :
>>     > line 1 did not have 38 elements
>>
>> I see. Now try
>>
>>     options(error=recover)
>>     tools:::.build_packages("<your_source_package_path>/pRRophetic")
>>
>>
>>     > ERROR: installing package indices failed
>>     > * removing '/tmp/Rtmp6cwCZH/Rinst2f8f69e1ee/pRRophetic'
>>
>
>
>
> --
> Dr. Paul Geeleher, PhD
> Section of Hematology-Oncology
> Department of Medicine
> The University of Chicago
> 900 E. 57th St.,
> KCBD, Room 7144
> Chicago, IL 60637
> --
> www.bioinformaticstutorials.com



-- 
Dr. Paul Geeleher, PhD
Section of Hematology-Oncology
Department of Medicine
The University of Chicago
900 E. 57th St.,
KCBD, Room 7144
Chicago, IL 60637
--
www.bioinformaticstutorials.com


From pgilbert902 at gmail.com  Mon Mar 24 22:11:37 2014
From: pgilbert902 at gmail.com (Paul Gilbert)
Date: Mon, 24 Mar 2014 17:11:37 -0400
Subject: [Rd] Varying results of package checks due to random seed
In-Reply-To: <20140322173237.GA22878@cs.toronto.edu>
References: <mailman.23.1395486006.17262.r-devel@r-project.org>
	<20140322173237.GA22878@cs.toronto.edu>
Message-ID: <53309F89.1060002@gmail.com>



On 03/22/2014 01:32 PM, Radford Neal wrote:
>> From: Philippe GROSJEAN <Philippe.GROSJEAN at umons.ac.be>
>>
>> ... for latest CRAN version, we have successfully installed 4999
>> packages among the 5321 CRAN package on our platform. ... It is
>> strange that a large portion of R CMD check errors on CRAN occur and
>> disappear *without any version update* of a package or any of its
>> direct or indirect dependencies! That is, a fraction of errors or
>> warnings seem to appear and disappear without any code update.
>
> Some of these are likely the result of packages running tests using
> random number generation without setting the random numbers seed, in
> which case the seed is set based on the current time and process id,
> with an obvious possibility of results varying from run to run.
>
> In the current development version of pqR (in branch 19-mods, found at
> https://github.com/radfordneal/pqR/tree/19-mods), I have implemented a
> change so that if the R_SEED environment variable is set, the random
> seed is initialized to its value, rather than from the time and
> process id.  This was motivated by exactly this problem - I can now
> just set R_SEED to something before running all the package checks.

Beware, if you are serious about reproducing things, that you really 
need to save information about the uniform and other generators you use, 
such as the normal generator. The defaults do not change often, but have 
in the past, and could in the future if something better comes along. 
There are some small utilities and examples in the package setRNG which 
can help.

Also remember that you need to beware of a side effect of the 
environment variable approach. It is great for reproducing things, as 
you would want to do in package tests, but be careful how you use it in 
functions as it may mess up the randomness if you always set the seed to 
the same starting value.

Paul

>
>     Radford Neal
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From maechler at stat.math.ethz.ch  Tue Mar 25 09:57:06 2014
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 25 Mar 2014 09:57:06 +0100
Subject: [Rd] "ERROR: installing package indices failed" message when
 building my package?
In-Reply-To: <CAKt-OfwjneoA7LyRX3g2N0FRx+euxcSqw=69ttTc+YWjo7Yo+Q@mail.gmail.com>
References: <CAKt-OfzKGBBvPmrA1vhwh-oqXevj=QJrVLUc8J=93oDH+o33Hw@mail.gmail.com>
	<21296.3752.592995.240438@stat.math.ethz.ch>
	<CAKt-OfyKXVP2CGgBc6ivFYLig3OC39zr7wtF_0akPLGTJY3RqQ@mail.gmail.com>
	<21296.17939.287540.684827@stat.math.ethz.ch>
	<CAKt-OfwjneoA7LyRX3g2N0FRx+euxcSqw=69ttTc+YWjo7Yo+Q@mail.gmail.com>
Message-ID: <21297.17634.981924.366289@stat.math.ethz.ch>

>>>>> Paul Geeleher <paulgeeleher at gmail.com>
>>>>>     on Mon, 24 Mar 2014 09:58:52 -0500 writes:

    > Hey Martin,
    > The output looks similar:

Well, of course
but I assumed you would know how to continue after the error
which should have "jumped into the debugger" with the following

    >> options(error=recover)
       ----------------------
    >> tools:::.build_packages("pRRophetic")

and I assumed you know how to inspect things once inside there ...


    > * checking for file 'pRRophetic/DESCRIPTION' ... OK
    > * preparing 'pRRophetic':
    > * checking DESCRIPTION meta-information ... OK
    > * installing the package to build vignettes
    > -----------------------------------
    > * installing *source* package 'pRRophetic' ...
    > ** R
    > ** data
    > ** inst
    > ** preparing package for lazy loading
    > Warning: replacing previous import by 'genefilter::Anova' when loading
    > 'pRRophetic'
    > ** help
    > *** installing help indices
    > ** building package indices
    > Error in scan(file, what, nmax, sep, dec, quote, skip, nlines, na.strings,  :
    > line 1 did not have 38 elements
    > ERROR: installing package indices failed
    > * removing '/tmp/Rtmpe59RbM/Rinst54e22da5e85a/pRRophetic'
    > -----------------------------------
    > ERROR: package installation failed



    > Thanks again,

    > Paul.


    > On Mon, Mar 24, 2014 at 9:49 AM, Martin Maechler
    > <maechler at stat.math.ethz.ch> wrote:
    >>>>>>> Paul Geeleher <paulgeeleher at gmail.com>
    >>>>>>> on Mon, 24 Mar 2014 09:37:57 -0500 writes:
    >> 
    >> > Hey Martin,
    >> > Thanks for your reply. I actually have documentation, but I pasted a
    >> > version of the output that was created when I removed the .Rd files
    >> > (as I suspected they may be causing the problem). Here's the output
    >> > when the man files are all included (its more or less the same):
    >> 
    >> 
    >> > $ R CMD build --verbose r_package_files/
    >> > Warning: unknown option '--verbose'
    >> > * checking for file 'r_package_files/DESCRIPTION' ... OK
    >> > * preparing 'pRRophetic':
    >> > * checking DESCRIPTION meta-information ... OK
    >> > * installing the package to build vignettes
    >> > -----------------------------------
    >> > * installing *source* package 'pRRophetic' ...
    >> > ** R
    >> > ** data
    >> > ** inst
    >> > ** preparing package for lazy loading
    >> > Warning: replacing previous import by 'genefilter::Anova' when loading
    >> > 'pRRophetic'
    >> > ** help
    >> > *** installing help indices
    >> > ** building package indices
    >> > Error in scan(file, what, nmax, sep, dec, quote, skip, nlines, na.strings,  :
    >> > line 1 did not have 38 elements
    >> 
    >> I see. Now try
    >> 
    >> options(error=recover)
    >> tools:::.build_packages("<your_source_package_path>/pRRophetic")
    >> 
    >> 
    >> > ERROR: installing package indices failed
    >> > * removing '/tmp/Rtmp6cwCZH/Rinst2f8f69e1ee/pRRophetic'
    >> 



    > -- 
    > Dr. Paul Geeleher, PhD
    > Section of Hematology-Oncology
    > Department of Medicine
    > The University of Chicago
    > 900 E. 57th St.,
    > KCBD, Room 7144
    > Chicago, IL 60637
    > --
    > www.bioinformaticstutorials.com


From kirill.mueller at ivt.baug.ethz.ch  Wed Mar 26 09:58:29 2014
From: kirill.mueller at ivt.baug.ethz.ch (=?ISO-8859-1?Q?Kirill_M=FCller?=)
Date: Wed, 26 Mar 2014 09:58:29 +0100
Subject: [Rd] NOTE when detecting mismatch in output, and codes for NOTEs,
 WARNINGs and ERRORs
Message-ID: <533296B5.4000307@ivt.baug.ethz.ch>

Dear list


It is possible to store expected output for tests and examples. From the 
manual: "If tests has a subdirectory Examples containing a file 
pkg-Ex.Rout.save, this is compared to the output file for running the 
examples when the latter are checked." And, earlier (written in the 
context of test output, but apparently applies here as well): "..., 
these two are compared, with differences being reported but not causing 
an error."

I think a NOTE would be appropriate here, in order to be able to detect 
this by only looking at the summary. Is there a reason for not flagging 
differences here?

The following is slightly related: Some compilers and static code 
analysis tools assign a numeric code to each type of error or warning 
they check for, and print it. Would that be possible to do for the 
anomalies detected by R CMD check? The most significant digit could 
denote the "severity" of the NOTE, WARNING or ERROR. This would further 
simplify (semi-)automated analysis of the output of R CMD check, e.g. in 
the context of automated tests.


Best regards

Kirill


From bodenhofer at bioinf.jku.at  Wed Mar 26 12:26:16 2014
From: bodenhofer at bioinf.jku.at (Ulrich Bodenhofer)
Date: Wed, 26 Mar 2014 12:26:16 +0100
Subject: [Rd] Conflicting definitions for function redefined as S4 generics
Message-ID: <5332B958.5020703@bioinf.jku.at>

[cross-posted to R-devel and bioc-devel]

Hi,

I am trying to implement a 'sort' method in one of the CRAN packages I 
am maintaining ('apcluster'). I started with using setMethod("sort", 
...) in my package, which worked fine. Since many users of my package 
are from the bioinformatics field, I want to ensure that my package 
works smoothly with Bioconductor. The problem is that the BiocGenerics 
package also redefines 'sort' as an S4 generic. If I load BiocGenerics 
before my package, everything is fine. If I load BiocGeneric after I 
have loaded my package, my setMethod("sort", ...) is overridden by 
BiocGenerics and does not work anymore. A simple solution would be to 
import BiocGenerics in my package, but I do not want this, since many of 
this package's users are outside the bioinformatics domain. Moreover, I 
am reluctant to include a dependency to a Bioconductor package in a CRAN 
package. I thought that maybe I could protect my setMethod("sort", ...) 
from being overridden by BiocGeneric by sealed=TRUE, but that did not 
work either. Any ideas are gratefully appreciated!

Thanks a lot,
Ulrich


------------------------------------------------------------------------
*Dr. Ulrich Bodenhofer*
Associate Professor
Institute of Bioinformatics

*Johannes Kepler University*
Altenberger Str. 69
4040 Linz, Austria

Tel. +43 732 2468 4526
Fax +43 732 2468 4539
bodenhofer at bioinf.jku.at <mailto:bodenhofer at bioinf.jku.at>
http://www.bioinf.jku.at/ <http://www.bioinf.jku.at>


From lawrence.michael at gene.com  Wed Mar 26 12:38:26 2014
From: lawrence.michael at gene.com (Michael Lawrence)
Date: Wed, 26 Mar 2014 04:38:26 -0700
Subject: [Rd] Conflicting definitions for function redefined as S4
	generics
In-Reply-To: <5332B958.5020703@bioinf.jku.at>
References: <5332B958.5020703@bioinf.jku.at>
Message-ID: <CAOQ5NyfQ8eFc2mLODPR-FB=71UuUjVZjz4srfmRSj-TOJBNmOQ@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140326/3780129f/attachment.pl>

From gmbecker at ucdavis.edu  Wed Mar 26 14:13:53 2014
From: gmbecker at ucdavis.edu (Gabriel Becker)
Date: Wed, 26 Mar 2014 06:13:53 -0700
Subject: [Rd] Conflicting definitions for function redefined as S4
	generics
In-Reply-To: <CAOQ5NyfQ8eFc2mLODPR-FB=71UuUjVZjz4srfmRSj-TOJBNmOQ@mail.gmail.com>
References: <5332B958.5020703@bioinf.jku.at>
	<CAOQ5NyfQ8eFc2mLODPR-FB=71UuUjVZjz4srfmRSj-TOJBNmOQ@mail.gmail.com>
Message-ID: <CADwqtCOPEtXcj_06=PGnrEBMOkyag+KonAM9WKiunL2M8WYp5w@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140326/8af847cf/attachment.pl>

From lawrence.michael at gene.com  Wed Mar 26 14:44:52 2014
From: lawrence.michael at gene.com (Michael Lawrence)
Date: Wed, 26 Mar 2014 06:44:52 -0700
Subject: [Rd] Conflicting definitions for function redefined as S4
	generics
In-Reply-To: <CADwqtCOPEtXcj_06=PGnrEBMOkyag+KonAM9WKiunL2M8WYp5w@mail.gmail.com>
References: <5332B958.5020703@bioinf.jku.at>
	<CAOQ5NyfQ8eFc2mLODPR-FB=71UuUjVZjz4srfmRSj-TOJBNmOQ@mail.gmail.com>
	<CADwqtCOPEtXcj_06=PGnrEBMOkyag+KonAM9WKiunL2M8WYp5w@mail.gmail.com>
Message-ID: <CAOQ5Nye=K4ekaJ=yYXX69MdddHMNLXtg5yxtrsmfSO_gkZrzvA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140326/87d37b3b/attachment.pl>

From murdoch.duncan at gmail.com  Wed Mar 26 14:48:33 2014
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Wed, 26 Mar 2014 09:48:33 -0400
Subject: [Rd] Conflicting definitions for function redefined as S4
	generics
In-Reply-To: <CADwqtCOPEtXcj_06=PGnrEBMOkyag+KonAM9WKiunL2M8WYp5w@mail.gmail.com>
References: <5332B958.5020703@bioinf.jku.at>	<CAOQ5NyfQ8eFc2mLODPR-FB=71UuUjVZjz4srfmRSj-TOJBNmOQ@mail.gmail.com>
	<CADwqtCOPEtXcj_06=PGnrEBMOkyag+KonAM9WKiunL2M8WYp5w@mail.gmail.com>
Message-ID: <5332DAB1.3090201@gmail.com>

On 26/03/2014, 9:13 AM, Gabriel Becker wrote:
> Perhaps a patch to R such that generics don't clobber each-other's method
> tables if the signatures agree? I haven't dug deeply, but simply merging
> the method tables seems like it would be safe when there are no conflicts.
>
> That way this type of multiplicity would not be a problem, though it
> wouldn't help (as it shouldn't) if the two generics didn't agree on
> signature or both carried methods for the same class signature.

I don't think R should base the decision on the signature.

There are two very different situations where this might come up.  In 
one, package A and package B might both define a generic named foo() 
that happens to have the same signature, but with nothing in common. 
That should be allowed, and should behave the same as when they both 
create functions with the same name:  it should be up to the user to 
specify which generic is being called.  If R merged the two generics 
into one, there would be chaos.

The other situation is more likely to apply to this case.  It sounds as 
though both apcluster and BiocGenerics are creating a sort() generic by 
promoting the base package S3 generic into an S4 generic.  Clearly they 
should not be creating separate generics, there's just one.

I don't know if there's something wrong with the way apcluster or 
BiocGenerics are doing things, or something wrong with the way the 
methods package is creating the generic, but it sure looks like a bug 
somewhere.

Duncan Murdoch

>
> ~G
>
>
> On Wed, Mar 26, 2014 at 4:38 AM, Michael Lawrence <lawrence.michael at gene.com
>> wrote:
>
>> The BiocGenerics package was designed to solve this issue within
>> Bioconductor. It wouldn't be the worst thing in the world to depend on the
>> simple BiocGenerics package for now, but ideally the base generics would be
>> defined higher up, perhaps in the methods package itself. Maybe someone
>> else has a more creative solution, but any sort of conditional/dynamic
>> approach would probably be too problematic in comparison.
>>
>> Michael
>>
>>
>>
>> On Wed, Mar 26, 2014 at 4:26 AM, Ulrich Bodenhofer <
>> bodenhofer at bioinf.jku.at
>>> wrote:
>>
>>> [cross-posted to R-devel and bioc-devel]
>>>
>>> Hi,
>>>
>>> I am trying to implement a 'sort' method in one of the CRAN packages I am
>>> maintaining ('apcluster'). I started with using setMethod("sort", ...) in
>>> my package, which worked fine. Since many users of my package are from
>> the
>>> bioinformatics field, I want to ensure that my package works smoothly
>> with
>>> Bioconductor. The problem is that the BiocGenerics package also redefines
>>> 'sort' as an S4 generic. If I load BiocGenerics before my package,
>>> everything is fine. If I load BiocGeneric after I have loaded my package,
>>> my setMethod("sort", ...) is overridden by BiocGenerics and does not work
>>> anymore. A simple solution would be to import BiocGenerics in my package,
>>> but I do not want this, since many of this package's users are outside
>> the
>>> bioinformatics domain. Moreover, I am reluctant to include a dependency
>> to
>>> a Bioconductor package in a CRAN package. I thought that maybe I could
>>> protect my setMethod("sort", ...) from being overridden by BiocGeneric by
>>> sealed=TRUE, but that did not work either. Any ideas are gratefully
>>> appreciated!
>>>
>>> Thanks a lot,
>>> Ulrich
>>>
>>>
>>> ------------------------------------------------------------------------
>>> *Dr. Ulrich Bodenhofer*
>>> Associate Professor
>>> Institute of Bioinformatics
>>>
>>> *Johannes Kepler University*
>>> Altenberger Str. 69
>>> 4040 Linz, Austria
>>>
>>> Tel. +43 732 2468 4526
>>> Fax +43 732 2468 4539
>>> bodenhofer at bioinf.jku.at <mailto:bodenhofer at bioinf.jku.at>
>>> http://www.bioinf.jku.at/ <http://www.bioinf.jku.at>
>>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>
>>
>>          [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>
>
>


From bodenhofer at bioinf.jku.at  Wed Mar 26 15:11:09 2014
From: bodenhofer at bioinf.jku.at (Ulrich Bodenhofer)
Date: Wed, 26 Mar 2014 15:11:09 +0100
Subject: [Rd] Conflicting definitions for function redefined as S4
	generics
In-Reply-To: <5332DAB1.3090201@gmail.com>
References: <5332B958.5020703@bioinf.jku.at>	<CAOQ5NyfQ8eFc2mLODPR-FB=71UuUjVZjz4srfmRSj-TOJBNmOQ@mail.gmail.com>
	<CADwqtCOPEtXcj_06=PGnrEBMOkyag+KonAM9WKiunL2M8WYp5w@mail.gmail.com>
	<5332DAB1.3090201@gmail.com>
Message-ID: <5332DFFD.6060602@bioinf.jku.at>

First of all, thanks for the very interesting and encouraging replies 
that have been posted so far!

Let me quickly add what I have tried up to now:

- setMethod("sort", signature("ExClust"), function(x, decreasing=FALSE, 
%...%) %...% , sealed=TRUE) without any call to setGeneric(), i.e. 
assuming that setMethod() would implicitly create an S4 generic out of 
the S3 method sort(). Note that '%...%' in the code snippet stands for 
some details that I left out.

- setGeneric("sort", def=function(x, decreasing=FALSE, ...) 
standardGeneric("sort")), i.e. consistency with the S3 generic of sort() 
in 'base', plus the call to setMethod() as shown above.

- setGeneric("sort", signature="x"), i.e. consistency with the generic's 
definition in BiocGenerics, as suggested by Martin Morgan, plus the call 
to setMethod() as shown above.

For all three trials, the result was exactly the same: (1) everything 
works nicely if I load BiocGenerics before apcluster; (2) if I load 
BiocGenerics after apcluster, apcluster's sort() function is broken and 
gives the following error:

Error in rank(x, ties.method = "min", na.last = "keep") :
   unimplemented type 'list' in 'greater'
In addition: Warning message:
In is.na(x) : is.na() applied to non-(list or vector) of type 'S4'

Obviously, sort() is dispatched to the definition made by BiocGenerics:

 > showMethods("sort", includeDefs=TRUE)
Function: sort (package BiocGenerics)
x="ANY"
function (x, decreasing = FALSE, ...)
{
     if (!is.logical(decreasing) || length(decreasing) != 1L)
         stop("'decreasing' must be a length-1 logical vector.\nDid you 
intend to set 'partial'?")
     UseMethod("sort")
}

So the method registered for class 'ExClust' is  lost if BiocGenerics is 
attached. Just for your information: all these tests have been done with 
R 3.0.2 and Bioconductor 2.13 (BiocGenerics version 0.8.0).

Thanks and best regards,
Ulrich



On 03/26/2014 02:48 PM, Duncan Murdoch wrote:
> On 26/03/2014, 9:13 AM, Gabriel Becker wrote:
>> Perhaps a patch to R such that generics don't clobber each-other's 
>> method
>> tables if the signatures agree? I haven't dug deeply, but simply merging
>> the method tables seems like it would be safe when there are no 
>> conflicts.
>>
>> That way this type of multiplicity would not be a problem, though it
>> wouldn't help (as it shouldn't) if the two generics didn't agree on
>> signature or both carried methods for the same class signature.
>
> I don't think R should base the decision on the signature.
>
> There are two very different situations where this might come up. In 
> one, package A and package B might both define a generic named foo() 
> that happens to have the same signature, but with nothing in common. 
> That should be allowed, and should behave the same as when they both 
> create functions with the same name:  it should be up to the user to 
> specify which generic is being called.  If R merged the two generics 
> into one, there would be chaos.
>
> The other situation is more likely to apply to this case.  It sounds 
> as though both apcluster and BiocGenerics are creating a sort() 
> generic by promoting the base package S3 generic into an S4 generic.  
> Clearly they should not be creating separate generics, there's just one.
>
> I don't know if there's something wrong with the way apcluster or 
> BiocGenerics are doing things, or something wrong with the way the 
> methods package is creating the generic, but it sure looks like a bug 
> somewhere.
>
> Duncan Murdoch
>
>>
>> ~G
>>
>>
>> On Wed, Mar 26, 2014 at 4:38 AM, Michael Lawrence 
>> <lawrence.michael at gene.com
>>> wrote:
>>
>>> The BiocGenerics package was designed to solve this issue within
>>> Bioconductor. It wouldn't be the worst thing in the world to depend 
>>> on the
>>> simple BiocGenerics package for now, but ideally the base generics 
>>> would be
>>> defined higher up, perhaps in the methods package itself. Maybe someone
>>> else has a more creative solution, but any sort of conditional/dynamic
>>> approach would probably be too problematic in comparison.
>>>
>>> Michael
>>>
>>>
>>>
>>> On Wed, Mar 26, 2014 at 4:26 AM, Ulrich Bodenhofer <
>>> bodenhofer at bioinf.jku.at
>>>> wrote:
>>>
>>>> [cross-posted to R-devel and bioc-devel]
>>>>
>>>> Hi,
>>>>
>>>> I am trying to implement a 'sort' method in one of the CRAN 
>>>> packages I am
>>>> maintaining ('apcluster'). I started with using setMethod("sort", 
>>>> ...) in
>>>> my package, which worked fine. Since many users of my package are from
>>> the
>>>> bioinformatics field, I want to ensure that my package works smoothly
>>> with
>>>> Bioconductor. The problem is that the BiocGenerics package also 
>>>> redefines
>>>> 'sort' as an S4 generic. If I load BiocGenerics before my package,
>>>> everything is fine. If I load BiocGeneric after I have loaded my 
>>>> package,
>>>> my setMethod("sort", ...) is overridden by BiocGenerics and does 
>>>> not work
>>>> anymore. A simple solution would be to import BiocGenerics in my 
>>>> package,
>>>> but I do not want this, since many of this package's users are outside
>>> the
>>>> bioinformatics domain. Moreover, I am reluctant to include a 
>>>> dependency
>>> to
>>>> a Bioconductor package in a CRAN package. I thought that maybe I could
>>>> protect my setMethod("sort", ...) from being overridden by 
>>>> BiocGeneric by
>>>> sealed=TRUE, but that did not work either. Any ideas are gratefully
>>>> appreciated!
>>>>
>>>> Thanks a lot,
>>>> Ulrich


From geoffjentry at hexdump.org  Wed Mar 26 16:37:53 2014
From: geoffjentry at hexdump.org (Geoff Jentry)
Date: Wed, 26 Mar 2014 08:37:53 -0700 (PDT)
Subject: [Rd] The case for freezing CRAN
In-Reply-To: <21290.60882.865159.407363@max.nulle.part>
References: <3dfcdc$fuu13r@ironport9.mayo.edu>
	<21290.60882.865159.407363@max.nulle.part>
Message-ID: <alpine.DEB.2.00.1403260811040.27911@cardinals.dreamhost.com>

On Thu, 20 Mar 2014, Dirk Eddelbuettel wrote:
> o Roger correctly notes that R scripts and packages are just one issue.
>   Compilers, libraries and the OS matter.  To me, the natural approach these
>   days would be to think of something based on Docker or Vagrant or (if you
>   must, VirtualBox).  The newer alternatives make snapshotting very cheap
>   (eg by using Linux LXC).  That approach reproduces a full environemnt as
>   best as we can while still ignoring the hardware layer (and some readers
>   may recall the infamous Pentium bug of two decades ago).

At one of my previous jobs we did effectively this (albeit in a lower tech 
fashion). Every project had its own environment, complete with the exact 
snapshot of R & packages used, etc. All scripts/code was kept in that 
environment in a versioned fashion such that at any point one could go to 
any stage of development of that paper/project's analysis and reproduce it 
exactly.

It was hugely inefficient in terms of storage, but it solved the problem 
we're discussing here. As you note, with the tools available today it'd be 
trivial to distribute that environment for people to reproduce results.


From romain at r-enthusiasts.com  Wed Mar 26 17:22:01 2014
From: romain at r-enthusiasts.com (=?windows-1252?Q?Romain_Fran=E7ois?=)
Date: Wed, 26 Mar 2014 17:22:01 +0100
Subject: [Rd] internal string comparison (Scollate)
Message-ID: <D3531066-EF05-4C8D-8B83-581C7F042D0D@r-enthusiasts.com>

Hello, 

I?d like to compare two strings internally the way R would, so I need Scollate which is not part of the authorized R api. 

So: 
 - Can Scollate (and perhaps Seql) be promoted to api ?
 - If not what are the alternatives ? Using strcmp or stroll does not seem as general as Scollate. 

Romain

PS: Here is some context: https://github.com/hadley/dplyr/issues/325

From jmc at r-project.org  Wed Mar 26 17:40:31 2014
From: jmc at r-project.org (John Chambers)
Date: Wed, 26 Mar 2014 09:40:31 -0700
Subject: [Rd] Conflicting definitions for function redefined as S4
	generics
In-Reply-To: <5332DFFD.6060602@bioinf.jku.at>
References: <5332B958.5020703@bioinf.jku.at>	<CAOQ5NyfQ8eFc2mLODPR-FB=71UuUjVZjz4srfmRSj-TOJBNmOQ@mail.gmail.com>	<CADwqtCOPEtXcj_06=PGnrEBMOkyag+KonAM9WKiunL2M8WYp5w@mail.gmail.com>	<5332DAB1.3090201@gmail.com>
	<5332DFFD.6060602@bioinf.jku.at>
Message-ID: <533302FF.4010301@r-project.org>

I haven't looked at this in detail, but my guess is the following is the 
distinction:

A simple call setGeneric("sort") makes a generic of the existing 
function _with the existing package_:

 > setGeneric("sort")
[1] "sort"
 > sort
standardGeneric for "sort" defined from package "base"

function (x, decreasing = FALSE, ...)
standardGeneric("sort")
<environment: 0x7fdf74335640>
Methods may be defined for arguments: x, decreasing
Use  showMethods("sort")  for currently available ones.

The same thing will, I believe, happen automatically if one calls 
setMethod() without a prior call to setGeneric().

What BioGenerics does is different:  it excludes the two trailing 
arguments and so creates a new generic in its own namespace.

Similarly (from the global environment in this case):

 > setGeneric("sort", signature="x")
Creating a new generic function for 'sort' in the global environment
[1] "sort"
 > sort
standardGeneric for "sort" defined from package ".GlobalEnv"

function (x, decreasing = FALSE, ...)
standardGeneric("sort")
<environment: 0x7fd33b21bb78>
Methods may be defined for arguments: x
Use  showMethods("sort")  for currently available ones.


When packages are loaded, the methods in the new package are installed 
in the generic function (in memory) that corresponds to the information 
in the methods as to generic name and package slot.

As Duncan points out, it's essential to keep functions of the same name 
but different packages distinct.  Like all R objects, generic functions 
are referred to by the combination of a name and an environment, here a 
package namespace.

Just how this sorts out into the symptoms reported I can't say, but I 
suspect this is the underlying issue.

John





On 3/26/14, 7:11 AM, Ulrich Bodenhofer wrote:
> First of all, thanks for the very interesting and encouraging replies
> that have been posted so far!
>
> Let me quickly add what I have tried up to now:
>
> - setMethod("sort", signature("ExClust"), function(x, decreasing=FALSE,
> %...%) %...% , sealed=TRUE) without any call to setGeneric(), i.e.
> assuming that setMethod() would implicitly create an S4 generic out of
> the S3 method sort(). Note that '%...%' in the code snippet stands for
> some details that I left out.
>
> - setGeneric("sort", def=function(x, decreasing=FALSE, ...)
> standardGeneric("sort")), i.e. consistency with the S3 generic of sort()
> in 'base', plus the call to setMethod() as shown above.
>
> - setGeneric("sort", signature="x"), i.e. consistency with the generic's
> definition in BiocGenerics, as suggested by Martin Morgan, plus the call
> to setMethod() as shown above.
>
> For all three trials, the result was exactly the same: (1) everything
> works nicely if I load BiocGenerics before apcluster; (2) if I load
> BiocGenerics after apcluster, apcluster's sort() function is broken and
> gives the following error:
>
> Error in rank(x, ties.method = "min", na.last = "keep") :
>    unimplemented type 'list' in 'greater'
> In addition: Warning message:
> In is.na(x) : is.na() applied to non-(list or vector) of type 'S4'
>
> Obviously, sort() is dispatched to the definition made by BiocGenerics:
>
>  > showMethods("sort", includeDefs=TRUE)
> Function: sort (package BiocGenerics)
> x="ANY"
> function (x, decreasing = FALSE, ...)
> {
>      if (!is.logical(decreasing) || length(decreasing) != 1L)
>          stop("'decreasing' must be a length-1 logical vector.\nDid you
> intend to set 'partial'?")
>      UseMethod("sort")
> }
>
> So the method registered for class 'ExClust' is  lost if BiocGenerics is
> attached. Just for your information: all these tests have been done with
> R 3.0.2 and Bioconductor 2.13 (BiocGenerics version 0.8.0).
>
> Thanks and best regards,
> Ulrich
>
>
>
> On 03/26/2014 02:48 PM, Duncan Murdoch wrote:
>> On 26/03/2014, 9:13 AM, Gabriel Becker wrote:
>>> Perhaps a patch to R such that generics don't clobber each-other's
>>> method
>>> tables if the signatures agree? I haven't dug deeply, but simply merging
>>> the method tables seems like it would be safe when there are no
>>> conflicts.
>>>
>>> That way this type of multiplicity would not be a problem, though it
>>> wouldn't help (as it shouldn't) if the two generics didn't agree on
>>> signature or both carried methods for the same class signature.
>>
>> I don't think R should base the decision on the signature.
>>
>> There are two very different situations where this might come up. In
>> one, package A and package B might both define a generic named foo()
>> that happens to have the same signature, but with nothing in common.
>> That should be allowed, and should behave the same as when they both
>> create functions with the same name:  it should be up to the user to
>> specify which generic is being called.  If R merged the two generics
>> into one, there would be chaos.
>>
>> The other situation is more likely to apply to this case.  It sounds
>> as though both apcluster and BiocGenerics are creating a sort()
>> generic by promoting the base package S3 generic into an S4 generic.
>> Clearly they should not be creating separate generics, there's just one.
>>
>> I don't know if there's something wrong with the way apcluster or
>> BiocGenerics are doing things, or something wrong with the way the
>> methods package is creating the generic, but it sure looks like a bug
>> somewhere.
>>
>> Duncan Murdoch
>>
>>>
>>> ~G
>>>
>>>
>>> On Wed, Mar 26, 2014 at 4:38 AM, Michael Lawrence
>>> <lawrence.michael at gene.com
>>>> wrote:
>>>
>>>> The BiocGenerics package was designed to solve this issue within
>>>> Bioconductor. It wouldn't be the worst thing in the world to depend
>>>> on the
>>>> simple BiocGenerics package for now, but ideally the base generics
>>>> would be
>>>> defined higher up, perhaps in the methods package itself. Maybe someone
>>>> else has a more creative solution, but any sort of conditional/dynamic
>>>> approach would probably be too problematic in comparison.
>>>>
>>>> Michael
>>>>
>>>>
>>>>
>>>> On Wed, Mar 26, 2014 at 4:26 AM, Ulrich Bodenhofer <
>>>> bodenhofer at bioinf.jku.at
>>>>> wrote:
>>>>
>>>>> [cross-posted to R-devel and bioc-devel]
>>>>>
>>>>> Hi,
>>>>>
>>>>> I am trying to implement a 'sort' method in one of the CRAN
>>>>> packages I am
>>>>> maintaining ('apcluster'). I started with using setMethod("sort",
>>>>> ...) in
>>>>> my package, which worked fine. Since many users of my package are from
>>>> the
>>>>> bioinformatics field, I want to ensure that my package works smoothly
>>>> with
>>>>> Bioconductor. The problem is that the BiocGenerics package also
>>>>> redefines
>>>>> 'sort' as an S4 generic. If I load BiocGenerics before my package,
>>>>> everything is fine. If I load BiocGeneric after I have loaded my
>>>>> package,
>>>>> my setMethod("sort", ...) is overridden by BiocGenerics and does
>>>>> not work
>>>>> anymore. A simple solution would be to import BiocGenerics in my
>>>>> package,
>>>>> but I do not want this, since many of this package's users are outside
>>>> the
>>>>> bioinformatics domain. Moreover, I am reluctant to include a
>>>>> dependency
>>>> to
>>>>> a Bioconductor package in a CRAN package. I thought that maybe I could
>>>>> protect my setMethod("sort", ...) from being overridden by
>>>>> BiocGeneric by
>>>>> sealed=TRUE, but that did not work either. Any ideas are gratefully
>>>>> appreciated!
>>>>>
>>>>> Thanks a lot,
>>>>> Ulrich
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From edd at debian.org  Wed Mar 26 17:50:41 2014
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 26 Mar 2014 11:50:41 -0500
Subject: [Rd] internal string comparison (Scollate)
In-Reply-To: <D3531066-EF05-4C8D-8B83-581C7F042D0D@r-enthusiasts.com>
References: <D3531066-EF05-4C8D-8B83-581C7F042D0D@r-enthusiasts.com>
Message-ID: <21299.1377.790733.398726@max.nulle.part>


On 26 March 2014 at 17:22, Romain Fran?ois wrote:
| I?d like to compare two strings internally the way R would, so I need Scollate which is not part of the authorized R api. 
| 
| So: 
|  - Can Scollate (and perhaps Seql) be promoted to api ?
|  - If not what are the alternatives ? Using strcmp or stroll does not seem as general as Scollate. 

I'd add a third option:

   - Put this in a new package and register the functions you want.

as I don't see a R Core change on the public vs non-public APIs anytime soon.

But we have CRAN, and we can cheaply and reliably import functions from other
packages.  Yes, it is code duplication, but it offers a layer of indirection
that permits possibles changes to such an API.

I have been meaning to put such a package up for serialization code. It will
contain a few lines of C code from base R. Junji and Ei-ji have already
placed this in their Rhpc package. I am using them (in yet another copy) in
my unfinished Redis package in order to get to serialization from C++.

Dirk

-- 
Dirk Eddelbuettel | edd at debian.org | http://dirk.eddelbuettel.com


From gmbecker at ucdavis.edu  Wed Mar 26 18:03:09 2014
From: gmbecker at ucdavis.edu (Gabriel Becker)
Date: Wed, 26 Mar 2014 10:03:09 -0700
Subject: [Rd] internal string comparison (Scollate)
In-Reply-To: <21299.1377.790733.398726@max.nulle.part>
References: <D3531066-EF05-4C8D-8B83-581C7F042D0D@r-enthusiasts.com>
	<21299.1377.790733.398726@max.nulle.part>
Message-ID: <CADwqtCOwJCCWYVEk5KHs1auzyBKr1iL1OdDxpCMwV0rAHLEdPA@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140326/7c24cd7b/attachment.pl>

From radford at cs.toronto.edu  Wed Mar 26 18:24:19 2014
From: radford at cs.toronto.edu (Radford Neal)
Date: Wed, 26 Mar 2014 13:24:19 -0400
Subject: [Rd] R-devel Digest, Vol 133, Issue 23
In-Reply-To: <mailman.15.1395658807.28229.r-devel@r-project.org>
References: <mailman.15.1395658807.28229.r-devel@r-project.org>
Message-ID: <20140326172419.GA20735@cs.toronto.edu>

> From: Richard Cotton <richierocks at gmail.com>
> 
> The rep function is very versatile, but that versatility comes at a
> cost: it takes a bit of effort to learn (and remember) its syntax.
> This is a problem, since rep is one of the first functions many
> beginners will come across.  Of the three main uses of rep, two have
> simpler alternatives.
> 
> rep(x, times = ) has rep.int
> rep(x, length.out  = ) has rep_len
> 
> I think that a rep_each function would be a worthy addition for the
> third use case
> 
> rep(x, each = )
> 
> (It might also be worth having rep_times as a synonym for rep.int.)

I think this is exactly the wrong approach.  Indeed, the aim should be
to get rid of functions like rep.int (or at least discourage their
use, even if they have to be kept for compatibility).

Why is rep_each(x,n) better than rep(x,each=n)?  There is no saving in
typing (which would be trivial anyway).  There *ought* to be no
significant difference in speed (though that seems to have been the
motive for rep.int).  Are you trying to let students learn R without
ever learning about specifying arguments by name?

And where would you stop?  How about seq_by(a,b,s) rather than having
to arduously type seq(a,b,by=s)?  Maybe we should have glm_binomial,
glm_poisson, etc. so we don't have to remember the "family" argument?
This way lies madness...

   Radford Neal


From pgilbert902 at gmail.com  Wed Mar 26 18:46:56 2014
From: pgilbert902 at gmail.com (Paul Gilbert)
Date: Wed, 26 Mar 2014 13:46:56 -0400
Subject: [Rd] NOTE when detecting mismatch in output, and codes for NOTEs,
 WARNINGs and ERRORs
In-Reply-To: <533296B5.4000307@ivt.baug.ethz.ch>
References: <533296B5.4000307@ivt.baug.ethz.ch>
Message-ID: <53331290.9020800@gmail.com>



On 03/26/2014 04:58 AM, Kirill M?ller wrote:
> Dear list
>
>
> It is possible to store expected output for tests and examples. From the
> manual: "If tests has a subdirectory Examples containing a file
> pkg-Ex.Rout.save, this is compared to the output file for running the
> examples when the latter are checked." And, earlier (written in the
> context of test output, but apparently applies here as well): "...,
> these two are compared, with differences being reported but not causing
> an error."
>
> I think a NOTE would be appropriate here, in order to be able to detect
> this by only looking at the summary. Is there a reason for not flagging
> differences here?

The problem is that differences occur too often because this is a 
comparison of characters in the output files (a diff). Any output that 
is affected by locale, node name or Internet downloads, time, host, or 
OS, is likely to cause a difference. Also, if you print results to a 
high precision you will get differences on different systems, depending 
on OS, 32 vs 64 bit, numerical libraries, etc.  A better test strategy 
when it is numerical results that you want to compare is to do a 
numerical comparison and throw an error if the result is not good, 
something like

   r <- result from your function
   rGood <- known good value
   fuzz <- 1e-12  #tolerance

   if (fuzz < max(abs(r - rGood))) stop('Test xxx failed.')

It is more work to set up, but the maintenance will be less, especially 
when you consider that your tests need to run on different OSes on CRAN.

You can also use try() and catch error codes if you want to check those.

Paul

>
> The following is slightly related: Some compilers and static code
> analysis tools assign a numeric code to each type of error or warning
> they check for, and print it. Would that be possible to do for the
> anomalies detected by R CMD check? The most significant digit could
> denote the "severity" of the NOTE, WARNING or ERROR. This would further
> simplify (semi-)automated analysis of the output of R CMD check, e.g. in
> the context of automated tests.
>
>
> Best regards
>
> Kirill
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From hpages at fhcrc.org  Wed Mar 26 18:54:38 2014
From: hpages at fhcrc.org (=?ISO-8859-1?Q?Herv=E9_Pag=E8s?=)
Date: Wed, 26 Mar 2014 10:54:38 -0700
Subject: [Rd] [Bioc-devel] Conflicting definitions for function
 redefined as S4 generics
In-Reply-To: <CAOQ5Nye=K4ekaJ=yYXX69MdddHMNLXtg5yxtrsmfSO_gkZrzvA@mail.gmail.com>
References: <5332B958.5020703@bioinf.jku.at>	<CAOQ5NyfQ8eFc2mLODPR-FB=71UuUjVZjz4srfmRSj-TOJBNmOQ@mail.gmail.com>	<CADwqtCOPEtXcj_06=PGnrEBMOkyag+KonAM9WKiunL2M8WYp5w@mail.gmail.com>
	<CAOQ5Nye=K4ekaJ=yYXX69MdddHMNLXtg5yxtrsmfSO_gkZrzvA@mail.gmail.com>
Message-ID: <5333145E.1060005@fhcrc.org>

Hi,

I agree. I can't think of an easy way to avoid this kind of clashes
between BioC and non-BioC S4 generics, other than by having things
like sort() already defined as an S4 generic in base R.

Note that, just having setMethod("sort", ...) in your package Ulrich, 
and not putting a setGeneric() statement, is usually the right thing
to do. Because then there will be no clash as long as everybody else
does the same thing. That's because if several packages with a
setMethod("sort", ...) statement are loaded, an implicit S4 generic
is created when the 1st package is loaded, and then "sort" methods
are attached to it when subsequent packages are loaded.

Unfortunately, the implicit S4 generic one gets when doing this
doesn't always have an optimum signature. For example, for sort()
we get dispatch on 'x' *and* 'decreasing':

   > sort
   standardGeneric for "sort" defined from package "base"

   function (x, decreasing = FALSE, ...)
   standardGeneric("sort")
   <environment: 0x230bf28>
   Methods may be defined for arguments: x, decreasing
   Use  showMethods("sort")  for currently available ones.

This is why in BiocGenerics we have:

   setGeneric("sort", signature="x")

The downside of this is that now if you load BiocGenerics after your
package, a new S4 generic is created for sort(), which overrides the
implicit S4 generic that was created when your package was loaded.
Of course we wouldn't need to do this in BiocGenerics if the implicit
S4 generic for sort() had the correct signature, or if this setGeneric()
statement we have in BiocGenerics was somewhere in base R.

Another reason for explicitly promoting some base R functions into
S4 generics in BiocGenerics is to have a man page for the generic.
That gives us a place to document some aspects of the S4 generic that
are not covered by the base man page. That's why BiocGenerics has
things like:

   setGeneric("nrow")

   setGeneric("relist")

The signatures of these generics is the same as the signature of
the implicit generic! But these explicit generics can be exported
and documented.

Back to the original issue: In the particular case of sort() though,
since base::sort() is an S3 generic, one possible workaround for you
is to define an S3 method for your objects.

Cheers,
H.


On 03/26/2014 06:44 AM, Michael Lawrence wrote:
> That might be worth thinking about generally, but it would still be nice to
> have the base generics pre-defined, so that people are not copy and pasting
> the definitions everywhere, hoping that they stay consistent.
>
>
> On Wed, Mar 26, 2014 at 6:13 AM, Gabriel Becker <gmbecker at ucdavis.edu>wrote:
>
>> Perhaps a patch to R such that generics don't clobber each-other's method
>> tables if the signatures agree? I haven't dug deeply, but simply merging
>> the method tables seems like it would be safe when there are no conflicts.
>>
>> That way this type of multiplicity would not be a problem, though it
>> wouldn't help (as it shouldn't) if the two generics didn't agree on
>> signature or both carried methods for the same class signature.
>>
>> ~G
>>
>>
>> On Wed, Mar 26, 2014 at 4:38 AM, Michael Lawrence <
>> lawrence.michael at gene.com> wrote:
>>
>>> The BiocGenerics package was designed to solve this issue within
>>> Bioconductor. It wouldn't be the worst thing in the world to depend on the
>>> simple BiocGenerics package for now, but ideally the base generics would
>>> be
>>> defined higher up, perhaps in the methods package itself. Maybe someone
>>> else has a more creative solution, but any sort of conditional/dynamic
>>> approach would probably be too problematic in comparison.
>>>
>>> Michael
>>>
>>>
>>>
>>> On Wed, Mar 26, 2014 at 4:26 AM, Ulrich Bodenhofer <
>>> bodenhofer at bioinf.jku.at
>>>> wrote:
>>>
>>>> [cross-posted to R-devel and bioc-devel]
>>>>
>>>> Hi,
>>>>
>>>> I am trying to implement a 'sort' method in one of the CRAN packages I
>>> am
>>>> maintaining ('apcluster'). I started with using setMethod("sort", ...)
>>> in
>>>> my package, which worked fine. Since many users of my package are from
>>> the
>>>> bioinformatics field, I want to ensure that my package works smoothly
>>> with
>>>> Bioconductor. The problem is that the BiocGenerics package also
>>> redefines
>>>> 'sort' as an S4 generic. If I load BiocGenerics before my package,
>>>> everything is fine. If I load BiocGeneric after I have loaded my
>>> package,
>>>> my setMethod("sort", ...) is overridden by BiocGenerics and does not
>>> work
>>>> anymore. A simple solution would be to import BiocGenerics in my
>>> package,
>>>> but I do not want this, since many of this package's users are outside
>>> the
>>>> bioinformatics domain. Moreover, I am reluctant to include a dependency
>>> to
>>>> a Bioconductor package in a CRAN package. I thought that maybe I could
>>>> protect my setMethod("sort", ...) from being overridden by BiocGeneric
>>> by
>>>> sealed=TRUE, but that did not work either. Any ideas are gratefully
>>>> appreciated!
>>>>
>>>> Thanks a lot,
>>>> Ulrich
>>>>
>>>>
>>>> ------------------------------------------------------------------------
>>>> *Dr. Ulrich Bodenhofer*
>>>> Associate Professor
>>>> Institute of Bioinformatics
>>>>
>>>> *Johannes Kepler University*
>>>> Altenberger Str. 69
>>>> 4040 Linz, Austria
>>>>
>>>> Tel. +43 732 2468 4526
>>>> Fax +43 732 2468 4539
>>>> bodenhofer at bioinf.jku.at <mailto:bodenhofer at bioinf.jku.at>
>>>> http://www.bioinf.jku.at/ <http://www.bioinf.jku.at>
>>>>
>>>> ______________________________________________
>>>> R-devel at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>>
>>>
>>>          [[alternative HTML version deleted]]
>>>
>>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>
>>
>>
>>
>> --
>> Gabriel Becker
>> Graduate Student
>> Statistics Department
>> University of California, Davis
>>
>
> 	[[alternative HTML version deleted]]
>
> _______________________________________________
> Bioc-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/bioc-devel
>

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fhcrc.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From romain at r-enthusiasts.com  Wed Mar 26 19:09:40 2014
From: romain at r-enthusiasts.com (=?iso-8859-1?Q?Romain_Fran=E7ois?=)
Date: Wed, 26 Mar 2014 19:09:40 +0100
Subject: [Rd] internal string comparison (Scollate)
In-Reply-To: <CADwqtCOwJCCWYVEk5KHs1auzyBKr1iL1OdDxpCMwV0rAHLEdPA@mail.gmail.com>
References: <D3531066-EF05-4C8D-8B83-581C7F042D0D@r-enthusiasts.com>
	<21299.1377.790733.398726@max.nulle.part>
	<CADwqtCOwJCCWYVEk5KHs1auzyBKr1iL1OdDxpCMwV0rAHLEdPA@mail.gmail.com>
Message-ID: <C2D5C840-B9A6-430C-B16F-8ED59460516B@r-enthusiasts.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140326/c7c61263/attachment.pl>

From pdalgd at gmail.com  Wed Mar 26 23:00:02 2014
From: pdalgd at gmail.com (peter dalgaard)
Date: Wed, 26 Mar 2014 23:00:02 +0100
Subject: [Rd] R-devel Digest, Vol 133, Issue 23
In-Reply-To: <20140326172419.GA20735@cs.toronto.edu>
References: <mailman.15.1395658807.28229.r-devel@r-project.org>
	<20140326172419.GA20735@cs.toronto.edu>
Message-ID: <1AF47B57-FB82-49FC-8967-66EA1E0D4FD5@gmail.com>


On 26 Mar 2014, at 18:24 , Radford Neal <radford at cs.toronto.edu> wrote:

>> From: Richard Cotton <richierocks at gmail.com>
>> 
>> The rep function is very versatile, but that versatility comes at a
>> cost: it takes a bit of effort to learn (and remember) its syntax.
>> This is a problem, since rep is one of the first functions many
>> beginners will come across.  Of the three main uses of rep, two have
>> simpler alternatives.
>> 
>> rep(x, times = ) has rep.int
>> rep(x, length.out  = ) has rep_len
>> 
>> I think that a rep_each function would be a worthy addition for the
>> third use case
>> 
>> rep(x, each = )
>> 
>> (It might also be worth having rep_times as a synonym for rep.int.)
> 
> I think this is exactly the wrong approach.  Indeed, the aim should be
> to get rid of functions like rep.int (or at least discourage their
> use, even if they have to be kept for compatibility).
> 
> Why is rep_each(x,n) better than rep(x,each=n)?  There is no saving in
> typing (which would be trivial anyway).  There *ought* to be no
> significant difference in speed (though that seems to have been the
> motive for rep.int).  Are you trying to let students learn R without
> ever learning about specifying arguments by name?
> 
> And where would you stop?  How about seq_by(a,b,s) rather than having
> to arduously type seq(a,b,by=s)?  Maybe we should have glm_binomial,
> glm_poisson, etc. so we don't have to remember the "family" argument?
> This way lies madness...

Spot on. 

Well, maybe a slight disagreement: In a weakly typed language like R, you will always have performance losses due to type testing and dispatching, and no compiler/interpreter is intelligent enough to predict the types so that this can be avoided. Some amout of hinting is needed for reliable speedups, either by having special functions for simple cases (allowed to make assumptions on their inputs), or some sort of #pragma-like construction.

Actually, rep.int seems to be a poor example of this since the speedup is pretty negligible unless you do huge amounts of short replicates. I expect that the S-PLUS compatibility was the main reason to have it. Case in point:

> system.time(for(i in 1:10000000) rep("a",10))
   user  system elapsed 
 16.721   0.125  19.037 
> system.time(for(i in 1:10000000) rep.int("a",10))
   user  system elapsed 
 14.356   0.050  14.611 
> system.time(for(i in 1:1000000) rep("a",1000))
   user  system elapsed 
 11.655   2.157  14.263 
> system.time(for(i in 1:1000000) rep.int("a",1000))
   user  system elapsed 
 10.957   1.708  12.917 

For more spectacular speedups compare seq(1,10) to seq_len(10) or even just to 1:10. Then again, the slowdown in seq() is so large that it is hard to believe it to be completely unavoidable.
  

-- 
Peter Dalgaard, Professor,
Center for Statistics, Copenhagen Business School
Solbjerg Plads 3, 2000 Frederiksberg, Denmark
Phone: (+45)38153501
Email: pd.mes at cbs.dk  Priv: PDalgd at gmail.com


From ripley at stats.ox.ac.uk  Wed Mar 26 23:43:53 2014
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 26 Mar 2014 22:43:53 +0000
Subject: [Rd] R-devel Digest, Vol 133, Issue 23
In-Reply-To: <1AF47B57-FB82-49FC-8967-66EA1E0D4FD5@gmail.com>
References: <mailman.15.1395658807.28229.r-devel@r-project.org>	<20140326172419.GA20735@cs.toronto.edu>
	<1AF47B57-FB82-49FC-8967-66EA1E0D4FD5@gmail.com>
Message-ID: <53335829.2090808@stats.ox.ac.uk>

On 26/03/2014 22:00, peter dalgaard wrote:
>
> On 26 Mar 2014, at 18:24 , Radford Neal <radford at cs.toronto.edu> wrote:
>
>>> From: Richard Cotton <richierocks at gmail.com>
>>>
>>> The rep function is very versatile, but that versatility comes at a
>>> cost: it takes a bit of effort to learn (and remember) its syntax.
>>> This is a problem, since rep is one of the first functions many
>>> beginners will come across.  Of the three main uses of rep, two have
>>> simpler alternatives.
>>>
>>> rep(x, times = ) has rep.int
>>> rep(x, length.out  = ) has rep_len
>>>
>>> I think that a rep_each function would be a worthy addition for the
>>> third use case
>>>
>>> rep(x, each = )
>>>
>>> (It might also be worth having rep_times as a synonym for rep.int.)
>>
>> I think this is exactly the wrong approach.  Indeed, the aim should be
>> to get rid of functions like rep.int (or at least discourage their
>> use, even if they have to be kept for compatibility).
>>
>> Why is rep_each(x,n) better than rep(x,each=n)?  There is no saving in
>> typing (which would be trivial anyway).  There *ought* to be no
>> significant difference in speed (though that seems to have been the
>> motive for rep.int).  Are you trying to let students learn R without
>> ever learning about specifying arguments by name?
>>
>> And where would you stop?  How about seq_by(a,b,s) rather than having
>> to arduously type seq(a,b,by=s)?  Maybe we should have glm_binomial,
>> glm_poisson, etc. so we don't have to remember the "family" argument?
>> This way lies madness...
>
> Spot on.
>
> Well, maybe a slight disagreement: In a weakly typed language like R, you will always have performance losses due to type testing and dispatching, and no compiler/interpreter is intelligent enough to predict the types so that this can be avoided. Some amout of hinting is needed for reliable speedups, either by having special functions for simple cases (allowed to make assumptions on their inputs), or some sort of #pragma-like construction.
>
> Actually, rep.int seems to be a poor example of this since the speedup is pretty negligible unless you do huge amounts of short replicates. I expect that the S-PLUS compatibility was the main reason to have it. Case in point:

As the help says:

      Function ?rep.int? is a simple case handled by internal code, and
      provided as a separate function partly for S compatibility and
      partly for speed (especially when names can be dropped).

E.g.

 > a <- letters[1:10]; names(a) <- a
 > system.time(for(i in 1:1000000) rep.int(a,10))
    user  system elapsed
   1.568   0.001   1.574
 > system.time(for(i in 1:1000000) rep(a,10))
    user  system elapsed
   2.804   0.002   2.816

There are also rare occasions where it is useful to use rep.int to 
circumvent method dispatch.

Note that rep() was an interpreted function when that comment was first 
written, and the gap was much larger then.  (Nor was it byte-compiled, 
nor generic.)  For the version of rep in R 0.65.1:

 > system.time(for(i in 1:1000000) rep("a",10))
    user  system elapsed
   1.612   0.000   1.616

vs the current

 > system.time(for(i in 1:1000000) rep("a",10))
    user  system elapsed
   0.518   0.000   0.519
 > system.time(for(i in 1:1000000) rep.int("a",10))
    user  system elapsed
   0.471   0.000   0.473

>> system.time(for(i in 1:10000000) rep("a",10))
>     user  system elapsed
>   16.721   0.125  19.037
>> system.time(for(i in 1:10000000) rep.int("a",10))
>     user  system elapsed
>   14.356   0.050  14.611
>> system.time(for(i in 1:1000000) rep("a",1000))
>     user  system elapsed
>   11.655   2.157  14.263
>> system.time(for(i in 1:1000000) rep.int("a",1000))
>     user  system elapsed
>   10.957   1.708  12.917

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (se3lf)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From hpages at fhcrc.org  Thu Mar 27 00:43:14 2014
From: hpages at fhcrc.org (=?ISO-8859-1?Q?Herv=E9_Pag=E8s?=)
Date: Wed, 26 Mar 2014 16:43:14 -0700
Subject: [Rd] R-devel Digest, Vol 133, Issue 23
In-Reply-To: <20140326172419.GA20735@cs.toronto.edu>
References: <mailman.15.1395658807.28229.r-devel@r-project.org>
	<20140326172419.GA20735@cs.toronto.edu>
Message-ID: <53336612.4060400@fhcrc.org>

Hi,

On 03/26/2014 10:24 AM, Radford Neal wrote:
>> From: Richard Cotton <richierocks at gmail.com>
>>
>> The rep function is very versatile, but that versatility comes at a
>> cost: it takes a bit of effort to learn (and remember) its syntax.
>> This is a problem, since rep is one of the first functions many
>> beginners will come across.  Of the three main uses of rep, two have
>> simpler alternatives.
>>
>> rep(x, times = ) has rep.int
>> rep(x, length.out  = ) has rep_len
>>
>> I think that a rep_each function would be a worthy addition for the
>> third use case
>>
>> rep(x, each = )
>>
>> (It might also be worth having rep_times as a synonym for rep.int.)
>
> I think this is exactly the wrong approach.  Indeed, the aim should be
> to get rid of functions like rep.int (or at least discourage their
> use, even if they have to be kept for compatibility).
>
> Why is rep_each(x,n) better than rep(x,each=n)?

According to the NEWS file, it seems that R core felt that having
rep_len() was a good idea.

   There is a new function rep_len() analogous to rep.int() for when
   speed is required (and names are not).

Now one might wonder (and your students might wonder too) why having
rep_each() "for when speed is required (and names are not)" is not a
good idea.

By having rep_len(), rep_each(), and rep_times(), the 3 extra arguments
in rep(x, ...) would be covered. Plus, when I use tab completion after
typing rep_, I would get a nice summary and would be able to quickly
choose. Right now, when I do this, one function is missing, and one has
a misleading name. So I'd rather have no specialized function at all,
or have the 3. Would be cleaner and less confusing than the current
situation.

Cheers,
H.


> There is no saving in
> typing (which would be trivial anyway).  There *ought* to be no
> significant difference in speed (though that seems to have been the
> motive for rep.int).  Are you trying to let students learn R without
> ever learning about specifying arguments by name?
>
> And where would you stop?  How about seq_by(a,b,s) rather than having
> to arduously type seq(a,b,by=s)?  Maybe we should have glm_binomial,
> glm_poisson, etc. so we don't have to remember the "family" argument?
> This way lies madness...
>
>     Radford Neal
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fhcrc.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From bodenhofer at bioinf.jku.at  Thu Mar 27 10:13:31 2014
From: bodenhofer at bioinf.jku.at (Ulrich Bodenhofer)
Date: Thu, 27 Mar 2014 10:13:31 +0100
Subject: [Rd] Conflicting definitions for function redefined as S4
	generics
In-Reply-To: <CAOQ5Nye=K4ekaJ=yYXX69MdddHMNLXtg5yxtrsmfSO_gkZrzvA@mail.gmail.com>
References: <5332B958.5020703@bioinf.jku.at>	<CAOQ5NyfQ8eFc2mLODPR-FB=71UuUjVZjz4srfmRSj-TOJBNmOQ@mail.gmail.com>	<CADwqtCOPEtXcj_06=PGnrEBMOkyag+KonAM9WKiunL2M8WYp5w@mail.gmail.com>
	<CAOQ5Nye=K4ekaJ=yYXX69MdddHMNLXtg5yxtrsmfSO_gkZrzvA@mail.gmail.com>
Message-ID: <5333EBBB.80007@bioinf.jku.at>

I fully agree, Michael, that this would be a great thing to have! I have 
often wondered why R and the standard packages are still sticking so 
much to the old-style S3 flavor though S4 is part of standard R. I 
acknowledge that backward compatibility is important, but, as far as I 
got it, redefining a function or S3 generic as an S4 generic should not 
harm existing functionality (if done properly). If it turns out not to 
be a good option to do this in the base package, why not as part of the 
methods package? That will leave existing functionality of base 
unchanged and will provide a clean situation to all users/packages using S4.

This should not create a compatibility problem on the Bioconductor side 
either, since Bioconductor releases are explicitly bound to specific R 
versions. Once again: I fully support this idea (not only for sort(), 
but also for a wide range of other functions), though, not being an R 
core team member, I do not really feel in the position to demand such a 
fundamental change.

For the time being, it seems I have three options:

1) not supplying the sort() function yet (it is not yet in the release, 
but only in my internal devel version)
2) including a dependency to BiocGenerics
3) leaving the problem open, mentioning in the documentation that users 
who want to use apcluster in conjunction with Bioconductor should load 
BiocGenerics first

As far as I got it, there seems to be no other clean way to get rid of 
the problem, right?

Best regards,
Ulrich


On 03/26/2014 02:44 PM, Michael Lawrence wrote:
> That might be worth thinking about generally, but it would still be 
> nice to have the base generics pre-defined, so that people are not 
> copy and pasting the definitions everywhere, hoping that they stay 
> consistent.
>
>
> On Wed, Mar 26, 2014 at 6:13 AM, Gabriel Becker <gmbecker at ucdavis.edu 
> <mailto:gmbecker at ucdavis.edu>> wrote:
>
>     Perhaps a patch to R such that generics don't clobber each-other's
>     method tables if the signatures agree? I haven't dug deeply, but
>     simply merging the method tables seems like it would be safe when
>     there are no conflicts.
>
>     That way this type of multiplicity would not be a problem, though
>     it wouldn't help (as it shouldn't) if the two generics didn't
>     agree on signature or both carried methods for the same class
>     signature.
>
>     ~G
>
>
>     On Wed, Mar 26, 2014 at 4:38 AM, Michael Lawrence
>     <lawrence.michael at gene.com <mailto:lawrence.michael at gene.com>> wrote:
>
>         The BiocGenerics package was designed to solve this issue within
>         Bioconductor. It wouldn't be the worst thing in the world to
>         depend on the
>         simple BiocGenerics package for now, but ideally the base
>         generics would be
>         defined higher up, perhaps in the methods package itself.
>         Maybe someone
>         else has a more creative solution, but any sort of
>         conditional/dynamic
>         approach would probably be too problematic in comparison.
>
>         Michael
>
>
>
>         On Wed, Mar 26, 2014 at 4:26 AM, Ulrich Bodenhofer
>         <bodenhofer at bioinf.jku.at <mailto:bodenhofer at bioinf.jku.at>
>         > wrote:
>
>         > [cross-posted to R-devel and bioc-devel]
>         >
>         > Hi,
>         >
>         > I am trying to implement a 'sort' method in one of the CRAN
>         packages I am
>         > maintaining ('apcluster'). I started with using
>         setMethod("sort", ...) in
>         > my package, which worked fine. Since many users of my
>         package are from the
>         > bioinformatics field, I want to ensure that my package works
>         smoothly with
>         > Bioconductor. The problem is that the BiocGenerics package
>         also redefines
>         > 'sort' as an S4 generic. If I load BiocGenerics before my
>         package,
>         > everything is fine. If I load BiocGeneric after I have
>         loaded my package,
>         > my setMethod("sort", ...) is overridden by BiocGenerics and
>         does not work
>         > anymore. A simple solution would be to import BiocGenerics
>         in my package,
>         > but I do not want this, since many of this package's users
>         are outside the
>         > bioinformatics domain. Moreover, I am reluctant to include a
>         dependency to
>         > a Bioconductor package in a CRAN package. I thought that
>         maybe I could
>         > protect my setMethod("sort", ...) from being overridden by
>         BiocGeneric by
>         > sealed=TRUE, but that did not work either. Any ideas are
>         gratefully
>         > appreciated!
>         >
>         > Thanks a lot,
>         > Ulrich
>         >
>


From renaud at mancala.cbio.uct.ac.za  Thu Mar 27 16:28:07 2014
From: renaud at mancala.cbio.uct.ac.za (Renaud Gaujoux)
Date: Thu, 27 Mar 2014 17:28:07 +0200
Subject: [Rd] Proxy settings not honoured anymore
Message-ID: <CAHavPHHKXmEfk-2DryWwd0tE+BXAdYzq+Aqay9hxU-tf3x0Xwg@mail.gmail.com>

Hi,

it seems that my proxy settings are not picked up by the R console any
longer, although the environment variable http_proxy is set and
exported.
Is anybody experiencing this issue as well?
Thank you.

Bests,
Renaud

# System info (from R --vanilla)
> sessionInfo()
R version 3.0.3 (2014-03-06)
Platform: x86_64-pc-linux-gnu (64-bit)

locale:
 [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
 [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
 [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8
 [7] LC_PAPER=en_US.UTF-8       LC_NAME=C
 [9] LC_ADDRESS=C               LC_TELEPHONE=C
[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base


From renaud at mancala.cbio.uct.ac.za  Thu Mar 27 16:38:33 2014
From: renaud at mancala.cbio.uct.ac.za (Renaud Gaujoux)
Date: Thu, 27 Mar 2014 17:38:33 +0200
Subject: [Rd] Proxy settings not honoured anymore
In-Reply-To: <CAHavPHHKXmEfk-2DryWwd0tE+BXAdYzq+Aqay9hxU-tf3x0Xwg@mail.gmail.com>
References: <CAHavPHHKXmEfk-2DryWwd0tE+BXAdYzq+Aqay9hxU-tf3x0Xwg@mail.gmail.com>
Message-ID: <CAHavPHGZcntAsix8M=43JbMcD-hmMoVtr+SkRaW3UriQO3yVhA@mail.gmail.com>

All right, just ignore this silly post, things magically came back
into place... :|

On 27 March 2014 17:28, Renaud Gaujoux <renaud at mancala.cbio.uct.ac.za> wrote:
> Hi,
>
> it seems that my proxy settings are not picked up by the R console any
> longer, although the environment variable http_proxy is set and
> exported.
> Is anybody experiencing this issue as well?
> Thank you.
>
> Bests,
> Renaud
>
> # System info (from R --vanilla)
>> sessionInfo()
> R version 3.0.3 (2014-03-06)
> Platform: x86_64-pc-linux-gnu (64-bit)
>
> locale:
>  [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C
>  [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8
>  [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8
>  [7] LC_PAPER=en_US.UTF-8       LC_NAME=C
>  [9] LC_ADDRESS=C               LC_TELEPHONE=C
> [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C
>
> attached base packages:
> [1] stats     graphics  grDevices utils     datasets  methods   base


From jmc at r-project.org  Thu Mar 27 18:06:14 2014
From: jmc at r-project.org (John Chambers)
Date: Thu, 27 Mar 2014 10:06:14 -0700
Subject: [Rd] Conflicting definitions for function redefined as S4
	generics
In-Reply-To: <5333EBBB.80007@bioinf.jku.at>
References: <5332B958.5020703@bioinf.jku.at>	<CAOQ5NyfQ8eFc2mLODPR-FB=71UuUjVZjz4srfmRSj-TOJBNmOQ@mail.gmail.com>	<CADwqtCOPEtXcj_06=PGnrEBMOkyag+KonAM9WKiunL2M8WYp5w@mail.gmail.com>
	<CAOQ5Nye=K4ekaJ=yYXX69MdddHMNLXtg5yxtrsmfSO_gkZrzvA@mail.gmail.com>
	<5333EBBB.80007@bioinf.jku.at>
Message-ID: <85DEF0D0-FBFA-4496-870B-FBABEEA08DEF@r-project.org>

I'm sympathetic to the problem.  But, whatever my opinion, it's not likely that the basic R paradigm with respect to S3/S4 methods will change much, and certainly not for a year.

Meanwhile, let's remember the essential idea.  Every function has a corresponding implicit generic form (well, partially ignoring primitives for the moment).

The standard approach to defining methods for a non-generic is either to just use setMethod() or to use the simple form of setGeneric("foo").  EIther way, the generic function and the method refer to the package from which foo() came.  If all packages defining methods for foo() follow this pattern, the result is a single table of methods in the generic for foo() during an R session.

See the "Basic Use" section of ?setGeneric. 

The difficulties come when some package sets up a _different_ version of foo() as a generic.  This becomes a separate, incompatible, generic function. When still other packages are involved, there is a potential for methods to be divided among multiple tables.  If people feel they need to do this, they have to sort out the consequences.  Ideally, in my opinion, they should rename the function so users can choose which version to call.

Finally, even if we managed to incorporate implicit generic versions of functions in base (don't hold your breath), it's extremely unlikely that these would lop off arguments from the non-generic function.  There is no real reason to prohibit some formal arguments from being in the formal arguments to the generic. 

In a few cases, some arguments may be prohibited from being dispatched on, e.g., if those arguments have to be evaluated in a non-standard way, and that is handled by the signature= argument.  In any case, the implicitGeneric() mechanism is designed to handle such issues.  Meaning that package programming should be fairly immune to change, so long as the Basic Use is followed.

Summary: So long as the recommendations of Basic Use are followed, I don't see the problem of multiple versions.  There are other aspects of the non-inclusion of S4 in the R paradigm that cause difficulties, but basic use approach should provide one consistent table of methods for each function.

John


On Mar 27, 2014, at 2:13 AM, Ulrich Bodenhofer <bodenhofer at bioinf.jku.at> wrote:

> I fully agree, Michael, that this would be a great thing to have! I have often wondered why R and the standard packages are still sticking so much to the old-style S3 flavor though S4 is part of standard R. I acknowledge that backward compatibility is important, but, as far as I got it, redefining a function or S3 generic as an S4 generic should not harm existing functionality (if done properly). If it turns out not to be a good option to do this in the base package, why not as part of the methods package? That will leave existing functionality of base unchanged and will provide a clean situation to all users/packages using S4.
> 
> This should not create a compatibility problem on the Bioconductor side either, since Bioconductor releases are explicitly bound to specific R versions. Once again: I fully support this idea (not only for sort(), but also for a wide range of other functions), though, not being an R core team member, I do not really feel in the position to demand such a fundamental change.
> 
> For the time being, it seems I have three options:
> 
> 1) not supplying the sort() function yet (it is not yet in the release, but only in my internal devel version)
> 2) including a dependency to BiocGenerics
> 3) leaving the problem open, mentioning in the documentation that users who want to use apcluster in conjunction with Bioconductor should load BiocGenerics first
> 
> As far as I got it, there seems to be no other clean way to get rid of the problem, right?
> 
> Best regards,
> Ulrich
> 
> 
> On 03/26/2014 02:44 PM, Michael Lawrence wrote:
>> That might be worth thinking about generally, but it would still be nice to have the base generics pre-defined, so that people are not copy and pasting the definitions everywhere, hoping that they stay consistent.
>> 
>> 
>> On Wed, Mar 26, 2014 at 6:13 AM, Gabriel Becker <gmbecker at ucdavis.edu <mailto:gmbecker at ucdavis.edu>> wrote:
>> 
>>    Perhaps a patch to R such that generics don't clobber each-other's
>>    method tables if the signatures agree? I haven't dug deeply, but
>>    simply merging the method tables seems like it would be safe when
>>    there are no conflicts.
>> 
>>    That way this type of multiplicity would not be a problem, though
>>    it wouldn't help (as it shouldn't) if the two generics didn't
>>    agree on signature or both carried methods for the same class
>>    signature.
>> 
>>    ~G
>> 
>> 
>>    On Wed, Mar 26, 2014 at 4:38 AM, Michael Lawrence
>>    <lawrence.michael at gene.com <mailto:lawrence.michael at gene.com>> wrote:
>> 
>>        The BiocGenerics package was designed to solve this issue within
>>        Bioconductor. It wouldn't be the worst thing in the world to
>>        depend on the
>>        simple BiocGenerics package for now, but ideally the base
>>        generics would be
>>        defined higher up, perhaps in the methods package itself.
>>        Maybe someone
>>        else has a more creative solution, but any sort of
>>        conditional/dynamic
>>        approach would probably be too problematic in comparison.
>> 
>>        Michael
>> 
>> 
>> 
>>        On Wed, Mar 26, 2014 at 4:26 AM, Ulrich Bodenhofer
>>        <bodenhofer at bioinf.jku.at <mailto:bodenhofer at bioinf.jku.at>
>>        > wrote:
>> 
>>        > [cross-posted to R-devel and bioc-devel]
>>        >
>>        > Hi,
>>        >
>>        > I am trying to implement a 'sort' method in one of the CRAN
>>        packages I am
>>        > maintaining ('apcluster'). I started with using
>>        setMethod("sort", ...) in
>>        > my package, which worked fine. Since many users of my
>>        package are from the
>>        > bioinformatics field, I want to ensure that my package works
>>        smoothly with
>>        > Bioconductor. The problem is that the BiocGenerics package
>>        also redefines
>>        > 'sort' as an S4 generic. If I load BiocGenerics before my
>>        package,
>>        > everything is fine. If I load BiocGeneric after I have
>>        loaded my package,
>>        > my setMethod("sort", ...) is overridden by BiocGenerics and
>>        does not work
>>        > anymore. A simple solution would be to import BiocGenerics
>>        in my package,
>>        > but I do not want this, since many of this package's users
>>        are outside the
>>        > bioinformatics domain. Moreover, I am reluctant to include a
>>        dependency to
>>        > a Bioconductor package in a CRAN package. I thought that
>>        maybe I could
>>        > protect my setMethod("sort", ...) from being overridden by
>>        BiocGeneric by
>>        > sealed=TRUE, but that did not work either. Any ideas are
>>        gratefully
>>        > appreciated!
>>        >
>>        > Thanks a lot,
>>        > Ulrich
>>        >
>> 
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From hpages at fhcrc.org  Thu Mar 27 18:31:09 2014
From: hpages at fhcrc.org (=?ISO-8859-1?Q?Herv=E9_Pag=E8s?=)
Date: Thu, 27 Mar 2014 10:31:09 -0700
Subject: [Rd] [Bioc-devel] Conflicting definitions for function
 redefined as S4 generics
In-Reply-To: <5333EBBB.80007@bioinf.jku.at>
References: <5332B958.5020703@bioinf.jku.at>	<CAOQ5NyfQ8eFc2mLODPR-FB=71UuUjVZjz4srfmRSj-TOJBNmOQ@mail.gmail.com>	<CADwqtCOPEtXcj_06=PGnrEBMOkyag+KonAM9WKiunL2M8WYp5w@mail.gmail.com>	<CAOQ5Nye=K4ekaJ=yYXX69MdddHMNLXtg5yxtrsmfSO_gkZrzvA@mail.gmail.com>
	<5333EBBB.80007@bioinf.jku.at>
Message-ID: <5334605D.5080809@fhcrc.org>



On 03/27/2014 02:13 AM, Ulrich Bodenhofer wrote:
> I fully agree, Michael, that this would be a great thing to have! I have
> often wondered why R and the standard packages are still sticking so
> much to the old-style S3 flavor though S4 is part of standard R. I
> acknowledge that backward compatibility is important, but, as far as I
> got it, redefining a function or S3 generic as an S4 generic should not
> harm existing functionality (if done properly). If it turns out not to
> be a good option to do this in the base package, why not as part of the
> methods package? That will leave existing functionality of base
> unchanged and will provide a clean situation to all users/packages using
> S4.
>
> This should not create a compatibility problem on the Bioconductor side
> either, since Bioconductor releases are explicitly bound to specific R
> versions. Once again: I fully support this idea (not only for sort(),
> but also for a wide range of other functions), though, not being an R
> core team member, I do not really feel in the position to demand such a
> fundamental change.
>
> For the time being, it seems I have three options:
>
> 1) not supplying the sort() function yet (it is not yet in the release,
> but only in my internal devel version)
> 2) including a dependency to BiocGenerics
> 3) leaving the problem open, mentioning in the documentation that users
> who want to use apcluster in conjunction with Bioconductor should load
> BiocGenerics first

4) define an S3 method, as mentioned in my previous post

H.

>
> As far as I got it, there seems to be no other clean way to get rid of
> the problem, right?
>
> Best regards,
> Ulrich
>
>
> On 03/26/2014 02:44 PM, Michael Lawrence wrote:
>> That might be worth thinking about generally, but it would still be
>> nice to have the base generics pre-defined, so that people are not
>> copy and pasting the definitions everywhere, hoping that they stay
>> consistent.
>>
>>
>> On Wed, Mar 26, 2014 at 6:13 AM, Gabriel Becker <gmbecker at ucdavis.edu
>> <mailto:gmbecker at ucdavis.edu>> wrote:
>>
>>     Perhaps a patch to R such that generics don't clobber each-other's
>>     method tables if the signatures agree? I haven't dug deeply, but
>>     simply merging the method tables seems like it would be safe when
>>     there are no conflicts.
>>
>>     That way this type of multiplicity would not be a problem, though
>>     it wouldn't help (as it shouldn't) if the two generics didn't
>>     agree on signature or both carried methods for the same class
>>     signature.
>>
>>     ~G
>>
>>
>>     On Wed, Mar 26, 2014 at 4:38 AM, Michael Lawrence
>>     <lawrence.michael at gene.com <mailto:lawrence.michael at gene.com>> wrote:
>>
>>         The BiocGenerics package was designed to solve this issue within
>>         Bioconductor. It wouldn't be the worst thing in the world to
>>         depend on the
>>         simple BiocGenerics package for now, but ideally the base
>>         generics would be
>>         defined higher up, perhaps in the methods package itself.
>>         Maybe someone
>>         else has a more creative solution, but any sort of
>>         conditional/dynamic
>>         approach would probably be too problematic in comparison.
>>
>>         Michael
>>
>>
>>
>>         On Wed, Mar 26, 2014 at 4:26 AM, Ulrich Bodenhofer
>>         <bodenhofer at bioinf.jku.at <mailto:bodenhofer at bioinf.jku.at>
>>         > wrote:
>>
>>         > [cross-posted to R-devel and bioc-devel]
>>         >
>>         > Hi,
>>         >
>>         > I am trying to implement a 'sort' method in one of the CRAN
>>         packages I am
>>         > maintaining ('apcluster'). I started with using
>>         setMethod("sort", ...) in
>>         > my package, which worked fine. Since many users of my
>>         package are from the
>>         > bioinformatics field, I want to ensure that my package works
>>         smoothly with
>>         > Bioconductor. The problem is that the BiocGenerics package
>>         also redefines
>>         > 'sort' as an S4 generic. If I load BiocGenerics before my
>>         package,
>>         > everything is fine. If I load BiocGeneric after I have
>>         loaded my package,
>>         > my setMethod("sort", ...) is overridden by BiocGenerics and
>>         does not work
>>         > anymore. A simple solution would be to import BiocGenerics
>>         in my package,
>>         > but I do not want this, since many of this package's users
>>         are outside the
>>         > bioinformatics domain. Moreover, I am reluctant to include a
>>         dependency to
>>         > a Bioconductor package in a CRAN package. I thought that
>>         maybe I could
>>         > protect my setMethod("sort", ...) from being overridden by
>>         BiocGeneric by
>>         > sealed=TRUE, but that did not work either. Any ideas are
>>         gratefully
>>         > appreciated!
>>         >
>>         > Thanks a lot,
>>         > Ulrich
>>         >
>>
>
> _______________________________________________
> Bioc-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/bioc-devel

-- 
Herv? Pag?s

Program in Computational Biology
Division of Public Health Sciences
Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N, M1-B514
P.O. Box 19024
Seattle, WA 98109-1024

E-mail: hpages at fhcrc.org
Phone:  (206) 667-5791
Fax:    (206) 667-1319


From edd at debian.org  Thu Mar 27 19:08:25 2014
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 27 Mar 2014 13:08:25 -0500
Subject: [Rd] internal string comparison (Scollate)
In-Reply-To: <C2D5C840-B9A6-430C-B16F-8ED59460516B@r-enthusiasts.com>
References: <D3531066-EF05-4C8D-8B83-581C7F042D0D@r-enthusiasts.com>
	<21299.1377.790733.398726@max.nulle.part>
	<CADwqtCOwJCCWYVEk5KHs1auzyBKr1iL1OdDxpCMwV0rAHLEdPA@mail.gmail.com>
	<C2D5C840-B9A6-430C-B16F-8ED59460516B@r-enthusiasts.com>
Message-ID: <21300.26905.645966.939976@max.nulle.part>


On 26 March 2014 at 19:09, Romain Fran?ois wrote:
| That?s one part of the problem. Indeed I?d rather use something rather than
| copy and paste it and run the risk of being outdated. The answer to that is

We all would. But "they" won't let us by refusing to create more API access points.

| testing though. I can develop a test suite that can let me know I?m out of

Correct.

| date and I need to copy and paste some new code, etc ? Done that before, this
| is tedious, but so what. 
| 
| The other part of the problem (the real part of the problem actually) is that,
| at least when R is built with ICU support, Scollate will depend on a the
| collator pointer in util.c
| https://github.com/wch/r-source/blob/trunk/src/main/util.c#L1777
| 
| And this can be controlled by the base::icuSetCollate function. Of course the
| collator pointer is not public.

So the next (and even less pleasant) answer is to build a new package which
links to, (or worse yet, embeds) libicu.  

As you want ICU behaviour, you will need ICU code.

Dirk

-- 
Dirk Eddelbuettel | edd at debian.org | http://dirk.eddelbuettel.com


From kevinushey at gmail.com  Thu Mar 27 20:01:46 2014
From: kevinushey at gmail.com (Kevin Ushey)
Date: Thu, 27 Mar 2014 12:01:46 -0700
Subject: [Rd] internal string comparison (Scollate)
In-Reply-To: <21300.26905.645966.939976@max.nulle.part>
References: <D3531066-EF05-4C8D-8B83-581C7F042D0D@r-enthusiasts.com>
	<21299.1377.790733.398726@max.nulle.part>
	<CADwqtCOwJCCWYVEk5KHs1auzyBKr1iL1OdDxpCMwV0rAHLEdPA@mail.gmail.com>
	<C2D5C840-B9A6-430C-B16F-8ED59460516B@r-enthusiasts.com>
	<21300.26905.645966.939976@max.nulle.part>
Message-ID: <CAJXgQP3sVi8wKo8tdu_8xckCPj7=Goh9SZC90_-WKiMwVSOHHA@mail.gmail.com>

I too think it would be useful if R exported some version of its
string sorting routines, since sorting strings while respecting
locale, and doing so in a portable fashion while respecting the user's
environment, is not trivial. R holds a fast, portable, well-tested
solution, and I think package developers would be very appreciative if
some portion of this was exposed at the C level.

If not `Scollate`, then perhaps other candidates could be the more
generic `sortVector`, or the more string-specific (and NA-respecting)
`scmp`.

I understand that the volunteers at R Core have limited time and
resources, and exposing an API imposes additional maintenance burdens
on an already thinly stretched team, but this is a situation where the
R users and package authors alike could benefit. Or, if there are
other reasons why exporting such routines is not possible nor
recommended, it would be very informative to know why.

Thanks,
Kevin

On Thu, Mar 27, 2014 at 11:08 AM, Dirk Eddelbuettel <edd at debian.org> wrote:
>
> On 26 March 2014 at 19:09, Romain Fran?ois wrote:
> | That's one part of the problem. Indeed I'd rather use something rather than
> | copy and paste it and run the risk of being outdated. The answer to that is
>
> We all would. But "they" won't let us by refusing to create more API access points.
>
> | testing though. I can develop a test suite that can let me know I'm out of
>
> Correct.
>
> | date and I need to copy and paste some new code, etc ... Done that before, this
> | is tedious, but so what.
> |
> | The other part of the problem (the real part of the problem actually) is that,
> | at least when R is built with ICU support, Scollate will depend on a the
> | collator pointer in util.c
> | https://github.com/wch/r-source/blob/trunk/src/main/util.c#L1777
> |
> | And this can be controlled by the base::icuSetCollate function. Of course the
> | collator pointer is not public.
>
> So the next (and even less pleasant) answer is to build a new package which
> links to, (or worse yet, embeds) libicu.
>
> As you want ICU behaviour, you will need ICU code.
>
> Dirk
>
> --
> Dirk Eddelbuettel | edd at debian.org | http://dirk.eddelbuettel.com
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From murdoch.duncan at gmail.com  Thu Mar 27 22:12:29 2014
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Thu, 27 Mar 2014 17:12:29 -0400
Subject: [Rd] internal string comparison (Scollate)
In-Reply-To: <CAJXgQP3sVi8wKo8tdu_8xckCPj7=Goh9SZC90_-WKiMwVSOHHA@mail.gmail.com>
References: <D3531066-EF05-4C8D-8B83-581C7F042D0D@r-enthusiasts.com>	<21299.1377.790733.398726@max.nulle.part>	<CADwqtCOwJCCWYVEk5KHs1auzyBKr1iL1OdDxpCMwV0rAHLEdPA@mail.gmail.com>	<C2D5C840-B9A6-430C-B16F-8ED59460516B@r-enthusiasts.com>	<21300.26905.645966.939976@max.nulle.part>
	<CAJXgQP3sVi8wKo8tdu_8xckCPj7=Goh9SZC90_-WKiMwVSOHHA@mail.gmail.com>
Message-ID: <5334943D.1020901@gmail.com>

On 14-03-27 3:01 PM, Kevin Ushey wrote:
> I too think it would be useful if R exported some version of its
> string sorting routines, since sorting strings while respecting
> locale, and doing so in a portable fashion while respecting the user's
> environment, is not trivial. R holds a fast, portable, well-tested
> solution, and I think package developers would be very appreciative if
> some portion of this was exposed at the C level.

It does.  You can put your strings in an R STRSXP vector, and call the R 
sort function on it.

The usual objection to constructing an R expression and evaluating it is 
that it is slow, but if you are talking about sorting, the time spent in 
the sort is likely to dominate the time spent in the setup.

>
> If not `Scollate`, then perhaps other candidates could be the more
> generic `sortVector`, or the more string-specific (and NA-respecting)
> `scmp`.

Evaluating an R expression gives you sortVector.

I can see an argument for Scollate being useful (sorting isn't the only 
reason to compare strings), but I can see arguments against exposing it 
too.  Take a look at the source:  it needs to be used carefully.  In 
particular, it can return a 0 for unequal strings, and users are likely 
to get messed up by that, or to submit bogus bug reports.  And it's not 
impossible to work around: if you can collect the universe of strings to 
compare in advance, then just use order() to convert them to integer 
values, and compare those.

Duncan Murdoch

>
> I understand that the volunteers at R Core have limited time and
> resources, and exposing an API imposes additional maintenance burdens
> on an already thinly stretched team, but this is a situation where the
> R users and package authors alike could benefit. Or, if there are
> other reasons why exporting such routines is not possible nor
> recommended, it would be very informative to know why.
>
> Thanks,
> Kevin
>
> On Thu, Mar 27, 2014 at 11:08 AM, Dirk Eddelbuettel <edd at debian.org> wrote:
>>
>> On 26 March 2014 at 19:09, Romain Fran?ois wrote:
>> | That's one part of the problem. Indeed I'd rather use something rather than
>> | copy and paste it and run the risk of being outdated. The answer to that is
>>
>> We all would. But "they" won't let us by refusing to create more API access points.
>>
>> | testing though. I can develop a test suite that can let me know I'm out of
>>
>> Correct.
>>
>> | date and I need to copy and paste some new code, etc ... Done that before, this
>> | is tedious, but so what.
>> |
>> | The other part of the problem (the real part of the problem actually) is that,
>> | at least when R is built with ICU support, Scollate will depend on a the
>> | collator pointer in util.c
>> | https://github.com/wch/r-source/blob/trunk/src/main/util.c#L1777
>> |
>> | And this can be controlled by the base::icuSetCollate function. Of course the
>> | collator pointer is not public.
>>
>> So the next (and even less pleasant) answer is to build a new package which
>> links to, (or worse yet, embeds) libicu.
>>
>> As you want ICU behaviour, you will need ICU code.
>>
>> Dirk
>>
>> --
>> Dirk Eddelbuettel | edd at debian.org | http://dirk.eddelbuettel.com
>>
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


From Rainer at krugs.de  Fri Mar 28 12:01:01 2014
From: Rainer at krugs.de (Rainer M Krug)
Date: Fri, 28 Mar 2014 12:01:01 +0100
Subject: [Rd] creating namespaces outside packages
Message-ID: <m2wqfewpsi.fsf@krugs.de>

Hi

I would like to use namespaces outside packages, but I could not find
any references on how to do it (only a thread [1] which says "use a
package"). Using a package is not possible in my case, as I am passing
variables from org-mode / emacs to R and would like to avoid name
clashes. This is a dynamic process, and each time the code is evaluated,
the variable can be different.

I am putting them at the moment into an environment which is locked, but
I would like to avoid name clashes, so the idea of using environments.

So: is there a way to create a namespace and populate it as I can do
with an environment?

Thanks,

Rainer


Footnotes: 
[1]  http://r.789695.n4.nabble.com/create-namespace-without-creating-a-package-td3485968.html

-- 
Rainer M. Krug
email: Rainer<at>krugs<dot>de
PGP: 0x0F52F982
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 494 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140328/23cb2d6a/attachment.bin>

From murdoch.duncan at gmail.com  Fri Mar 28 12:58:56 2014
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Fri, 28 Mar 2014 07:58:56 -0400
Subject: [Rd] creating namespaces outside packages
In-Reply-To: <m2wqfewpsi.fsf@krugs.de>
References: <m2wqfewpsi.fsf@krugs.de>
Message-ID: <53356400.7040000@gmail.com>

On 28/03/2014, 7:01 AM, Rainer M Krug wrote:
> Hi
>
> I would like to use namespaces outside packages, but I could not find
> any references on how to do it (only a thread [1] which says "use a
> package"). Using a package is not possible in my case, as I am passing
> variables from org-mode / emacs to R and would like to avoid name
> clashes. This is a dynamic process, and each time the code is evaluated,
> the variable can be different.
>
> I am putting them at the moment into an environment which is locked, but
> I would like to avoid name clashes, so the idea of using environments.
>
> So: is there a way to create a namespace and populate it as I can do
> with an environment?

I don't know what you think is the difference between a namespace and an 
environment.  I would say a namespace is one of the environments 
associated with a package, i.e. it's just an environment in a particular 
context.

So depending on what you are trying to accomplish, it may be fine to 
just set up an environment.  Why do you think that won't work?

Duncan Murdoch


From Rainer at krugs.de  Fri Mar 28 13:42:29 2014
From: Rainer at krugs.de (Rainer M Krug)
Date: Fri, 28 Mar 2014 13:42:29 +0100
Subject: [Rd] SOLVED: creating namespaces outside packages
In-Reply-To: <53356400.7040000@gmail.com> (Duncan Murdoch's message of "Fri,
	28 Mar 2014 07:58:56 -0400")
References: <m2wqfewpsi.fsf@krugs.de> <53356400.7040000@gmail.com>
Message-ID: <m2k3bewl3e.fsf_-_@krugs.de>

Duncan Murdoch <murdoch.duncan at gmail.com> writes:

> On 28/03/2014, 7:01 AM, Rainer M Krug wrote:
>> Hi
>>
>> I would like to use namespaces outside packages, but I could not find
>> any references on how to do it (only a thread [1] which says "use a
>> package"). Using a package is not possible in my case, as I am passing
>> variables from org-mode / emacs to R and would like to avoid name
>> clashes. This is a dynamic process, and each time the code is evaluated,
>> the variable can be different.
>>
>> I am putting them at the moment into an environment which is locked, but
>> I would like to avoid name clashes, so the idea of using environments.
>>
>> So: is there a way to create a namespace and populate it as I can do
>> with an environment?
>
> I don't know what you think is the difference between a namespace and
> an environment.  I would say a namespace is one of the environments
> associated with a package, i.e. it's just an environment in a
> particular context.
>
> So depending on what you are trying to accomplish, it may be fine to
> just set up an environment.  Why do you think that won't work?

An environment is what I use at the moment, but there is one aspect I
was not happy with: accessing the original value in the environment when
it is overwritten in a higher environment in the search path. But then I
discovered then notation of using $:

,----
| env <- new.env()
| assign("value", 99, env)
| attach(env)
| value <- FALSE
| value
| env$value
`----

So I can access the original value. I only thought initially about
the :: (or is it :::) to access the objects in a namespace, which do not
work for an environment.

So: Different solution and works perfectly.

Thanks,

Rainer




>
> Duncan Murdoch
>

-- 
Rainer M. Krug
email: Rainer<at>krugs<dot>de
PGP: 0x0F52F982
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: application/pgp-signature
Size: 494 bytes
Desc: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140328/bc726fd1/attachment.bin>

From schreierlc at gmail.com  Fri Mar 28 19:39:51 2014
From: schreierlc at gmail.com (lschreier)
Date: Fri, 28 Mar 2014 11:39:51 -0700 (PDT)
Subject: [Rd] savePlot() under Windows
In-Reply-To: <CANROs4f595-uM3SUm4HBfKLYU2UKXPuLkXMOoNdPTQSrEAcfrA@mail.gmail.com>
References: <CANROs4f595-uM3SUm4HBfKLYU2UKXPuLkXMOoNdPTQSrEAcfrA@mail.gmail.com>
Message-ID: <1396031991591-4687780.post@n4.nabble.com>

Hi
 I know I'm a year late to the party, but under 3.0.2 on a windows pc, I
can't get saveplot to save a file even to my desktop -- is there up-to-date
documentation or code snippets someone can share (the doc in the help files
dont work for me)

thanks in advance
Lou




--
View this message in context: http://r.789695.n4.nabble.com/savePlot-under-Windows-tp4663712p4687780.html
Sent from the R devel mailing list archive at Nabble.com.


From florian.burkart at gmail.com  Fri Mar 28 22:55:37 2014
From: florian.burkart at gmail.com (Florian Burkart)
Date: Fri, 28 Mar 2014 22:55:37 +0100
Subject: [Rd] Error: C stack usage is too close to the limit
Message-ID: <CAPSN_LUYkKv42YecOApxQwKZu7mNNBOWP4sgBO=0JHzQDzHS8g@mail.gmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
URL: <https://stat.ethz.ch/pipermail/r-devel/attachments/20140328/88584b28/attachment.pl>

From ripley at stats.ox.ac.uk  Sat Mar 29 00:13:34 2014
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 28 Mar 2014 23:13:34 +0000
Subject: [Rd] Error: C stack usage is too close to the limit
In-Reply-To: <CAPSN_LUYkKv42YecOApxQwKZu7mNNBOWP4sgBO=0JHzQDzHS8g@mail.gmail.com>
References: <CAPSN_LUYkKv42YecOApxQwKZu7mNNBOWP4sgBO=0JHzQDzHS8g@mail.gmail.com>
Message-ID: <5336021E.9040309@stats.ox.ac.uk>

On 28/03/2014 21:55, Florian Burkart wrote:
> Hi,
>
> I have been using my own C++ plugin for a while.
>
> On a new machine I now keep getting the
>
> C stack usage is too close to the limit
>
> error. I played with it and it appears to come after I printed a fixed
> number of text to R via Rprintf (not from the main thread).
>
> Didn't happen on the old machine. Old version was 2.14. Now on 3.0.3.
>
> Not sure whats going on.

Did you read 'Writing R Extensions' about the support for threads?  E.g.

'Calling any of the R API from threaded code is ?for experts only?: they 
will need to read the source code to determine if it is thread-safe.  In 
particular, code which makes use of the stack-checking mechanism must 
not be called from threaded code.'

It reads like that is what you did.


>
> Florian
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
>


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


From randy.cs.lai at gmail.com  Sat Mar 29 09:12:32 2014
From: randy.cs.lai at gmail.com (Randy Lai)
Date: Sat, 29 Mar 2014 01:12:32 -0700
Subject: [Rd] Error: C stack usage is too close to the limit
In-Reply-To: <CAPSN_LUYkKv42YecOApxQwKZu7mNNBOWP4sgBO=0JHzQDzHS8g@mail.gmail.com>
References: <CAPSN_LUYkKv42YecOApxQwKZu7mNNBOWP4sgBO=0JHzQDzHS8g@mail.gmail.com>
Message-ID: <80195E5E-FBA4-472A-A161-C0286CC134A0@gmail.com>

I have similar experience previously. What I done was disabling stack limit checking
    
    R_CStackLimit = -1;

and increasing the stack size to .16*1024*1024.

Hope it helps.

Randy

On Mar 28, 2014, at 2:55 PM, Florian Burkart <florian.burkart at gmail.com> wrote:

> Hi,
> 
> I have been using my own C++ plugin for a while.
> 
> On a new machine I now keep getting the
> 
> C stack usage is too close to the limit
> 
> error. I played with it and it appears to come after I printed a fixed
> number of text to R via Rprintf (not from the main thread).
> 
> Didn't happen on the old machine. Old version was 2.14. Now on 3.0.3.
> 
> Not sure whats going on.
> 
> Florian
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From simon.urbanek at r-project.org  Sat Mar 29 20:35:05 2014
From: simon.urbanek at r-project.org (Simon Urbanek)
Date: Sat, 29 Mar 2014 15:35:05 -0400
Subject: [Rd] Error: C stack usage is too close to the limit
In-Reply-To: <80195E5E-FBA4-472A-A161-C0286CC134A0@gmail.com>
References: <CAPSN_LUYkKv42YecOApxQwKZu7mNNBOWP4sgBO=0JHzQDzHS8g@mail.gmail.com>
	<80195E5E-FBA4-472A-A161-C0286CC134A0@gmail.com>
Message-ID: <A2449657-C45A-4845-B723-3CBF992F6A73@r-project.org>


On Mar 29, 2014, at 4:12 AM, Randy Lai <randy.cs.lai at gmail.com> wrote:

> I have similar experience previously. What I done was disabling stack limit checking
> 
>    R_CStackLimit = -1;
> 
> and increasing the stack size to .16*1024*1024.
> 
> Hope it helps.
> 

Well, it will result in R crashing instead :)

So be careful with this approach - you will get crashes that will seem inexplicable. Also note that calling R from different threads is unsafe unless you also make sure that the error handling doesn't cross stacks or thread boundaries. You should rather enqueue R calls to the main thread, unless you are really a guru and know how to avoid such issues across signals and errors.

Cheers,
Simon


> Randy
> 
> On Mar 28, 2014, at 2:55 PM, Florian Burkart <florian.burkart at gmail.com> wrote:
> 
>> Hi,
>> 
>> I have been using my own C++ plugin for a while.
>> 
>> On a new machine I now keep getting the
>> 
>> C stack usage is too close to the limit
>> 
>> error. I played with it and it appears to come after I printed a fixed
>> number of text to R via Rprintf (not from the main thread).
>> 
>> Didn't happen on the old machine. Old version was 2.14. Now on 3.0.3.
>> 
>> Not sure whats going on.
>> 
>> Florian
>> 
>> 	[[alternative HTML version deleted]]
>> 
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> 


From mtmorgan at fhcrc.org  Sun Mar 30 21:50:22 2014
From: mtmorgan at fhcrc.org (Martin Morgan)
Date: Sun, 30 Mar 2014 12:50:22 -0700
Subject: [Rd] CXX_STD and configure.ac in packages
Message-ID: <5338757E.60305@fhcrc.org>

In C++ code for use in a R-3.1.0 package, my specific problem is that I would 
like to use <unordered_map> if it is available, or <tr1/unordered_map> if not, 
or <map> if all else fails.

I (think I) can accomplish this with configure.ac as

AC_INIT("DESCRIPTION")

CXX=`"${R_HOME}/bin/R" CMD config CXX`
CXXFLAGS=`"${R_HOME}/bin/R" CMD config CXXFLAGS`

AC_CONFIG_HEADERS([src/config.h])
AC_LANG(C++)
AC_CHECK_HEADERS([unordered_map tr1/unordered_map])
AC_OUTPUT

Use of configure.ac does not seem to be entirely consistent with section 1.2.4 
of Writing R Extensions, where one is advised that to use C++(11? see below) 
code one should

     CXX_STD = CXX11

in Makevars(.win). My code does not require a compiler that supports the full 
C++11 feature set. In addition, I do not understand the logic of setting a 
variable that influences compiler flags in Makevars -- configure.ac will see a 
compiler with inaccurate flags.

Is use of configure.ac orthogonal to setting CXX_STD=CXX11?

Some minor typos:

/R-3-1-branch$ svn diff
Index: doc/manual/R-exts.texi
===================================================================
--- doc/manual/R-exts.texi	(revision 65339)
+++ doc/manual/R-exts.texi	(working copy)
@@ -2250,7 +2250,7 @@
  @subsection Using C++11 code

  @R{} can be built without a C++ compiler although one is available
-(but not necessarily installed) or all known @R{} platforms.
+(but not necessarily installed) on all known @R{} platforms.
  For full portability across platforms, all
  that can be assumed is approximate support for the C++98 standard (the
  widely used @command{g++} deviates considerably from the standard).
@@ -2272,7 +2272,7 @@
  support a flag @option{-std=c++0x}, but the latter only provides partial
  support for the C++11 standard.

-In order to use C++ code in a package, the package's @file{Makevars}
+In order to use C++11 code in a package, the package's @file{Makevars}
  file (or @file{Makevars.win} on Windows) should include the line

  @example
@@ -2329,7 +2329,7 @@
  anything other than the GNU version of C++98 and GNU extensions (which
  include TR1).  The default compiler on Windows is GCC 4.6.x and supports
  the @option{-std=c++0x} flag and some C++11 features (see
- at uref{http://gcc.gnu.org/gcc-4.6/cxx0x_status.html}.  On these
+ at uref{http://gcc.gnu.org/gcc-4.6/cxx0x_status.html}).  On these
  platforms, it is necessary to select a different compiler for C++11, as
  described above, @emph{via} personal @file{Makevars} files.  For
  example, on OS X 10.7 or later one could select @command{clang++}.

-- 
Computational Biology / Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N.
PO Box 19024 Seattle, WA 98109

Location: Arnold Building M1 B861
Phone: (206) 667-2793


From edd at debian.org  Mon Mar 31 01:33:57 2014
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 30 Mar 2014 18:33:57 -0500
Subject: [Rd] CXX_STD and configure.ac in packages
In-Reply-To: <5338757E.60305@fhcrc.org>
References: <5338757E.60305@fhcrc.org>
Message-ID: <21304.43493.907516.104083@max.nulle.part>


Hi Martin,

On 30 March 2014 at 12:50, Martin Morgan wrote:
| In C++ code for use in a R-3.1.0 package, my specific problem is that I would 
| like to use <unordered_map> if it is available, or <tr1/unordered_map> if not, 
| or <map> if all else fails.

This non-standardization over the last decade caused a lot of headaches.  

We do have a set of tests in Rcpp (see includes/Rcpp/platform/compiler.h) to
find unordered maps and sets (based on the compiler version). Not too
elegant, but it works.

One possible alternative is to simply use Boost, which is now easy thanks to
the BH package.  Then Boost abstracts this for you and you use Boost for
unordered_map and/or unordered_set.
 
| I (think I) can accomplish this with configure.ac as
| 
| AC_INIT("DESCRIPTION")
| 
| CXX=`"${R_HOME}/bin/R" CMD config CXX`
| CXXFLAGS=`"${R_HOME}/bin/R" CMD config CXXFLAGS`
| 
| AC_CONFIG_HEADERS([src/config.h])
| AC_LANG(C++)
| AC_CHECK_HEADERS([unordered_map tr1/unordered_map])
| AC_OUTPUT
| 
| Use of configure.ac does not seem to be entirely consistent with section 1.2.4 
| of Writing R Extensions, where one is advised that to use C++(11? see below) 
| code one should
| 
|      CXX_STD = CXX11
| 
| in Makevars(.win). My code does not require a compiler that supports the full 
| C++11 feature set. In addition, I do not understand the logic of setting a 
| variable that influences compiler flags in Makevars -- configure.ac will see a 
| compiler with inaccurate flags.

There were some earlier discussions. Basically, R 'learns' what it has
available when it is built, and the CXX_STD = CXX11 setting then selects 
C++11 (or, on older compilers, C++0x) if available.

But you should be able to get with that. The tr1/ directory is by now pretty
old.  Looking at my current server, which has been around a few years and
upgraded, I see

edd at max:~$ ls -l /usr/include/c++/4.?/unordered_map
-rw-r--r-- 1 root root 2448 Jan 30  2013 /usr/include/c++/4.4/unordered_map
-rw-r--r-- 1 root root 1821 Jul  2  2012 /usr/include/c++/4.5/unordered_map
-rw-r--r-- 1 root root 1852 Jun 19  2013 /usr/include/c++/4.6/unordered_map
-rw-r--r-- 1 root root 1889 Sep 23  2013 /usr/include/c++/4.7/unordered_map
-rw-r--r-- 1 root root 1857 Nov 15 09:22 /usr/include/c++/4.8/unordered_map
edd at max:~$ 

Are you really going to get a compiler older than 4.4 (if in g++ world) ?

I'd try to avoid the configure dance unless you really feel you must have it.

Dirk

-- 
Dirk Eddelbuettel | edd at debian.org | http://dirk.eddelbuettel.com


From djsamperi at gmail.com  Mon Mar 31 03:20:57 2014
From: djsamperi at gmail.com (Dominick Samperi)
Date: Sun, 30 Mar 2014 21:20:57 -0400
Subject: [Rd] rgl question
Message-ID: <CADUbQ5iKuXw_A7R2-6JZk0GK99jEOr2eRBBy2_CdfmKAX2Aj7A@mail.gmail.com>

Hello,

If I call lines3d(x,y,z) I get lines connecting each point, but
when I call rgl.lines(x,y,z) I get dashed lines, and adding
something like type='l' leads to an error message. The
docs seem to suggest that rgl.lines() calls lines3d(), so
I would expect the result to be the same.

Any tips would be appreciated.

Thanks,
Dominick


From romain at r-enthusiasts.com  Mon Mar 31 08:22:35 2014
From: romain at r-enthusiasts.com (Romain Francois)
Date: Mon, 31 Mar 2014 08:22:35 +0200
Subject: [Rd] CXX_STD and configure.ac in packages
In-Reply-To: <5338757E.60305@fhcrc.org>
References: <5338757E.60305@fhcrc.org>
Message-ID: <758D1C72-0DB0-406D-A001-1F187FC250F0@r-enthusiasts.com>

Hi, 

My advice would be to use SystemRequirements: C++11

As <unordered_map> is definitely a part of C++11, assuming this version of the standard gives it to you. Your package may not compile on platforms where a C++11 compiler is not available, but perhaps if this becomes a pattern, then such compilers will start to be available, as in the current version of OSX and recent enough versions of various linux distributions. 

The subset of feature that the version of gcc gives you with Rtools might be enough. 

Alternatively, if you use Rcpp, you can use the RCPP_UNORDERED_MAP macro which will expand to either unordered_map or tr1::unordered_map, all the condition compiling is done in Rcpp. 

Romain

Le 30 mars 2014 ? 21:50, Martin Morgan <mtmorgan at fhcrc.org> a ?crit :

> In C++ code for use in a R-3.1.0 package, my specific problem is that I would like to use <unordered_map> if it is available, or <tr1/unordered_map> if not, or <map> if all else fails.
> 
> I (think I) can accomplish this with configure.ac as
> 
> AC_INIT("DESCRIPTION")
> 
> CXX=`"${R_HOME}/bin/R" CMD config CXX`
> CXXFLAGS=`"${R_HOME}/bin/R" CMD config CXXFLAGS`
> 
> AC_CONFIG_HEADERS([src/config.h])
> AC_LANG(C++)
> AC_CHECK_HEADERS([unordered_map tr1/unordered_map])
> AC_OUTPUT
> 
> Use of configure.ac does not seem to be entirely consistent with section 1.2.4 of Writing R Extensions, where one is advised that to use C++(11? see below) code one should
> 
>    CXX_STD = CXX11
> 
> in Makevars(.win). My code does not require a compiler that supports the full C++11 feature set. In addition, I do not understand the logic of setting a variable that influences compiler flags in Makevars -- configure.ac will see a compiler with inaccurate flags.
> 
> Is use of configure.ac orthogonal to setting CXX_STD=CXX11?
> 
> Some minor typos:
> 
> /R-3-1-branch$ svn diff
> Index: doc/manual/R-exts.texi
> ===================================================================
> --- doc/manual/R-exts.texi	(revision 65339)
> +++ doc/manual/R-exts.texi	(working copy)
> @@ -2250,7 +2250,7 @@
> @subsection Using C++11 code
> 
> @R{} can be built without a C++ compiler although one is available
> -(but not necessarily installed) or all known @R{} platforms.
> +(but not necessarily installed) on all known @R{} platforms.
> For full portability across platforms, all
> that can be assumed is approximate support for the C++98 standard (the
> widely used @command{g++} deviates considerably from the standard).
> @@ -2272,7 +2272,7 @@
> support a flag @option{-std=c++0x}, but the latter only provides partial
> support for the C++11 standard.
> 
> -In order to use C++ code in a package, the package's @file{Makevars}
> +In order to use C++11 code in a package, the package's @file{Makevars}
> file (or @file{Makevars.win} on Windows) should include the line
> 
> @example
> @@ -2329,7 +2329,7 @@
> anything other than the GNU version of C++98 and GNU extensions (which
> include TR1).  The default compiler on Windows is GCC 4.6.x and supports
> the @option{-std=c++0x} flag and some C++11 features (see
> - at uref{http://gcc.gnu.org/gcc-4.6/cxx0x_status.html}.  On these
> + at uref{http://gcc.gnu.org/gcc-4.6/cxx0x_status.html}).  On these
> platforms, it is necessary to select a different compiler for C++11, as
> described above, @emph{via} personal @file{Makevars} files.  For
> example, on OS X 10.7 or later one could select @command{clang++}.
> 
> -- 
> Computational Biology / Fred Hutchinson Cancer Research Center
> 1100 Fairview Ave. N.
> PO Box 19024 Seattle, WA 98109
> 
> Location: Arnold Building M1 B861
> Phone: (206) 667-2793
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From plummerm at iarc.fr  Mon Mar 31 09:09:11 2014
From: plummerm at iarc.fr (Martyn Plummer)
Date: Mon, 31 Mar 2014 07:09:11 +0000
Subject: [Rd] CXX_STD and configure.ac in packages
In-Reply-To: <758D1C72-0DB0-406D-A001-1F187FC250F0@r-enthusiasts.com>
References: <5338757E.60305@fhcrc.org>,
	<758D1C72-0DB0-406D-A001-1F187FC250F0@r-enthusiasts.com>
Message-ID: <31E214B6DF75104E942856C2F74953CB050F4790@exchange>

Hi Martin,

Thanks for the patch. I have applied it. I also added CXX1X and friends to the list of approved variables for R CMD config.
So you can now query the existence of C++11 support with `R CMD config CXX1X` (It is empty if C++11 support is not available)
and then take appropriate action in your configure script if, in Dirk's words, you want to do the configure dance.

The philosophy underlying C++ support in R is that there are only two standards - C++98 and C++11 - and that
you should write to one of those standards. Nobody should be writing new code that uses TR1 extensions now: they are
superseded by the new standard.

The map and unordered_map classes are a corner case, as they offer the same functionality but latter has much better
complexity guarantees, so it is tempting to use it when available.  But from a global perspective you should think of
C++98 and C++11 as two different languages.

Martyn


________________________________________
From: r-devel-bounces at r-project.org [r-devel-bounces at r-project.org] on behalf of Romain Francois [romain at r-enthusiasts.com]
Sent: 31 March 2014 08:22
To: Martin Morgan
Cc: R-devel
Subject: Re: [Rd] CXX_STD and configure.ac in packages

Hi,

My advice would be to use SystemRequirements: C++11

As <unordered_map> is definitely a part of C++11, assuming this version of the standard gives it to you. Your package may not compile on platforms where a C++11 compiler is not available, but perhaps if this becomes a pattern, then such compilers will start to be available, as in the current version of OSX and recent enough versions of various linux distributions.

The subset of feature that the version of gcc gives you with Rtools might be enough.

Alternatively, if you use Rcpp, you can use the RCPP_UNORDERED_MAP macro which will expand to either unordered_map or tr1::unordered_map, all the condition compiling is done in Rcpp.

Romain

Le 30 mars 2014 ? 21:50, Martin Morgan <mtmorgan at fhcrc.org> a ?crit :

> In C++ code for use in a R-3.1.0 package, my specific problem is that I would like to use <unordered_map> if it is available, or <tr1/unordered_map> if not, or <map> if all else fails.
>
> I (think I) can accomplish this with configure.ac as
>
> AC_INIT("DESCRIPTION")
>
> CXX=`"${R_HOME}/bin/R" CMD config CXX`
> CXXFLAGS=`"${R_HOME}/bin/R" CMD config CXXFLAGS`
>
> AC_CONFIG_HEADERS([src/config.h])
> AC_LANG(C++)
> AC_CHECK_HEADERS([unordered_map tr1/unordered_map])
> AC_OUTPUT
>
> Use of configure.ac does not seem to be entirely consistent with section 1.2.4 of Writing R Extensions, where one is advised that to use C++(11? see below) code one should
>
>    CXX_STD = CXX11
>
> in Makevars(.win). My code does not require a compiler that supports the full C++11 feature set. In addition, I do not understand the logic of setting a variable that influences compiler flags in Makevars -- configure.ac will see a compiler with inaccurate flags.
>
> Is use of configure.ac orthogonal to setting CXX_STD=CXX11?
>
> Some minor typos:
>
> /R-3-1-branch$ svn diff
> Index: doc/manual/R-exts.texi
> ===================================================================
> --- doc/manual/R-exts.texi    (revision 65339)
> +++ doc/manual/R-exts.texi    (working copy)
> @@ -2250,7 +2250,7 @@
> @subsection Using C++11 code
>
> @R{} can be built without a C++ compiler although one is available
> -(but not necessarily installed) or all known @R{} platforms.
> +(but not necessarily installed) on all known @R{} platforms.
> For full portability across platforms, all
> that can be assumed is approximate support for the C++98 standard (the
> widely used @command{g++} deviates considerably from the standard).
> @@ -2272,7 +2272,7 @@
> support a flag @option{-std=c++0x}, but the latter only provides partial
> support for the C++11 standard.
>
> -In order to use C++ code in a package, the package's @file{Makevars}
> +In order to use C++11 code in a package, the package's @file{Makevars}
> file (or @file{Makevars.win} on Windows) should include the line
>
> @example
> @@ -2329,7 +2329,7 @@
> anything other than the GNU version of C++98 and GNU extensions (which
> include TR1).  The default compiler on Windows is GCC 4.6.x and supports
> the @option{-std=c++0x} flag and some C++11 features (see
> - at uref{http://gcc.gnu.org/gcc-4.6/cxx0x_status.html}.  On these
> + at uref{http://gcc.gnu.org/gcc-4.6/cxx0x_status.html}).  On these
> platforms, it is necessary to select a different compiler for C++11, as
> described above, @emph{via} personal @file{Makevars} files.  For
> example, on OS X 10.7 or later one could select @command{clang++}.
>
> --
> Computational Biology / Fred Hutchinson Cancer Research Center
> 1100 Fairview Ave. N.
> PO Box 19024 Seattle, WA 98109
>
> Location: Arnold Building M1 B861
> Phone: (206) 667-2793
>
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

______________________________________________
R-devel at r-project.org mailing list
https://stat.ethz.ch/mailman/listinfo/r-devel
-----------------------------------------------------------------------
This message and its attachments are strictly confidenti...{{dropped:8}}


From murdoch.duncan at gmail.com  Mon Mar 31 11:44:10 2014
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Mon, 31 Mar 2014 05:44:10 -0400
Subject: [Rd] rgl question
In-Reply-To: <CADUbQ5iKuXw_A7R2-6JZk0GK99jEOr2eRBBy2_CdfmKAX2Aj7A@mail.gmail.com>
References: <CADUbQ5iKuXw_A7R2-6JZk0GK99jEOr2eRBBy2_CdfmKAX2Aj7A@mail.gmail.com>
Message-ID: <533938EA.30304@gmail.com>

On 30/03/2014, 9:20 PM, Dominick Samperi wrote:
> Hello,
>
> If I call lines3d(x,y,z) I get lines connecting each point, but
> when I call rgl.lines(x,y,z) I get dashed lines, and adding
> something like type='l' leads to an error message. The
> docs seem to suggest that rgl.lines() calls lines3d(), so
> I would expect the result to be the same.
>
> Any tips would be appreciated.

The difference is in how they use the material properties:  rgl.lines 
sets them permanently, lines3d restores the original value after the 
call.  So I'd guess your call to rgl.lines followed a call to another 
rgl.* function that set the lty property to dashed.

Duncan Murdoch


From plummerm at iarc.fr  Mon Mar 31 12:20:35 2014
From: plummerm at iarc.fr (Martyn Plummer)
Date: Mon, 31 Mar 2014 10:20:35 +0000
Subject: [Rd] CXX_STD and configure.ac in packages
In-Reply-To: <31E214B6DF75104E942856C2F74953CB050F4790@exchange>
References: <5338757E.60305@fhcrc.org>	,
	<758D1C72-0DB0-406D-A001-1F187FC250F0@r-enthusiasts.com>
	<31E214B6DF75104E942856C2F74953CB050F4790@exchange>
Message-ID: <1396261234.4945.86.camel@braque.iarc.fr>

On Mon, 2014-03-31 at 07:09 +0000, Martyn Plummer wrote:
> Hi Martin,
> 
> Thanks for the patch. I have applied it. I also added CXX1X and friends to the list of approved variables for R CMD config.
> So you can now query the existence of C++11 support with `R CMD config CXX1X` (It is empty if C++11 support is not available)
> and then take appropriate action in your configure script if, in Dirk's words, you want to do the configure dance.
> 
> The philosophy underlying C++ support in R is that there are only two standards - C++98 and C++11 - and that
> you should write to one of those standards. 

A should add a clarification. The way I wrote this makes it sound like
an even-handed choice, but only C++98 has cross-platform support. If you
use C++11 then many users will not currently be able to use your code. 

> Nobody should be writing new code that uses TR1 extensions now: they are
> superseded by the new standard.
> 
> The map and unordered_map classes are a corner case, as they offer the same functionality but latter has much better
> complexity guarantees, so it is tempting to use it when available.  But from a global perspective you should think of
> C++98 and C++11 as two different languages.
> 
> Martyn
> 
> 
> ________________________________________
> From: r-devel-bounces at r-project.org [r-devel-bounces at r-project.org] on behalf of Romain Francois [romain at r-enthusiasts.com]
> Sent: 31 March 2014 08:22
> To: Martin Morgan
> Cc: R-devel
> Subject: Re: [Rd] CXX_STD and configure.ac in packages
> 
> Hi,
> 
> My advice would be to use SystemRequirements: C++11
> 
> As <unordered_map> is definitely a part of C++11, assuming this version of the standard gives it to you. Your package may not compile on platforms where a C++11 compiler is not available, but perhaps if this becomes a pattern, then such compilers will start to be available, as in the current version of OSX and recent enough versions of various linux distributions.
> 
> The subset of feature that the version of gcc gives you with Rtools might be enough.
> 
> Alternatively, if you use Rcpp, you can use the RCPP_UNORDERED_MAP macro which will expand to either unordered_map or tr1::unordered_map, all the condition compiling is done in Rcpp.
> 
> Romain
> 
> Le 30 mars 2014 ? 21:50, Martin Morgan <mtmorgan at fhcrc.org> a ?crit :
> 
> > In C++ code for use in a R-3.1.0 package, my specific problem is that I would like to use <unordered_map> if it is available, or <tr1/unordered_map> if not, or <map> if all else fails.
> >
> > I (think I) can accomplish this with configure.ac as
> >
> > AC_INIT("DESCRIPTION")
> >
> > CXX=`"${R_HOME}/bin/R" CMD config CXX`
> > CXXFLAGS=`"${R_HOME}/bin/R" CMD config CXXFLAGS`
> >
> > AC_CONFIG_HEADERS([src/config.h])
> > AC_LANG(C++)
> > AC_CHECK_HEADERS([unordered_map tr1/unordered_map])
> > AC_OUTPUT
> >
> > Use of configure.ac does not seem to be entirely consistent with section 1.2.4 of Writing R Extensions, where one is advised that to use C++(11? see below) code one should
> >
> >    CXX_STD = CXX11
> >
> > in Makevars(.win). My code does not require a compiler that supports the full C++11 feature set. In addition, I do not understand the logic of setting a variable that influences compiler flags in Makevars -- configure.ac will see a compiler with inaccurate flags.
> >
> > Is use of configure.ac orthogonal to setting CXX_STD=CXX11?
> >
> > Some minor typos:
> >
> > /R-3-1-branch$ svn diff
> > Index: doc/manual/R-exts.texi
> > ===================================================================
> > --- doc/manual/R-exts.texi    (revision 65339)
> > +++ doc/manual/R-exts.texi    (working copy)
> > @@ -2250,7 +2250,7 @@
> > @subsection Using C++11 code
> >
> > @R{} can be built without a C++ compiler although one is available
> > -(but not necessarily installed) or all known @R{} platforms.
> > +(but not necessarily installed) on all known @R{} platforms.
> > For full portability across platforms, all
> > that can be assumed is approximate support for the C++98 standard (the
> > widely used @command{g++} deviates considerably from the standard).
> > @@ -2272,7 +2272,7 @@
> > support a flag @option{-std=c++0x}, but the latter only provides partial
> > support for the C++11 standard.
> >
> > -In order to use C++ code in a package, the package's @file{Makevars}
> > +In order to use C++11 code in a package, the package's @file{Makevars}
> > file (or @file{Makevars.win} on Windows) should include the line
> >
> > @example
> > @@ -2329,7 +2329,7 @@
> > anything other than the GNU version of C++98 and GNU extensions (which
> > include TR1).  The default compiler on Windows is GCC 4.6.x and supports
> > the @option{-std=c++0x} flag and some C++11 features (see
> > - at uref{http://gcc.gnu.org/gcc-4.6/cxx0x_status.html}.  On these
> > + at uref{http://gcc.gnu.org/gcc-4.6/cxx0x_status.html}).  On these
> > platforms, it is necessary to select a different compiler for C++11, as
> > described above, @emph{via} personal @file{Makevars} files.  For
> > example, on OS X 10.7 or later one could select @command{clang++}.
> >
> > --
> > Computational Biology / Fred Hutchinson Cancer Research Center
> > 1100 Fairview Ave. N.
> > PO Box 19024 Seattle, WA 98109
> >
> > Location: Arnold Building M1 B861
> > Phone: (206) 667-2793
> >
> > ______________________________________________
> > R-devel at r-project.org mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-devel
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel
> -----------------------------------------------------------------------
> This message and its attachments are strictly confidenti...{{dropped:8}}
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel

-----------------------------------------------------------------------
This message and its attachments are strictly confidenti...{{dropped:8}}


From romain at r-enthusiasts.com  Mon Mar 31 13:30:08 2014
From: romain at r-enthusiasts.com (=?windows-1252?Q?Romain_Fran=E7ois?=)
Date: Mon, 31 Mar 2014 13:30:08 +0200
Subject: [Rd] CXX_STD and configure.ac in packages
In-Reply-To: <1396261234.4945.86.camel@braque.iarc.fr>
References: <5338757E.60305@fhcrc.org>	,
	<758D1C72-0DB0-406D-A001-1F187FC250F0@r-enthusiasts.com>
	<31E214B6DF75104E942856C2F74953CB050F4790@exchange>
	<1396261234.4945.86.camel@braque.iarc.fr>
Message-ID: <A41E34F1-8B60-4E9E-8099-BD2E25800E8C@r-enthusiasts.com>

Le 31 mars 2014 ? 12:20, Martyn Plummer <plummerm at iarc.fr> a ?crit :

> On Mon, 2014-03-31 at 07:09 +0000, Martyn Plummer wrote:
>> Hi Martin,
>> 
>> Thanks for the patch. I have applied it. I also added CXX1X and friends to the list of approved variables for R CMD config.
>> So you can now query the existence of C++11 support with `R CMD config CXX1X` (It is empty if C++11 support is not available)
>> and then take appropriate action in your configure script if, in Dirk's words, you want to do the configure dance.
>> 
>> The philosophy underlying C++ support in R is that there are only two standards - C++98 and C++11 - and that
>> you should write to one of those standards. 
> 
> A should add a clarification. The way I wrote this makes it sound like
> an even-handed choice, but only C++98 has cross-platform support. If you
> use C++11 then many users will not currently be able to use your code. 

OTOH, if nobody goes there, the need for C++11 might not be perceived as important by people who take care of cross platform support. 

Probably not Martin?s fight. One can do the gymnastics to get an unordered_map with C++98 (through boost, tr1, etc ...), but C++11 brings a great combination of new features that make it a better language, and I agree that it is almost a new language. And once you start using it, it is hard to look back. 

>> Nobody should be writing new code that uses TR1 extensions now: they are
>> superseded by the new standard.
>> 
>> The map and unordered_map classes are a corner case, as they offer the same functionality but latter has much better
>> complexity guarantees, so it is tempting to use it when available.  But from a global perspective you should think of
>> C++98 and C++11 as two different languages.
>> 
>> Martyn
>> 
>> ________________________________________
>> From: r-devel-bounces at r-project.org [r-devel-bounces at r-project.org] on behalf of Romain Francois [romain at r-enthusiasts.com]
>> Sent: 31 March 2014 08:22
>> To: Martin Morgan
>> Cc: R-devel
>> Subject: Re: [Rd] CXX_STD and configure.ac in packages
>> 
>> Hi,
>> 
>> My advice would be to use SystemRequirements: C++11
>> 
>> As <unordered_map> is definitely a part of C++11, assuming this version of the standard gives it to you. Your package may not compile on platforms where a C++11 compiler is not available, but perhaps if this becomes a pattern, then such compilers will start to be available, as in the current version of OSX and recent enough versions of various linux distributions.
>> 
>> The subset of feature that the version of gcc gives you with Rtools might be enough.
>> 
>> Alternatively, if you use Rcpp, you can use the RCPP_UNORDERED_MAP macro which will expand to either unordered_map or tr1::unordered_map, all the condition compiling is done in Rcpp.
>> 
>> Romain
>> 
>> Le 30 mars 2014 ? 21:50, Martin Morgan <mtmorgan at fhcrc.org> a ?crit :
>> 
>>> In C++ code for use in a R-3.1.0 package, my specific problem is that I would like to use <unordered_map> if it is available, or <tr1/unordered_map> if not, or <map> if all else fails.
>>> 
>>> I (think I) can accomplish this with configure.ac as
>>> 
>>> AC_INIT("DESCRIPTION")
>>> 
>>> CXX=`"${R_HOME}/bin/R" CMD config CXX`
>>> CXXFLAGS=`"${R_HOME}/bin/R" CMD config CXXFLAGS`
>>> 
>>> AC_CONFIG_HEADERS([src/config.h])
>>> AC_LANG(C++)
>>> AC_CHECK_HEADERS([unordered_map tr1/unordered_map])
>>> AC_OUTPUT
>>> 
>>> Use of configure.ac does not seem to be entirely consistent with section 1.2.4 of Writing R Extensions, where one is advised that to use C++(11? see below) code one should
>>> 
>>>   CXX_STD = CXX11
>>> 
>>> in Makevars(.win). My code does not require a compiler that supports the full C++11 feature set. In addition, I do not understand the logic of setting a variable that influences compiler flags in Makevars -- configure.ac will see a compiler with inaccurate flags.
>>> 
>>> Is use of configure.ac orthogonal to setting CXX_STD=CXX11?
>>> 
>>> Some minor typos:
>>> 
>>> /R-3-1-branch$ svn diff
>>> Index: doc/manual/R-exts.texi
>>> ===================================================================
>>> --- doc/manual/R-exts.texi    (revision 65339)
>>> +++ doc/manual/R-exts.texi    (working copy)
>>> @@ -2250,7 +2250,7 @@
>>> @subsection Using C++11 code
>>> 
>>> @R{} can be built without a C++ compiler although one is available
>>> -(but not necessarily installed) or all known @R{} platforms.
>>> +(but not necessarily installed) on all known @R{} platforms.
>>> For full portability across platforms, all
>>> that can be assumed is approximate support for the C++98 standard (the
>>> widely used @command{g++} deviates considerably from the standard).
>>> @@ -2272,7 +2272,7 @@
>>> support a flag @option{-std=c++0x}, but the latter only provides partial
>>> support for the C++11 standard.
>>> 
>>> -In order to use C++ code in a package, the package's @file{Makevars}
>>> +In order to use C++11 code in a package, the package's @file{Makevars}
>>> file (or @file{Makevars.win} on Windows) should include the line
>>> 
>>> @example
>>> @@ -2329,7 +2329,7 @@
>>> anything other than the GNU version of C++98 and GNU extensions (which
>>> include TR1).  The default compiler on Windows is GCC 4.6.x and supports
>>> the @option{-std=c++0x} flag and some C++11 features (see
>>> - at uref{http://gcc.gnu.org/gcc-4.6/cxx0x_status.html}.  On these
>>> + at uref{http://gcc.gnu.org/gcc-4.6/cxx0x_status.html}).  On these
>>> platforms, it is necessary to select a different compiler for C++11, as
>>> described above, @emph{via} personal @file{Makevars} files.  For
>>> example, on OS X 10.7 or later one could select @command{clang++}.
>>> 
>>> --
>>> Computational Biology / Fred Hutchinson Cancer Research Center
>>> 1100 Fairview Ave. N.
>>> PO Box 19024 Seattle, WA 98109
>>> 
>>> Location: Arnold Building M1 B861
>>> Phone: (206) 667-2793
>>> 
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>> 
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>> -----------------------------------------------------------------------
>> This message and its attachments are strictly confidenti...{{dropped:8}}
>> 
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
> 
> -----------------------------------------------------------------------
> This message and its attachments are strictly confiden...{{dropped:9}}


From mtmorgan at fhcrc.org  Mon Mar 31 16:09:09 2014
From: mtmorgan at fhcrc.org (Martin Morgan)
Date: Mon, 31 Mar 2014 07:09:09 -0700
Subject: [Rd] CXX_STD and configure.ac in packages
In-Reply-To: <A41E34F1-8B60-4E9E-8099-BD2E25800E8C@r-enthusiasts.com>
References: <5338757E.60305@fhcrc.org>	,
	<758D1C72-0DB0-406D-A001-1F187FC250F0@r-enthusiasts.com>
	<31E214B6DF75104E942856C2F74953CB050F4790@exchange>
	<1396261234.4945.86.camel@braque.iarc.fr>
	<A41E34F1-8B60-4E9E-8099-BD2E25800E8C@r-enthusiasts.com>
Message-ID: <53397705.9050608@fhcrc.org>

On 03/31/2014 04:30 AM, Romain Fran?ois wrote:
> Le 31 mars 2014 ? 12:20, Martyn Plummer <plummerm at iarc.fr> a ?crit :
>
>> On Mon, 2014-03-31 at 07:09 +0000, Martyn Plummer wrote:
>>> Hi Martin,
>>>
>>> Thanks for the patch. I have applied it. I also added CXX1X and friends to the list of approved variables for R CMD config.
>>> So you can now query the existence of C++11 support with `R CMD config CXX1X` (It is empty if C++11 support is not available)
>>> and then take appropriate action in your configure script if, in Dirk's words, you want to do the configure dance.

Thanks, this is what I was looking for.

>>>
>>> The philosophy underlying C++ support in R is that there are only two standards - C++98 and C++11 - and that
>>> you should write to one of those standards.
>>
>> A should add a clarification. The way I wrote this makes it sound like
>> an even-handed choice, but only C++98 has cross-platform support. If you
>> use C++11 then many users will not currently be able to use your code.

Yes, the Writing R Extensions section at first seduced me into thinking that I 
could get broad support for C++11 with a simple macro, but obviously that can 
only come from the underlying compilers and R is making no guarantees about these.

> OTOH, if nobody goes there, the need for C++11 might not be perceived as important by people who take care of cross platform support.
>
> Probably not Martin?s fight. One can do the gymnastics to get an unordered_map with C++98 (through boost, tr1, etc ...), but C++11 brings a great combination of new features that make it a better language, and I agree that it is almost a new language. And once you start using it, it is hard to look back.
>
>>> Nobody should be writing new code that uses TR1 extensions now: they are
>>> superseded by the new standard.

For me unordered_map is a small part of a large mostly C code base; using it 
instead of map has substantial benefits, but restricting package use to C++11 
isn't really on the table in this particular case.

I'll take Martyn's philosophical statement that for R there are only two 
standards -- C++98 and C++11, with attendant trade-offs -- as a guiding 
principle and as a pragmatic solution avoid my complicated unordered_map 
configure dance for now.

Thanks all for the various inputs.

Martin Morgan

>>>
>>> The map and unordered_map classes are a corner case, as they offer the same functionality but latter has much better
>>> complexity guarantees, so it is tempting to use it when available.  But from a global perspective you should think of
>>> C++98 and C++11 as two different languages.
>>>
>>> Martyn
>>>
>>> ________________________________________
>>> From: r-devel-bounces at r-project.org [r-devel-bounces at r-project.org] on behalf of Romain Francois [romain at r-enthusiasts.com]
>>> Sent: 31 March 2014 08:22
>>> To: Martin Morgan
>>> Cc: R-devel
>>> Subject: Re: [Rd] CXX_STD and configure.ac in packages
>>>
>>> Hi,
>>>
>>> My advice would be to use SystemRequirements: C++11
>>>
>>> As <unordered_map> is definitely a part of C++11, assuming this version of the standard gives it to you. Your package may not compile on platforms where a C++11 compiler is not available, but perhaps if this becomes a pattern, then such compilers will start to be available, as in the current version of OSX and recent enough versions of various linux distributions.
>>>
>>> The subset of feature that the version of gcc gives you with Rtools might be enough.
>>>
>>> Alternatively, if you use Rcpp, you can use the RCPP_UNORDERED_MAP macro which will expand to either unordered_map or tr1::unordered_map, all the condition compiling is done in Rcpp.
>>>
>>> Romain
>>>
>>> Le 30 mars 2014 ? 21:50, Martin Morgan <mtmorgan at fhcrc.org> a ?crit :
>>>
>>>> In C++ code for use in a R-3.1.0 package, my specific problem is that I would like to use <unordered_map> if it is available, or <tr1/unordered_map> if not, or <map> if all else fails.
>>>>
>>>> I (think I) can accomplish this with configure.ac as
>>>>
>>>> AC_INIT("DESCRIPTION")
>>>>
>>>> CXX=`"${R_HOME}/bin/R" CMD config CXX`
>>>> CXXFLAGS=`"${R_HOME}/bin/R" CMD config CXXFLAGS`
>>>>
>>>> AC_CONFIG_HEADERS([src/config.h])
>>>> AC_LANG(C++)
>>>> AC_CHECK_HEADERS([unordered_map tr1/unordered_map])
>>>> AC_OUTPUT
>>>>
>>>> Use of configure.ac does not seem to be entirely consistent with section 1.2.4 of Writing R Extensions, where one is advised that to use C++(11? see below) code one should
>>>>
>>>>    CXX_STD = CXX11
>>>>
>>>> in Makevars(.win). My code does not require a compiler that supports the full C++11 feature set. In addition, I do not understand the logic of setting a variable that influences compiler flags in Makevars -- configure.ac will see a compiler with inaccurate flags.
>>>>
>>>> Is use of configure.ac orthogonal to setting CXX_STD=CXX11?
>>>>
>>>> Some minor typos:
>>>>
>>>> /R-3-1-branch$ svn diff
>>>> Index: doc/manual/R-exts.texi
>>>> ===================================================================
>>>> --- doc/manual/R-exts.texi    (revision 65339)
>>>> +++ doc/manual/R-exts.texi    (working copy)
>>>> @@ -2250,7 +2250,7 @@
>>>> @subsection Using C++11 code
>>>>
>>>> @R{} can be built without a C++ compiler although one is available
>>>> -(but not necessarily installed) or all known @R{} platforms.
>>>> +(but not necessarily installed) on all known @R{} platforms.
>>>> For full portability across platforms, all
>>>> that can be assumed is approximate support for the C++98 standard (the
>>>> widely used @command{g++} deviates considerably from the standard).
>>>> @@ -2272,7 +2272,7 @@
>>>> support a flag @option{-std=c++0x}, but the latter only provides partial
>>>> support for the C++11 standard.
>>>>
>>>> -In order to use C++ code in a package, the package's @file{Makevars}
>>>> +In order to use C++11 code in a package, the package's @file{Makevars}
>>>> file (or @file{Makevars.win} on Windows) should include the line
>>>>
>>>> @example
>>>> @@ -2329,7 +2329,7 @@
>>>> anything other than the GNU version of C++98 and GNU extensions (which
>>>> include TR1).  The default compiler on Windows is GCC 4.6.x and supports
>>>> the @option{-std=c++0x} flag and some C++11 features (see
>>>> - at uref{http://gcc.gnu.org/gcc-4.6/cxx0x_status.html}.  On these
>>>> + at uref{http://gcc.gnu.org/gcc-4.6/cxx0x_status.html}).  On these
>>>> platforms, it is necessary to select a different compiler for C++11, as
>>>> described above, @emph{via} personal @file{Makevars} files.  For
>>>> example, on OS X 10.7 or later one could select @command{clang++}.
>>>>
>>>> --
>>>> Computational Biology / Fred Hutchinson Cancer Research Center
>>>> 1100 Fairview Ave. N.
>>>> PO Box 19024 Seattle, WA 98109
>>>>
>>>> Location: Arnold Building M1 B861
>>>> Phone: (206) 667-2793
>>>>
>>>> ______________________________________________
>>>> R-devel at r-project.org mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>> -----------------------------------------------------------------------
>>> This message and its attachments are strictly confidenti...{{dropped:8}}
>>>
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>>
>> -----------------------------------------------------------------------
>> This message and its attachments are strictly confidential. If you are
>> not the intended recipient of this message, please immediately notify
>> the sender and delete it. Since its integrity cannot be guaranteed,
>> its content cannot involve the sender's responsibility. Any misuse,
>> any disclosure or publication of its content, either whole or partial,
>> is prohibited, exception made of formally approved use
>> -----------------------------------------------------------------------
>


-- 
Computational Biology / Fred Hutchinson Cancer Research Center
1100 Fairview Ave. N.
PO Box 19024 Seattle, WA 98109

Location: Arnold Building M1 B861
Phone: (206) 667-2793


From romain at r-enthusiasts.com  Mon Mar 31 16:44:54 2014
From: romain at r-enthusiasts.com (=?windows-1252?Q?Romain_Fran=E7ois?=)
Date: Mon, 31 Mar 2014 16:44:54 +0200
Subject: [Rd] internal string comparison (Scollate)
In-Reply-To: <5334943D.1020901@gmail.com>
References: <D3531066-EF05-4C8D-8B83-581C7F042D0D@r-enthusiasts.com>	<21299.1377.790733.398726@max.nulle.part>	<CADwqtCOwJCCWYVEk5KHs1auzyBKr1iL1OdDxpCMwV0rAHLEdPA@mail.gmail.com>	<C2D5C840-B9A6-430C-B16F-8ED59460516B@r-enthusiasts.com>	<21300.26905.645966.939976@max.nulle.part>
	<CAJXgQP3sVi8wKo8tdu_8xckCPj7=Goh9SZC90_-WKiMwVSOHHA@mail.gmail.com>
	<5334943D.1020901@gmail.com>
Message-ID: <6773B8A8-4F2F-453C-9412-89F102CB1CC1@r-enthusiasts.com>

Hello, 

The use case I have might involve sorting many small such STRSXP vectors. 

If I have Scollate, I don?t need to materialize the vectors and I can use the sorting algorithm I choose. 

Here is some made up data: 

df <- data.frame( 
  x = sample( 1:10, 1000, replace = TRUE), 
  y = sample( 1:100, 100, replace = TRUE), 
  z = replicate( 10000, paste( sample(letters, sample(1:100, size = 1), replace = TRUE ), collapse = "" ) ), 
  stringsAsFactors = FALSE
)

For which I?d like something like what order( df$x, df$y, df$z ) gives me. 

For example: 

> system.time( res1 <- order( df$x, df$y, df$z) )
utilisateur     syst?me      ?coul?
      0.017       0.000       0.017
> system.time( res2 <- dplyr::order_( df$x, df$y, df$z ) )
utilisateur     syst?me      ?coul?
      0.005       0.000       0.005
> identical( res1, res2 )
[1] TRUE

The way dplyr::order_ is implemented I don?t need to materialize 500 STRSXP vectors and call order or sort on them ( 492 == nrow( unique( df[, c("x", "y" ) ] ) ) )

I just need to be able to compare two scalars together (either two ints, two doubles, or two CHARSXP SEXP). We already have special code to handle what it means to compare int, double etc in the R world with NA and NaN, etc ... 

Scollate would give a way to compare two CHARSXP SEXP, the way R would. Of course one has to be careful how it is called, I have read the source. 

Materialising temporary values into an R vector may be the R way of doing things, but sometimes it is a waste of both memory and time. Yes, this is about performance. We are often asked to choose between performance and correctness when in fact we can have both. 

Romain

Le 27 mars 2014 ? 22:12, Duncan Murdoch <murdoch.duncan at gmail.com> a ?crit :

> On 14-03-27 3:01 PM, Kevin Ushey wrote:
>> I too think it would be useful if R exported some version of its
>> string sorting routines, since sorting strings while respecting
>> locale, and doing so in a portable fashion while respecting the user's
>> environment, is not trivial. R holds a fast, portable, well-tested
>> solution, and I think package developers would be very appreciative if
>> some portion of this was exposed at the C level.
> 
> It does.  You can put your strings in an R STRSXP vector, and call the R sort function on it.
> 
> The usual objection to constructing an R expression and evaluating it is that it is slow, but if you are talking about sorting, the time spent in the sort is likely to dominate the time spent in the setup.
> 
>> 
>> If not `Scollate`, then perhaps other candidates could be the more
>> generic `sortVector`, or the more string-specific (and NA-respecting)
>> `scmp`.
> 
> Evaluating an R expression gives you sortVector.
> 
> I can see an argument for Scollate being useful (sorting isn't the only reason to compare strings), but I can see arguments against exposing it too.  Take a look at the source:  it needs to be used carefully.  In particular, it can return a 0 for unequal strings, and users are likely to get messed up by that, or to submit bogus bug reports.  And it's not impossible to work around: if you can collect the universe of strings to compare in advance, then just use order() to convert them to integer values, and compare those.
> 
> Duncan Murdoch
> 
>> 
>> I understand that the volunteers at R Core have limited time and
>> resources, and exposing an API imposes additional maintenance burdens
>> on an already thinly stretched team, but this is a situation where the
>> R users and package authors alike could benefit. Or, if there are
>> other reasons why exporting such routines is not possible nor
>> recommended, it would be very informative to know why.
>> 
>> Thanks,
>> Kevin
>> 
>> On Thu, Mar 27, 2014 at 11:08 AM, Dirk Eddelbuettel <edd at debian.org> wrote:
>>> 
>>> On 26 March 2014 at 19:09, Romain Fran?ois wrote:
>>> | That's one part of the problem. Indeed I'd rather use something rather than
>>> | copy and paste it and run the risk of being outdated. The answer to that is
>>> 
>>> We all would. But "they" won't let us by refusing to create more API access points.
>>> 
>>> | testing though. I can develop a test suite that can let me know I'm out of
>>> 
>>> Correct.
>>> 
>>> | date and I need to copy and paste some new code, etc ... Done that before, this
>>> | is tedious, but so what.
>>> |
>>> | The other part of the problem (the real part of the problem actually) is that,
>>> | at least when R is built with ICU support, Scollate will depend on a the
>>> | collator pointer in util.c
>>> | https://github.com/wch/r-source/blob/trunk/src/main/util.c#L1777
>>> |
>>> | And this can be controlled by the base::icuSetCollate function. Of course the
>>> | collator pointer is not public.
>>> 
>>> So the next (and even less pleasant) answer is to build a new package which
>>> links to, (or worse yet, embeds) libicu.
>>> 
>>> As you want ICU behaviour, you will need ICU code.
>>> 
>>> Dirk
>>> 
>>> --
>>> Dirk Eddelbuettel | edd at debian.org | http://dirk.eddelbuettel.com
>>> 
>>> ______________________________________________
>>> R-devel at r-project.org mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-devel
>> 
>> ______________________________________________
>> R-devel at r-project.org mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-devel
>> 
> 
> ______________________________________________
> R-devel at r-project.org mailing list
> https://stat.ethz.ch/mailman/listinfo/r-devel


From djsamperi at gmail.com  Mon Mar 31 18:56:51 2014
From: djsamperi at gmail.com (Dominick Samperi)
Date: Mon, 31 Mar 2014 12:56:51 -0400
Subject: [Rd] rgl question
In-Reply-To: <533938EA.30304@gmail.com>
References: <CADUbQ5iKuXw_A7R2-6JZk0GK99jEOr2eRBBy2_CdfmKAX2Aj7A@mail.gmail.com>
	<533938EA.30304@gmail.com>
Message-ID: <CADUbQ5juFZ0R2O-93kEFPfGF2U8O4n+YRZMeCs2z4i0j8JVA3g@mail.gmail.com>

Thanks for the comment. No, there were no such prior calls,
unless rgl.lines() itself sets lty to dashed?

Here is a simple session run under Windows:
library(rgl)
x <- 1:20
y <- 1:20
z <- 1:20
rgl.lines(x,y,z) # displays dashed line
lines3d(x,y,z)  # displays solid line

I'm using R 3.1.0 alpha

On Mon, Mar 31, 2014 at 5:44 AM, Duncan Murdoch
<murdoch.duncan at gmail.com> wrote:
> On 30/03/2014, 9:20 PM, Dominick Samperi wrote:
>>
>> Hello,
>>
>> If I call lines3d(x,y,z) I get lines connecting each point, but
>> when I call rgl.lines(x,y,z) I get dashed lines, and adding
>> something like type='l' leads to an error message. The
>> docs seem to suggest that rgl.lines() calls lines3d(), so
>> I would expect the result to be the same.
>>
>> Any tips would be appreciated.
>
>
> The difference is in how they use the material properties:  rgl.lines sets
> them permanently, lines3d restores the original value after the call.  So
> I'd guess your call to rgl.lines followed a call to another rgl.* function
> that set the lty property to dashed.
>
> Duncan Murdoch


From murdoch.duncan at gmail.com  Mon Mar 31 19:49:18 2014
From: murdoch.duncan at gmail.com (Duncan Murdoch)
Date: Mon, 31 Mar 2014 13:49:18 -0400
Subject: [Rd] rgl question
In-Reply-To: <CADUbQ5juFZ0R2O-93kEFPfGF2U8O4n+YRZMeCs2z4i0j8JVA3g@mail.gmail.com>
References: <CADUbQ5iKuXw_A7R2-6JZk0GK99jEOr2eRBBy2_CdfmKAX2Aj7A@mail.gmail.com>	<533938EA.30304@gmail.com>
	<CADUbQ5juFZ0R2O-93kEFPfGF2U8O4n+YRZMeCs2z4i0j8JVA3g@mail.gmail.com>
Message-ID: <5339AA9E.7090409@gmail.com>

On 31/03/2014 12:56 PM, Dominick Samperi wrote:
> Thanks for the comment. No, there were no such prior calls,
> unless rgl.lines() itself sets lty to dashed?
>
> Here is a simple session run under Windows:
> library(rgl)
> x <- 1:20
> y <- 1:20
> z <- 1:20
> rgl.lines(x,y,z) # displays dashed line
> lines3d(x,y,z)  # displays solid line

Sorry, what I said was true, but wasn't helpful.  The real explanation 
is that rgl.lines corresponds to segments3d, not to lines3d.  It pairs 
up the points and draws line segments, it doesn't join the points.  Use 
rgl.linestrips (the OpenGL terminology) if you want the equivalent of 
lines3d but with the persistant material properties.

I had forgotten that, because I never use the rgl.* functions.  I would 
say "neither should you", but there might be some good reason to do so.

Duncan Murdoch
>
> I'm using R 3.1.0 alpha
>
> On Mon, Mar 31, 2014 at 5:44 AM, Duncan Murdoch
> <murdoch.duncan at gmail.com> wrote:
> > On 30/03/2014, 9:20 PM, Dominick Samperi wrote:
> >>
> >> Hello,
> >>
> >> If I call lines3d(x,y,z) I get lines connecting each point, but
> >> when I call rgl.lines(x,y,z) I get dashed lines, and adding
> >> something like type='l' leads to an error message. The
> >> docs seem to suggest that rgl.lines() calls lines3d(), so
> >> I would expect the result to be the same.
> >>
> >> Any tips would be appreciated.
> >
> >
> > The difference is in how they use the material properties:  rgl.lines sets
> > them permanently, lines3d restores the original value after the call.  So
> > I'd guess your call to rgl.lines followed a call to another rgl.* function
> > that set the lty property to dashed.
> >
> > Duncan Murdoch


